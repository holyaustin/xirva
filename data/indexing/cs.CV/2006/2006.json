[{"id": "2006.00027", "submitter": "Valery Naranjo Ornedo", "authors": "Gabriel Garc\\'ia, Roc\\'io del Amor, Adri\\'an Colomer, Valery Naranjo", "title": "Glaucoma Detection From Raw Circumapillary OCT Images Using Fully\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, glaucoma is the leading cause of blindness worldwide. We propose in\nthis paper two different deep-learning-based approaches to address glaucoma\ndetection just from raw circumpapillary OCT images. The first one is based on\nthe development of convolutional neural networks (CNNs) trained from scratch.\nThe second one lies in fine-tuning some of the most common state-of-the-art\nCNNs architectures. The experiments were performed on a private database\ncomposed of 93 glaucomatous and 156 normal B-scans around the optic nerve head\nof the retina, which were diagnosed by expert ophthalmologists. The validation\nresults evidence that fine-tuned CNNs outperform the networks trained from\nscratch when small databases are addressed. Additionally, the VGG family of\nnetworks reports the most promising results, with an area under the ROC curve\nof 0.96 and an accuracy of 0.92, during the prediction of the independent test\nset.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 18:31:07 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Garc\u00eda", "Gabriel", ""], ["del Amor", "Roc\u00edo", ""], ["Colomer", "Adri\u00e1n", ""], ["Naranjo", "Valery", ""]]}, {"id": "2006.00033", "submitter": "Weimin Zhou", "authors": "Weimin Zhou, Sayantan Bhadra, Frank J. Brooks, Hua Li, Mark A.\n  Anastasio", "title": "Learning stochastic object models from medical imaging measurements\n  using Progressively-Growing AmbientGANs", "comments": "Submitted to IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been advocated that medical imaging systems and reconstruction\nalgorithms should be assessed and optimized by use of objective measures of\nimage quality that quantify the performance of an observer at specific\ndiagnostic tasks. One important source of variability that can significantly\nlimit observer performance is variation in the objects to-be-imaged. This\nsource of variability can be described by stochastic object models (SOMs). A\nSOM is a generative model that can be employed to establish an ensemble of\nto-be-imaged objects with prescribed statistical properties. In order to\naccurately model variations in anatomical structures and object textures, it is\ndesirable to establish SOMs from experimental imaging measurements acquired by\nuse of a well-characterized imaging system. Deep generative neural networks,\nsuch as generative adversarial networks (GANs) hold great potential for this\ntask. However, conventional GANs are typically trained by use of reconstructed\nimages that are influenced by the effects of measurement noise and the\nreconstruction process. To circumvent this, an AmbientGAN has been proposed\nthat augments a GAN with a measurement operator. However, the original\nAmbientGAN could not immediately benefit from modern training procedures, such\nas progressive growing, which limited its ability to be applied to\nrealistically sized medical image data. To circumvent this, in this work, a new\nProgressive Growing AmbientGAN (ProAmGAN) strategy is developed for\nestablishing SOMs from medical imaging measurements. Stylized numerical studies\ncorresponding to common medical imaging modalities are conducted to demonstrate\nand validate the proposed method for establishing SOMs.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 18:45:37 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Zhou", "Weimin", ""], ["Bhadra", "Sayantan", ""], ["Brooks", "Frank J.", ""], ["Li", "Hua", ""], ["Anastasio", "Mark A.", ""]]}, {"id": "2006.00058", "submitter": "Christopher George", "authors": "Christopher A. George, Eduardo A. Barrera, Kenric P. Nelson", "title": "Applying the Decisiveness and Robustness Metrics to Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review three recently-proposed classifier quality metrics and consider\ntheir suitability for large-scale classification challenges such as applying\nconvolutional neural networks to the 1000-class ImageNet dataset. These\nmetrics, referred to as the \"geometric accuracy,\" \"decisiveness,\" and\n\"robustness,\" are based on the generalized mean ($\\rho$ equals 0, 1, and -2/3,\nrespectively) of the classifier's self-reported and measured probabilities of\ncorrect classification. We also propose some minor clarifications to\nstandardize the metric definitions. With these updates, we show some examples\nof calculating the metrics using deep convolutional neural networks (AlexNet\nand DenseNet) acting on large datasets (the German Traffic Sign Recognition\nBenchmark and ImageNet).\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 20:05:06 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["George", "Christopher A.", ""], ["Barrera", "Eduardo A.", ""], ["Nelson", "Kenric P.", ""]]}, {"id": "2006.00059", "submitter": "Mehul Bhatt", "authors": "Vasiliki Kondyli and Mehul Bhatt and Jakob Suchan", "title": "Towards a Human-Centred Cognitive Model of Visuospatial Complexity in\n  Everyday Driving", "comments": "9th European Starting AI Researchers Symposium (STAIRS), at ECAI\n  2020, the 24th European Conference on Artificial Intelligence (ECAI).,\n  Santiago de Compostela, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a human-centred, cognitive model of visuospatial complexity in\neveryday, naturalistic driving conditions. With a focus on visual perception,\nthe model incorporates quantitative, structural, and dynamic attributes\nidentifiable in the chosen context; the human-centred basis of the model lies\nin its behavioural evaluation with human subjects with respect to\npsychophysical measures pertaining to embodied visuoauditory attention. We\nreport preliminary steps to apply the developed cognitive model of visuospatial\ncomplexity for human-factors guided dataset creation and benchmarking, and for\nits use as a semantic template for the (explainable) computational analysis of\nvisuospatial complexity.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 20:12:39 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 07:01:09 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Kondyli", "Vasiliki", ""], ["Bhatt", "Mehul", ""], ["Suchan", "Jakob", ""]]}, {"id": "2006.00063", "submitter": "Nishanth T Arun", "authors": "Nishanth Thumbavanam Arun, Nathan Gaw, Praveer Singh, Ken Chang,\n  Katharina Viktoria Hoebel, Jay Patel, Mishka Gidwani, Jayashree\n  Kalpathy-Cramer", "title": "Assessing the validity of saliency maps for abnormality localization in\n  medical imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/02X3kfP6W4", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency maps have become a widely used method to assess which areas of the\ninput image are most pertinent to the prediction of a trained neural network.\nHowever, in the context of medical imaging, there is no study to our knowledge\nthat has examined the efficacy of these techniques and quantified them using\noverlap with ground truth bounding boxes. In this work, we explored the\ncredibility of the various existing saliency map methods on the RSNA Pneumonia\ndataset. We found that GradCAM was the most sensitive to model parameter and\nlabel randomization, and was highly agnostic to model architecture.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 20:17:26 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Arun", "Nishanth Thumbavanam", ""], ["Gaw", "Nathan", ""], ["Singh", "Praveer", ""], ["Chang", "Ken", ""], ["Hoebel", "Katharina Viktoria", ""], ["Patel", "Jay", ""], ["Gidwani", "Mishka", ""], ["Kalpathy-Cramer", "Jayashree", ""]]}, {"id": "2006.00067", "submitter": "Brian Leahy", "authors": "Brian D. Leahy, Won-Dong Jang, Helen Y. Yang, Robbert Struyven,\n  Donglai Wei, Zhe Sun, Kylie R. Lee, Charlotte Royston, Liz Cam, Yael Kalma,\n  Foad Azem, Dalit Ben-Yosef, Hanspeter Pfister, Daniel Needleman", "title": "Automated Measurements of Key Morphological Features of Human Embryos\n  for IVF", "comments": "to be presented at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in clinical In-Vitro Fertilization (IVF) is selecting the\nhighest quality embryo to transfer to the patient in the hopes of achieving a\npregnancy. Time-lapse microscopy provides clinicians with a wealth of\ninformation for selecting embryos. However, the resulting movies of embryos are\ncurrently analyzed manually, which is time consuming and subjective. Here, we\nautomate feature extraction of time-lapse microscopy of human embryos with a\nmachine-learning pipeline of five convolutional neural networks (CNNs). Our\npipeline consists of (1) semantic segmentation of the regions of the embryo,\n(2) regression predictions of fragment severity, (3) classification of the\ndevelopmental stage, and object instance segmentation of (4) cells and (5)\npronuclei. Our approach greatly speeds up the measurement of quantitative,\nbiologically relevant features that may aid in embryo selection.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 20:27:17 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 21:34:27 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Leahy", "Brian D.", ""], ["Jang", "Won-Dong", ""], ["Yang", "Helen Y.", ""], ["Struyven", "Robbert", ""], ["Wei", "Donglai", ""], ["Sun", "Zhe", ""], ["Lee", "Kylie R.", ""], ["Royston", "Charlotte", ""], ["Cam", "Liz", ""], ["Kalma", "Yael", ""], ["Azem", "Foad", ""], ["Ben-Yosef", "Dalit", ""], ["Pfister", "Hanspeter", ""], ["Needleman", "Daniel", ""]]}, {"id": "2006.00074", "submitter": "Luyao Shi", "authors": "Luyao Shi, Deepta Rajan, Shafiq Abedin, Manikanta Srikar Yellapragada,\n  David Beymer, Ehsan Dehghan", "title": "Automatic Diagnosis of Pulmonary Embolism Using an Attention-guided\n  Framework: A Large-scale Study", "comments": "MIDL 2020 Full Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulmonary Embolism (PE) is a life-threatening disorder associated with high\nmortality and morbidity. Prompt diagnosis and immediate initiation of\ntherapeutic action is important. We explored a deep learning model to detect PE\non volumetric contrast-enhanced chest CT scans using a 2-stage training\nstrategy. First, a residual convolutional neural network (ResNet) was trained\nusing annotated 2D images. In addition to the classification loss, an attention\nloss was added during training to help the network focus attention on PE. Next,\na recurrent network was used to scan sequentially through the features provided\nby the pre-trained ResNet to detect PE. This combination allows the network to\nbe trained using both a limited and sparse set of pixel-level annotated images\nand a large number of easily obtainable patient-level image-label pairs. We\nused 1,670 sparsely annotated studies and more than 10,000 labeled studies in\nour training. On a test set with 2,160 patient studies, the proposed method\nachieved an area under the ROC curve (AUC) of 0.812. The proposed framework is\nalso able to provide localized attention maps that indicate possible PE\nlesions, which could potentially help radiologists accelerate the diagnostic\nprocess.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 20:46:24 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Shi", "Luyao", ""], ["Rajan", "Deepta", ""], ["Abedin", "Shafiq", ""], ["Yellapragada", "Manikanta Srikar", ""], ["Beymer", "David", ""], ["Dehghan", "Ehsan", ""]]}, {"id": "2006.00080", "submitter": "Qi Chang", "authors": "Qi Chang, Hui Qu, Yikai Zhang, Mert Sabuncu, Chao Chen, Tong Zhang and\n  Dimitris Metaxas", "title": "Synthetic Learning: Learn From Distributed Asynchronized Discriminator\n  GAN Without Sharing Medical Image Data", "comments": null, "journal-ref": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020, pp. 13856-13866", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a data privacy-preserving and communication\nefficient distributed GAN learning framework named Distributed Asynchronized\nDiscriminator GAN (AsynDGAN). Our proposed framework aims to train a central\ngenerator learns from distributed discriminator, and use the generated\nsynthetic image solely to train the segmentation model.We validate the proposed\nframework on the application of health entities learning problem which is known\nto be privacy sensitive. Our experiments show that our approach: 1) could learn\nthe real image's distribution from multiple datasets without sharing the\npatient's raw data. 2) is more efficient and requires lower bandwidth than\nother distributed deep learning methods. 3) achieves higher performance\ncompared to the model trained by one real dataset, and almost the same\nperformance compared to the model trained by all real datasets. 4) has provable\nguarantees that the generator could learn the distributed distribution in an\nall important fashion thus is unbiased.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 21:05:49 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 04:18:29 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Chang", "Qi", ""], ["Qu", "Hui", ""], ["Zhang", "Yikai", ""], ["Sabuncu", "Mert", ""], ["Chen", "Chao", ""], ["Zhang", "Tong", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "2006.00083", "submitter": "Bianca Lassen-Schmidt", "authors": "Bianca Lassen-Schmidt, Alessa Hering, Stefan Krass, Hans Meine", "title": "Automatic segmentation of the pulmonary lobes with a 3D u-net and\n  optimized loss function", "comments": "MIDL2020 short paper", "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/AkziGgmwl", "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully-automatic lung lobe segmentation is challenging due to anatomical\nvariations, pathologies, and incomplete fissures. We trained a 3D u-net for\npulmonary lobe segmentation on 49 mainly publically available datasets and\nintroduced a weighted Dice loss function to emphasize the lobar boundaries. To\nvalidate the performance of the proposed method we compared the results to two\nother methods. The new loss function improved the mean distance to 1.46 mm\n(compared to 2.08 mm for simple loss function without weighting).\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 21:18:34 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Lassen-Schmidt", "Bianca", ""], ["Hering", "Alessa", ""], ["Krass", "Stefan", ""], ["Meine", "Hans", ""]]}, {"id": "2006.00086", "submitter": "Eric Wu", "authors": "Eric Wu, Kevin Wu, William Lotter", "title": "Synthesizing lesions using contextual GANs improves breast cancer\n  classification on mammograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data scarcity and class imbalance are two fundamental challenges in many\nmachine learning applications to healthcare. Breast cancer classification in\nmammography exemplifies these challenges, with a malignancy rate of around 0.5%\nin a screening population, which is compounded by the relatively small size of\nlesions (~1% of the image) in malignant cases. Simultaneously, the prevalence\nof screening mammography creates a potential abundance of non-cancer exams to\nuse for training. Altogether, these characteristics lead to overfitting on\ncancer cases, while under-utilizing non-cancer data. Here, we present a novel\ngenerative adversarial network (GAN) model for data augmentation that can\nrealistically synthesize and remove lesions on mammograms. With self-attention\nand semi-supervised learning components, the U-net-based architecture can\ngenerate high resolution (256x256px) outputs, as necessary for mammography.\nWhen augmenting the original training set with the GAN-generated samples, we\nfind a significant improvement in malignancy classification performance on a\ntest set of real mammogram patches. Overall, the empirical results of our\nalgorithm and the relevance to other medical imaging paradigms point to\npotentially fruitful further applications.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 21:23:00 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Wu", "Eric", ""], ["Wu", "Kevin", ""], ["Lotter", "William", ""]]}, {"id": "2006.00090", "submitter": "Marianne Rakic", "authors": "Marianne Rakic, John Guttag and Adrian V. Dalca", "title": "Anatomical Predictions using Subject-Specific Medical Data", "comments": "Accepted as a short paper to MIDL2020. Keywords: Medical Imaging,\n  Multi-Modal, Prediction", "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/apwZYLKTCo", "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changes over time in brain anatomy can provide important insight for\ntreatment design or scientific analyses. We present a method that predicts how\na brain MRI for an individual will change over time. We model changes using a\ndiffeomorphic deformation field that we predict using function using\nconvolutional neural networks. Given a predicted deformation field, a baseline\nscan can be warped to give a prediction of the brain scan at a future time. We\ndemonstrate the method using the ADNI cohort, and analyze how performance is\naffected by model variants and the subject-specific information provided. We\nshow that the model provides good predictions and that external clinical data\ncan improve predictions.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 21:30:46 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Rakic", "Marianne", ""], ["Guttag", "John", ""], ["Dalca", "Adrian V.", ""]]}, {"id": "2006.00100", "submitter": "Sharmishtaa Seshamani", "authors": "Sharmishtaa Seshamani, Leila Elabbady, Casey Schneider-Mizell,\n  Gayathri Mahalingam, Sven Dorkenwald, Agnes Bodor, Thomas Macrina, Daniel\n  Bumbarger, JoAnn Buchanan, Marc Takeno, Wenjing Yin, Derrick Brittain, Russel\n  Torres, Daniel Kapner, Kisuk lee, Ran Lu, Jinpeng Wu, Nuno daCosta, Clay\n  Reid, Forrest Collman", "title": "Automated Neuron Shape Analysis from Electron Microscopy", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphology based analysis of cell types has been an area of great interest to\nthe neuroscience community for several decades. Recently, high resolution\nelectron microscopy (EM) datasets of the mouse brain have opened up\nopportunities for data analysis at a level of detail that was previously\nimpossible. These datasets are very large in nature and thus, manual analysis\nis not a practical solution. Of particular interest are details to the level of\npost synaptic structures. This paper proposes a fully automated framework for\nanalysis of post-synaptic structure based neuron analysis from EM data. The\nprocessing framework involves shape extraction, representation with an\nautoencoder, and whole cell modeling and analysis based on shape distributions.\nWe apply our novel framework on a dataset of 1031 neurons obtained from imaging\na 1mm x 1mm x 40 micrometer volume of the mouse visual cortex and show the\nstrength of our method in clustering and classification of neuronal shapes.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 22:19:00 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Seshamani", "Sharmishtaa", ""], ["Elabbady", "Leila", ""], ["Schneider-Mizell", "Casey", ""], ["Mahalingam", "Gayathri", ""], ["Dorkenwald", "Sven", ""], ["Bodor", "Agnes", ""], ["Macrina", "Thomas", ""], ["Bumbarger", "Daniel", ""], ["Buchanan", "JoAnn", ""], ["Takeno", "Marc", ""], ["Yin", "Wenjing", ""], ["Brittain", "Derrick", ""], ["Torres", "Russel", ""], ["Kapner", "Daniel", ""], ["lee", "Kisuk", ""], ["Lu", "Ran", ""], ["Wu", "Jinpeng", ""], ["daCosta", "Nuno", ""], ["Reid", "Clay", ""], ["Collman", "Forrest", ""]]}, {"id": "2006.00112", "submitter": "Weimin Zhou", "authors": "Weimin Zhou, Hua Li, Mark A. Anastasio", "title": "Approximating the Ideal Observer for joint signal detection and\n  localization tasks by use of supervised learning methods", "comments": "IEEE Transactions on Medical Imaging (Early Access), 2020", "journal-ref": null, "doi": "10.1109/TMI.2020.3009022", "report-no": null, "categories": "eess.SP cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging systems are commonly assessed and optimized by use of\nobjective measures of image quality (IQ). The Ideal Observer (IO) performance\nhas been advocated to provide a figure-of-merit for use in assessing and\noptimizing imaging systems because the IO sets an upper performance limit among\nall observers. When joint signal detection and localization tasks are\nconsidered, the IO that employs a modified generalized likelihood ratio test\nmaximizes observer performance as characterized by the localization receiver\noperating characteristic (LROC) curve. Computations of likelihood ratios are\nanalytically intractable in the majority of cases. Therefore, sampling-based\nmethods that employ Markov-Chain Monte Carlo (MCMC) techniques have been\ndeveloped to approximate the likelihood ratios. However, the applications of\nMCMC methods have been limited to relatively simple object models. Supervised\nlearning-based methods that employ convolutional neural networks have been\nrecently developed to approximate the IO for binary signal detection tasks. In\nthis paper, the ability of supervised learning-based methods to approximate the\nIO for joint signal detection and localization tasks is explored. Both\nbackground-known-exactly and background-known-statistically signal detection\nand localization tasks are considered. The considered object models include a\nlumpy object model and a clustered lumpy model, and the considered measurement\nnoise models include Laplacian noise, Gaussian noise, and mixed\nPoisson-Gaussian noise. The LROC curves produced by the supervised\nlearning-based method are compared to those produced by the MCMC approach or\nanalytical computation when feasible. The potential utility of the proposed\nmethod for computing objective measures of IQ for optimizing imaging system\nperformance is explored.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 22:53:45 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 02:01:08 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Zhou", "Weimin", ""], ["Li", "Hua", ""], ["Anastasio", "Mark A.", ""]]}, {"id": "2006.00115", "submitter": "Daniel Moyer", "authors": "Daniel Moyer, Greg Ver Steeg, Paul M. Thompson", "title": "Overview of Scanner Invariant Representations", "comments": "Accepted as a short paper in MIDL 2020. In accordance with the MIDL\n  2020 Call for Papers, this short paper is an overview of an already published\n  work arXiv:1904.05375, and was submitted to MIDL in order to allow\n  presentation and discussion at the meeting", "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/yqm9RD_XHT", "categories": "q-bio.QM cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooled imaging data from multiple sources is subject to bias from each\nsource. Studies that do not correct for these scanner/site biases at best lose\nstatistical power, and at worst leave spurious correlations in their data.\nEstimation of the bias effects is non-trivial due to the paucity of data with\ncorrespondence across sites, so called \"traveling phantom\" data, which is\nexpensive to collect. Nevertheless, numerous solutions leveraging direct\ncorrespondence have been proposed. In contrast to this, Moyer et al. (2019)\nproposes an unsupervised solution using invariant representations, one which\ndoes not require correspondence and thus does not require paired images. By\nleveraging the data processing inequality, an invariant representation can then\nbe used to create an image reconstruction that is uninformative of its original\nsource, yet still faithful to the underlying structure. In the present abstract\nwe provide an overview of this method.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 22:56:47 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Moyer", "Daniel", ""], ["Steeg", "Greg Ver", ""], ["Thompson", "Paul M.", ""]]}, {"id": "2006.00143", "submitter": "Mengyan Li", "authors": "Jun Yu, Mengyan Li, Xinlong Hao and Guochen Xie", "title": "Deep Fusion Siamese Network for Automatic Kinship Verification", "comments": "8 pages, 8 figures", "journal-ref": "2020 15th IEEE Conference on Automatic Face and Gesture\n  Recognition; 4th Recognizing Families In the Wild (RFIW)", "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic kinship verification aims to determine whether some individuals\nbelong to the same family. It is of great research significance to help missing\npersons reunite with their families. In this work, the challenging problem is\nprogressively addressed in two respects. First, we propose a deep siamese\nnetwork to quantify the relative similarity between two individuals. When given\ntwo input face images, the deep siamese network extracts the features from them\nand fuses these features by combining and concatenating. Then, the fused\nfeatures are fed into a fully-connected network to obtain the similarity score\nbetween two faces, which is used to verify the kinship. To improve the\nperformance, a jury system is also employed for multi-model fusion. Second, two\ndeep siamese networks are integrated into a deep triplet network for\ntri-subject (i.e., father, mother and child) kinship verification, which is\nintended to decide whether a child is related to a pair of parents or not.\nSpecifically, the obtained similarity scores of father-child and mother-child\nare weighted to generate the parent-child similarity score for kinship\nverification. Recognizing Families In the Wild (RFIW) is a challenging kinship\nrecognition task with multiple tracks, which is based on Families in the Wild\n(FIW), a large-scale and comprehensive image database for automatic kinship\nrecognition. The Kinship Verification (track I) and Tri-Subject Verification\n(track II) are supported during the ongoing RFIW2020 Challenge. Our team\n(ustc-nelslip) ranked 1st in track II, and 3rd in track I. The code is\navailable at https://github.com/gniknoil/FG2020-kinship.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 01:43:59 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 12:19:00 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Yu", "Jun", ""], ["Li", "Mengyan", ""], ["Hao", "Xinlong", ""], ["Xie", "Guochen", ""]]}, {"id": "2006.00154", "submitter": "Zhiguang Zhang", "authors": "Zhipeng Luo, Zhiguang Zhang, Zhenyu Xu, Lixuan Che", "title": "Challenge report: Recognizing Families In the Wild Data Challenge", "comments": "RFIW,IEEE FG2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a brief report to our submission to the Recognizing Families In\nthe Wild Data Challenge (4th Edition), in conjunction with FG 2020 Forum.\nAutomatic kinship recognition has attracted many researchers' attention for its\nfull application, but it is still a very challenging task because of the\nlimited information that can be used to determine whether a pair of faces are\nblood relatives or not. In this paper, we studied previous methods and proposed\nour method. We try many methods, like deep metric learning-based, to extract\ndeep embedding feature for every image, then determine if they are blood\nrelatives by Euclidean distance or method based on classes. Finally, we find\nsome tricks like sampling more negative samples and high resolution that can\nhelp get better performance. Moreover, we proposed a symmetric network with a\nbinary classification based method to get our best score in all tasks.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 03:01:56 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Luo", "Zhipeng", ""], ["Zhang", "Zhiguang", ""], ["Xu", "Zhenyu", ""], ["Che", "Lixuan", ""]]}, {"id": "2006.00155", "submitter": "Hantao Yao", "authors": "Hantao Yao, Changsheng Xu", "title": "Joint Person Objectness and Repulsion for Person Search", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.3038347", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person search targets to search the probe person from the unconstrainted\nscene images, which can be treated as the combination of person detection and\nperson matching. However, the existing methods based on the Detection-Matching\nframework ignore the person objectness and repulsion (OR) which are both\nbeneficial to reduce the effect of distractor images. In this paper, we propose\nan OR similarity by jointly considering the objectness and repulsion\ninformation. Besides the traditional visual similarity term, the OR similarity\nalso contains an objectness term and a repulsion term. The objectness term can\nreduce the similarity of distractor images that not contain a person and boost\nthe performance of person search by improving the ranking of positive samples.\nBecause the probe person has a different person ID with its \\emph{neighbors},\nthe gallery images having a higher similarity with the \\emph{neighbors of\nprobe} should have a lower similarity with the probe person. Based on this\nrepulsion constraint, the repulsion term is proposed to reduce the similarity\nof distractor images that are not most similar to the probe person. Treating\nthe Faster R-CNN as the person detector, the OR similarity is evaluated on PRW\nand CUHK-SYSU datasets by the Detection-Matching framework with six description\nmodels. The extensive experiments demonstrate that the proposed OR similarity\ncan effectively reduce the similarity of distractor samples and further boost\nthe performance of person search, e.g., improve the mAP from 92.32% to 93.23%\nfor CUHK-SYSY dataset, and from 50.91% to 52.30% for PRW datasets.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 03:04:33 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Yao", "Hantao", ""], ["Xu", "Changsheng", ""]]}, {"id": "2006.00171", "submitter": "Haimiao Zhang", "authors": "Haimiao Zhang, Baodong Liu, Hengyong Yu, Bin Dong", "title": "MetaInv-Net: Meta Inversion Network for Sparse View CT Image\n  Reconstruction", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG math.OC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray Computed Tomography (CT) is widely used in clinical applications such\nas diagnosis and image-guided interventions. In this paper, we propose a new\ndeep learning based model for CT image reconstruction with the backbone network\narchitecture built by unrolling an iterative algorithm. However, unlike the\nexisting strategy to include as many data-adaptive components in the unrolled\ndynamics model as possible, we find that it is enough to only learn the parts\nwhere traditional designs mostly rely on intuitions and experience. More\nspecifically, we propose to learn an initializer for the conjugate gradient\n(CG) algorithm that involved in one of the subproblems of the backbone model.\nOther components, such as image priors and hyperparameters, are kept as the\noriginal design. Since a hypernetwork is introduced to inference on the\ninitialization of the CG module, it makes the proposed model a certain\nmeta-learning model. Therefore, we shall call the proposed model the\nmeta-inversion network (MetaInv-Net). The proposed MetaInv-Net can be designed\nwith much less trainable parameters while still preserves its superior image\nreconstruction performance than some state-of-the-art deep models in CT\nimaging. In simulated and real data experiments, MetaInv-Net performs very well\nand can be generalized beyond the training setting, i.e., to other scanning\nsettings, noise levels, and data sets.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 04:19:09 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 04:03:49 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 01:17:18 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Zhang", "Haimiao", ""], ["Liu", "Baodong", ""], ["Yu", "Hengyong", ""], ["Dong", "Bin", ""]]}, {"id": "2006.00174", "submitter": "Guochen Xie", "authors": "Jun Yu, Guochen Xie, Mengyan Li and Xinlong Hao", "title": "Retrieval of Family Members Using Siamese Neural Network", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieval of family members in the wild aims at finding family members of the\ngiven subject in the dataset, which is useful in finding the lost children and\nanalyzing the kinship. However, due to the diversity in age, gender, pose and\nillumination of the collected data, this task is always challenging. To solve\nthis problem, we propose our solution with deep Siamese neural network. Our\nsolution can be divided into two parts: similarity computation and ranking. In\ntraining procedure, the Siamese network firstly takes two candidate images as\ninput and produces two feature vectors. And then, the similarity between the\ntwo vectors is computed with several fully connected layers. While in inference\nprocedure, we try another similarity computing method by dropping the followed\nseveral fully connected layers and directly computing the cosine similarity of\nthe two feature vectors. After similarity computation, we use the ranking\nalgorithm to merge the similarity scores with the same identity and output the\nordered list according to their similarities. To gain further improvement, we\ntry different combinations of backbones, training methods and similarity\ncomputing methods. Finally, we submit the best combination as our solution and\nour team(ustc-nelslip) obtains favorable result in the track3 of the RFIW2020\nchallenge with the first runner-up, which verifies the effectiveness of our\nmethod. Our code is available at: https://github.com/gniknoil/FG2020-kinship\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 04:32:16 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Yu", "Jun", ""], ["Xie", "Guochen", ""], ["Li", "Mengyan", ""], ["Hao", "Xinlong", ""]]}, {"id": "2006.00176", "submitter": "Yen-Cheng Liu", "authors": "Yen-Cheng Liu, Junjiao Tian, Nathaniel Glaser, Zsolt Kira", "title": "When2com: Multi-Agent Perception via Communication Graph Grouping", "comments": "Accepted to CVPR 2020; for the project page, see\n  https://ycliu93.github.io/projects/multi-agent-perception.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While significant advances have been made for single-agent perception, many\napplications require multiple sensing agents and cross-agent communication due\nto benefits such as coverage and robustness. It is therefore critical to\ndevelop frameworks which support multi-agent collaborative perception in a\ndistributed and bandwidth-efficient manner. In this paper, we address the\ncollaborative perception problem, where one agent is required to perform a\nperception task and can communicate and share information with other agents on\nthe same task. Specifically, we propose a communication framework by learning\nboth to construct communication groups and decide when to communicate. We\ndemonstrate the generalizability of our framework on two different perception\ntasks and show that it significantly reduces communication bandwidth while\nmaintaining superior performance.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 04:41:32 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 19:32:30 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Liu", "Yen-Cheng", ""], ["Tian", "Junjiao", ""], ["Glaser", "Nathaniel", ""], ["Kira", "Zsolt", ""]]}, {"id": "2006.00186", "submitter": "Samrat Kumar Dey", "authors": "Md. Moshiur Rahman, Samrat Kumar Dey, and Kabid Hassan Shibly", "title": "Advanced Single Image Resolution Upsurging Using a Generative\n  Adversarial Network", "comments": "10 pages, 4 figures, 1 Table", "journal-ref": "Signal & Image Processing: An International Journal (SIPIJ)\n  Vol.11, No.1, February 2020", "doi": "10.5121/sipij.2020.11105", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The resolution of an image is a very important criterion for evaluating the\nquality of the image. A higher resolution of an image is always preferable as\nimages of lower resolution are unsuitable due to fuzzy quality. A higher\nresolution of an image is important for various fields such as medical imaging;\nastronomy works and so on as images of lower resolution becomes unclear and\nindistinct when their sizes are enlarged. In recent times, various research\nworks are performed to generate a higher resolution of an image from its lower\nresolution. In this paper, we have proposed a technique of generating higher\nresolution images form lower resolution using Residual in Residual Dense Block\nnetwork architecture with a deep network. We have also compared our method with\nother methods to prove that our method provides better visual quality images.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 05:40:44 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Rahman", "Md. Moshiur", ""], ["Dey", "Samrat Kumar", ""], ["Shibly", "Kabid Hassan", ""]]}, {"id": "2006.00187", "submitter": "Lipu Zhou", "authors": "Lipu Zhou, Daniel Koppel, Hui Ju, Frank Steinbruecker, Michael Kaess", "title": "An Efficient Planar Bundle Adjustment Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient algorithm for the least-squares problem\nusing the point-to-plane cost, which aims to jointly optimize depth sensor\nposes and plane parameters for 3D reconstruction. We call this least-squares\nproblem \\textbf{Planar Bundle Adjustment} (PBA), due to the similarity between\nthis problem and the original Bundle Adjustment (BA) in visual reconstruction.\nAs planes ubiquitously exist in the man-made environment, they are generally\nused as landmarks in SLAM algorithms for various depth sensors. PBA is\nimportant to reduce drift and improve the quality of the map. However, directly\nadopting the well-established BA framework in visual reconstruction will result\nin a very inefficient solution for PBA. This is because a 3D point only has one\nobservation at a camera pose. In contrast, a depth sensor can record hundreds\nof points in a plane at a time, which results in a very large nonlinear\nleast-squares problem even for a small-scale space. Fortunately, we find that\nthere exist a special structure of the PBA problem. We introduce a reduced\nJacobian matrix and a reduced residual vector, and prove that they can replace\nthe original Jacobian matrix and residual vector in the generally adopted\nLevenberg-Marquardt (LM) algorithm. This significantly reduces the\ncomputational cost. Besides, when planes are combined with other features for\n3D reconstruction, the reduced Jacobian matrix and residual vector can also\nreplace the corresponding parts derived from planes. Our experimental results\nverify that our algorithm can significantly reduce the computational time\ncompared to the solution using the traditional BA framework. Besides, our\nalgorithm is faster, more accuracy, and more robust to initialization errors\ncompared to the start-of-the-art solution using the plane-to-plane cost\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 05:54:22 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 07:52:42 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Zhou", "Lipu", ""], ["Koppel", "Daniel", ""], ["Ju", "Hui", ""], ["Steinbruecker", "Frank", ""], ["Kaess", "Michael", ""]]}, {"id": "2006.00190", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Rishabh Baghel, Ravi Kiran Sarvadevabhatla", "title": "OPAL-Net: A Generative Model for Part-based Object Layout Generation", "comments": "Code repository at https://github.com/atmacvit/opalnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose OPAL-Net, a novel hierarchical architecture for part-based layout\ngeneration of objects from multiple categories using a single unified model. We\nadopt a coarse-to-fine strategy involving semantically conditioned\nautoregressive generation of bounding box layouts and pixel-level part layouts\nfor objects. We use Graph Convolutional Networks, Deep Recurrent Networks along\nwith custom-designed Conditional Variational Autoencoders to enable flexible,\ndiverse and category-aware generation of object layouts. We train OPAL-Net on\nPASCAL-Parts dataset. The generated samples and corresponding evaluation scores\ndemonstrate the versatility of OPAL-Net compared to ablative variants and\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 06:25:19 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Baghel", "Rishabh", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "2006.00197", "submitter": "Muhammad Bilal", "authors": "J.D. Bodapati, N. Veeranjaneyulu, S.N. Shareef, S. Hakak, M. Bilal,\n  P.K.R. Maddikunta, O. Jo", "title": "Blended Multi-Modal Deep ConvNet Features for Diabetic Retinopathy\n  Severity Prediction", "comments": "18 pages, 8 figures, published in Electronics MDPI journal", "journal-ref": "Electronics 2020, 9, 914", "doi": "10.3390/electronics9060914", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic Retinopathy (DR) is one of the major causes of visual impairment and\nblindness across the world. It is usually found in patients who suffer from\ndiabetes for a long period. The major focus of this work is to derive optimal\nrepresentation of retinal images that further helps to improve the performance\nof DR recognition models. To extract optimal representation, features extracted\nfrom multiple pre-trained ConvNet models are blended using proposed multi-modal\nfusion module. These final representations are used to train a Deep Neural\nNetwork (DNN) used for DR identification and severity level prediction. As each\nConvNet extracts different features, fusing them using 1D pooling and cross\npooling leads to better representation than using features extracted from a\nsingle ConvNet. Experimental studies on benchmark Kaggle APTOS 2019 contest\ndataset reveals that the model trained on proposed blended feature\nrepresentations is superior to the existing methods. In addition, we notice\nthat cross average pooling based fusion of features from Xception and VGG16 is\nthe most appropriate for DR recognition. With the proposed model, we achieve an\naccuracy of 97.41%, and a kappa statistic of 94.82 for DR identification and an\naccuracy of 81.7% and a kappa statistic of 71.1% for severity level prediction.\nAnother interesting observation is that DNN with dropout at input layer\nconverges more quickly when trained using blended features, compared to the\nsame model trained using uni-modal deep features.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 06:46:26 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Bodapati", "J. D.", ""], ["Veeranjaneyulu", "N.", ""], ["Shareef", "S. N.", ""], ["Hakak", "S.", ""], ["Bilal", "M.", ""], ["Maddikunta", "P. K. R.", ""], ["Jo", "O.", ""]]}, {"id": "2006.00202", "submitter": "Chen Chao", "authors": "Chao Chen, Zhihong Chen, Xinyu Jin, Lanjuan Li, William Speier, Corey\n  W. Arnold", "title": "Attention-Guided Discriminative Region Localization and Label\n  Distribution Learning for Bone Age Assessment", "comments": "codes are available at\n  https://github.com/chenchao666/Bone-Age-Assessment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bone age assessment (BAA) is clinically important as it can be used to\ndiagnose endocrine and metabolic disorders during child development. Existing\ndeep learning based methods for classifying bone age use the global image as\ninput, or exploit local information by annotating extra bounding boxes or key\npoints. However, training with the global image underutilizes discriminative\nlocal information, while providing extra annotations is expensive and\nsubjective. In this paper, we propose an attention-guided approach to\nautomatically localize the discriminative regions for BAA without any extra\nannotations. Specifically, we first train a classification model to learn the\nattention maps of the discriminative regions, finding the hand region, the most\ndiscriminative region (the carpal bones), and the next most discriminative\nregion (the metacarpal bones). Guided by those attention maps, we then crop the\ninformative local regions from the original image and aggregate different\nregions for BAA. Instead of taking BAA as a general regression task, which is\nsuboptimal due to the label ambiguity problem in the age label space, we\npropose using joint age distribution learning and expectation regression, which\nmakes use of the ordinal relationship among hand images with different\nindividual ages and leads to more robust age estimation. Extensive experiments\nare conducted on the RSNA pediatric bone age data set. Using no training\nannotations, our method achieves competitive results compared with existing\nstate-of-the-art semi-automatic deep learning-based methods that require manual\nannotation. Code is available at https:\n//github.com/chenchao666/Bone-Age-Assessment.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 07:04:49 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 04:58:22 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Chen", "Chao", ""], ["Chen", "Zhihong", ""], ["Jin", "Xinyu", ""], ["Li", "Lanjuan", ""], ["Speier", "William", ""], ["Arnold", "Corey W.", ""]]}, {"id": "2006.00212", "submitter": "Bo Pang", "authors": "Bo Pang, Kaiwen Zha, Hanwen Cao, Jiajun Tang, Minghui Yu, Cewu Lu", "title": "Complex Sequential Understanding through the Awareness of Spatial and\n  Temporal Concepts", "comments": "15 pages, 5 figures, 8 tables", "journal-ref": "Nat Mach Intell 2, 24-253 (2020)", "doi": "10.1038/s42256-020-0168-3", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding sequential information is a fundamental task for artificial\nintelligence. Current neural networks attempt to learn spatial and temporal\ninformation as a whole, limited their abilities to represent large scale\nspatial representations over long-range sequences. Here, we introduce a new\nmodeling strategy called Semi-Coupled Structure (SCS), which consists of deep\nneural networks that decouple the complex spatial and temporal concepts\nlearning. Semi-Coupled Structure can learn to implicitly separate input\ninformation into independent parts and process these parts respectively.\nExperiments demonstrate that a Semi-Coupled Structure can successfully annotate\nthe outline of an object in images sequentially and perform video action\nrecognition. For sequence-to-sequence problems, a Semi-Coupled Structure can\npredict future meteorological radar echo images based on observed images. Taken\ntogether, our results demonstrate that a Semi-Coupled Structure has the\ncapacity to improve the performance of LSTM-like models on large scale\nsequential tasks.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 07:51:50 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Pang", "Bo", ""], ["Zha", "Kaiwen", ""], ["Cao", "Hanwen", ""], ["Tang", "Jiajun", ""], ["Yu", "Minghui", ""], ["Lu", "Cewu", ""]]}, {"id": "2006.00223", "submitter": "Shanshan Wang", "authors": "Shanshan Wang, Lei Zhang", "title": "Self-adaptive Re-weighted Adversarial Domain Adaptation", "comments": "to appear in IJCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing adversarial domain adaptation methods mainly consider the marginal\ndistribution and these methods may lead to either under transfer or negative\ntransfer. To address this problem, we present a self-adaptive re-weighted\nadversarial domain adaptation approach, which tries to enhance domain alignment\nfrom the perspective of conditional distribution. In order to promote positive\ntransfer and combat negative transfer, we reduce the weight of the adversarial\nloss for aligned features while increasing the adversarial force for those\npoorly aligned measured by the conditional entropy. Additionally, triplet loss\nleveraging source samples and pseudo-labeled target samples is employed on the\nconfusing domain. Such metric loss ensures the distance of the intra-class\nsample pairs closer than the inter-class pairs to achieve the class-level\nalignment. In this way, the high accurate pseudolabeled target samples and\nsemantic alignment can be captured simultaneously in the co-training process.\nOur method achieved low joint error of the ideal source and target hypothesis.\nThe expected target error can then be upper bounded following Ben-David's\ntheorem. Empirical evidence demonstrates that the proposed model outperforms\nstate of the arts on standard domain adaptation datasets.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 08:35:18 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 03:08:41 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Wang", "Shanshan", ""], ["Zhang", "Lei", ""]]}, {"id": "2006.00226", "submitter": "Fahri Aydos Mr.", "authors": "Fahri Aydos, A. Murat \\\"Ozbayo\\u{g}lu, Yahya \\c{S}irin, M. Fatih\n  Demirci", "title": "Web page classification with Google Image Search results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel method that combines multiple neural\nnetwork results to decide the class of the input. This is the first study which\nused the method for web pages classification. In our model, each element is\nrepresented by multiple descriptive images. After the training process of the\nneural network model, each element is classified by calculating its descriptive\nimage results. We apply our idea to the web page classification problem using\nGoogle Image Search results as descriptive images. We obtained a classification\nrate of 94.90% on the WebScreenshots dataset that contains 20000 web sites in 4\nclasses. The method is easily applicable to similar problems.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 09:16:20 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 06:23:37 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Aydos", "Fahri", ""], ["\u00d6zbayo\u011flu", "A. Murat", ""], ["\u015eirin", "Yahya", ""], ["Demirci", "M. Fatih", ""]]}, {"id": "2006.00234", "submitter": "MinChao Yan", "authors": "Fan Zhang, MinChao Yan, Chen Hu, Jun Ni, Fei Ma", "title": "Integrating global spatial features in CNN based Hyperspectral/SAR\n  imagery classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The land cover classification has played an important role in remote sensing\nbecause it can intelligently identify things in one huge remote sensing image\nto reduce the work of humans. However, a lot of classification methods are\ndesigned based on the pixel feature or limited spatial feature of the remote\nsensing image, which limits the classification accuracy and universality of\ntheir methods. This paper proposed a novel method to take into the information\nof remote sensing image, i.e., geographic latitude-longitude information. In\naddition, a dual-branch convolutional neural network (CNN) classification\nmethod is designed in combination with the global information to mine the pixel\nfeatures of the image. Then, the features of the two neural networks are fused\nwith another fully neural network to realize the classification of remote\nsensing images. Finally, two remote sensing images are used to verify the\neffectiveness of our method, including hyperspectral imaging (HSI) and\npolarimetric synthetic aperture radar (PolSAR) imagery. The result of the\nproposed method is superior to the traditional single-channel convolutional\nneural network.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 10:00:10 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 09:00:59 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhang", "Fan", ""], ["Yan", "MinChao", ""], ["Hu", "Chen", ""], ["Ni", "Jun", ""], ["Ma", "Fei", ""]]}, {"id": "2006.00235", "submitter": "Xiaozhen Xie", "authors": "Haijin Zeng, Xiaozhen Xie, Jifeng Ning", "title": "Hyperspectral Image Denoising via Global Spatial-Spectral Total\n  Variation Regularized Nonconvex Local Low-Rank Tensor Approximation", "comments": null, "journal-ref": "Signal Processing Volume 178, January 2021, 107805", "doi": "10.1109/TGRS.2020.3007945", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) denoising aims to restore clean HSI from the\nnoise-contaminated one. Noise contamination can often be caused during data\nacquisition and conversion. In this paper, we propose a novel spatial-spectral\ntotal variation (SSTV) regularized nonconvex local low-rank (LR) tensor\napproximation method to remove mixed noise in HSIs. From one aspect, the clean\nHSI data have its underlying local LR tensor property, even though the real HSI\ndata may not be globally low-rank due to out-liers and non-Gaussian noise.\nAccording to this fact, we propose a novel tensor $L_{\\gamma}$-norm to\nformulate the local LR prior. From another aspect, HSIs are assumed to be\npiecewisely smooth in the global spatial and spectral domains. Instead of\ntraditional bandwise total variation, we use the SSTV regularization to\nsimultaneously consider global spatial structure and spectral correlation of\nneighboring bands. Results on simulated and real HSI datasets indicate that the\nuse of local LR tensor penalty and global SSTV can boost the preserving of\nlocal details and overall structural information in HSIs.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 10:03:39 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zeng", "Haijin", ""], ["Xie", "Xiaozhen", ""], ["Ning", "Jifeng", ""]]}, {"id": "2006.00251", "submitter": "Anthony DiSpirito", "authors": "Anthony DiSpirito III, Daiwei Li, Tri Vu, Maomao Chen, Dong Zhang,\n  Jianwen Luo, Roarke Horstmeyer, and Junjie Yao", "title": "Reconstructing undersampled photoacoustic microscopy images using deep\n  learning", "comments": "12 pages, 7 main figures, 3 supplemental figures (see last 2 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One primary technical challenge in photoacoustic microscopy (PAM) is the\nnecessary compromise between spatial resolution and imaging speed. In this\nstudy, we propose a novel application of deep learning principles to\nreconstruct undersampled PAM images and transcend the trade-off between spatial\nresolution and imaging speed. We compared various convolutional neural network\n(CNN) architectures, and selected a fully dense U-net (FD U-net) model that\nproduced the best results. To mimic various undersampling conditions in\npractice, we artificially downsampled fully-sampled PAM images of mouse brain\nvasculature at different ratios. This allowed us to not only definitively\nestablish the ground truth, but also train and test our deep learning model at\nvarious imaging conditions. Our results and numerical analysis have\ncollectively demonstrated the robust performance of our model to reconstruct\nPAM images with as few as 2% of the original pixels, which may effectively\nshorten the imaging time without substantially sacrificing the image quality.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 12:39:52 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["DiSpirito", "Anthony", "III"], ["Li", "Daiwei", ""], ["Vu", "Tri", ""], ["Chen", "Maomao", ""], ["Zhang", "Dong", ""], ["Luo", "Jianwen", ""], ["Horstmeyer", "Roarke", ""], ["Yao", "Junjie", ""]]}, {"id": "2006.00269", "submitter": "Jia Li", "authors": "Jiawei Zhao, Yifan Zhao, Jia Li, Xiaowu Chen", "title": "Is Depth Really Necessary for Salient Object Detection?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection (SOD) is a crucial and preliminary task for many\ncomputer vision applications, which have made progress with deep CNNs. Most of\nthe existing methods mainly rely on the RGB information to distinguish the\nsalient objects, which faces difficulties in some complex scenarios. To solve\nthis, many recent RGBD-based networks are proposed by adopting the depth map as\nan independent input and fuse the features with RGB information. Taking the\nadvantages of RGB and RGBD methods, we propose a novel depth-aware salient\nobject detection framework, which has following superior designs: 1) It only\ntakes the depth information as training data while only relies on RGB\ninformation in the testing phase. 2) It comprehensively optimizes SOD features\nwith multi-level depth-aware regularizations. 3) The depth information also\nserves as error-weighted map to correct the segmentation process. With these\ninsightful designs combined, we make the first attempt in realizing an unified\ndepth-aware framework with only RGB information as input for inference, which\nnot only surpasses the state-of-the-art performances on five public RGB SOD\nbenchmarks, but also surpasses the RGBD-based methods on five benchmarks by a\nlarge margin, while adopting less information and implementation\nlight-weighted. The code and model will be publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 13:40:03 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 01:07:49 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Zhao", "Jiawei", ""], ["Zhao", "Yifan", ""], ["Li", "Jia", ""], ["Chen", "Xiaowu", ""]]}, {"id": "2006.00273", "submitter": "Mahbubunnabi Tamal", "authors": "Mahbubunnabi Tamal", "title": "Positron Emission Tomography (PET) image enhancement using a gradient\n  vector orientation based nonlinear diffusion filter (GVOF) for accurate\n  quantitation of radioactivity concentration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To accurately quantify in vivo radiotracer uptake using Positron Emission\nTomography (PET) is a challenging task due to low signal-to-noise ratio (SNR)\nand poor spatial resolution of PET camera along with the finite image sampling\nconstraint. Furthermore, inter lesion variations of the SNR and contrast along\nwith the variations in size of the lesion make the quantitation even more\ndifficult. One of the ways to improve the quantitation is via post\nreconstruction filtering with Gaussian Filter (GF). Edge preserving Bilateral\nFilter (BF) and Nonlinear Diffusion Filter (NDF) are the alternatives to GF\nthat can improve the SNR without degrading the image resolution. However, the\nperformance of these edge preserving methods are only optimum for high count\nand low noise cases. A novel parameter free gradient vector orientation based\nnonlinear diffusion filter (GVOF) is proposed in this paper that is insensitive\nto statistical fluctuations (e. g., SNR, contrast, size etc.). GVOF method\napplied on the PET images collected with the NEMA phantom with varying levels\nof contrast and noise reveals that the GVOF method provides the highest SNR,\nCNR (contrast-to-noise ratio) and resolution compared to the original and other\nfiltered images. The percentage bias in estimating the maximum activity\nrepresenting SUVmax (Maximum Standardized Uptake Value) for the spheres with\ndiameter > 2cm where the partial volume effects (PVE) is negligible is the\nlowest for the GVOF method. The GVOF method also improves the maximum intensity\nreproducibility. Robustness of the GVOF against variation in sizes, contrast\nlevels and SNR makes it a suitable post filtering method for both accurate\ndiagnosis and response assessment. Furthermore, its capability to provide\naccurate quantitative measurements irrespective of the SNR, it can also be\neffective in reduction of radioactivity dose.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 13:57:02 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Tamal", "Mahbubunnabi", ""]]}, {"id": "2006.00303", "submitter": "Jianqiang Wan", "authors": "Jianqiang Wan, Yang Liu, Donglai Wei, Xiang Bai, Yongchao Xu", "title": "Super-BPD: Super Boundary-to-Pixel Direction for Fast Image Segmentation", "comments": "Accepted to CVPR 2020. 10 pages, 9 figures. Code available at https:\n  //github.com/JianqiangWan/Super-BPD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is a fundamental vision task and a crucial step for many\napplications. In this paper, we propose a fast image segmentation method based\non a novel super boundary-to-pixel direction (super-BPD) and a customized\nsegmentation algorithm with super-BPD. Precisely, we define BPD on each pixel\nas a two-dimensional unit vector pointing from its nearest boundary to the\npixel. In the BPD, nearby pixels from different regions have opposite\ndirections departing from each other, and adjacent pixels in the same region\nhave directions pointing to the other or each other (i.e., around medial\npoints). We make use of such property to partition an image into super-BPDs,\nwhich are novel informative superpixels with robust direction similarity for\nfast grouping into segmentation regions. Extensive experimental results on\nBSDS500 and Pascal Context demonstrate the accuracy and efficency of the\nproposed super-BPD in segmenting images. In practice, the proposed super-BPD\nachieves comparable or superior performance with MCG while running at ~25fps\nvs. 0.07fps. Super-BPD also exhibits a noteworthy transferability to unseen\nscenes. The code is publicly available at\nhttps://github.com/JianqiangWan/Super-BPD.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 16:00:54 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Wan", "Jianqiang", ""], ["Liu", "Yang", ""], ["Wei", "Donglai", ""], ["Bai", "Xiang", ""], ["Xu", "Yongchao", ""]]}, {"id": "2006.00304", "submitter": "Shiv Gehlot", "authors": "Shiv Gehlot and Anubha Gupta and Ritu Gupta", "title": "SDCT-AuxNet$^{\\theta}$: DCT Augmented Stain Deconvolutional CNN with\n  Auxiliary Classifier for Cancer Diagnosis", "comments": "The final version of this preprint has been published in Medical\n  Image Analysis", "journal-ref": "Medical Image Analysis, 61, 101661, 2020", "doi": "10.1016/j.media.2020.101661", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acute lymphoblastic leukemia (ALL) is a pervasive pediatric white blood cell\ncancer across the globe. With the popularity of convolutional neural networks\n(CNNs), computer-aided diagnosis of cancer has attracted considerable\nattention. Such tools are easily deployable and are cost-effective. Hence,\nthese can enable extensive coverage of cancer diagnostic facilities. However,\nthe development of such a tool for ALL cancer was challenging so far due to the\nnon-availability of a large training dataset. The visual similarity between the\nmalignant and normal cells adds to the complexity of the problem. This paper\ndiscusses the recent release of a large dataset and presents a novel deep\nlearning architecture for the classification of cell images of ALL cancer. The\nproposed architecture, namely, SDCT-AuxNet$^{\\theta}$ is a 2-module framework\nthat utilizes a compact CNN as the main classifier in one module and a Kernel\nSVM as the auxiliary classifier in the other one. While CNN classifier uses\nfeatures through bilinear-pooling, spectral-averaged features are used by the\nauxiliary classifier. Further, this CNN is trained on the stain deconvolved\nquantity images in the optical density domain instead of the conventional RGB\nimages. A novel test strategy is proposed that exploits both the classifiers\nfor decision making using the confidence scores of their predicted class\nlabels. Elaborate experiments have been carried out on our recently released\npublic dataset of 15114 images of ALL cancer and healthy cells to establish the\nvalidity of the proposed methodology that is also robust to subject-level\nvariability. A weighted F1 score of 94.8$\\%$ is obtained that is best so far on\nthis challenging dataset.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 16:01:31 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 01:47:54 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Gehlot", "Shiv", ""], ["Gupta", "Anubha", ""], ["Gupta", "Ritu", ""]]}, {"id": "2006.00327", "submitter": "Ti Bai", "authors": "Ti Bai, Dan Nguyen, Biling Wang and Steve Jiang", "title": "Probabilistic self-learning framework for Low-dose CT Denoising", "comments": null, "journal-ref": null, "doi": "10.1002/mp.14796", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the indispensable role of X-ray computed tomography (CT) in\ndiagnostic medicine field, the associated ionizing radiation is still a major\nconcern considering that it may cause genetic and cancerous diseases.\nDecreasing the exposure can reduce the dose and hence the radiation-related\nrisk, but will also induce higher quantum noise. Supervised deep learning can\nbe used to train a neural network to denoise the low-dose CT (LDCT). However,\nits success requires massive pixel-wise paired LDCT and normal-dose CT (NDCT)\nimages, which are rarely available in real practice. To alleviate this problem,\nin this paper, a shift-invariant property based neural network was devised to\nlearn the inherent pixel correlations and also the noise distribution by only\nusing the LDCT images, shaping into our probabilistic self-learning framework.\nExperimental results demonstrated that the proposed method outperformed the\ncompetitors, producing an enhanced LDCT image that has similar image style as\nthe routine NDCT which is highly-preferable in clinic practice.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 17:47:10 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 04:41:30 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Bai", "Ti", ""], ["Nguyen", "Dan", ""], ["Wang", "Biling", ""], ["Jiang", "Steve", ""]]}, {"id": "2006.00345", "submitter": "Eftychios Protopapadakis", "authors": "Eftychios Protopapadakis, Anastasios Doulamis, Nikolaos Doulamis and\n  Evangelos Maltezos", "title": "Semi-Supervised Fine-Tuning for Deep Learning Models in Remote Sensing\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A combinatory approach of two well-known fields: deep learning and semi\nsupervised learning is presented, to tackle the land cover identification\nproblem. The proposed methodology demonstrates the impact on the performance of\ndeep learning models, when SSL approaches are used as performance functions\nduring training. Obtained results, at pixel level segmentation tasks over\northoimages, suggest that SSL enhanced loss functions can be beneficial in\nmodels' performance.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 19:54:32 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Protopapadakis", "Eftychios", ""], ["Doulamis", "Anastasios", ""], ["Doulamis", "Nikolaos", ""], ["Maltezos", "Evangelos", ""]]}, {"id": "2006.00356", "submitter": "Davood Karimi", "authors": "Davood Karimi, Simon K. Warfield, Ali Gholipour", "title": "Critical Assessment of Transfer Learning for Medical Image Segmentation\n  with Fully Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning is widely used for training machine learning models. Here,\nwe study the role of transfer learning for training fully convolutional\nnetworks (FCNs) for medical image segmentation. Our experiments show that\nalthough transfer learning reduces the training time on the target task, the\nimprovement in segmentation accuracy is highly task/data-dependent. Larger\nimprovements in accuracy are observed when the segmentation task is more\nchallenging and the target training data is smaller. We observe that\nconvolutional filters of an FCN change little during training for medical image\nsegmentation, and still look random at convergence. We further show that quite\naccurate FCNs can be built by freezing the encoder section of the network at\nrandom values and only training the decoder section. At least for medical image\nsegmentation, this finding challenges the common belief that the encoder\nsection needs to learn data/task-specific representations. We examine the\nevolution of FCN representations to gain a better insight into the effects of\ntransfer learning on the training dynamics. Our analysis shows that although\nFCNs trained via transfer learning learn different representations than FCNs\ntrained with random initialization, the variability among FCNs trained via\ntransfer learning can be as high as that among FCNs trained with random\ninitialization. Moreover, feature reuse is not restricted to the early encoder\nlayers; rather, it can be more significant in deeper layers. These findings\noffer new insights and suggest alternative ways of training FCNs for medical\nimage segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 20:36:05 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Karimi", "Davood", ""], ["Warfield", "Simon K.", ""], ["Gholipour", "Ali", ""]]}, {"id": "2006.00367", "submitter": "Olasimbo Ayodeji Arigbabu", "authors": "Olasimbo Ayodeji Arigbabu", "title": "Entropy Decision Fusion for Smartphone Sensor based Human Activity\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human activity recognition serves an important part in building continuous\nbehavioral monitoring systems, which are deployable for visual surveillance,\npatient rehabilitation, gaming, and even personally inclined smart homes. This\npaper demonstrates our efforts to develop a collaborative decision fusion\nmechanism for integrating the predicted scores from multiple learning\nalgorithms trained on smartphone sensor based human activity data. We present\nan approach for fusing convolutional neural network, recurrent convolutional\nnetwork, and support vector machine by computing and fusing the relative\nweighted scores from each classifier based on Tsallis entropy to improve human\nactivity recognition performance. To assess the suitability of this approach,\nexperiments are conducted on two benchmark datasets, UCI-HAR and WISDM. The\nrecognition results attained using the proposed approach are comparable to\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 21:09:38 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Arigbabu", "Olasimbo Ayodeji", ""]]}, {"id": "2006.00412", "submitter": "Hantao Yao", "authors": "Hantao Yao, Shaobo Min, Yongdong Zhang, Changsheng Xu", "title": "Attribute-Induced Bias Eliminating for Transductive Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transductive Zero-shot learning (ZSL) targets to recognize the unseen\ncategories by aligning the visual and semantic information in a joint embedding\nspace. There exist four kinds of domain biases in Transductive ZSL, i.e.,\nvisual bias and semantic bias between two domains and two visual-semantic\nbiases in respective seen and unseen domains, but existing work only focuses on\nthe part of them, which leads to severe semantic ambiguity during the knowledge\ntransfer. To solve the above problem, we propose a novel Attribute-Induced Bias\nEliminating (AIBE) module for Transductive ZSL. Specifically, for the visual\nbias between two domains, the Mean-Teacher module is first leveraged to bridge\nthe visual representation discrepancy between two domains with unsupervised\nlearning and unlabelled images. Then, an attentional graph attribute embedding\nis proposed to reduce the semantic bias between seen and unseen categories,\nwhich utilizes the graph operation to capture the semantic relationship between\ncategories. Besides, to reduce the semantic-visual bias in the seen domain, we\nalign the visual center of each category, instead of the individual visual data\npoint, with the corresponding semantic attributes, which further preserves the\nsemantic relationship in the embedding space. Finally, for the semantic-visual\nbias in the unseen domain, an unseen semantic alignment constraint is designed\nto align visual and semantic space in an unsupervised manner. The evaluations\non several benchmarks demonstrate the effectiveness of the proposed method,\ne.g., obtaining the 82.8%/75.5%, 97.1%/82.5%, and 73.2%/52.1% for\nConventional/Generalized ZSL settings for CUB, AwA2, and SUN datasets,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 02:08:01 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Yao", "Hantao", ""], ["Min", "Shaobo", ""], ["Zhang", "Yongdong", ""], ["Xu", "Changsheng", ""]]}, {"id": "2006.00414", "submitter": "Ange Lou", "authors": "Ange Lou, Shuyue Guan, Murray Loew", "title": "DC-UNet: Rethinking the U-Net Architecture with Dual Channel Efficient\n  CNN for Medical Images Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning has become much more popular in computer vision area.\nThe Convolution Neural Network (CNN) has brought a breakthrough in images\nsegmentation areas, especially, for medical images. In this regard, U-Net is\nthe predominant approach to medical image segmentation task. The U-Net not only\nperforms well in segmenting multimodal medical images generally, but also in\nsome tough cases of them. However, we found that the classical U-Net\narchitecture has limitation in several aspects. Therefore, we applied\nmodifications: 1) designed efficient CNN architecture to replace encoder and\ndecoder, 2) applied residual module to replace skip connection between encoder\nand decoder to improve based on the-state-of-the-art U-Net model. Following\nthese modifications, we designed a novel architecture--DC-UNet, as a potential\nsuccessor to the U-Net architecture. We created a new effective CNN\narchitecture and build the DC-UNet based on this CNN. We have evaluated our\nmodel on three datasets with tough cases and have obtained a relative\nimprovement in performance of 2.90%, 1.49% and 11.42% respectively compared\nwith classical U-Net. In addition, we used the Tanimoto similarity to replace\nthe Jaccard similarity for gray-to-gray image comparisons.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 02:23:55 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Lou", "Ange", ""], ["Guan", "Shuyue", ""], ["Loew", "Murray", ""]]}, {"id": "2006.00422", "submitter": "Deepak Singla", "authors": "Deepak Singla, Vivek Mohan, Tarun Pulluri, Andres Ussa, Pradeep Kumar\n  Gopalakrishnan, Bharath Ramesh and Arindam Basu", "title": "EBBINNOT: A Hardware Efficient Hybrid Event-Frame Tracker for Stationary\n  Neuromorphic Vision Sensors", "comments": "15 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic vision sensors (NVS) have been recently explored to tackle\nscenarios where conventional sensors result in high data rate and processing\ntime. This paper presents a hybrid event-frame approach for detecting and\ntracking objects recorded by a stationary neuromorphic sensor, thereby\nexploiting the sparse NVS output in a low-power setting for traffic monitoring.\nSpecifically, we propose a hardware efficient processing pipeline that\noptimizes memory and computational needs. The usage of NVS gives the advantage\nof rejecting background while it has a unique disadvantage of fragmented\nobjects. To exploit the background removal, we propose an event-based binary\nimage creation that signals presence or absence of events in a frame duration.\nThis reduces memory requirement and enables usage of simple algorithms like\nmedian filtering and connected component labeling for denoise and region\nproposal respectively. To overcome the fragmentation issue, a YOLO-inspired\nneural network based detector and classifier to merge fragmented region\nproposals has been proposed. Finally, an overlap based tracker exploiting\noverlap between detections and tracks is proposed with heuristics to overcome\nocclusion. The proposed pipeline is evaluated with more than 5 hours of traffic\nrecording spanning three different locations on two different NVS and\ndemonstrate similar performance. Compared to existing event-based feature\ntrackers, our method provides similar accuracy while needing 6 times less\ncomputes. To the best of our knowledge, this is the first time a stationary NVS\nbased traffic monitoring solution is extensively compared to simultaneously\nrecorded RGB frame-methods while showing tremendous promise by outperforming\nstate-of-the-art deep learning solutions.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 03:01:35 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 06:52:08 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Singla", "Deepak", ""], ["Mohan", "Vivek", ""], ["Pulluri", "Tarun", ""], ["Ussa", "Andres", ""], ["Gopalakrishnan", "Pradeep Kumar", ""], ["Ramesh", "Bharath", ""], ["Basu", "Arindam", ""]]}, {"id": "2006.00429", "submitter": "Song Bo Yang", "authors": "Song-Bo Yang, Tian-li Yu", "title": "Pseudo-Representation Labeling Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, semi-supervised learning (SSL) has shown tremendous success\nin leveraging unlabeled data to improve the performance of deep learning\nmodels, which significantly reduces the demand for large amounts of labeled\ndata. Many SSL techniques have been proposed and have shown promising\nperformance on famous datasets such as ImageNet and CIFAR-10. However, some\nexiting techniques (especially data augmentation based) are not suitable for\nindustrial applications empirically. Therefore, this work proposes the\npseudo-representation labeling, a simple and flexible framework that utilizes\npseudo-labeling techniques to iteratively label a small amount of unlabeled\ndata and use them as training data. In addition, our framework is integrated\nwith self-supervised representation learning such that the classifier gains\nbenefits from representation learning of both labeled and unlabeled data. This\nframework can be implemented without being limited at the specific model\nstructure, but a general technique to improve the existing model. Compared with\nthe existing approaches, the pseudo-representation labeling is more intuitive\nand can effectively solve practical problems in the real world. Empirically, it\noutperforms the current state-of-the-art semi-supervised learning methods in\nindustrial types of classification problems such as the WM-811K wafer map and\nthe MIT-BIH Arrhythmia dataset.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 03:55:41 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Yang", "Song-Bo", ""], ["Yu", "Tian-li", ""]]}, {"id": "2006.00439", "submitter": "Feifan Lv", "authors": "Feifan Lv, Bo Liu, Feng Lu", "title": "Fast Enhancement for Non-Uniform Illumination Images using Light-weight\n  CNNs", "comments": "9 pages, 12 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new light-weight convolutional neural network (5k\nparameters) for non-uniform illumination image enhancement to handle color,\nexposure, contrast, noise and artifacts, etc., simultaneously and effectively.\nMore concretely, the input image is first enhanced using Retinex model from\ndual different aspects (enhancing under-exposure and suppressing\nover-exposure), respectively. Then, these two enhanced results and the original\nimage are fused to obtain an image with satisfactory brightness, contrast and\ndetails. Finally, the extra noise and compression artifacts are removed to get\nthe final result. To train this network, we propose a semi-supervised\nretouching solution and construct a new dataset (82k images) contains various\nscenes and light conditions. Our model can enhance 0.5 mega-pixel (like\n600*800) images in real time (50 fps), which is faster than existing\nenhancement methods. Extensive experiments show that our solution is fast and\neffective to deal with non-uniform illumination images.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 05:14:29 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Lv", "Feifan", ""], ["Liu", "Bo", ""], ["Lu", "Feng", ""]]}, {"id": "2006.00465", "submitter": "Girma Sisay Mr.", "authors": "Girma Negashe, Adane Mamuye", "title": "Modified Segmentation Algorithm for Recognition of Older Geez Scripts\n  Written on Vellum", "comments": "7 pages, 12 figures, AfricaNLP2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of handwritten document aims at transforming document images into\na machine understandable format. Handwritten document recognition is the most\nchallenging area in the field of pattern recognition. It becomes more complex\nwhen a document was written on vellum before hundreds of years, like older Geez\nscripts. In this study, we introduced a modified segmentation approach to\nrecognize older Geez scripts. We used adaptive filtering for noise reduction,\nIsodata iterative global thresholding for document image binarization, modified\nbounding box projection to segment distinct strokes between Geez characters,\nnumbers, and punctuation marks. SVM multiclass classifier scored 79.32%\nrecognition accuracy with the modified segmentation algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 08:16:27 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Negashe", "Girma", ""], ["Mamuye", "Adane", ""]]}, {"id": "2006.00467", "submitter": "Yura Zharkovsky", "authors": "Yura Zharkovsky, Ovadya Menadeva", "title": "End-to-End Change Detection for High Resolution Drone Images with GAN\n  Architecture", "comments": "This paper will be presented at IMVC 2020\n  (https://www.imvc.co.il/Program/GeneralProgram.aspx)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring large areas is presently feasible with high resolution drone\ncameras, as opposed to time-consuming and expensive ground surveys. In this\nwork we reveal for the first time, the potential of using a state-of-the-art\nchange detection GAN based algorithm with high resolution drone images for\ninfrastructure inspection. We demonstrate this concept on solar panel\ninstallation. A deep learning, data-driven algorithm for identifying changes\nbased on a change detection deep learning algorithm was proposed. We use the\nConditional Adversarial Network approach to present a framework for change\ndetection in images. The proposed network architecture is based on pix2pix GAN\nframework. Extensive experimental results have shown that our proposed approach\noutperforms the other state-of-the-art change detection methods.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 08:19:11 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Zharkovsky", "Yura", ""], ["Menadeva", "Ovadya", ""]]}, {"id": "2006.00472", "submitter": "Jingtao Guo", "authors": "Jingtao Guo, Yi Liu, Zhenzhen Qian, Zuowei Zhou", "title": "Exemplar-based Generative Facial Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image synthesis has witnessed substantial progress due to the increasing\npower of generative model. This paper we propose a novel generative approach\nfor exemplar based facial editing in the form of the region inpainting. Our\nmethod first masks the facial editing region to eliminates the pixel\nconstraints of the original image, then exemplar based facial editing can be\nachieved by learning the corresponding information from the reference image to\ncomplete the masked region. In additional, we impose the attribute labels\nconstraint to model disentangled encodings in order to avoid undesired\ninformation being transferred from the exemplar to the original image editing\nregion. Experimental results demonstrate our method can produce diverse and\npersonalized face editing results and provide far more user control flexibility\nthan nearly all existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 09:15:28 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Guo", "Jingtao", ""], ["Liu", "Yi", ""], ["Qian", "Zhenzhen", ""], ["Zhou", "Zuowei", ""]]}, {"id": "2006.00473", "submitter": "Dana Weitzner", "authors": "Dana Weitzner, David Mendlovic and Raja Giryes", "title": "Face Authentication from Grayscale Coded Light Field", "comments": "To be published at ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face verification is a fast-growing authentication tool for everyday systems,\nsuch as smartphones. While current 2D face recognition methods are very\naccurate, it has been suggested recently that one may wish to add a 3D sensor\nto such solutions to make them more reliable and robust to spoofing, e.g.,\nusing a 2D print of a person's face. Yet, this requires an additional\nrelatively expensive depth sensor. To mitigate this, we propose a novel\nauthentication system, based on slim grayscale coded light field imaging. We\nprovide a reconstruction free fast anti-spoofing mechanism, working directly on\nthe coded image. It is followed by a multi-view, multi-modal face verification\nnetwork that given grayscale data together with a low-res depth map achieves\ncompetitive results to the RGB case. We demonstrate the effectiveness of our\nsolution on a simulated 3D (RGBD) version of LFW, which will be made public,\nand a set of real faces acquired by a light field computational camera.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 09:21:17 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Weitzner", "Dana", ""], ["Mendlovic", "David", ""], ["Giryes", "Raja", ""]]}, {"id": "2006.00545", "submitter": "Ajay Tanwani", "authors": "Ajay Kumar Tanwani, Pierre Sermanet, Andy Yan, Raghav Anand, Mariano\n  Phielipp, Ken Goldberg", "title": "Motion2Vec: Semi-Supervised Representation Learning from Surgical Videos", "comments": "IEEE International Conference on Robotics and Automation (ICRA), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning meaningful visual representations in an embedding space can\nfacilitate generalization in downstream tasks such as action segmentation and\nimitation. In this paper, we learn a motion-centric representation of surgical\nvideo demonstrations by grouping them into action segments/sub-goals/options in\na semi-supervised manner. We present Motion2Vec, an algorithm that learns a\ndeep embedding feature space from video observations by minimizing a metric\nlearning loss in a Siamese network: images from the same action segment are\npulled together while pushed away from randomly sampled images of other\nsegments, while respecting the temporal ordering of the images. The embeddings\nare iteratively segmented with a recurrent neural network for a given\nparametrization of the embedding space after pre-training the Siamese network.\nWe only use a small set of labeled video segments to semantically align the\nembedding space and assign pseudo-labels to the remaining unlabeled data by\ninference on the learned model parameters. We demonstrate the use of this\nrepresentation to imitate surgical suturing motions from publicly available\nvideos of the JIGSAWS dataset. Results give 85.5 % segmentation accuracy on\naverage suggesting performance improvement over several state-of-the-art\nbaselines, while kinematic pose imitation gives 0.94 centimeter error in\nposition per observation on the test set. Videos, code and data are available\nat https://sites.google.com/view/motion2vec\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 15:46:01 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Tanwani", "Ajay Kumar", ""], ["Sermanet", "Pierre", ""], ["Yan", "Andy", ""], ["Anand", "Raghav", ""], ["Phielipp", "Mariano", ""], ["Goldberg", "Ken", ""]]}, {"id": "2006.00568", "submitter": "Bangyong Sun", "authors": "Bangyong Sun, Vincent Whannou de Dravo and Zhe Yu", "title": "A General-Purpose Dehazing Algorithm based on Local Contrast Enhancement\n  Approaches", "comments": "We draw the attention of the reader that this is a work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dehazing is in the image processing and computer vision communities, the task\nof enhancing the image taken in foggy conditions. To better understand this\ntype of algorithm, we present in this document a dehazing method which is\nsuitable for several local contrast adjustment algorithms. We base it on two\nfilters. The first filter is built with a step of normalization with some other\nstatistical tricks while the last represents the local contrast improvement\nalgorithm. Thus, it can work on both CPU and GPU for real-time applications. We\nhope that our approach will open the door to new ideas in the community. Other\nadvantages of our method are first that it does not need to be trained, then it\ndoes not need additional optimization processing. Furthermore, it can be used\nas a pre-treatment or post-processing step in many vision tasks. In addition,\nit does not need to convert the problem into a physical interpretation, and\nfinally that it is very fast. This family of defogging algorithms is fairly\nsimple, but it shows promising results compared to state-of-the-art algorithms\nbased not only on a visual assessment but also on objective criteria.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 17:25:22 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Sun", "Bangyong", ""], ["de Dravo", "Vincent Whannou", ""], ["Yu", "Zhe", ""]]}, {"id": "2006.00601", "submitter": "Chao Wang", "authors": "Chao Wang, Min Tao, James Nagy, Yifei Lou", "title": "Limited-angle CT reconstruction via the L1/L2 minimization", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider minimizing the L1/L2 term on the gradient for a\nlimited-angle scanning problem in computed tomography (CT) reconstruction. We\ndesign a specific splitting framework for an unconstrained optimization model\nso that the alternating direction method of multipliers (ADMM) has guaranteed\nconvergence under certain conditions. In addition, we incorporate a box\nconstraint that is reasonable for imaging applications, and the convergence for\nthe additional box constraint can also be established. Numerical results on\nboth synthetic and experimental datasets demonstrate the effectiveness and\nefficiency of our proposed approaches, showing significant improvements over\nthe state-of-the-art methods in the limited-angle CT reconstruction.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 20:22:30 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 03:56:36 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 04:52:26 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 02:00:03 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Wang", "Chao", ""], ["Tao", "Min", ""], ["Nagy", "James", ""], ["Lou", "Yifei", ""]]}, {"id": "2006.00626", "submitter": "Yin Li", "authors": "Yin Li, Miao Liu, James M. Rehg", "title": "In the Eye of the Beholder: Gaze and Actions in First Person Video", "comments": "Submitted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of jointly determining what a person is doing and where\nthey are looking based on the analysis of video captured by a headworn camera.\nTo facilitate our research, we first introduce the EGTEA Gaze+ dataset. Our\ndataset comes with videos, gaze tracking data, hand masks and action\nannotations, thereby providing the most comprehensive benchmark for First\nPerson Vision (FPV). Moving beyond the dataset, we propose a novel deep model\nfor joint gaze estimation and action recognition in FPV. Our method describes\nthe participant's gaze as a probabilistic variable and models its distribution\nusing stochastic units in a deep network. We further sample from these\nstochastic units, generating an attention map to guide the aggregation of\nvisual features for action recognition. Our method is evaluated on our EGTEA\nGaze+ dataset and achieves a performance level that exceeds the\nstate-of-the-art by a significant margin. More importantly, we demonstrate that\nour model can be applied to larger scale FPV dataset---EPIC-Kitchens even\nwithout using gaze, offering new state-of-the-art results on FPV action\nrecognition.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 22:06:06 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 05:00:32 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Li", "Yin", ""], ["Liu", "Miao", ""], ["Rehg", "James M.", ""]]}, {"id": "2006.00644", "submitter": "Mahdi Elhousni", "authors": "Mahdi Elhousni, Yecheng Lyu, Ziming Zhang, Xinming Huang", "title": "Automatic Building and Labeling of HD Maps with Deep Learning", "comments": "Accepted by IAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a world where autonomous driving cars are becoming increasingly more\ncommon, creating an adequate infrastructure for this new technology is\nessential. This includes building and labeling high-definition (HD) maps\naccurately and efficiently. Today, the process of creating HD maps requires a\nlot of human input, which takes time and is prone to errors. In this paper, we\npropose a novel method capable of generating labelled HD maps from raw sensor\ndata. We implemented and tested our methods on several urban scenarios using\ndata collected from our test vehicle. The results show that the pro-posed deep\nlearning based method can produce highly accurate HD maps. This approach speeds\nup the process of building and labeling HD maps, which can make meaningful\ncontribution to the deployment of autonomous vehicle.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 00:02:45 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Elhousni", "Mahdi", ""], ["Lyu", "Yecheng", ""], ["Zhang", "Ziming", ""], ["Huang", "Xinming", ""]]}, {"id": "2006.00648", "submitter": "Mahdi Elhousni", "authors": "Mahdi Elhousni and Xinming Huang", "title": "A Survey on 3D LiDAR Localization for Autonomous Vehicles", "comments": "Accepted by IV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR sensors are becoming one of the most essential sensors in achieving\nfull autonomy for self driving cars. LiDARs are able to produce rich, dense and\nprecise spatial data, which can tremendously help in localizing and tracking a\nmoving vehicle. In this paper, we review the latest finding in 3D LiDAR\nlocalization for autonomous driving cars, and analyse the results obtained by\neach method, in an effort to guide the research community towards the path that\nseems to be the most promising.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 00:19:35 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 17:07:12 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Elhousni", "Mahdi", ""], ["Huang", "Xinming", ""]]}, {"id": "2006.00654", "submitter": "Rafael Biazus Mangolin", "authors": "Rafael B. Mangolin, Rodolfo M. Pereira, Alceu S. Britto Jr., Carlos N.\n  Silla Jr., Val\\'eria D. Feltrim, Diego Bertolini and Yandre M. G. Costa", "title": "A multimodal approach for multi-label movie genre classification", "comments": "21 pages and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Movie genre classification is a challenging task that has increasingly\nattracted the attention of researchers. In this paper, we addressed the\nmulti-label classification of the movie genres in a multimodal way. For this\npurpose, we created a dataset composed of trailer video clips, subtitles,\nsynopses, and movie posters taken from 152,622 movie titles from The Movie\nDatabase. The dataset was carefully curated and organized, and it was also made\navailable as a contribution of this work. Each movie of the dataset was labeled\naccording to a set of eighteen genre labels. We extracted features from these\ndata using different kinds of descriptors, namely Mel Frequency Cepstral\nCoefficients, Statistical Spectrum Descriptor , Local Binary Pattern with\nspectrograms, Long-Short Term Memory, and Convolutional Neural Networks. The\ndescriptors were evaluated using different classifiers, such as BinaryRelevance\nand ML-kNN. We have also investigated the performance of the combination of\ndifferent classifiers/features using a late fusion strategy, which obtained\nencouraging results. Based on the F-Score metric, our best result, 0.628, was\nobtained by the fusion of a classifier created using LSTM on the synopses, and\na classifier created using CNN on movie trailer frames. When considering the\nAUC-PR metric, the best result, 0.673, was also achieved by combining those\nrepresentations, but in addition, a classifier based on LSTM created from the\nsubtitles was used. These results corroborate the existence of complementarity\namong classifiers based on different sources of information in this field of\napplication. As far as we know, this is the most comprehensive study developed\nin terms of the diversity of multimedia sources of information to perform movie\ngenre classification.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 00:51:39 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Mangolin", "Rafael B.", ""], ["Pereira", "Rodolfo M.", ""], ["Britto", "Alceu S.", "Jr."], ["Silla", "Carlos N.", "Jr."], ["Feltrim", "Val\u00e9ria D.", ""], ["Bertolini", "Diego", ""], ["Costa", "Yandre M. G.", ""]]}, {"id": "2006.00684", "submitter": "Alireza Rezvanifar", "authors": "Alireza Rezvanifar, Melissa Cote, Alexandra Branzan Albu", "title": "Symbol Spotting on Digital Architectural Floor Plans Using a Deep\n  Learning-based Framework", "comments": "Accepted to CVPR2020 Workshop on Text and Documents in the Deep\n  Learning Era", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This papers focuses on symbol spotting on real-world digital architectural\nfloor plans with a deep learning (DL)-based framework. Traditional on-the-fly\nsymbol spotting methods are unable to address the semantic challenge of\ngraphical notation variability, i.e. low intra-class symbol similarity, an\nissue that is particularly important in architectural floor plan analysis. The\npresence of occlusion and clutter, characteristic of real-world plans, along\nwith a varying graphical symbol complexity from almost trivial to highly\ncomplex, also pose challenges to existing spotting methods. In this paper, we\naddress all of the above issues by leveraging recent advances in DL and\nadapting an object detection framework based on the You-Only-Look-Once (YOLO)\narchitecture. We propose a training strategy based on tiles, avoiding many\nissues particular to DL-based object detection networks related to the relative\nsmall size of symbols compared to entire floor plans, aspect ratios, and data\naugmentation. Experiments on real-world floor plans demonstrate that our method\nsuccessfully detects architectural symbols with low intra-class similarity and\nof variable graphical complexity, even in the presence of heavy occlusion and\nclutter. Additional experiments on the public SESYD dataset confirm that our\nproposed approach can deal with various degradation and noise levels and\noutperforms other symbol spotting methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 03:16:05 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Rezvanifar", "Alireza", ""], ["Cote", "Melissa", ""], ["Albu", "Alexandra Branzan", ""]]}, {"id": "2006.00710", "submitter": "Alexey Sidnev", "authors": "Alexey Sidnev, Alexander Krapivin, Alexey Trushkov, Ekaterina\n  Krasikova, Maxim Kazakov, Mikhail Viryasov", "title": "DeepMark++: Real-time Clothing Detection at the Edge", "comments": "Winter Conference on Applications of Computer Vision (WACV21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clothing recognition is the most fundamental AI application challenge within\nthe fashion domain. While existing solutions offer decent recognition accuracy,\nthey are generally slow and require significant computational resources. In\nthis paper we propose a single-stage approach to overcome this obstacle and\ndeliver rapid clothing detection and keypoint estimation. Our solution is based\non a multi-target network CenterNet, and we introduce several powerful\npost-processing techniques to enhance performance. Our most accurate model\nachieves results comparable to state-of-the-art solutions on the DeepFashion2\ndataset, and our light and fast model runs at 17 FPS on the Huawei P40 Pro\nsmartphone. In addition, we achieved second place in the DeepFashion2 Landmark\nEstimation Challenge 2020 with 0.582 mAP on the test dataset.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 04:36:57 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 07:52:58 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 07:47:43 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Sidnev", "Alexey", ""], ["Krapivin", "Alexander", ""], ["Trushkov", "Alexey", ""], ["Krasikova", "Ekaterina", ""], ["Kazakov", "Maxim", ""], ["Viryasov", "Mikhail", ""]]}, {"id": "2006.00727", "submitter": "Abhishek Moturu", "authors": "Alex Chang, Vinith M. Suriyakumar, Abhishek Moturu, Nipaporn\n  Tewattanarat, Andrea Doria, Anna Goldenberg", "title": "Using Generative Models for Pediatric wbMRI", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/BXC_fpbLe", "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Early detection of cancer is key to a good prognosis and requires frequent\ntesting, especially in pediatrics. Whole-body magnetic resonance imaging\n(wbMRI) is an essential part of several well-established screening protocols,\nwith screening starting in early childhood. To date, machine learning (ML) has\nbeen used on wbMRI images to stage adult cancer patients. It is not possible to\nuse such tools in pediatrics due to the changing bone signal throughout growth,\nthe difficulty of obtaining these images in young children due to movement and\nlimited compliance, and the rarity of positive cases. We evaluate the quality\nof wbMRI images generated using generative adversarial networks (GANs) trained\non wbMRI data from The Hospital for Sick Children in Toronto. We use the Frchet\nInception Distance (FID) metric, Domain Frchet Distance (DFD), and blind tests\nwith a radiology fellow for evaluation. We demonstrate that StyleGAN2 provides\nthe best performance in generating wbMRI images with respect to all three\nmetrics.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 05:29:18 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Chang", "Alex", ""], ["Suriyakumar", "Vinith M.", ""], ["Moturu", "Abhishek", ""], ["Tewattanarat", "Nipaporn", ""], ["Doria", "Andrea", ""], ["Goldenberg", "Anna", ""]]}, {"id": "2006.00730", "submitter": "Mizuho Nishio", "authors": "Mizuho Nishio, Shunjiro Noguchi, Hidetoshi Matsuo, Takamichi Murakami", "title": "Automatic classification between COVID-19 pneumonia, non-COVID-19\n  pneumonia, and the healthy on chest X-ray image: combination of data\n  augmentation methods", "comments": null, "journal-ref": "Sci Rep 10, 17532 (2020)", "doi": "10.1038/s41598-020-74539-2", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: This study aimed to develop and validate computer-aided diagnosis\n(CXDx) system for classification between COVID-19 pneumonia, non-COVID-19\npneumonia, and the healthy on chest X-ray (CXR) images.\n  Materials and Methods: From two public datasets, 1248 CXR images were\nobtained, which included 215, 533, and 500 CXR images of COVID-19 pneumonia\npatients, non-COVID-19 pneumonia patients, and the healthy samples. The\nproposed CADx system utilized VGG16 as a pre-trained model and combination of\nconventional method and mixup as data augmentation methods. Other types of\npre-trained models were compared with the VGG16-based model. Single type or no\ndata augmentation methods were also evaluated. Splitting of\ntraining/validation/test sets was used when building and evaluating the CADx\nsystem. Three-category accuracy was evaluated for test set with 125 CXR images.\n  Results: The three-category accuracy of the CAD system was 83.6% between\nCOVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy. Sensitivity for\nCOVID-19 pneumonia was more than 90%. The combination of conventional method\nand mixup was more useful than single type or no data augmentation method.\n  Conclusion: This study was able to create an accurate CADx system for the\n3-category classification. Source code of our CADx system is available as open\nsource for COVID-19 research.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 05:34:53 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 03:47:25 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Nishio", "Mizuho", ""], ["Noguchi", "Shunjiro", ""], ["Matsuo", "Hidetoshi", ""], ["Murakami", "Takamichi", ""]]}, {"id": "2006.00752", "submitter": "Xin Jin", "authors": "Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen", "title": "Global Distance-distributions Separation for Unsupervised Person\n  Re-identification", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised person re-identification (ReID) often has poor scalability and\nusability in real-world deployments due to domain gaps and the lack of\nannotations for the target domain data. Unsupervised person ReID through domain\nadaptation is attractive yet challenging. Existing unsupervised ReID approaches\noften fail in correctly identifying the positive samples and negative samples\nthrough the distance-based matching/ranking. The two distributions of distances\nfor positive sample pairs (Pos-distr) and negative sample pairs (Neg-distr) are\noften not well separated, having large overlap. To address this problem, we\nintroduce a global distance-distributions separation (GDS) constraint over the\ntwo distributions to encourage the clear separation of positive and negative\nsamples from a global view. We model the two global distance distributions as\nGaussian distributions and push apart the two distributions while encouraging\ntheir sharpness in the unsupervised training process. Particularly, to model\nthe distributions from a global view and facilitate the timely updating of the\ndistributions and the GDS related losses, we leverage a momentum update\nmechanism for building and maintaining the distribution parameters (mean and\nvariance) and calculate the loss on the fly during the training.\nDistribution-based hard mining is proposed to further promote the separation of\nthe two distributions. We validate the effectiveness of the GDS constraint in\nunsupervised ReID networks. Extensive experiments on multiple ReID benchmark\ndatasets show our method leads to significant improvement over the baselines\nand achieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 07:05:39 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 02:25:10 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 09:27:59 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Jin", "Xin", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Chen", "Zhibo", ""]]}, {"id": "2006.00753", "submitter": "Peng Wang", "authors": "Chenyu Gao and Qi Zhu and Peng Wang and Hui Li and Yuliang Liu and\n  Anton van den Hengel and Qi Wu", "title": "Structured Multimodal Attentions for TextVQA", "comments": "19 pages, winner of TextVQA Challenge 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text based Visual Question Answering (TextVQA) is a recently raised challenge\nthat requires a machine to read text in images and answer natural language\nquestions by jointly reasoning over the question, Optical Character Recognition\n(OCR) tokens and visual content. Most of the state-of-the-art (SoTA) VQA\nmethods fail to answer these questions because of i) poor text reading ability;\nii) lacking of text-visual reasoning capacity; and iii) adopting a\ndiscriminative answering mechanism instead of a generative one which is hard to\ncover both OCR tokens and general text tokens in the final answer. In this\npaper, we propose a structured multimodal attention (SMA) neural network to\nsolve the above issues. Our SMA first uses a structural graph representation to\nencode the object-object, object-text and text-text relationships appearing in\nthe image, and then design a multimodal graph attention network to reason over\nit. Finally, the outputs from the above module are processed by a global-local\nattentional answering module to produce an answer that covers tokens from both\nOCR and general text iteratively. Our proposed model outperforms the SoTA\nmodels on TextVQA dataset and all three tasks of ST-VQA dataset. To provide an\nupper bound for our method and a fair testing base for further works, we also\nprovide human-annotated ground-truth OCR annotations for the TextVQA dataset,\nwhich were not given in the original release.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 07:07:36 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Gao", "Chenyu", ""], ["Zhu", "Qi", ""], ["Wang", "Peng", ""], ["Li", "Hui", ""], ["Liu", "Yuliang", ""], ["Hengel", "Anton van den", ""], ["Wu", "Qi", ""]]}, {"id": "2006.00757", "submitter": "Jun Fu", "authors": "Jun Fu and Jianfeng Xu and Kazuyuki Tasaka and Zhibo Chen", "title": "Residual Squeeze-and-Excitation Network for Fast Image Deraining", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image deraining is an important image processing task as rain streaks not\nonly severely degrade the visual quality of images but also significantly\naffect the performance of high-level vision tasks. Traditional methods\nprogressively remove rain streaks via different recurrent neural networks.\nHowever, these methods fail to yield plausible rain-free images in an efficient\nmanner. In this paper, we propose a residual squeeze-and-excitation network\ncalled RSEN for fast image deraining as well as superior deraining performance\ncompared with state-of-the-art approaches. Specifically, RSEN adopts a\nlightweight encoder-decoder architecture to conduct rain removal in one stage.\nBesides, both encoder and decoder adopt a novel residual squeeze-and-excitation\nblock as the core of feature extraction, which contains a residual block for\nproducing hierarchical features, followed by a squeeze-and-excitation block for\nchannel-wisely enhancing the resulted hierarchical features. Experimental\nresults demonstrate that our method can not only considerably reduce the\ncomputational complexity but also significantly improve the deraining\nperformance compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 07:17:01 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Fu", "Jun", ""], ["Xu", "Jianfeng", ""], ["Tasaka", "Kazuyuki", ""], ["Chen", "Zhibo", ""]]}, {"id": "2006.00781", "submitter": "Xiaolei Yin", "authors": "Xiao-Lei Yin, Dong-Xue Liang, Lu Wang, Jing Qiu, Zhi-Yun Yang, Jun-Hui\n  Xing, Jian-Zeng Dong and Zhao-Yuan Ma", "title": "Reducing the X-ray radiation exposure frequency in cardio-angiography\n  via deep-learning based video interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cardiac coronary angiography is a major technology to assist doctors during\ncardiac interventional surgeries. Under the exposure of X-ray radiation,\ndoctors inject contrast agents through catheters to determine the position and\nstatus of coronary vessels in real time. To get a coronary angiography video\nwith a high frame rate, the doctor needs to increase the exposure frequency and\nintensity of the X-ray. This will inevitably increase the X-ray harm to both\npatients and surgeons. In this work, we innovatively utilize a deep-learning\nbased video interpolation algorithm to interpolate coronary angiography videos.\nMoreover, we establish a new coronary angiography image dataset ,which contains\n95,039 triplets images to retrain the video interpolation network model. Using\nthe retrained network we synthesize high frame rate coronary angiography video\nfrom the low frame rate coronary angiography video. The average peak signal to\nnoise ratio(PSNR) of those synthesized video frames reaches 34dB. Extensive\nexperiment results demonstrate the feasibility of using the video frame\ninterpolation algorithm to synthesize continuous and clear high frame rate\ncoronary angiography video. With the help of this technology, doctors can\nsignificantly reduce exposure frequency and intensity of the X-ray during\ncoronary angiography.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 08:14:10 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Yin", "Xiao-Lei", ""], ["Liang", "Dong-Xue", ""], ["Wang", "Lu", ""], ["Qiu", "Jing", ""], ["Yang", "Zhi-Yun", ""], ["Xing", "Jun-Hui", ""], ["Dong", "Jian-Zeng", ""], ["Ma", "Zhao-Yuan", ""]]}, {"id": "2006.00785", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Benet Oriol, Jordi Luque, Ferran Diego and Xavier Giro-i-Nieto", "title": "Transcription-Enriched Joint Embeddings for Spoken Descriptions of\n  Images and Videos", "comments": "Accepted for presentation at EPIC@CVPR2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an effective approach for training unique embedding\nrepresentations by combining three simultaneous modalities: image and spoken\nand textual narratives. The proposed methodology departs from a baseline system\nthat spawns a embedding space trained with only spoken narratives and image\ncues. Our experiments on the EPIC-Kitchen and Places Audio Caption datasets\nshow that introducing the human-generated textual transcriptions of the spoken\nnarratives helps to the training procedure yielding to get better embedding\nrepresentations. The triad speech, image and words allows for a better estimate\nof the point embedding and show an improving of the performance within tasks\nlike image and speech retrieval, even when text third modality, text, is not\npresent in the task.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 08:18:15 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Oriol", "Benet", ""], ["Luque", "Jordi", ""], ["Diego", "Ferran", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "2006.00809", "submitter": "Konstantin Sofiiuk", "authors": "Konstantin Sofiiuk, Polina Popenova and Anton Konushin", "title": "Foreground-aware Semantic Representations for Image Harmonization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image harmonization is an important step in photo editing to achieve visual\nconsistency in composite images by adjusting the appearances of foreground to\nmake it compatible with background. Previous approaches to harmonize composites\nare based on training of encoder-decoder networks from scratch, which makes it\nchallenging for a neural network to learn a high-level representation of\nobjects. We propose a novel architecture to utilize the space of high-level\nfeatures learned by a pre-trained classification network. We create our models\nas a combination of existing encoder-decoder architectures and a pre-trained\nforeground-aware deep high-resolution network. We extensively evaluate the\nproposed method on existing image harmonization benchmark and set up a new\nstate-of-the-art in terms of MSE and PSNR metrics. The code and trained models\nare available at \\url{https://github.com/saic-vul/image_harmonization}.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 09:27:20 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Sofiiuk", "Konstantin", ""], ["Popenova", "Polina", ""], ["Konushin", "Anton", ""]]}, {"id": "2006.00816", "submitter": "Christos Strydis", "authors": "Paul Bakker, Henk-Jan Boele, Zaid Al-Ars and Christos Strydis", "title": "Real-Time Face and Landmark Localization for Eyeblink Detection", "comments": "Added public gitlab repo link with paper source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pavlovian eyeblink conditioning is a powerful experiment used in the field of\nneuroscience to measure multiple aspects of how we learn in our daily life. To\ntrack the movement of the eyelid during an experiment, researchers have\ntraditionally made use of potentiometers or electromyography. More recently,\nthe use of computer vision and image processing alleviated the need for these\ntechniques but currently employed methods require human intervention and are\nnot fast enough to enable real-time processing. In this work, a face- and\nlandmark-detection algorithm have been carefully combined in order to provide\nfully automated eyelid tracking, and have further been accelerated to make the\nfirst crucial step towards online, closed-loop experiments. Such experiments\nhave not been achieved so far and are expected to offer significant insights in\nthe workings of neurological and psychiatric disorders. Based on an extensive\nliterature search, various different algorithms for face detection and landmark\ndetection have been analyzed and evaluated. Two algorithms were identified as\nmost suitable for eyelid detection: the Histogram-of-Oriented-Gradients (HOG)\nalgorithm for face detection and the Ensemble-of-Regression-Trees (ERT)\nalgorithm for landmark detection. These two algorithms have been accelerated on\nGPU and CPU, achieving speedups of 1,753$\\times$ and 11$\\times$, respectively.\nTo demonstrate the usefulness of our eyelid-detection algorithm, a research\nhypothesis was formed and a well-established neuroscientific experiment was\nemployed: eyeblink detection. Our experimental evaluation reveals an overall\napplication runtime of 0.533 ms per frame, which is 1,101$\\times$ faster than\nthe sequential implementation and well within the real-time requirements of\neyeblink conditioning in humans, i.e. faster than 500 frames per second.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 09:46:25 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 16:16:11 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Bakker", "Paul", ""], ["Boele", "Henk-Jan", ""], ["Al-Ars", "Zaid", ""], ["Strydis", "Christos", ""]]}, {"id": "2006.00821", "submitter": "Shoaib Azam", "authors": "Farzeen Munir, Shoaib Azam, Muhammd Aasim Rafique, Ahmad Muqeem Sheri,\n  Moongu Jeon, Witold Pedrycz", "title": "Exploring Thermal Images for Object Detection in Underexposure Regions\n  for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underexposure regions are vital to construct a complete perception of the\nsurroundings for safe autonomous driving. The availability of thermal cameras\nhas provided an essential alternate to explore regions where other optical\nsensors lack in capturing interpretable signals. A thermal camera captures an\nimage using the heat difference emitted by objects in the infrared spectrum,\nand object detection in thermal images becomes effective for autonomous driving\nin challenging conditions. Although object detection in the visible spectrum\ndomain imaging has matured, thermal object detection lacks effectiveness. A\nsignificant challenge is scarcity of labeled data for the thermal domain which\nis desiderata for SOTA artificial intelligence techniques. This work proposes a\ndomain adaptation framework which employs a style transfer technique for\ntransfer learning from visible spectrum images to thermal images. The framework\nuses a generative adversarial network (GAN) to transfer the low-level features\nfrom the visible spectrum domain to the thermal domain through style\nconsistency. The efficacy of the proposed method of object detection in thermal\nimages is evident from the improved results when used styled images from\npublicly available thermal image datasets (FLIR ADAS and KAIST Multi-Spectral).\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 09:59:09 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 09:24:14 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Munir", "Farzeen", ""], ["Azam", "Shoaib", ""], ["Rafique", "Muhammd Aasim", ""], ["Sheri", "Ahmad Muqeem", ""], ["Jeon", "Moongu", ""], ["Pedrycz", "Witold", ""]]}, {"id": "2006.00830", "submitter": "Fadime Sener", "authors": "Fadime Sener and Dipika Singhania and Angela Yao", "title": "Temporal Aggregate Representations for Long-Range Video Understanding", "comments": "ECCV 2020, European Conference on Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future prediction, especially in long-range videos, requires reasoning from\ncurrent and past observations. In this work, we address questions of temporal\nextent, scaling, and level of semantic abstraction with a flexible\nmulti-granular temporal aggregation framework. We show that it is possible to\nachieve state of the art in both next action and dense anticipation with simple\ntechniques such as max-pooling and attention. To demonstrate the anticipation\ncapabilities of our model, we conduct experiments on Breakfast, 50Salads, and\nEPIC-Kitchens datasets, where we achieve state-of-the-art results. With minimal\nmodifications, our model can also be extended for video segmentation and action\nrecognition.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 10:17:55 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 23:33:43 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Sener", "Fadime", ""], ["Singhania", "Dipika", ""], ["Yao", "Angela", ""]]}, {"id": "2006.00836", "submitter": "Markku Luotamo", "authors": "Markku Luotamo, Sari Mets\\\"am\\\"aki, Arto Klami", "title": "Multi-scale Cloud Detection in Remote Sensing Images using a Dual\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2020.3015272", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Semantic segmentation by convolutional neural networks (CNN) has advanced the\nstate of the art in pixel-level classification of remote sensing images.\nHowever, processing large images typically requires analyzing the image in\nsmall patches, and hence features that have large spatial extent still cause\nchallenges in tasks such as cloud masking. To support a wider scale of spatial\nfeatures while simultaneously reducing computational requirements for large\nsatellite images, we propose an architecture of two cascaded CNN model\ncomponents successively processing undersampled and full resolution images. The\nfirst component distinguishes between patches in the inner cloud area from\npatches at the cloud's boundary region. For the cloud-ambiguous edge patches\nrequiring further segmentation, the framework then delegates computation to a\nfine-grained model component. We apply the architecture to a cloud detection\ndataset of complete Sentinel-2 multispectral images, approximately annotated\nfor minimal false negatives in a land use application. On this specific task\nand data, we achieve a 16\\% relative improvement in pixel accuracy over a CNN\nbaseline based on patching.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 10:27:42 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Luotamo", "Markku", ""], ["Mets\u00e4m\u00e4ki", "Sari", ""], ["Klami", "Arto", ""]]}, {"id": "2006.00842", "submitter": "Ben Wang", "authors": "Ben Wang", "title": "LFTag: A Scalable Visual Fiducial System with Low Spatial Frequency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual fiducial systems are a key component of many robotics and AR/VR\napplications for 6-DOF monocular relative pose estimation and target\nidentification. This paper presents LFTag, a visual fiducial system based on\ntopological detection and relative position data encoding which optimizes data\ndensity within spatial frequency constraints. The marker is constructed to\nresolve rotational ambiguity, which combined with the robust geometric and\ntopological false positive rejection, allows all marker bits to be used for\ndata.\n  When compared to existing state-of-the-art square binary markers (AprilTag)\nand topological markers (TopoTag) in simulation, the proposed fiducial system\n(LFTag) offers significant advances in dictionary size and range. LFTag 3x3\nachieves 546 times the dictionary size of AprilTag 25h9 and LFTag 4x4 achieves\n126 thousand times the dictionary size of AprilTag 41h12 while simultaneously\nachieving longer detection range. LFTag 3x3 also achieves more than twice the\ndetection range of TopoTag 4x4 at the same dictionary size.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 10:34:34 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Wang", "Ben", ""]]}, {"id": "2006.00857", "submitter": "Guibin Chen", "authors": "Guibin Chen, Jiong Deng, Dongze Huang, Shuo Zhang", "title": "3D Lidar Mapping Relative Accuracy Automatic Evaluation Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HD (High Definition) map based on 3D lidar plays a vital role in autonomous\nvehicle localization, planning, decision-making, perception, etc. Many 3D lidar\nmapping technologies related to SLAM (Simultaneous Localization and Mapping)\nare used in HD map construction to ensure its high accuracy. To evaluate the\naccuracy of 3D lidar mapping, the most common methods use ground truth of poses\nto calculate the error between estimated poses and ground truth, however it's\nusually so difficult to get the ground truth of poses in the actual lidar\nmapping for autonomous vehicle. In this paper, we proposed a relative accuracy\nevaluation algorithm that can automatically evaluate the accuracy of HD map\nbuilt by 3D lidar mapping without ground truth. A method for detecting the\ndegree of ghosting in point cloud map quantitatively is designed to reflect the\naccuracy indirectly, which takes advantage of the principle of light traveling\nin a straight line and the fact that light can not penetrate opaque objects.\nOur experimental results confirm that the proposed evaluation algorithm can\nautomatically and efficiently detect the bad poses whose accuracy are less than\nthe set threshold such as 0.1m, then calculate the bad poses percentage P_bad\nin all estimated poses to obtain the final accuracy metric P_acc = 1 - P_bad.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 11:30:31 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Chen", "Guibin", ""], ["Deng", "Jiong", ""], ["Huang", "Dongze", ""], ["Zhang", "Shuo", ""]]}, {"id": "2006.00878", "submitter": "Hanrong Ye", "authors": "Hanrong Ye, Hong Liu, Fanyang Meng, Xia Li", "title": "Bi-directional Exponential Angular Triplet Loss for RGB-Infrared Person\n  Re-Identification", "comments": "First Submission: April 2019. The final revision accepted by the IEEE\n  Transactions on Image Processing in December 2020", "journal-ref": null, "doi": "10.1109/TIP.2020.3045261", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  RGB-Infrared person re-identification (RGB-IR Re- ID) is a cross-modality\nmatching problem, where the modality discrepancy is a big challenge. Most\nexisting works use Euclidean metric based constraints to resolve the\ndiscrepancy between features of images from different modalities. However,\nthese methods are incapable of learning angularly discriminative feature\nembedding because Euclidean distance cannot measure the included angle between\nembedding vectors effectively. As an angularly discriminative feature space is\nimportant for classifying the human images based on their embedding vectors, in\nthis paper, we propose a novel ranking loss function, named Bi-directional\nExponential Angular Triplet Loss, to help learn an angularly separable common\nfeature space by explicitly constraining the included angles between embedding\nvectors. Moreover, to help stabilize and learn the magnitudes of embedding\nvectors, we adopt a common space batch normalization layer. The quantitative\nand qualitative experiments on the SYSU-MM01 and RegDB dataset support our\nanalysis. On SYSU-MM01 dataset, the performance is improved from 7.40% / 11.46%\nto 38.57% / 38.61% for rank-1 accuracy / mAP compared with the baseline. The\nproposed method can be generalized to the task of single-modality Re-ID and\nimproves the rank-1 accuracy / mAP from 92.0% / 81.7% to 94.7% / 86.6% on the\nMarket-1501 dataset, from 82.6% / 70.6% to 87.6% / 77.1% on the DukeMTMC-reID\ndataset. Code: https://github.com/prismformore/expAT\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 12:26:08 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 06:57:20 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 03:39:10 GMT"}, {"version": "v4", "created": "Mon, 14 Dec 2020 03:41:54 GMT"}, {"version": "v5", "created": "Tue, 29 Dec 2020 08:36:53 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Ye", "Hanrong", ""], ["Liu", "Hong", ""], ["Meng", "Fanyang", ""], ["Li", "Xia", ""]]}, {"id": "2006.00904", "submitter": "Jesus Hormigo", "authors": "Jose David Fern\\'andez Rodr\\'iguez, David Daniel Albarrac\\'in Molina,\n  Jes\\'us Hormigo Cebolla", "title": "Implementing AI-powered semantic character recognition in motor racing\n  sports", "comments": "8 pages, 7 figures, 2020 NAB Broadcast Engineering and Information\n  Technology (BEIT) Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oftentimes TV producers of motor-racing programs overlay visual and textual\nmedia to provide on-screen context about drivers, such as a driver's name,\nposition or photo. Typically this is accomplished by a human producer who\nvisually identifies the drivers on screen, manually toggling the contextual\nmedia associated to each one and coordinating with cameramen and other TV\nproducers to keep the racer in the shot while the contextual media is on\nscreen. This labor-intensive and highly dedicated process is mostly suited to\nstatic overlays and makes it difficult to overlay contextual information about\nmany drivers at the same time in short shots. This paper presents a system that\nlargely automates these tasks and enables dynamic overlays using deep learning\nto track the drivers as they move on screen, without human intervention. This\nsystem is not merely theoretical, but an implementation has already been\ndeployed during live races by a TV production company at Formula E races. We\npresent the challenges faced during the implementation and discuss the\nimplications. Additionally, we cover future applications and roadmap of this\nnew technological development.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 12:59:56 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Rodr\u00edguez", "Jose David Fern\u00e1ndez", ""], ["Molina", "David Daniel Albarrac\u00edn", ""], ["Cebolla", "Jes\u00fas Hormigo", ""]]}, {"id": "2006.00923", "submitter": "Lluis Gomez", "authors": "Llu\\'is G\\'omez, Ali Furkan Biten, Rub\\`en Tito, Andr\\'es Mafla,\n  Mar\\c{c}al Rusi\\~nol, Ernest Valveny, Dimosthenis Karatzas", "title": "Multimodal grid features and cell pointers for Scene Text Visual\n  Question Answering", "comments": "This paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new model for the task of scene text visual question\nanswering, in which questions about a given image can only be answered by\nreading and understanding scene text that is present in it. The proposed model\nis based on an attention mechanism that attends to multi-modal features\nconditioned to the question, allowing it to reason jointly about the textual\nand visual modalities in the scene. The output weights of this attention module\nover the grid of multi-modal spatial features are interpreted as the\nprobability that a certain spatial location of the image contains the answer\ntext the to the given question. Our experiments demonstrate competitive\nperformance in two standard datasets. Furthermore, this paper provides a novel\nanalysis of the ST-VQA dataset based on a human performance study.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 13:17:44 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 10:47:17 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["G\u00f3mez", "Llu\u00eds", ""], ["Biten", "Ali Furkan", ""], ["Tito", "Rub\u00e8n", ""], ["Mafla", "Andr\u00e9s", ""], ["Rusi\u00f1ol", "Mar\u00e7al", ""], ["Valveny", "Ernest", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "2006.00954", "submitter": "Gianni Franchi", "authors": "Gianni Franchi, Andrei Bursuc, Emanuel Aldea, Severine Dubuisson,\n  Isabelle Bloch", "title": "One Versus all for deep Neural Network Incertitude (OVNNI)\n  quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are powerful learning models yet their results\nare not always reliable. This is due to the fact that modern DNNs are usually\nuncalibrated and we cannot characterize their epistemic uncertainty. In this\nwork, we propose a new technique to quantify the epistemic uncertainty of data\neasily. This method consists in mixing the predictions of an ensemble of DNNs\ntrained to classify One class vs All the other classes (OVA) with predictions\nfrom a standard DNN trained to perform All vs All (AVA) classification. On the\none hand, the adjustment provided by the AVA DNN to the score of the base\nclassifiers allows for a more fine-grained inter-class separation. On the other\nhand, the two types of classifiers enforce mutually their detection of\nout-of-distribution (OOD) samples, circumventing entirely the requirement of\nusing such samples during training. Our method achieves state of the art\nperformance in quantifying OOD data across multiple datasets and architectures\nwhile requiring little hyper-parameter tuning.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 14:06:12 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Franchi", "Gianni", ""], ["Bursuc", "Andrei", ""], ["Aldea", "Emanuel", ""], ["Dubuisson", "Severine", ""], ["Bloch", "Isabelle", ""]]}, {"id": "2006.00980", "submitter": "Jan Egger", "authors": "Jianning Li, Antonio Pepe, Christina Gsaxner, Jan Egger", "title": "An Online Platform for Automatic Skull Defect Restoration and Cranial\n  Implant Design", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a fully automatic system for cranial implant design, a common\ntask in cranioplasty operations. The system is currently integrated in\nStudierfenster (http://studierfenster.tugraz.at/), an online, cloud-based\nmedical image processing platform for medical imaging applications. Enhanced by\ndeep learning algorithms, the system automatically restores the missing part of\na skull (i.e., skull shape completion) and generates the desired implant by\nsubtracting the defective skull from the completed skull. The generated implant\ncan be downloaded in the STereoLithography (.stl) format directly via the\nbrowser interface of the system. The implant model can then be sent to a 3D\nprinter for in loco implant manufacturing. Furthermore, thanks to the standard\nformat, the user can thereafter load the model into another application for\npost-processing whenever necessary. Such an automatic cranial implant design\nsystem can be integrated into the clinical practice to improve the current\nroutine for surgeries related to skull defect repair (e.g., cranioplasty). Our\nsystem, although currently intended for educational and research use only, can\nbe seen as an application of additive manufacturing for fast, patient-specific\nimplant design.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 14:41:33 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Li", "Jianning", ""], ["Pepe", "Antonio", ""], ["Gsaxner", "Christina", ""], ["Egger", "Jan", ""]]}, {"id": "2006.01015", "submitter": "Christopher Hahne", "authors": "Christopher Hahne, Amar Aggoun", "title": "PlenoptiSign: an optical design tool for plenoptic imaging", "comments": "https://github.com/hahnec/plenoptisign/", "journal-ref": null, "doi": "10.1016/j.softx.2019.100259", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plenoptic imaging enables a light-field to be captured by a single monocular\nobjective lens and an array of micro lenses attached to an image sensor. Metric\ndistances of the light-field's depth planes remain unapparent prior to\nacquisition. Recent research showed that sampled depth locations rely on the\nparameters of the system's optical components. This paper presents\nPlenoptiSign, which implements these findings as a Python software package to\nhelp assist in an experimental or prototyping stage of a plenoptic system.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 15:21:44 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hahne", "Christopher", ""], ["Aggoun", "Amar", ""]]}, {"id": "2006.01030", "submitter": "Anatoly Belikov MSc", "authors": "Anatoly Belikov and Alexey Potapov", "title": "GoodPoint: unsupervised learning of keypoint detection and description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new algorithm for unsupervised learning of keypoint\ndetectors and descriptors, which demonstrates fast convergence and good\nperformance across different datasets. The training procedure uses homographic\ntransformation of images. The proposed model learns to detect points and\ngenerate descriptors on pairs of transformed images, which are easy for it to\ndistinguish and repeatedly detect. The trained model follows SuperPoint\narchitecture for ease of comparison, and demonstrates similar performance on\nnatural images from HPatches dataset, and better performance on retina images\nfrom Fundus Image Registration Dataset, which contain low number of corner-like\nfeatures. For HPatches and other datasets, coverage was also computed to\nprovide better estimation of model quality.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 15:57:30 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Belikov", "Anatoly", ""], ["Potapov", "Alexey", ""]]}, {"id": "2006.01031", "submitter": "Valentin Peretroukhin", "authors": "Valentin Peretroukhin, Matthew Giamou, David M. Rosen, W. Nicholas\n  Greene, Nicholas Roy, Jonathan Kelly", "title": "A Smooth Representation of Belief over SO(3) for Deep Rotation Learning\n  with Uncertainty", "comments": "In Proceedings of Robotics: Science and Systems (RSS'20), Corvallis ,\n  Oregon, USA, Jul. 12-16, 2020", "journal-ref": null, "doi": "10.15607/RSS.2020.XVI.007", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate rotation estimation is at the heart of robot perception tasks such\nas visual odometry and object pose estimation. Deep neural networks have\nprovided a new way to perform these tasks, and the choice of rotation\nrepresentation is an important part of network design. In this work, we present\na novel symmetric matrix representation of the 3D rotation group, SO(3), with\ntwo important properties that make it particularly suitable for learned models:\n(1) it satisfies a smoothness property that improves convergence and\ngeneralization when regressing large rotation targets, and (2) it encodes a\nsymmetric Bingham belief over the space of unit quaternions, permitting the\ntraining of uncertainty-aware models. We empirically validate the benefits of\nour formulation by training deep neural rotation regressors on two data\nmodalities. First, we use synthetic point-cloud data to show that our\nrepresentation leads to superior predictive accuracy over existing\nrepresentations for arbitrary rotation targets. Second, we use image data\ncollected onboard ground and aerial vehicles to demonstrate that our\nrepresentation is amenable to an effective out-of-distribution (OOD) rejection\ntechnique that significantly improves the robustness of rotation estimates to\nunseen environmental effects and corrupted input images, without requiring the\nuse of an explicit likelihood loss, stochastic sampling, or an auxiliary\nclassifier. This capability is key for safety-critical applications where\ndetecting novel inputs can prevent catastrophic failure of learned models.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 15:57:45 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 03:38:06 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 16:21:55 GMT"}, {"version": "v4", "created": "Sun, 17 Jan 2021 19:47:56 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Peretroukhin", "Valentin", ""], ["Giamou", "Matthew", ""], ["Rosen", "David M.", ""], ["Greene", "W. Nicholas", ""], ["Roy", "Nicholas", ""], ["Kelly", "Jonathan", ""]]}, {"id": "2006.01047", "submitter": "Lin Gao", "authors": "Shu-Yu Chen, Wanchao Su, Lin Gao, Shihong Xia, Hongbo Fu", "title": "Deep Generation of Face Images from Sketches", "comments": "Accepted to Siggraph 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep image-to-image translation techniques allow fast generation of\nface images from freehand sketches. However, existing solutions tend to overfit\nto sketches, thus requiring professional sketches or even edge maps as input.\nTo address this issue, our key idea is to implicitly model the shape space of\nplausible face images and synthesize a face image in this space to approximate\nan input sketch. We take a local-to-global approach. We first learn feature\nembeddings of key face components, and push corresponding parts of input\nsketches towards underlying component manifolds defined by the feature vectors\nof face component samples. We also propose another deep neural network to learn\nthe mapping from the embedded component features to realistic images with\nmulti-channel feature maps as intermediate results to improve the information\nflow. Our method essentially uses input sketches as soft constraints and is\nthus able to produce high-quality face images even from rough and/or incomplete\nsketches. Our tool is easy to use even for non-artists, while still supporting\nfine-grained control of shape details. Both qualitative and quantitative\nevaluations show the superior generation ability of our system to existing and\nalternative solutions. The usability and expressiveness of our system are\nconfirmed by a user study.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 16:20:23 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 02:37:46 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Chen", "Shu-Yu", ""], ["Su", "Wanchao", ""], ["Gao", "Lin", ""], ["Xia", "Shihong", ""], ["Fu", "Hongbo", ""]]}, {"id": "2006.01053", "submitter": "David Fuentes-Jimenez", "authors": "David Fuentes-Jimenez, Roberto Martin-Lopez, Cristina\n  Losada-Gutierrez, David Casillas-Perez, Javier Macias-Guarasa, Daniel\n  Pizarro, Carlos A.Luna", "title": "DPDnet: A Robust People Detector using Deep Learning with an Overhead\n  Depth Camera", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2019.113168", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose a method based on deep learning that detects\nmultiple people from a single overhead depth image with high reliability. Our\nneural network, called DPDnet, is based on two fully-convolutional\nencoder-decoder neural blocks based on residual layers. The Main Block takes a\ndepth image as input and generates a pixel-wise confidence map, where each\ndetected person in the image is represented by a Gaussian-like distribution.\nThe refinement block combines the depth image and the output from the main\nblock, to refine the confidence map. Both blocks are simultaneously trained\nend-to-end using depth images and head position labels. The experimental work\nshows that DPDNet outperforms state-of-the-art methods, with accuracies greater\nthan 99% in three different publicly available datasets, without retraining not\nfine-tuning. In addition, the computational complexity of our proposal is\nindependent of the number of people in the scene and runs in real time using\nconventional GPUs.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 16:28:25 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Fuentes-Jimenez", "David", ""], ["Martin-Lopez", "Roberto", ""], ["Losada-Gutierrez", "Cristina", ""], ["Casillas-Perez", "David", ""], ["Macias-Guarasa", "Javier", ""], ["Pizarro", "Daniel", ""], ["Luna", "Carlos A.", ""]]}, {"id": "2006.01174", "submitter": "Antonio Pertusa", "authors": "Maria de la Iglesia Vay\\'a, Jose Manuel Saborit, Joaquim Angel\n  Montell, Antonio Pertusa, Aurelia Bustos, Miguel Cazorla, Joaquin Galant,\n  Xavier Barber, Domingo Orozco-Beltr\\'an, Francisco Garc\\'ia-Garc\\'ia, Marisa\n  Caparr\\'os, Germ\\'an Gonz\\'alez and Jose Mar\\'ia Salinas", "title": "BIMCV COVID-19+: a large annotated dataset of RX and CT images from\n  COVID-19 patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes BIMCV COVID-19+, a large dataset from the Valencian\nRegion Medical ImageBank (BIMCV) containing chest X-ray images CXR (CR, DX) and\ncomputed tomography (CT) imaging of COVID-19+ patients along with their\nradiological findings and locations, pathologies, radiological reports (in\nSpanish), DICOM metadata, Polymerase chain reaction (PCR), Immunoglobulin G\n(IgG) and Immunoglobulin M (IgM) diagnostic antibody tests. The findings have\nbeen mapped onto standard Unified Medical Language System (UMLS) terminology\nand cover a wide spectrum of thoracic entities, unlike the considerably more\nreduced number of entities annotated in previous datasets. Images are stored in\nhigh resolution and entities are localized with anatomical labels and stored in\na Medical Imaging Data Structure (MIDS) format. In addition, 10 images were\nannotated by a team of radiologists to include semantic segmentation of\nradiological findings. This first iteration of the database includes 1,380 CX,\n885 DX and 163 CT studies from 1,311 COVID-19+ patients. This is, to the best\nof our knowledge, the largest COVID-19+ dataset of images available in an open\nformat. The dataset can be downloaded from\nhttp://bimcv.cipf.es/bimcv-projects/bimcv-covid19.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 18:06:21 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 12:13:59 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 12:53:43 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Vay\u00e1", "Maria de la Iglesia", ""], ["Saborit", "Jose Manuel", ""], ["Montell", "Joaquim Angel", ""], ["Pertusa", "Antonio", ""], ["Bustos", "Aurelia", ""], ["Cazorla", "Miguel", ""], ["Galant", "Joaquin", ""], ["Barber", "Xavier", ""], ["Orozco-Beltr\u00e1n", "Domingo", ""], ["Garc\u00eda-Garc\u00eda", "Francisco", ""], ["Caparr\u00f3s", "Marisa", ""], ["Gonz\u00e1lez", "Germ\u00e1n", ""], ["Salinas", "Jose Mar\u00eda", ""]]}, {"id": "2006.01201", "submitter": "Mingyuan Meng", "authors": "Mingyuan Meng, Shaojun Liu", "title": "High-quality Panorama Stitching based on Asymmetric Bidirectional\n  Optical Flow", "comments": "Published at the 5th International Conference on Computational\n  Intelligence and Applications (ICCIA 2020)", "journal-ref": "2020 5th International Conference on Computational Intelligence\n  and Applications (ICCIA), Beijing, China, 2020, pp. 118-122", "doi": "10.1109/ICCIA49625.2020.00030", "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a panorama stitching algorithm based on asymmetric\nbidirectional optical flow. This algorithm expects multiple photos captured by\nfisheye lens cameras as input, and then, through the proposed algorithm, these\nphotos can be merged into a high-quality 360-degree spherical panoramic image.\nFor photos taken from a distant perspective, the parallax among them is\nrelatively small, and the obtained panoramic image can be nearly seamless and\nundistorted. For photos taken from a close perspective or with a relatively\nlarge parallax, a seamless though partially distorted panoramic image can also\nbe obtained. Besides, with the help of Graphics Processing Unit (GPU), this\nalgorithm can complete the whole stitching process at a very fast speed:\ntypically, it only takes less than 30s to obtain a panoramic image of\n9000-by-4000 pixels, which means our panorama stitching algorithm is of high\nvalue in many real-time applications. Our code is available at\nhttps://github.com/MungoMeng/Panorama-OpticalFlow.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 18:54:11 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 08:15:12 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2020 00:35:50 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Meng", "Mingyuan", ""], ["Liu", "Shaojun", ""]]}, {"id": "2006.01228", "submitter": "Michael Alexander Beck", "authors": "Michael A. Beck, Chen-Yi Liu, Christopher P. Bidinosti, Christopher J.\n  Henry, Cara M. Godee, Manisha Ajmani", "title": "An embedded system for the automated generation of labeled plant images\n  to enable machine learning applications in agriculture", "comments": "35 pages, 8 figures, Preprint submitted to PLoS One", "journal-ref": null, "doi": "10.1371/journal.pone.0243923", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A lack of sufficient training data, both in terms of variety and quantity, is\noften the bottleneck in the development of machine learning (ML) applications\nin any domain. For agricultural applications, ML-based models designed to\nperform tasks such as autonomous plant classification will typically be coupled\nto just one or perhaps a few plant species. As a consequence, each\ncrop-specific task is very likely to require its own specialized training data,\nand the question of how to serve this need for data now often overshadows the\nmore routine exercise of actually training such models. To tackle this problem,\nwe have developed an embedded robotic system to automatically generate and\nlabel large datasets of plant images for ML applications in agriculture. The\nsystem can image plants from virtually any angle, thereby ensuring a wide\nvariety of data; and with an imaging rate of up to one image per second, it can\nproduce lableled datasets on the scale of thousands to tens of thousands of\nimages per day. As such, this system offers an important alternative to time-\nand cost-intensive methods of manual generation and labeling. Furthermore, the\nuse of a uniform background made of blue keying fabric enables additional image\nprocessing techniques such as background replacement and plant segmentation. It\nalso helps in the training process, essentially forcing the model to focus on\nthe plant features and eliminating random correlations. To demonstrate the\ncapabilities of our system, we generated a dataset of over 34,000 labeled\nimages, with which we trained an ML-model to distinguish grasses from\nnon-grasses in test data from a variety of sources. We now plan to generate\nmuch larger datasets of Canadian crop plants and weeds that will be made\npublicly available in the hope of further enabling ML applications in the\nagriculture sector.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 20:01:20 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 19:50:14 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Beck", "Michael A.", ""], ["Liu", "Chen-Yi", ""], ["Bidinosti", "Christopher P.", ""], ["Henry", "Christopher J.", ""], ["Godee", "Cara M.", ""], ["Ajmani", "Manisha", ""]]}, {"id": "2006.01232", "submitter": "Albara Ramli", "authors": "Albara Ah Ramli, Rex Liu, Rahul Krishnamoorthy, Vishal I B, Xiaoxiao\n  Wang, Ilias Tagkopoulos, and Xin Liu", "title": "BWCNN: Blink to Word, a Real-Time Convolutional Neural Network Approach", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": "10.1007/978-3-030-59615-6_10", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amyotrophic lateral sclerosis (ALS) is a progressive neurodegenerative\ndisease of the brain and the spinal cord, which leads to paralysis of motor\nfunctions. Patients retain their ability to blink, which can be used for\ncommunication. Here, We present an Artificial Intelligence (AI) system that\nuses eye-blinks to communicate with the outside world, running on real-time\nInternet-of-Things (IoT) devices. The system uses a Convolutional Neural\nNetwork (CNN) to find the blinking pattern, which is defined as a series of\nOpen and Closed states. Each pattern is mapped to a collection of words that\nmanifest the patient's intent. To investigate the best trade-off between\naccuracy and latency, we investigated several Convolutional Network\narchitectures, such as ResNet, SqueezeNet, DenseNet, and InceptionV3, and\nevaluated their performance. We found that the InceptionV3 architecture, after\nhyper-parameter fine-tuning on the specific task led to the best performance\nwith an accuracy of 99.20% and 94ms latency. This work demonstrates how the\nlatest advances in deep learning architectures can be adapted for clinical\nsystems that ameliorate the patient's quality of life regardless of the\npoint-of-care.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 20:07:44 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ramli", "Albara Ah", ""], ["Liu", "Rex", ""], ["Krishnamoorthy", "Rahul", ""], ["B", "Vishal I", ""], ["Wang", "Xiaoxiao", ""], ["Tagkopoulos", "Ilias", ""], ["Liu", "Xin", ""]]}, {"id": "2006.01250", "submitter": "Abhinav Sagar", "authors": "Abhinav Sagar", "title": "RUHSNet: 3D Object Detection Using Lidar Data in Real Time", "comments": "The results in this paper is not correct as assumptions used while\n  designing the network was found to be wrong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of 3D object detection from point cloud\ndata in real time. For autonomous vehicles to work, it is very important for\nthe perception component to detect the real world objects with both high\naccuracy and fast inference. We propose a novel neural network architecture\nalong with the training and optimization details for detecting 3D objects in\npoint cloud data. We compare the results with different backbone architectures\nincluding the standard ones like VGG, ResNet, Inception with our backbone. Also\nwe present the optimization and ablation studies including designing an\nefficient anchor. We use the Kitti 3D Birds Eye View dataset for benchmarking\nand validating our results. Our work surpasses the state of the art in this\ndomain both in terms of average precision and speed running at > 30 FPS. This\nmakes it a feasible option to be deployed in real time applications including\nself driving cars.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 09:41:46 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 12:52:40 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 09:40:26 GMT"}, {"version": "v4", "created": "Wed, 12 Aug 2020 20:01:43 GMT"}, {"version": "v5", "created": "Tue, 15 Sep 2020 13:26:09 GMT"}, {"version": "v6", "created": "Mon, 21 Jun 2021 18:21:58 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Sagar", "Abhinav", ""]]}, {"id": "2006.01263", "submitter": "Shruti Jadon", "authors": "Shruti Jadon, Owen P. Leary, Ian Pan, Tyler J. Harder, David W.\n  Wright, Lisa H. Merck, Derek L. Merck", "title": "A comparative study of 2D image segmentation algorithms for traumatic\n  brain lesions using CT data from the ProTECTIII multicenter clinical trial", "comments": "9 pages, 3 figures, 3 tables", "journal-ref": "SPIE MEDICAL IMAGING 2020", "doi": "10.1117/12.2566332", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated segmentation of medical imaging is of broad interest to clinicians\nand machine learning researchers alike. The goal of segmentation is to increase\nefficiency and simplicity of visualization and quantification of regions of\ninterest within a medical image. Image segmentation is a difficult task because\nof multiparametric heterogeneity within the images, an obstacle that has proven\nespecially challenging in efforts to automate the segmentation of brain lesions\nfrom non-contrast head computed tomography (CT). In this research, we have\nexperimented with multiple available deep learning architectures to segment\ndifferent phenotypes of hemorrhagic lesions found after moderate to severe\ntraumatic brain injury (TBI). These include: intraparenchymal hemorrhage (IPH),\nsubdural hematoma (SDH), epidural hematoma (EDH), and traumatic contusions. We\nwere able to achieve an optimal Dice Coefficient1 score of 0.94 using UNet++ 2D\nArchitecture with Focal Tversky Loss Function, an increase from 0.85 using UNet\n2D with Binary Cross-Entropy Loss Function in intraparenchymal hemorrhage (IPH)\ncases. Furthermore, using the same setting, we were able to achieve the Dice\nCoefficient score of 0.90 and 0.86 in cases of Extra-Axial bleeds and Traumatic\ncontusions, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 21:00:20 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Jadon", "Shruti", ""], ["Leary", "Owen P.", ""], ["Pan", "Ian", ""], ["Harder", "Tyler J.", ""], ["Wright", "David W.", ""], ["Merck", "Lisa H.", ""], ["Merck", "Derek L.", ""]]}, {"id": "2006.01286", "submitter": "Corneliu Arsene Dr", "authors": "Corneliu Arsene", "title": "Fusion of Real Time Thermal Image and 1D/2D/3D Depth Laser Readings for\n  Remote Thermal Sensing in Industrial Plants by Means of UAVs and/or Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents fast procedures for thermal infrared remote sensing in\ndark, GPS-denied environments, such as those found in industrial plants such as\nin High-Voltage Direct Current (HVDC) converter stations. These procedures are\nbased on the combination of the depth estimation obtained from either a\n1-Dimensional LIDAR laser or a 2-Dimensional Hokuyo laser or a 3D MultiSense\nSLB laser sensor and the visible and thermal cameras from a FLIR Duo R\ndual-sensor thermal camera. The combination of these sensors/cameras is\nsuitable to be mounted on Unmanned Aerial Vehicles (UAVs) and/or robots in\norder to provide reliable information about the potential malfunctions, which\ncan be found within the hazardous environment. For example, the capabilities of\nthe developed software and hardware system corresponding to the combination of\nthe 1-D LIDAR sensor and the FLIR Duo R dual-sensor thermal camera is assessed\nfrom the point of the accuracy of results and the required computational times:\nthe obtained computational times are under 10 ms, with a maximum localization\nerror of 8 mm and an average standard deviation for the measured temperatures\nof 1.11 degree Celsius, which results are obtained for a number of test cases.\nThe paper is structured as follows: the description of the system used for\nidentification and localization of hotspots in industrial plants is presented\nin section II. In section III, the method for faults identification and\nlocalization in plants by using a 1-Dimensional LIDAR laser sensor and thermal\nimages is described together with results. In section IV the real time thermal\nimage processing is presented. Fusion of the 2-Dimensional depth laser Hokuyo\nand the thermal images is described in section V. In section VI the combination\nof the 3D MultiSense SLB laser and thermal images is described. In section VII\na discussion and several conclusions are drawn.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 21:52:39 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 12:58:14 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2020 10:22:23 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Arsene", "Corneliu", ""]]}, {"id": "2006.01310", "submitter": "Antonio Ricardo Alexandre Brasil", "authors": "Antonio Ricardo Alexandre Brasil and Jefferson Oliveira Andrade and\n  Karin Satie Komati", "title": "Eye Movements Biometrics: A Bibliometric Analysis from 2004 to 2019", "comments": "9 pages, 2 figures, journal", "journal-ref": "International Journal of Computer Applications 176(24):1-9, May\n  2020", "doi": "10.5120/ijca2020920243", "report-no": null, "categories": "cs.HC cs.CV cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person identification based on eye movements is getting more and more\nattention, as it is anti-spoofing resistant and can be useful for continuous\nauthentication. Therefore, it is noteworthy for researchers to know who and\nwhat is relevant in the field, including authors, journals, conferences, and\ninstitutions. This paper presents a comprehensive quantitative overview of the\nfield of eye movement biometrics using a bibliometric approach. All data and\nanalyses are based on documents written in English published between 2004 and\n2019. Scopus was used to perform information retrieval. This research focused\non temporal evolution, leading authors, most cited papers, leading journals,\ncompetitions and collaboration networks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 23:14:10 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Brasil", "Antonio Ricardo Alexandre", ""], ["Andrade", "Jefferson Oliveira", ""], ["Komati", "Karin Satie", ""]]}, {"id": "2006.01315", "submitter": "Oualid Laiadi", "authors": "Oualid Laiadi and Abdelmalik Ouamane and Abdelhamid Benakcha and\n  Abdelmalik Taleb-Ahmed and Abdenour Hadid", "title": "Multi-view Deep Features for Robust Facial Kinship Verification", "comments": "Will appear as part of RFIW2020 in the Proceedings of 2020\n  International Conference on Automatic Face and Gesture Recognition (IEEE\n  AMFG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic kinship verification from facial images is an emerging research\ntopic in machine learning community. In this paper, we proposed an effective\nfacial features extraction model based on multi-view deep features. Thus, we\nused four pre-trained deep learning models using eight features layers (FC6 and\nFC7 layers of each VGG-F, VGG-M, VGG-S and VGG-Face models) to train the\nproposed Multilinear Side-Information based Discriminant Analysis integrating\nWithin Class Covariance Normalization (MSIDA+WCCN) method. Furthermore, we show\nthat how can metric learning methods based on WCCN method integration improves\nthe Simple Scoring Cosine similarity (SSC) method. We refer that we used the\nSSC method in RFIW'20 competition using the eight deep features concatenation.\nThus, the integration of WCCN in the metric learning methods decreases the\nintra-class variations effect introduced by the deep features weights. We\nevaluate our proposed method on two kinship benchmarks namely KinFaceW-I and\nKinFaceW-II databases using four Parent-Child relations (Father-Son,\nFather-Daughter, Mother-Son and Mother-Daughter). Thus, the proposed MSIDA+WCCN\nmethod improves the SSC method with 12.80% and 14.65% on KinFaceW-I and\nKinFaceW-II databases, respectively. The results obtained are positively\ncompared with some modern methods, including those that rely on deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 23:33:18 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Laiadi", "Oualid", ""], ["Ouamane", "Abdelmalik", ""], ["Benakcha", "Abdelhamid", ""], ["Taleb-Ahmed", "Abdelmalik", ""], ["Hadid", "Abdenour", ""]]}, {"id": "2006.01320", "submitter": "Fanqing Lin", "authors": "Fanqing Lin, Connor Wilhelm, Tony Martinez", "title": "Two-hand Global 3D Pose Estimation Using Monocular RGB", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the challenging task of estimating global 3D joint locations for\nboth hands via only monocular RGB input images. We propose a novel multi-stage\nconvolutional neural network based pipeline that accurately segments and\nlocates the hands despite occlusion between two hands and complex background\nnoise and estimates the 2D and 3D canonical joint locations without any depth\ninformation. Global joint locations with respect to the camera origin are\ncomputed using the hand pose estimations and the actual length of the key bone\nwith a novel projection algorithm. To train the CNNs for this new task, we\nintroduce a large-scale synthetic 3D hand pose dataset. We demonstrate that our\nsystem outperforms previous works on 3D canonical hand pose estimation\nbenchmark datasets with RGB-only information. Additionally, we present the\nfirst work that achieves accurate global 3D hand tracking on both hands using\nRGB-only inputs and provide extensive quantitative and qualitative evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 23:53:52 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 08:58:26 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 01:03:26 GMT"}, {"version": "v4", "created": "Tue, 25 Aug 2020 09:54:24 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Lin", "Fanqing", ""], ["Wilhelm", "Connor", ""], ["Martinez", "Tony", ""]]}, {"id": "2006.01367", "submitter": "Jiabao Wang", "authors": "Jiabao Wang, Yang Li, Yangshuo Zhang, Zhuang Miao, Rui Zhang", "title": "A heterogeneous branch and multi-level classification network for person\n  re-identification", "comments": null, "journal-ref": "Neurocomputing 404(2020)61-69", "doi": "10.1016/j.neucom.2020.05.007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks with multiple branches have recently been\nproved highly effective in person re-identification (re-ID). Researchers design\nmulti-branch networks using part models, yet they always attribute the\neffectiveness to multiple parts. In addition, existing multi-branch networks\nalways have isomorphic branches, which lack structural diversity. In order to\nimprove this problem, we propose a novel Heterogeneous Branch and Multi-level\nClassification Network (HBMCN), which is designed based on the pre-trained\nResNet-50 model. A new heterogeneous branch, SE-Res-Branch, is proposed based\non the SE-Res module, which consists of the Squeeze-and-Excitation block and\nthe residual block. Furthermore, a new multi-level classification joint\nobjective function is proposed for the supervised learning of HBMCN, whereby\nmulti-level features are extracted from multiple high-level layers and\nconcatenated to represent a person. Based on three public person re-ID\nbenchmarks (Market1501, DukeMTMC-reID and CUHK03), experimental results show\nthat the proposed HBMCN reaches 94.4%, 85.7% and 73.8% in Rank-1, and 85.7%,\n74.6% and 69.0% in mAP, achieving a state-of-the-art performance. Further\nanalysis demonstrates that the specially designed heterogeneous branch performs\nbetter than an isomorphic branch, and multi-level classification provides more\ndiscriminative features compared to single-level classification. As a result,\nHBMCN provides substantial further improvements in person re-ID tasks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 03:34:50 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Wang", "Jiabao", ""], ["Li", "Yang", ""], ["Zhang", "Yangshuo", ""], ["Miao", "Zhuang", ""], ["Zhang", "Rui", ""]]}, {"id": "2006.01385", "submitter": "Tianming Du", "authors": "Tianming Du, Honggang Zhang, Yuemeng Li, Hee Kwon Song, Yong Fan", "title": "Adaptive convolutional neural networks for k-space data interpolation in\n  fast magnetic resonance imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning in k-space has demonstrated great potential for image\nreconstruction from undersampled k-space data in fast magnetic resonance\nimaging (MRI). However, existing deep learning-based image reconstruction\nmethods typically apply weight-sharing convolutional neural networks (CNNs) to\nk-space data without taking into consideration the k-space data's spatial\nfrequency properties, leading to ineffective learning of the image\nreconstruction models. Moreover, complementary information of spatially\nadjacent slices is often ignored in existing deep learning methods. To overcome\nsuch limitations, we develop a deep learning algorithm, referred to as adaptive\nconvolutional neural networks for k-space data interpolation (ACNN-k-Space),\nwhich adopts a residual Encoder-Decoder network architecture to interpolate the\nundersampled k-space data by integrating spatially contiguous slices as\nmulti-channel input, along with k-space data from multiple coils if available.\nThe network is enhanced by self-attention layers to adaptively focus on k-space\ndata at different spatial frequencies and channels. We have evaluated our\nmethod on two public datasets and compared it with state-of-the-art existing\nmethods. Ablation studies and experimental results demonstrate that our method\neffectively reconstructs images from undersampled k-space data and achieves\nsignificantly better image reconstruction performance than current\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 04:29:33 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 18:15:47 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Du", "Tianming", ""], ["Zhang", "Honggang", ""], ["Li", "Yuemeng", ""], ["Song", "Hee Kwon", ""], ["Fan", "Yong", ""]]}, {"id": "2006.01408", "submitter": "Jay Paranjape", "authors": "Jay N. Paranjape, Rahul Kumar Dubey, Vijendran V Gopalan", "title": "Exploring the role of Input and Output Layers of a Deep Neural Network\n  in Adversarial Defense", "comments": "5 pages, 7 figures, to be presented at CONF-CDS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks are learning models having achieved state of the art\nperformance in many fields like prediction, computer vision, language\nprocessing and so on. However, it has been shown that certain inputs exist\nwhich would not trick a human normally, but may mislead the model completely.\nThese inputs are known as adversarial inputs. These inputs pose a high security\nthreat when such models are used in real world applications. In this work, we\nhave analyzed the resistance of three different classes of fully connected\ndense networks against the rarely tested non-gradient based adversarial\nattacks. These classes are created by manipulating the input and output layers.\nWe have proven empirically that owing to certain characteristics of the\nnetwork, they provide a high robustness against these attacks, and can be used\nin fine tuning other models to increase defense against adversarial attacks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 06:15:46 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Paranjape", "Jay N.", ""], ["Dubey", "Rahul Kumar", ""], ["Gopalan", "Vijendran V", ""]]}, {"id": "2006.01409", "submitter": "Siham Tabik", "authors": "S. Tabik, A. G\\'omez-R\\'ios, J.L. Mart\\'in-Rodr\\'iguez, I.\n  Sevillano-Garc\\'ia, M. Rey-Area, D. Charte, E. Guirado, J.L. Su\\'arez, J.\n  Luengo, M.A. Valero-Gonz\\'alez, P. Garc\\'ia-Villanova, E. Olmedo-S\\'anchez,\n  F. Herrera", "title": "COVIDGR dataset and COVID-SDNet methodology for predicting COVID-19\n  based on Chest X-Ray images", "comments": "Paper accepted in Journal of Biomedical And Health Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, Coronavirus disease (COVID-19), one of the most infectious\ndiseases in the 21st century, is diagnosed using RT-PCR testing, CT scans\nand/or Chest X-Ray (CXR) images. CT (Computed Tomography) scanners and RT-PCR\ntesting are not available in most medical centers and hence in many cases CXR\nimages become the most time/cost effective tool for assisting clinicians in\nmaking decisions. Deep learning neural networks have a great potential for\nbuilding COVID-19 triage systems and detecting COVID-19 patients, especially\npatients with low severity. Unfortunately, current databases do not allow\nbuilding such systems as they are highly heterogeneous and biased towards\nsevere cases. This paper is three-fold: (i) we demystify the high sensitivities\nachieved by most recent COVID-19 classification models, (ii) under a close\ncollaboration with Hospital Universitario Cl\\'inico San Cecilio, Granada,\nSpain, we built COVIDGR-1.0, a homogeneous and balanced database that includes\nall levels of severity, from normal with Positive RT-PCR, Mild, Moderate to\nSevere. COVIDGR-1.0 contains 426 positive and 426 negative PA (PosteroAnterior)\nCXR views and (iii) we propose COVID Smart Data based Network (COVID-SDNet)\nmethodology for improving the generalization capacity of COVID-classification\nmodels. Our approach reaches good and stable results with an accuracy of\n$97.72\\% \\pm 0.95 \\%$, $86.90\\% \\pm 3.20\\%$, $61.80\\% \\pm 5.49\\%$ in severe,\nmoderate and mild COVID-19 severity levels (Paper accepted for publication in\nJournal of Biomedical and Health Informatics). Our approach could help in the\nearly detection of COVID-19. COVIDGR-1.0 along with the severity level labels\nare available to the scientific community through this link\nhttps://dasci.es/es/transferencia/open-data/covidgr/.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 06:18:34 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 11:13:17 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 06:15:00 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Tabik", "S.", ""], ["G\u00f3mez-R\u00edos", "A.", ""], ["Mart\u00edn-Rodr\u00edguez", "J. L.", ""], ["Sevillano-Garc\u00eda", "I.", ""], ["Rey-Area", "M.", ""], ["Charte", "D.", ""], ["Guirado", "E.", ""], ["Su\u00e1rez", "J. L.", ""], ["Luengo", "J.", ""], ["Valero-Gonz\u00e1lez", "M. A.", ""], ["Garc\u00eda-Villanova", "P.", ""], ["Olmedo-S\u00e1nchez", "E.", ""], ["Herrera", "F.", ""]]}, {"id": "2006.01410", "submitter": "Yen-Ting Liu", "authors": "Yen-Ting Liu and Yu-Jhe Li and Yu-Chiang Frank Wang", "title": "Transforming Multi-Concept Attention into Video Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization is among challenging tasks in computer vision, which aims\nat identifying highlight frames or shots over a lengthy video input. In this\npaper, we propose an novel attention-based framework for video summarization\nwith complex video data. Unlike previous works which only apply attention\nmechanism on the correspondence between frames, our multi-concept video\nself-attention (MC-VSA) model is presented to identify informative regions\nacross temporal and concept video features, which jointly exploit context\ndiversity over time and space for summarization purposes. Together with\nconsistency between video and summary enforced in our framework, our model can\nbe applied to both labeled and unlabeled data, making our method preferable to\nreal-world applications. Extensive and complete experiments on two benchmarks\ndemonstrate the effectiveness of our model both quantitatively and\nqualitatively, and confirms its superiority over the stateof-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 06:23:50 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 03:30:07 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Liu", "Yen-Ting", ""], ["Li", "Yu-Jhe", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "2006.01413", "submitter": "Trong Huy Phan", "authors": "Trong Huy Phan, Kazuma Yamamoto", "title": "Resolving Class Imbalance in Object Detection with Weighted Cross\n  Entropy Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is an important task in computer vision which serves a lot\nof real-world applications such as autonomous driving, surveillance and\nrobotics. Along with the rapid thrive of large-scale data, numerous\nstate-of-the-art generalized object detectors (e.g. Faster R-CNN, YOLO, SSD)\nwere developed in the past decade. Despite continual efforts in model\nmodification and improvement in training strategies to boost detection\naccuracy, there are still limitations in performance of detectors when it comes\nto specialized datasets with uneven object class distributions. This originates\nfrom the common usage of Cross Entropy loss function for object classification\nsub-task that simply ignores the frequency of appearance of object class during\ntraining, and thus results in lower accuracies for object classes with fewer\nnumber of samples. Class-imbalance in general machine learning has been widely\nstudied, however, little attention has been paid on the subject of object\ndetection. In this paper, we propose to explore and overcome such problem by\napplication of several weighted variants of Cross Entropy loss, for examples\nBalanced Cross Entropy, Focal Loss and Class-Balanced Loss Based on Effective\nNumber of Samples to our object detector. Experiments with BDD100K (a highly\nclass-imbalanced driving database acquired from on-vehicle cameras capturing\nmostly Car-class objects and other minority object classes such as Bus, Person\nand Motor) have proven better class-wise performances of detector trained with\nthe afore-mentioned loss functions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 06:36:12 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Phan", "Trong Huy", ""], ["Yamamoto", "Kazuma", ""]]}, {"id": "2006.01423", "submitter": "Yucheng Chen", "authors": "Yucheng Chen, Yingli Tian and Mingyi He", "title": "Monocular Human Pose Estimation: A Survey of Deep Learning-based Methods", "comments": "This version corresponds to the pre-print of the paper accepted for\n  Computer Vision and Image Understanding (CVIU)", "journal-ref": "Computer Vision and Image Understanding (CVIU) 192 (2020) 102897", "doi": "10.1016/j.cviu.2019.102897", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based monocular human pose estimation, as one of the most fundamental\nand challenging problems in computer vision, aims to obtain posture of the\nhuman body from input images or video sequences. The recent developments of\ndeep learning techniques have been brought significant progress and remarkable\nbreakthroughs in the field of human pose estimation. This survey extensively\nreviews the recent deep learning-based 2D and 3D human pose estimation methods\npublished since 2014. This paper summarizes the challenges, main frameworks,\nbenchmark datasets, evaluation metrics, performance comparison, and discusses\nsome promising future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 07:07:45 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Chen", "Yucheng", ""], ["Tian", "Yingli", ""], ["He", "Mingyi", ""]]}, {"id": "2006.01424", "submitter": "Yiqun Mei", "authors": "Yiqun Mei, Yuchen Fan, Yuqian Zhou, Lichao Huang, Thomas S. Huang,\n  Humphrey Shi", "title": "Image Super-Resolution with Cross-Scale Non-Local Attention and\n  Exhaustive Self-Exemplars Mining", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolution-based single image super-resolution (SISR) networks embrace\nthe benefits of learning from large-scale external image resources for local\nrecovery, yet most existing works have ignored the long-range feature-wise\nsimilarities in natural images. Some recent works have successfully leveraged\nthis intrinsic feature correlation by exploring non-local attention modules.\nHowever, none of the current deep models have studied another inherent property\nof images: cross-scale feature correlation. In this paper, we propose the first\nCross-Scale Non-Local (CS-NL) attention module with integration into a\nrecurrent neural network. By combining the new CS-NL prior with local and\nin-scale non-local priors in a powerful recurrent fusion cell, we can find more\ncross-scale feature correlations within a single low-resolution (LR) image. The\nperformance of SISR is significantly improved by exhaustively integrating all\npossible priors. Extensive experiments demonstrate the effectiveness of the\nproposed CS-NL module by setting new state-of-the-arts on multiple SISR\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 07:08:58 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Mei", "Yiqun", ""], ["Fan", "Yuchen", ""], ["Zhou", "Yuqian", ""], ["Huang", "Lichao", ""], ["Huang", "Thomas S.", ""], ["Shi", "Humphrey", ""]]}, {"id": "2006.01431", "submitter": "Minxuan Lin", "authors": "Minxuan Lin, Fan Tang, Weiming Dong, Xiao Li, Chongyang Ma, Changsheng\n  Xu", "title": "Distribution Aligned Multimodal and Multi-Domain Image Stylization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal and multi-domain stylization are two important problems in the\nfield of image style transfer. Currently, there are few methods that can\nperform both multimodal and multi-domain stylization simultaneously. In this\npaper, we propose a unified framework for multimodal and multi-domain style\ntransfer with the support of both exemplar-based reference and randomly sampled\nguidance. The key component of our method is a novel style distribution\nalignment module that eliminates the explicit distribution gaps between various\nstyle domains and reduces the risk of mode collapse. The multimodal diversity\nis ensured by either guidance from multiple images or random style code, while\nthe multi-domain controllability is directly achieved by using a domain label.\nWe validate our proposed framework on painting style transfer with a variety of\ndifferent artistic styles and genres. Qualitative and quantitative comparisons\nwith state-of-the-art methods demonstrate that our method can generate\nhigh-quality results of multi-domain styles and multimodal instances with\nreference style guidance or random sampled style.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 07:25:53 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Lin", "Minxuan", ""], ["Tang", "Fan", ""], ["Dong", "Weiming", ""], ["Li", "Xiao", ""], ["Ma", "Chongyang", ""], ["Xu", "Changsheng", ""]]}, {"id": "2006.01435", "submitter": "Chen Gao", "authors": "Chen Gao, Si Liu, Ran He, Shuicheng Yan, Bo Li", "title": "Recapture as You Want", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing prevalence and more powerful camera systems of mobile\ndevices, people can conveniently take photos in their daily life, which\nnaturally brings the demand for more intelligent photo post-processing\ntechniques, especially on those portrait photos. In this paper, we present a\nportrait recapture method enabling users to easily edit their portrait to\ndesired posture/view, body figure and clothing style, which are very\nchallenging to achieve since it requires to simultaneously perform non-rigid\ndeformation of human body, invisible body-parts reasoning and semantic-aware\nediting. We decompose the editing procedure into semantic-aware geometric and\nappearance transformation. In geometric transformation, a semantic layout map\nis generated that meets user demands to represent part-level spatial\nconstraints and further guides the semantic-aware appearance transformation. In\nappearance transformation, we design two novel modules, Semantic-aware\nAttentive Transfer (SAT) and Layout Graph Reasoning (LGR), to conduct\nintra-part transfer and inter-part reasoning, respectively. SAT module produces\neach human part by paying attention to the semantically consistent regions in\nthe source portrait. It effectively addresses the non-rigid deformation issue\nand well preserves the intrinsic structure/appearance with rich texture\ndetails. LGR module utilizes body skeleton knowledge to construct a layout\ngraph that connects all relevant part features, where graph reasoning mechanism\nis used to propagate information among part nodes to mine their relations. In\nthis way, LGR module infers invisible body parts and guarantees global\ncoherence among all the parts. Extensive experiments on DeepFashion,\nMarket-1501 and in-the-wild photos demonstrate the effectiveness and\nsuperiority of our approach. Video demo is at:\n\\url{https://youtu.be/vTyq9HL6jgw}.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 07:43:53 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Gao", "Chen", ""], ["Liu", "Si", ""], ["He", "Ran", ""], ["Yan", "Shuicheng", ""], ["Li", "Bo", ""]]}, {"id": "2006.01438", "submitter": "Andres Asensio Ramos", "authors": "A. Asensio Ramos (IAC, ULL), N. Olspert (MPS)", "title": "Learning to do multiframe wavefront sensing unsupervisedly: applications\n  to blind deconvolution", "comments": "11 pages, 4 figures, accepted for publication in A&A", "journal-ref": "A&A 646, A100 (2021)", "doi": "10.1051/0004-6361/202038552", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observations from ground based telescopes are affected by the presence of the\nEarth atmosphere, which severely perturbs them. The use of adaptive optics\ntechniques has allowed us to partly beat this limitation. However, image\nselection or post-facto image reconstruction methods applied to bursts of\nshort-exposure images are routinely needed to reach the diffraction limit. Deep\nlearning has been recently proposed as an efficient way to accelerate these\nimage reconstructions. Currently, these deep neural networks are trained with\nsupervision, so that either standard deconvolution algorithms need to be\napplied a-priori or complex simulations of the solar magneto-convection need to\nbe carried out to generate the training sets. Our aim here is to propose a\ngeneral unsupervised training scheme that allows multiframe blind deconvolution\ndeep learning systems to be trained simply with observations. The approach can\nbe applied for the correction of point-like as well as extended objects.\nLeveraging the linear image formation theory and a probabilistic approach to\nthe blind deconvolution problem produces a physically-motivated loss function.\nThe optimization of this loss function allows an end-to-end training of a\nmachine learning model composed of three neural networks. As examples, we apply\nthis procedure to the deconvolution of stellar data from the FastCam instrument\nand to solar extended data from the Swedish Solar Telescope. The analysis\ndemonstrates that the proposed neural model can be successfully trained without\nsupervision using observations only. It provides estimations of the\ninstantaneous wavefronts, from which a corrected image can be found using\nstandard deconvolution technniques. The network model is roughly three orders\nof magnitude faster than applying standard deconvolution based on optimization\nand shows potential to be used on real-time at the telescope.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 08:02:12 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 11:33:48 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Ramos", "A. Asensio", "", "IAC, ULL"], ["Olspert", "N.", "", "MPS"]]}, {"id": "2006.01441", "submitter": "Mikhail Belyaev", "authors": "Mikhail Goncharov, Maxim Pisov, Alexey Shevtsov, Boris Shirokikh,\n  Anvar Kurmukov, Ivan Blokhin, Valeria Chernina, Alexander Solovev, Victor\n  Gombolevskiy, Sergey Morozov, Mikhail Belyaev", "title": "CT-based COVID-19 Triage: Deep Multitask Learning Improves Joint\n  Identification and Severity Quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current COVID-19 pandemic overloads healthcare systems, including\nradiology departments. Though several deep learning approaches were developed\nto assist in CT analysis, nobody considered study triage directly as a computer\nscience problem. We describe two basic setups: Identification of COVID-19 to\nprioritize studies of potentially infected patients to isolate them as early as\npossible; Severity quantification to highlight studies of severe patients and\ndirect them to a hospital or provide emergency medical care. We formalize these\ntasks as binary classification and estimation of affected lung percentage.\nThough similar problems were well-studied separately, we show that existing\nmethods provide reasonable quality only for one of these setups. We employ a\nmultitask approach to consolidate both triage approaches and propose a\nconvolutional neural network to combine all available labels within a single\nmodel. In contrast with the most popular multitask approaches, we add\nclassification layers to the most spatially detailed upper part of U-Net\ninstead of the bottom, less detailed latent representation. We train our model\non approximately 2000 publicly available CT studies and test it with a\ncarefully designed set consisting of 32 COVID-19 studies, 30 cases with\nbacterial pneumonia, 31 healthy patients, and 30 patients with other lung\npathologies to emulate a typical patient flow in an out-patient hospital. The\nproposed multitask model outperforms the latent-based one and achieves ROC AUC\nscores ranging from 0.87+-01 (bacterial pneumonia) to 0.97+-01 (healthy\ncontrols) for Identification of COVID-19 and 0.97+-01 Spearman Correlation for\nSeverity quantification. We release all the code and create a public\nleaderboard, where other community members can test their models on our test\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 08:05:06 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 14:09:54 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2020 05:32:20 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Goncharov", "Mikhail", ""], ["Pisov", "Maxim", ""], ["Shevtsov", "Alexey", ""], ["Shirokikh", "Boris", ""], ["Kurmukov", "Anvar", ""], ["Blokhin", "Ivan", ""], ["Chernina", "Valeria", ""], ["Solovev", "Alexander", ""], ["Gombolevskiy", "Victor", ""], ["Morozov", "Sergey", ""], ["Belyaev", "Mikhail", ""]]}, {"id": "2006.01456", "submitter": "Utku Ozbulak", "authors": "Utku Ozbulak, Manvel Gasparyan, Wesley De Neve, Arnout Van Messem", "title": "Perturbation Analysis of Gradient-based Adversarial Attacks", "comments": "Accepted for publication in Pattern Recognition Letters, 2020", "journal-ref": "Pattern Recognition Letters 2020, Volume 135, Pages 133-120", "doi": "10.1016/j.patrec.2020.04.034", "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the discovery of adversarial examples and their adverse effects on deep\nlearning models, many studies focused on finding more diverse methods to\ngenerate these carefully crafted samples. Although empirical results on the\neffectiveness of adversarial example generation methods against defense\nmechanisms are discussed in detail in the literature, an in-depth study of the\ntheoretical properties and the perturbation effectiveness of these adversarial\nattacks has largely been lacking. In this paper, we investigate the objective\nfunctions of three popular methods for adversarial example generation: the\nL-BFGS attack, the Iterative Fast Gradient Sign attack, and Carlini & Wagner's\nattack (CW). Specifically, we perform a comparative and formal analysis of the\nloss functions underlying the aforementioned attacks while laying out\nlarge-scale experimental results on ImageNet dataset. This analysis exposes (1)\nthe faster optimization speed as well as the constrained optimization space of\nthe cross-entropy loss, (2) the detrimental effects of using the signature of\nthe cross-entropy loss on optimization precision as well as optimization space,\nand (3) the slow optimization speed of the logit loss in the context of\nadversariality. Our experiments reveal that the Iterative Fast Gradient Sign\nattack, which is thought to be fast for generating adversarial examples, is the\nworst attack in terms of the number of iterations required to create\nadversarial examples in the setting of equal perturbation. Moreover, our\nexperiments show that the underlying loss function of CW, which is criticized\nfor being substantially slower than other adversarial attacks, is not that much\nslower than other loss functions. Finally, we analyze how well neural networks\ncan identify adversarial perturbations generated by the attacks under\nconsideration, hereby revisiting the idea of adversarial retraining on\nImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 08:51:37 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Ozbulak", "Utku", ""], ["Gasparyan", "Manvel", ""], ["De Neve", "Wesley", ""], ["Van Messem", "Arnout", ""]]}, {"id": "2006.01469", "submitter": "Xibin Song", "authors": "Xibin Song, Yuchao Dai, Dingfu Zhou, Liu Liu, Wei Li, Hongdng Li,\n  Ruigang Yang", "title": "Channel Attention based Iterative Residual Learning for Depth Map\n  Super-Resolution", "comments": "accepted by Conference on Computer Vision and Pattern Recognition\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable progresses made in deep-learning based depth map\nsuper-resolution (DSR), how to tackle real-world degradation in low-resolution\n(LR) depth maps remains a major challenge. Existing DSR model is generally\ntrained and tested on synthetic dataset, which is very different from what\nwould get from a real depth sensor. In this paper, we argue that DSR models\ntrained under this setting are restrictive and not effective in dealing with\nreal-world DSR tasks. We make two contributions in tackling real-world\ndegradation of different depth sensors. First, we propose to classify the\ngeneration of LR depth maps into two types: non-linear downsampling with noise\nand interval downsampling, for which DSR models are learned correspondingly.\nSecond, we propose a new framework for real-world DSR, which consists of four\nmodules : 1) An iterative residual learning module with deep supervision to\nlearn effective high-frequency components of depth maps in a coarse-to-fine\nmanner; 2) A channel attention strategy to enhance channels with abundant\nhigh-frequency components; 3) A multi-stage fusion module to effectively\nre-exploit the results in the coarse-to-fine process; and 4) A depth refinement\nmodule to improve the depth map by TGV regularization and input loss. Extensive\nexperiments on benchmarking datasets demonstrate the superiority of our method\nover current state-of-the-art DSR methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 09:12:23 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Song", "Xibin", ""], ["Dai", "Yuchao", ""], ["Zhou", "Dingfu", ""], ["Liu", "Liu", ""], ["Li", "Wei", ""], ["Li", "Hongdng", ""], ["Yang", "Ruigang", ""]]}, {"id": "2006.01561", "submitter": "Mustafa Umit Oner", "authors": "Mustafa Umit Oner, Jared Marc Song Kye-Jet, Hwee Kuan Lee, Wing-Kin\n  Sung", "title": "Studying The Effect of MIL Pooling Filters on MIL Tasks", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are different multiple instance learning (MIL) pooling filters used in\nMIL models. In this paper, we study the effect of different MIL pooling filters\non the performance of MIL models in real world MIL tasks. We designed a neural\nnetwork based MIL framework with 5 different MIL pooling filters: `max',\n`mean', `attention', `distribution' and `distribution with attention'. We also\nformulated 5 different MIL tasks on a real world lymph node metastases dataset.\nWe found that the performance of our framework in a task is different for\ndifferent filters. We also observed that the performances of the five pooling\nfilters are also different from task to task. Hence, the selection of a correct\nMIL pooling filter for each MIL task is crucial for better performance.\nFurthermore, we noticed that models with `distribution' and `distribution with\nattention' pooling filters consistently perform well in almost all of the\ntasks. We attribute this phenomena to the amount of information captured by\n`distribution' based pooling filters. While point estimate based pooling\nfilters, like `max' and `mean', produce point estimates of distributions,\n`distribution' based pooling filters capture the full information in\ndistributions. Lastly, we compared the performance of our neural network model\nwith `distribution' pooling filter with the performance of the best MIL methods\nin the literature on classical MIL datasets and our model outperformed the\nothers.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 12:33:03 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Oner", "Mustafa Umit", ""], ["Kye-Jet", "Jared Marc Song", ""], ["Lee", "Hwee Kuan", ""], ["Sung", "Wing-Kin", ""]]}, {"id": "2006.01570", "submitter": "Ruben Wiersma", "authors": "Ruben Wiersma, Elmar Eisemann, Klaus Hildebrandt", "title": "CNNs on Surfaces using Rotation-Equivariant Features", "comments": "12 pages, 14 figures, 5 tables, to be published in ACM ToG (SIGGRAPH\n  2020)", "journal-ref": null, "doi": "10.1145/3386569.3392437", "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with a fundamental problem in geometric deep learning\nthat arises in the construction of convolutional neural networks on surfaces.\nDue to curvature, the transport of filter kernels on surfaces results in a\nrotational ambiguity, which prevents a uniform alignment of these kernels on\nthe surface. We propose a network architecture for surfaces that consists of\nvector-valued, rotation-equivariant features. The equivariance property makes\nit possible to locally align features, which were computed in arbitrary\ncoordinate systems, when aggregating features in a convolution layer. The\nresulting network is agnostic to the choices of coordinate systems for the\ntangent spaces on the surface. We implement our approach for triangle meshes.\nBased on circular harmonic functions, we introduce convolution filters for\nmeshes that are rotation-equivariant at the discrete level. We evaluate the\nresulting networks on shape correspondence and shape classifications tasks and\ncompare their performance to other approaches.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 12:46:00 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Wiersma", "Ruben", ""], ["Eisemann", "Elmar", ""], ["Hildebrandt", "Klaus", ""]]}, {"id": "2006.01615", "submitter": "Stefan H\\\"ormann", "authors": "Stefan H\\\"ormann, Martin Knoche, Gerhard Rigoll", "title": "A Multi-Task Comparator Framework for Kinship Verification", "comments": "To be published in IEEE FG 2020 - RFIW Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaches for kinship verification often rely on cosine distances between\nface identification features. However, due to gender bias inherent in these\nfeatures, it is hard to reliably predict whether two opposite-gender pairs are\nrelated. Instead of fine tuning the feature extractor network on kinship\nverification, we propose a comparator network to cope with this bias. After\nconcatenating both features, cascaded local expert networks extract the\ninformation most relevant for their corresponding kinship relation. We\ndemonstrate that our framework is robust against this gender bias and achieves\ncomparable results on two tracks of the RFIW Challenge 2020. Moreover, we show\nhow our framework can be further extended to handle partially known or unknown\nkinship relations.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 14:00:09 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["H\u00f6rmann", "Stefan", ""], ["Knoche", "Martin", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "2006.01629", "submitter": "Dongyang Liu", "authors": "Peng Wang, Dongyang Liu, Hui Li and Qi Wu", "title": "Give Me Something to Eat: Referring Expression Comprehension with\n  Commonsense Knowledge", "comments": "Accepted by ACMMM2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional referring expression comprehension (REF) assumes people to query\nsomething from an image by describing its visual appearance and spatial\nlocation, but in practice, we often ask for an object by describing its\naffordance or other non-visual attributes, especially when we do not have a\nprecise target. For example, sometimes we say 'Give me something to eat'. In\nthis case, we need to use commonsense knowledge to identify the objects in the\nimage. Unfortunately, these is no existing referring expression dataset\nreflecting this requirement, not to mention a model to tackle this challenge.\nIn this paper, we collect a new referring expression dataset, called KB-Ref,\ncontaining 43k expressions on 16k images. In KB-Ref, to answer each expression\n(detect the target object referred by the expression), at least one piece of\ncommonsense knowledge must be required. We then test state-of-the-art (SoTA)\nREF models on KB-Ref, finding that all of them present a large drop compared to\ntheir outstanding performance on general REF datasets. We also present an\nexpression conditioned image and fact attention (ECIFA) network that extract\ninformation from correlated image regions and commonsense knowledge facts. Our\nmethod leads to a significant improvement over SoTA REF models, although there\nis still a gap between this strong baseline and human performance. The dataset\nand baseline models will be released.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 14:12:43 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 11:24:00 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Peng", ""], ["Liu", "Dongyang", ""], ["Li", "Hui", ""], ["Wu", "Qi", ""]]}, {"id": "2006.01632", "submitter": "Mehul S. Raval", "authors": "Snehal Rajput, Mehul S Raval", "title": "A Review on End-To-End Methods for Brain Tumor Segmentation and Overall\n  Survival Prediction", "comments": "22 pages. Azerbaijan Journal for High Performance Computing, 2020", "journal-ref": null, "doi": "10.32010/26166127", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brain tumor segmentation intends to delineate tumor tissues from healthy\nbrain tissues. The tumor tissues include necrosis, peritumoral edema, and\nactive tumor. In contrast, healthy brain tissues include white matter, gray\nmatter, and cerebrospinal fluid. The MRI based brain tumor segmentation\nresearch is gaining popularity as; 1. It does not irradiate ionized radiation\nlike X-ray or computed tomography imaging. 2. It produces detailed pictures of\ninternal body structures. The MRI scans are input to deep learning-based\napproaches which are useful for automatic brain tumor segmentation. The\nfeatures from segments are fed to the classifier which predict the overall\nsurvival of the patient. The motive of this paper is to give an extensive\noverview of state-of-the-art jointly covering brain tumor segmentation and\noverall survival prediction.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 11:12:14 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Rajput", "Snehal", ""], ["Raval", "Mehul S", ""]]}, {"id": "2006.01645", "submitter": "Genta Kobayashi", "authors": "Genta Kobayashi and Hayaru Shouno", "title": "Interpretation of ResNet by Visualization of Preferred Stimulus in\n  Receptive Fields", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the methods used in image recognition is the Deep Convolutional Neural\nNetwork (DCNN). DCNN is a model in which the expressive power of features is\ngreatly improved by deepening the hidden layer of CNN. The architecture of CNNs\nis determined based on a model of the visual cortex of mammals. There is a\nmodel called Residual Network (ResNet) that has a skip connection. ResNet is an\nadvanced model in terms of the learning method, but it has not been interpreted\nfrom a biological viewpoint. In this research, we investigate the receptive\nfields of a ResNet on the classification task in ImageNet. We find that ResNet\nhas orientation selective neurons and double opponent color neurons. In\naddition, we suggest that some inactive neurons in the first layer of ResNet\naffect the classification task.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 14:25:26 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 11:26:17 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Kobayashi", "Genta", ""], ["Shouno", "Hayaru", ""]]}, {"id": "2006.01668", "submitter": "Radu Horaud P", "authors": "Xavier Alameda-Pineda, Vincent Drouard and Radu Horaud", "title": "Variational Inference and Learning of Piecewise-linear Dynamical Systems", "comments": "Submitted to IEEE Transactions on Neural Networks and Learning\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the temporal behavior of data is of primordial importance in many\nscientific and engineering fields. Baseline methods assume that both the\ndynamic and observation equations follow linear-Gaussian models. However, there\nare many real-world processes that cannot be characterized by a single linear\nbehavior. Alternatively, it is possible to consider a piecewise-linear model\nwhich, combined with a switching mechanism, is well suited when several modes\nof behavior are needed. Nevertheless, switching dynamical systems are\nintractable because of their computational complexity increases exponentially\nwith time. In this paper, we propose a variational approximation of piecewise\nlinear dynamical systems. We provide full details of the derivation of two\nvariational expectation-maximization algorithms, a filter and a smoother. We\nshow that the model parameters can be split into two sets, static and dynamic\nparameters, and that the former parameters can be estimated off-line together\nwith the number of linear modes, or the number of states of the switching\nvariable. We apply the proposed method to a visual tracking problem, namely\nhead-pose tracking, and we thoroughly compare our algorithm with several state\nof the art trackers.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 14:40:35 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 14:37:07 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Alameda-Pineda", "Xavier", ""], ["Drouard", "Vincent", ""], ["Horaud", "Radu", ""]]}, {"id": "2006.01687", "submitter": "Shasha Guo", "authors": "Shasha Guo, Lei Wang, Xiaofan Chen, Limeng Zhang, Ziyang Kang, Weixia\n  Xu", "title": "SeqXFilter: A Memory-efficient Denoising Filter for Dynamic Vision\n  Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic event-based dynamic vision sensors (DVS) have much faster\nsampling rates and a higher dynamic range than frame-based imaging sensors.\nHowever, they are sensitive to background activity (BA) events that are\nunwanted. There are some filters for tackling this problem based on\nspatio-temporal correlation. However, they are either memory-intensive or\ncomputing-intensive. We propose \\emph{SeqXFilter}, a spatio-temporal\ncorrelation filter with only a past event window that has an O(1) space\ncomplexity and has simple computations. We explore the spatial correlation of\nan event with its past few events by analyzing the distribution of the events\nwhen applying different functions on the spatial distances. We find the best\nfunction to check the spatio-temporal correlation for an event for\n\\emph{SeqXFilter}, best separating real events and noise events. We not only\ngive the visual denoising effect of the filter but also use two metrics for\nquantitatively analyzing the filter's performance. Four neuromorphic\nevent-based datasets, recorded from four DVS with different output sizes, are\nused for validation of our method. The experimental results show that\n\\emph{SeqXFilter} achieves similar performance as baseline NNb filters, but\nwith extremely small memory cost and simple computation logic.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 15:04:04 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Guo", "Shasha", ""], ["Wang", "Lei", ""], ["Chen", "Xiaofan", ""], ["Zhang", "Limeng", ""], ["Kang", "Ziyang", ""], ["Xu", "Weixia", ""]]}, {"id": "2006.01693", "submitter": "Farzad Khalvati", "authors": "Ruqian Hao, Khashayar Namdar, Lin Liu, Masoom A. Haider, Farzad\n  Khalvati", "title": "A Comprehensive Study of Data Augmentation Strategies for Prostate\n  Cancer Detection in Diffusion-weighted MRI using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation refers to a group of techniques whose goal is to battle\nlimited amount of available data to improve model generalization and push\nsample distribution toward the true distribution. While different augmentation\nstrategies and their combinations have been investigated for various computer\nvision tasks in the context of deep learning, a specific work in the domain of\nmedical imaging is rare and to the best of our knowledge, there has been no\ndedicated work on exploring the effects of various augmentation methods on the\nperformance of deep learning models in prostate cancer detection. In this work,\nwe have statically applied five most frequently used augmentation techniques\n(random rotation, horizontal flip, vertical flip, random crop, and translation)\nto prostate Diffusion-weighted Magnetic Resonance Imaging training dataset of\n217 patients separately and evaluated the effect of each method on the accuracy\nof prostate cancer detection. The augmentation algorithms were applied\nindependently to each data channel and a shallow as well as a deep\nConvolutional Neural Network (CNN) were trained on the five augmented sets\nseparately. We used Area Under Receiver Operating Characteristic (ROC) curve\n(AUC) to evaluate the performance of the trained CNNs on a separate test set of\n95 patients, using a validation set of 102 patients for finetuning. The shallow\nnetwork outperformed the deep network with the best 2D slice-based AUC of 0.85\nobtained by the rotation method.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 14:31:38 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Hao", "Ruqian", ""], ["Namdar", "Khashayar", ""], ["Liu", "Lin", ""], ["Haider", "Masoom A.", ""], ["Khalvati", "Farzad", ""]]}, {"id": "2006.01765", "submitter": "Sajad Saeedi", "authors": "Matthew Z. Wong (1), Benoit Guillard (2), Riku Murai (1), Sajad Saeedi\n  (3), Paul H.J. Kelly (1) ((1) Imperial College London, (2) EPFL Swiss Federal\n  Institute of Technology Lausanne, (3) Ryerson University)", "title": "AnalogNet: Convolutional Neural Network Inference on Analog Focal Plane\n  Sensor Processors", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a high-speed, energy-efficient Convolutional Neural Network (CNN)\narchitecture utilising the capabilities of a unique class of devices known as\nanalog Focal Plane Sensor Processors (FPSP), in which the sensor and the\nprocessor are embedded together on the same silicon chip. Unlike traditional\nvision systems, where the sensor array sends collected data to a separate\nprocessor for processing, FPSPs allow data to be processed on the imaging\ndevice itself. This unique architecture enables ultra-fast image processing and\nhigh energy efficiency, at the expense of limited processing resources and\napproximate computations. In this work, we show how to convert standard CNNs to\nFPSP code, and demonstrate a method of training networks to increase their\nrobustness to analog computation errors. Our proposed architecture, coined\nAnalogNet, reaches a testing accuracy of 96.9% on the MNIST handwritten digits\nrecognition task, at a speed of 2260 FPS, for a cost of 0.7 mJ per frame.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 16:44:43 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2020 17:19:36 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Wong", "Matthew Z.", ""], ["Guillard", "Benoit", ""], ["Murai", "Riku", ""], ["Saeedi", "Sajad", ""], ["Kelly", "Paul H. J.", ""]]}, {"id": "2006.01772", "submitter": "Angelika Skarysz", "authors": "Angelika Skarysz, Dahlia Salman, Michael Eddleston, Martin Sykora,\n  Eugenie Hunsicker, William H Nailon, Kareen Darnley, Duncan B McLaren, C L\n  Paul Thomas and Andrea Soltoggio", "title": "Fast and automated biomarker detection in breath samples with machine\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volatile organic compounds (VOCs) in human breath can reveal a large spectrum\nof health conditions and can be used for fast, accurate and non-invasive\ndiagnostics. Gas chromatography-mass spectrometry (GC-MS) is used to measure\nVOCs, but its application is limited by expert-driven data analysis that is\ntime-consuming, subjective and may introduce errors. We propose a system to\nperform GC-MS data analysis that exploits deep learning pattern recognition\nability to learn and automatically detect VOCs directly from raw data, thus\nbypassing expert-led processing. The new proposed approach showed to outperform\nthe expert-led analysis by detecting a significantly higher number of VOCs in\njust a fraction of time while maintaining high specificity. These results\nsuggest that the proposed method can help the large-scale deployment of\nbreath-based diagnosis by reducing time and cost, and increasing accuracy and\nconsistency.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 11:44:28 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Skarysz", "Angelika", ""], ["Salman", "Dahlia", ""], ["Eddleston", "Michael", ""], ["Sykora", "Martin", ""], ["Hunsicker", "Eugenie", ""], ["Nailon", "William H", ""], ["Darnley", "Kareen", ""], ["McLaren", "Duncan B", ""], ["Thomas", "C L Paul", ""], ["Soltoggio", "Andrea", ""]]}, {"id": "2006.01780", "submitter": "Rahat Yeasin Emon", "authors": "Rahat Yeasin Emon", "title": "A Novel Nudity Detection Algorithm for Web and Mobile Application\n  Development", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In our current web and mobile application development runtime nude image\ncontent detection is very important. This paper presents a runtime nudity\ndetection method for web and mobile application development. We use two\nparameters to detect the nude content of an image. One is the number of skin\npixels another is face region. A skin color model based on RGB, HSV color\nspaces are used to detect skin pixels in an image. Google vision api is used to\ndetect the face region. By the percentage of skin regions and face regions an\nimage is identified nude or not. The success of this algorithm exists in\ndetecting skin regions and face regions. The skin detection algorithm can\ndetect skin 95% accurately with a low false-positive rate and the google vision\napi for web and mobile applications can detect face 99% accurately with less\nthan 1 second time. From the experimental analysis, we have seen that the\nproposed algorithm can detect 95% percent accurately the nudity of an image.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 17:00:47 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 15:29:09 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Emon", "Rahat Yeasin", ""]]}, {"id": "2006.01785", "submitter": "Daniel T Chang", "authors": "Daniel T. Chang", "title": "Geometric Graph Representations and Geometric Graph Convolutions for\n  Deep Learning on Three-Dimensional (3D) Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The geometry of three-dimensional (3D) graphs, consisting of nodes and edges,\nplays a crucial role in many important applications. An excellent example is\nmolecular graphs, whose geometry influences important properties of a molecule\nincluding its reactivity and biological activity. To facilitate the\nincorporation of geometry in deep learning on 3D graphs, we define three types\nof geometric graph representations: positional, angle-geometric and\ndistance-geometric. For proof of concept, we use the distance-geometric graph\nrepresentation for geometric graph convolutions. Further, to utilize standard\ngraph convolution networks, we employ a simple edge weight / edge distance\ncorrelation scheme, whose parameters can be fixed using reference values or\ndetermined through Bayesian hyperparameter optimization. The results of\ngeometric graph convolutions, for the ESOL and Freesol datasets, show\nsignificant improvement over those of standard graph convolutions. Our work\ndemonstrates the feasibility and promise of incorporating geometry, using the\ndistance-geometric graph representation, in deep learning on 3D graphs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 17:08:59 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Chang", "Daniel T.", ""]]}, {"id": "2006.01795", "submitter": "Marco Ancona", "authors": "Marco Ancona and Cengiz \\\"Oztireli and Markus Gross", "title": "Shapley Value as Principled Metric for Structured Network Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured pruning is a well-known technique to reduce the storage size and\ninference cost of neural networks. The usual pruning pipeline consists of\nranking the network internal filters and activations with respect to their\ncontributions to the network performance, removing the units with the lowest\ncontribution, and fine-tuning the network to reduce the harm induced by\npruning. Recent results showed that random pruning performs on par with other\nmetrics, given enough fine-tuning resources. In this work, we show that this is\nnot true on a low-data regime when fine-tuning is either not possible or not\neffective. In this case, reducing the harm caused by pruning becomes crucial to\nretain the performance of the network. First, we analyze the problem of\nestimating the contribution of hidden units with tools suggested by cooperative\ngame theory and propose Shapley values as a principled ranking metric for this\ntask. We compare with several alternatives proposed in the literature and\ndiscuss how Shapley values are theoretically preferable. Finally, we compare\nall ranking metrics on the challenging scenario of low-data pruning, where we\ndemonstrate how Shapley values outperform other heuristics.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 17:26:49 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Ancona", "Marco", ""], ["\u00d6ztireli", "Cengiz", ""], ["Gross", "Markus", ""]]}, {"id": "2006.01797", "submitter": "Patrick Rosenberger", "authors": "Patrick Rosenberger, Akansel Cosgun, Rhys Newbury, Jun Kwan, Valerio\n  Ortenzi, Peter Corke and Manfred Grafinger", "title": "Object-Independent Human-to-Robot Handovers using Real Time Robotic\n  Vision", "comments": "IEEE Robotics and Automation Letters (RA-L). Preprint Version.\n  Accepted September, 2020. The code and videos can be found at\n  https://patrosat.github.io/h2r_handovers/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for safe and object-independent human-to-robot\nhandovers using real time robotic vision and manipulation. We aim for general\napplicability with a generic object detector, a fast grasp selection algorithm\nand by using a single gripper-mounted RGB-D camera, hence not relying on\nexternal sensors. The robot is controlled via visual servoing towards the\nobject of interest. Putting a high emphasis on safety, we use two perception\nmodules: human body part segmentation and hand/finger segmentation. Pixels that\nare deemed to belong to the human are filtered out from candidate grasp poses,\nhence ensuring that the robot safely picks the object without colliding with\nthe human partner. The grasp selection and perception modules run concurrently\nin real-time, which allows monitoring of the progress. In experiments with 13\nobjects, the robot was able to successfully take the object from the human in\n81.9% of the trials.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 17:29:20 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 16:40:13 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Rosenberger", "Patrick", ""], ["Cosgun", "Akansel", ""], ["Newbury", "Rhys", ""], ["Kwan", "Jun", ""], ["Ortenzi", "Valerio", ""], ["Corke", "Peter", ""], ["Grafinger", "Manfred", ""]]}, {"id": "2006.01804", "submitter": "Martin Weigert", "authors": "Debayan Saha, Uwe Schmidt, Qinrong Zhang, Aurelien Barbotin, Qi Hu, Na\n  Ji, Martin J. Booth, Martin Weigert, Eugene W. Myers", "title": "Practical sensorless aberration estimation for 3D microscopy with deep\n  learning", "comments": null, "journal-ref": null, "doi": "10.1364/OE.401933", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of optical aberrations from volumetric intensity images is a key\nstep in sensorless adaptive optics for 3D microscopy. Recent approaches based\non deep learning promise accurate results at fast processing speeds. However,\ncollecting ground truth microscopy data for training the network is typically\nvery difficult or even impossible thereby limiting this approach in practice.\nHere, we demonstrate that neural networks trained only on simulated data yield\naccurate predictions for real experimental images. We validate our approach on\nsimulated and experimental datasets acquired with two different microscopy\nmodalities, and also compare the results to non-learned methods. Additionally,\nwe study the predictability of individual aberrations with respect to their\ndata requirements and find that the symmetry of the wavefront plays a crucial\nrole. Finally, we make our implementation freely available as open source\nsoftware in Python.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 17:39:32 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 19:17:05 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Saha", "Debayan", ""], ["Schmidt", "Uwe", ""], ["Zhang", "Qinrong", ""], ["Barbotin", "Aurelien", ""], ["Hu", "Qi", ""], ["Ji", "Na", ""], ["Booth", "Martin J.", ""], ["Weigert", "Martin", ""], ["Myers", "Eugene W.", ""]]}, {"id": "2006.01888", "submitter": "Zhuoran Liu", "authors": "Zhuoran Liu and Martha Larson", "title": "Adversarial Item Promotion: Vulnerabilities at the Core of Top-N\n  Recommenders that Use Images to Address Cold Start", "comments": "Our code is available at https://github.com/liuzrcc/AIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  E-commerce platforms provide their customers with ranked lists of recommended\nitems matching the customers' preferences. Merchants on e-commerce platforms\nwould like their items to appear as high as possible in the top-N of these\nranked lists. In this paper, we demonstrate how unscrupulous merchants can\ncreate item images that artificially promote their products, improving their\nrankings. Recommender systems that use images to address the cold start problem\nare vulnerable to this security risk. We describe a new type of attack,\nAdversarial Item Promotion (AIP), that strikes directly at the core of Top-N\nrecommenders: the ranking mechanism itself. Existing work on adversarial images\nin recommender systems investigates the implications of conventional attacks,\nwhich target deep learning classifiers. In contrast, our AIP attacks are\nembedding attacks that seek to push features representations in a way that\nfools the ranker (not a classifier) and directly lead to item promotion. We\nintroduce three AIP attacks insider attack, expert attack, and semantic attack,\nwhich are defined with respect to three successively more realistic attack\nmodels. Our experiments evaluate the danger of these attacks when mounted\nagainst three representative visually-aware recommender algorithms in a\nframework that uses images to address cold start. We also evaluate potential\ndefenses, including adversarial training and find that common,\ncurrently-existing, techniques do not eliminate the danger of AIP attacks. In\nsum, we show that using images to address cold start opens recommender systems\nto potential threats with clear practical implications.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 19:12:13 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 11:46:09 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 13:05:48 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Liu", "Zhuoran", ""], ["Larson", "Martha", ""]]}, {"id": "2006.01895", "submitter": "Pengsheng Guo", "authors": "Pengsheng Guo, Chen-Yu Lee, Daniel Ulbricht", "title": "Learning to Branch for Multi-Task Learning", "comments": "Accepted at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training multiple tasks jointly in one deep network yields reduced latency\nduring inference and better performance over the single-task counterpart by\nsharing certain layers of a network. However, over-sharing a network could\nerroneously enforce over-generalization, causing negative knowledge transfer\nacross tasks. Prior works rely on human intuition or pre-computed task\nrelatedness scores for ad hoc branching structures. They provide sub-optimal\nend results and often require huge efforts for the trial-and-error process. In\nthis work, we present an automated multi-task learning algorithm that learns\nwhere to share or branch within a network, designing an effective network\ntopology that is directly optimized for multiple objectives across tasks.\nSpecifically, we propose a novel tree-structured design space that casts a tree\nbranching operation as a gumbel-softmax sampling procedure. This enables\ndifferentiable network splitting that is end-to-end trainable. We validate the\nproposed method on controlled synthetic data, CelebA, and Taskonomy.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 19:23:21 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 05:18:55 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Guo", "Pengsheng", ""], ["Lee", "Chen-Yu", ""], ["Ulbricht", "Daniel", ""]]}, {"id": "2006.01897", "submitter": "Tomohiro Maeda", "authors": "Tomohiro Maeda, Ankit Ranjan, Ramesh Raskar", "title": "Automatic Differentiation for All Photons Imaging to See Inside\n  Volumetric Scattering Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging through dense scattering media - such as biological tissue, fog, and\nsmoke - has applications in the medical and robotics fields. We propose a new\nframework using automatic differentiation for All Photons Imaging through\nhomogeneous scattering media with unknown optical properties for non-invasive\nsensing and diagnostics. We overcome the need for the imaging target to be\nvisible to the illumination source in All Photons Imaging, enabling practical\nand non-invasive imaging through turbid media with a simple optical setup. Our\nmethod does not require calibration to acquire the sensor position or optical\nproperties of the media.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 19:24:28 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Maeda", "Tomohiro", ""], ["Ranjan", "Ankit", ""], ["Raskar", "Ramesh", ""]]}, {"id": "2006.01910", "submitter": "Emiel Hoogeboom", "authors": "Emiel Hoogeboom, Victor Garcia Satorras, Jakub M. Tomczak, Max Welling", "title": "The Convolution Exponential and Generalized Sylvester Flows", "comments": "Accepted to Neural Information Processing Systems (NeurIPS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new method to build linear flows, by taking the\nexponential of a linear transformation. This linear transformation does not\nneed to be invertible itself, and the exponential has the following desirable\nproperties: it is guaranteed to be invertible, its inverse is straightforward\nto compute and the log Jacobian determinant is equal to the trace of the linear\ntransformation. An important insight is that the exponential can be computed\nimplicitly, which allows the use of convolutional layers. Using this insight,\nwe develop new invertible transformations named convolution exponentials and\ngraph convolution exponentials, which retain the equivariance of their\nunderlying transformations. In addition, we generalize Sylvester Flows and\npropose Convolutional Sylvester Flows which are based on the generalization and\nthe convolution exponential as basis change. Empirically, we show that the\nconvolution exponential outperforms other linear transformations in generative\nflows on CIFAR10 and the graph convolution exponential improves the performance\nof graph normalizing flows. In addition, we show that Convolutional Sylvester\nFlows improve performance over residual flows as a generative flow model\nmeasured in log-likelihood.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 19:43:36 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 10:24:08 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Hoogeboom", "Emiel", ""], ["Satorras", "Victor Garcia", ""], ["Tomczak", "Jakub M.", ""], ["Welling", "Max", ""]]}, {"id": "2006.01943", "submitter": "Dogucan Yaman", "authors": "Dogucan Yaman, Fevziye Irem Eyiokur, Haz{\\i}m Kemal Ekenel", "title": "Ear2Face: Deep Biometric Modality Mapping", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the correlation between different visual biometric\nmodalities. For this purpose, we present an end-to-end deep neural network\nmodel that learns a mapping between the biometric modalities. Namely, our goal\nis to generate a frontal face image of a subject given his/her ear image as the\ninput. We formulated the problem as a paired image-to-image translation task\nand collected datasets of ear and face image pairs from the Multi-PIE and FERET\ndatasets to train our GAN-based models. We employed feature reconstruction and\nstyle reconstruction losses in addition to adversarial and pixel losses. We\nevaluated the proposed method both in terms of reconstruction quality and in\nterms of person identification accuracy. To assess the generalization\ncapability of the learned mapping models, we also run cross-dataset\nexperiments. That is, we trained the model on the FERET dataset and tested it\non the Multi-PIE dataset and vice versa. We have achieved very promising\nresults, especially on the FERET dataset, generating visually appealing face\nimages from ear image inputs. Moreover, we attained a very high cross-modality\nperson identification performance, for example, reaching 90.9% Rank-10\nidentification accuracy on the FERET dataset.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 21:14:27 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Yaman", "Dogucan", ""], ["Eyiokur", "Fevziye Irem", ""], ["Ekenel", "Haz\u0131m Kemal", ""]]}, {"id": "2006.01945", "submitter": "Damian Campo", "authors": "Damian Campo, Giulia Slavic, Mohamad Baydoun, Lucio Marcenaro, Carlo\n  Regazzoni", "title": "Continual Learning of Predictive Models in Video Sequences via\n  Variational Autoencoders", "comments": "Manuscript accepted at the 27th IEEE International Conference on\n  Image Processing (ICIP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for performing continual learning of predictive\nmodels that facilitate the inference of future frames in video sequences. For a\nfirst given experience, an initial Variational Autoencoder, together with a set\nof fully connected neural networks are utilized to respectively learn the\nappearance of video frames and their dynamics at the latent space level. By\nemploying an adapted Markov Jump Particle Filter, the proposed method\nrecognizes new situations and integrates them as predictive models avoiding\ncatastrophic forgetting of previously learned tasks. For evaluating the\nproposed method, this article uses video sequences from a vehicle that performs\ndifferent tasks in a controlled environment.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 21:17:38 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Campo", "Damian", ""], ["Slavic", "Giulia", ""], ["Baydoun", "Mohamad", ""], ["Marcenaro", "Lucio", ""], ["Regazzoni", "Carlo", ""]]}, {"id": "2006.01959", "submitter": "Miguel Jaques", "authors": "Miguel Jaques, Michael Burke, Timothy Hospedales", "title": "NewtonianVAE: Proportional Control and Goal Identification from Pixels\n  via Physical Latent Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning low-dimensional latent state space dynamics models has been a\npowerful paradigm for enabling vision-based planning and learning for control.\nWe introduce a latent dynamics learning framework that is uniquely designed to\ninduce proportional controlability in the latent space, thus enabling the use\nof much simpler controllers than prior work. We show that our learned dynamics\nmodel enables proportional control from pixels, dramatically simplifies and\naccelerates behavioural cloning of vision-based controllers, and provides\ninterpretable goal discovery when applied to imitation learning of switching\ncontrollers from demonstration.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 21:41:38 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 21:49:30 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Jaques", "Miguel", ""], ["Burke", "Michael", ""], ["Hospedales", "Timothy", ""]]}, {"id": "2006.01964", "submitter": "Cenek Albl", "authors": "Cenek Albl, Zuzana Kukelova, Viktor Larsson, Tomas Pajdla, Konrad\n  Schindler", "title": "From two rolling shutters to one global shutter", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most consumer cameras are equipped with electronic rolling shutter, leading\nto image distortions when the camera moves during image capture. We explore a\nsurprisingly simple camera configuration that makes it possible to undo the\nrolling shutter distortion: two cameras mounted to have different rolling\nshutter directions. Such a setup is easy and cheap to build and it possesses\nthe geometric constraints needed to correct rolling shutter distortion using\nonly a sparse set of point correspondences between the two images. We derive\nequations that describe the underlying geometry for general and special motions\nand present an efficient method for finding their solutions. Our synthetic and\nreal experiments demonstrate that our approach is able to remove large rolling\nshutter distortions of all types without relying on any specific scene\nstructure.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 22:18:43 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Albl", "Cenek", ""], ["Kukelova", "Zuzana", ""], ["Larsson", "Viktor", ""], ["Pajdla", "Tomas", ""], ["Schindler", "Konrad", ""]]}, {"id": "2006.01967", "submitter": "Jiabao Wang", "authors": "Jiabao Wang, Yang Li, Shanshan Jiao, Zhuang Miao, Rui Zhang", "title": "Grafted network for person re-identification", "comments": null, "journal-ref": "Signal Processing: Image Communication 80(2020)115674", "doi": "10.1016/j.image.2019.115674", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have shown outstanding effectiveness in person\nre-identification (re-ID). However, the models always have large number of\nparameters and much computation for mobile application. In order to relieve\nthis problem, we propose a novel grafted network (GraftedNet), which is\ndesigned by grafting a high-accuracy rootstock and a light-weighted scion. The\nrootstock is based on the former parts of ResNet-50 to provide a strong\nbaseline, while the scion is a new designed module, composed of the latter\nparts of SqueezeNet, to compress the parameters. To extract more discriminative\nfeature representation, a joint multi-level and part-based feature is proposed.\nIn addition, to train GraftedNet efficiently, we propose an accompanying\nlearning method, by adding an accompanying branch to train the model in\ntraining and removing it in testing for saving parameters and computation. On\nthree public person re-ID benchmarks (Market1501, DukeMTMC-reID and CUHK03),\nthe effectiveness of GraftedNet are evaluated and its components are analyzed.\nExperimental results show that the proposed GraftedNet achieves 93.02%, 85.3%\nand 76.2% in Rank-1 and 81.6%, 74.7% and 71.6% in mAP, with only 4.6M\nparameters.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 22:33:44 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 05:25:28 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Wang", "Jiabao", ""], ["Li", "Yang", ""], ["Jiao", "Shanshan", ""], ["Miao", "Zhuang", ""], ["Zhang", "Rui", ""]]}, {"id": "2006.01983", "submitter": "Jwala Dhamala", "authors": "Jwala Dhamala, John L. Sapp, B. Milan Hor\\'acek, Linwei Wang", "title": "Quantifying the Uncertainty in Model Parameters Using Gaussian\n  Process-Based Markov Chain Monte Carlo: An Application to Cardiac\n  Electrophysiological Models", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-59050-9_18", "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of patient-specific model parameters is important for personalized\nmodeling, although sparse and noisy clinical data can introduce significant\nuncertainty in the estimated parameter values. This importance source of\nuncertainty, if left unquantified, will lead to unknown variability in model\noutputs that hinder their reliable adoptions. Probabilistic estimation model\nparameters, however, remains an unresolved challenge because standard Markov\nChain Monte Carlo sampling requires repeated model simulations that are\ncomputationally infeasible. A common solution is to replace the simulation\nmodel with a computationally-efficient surrogate for a faster sampling.\nHowever, by sampling from an approximation of the exact posterior probability\ndensity function (pdf) of the parameters, the efficiency is gained at the\nexpense of sampling accuracy. In this paper, we address this issue by\nintegrating surrogate modeling into Metropolis Hasting (MH) sampling of the\nexact posterior pdfs to improve its acceptance rate. It is done by first\nquickly constructing a Gaussian process (GP) surrogate of the exact posterior\npdfs using deterministic optimization. This efficient surrogate is then used to\nmodify commonly-used proposal distributions in MH sampling such that only\nproposals accepted by the surrogate will be tested by the exact posterior pdf\nfor acceptance/rejection, reducing unnecessary model simulations at unlikely\ncandidates. Synthetic and real-data experiments using the presented method show\na significant gain in computational efficiency without compromising the\naccuracy. In addition, insights into the non-identifiability and heterogeneity\nof tissue properties can be gained from the obtained posterior distributions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 23:48:15 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Dhamala", "Jwala", ""], ["Sapp", "John L.", ""], ["Hor\u00e1cek", "B. Milan", ""], ["Wang", "Linwei", ""]]}, {"id": "2006.01993", "submitter": "Kazuhiro Terao", "authors": "Corey Adams, Kazuhiro Terao, Taritree Wongjirad", "title": "PILArNet: Public Dataset for Particle Imaging Liquid Argon Detectors in\n  High Energy Physics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ins-det cs.CV cs.LG hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid advancement of machine learning solutions has often coincided with the\nproduction of a test public data set. Such datasets reduce the largest barrier\nto entry for tackling a problem -- procuring data -- while also providing a\nbenchmark to compare different solutions. Furthermore, large datasets have been\nused to train high-performing feature finders which are then used in new\napproaches to problems beyond that initially defined. In order to encourage the\nrapid development in the analysis of data collected using liquid argon time\nprojection chambers, a class of particle detectors used in high energy physics\nexperiments, we have produced the PILArNet, first 2D and 3D open dataset to be\nused for a couple of key analysis tasks. The initial dataset presented in this\npaper contains 300,000 samples simulated and recorded in three different volume\nsizes. The dataset is stored efficiently in sparse 2D and 3D matrix format with\nauxiliary information about simulated particles in the volume, and is made\navailable for public research use. In this paper we describe the dataset,\ntasks, and the method used to procure the sample.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 00:36:04 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Adams", "Corey", ""], ["Terao", "Kazuhiro", ""], ["Wongjirad", "Taritree", ""]]}, {"id": "2006.02000", "submitter": "Nemanja Djuric", "authors": "Nemanja Djuric, Henggang Cui, Zhaoen Su, Shangxuan Wu, Huahua Wang,\n  Fang-Chieh Chou, Luisa San Martin, Song Feng, Rui Hu, Yang Xu, Alyssa Dayan,\n  Sidney Zhang, Brian C. Becker, Gregory P. Meyer, Carlos Vallespi-Gonzalez,\n  Carl K. Wellington", "title": "MultiXNet: Multiclass Multistage Multimodal Motion Prediction", "comments": "Accepted for publication at IEEE Intelligent Vehicles Symposium (IV)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the critical pieces of the self-driving puzzle is understanding the\nsurroundings of a self-driving vehicle (SDV) and predicting how these\nsurroundings will change in the near future. To address this task we propose\nMultiXNet, an end-to-end approach for detection and motion prediction based\ndirectly on lidar sensor data. This approach builds on prior work by handling\nmultiple classes of traffic actors, adding a jointly trained second-stage\ntrajectory refinement step, and producing a multimodal probability distribution\nover future actor motion that includes both multiple discrete traffic behaviors\nand calibrated continuous position uncertainties. The method was evaluated on\nlarge-scale, real-world data collected by a fleet of SDVs in several cities,\nwith the results indicating that it outperforms existing state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 01:01:48 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 20:16:49 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2020 17:20:59 GMT"}, {"version": "v4", "created": "Mon, 24 May 2021 04:31:50 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Djuric", "Nemanja", ""], ["Cui", "Henggang", ""], ["Su", "Zhaoen", ""], ["Wu", "Shangxuan", ""], ["Wang", "Huahua", ""], ["Chou", "Fang-Chieh", ""], ["Martin", "Luisa San", ""], ["Feng", "Song", ""], ["Hu", "Rui", ""], ["Xu", "Yang", ""], ["Dayan", "Alyssa", ""], ["Zhang", "Sidney", ""], ["Becker", "Brian C.", ""], ["Meyer", "Gregory P.", ""], ["Vallespi-Gonzalez", "Carlos", ""], ["Wellington", "Carl K.", ""]]}, {"id": "2006.02003", "submitter": "Alexander Cao", "authors": "Alexander Cao, Yuan Luo, Diego Klabjan", "title": "Open-Set Recognition with Gaussian Mixture Variational Autoencoders", "comments": "12 pages including 8 figures and 4 tables, plus 6 pages of\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In inference, open-set classification is to either classify a sample into a\nknown class from training or reject it as an unknown class. Existing deep\nopen-set classifiers train explicit closed-set classifiers, in some cases\ndisjointly utilizing reconstruction, which we find dilutes the latent\nrepresentation's ability to distinguish unknown classes. In contrast, we train\nour model to cooperatively learn reconstruction and perform class-based\nclustering in the latent space. With this, our Gaussian mixture variational\nautoencoder (GMVAE) achieves more accurate and robust open-set classification\nresults, with an average F1 improvement of 29.5%, through extensive experiments\naided by analytical results.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 01:15:19 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Cao", "Alexander", ""], ["Luo", "Yuan", ""], ["Klabjan", "Diego", ""]]}, {"id": "2006.02026", "submitter": "Stanley Chan", "authors": "Abhiram Gnanasambandam and Stanley H. Chan", "title": "Image Classification in the Dark using Quanta Image Sensors", "comments": "Published in the 16th European Conference on Computer Vision (ECCV)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art image classifiers are trained and tested using\nwell-illuminated images. These images are typically captured by CMOS image\nsensors with at least tens of photons per pixel. However, in dark environments\nwhen the photon flux is low, image classification becomes difficult because the\nmeasured signal is suppressed by noise. In this paper, we present a new\nlow-light image classification solution using Quanta Image Sensors (QIS). QIS\nare a new type of image sensors that possess photon counting ability without\ncompromising on pixel size and spatial resolution. Numerous studies over the\npast decade have demonstrated the feasibility of QIS for low-light imaging, but\ntheir usage for image classification has not been studied. This paper fills the\ngap by presenting a student-teacher learning scheme which allows us to classify\nthe noisy QIS raw data. We show that with student-teacher learning, we are able\nto achieve image classification at a photon level of one photon per pixel or\nlower. Experimental results verify the effectiveness of the proposed method\ncompared to existing solutions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 03:39:07 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 03:23:11 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 20:22:46 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Gnanasambandam", "Abhiram", ""], ["Chan", "Stanley H.", ""]]}, {"id": "2006.02038", "submitter": "Lingzhi Zhang", "authors": "Lingzhi Zhang, Jiancong Wang, Yinshuang Xu, Jie Min, Tarmily Wen,\n  James C. Gee, Jianbo Shi", "title": "Nested Scale Editing for Conditional Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an image synthesis approach that provides stratified navigation in\nthe latent code space. With a tiny amount of partial or very low-resolution\nimage, our approach can consistently out-perform state-of-the-art counterparts\nin terms of generating the closest sampled image to the ground truth. We\nachieve this through scale-independent editing while expanding scale-specific\ndiversity. Scale-independence is achieved with a nested scale disentanglement\nloss. Scale-specific diversity is created by incorporating a progressive\ndiversification constraint. We introduce semantic persistency across the scales\nby sharing common latent codes. Together they provide better control of the\nimage synthesis process. We evaluate the effectiveness of our proposed approach\nthrough various tasks, including image outpainting, image superresolution, and\ncross-domain image translation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 04:29:21 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Zhang", "Lingzhi", ""], ["Wang", "Jiancong", ""], ["Xu", "Yinshuang", ""], ["Min", "Jie", ""], ["Wen", "Tarmily", ""], ["Gee", "James C.", ""], ["Shi", "Jianbo", ""]]}, {"id": "2006.02049", "submitter": "Xiaoliang Dai", "authors": "Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Bichen Wu, Zijian He, Zhen\n  Wei, Kan Chen, Yuandong Tian, Matthew Yu, Peter Vajda, Joseph E. Gonzalez", "title": "FBNetV3: Joint Architecture-Recipe Search using Predictor Pretraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) yields state-of-the-art neural networks that\noutperform their best manually-designed counterparts. However, previous NAS\nmethods search for architectures under one set of training hyper-parameters\n(i.e., a training recipe), overlooking superior architecture-recipe\ncombinations. To address this, we present Neural Architecture-Recipe Search\n(NARS) to search both (a) architectures and (b) their corresponding training\nrecipes, simultaneously. NARS utilizes an accuracy predictor that scores\narchitecture and training recipes jointly, guiding both sample selection and\nranking. Furthermore, to compensate for the enlarged search space, we leverage\n\"free\" architecture statistics (e.g., FLOP count) to pretrain the predictor,\nsignificantly improving its sample efficiency and prediction reliability. After\ntraining the predictor via constrained iterative optimization, we run fast\nevolutionary searches in just CPU minutes to generate architecture-recipe pairs\nfor a variety of resource constraints, called FBNetV3. FBNetV3 makes up a\nfamily of state-of-the-art compact neural networks that outperform both\nautomatically and manually-designed competitors. For example, FBNetV3 matches\nboth EfficientNet and ResNeSt accuracy on ImageNet with up to 2.0x and 7.1x\nfewer FLOPs, respectively. Furthermore, FBNetV3 yields significant performance\ngains for downstream object detection tasks, improving mAP despite 18% fewer\nFLOPs and 34% fewer parameters than EfficientNet-based equivalents.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 05:20:21 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 02:38:18 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 14:54:08 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Dai", "Xiaoliang", ""], ["Wan", "Alvin", ""], ["Zhang", "Peizhao", ""], ["Wu", "Bichen", ""], ["He", "Zijian", ""], ["Wei", "Zhen", ""], ["Chen", "Kan", ""], ["Tian", "Yuandong", ""], ["Yu", "Matthew", ""], ["Vajda", "Peter", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "2006.02051", "submitter": "Qiyao Deng", "authors": "Qiyao Deng, Jie Cao, Yunfan Liu, Zhenhua Chai, Qi Li and Zhenan Sun", "title": "Reference-guided Face Component Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Face portrait editing has achieved great progress in recent years. However,\nprevious methods either 1) operate on pre-defined face attributes, lacking the\nflexibility of controlling shapes of high-level semantic facial components\n(e.g., eyes, nose, mouth), or 2) take manually edited mask or sketch as an\nintermediate representation for observable changes, but such additional input\nusually requires extra efforts to obtain. To break the limitations (e.g. shape,\nmask or sketch) of the existing methods, we propose a novel framework termed\nr-FACE (Reference-guided FAce Component Editing) for diverse and controllable\nface component editing with geometric changes. Specifically, r-FACE takes an\nimage inpainting model as the backbone, utilizing reference images as\nconditions for controlling the shape of face components. In order to encourage\nthe framework to concentrate on the target face components, an example-guided\nattention module is designed to fuse attention features and the target face\ncomponent features extracted from the reference image. Through extensive\nexperimental validation and comparisons, we verify the effectiveness of the\nproposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 05:34:54 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 13:37:59 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Deng", "Qiyao", ""], ["Cao", "Jie", ""], ["Liu", "Yunfan", ""], ["Chai", "Zhenhua", ""], ["Li", "Qi", ""], ["Sun", "Zhenan", ""]]}, {"id": "2006.02068", "submitter": "Noriaki Hirose", "authors": "Noriaki Hirose, Satoshi Koide, Keisuke Kawano, Ruho Kondo", "title": "PLG-IN: Pluggable Geometric Consistency Loss with Wasserstein Distance\n  in Monocular Depth Estimation", "comments": "13 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel objective for penalizing geometric inconsistencies to\nimprove the depth and pose estimation performance of monocular camera images.\nOur objective is designed using the Wasserstein distance between two point\nclouds, estimated from images with different camera poses. The Wasserstein\ndistance can impose a soft and symmetric coupling between two point clouds,\nwhich suitably maintains geometric constraints and results in a differentiable\nobjective. By adding our objective to the those of other state-of-the-art\nmethods, we can effectively penalize geometric inconsistencies and obtain\nhighly accurate depth and pose estimations. Our proposed method is evaluated\nusing the KITTI dataset.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 06:50:42 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 02:55:13 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Hirose", "Noriaki", ""], ["Koide", "Satoshi", ""], ["Kawano", "Keisuke", ""], ["Kondo", "Ruho", ""]]}, {"id": "2006.02098", "submitter": "Tiberiu Teodor Cocias", "authors": "Tiberiu Cocias, Alexandru Razvant and Sorin Grigorescu", "title": "GFPNet: A Deep Network for Learning Shape Completion in Generic Fitted\n  Primitives", "comments": "8 pages, 14 figures, IEEE Robotics and Automation Letters. Preprint\n  Version. Accepted May, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an object reconstruction apparatus that uses the\nso-called Generic Primitives (GP) to complete shapes. A GP is a 3D point cloud\ndepicting a generalized shape of a class of objects. To reconstruct the objects\nin a scene we first fit a GP onto each occluded object to obtain an initial raw\nstructure. Secondly, we use a model-based deformation technique to fold the\nsurface of the GP over the occluded object. The deformation model is encoded\nwithin the layers of a Deep Neural Network (DNN), coined GFPNet. The objective\nof the network is to transfer the particularities of the object from the scene\nto the raw volume represented by the GP. We show that GFPNet competes with\nstate of the art shape completion methods by providing performance results on\nthe ModelNet and KITTI benchmarking datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 08:29:27 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Cocias", "Tiberiu", ""], ["Razvant", "Alexandru", ""], ["Grigorescu", "Sorin", ""]]}, {"id": "2006.02105", "submitter": "Michele Fraccaroli", "authors": "Michele Fraccaroli, Evelina Lamma, Fabrizio Riguzzi", "title": "Automatic Setting of DNN Hyper-Parameters by Mixing Bayesian\n  Optimization and Tuning Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques play an increasingly important role in industrial\nand research environments due to their outstanding results. However, the large\nnumber of hyper-parameters to be set may lead to errors if they are set\nmanually. The state-of-the-art hyper-parameters tuning methods are grid search,\nrandom search, and Bayesian Optimization. The first two methods are expensive\nbecause they try, respectively, all possible combinations and random\ncombinations of hyper-parameters. Bayesian Optimization, instead, builds a\nsurrogate model of the objective function, quantifies the uncertainty in the\nsurrogate using Gaussian Process Regression and uses an acquisition function to\ndecide where to sample the new set of hyper-parameters. This work faces the\nfield of Hyper-Parameters Optimization (HPO). The aim is to improve Bayesian\nOptimization applied to Deep Neural Networks. For this goal, we build a new\nalgorithm for evaluating and analyzing the results of the network on the\ntraining and validation sets and use a set of tuning rules to add new\nhyper-parameters and/or to reduce the hyper-parameter search space to select a\nbetter combination.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 08:53:48 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Fraccaroli", "Michele", ""], ["Lamma", "Evelina", ""], ["Riguzzi", "Fabrizio", ""]]}, {"id": "2006.02108", "submitter": "Tim Tang", "authors": "Tim Y. Tang, Daniele De Martini, Shangzhe Wu, Paul Newman", "title": "Self-Supervised Localisation between Range Sensors and Overhead Imagery", "comments": "Robotics: Science and Systems (RSS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Publicly available satellite imagery can be an ubiquitous, cheap, and\npowerful tool for vehicle localisation when a prior sensor map is unavailable.\nHowever, satellite images are not directly comparable to data from ground range\nsensors because of their starkly different modalities. We present a learned\nmetric localisation method that not only handles the modality difference, but\nis cheap to train, learning in a self-supervised fashion without metrically\naccurate ground truth. By evaluating across multiple real-world datasets, we\ndemonstrate the robustness and versatility of our method for various sensor\nconfigurations. We pay particular attention to the use of millimetre wave\nradar, which, owing to its complex interaction with the scene and its immunity\nto weather and lighting, makes for a compelling and valuable use case.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 08:58:54 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 12:49:46 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Tang", "Tim Y.", ""], ["De Martini", "Daniele", ""], ["Wu", "Shangzhe", ""], ["Newman", "Paul", ""]]}, {"id": "2006.02110", "submitter": "Igor Kviatkovsky", "authors": "Igor Kviatkovsky, Nadav Bhonker and Gerard Medioni", "title": "From Real to Synthetic and Back: Synthesizing Training Data for\n  Multi-Person Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for synthesizing naturally looking images of multiple\npeople interacting in a specific scenario. These images benefit from the\nadvantages of synthetic data: being fully controllable and fully annotated with\nany type of standard or custom-defined ground truth. To reduce the\nsynthetic-to-real domain gap, we introduce a pipeline consisting of the\nfollowing steps: 1) we render scenes in a context modeled after the real world,\n2) we train a human parsing model on the synthetic images, 3) we use the model\nto estimate segmentation maps for real images, 4) we train a conditional\ngenerative adversarial network (cGAN) to learn the inverse mapping -- from a\nsegmentation map to a real image, and 5) given new synthetic segmentation maps,\nwe use the cGAN to generate realistic images. An illustration of our pipeline\nis presented in Figure 2. We use the generated data to train a multi-task model\non the challenging tasks of UV mapping and dense depth estimation. We\ndemonstrate the value of the data generation and the trained model, both\nquantitatively and qualitatively on the CMU Panoptic Dataset.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 09:02:06 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Kviatkovsky", "Igor", ""], ["Bhonker", "Nadav", ""], ["Medioni", "Gerard", ""]]}, {"id": "2006.02158", "submitter": "Jisoo Jeong", "authors": "Jisoo Jeong, Vikas Verma, Minsung Hyun, Juho Kannala, Nojun Kwak", "title": "Interpolation-based semi-supervised learning for object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the data labeling cost for the object detection tasks being\nsubstantially more than that of the classification tasks, semi-supervised\nlearning methods for object detection have not been studied much. In this\npaper, we propose an Interpolation-based Semi-supervised learning method for\nobject Detection (ISD), which considers and solves the problems caused by\napplying conventional Interpolation Regularization (IR) directly to object\ndetection. We divide the output of the model into two types according to the\nobjectness scores of both original patches that are mixed in IR. Then, we apply\na separate loss suitable for each type in an unsupervised manner. The proposed\nlosses dramatically improve the performance of semi-supervised learning as well\nas supervised learning. In the supervised learning setting, our method improves\nthe baseline methods by a significant margin. In the semi-supervised learning\nsetting, our algorithm improves the performance on a benchmark dataset (PASCAL\nVOC and MSCOCO) in a benchmark architecture (SSD).\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 10:53:44 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 22:41:50 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Jeong", "Jisoo", ""], ["Verma", "Vikas", ""], ["Hyun", "Minsung", ""], ["Kannala", "Juho", ""], ["Kwak", "Nojun", ""]]}, {"id": "2006.02176", "submitter": "Lixiang Ru", "authors": "Lixiang Ru, Bo Du and Chen Wu", "title": "Multi-Temporal Scene Classification and Scene Change Detection with\n  Correlation based Fusion", "comments": "submitted", "journal-ref": null, "doi": "10.1109/TIP.2020.3039328", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying multi-temporal scene land-use categories and detecting their\nsemantic scene-level changes for imagery covering urban regions could\nstraightly reflect the land-use transitions. Existing methods for scene change\ndetection rarely focus on the temporal correlation of bi-temporal features, and\nare mainly evaluated on small scale scene change detection datasets. In this\nwork, we proposed a CorrFusion module that fuses the highly correlated\ncomponents in bi-temporal feature embeddings. We firstly extracts the deep\nrepresentations of the bi-temporal inputs with deep convolutional networks.\nThen the extracted features will be projected into a lower dimension space to\ncomputed the instance-level correlation. The cross-temporal fusion will be\nperformed based on the computed correlation in CorrFusion module. The final\nscene classification are obtained with softmax activation layers. In the\nobjective function, we introduced a new formulation for calculating the\ntemporal correlation. The detailed derivation of backpropagation gradients for\nthe proposed module is also given in this paper. Besides, we presented a much\nlarger scale scene change detection dataset and conducted experiments on this\ndataset. The experimental results demonstrated that our proposed CorrFusion\nmodule could remarkably improve the multi-temporal scene classification and\nscene change detection results.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 11:24:31 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Ru", "Lixiang", ""], ["Du", "Bo", ""], ["Wu", "Chen", ""]]}, {"id": "2006.02231", "submitter": "Suranga Seneviratne", "authors": "Naveen Karunanayake, Jathushan Rajasegaran, Ashanie Gunathillake,\n  Suranga Seneviratne, Guillaume Jourjon", "title": "A Multi-modal Neural Embeddings Approach for Detecting Mobile\n  Counterfeit Apps: A Case Study on Google Play Store", "comments": "arXiv admin note: substantial text overlap with arXiv:1804.09882", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfeit apps impersonate existing popular apps in attempts to misguide\nusers to install them for various reasons such as collecting personal\ninformation or spreading malware. Many counterfeits can be identified once\ninstalled, however even a tech-savvy user may struggle to detect them before\ninstallation. To this end, this paper proposes to leverage the recent advances\nin deep learning methods to create image and text embeddings so that\ncounterfeit apps can be efficiently identified when they are submitted for\npublication. We show that a novel approach of combining content embeddings and\nstyle embeddings outperforms the baseline methods for image similarity such as\nSIFT, SURF, and various image hashing methods. We first evaluate the\nperformance of the proposed method on two well-known datasets for evaluating\nimage similarity methods and show that content, style, and combined embeddings\nincrease precision@k and recall@k by 10%-15% and 12%-25%, respectively when\nretrieving five nearest neighbours. Second, specifically for the app\ncounterfeit detection problem, combined content and style embeddings achieve\n12% and 14% increase in precision@k and recall@k, respectively compared to the\nbaseline methods. Third, we present an analysis of approximately 1.2 million\napps from Google Play Store and identify a set of potential counterfeits for\ntop-10,000 popular apps. Under a conservative assumption, we were able to find\n2,040 potential counterfeits that contain malware in a set of 49,608 apps that\nshowed high similarity to one of the top-10,000 popular apps in Google Play\nStore. We also find 1,565 potential counterfeits asking for at least five\nadditional dangerous permissions than the original app and 1,407 potential\ncounterfeits having at least five extra third party advertisement libraries.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 07:10:21 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Karunanayake", "Naveen", ""], ["Rajasegaran", "Jathushan", ""], ["Gunathillake", "Ashanie", ""], ["Seneviratne", "Suranga", ""], ["Jourjon", "Guillaume", ""]]}, {"id": "2006.02271", "submitter": "Xiaozhou Lei", "authors": "Xiaozhou Lei, Minrui Fei, Wenju Zhou and Huiyu Zhou", "title": "Perceiving Unknown in Dark from Perspective of Cell Vibration", "comments": "13 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low light very likely leads to the degradation of image quality and even\ncauses visual tasks' failure. Existing image enhancement technologies are prone\nto over-enhancement or color distortion, and their adaptability is fairly\nlimited. In order to deal with these problems, we utilise the mechanism of\nbiological cell vibration to interpret the formation of color images. In\nparticular, we here propose a simple yet effective cell vibration energy (CVE)\nmapping method for image enhancement. Based on a hypothetical color-formation\nmechanism, our proposed method first uses cell vibration and photoreceptor\ncorrection to determine the photon flow energy for each color channel, and then\nreconstructs the color image with the maximum energy constraint of the visual\nsystem. Photoreceptor cells can adaptively adjust the feedback from the light\nintensity of the perceived environment. Based on this understanding, we here\npropose a new Gamma auto-adjustment method to modify Gamma values according to\nindividual images. Finally, a fusion method, combining CVE and Gamma\nauto-adjustment (CVE-G), is proposed to reconstruct the color image under the\nconstraint of lightness. Experimental results show that the proposed algorithm\nis superior to six state of the art methods in avoiding over-enhancement and\ncolor distortion, restoring the textures of dark areas and reproducing natural\ncolors. The source code will be released at\nhttps://github.com/leixiaozhou/CVE-G-Resource-Base.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 13:39:10 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Lei", "Xiaozhou", ""], ["Fei", "Minrui", ""], ["Zhou", "Wenju", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2006.02322", "submitter": "Aifu Han", "authors": "Aifu Han, Yongze Zhang, Ajuan Li, Changjin Li, Fengying Zhao, Qiujie\n  Dong, Qin Liu, Yanting Liu, Ximei Shen, Sunjie Yan and Shengzong Zhou", "title": "Efficient refinements on YOLOv3 for real-time detection and assessment\n  of diabetic foot Wagner grades", "comments": "11 pages with 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, the screening of Wagner grades of diabetic feet (DF) still relies\non professional podiatrists. However, in less-developed countries, podiatrists\nare scarce, which led to the majority of undiagnosed patients. In this study,\nwe proposed the real-time detection and location method for Wagner grades of DF\nbased on refinements on YOLOv3. We collected 2,688 data samples and implemented\nseveral methods, such as a visual coherent image mixup, label smoothing, and\ntraining scheduler revamping, based on the ablation study. The experimental\nresults suggested that the refinements on YOLOv3 achieved an accuracy of 91.95%\nand the inference speed of a single picture reaches 31ms with the NVIDIA Tesla\nV100. To test the performance of the model on a smartphone, we deployed the\nrefinements on YOLOv3 models on an Android 9 system smartphone. This work has\nthe potential to lead to a paradigm shift for clinical treatment of the DF in\nthe future, to provide an effective healthcare solution for DF tissue analysis\nand healing status.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 15:08:39 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 01:22:04 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Han", "Aifu", ""], ["Zhang", "Yongze", ""], ["Li", "Ajuan", ""], ["Li", "Changjin", ""], ["Zhao", "Fengying", ""], ["Dong", "Qiujie", ""], ["Liu", "Qin", ""], ["Liu", "Yanting", ""], ["Shen", "Ximei", ""], ["Yan", "Sunjie", ""], ["Zhou", "Shengzong", ""]]}, {"id": "2006.02330", "submitter": "Elif Vural", "authors": "Semih Kaya and Elif Vural", "title": "Learning Multi-Modal Nonlinear Embeddings: Performance Bounds and an\n  Algorithm", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3071688", "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many approaches exist in the literature to learn low-dimensional\nrepresentations for data collections in multiple modalities, the\ngeneralizability of multi-modal nonlinear embeddings to previously unseen data\nis a rather overlooked subject. In this work, we first present a theoretical\nanalysis of learning multi-modal nonlinear embeddings in a supervised setting.\nOur performance bounds indicate that for successful generalization in\nmulti-modal classification and retrieval problems, the regularity of the\ninterpolation functions extending the embedding to the whole data space is as\nimportant as the between-class separation and cross-modal alignment criteria.\nWe then propose a multi-modal nonlinear representation learning algorithm that\nis motivated by these theoretical findings, where the embeddings of the\ntraining samples are optimized jointly with the Lipschitz regularity of the\ninterpolators. Experimental comparison to recent multi-modal and single-modal\nlearning algorithms suggests that the proposed method yields promising\nperformance in multi-modal image classification and cross-modal image-text\nretrieval applications.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 15:22:16 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 22:01:04 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Kaya", "Semih", ""], ["Vural", "Elif", ""]]}, {"id": "2006.02333", "submitter": "Jakub Gwizda{\\l}a", "authors": "Alexandre Pierre Dherse, Martin Nicolas Everaert, Jakub Jan\n  Gwizda{\\l}a", "title": "Scene relighting with illumination estimation in the latent space on an\n  encoder-decoder scheme", "comments": "Report for the CS-413 project at EPFL, Switzerland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The image relighting task of transferring illumination conditions between two\nimages offers an interesting and difficult challenge with potential\napplications in photography, cinematography and computer graphics. In this\nreport we present methods that we tried to achieve that goal. Our models are\ntrained on a rendered dataset of artificial locations with varied scene\ncontent, light source location and color temperature. With this dataset, we\nused a network with illumination estimation component aiming to infer and\nreplace light conditions in the latent space representation of the concerned\nscenes.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 15:25:11 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Dherse", "Alexandre Pierre", ""], ["Everaert", "Martin Nicolas", ""], ["Gwizda\u0142a", "Jakub Jan", ""]]}, {"id": "2006.02334", "submitter": "Siyuan Qiao", "authors": "Siyuan Qiao, Liang-Chieh Chen, Alan Yuille", "title": "DetectoRS: Detecting Objects with Recursive Feature Pyramid and\n  Switchable Atrous Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern object detectors demonstrate outstanding performances by using\nthe mechanism of looking and thinking twice. In this paper, we explore this\nmechanism in the backbone design for object detection. At the macro level, we\npropose Recursive Feature Pyramid, which incorporates extra feedback\nconnections from Feature Pyramid Networks into the bottom-up backbone layers.\nAt the micro level, we propose Switchable Atrous Convolution, which convolves\nthe features with different atrous rates and gathers the results using switch\nfunctions. Combining them results in DetectoRS, which significantly improves\nthe performances of object detection. On COCO test-dev, DetectoRS achieves\nstate-of-the-art 55.7% box AP for object detection, 48.5% mask AP for instance\nsegmentation, and 50.0% PQ for panoptic segmentation. The code is made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 15:28:16 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 16:06:38 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Qiao", "Siyuan", ""], ["Chen", "Liang-Chieh", ""], ["Yuille", "Alan", ""]]}, {"id": "2006.02338", "submitter": "Mikael Brudfors", "authors": "Mikael Brudfors, Ya\\\"el Balbastre, Guillaume Flandin, Parashkev\n  Nachev, John Ashburner", "title": "Flexible Bayesian Modelling for Nonlinear Image Registration", "comments": "Accepted for MICCAI 2020", "journal-ref": null, "doi": "10.1007/978-3-030-59716-0_25", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a diffeomorphic registration algorithm that allows groups of\nimages to be accurately aligned to a common space, which we intend to\nincorporate into the SPM software. The idea is to perform inference in a\nprobabilistic graphical model that accounts for variability in both shape and\nappearance. The resulting framework is general and entirely unsupervised. The\nmodel is evaluated at inter-subject registration of 3D human brain scans. Here,\nthe main modeling assumption is that individual anatomies can be generated by\ndeforming a latent 'average' brain. The method is agnostic to imaging modality\nand can be applied with no prior processing. We evaluate the algorithm using\nfreely available, manually labelled datasets. In this validation we achieve\nstate-of-the-art results, within reasonable runtimes, against previous\nstate-of-the-art widely used, inter-subject registration algorithms. On the\nunprocessed dataset, the increase in overlap score is over 17%. These results\ndemonstrate the benefits of using informative computational anatomy frameworks\nfor nonlinear registration.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 15:33:14 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Brudfors", "Mikael", ""], ["Balbastre", "Ya\u00ebl", ""], ["Flandin", "Guillaume", ""], ["Nachev", "Parashkev", ""], ["Ashburner", "John", ""]]}, {"id": "2006.02379", "submitter": "Juli\\'an Tachella Dr", "authors": "Juli\\'an Tachella and Junqi Tang and Mike Davies", "title": "The Neural Tangent Link Between CNN Denoisers and Non-Local Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are now a well-established tool for\nsolving computational imaging problems. Modern CNN-based algorithms obtain\nstate-of-the-art performance in diverse image restoration problems.\nFurthermore, it has been recently shown that, despite being highly\noverparameterized, networks trained with a single corrupted image can still\nperform as well as fully trained networks. We introduce a formal link between\nsuch networks through their neural tangent kernel (NTK), and well-known\nnon-local filtering techniques, such as non-local means or BM3D. The filtering\nfunction associated with a given network architecture can be obtained in closed\nform without need to train the network, being fully characterized by the random\ninitialization of the network weights. While the NTK theory accurately predicts\nthe filter associated with networks trained using standard gradient descent,\nour analysis shows that it falls short to explain the behaviour of networks\ntrained using the popular Adam optimizer. The latter achieves a larger change\nof weights in hidden layers, adapting the non-local filtering function during\ntraining. We evaluate our findings via extensive image denoising experiments.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 16:50:54 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 16:02:57 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 16:41:44 GMT"}, {"version": "v4", "created": "Mon, 16 Nov 2020 23:06:53 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Tachella", "Juli\u00e1n", ""], ["Tang", "Junqi", ""], ["Davies", "Mike", ""]]}, {"id": "2006.02380", "submitter": "QiKui Zhu", "authors": "Qikui Zhu, Bo Du, Pingkun Yan", "title": "Self-supervised Training of Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have been successfully applied to analyze\nnon-grid data, where the classical convolutional neural networks (CNNs) cannot\nbe directly used. One similarity shared by GCNs and CNNs is the requirement of\nmassive amount of labeled data for network training. In addition, GCNs need the\nadjacency matrix as input to define the relationship between those non-grid\ndata, which leads to all of data including training, validation and test data\ntypically forms only one graph structures data for training. Furthermore, the\nadjacency matrix is usually pre-defined and stationary, which makes the data\naugmentation strategies cannot be employed on the constructed graph structures\ndata to augment the amount of training data. To further improve the learning\ncapacity and model performance under the limited training data, in this paper,\nwe propose two types of self-supervised learning strategies to exploit\navailable information from the input graph structure data itself. Our proposed\nself-supervised learning strategies are examined on two representative GCN\nmodels with three public citation network datasets - Citeseer, Cora and Pubmed.\nThe experimental results demonstrate the generalization ability as well as the\nportability of our proposed strategies, which can significantly improve the\nperformance of GCNs with the power of self-supervised learning in improving\nfeature learning.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 16:53:37 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Zhu", "Qikui", ""], ["Du", "Bo", ""], ["Yan", "Pingkun", ""]]}, {"id": "2006.02413", "submitter": "Lokender Tiwari", "authors": "Lokender Tiwari and Saket Anand", "title": "DGSAC: Density Guided Sampling and Consensus", "comments": "Working article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust multiple model fitting plays a crucial role in many computer vision\napplications. Unlike single model fitting problems, the multi-model fitting has\nadditional challenges. The unknown number of models and the inlier noise scale\nare the two most important of them, which are in general provided by the user\nusing ground-truth or some other auxiliary information. Mode seeking/\nclustering-based approaches crucially depend on the quality of model hypotheses\ngenerated. While preference analysis based guided sampling approaches have\nshown remarkable performance, they operate in a time budget framework, and the\nuser provides the time as a reasonable guess. In this paper, we deviate from\nthe mode seeking and time budget framework. We propose a concept called Kernel\nResidual Density (KRD) and apply it to various components of a multiple-model\nfitting pipeline. The Kernel Residual Density act as a key differentiator\nbetween inliers and outliers. We use KRD to guide and automatically stop the\nsampling process. The sampling process stops after generating a set of\nhypotheses that can explain all the data points. An explanation score is\nmaintained for each data point, which is updated on-the-fly. We propose two\nmodel selection algorithms, an optimal quadratic program based, and a greedy.\nUnlike mode seeking approaches, our model selection algorithms seek to find one\nrepresentative hypothesis for each genuine structure present in the data. We\nevaluate our method (dubbed as DGSAC) on a wide variety of tasks like planar\nsegmentation, motion segmentation, vanishing point estimation, plane fitting to\n3D point cloud, line, and circle fitting, which shows the effectiveness of our\nmethod and its unified nature.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 17:42:53 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Tiwari", "Lokender", ""], ["Anand", "Saket", ""]]}, {"id": "2006.02434", "submitter": "Mohammad Rajiur Rahman", "authors": "Mohammad Rajiur Rahman, Jaspal Subhlok and Shishir Shah", "title": "Visual Summarization of Lecture Video Segments for Enhanced Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lecture videos are an increasingly important learning resource for higher\neducation. However, the challenge of quickly finding the content of interest in\na lecture video is an important limitation of this format. This paper\nintroduces visual summarization of lecture video segments to enhance\nnavigation. A lecture video is divided into segments based on the\nframe-to-frame similarity of content. The user navigates the lecture video\ncontent by viewing a single frame visual and textual summary of each segment.\nThe paper presents a novel methodology to generate the visual summary of a\nlecture video segment by computing similarities between images extracted from\nthe segment and employing a graph-based algorithm to identify the subset of\nmost representative images. The results from this research are integrated into\na real-world lecture video management portal called Videopoints. To collect\nground truth for evaluation, a survey was conducted where multiple users\nmanually provided visual summaries for 40 lecture video segments. The users\nalso stated whether any images were not selected for the summary because they\nwere similar to other selected images. The graph based algorithm for\nidentifying summary images achieves 78% precision and 72% F1-measure with\nfrequently selected images as the ground truth, and 94% precision and 72%\nF1-measure with the union of all user selected images as the ground truth. For\n98% of algorithm selected visual summary images, at least one user also\nselected that image for their summary or considered it similar to another image\nthey selected. Over 65% of automatically generated summaries were rated as good\nor very good by the users on a 4-point scale from poor to very good. Overall,\nthe results establish that the methodology introduced in this paper produces\ngood quality visual summaries that are practically useful for lecture video\nnavigation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 16:53:54 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Rahman", "Mohammad Rajiur", ""], ["Subhlok", "Jaspal", ""], ["Shah", "Shishir", ""]]}, {"id": "2006.02474", "submitter": "Yuankai Huo", "authors": "Haichun Yang, Ruining Deng, Yuzhe Lu, Zheyu Zhu, Ye Chen, Joseph T.\n  Roland, Le Lu, Bennett A. Landman, Agnes B. Fogo, Yuankai Huo", "title": "CircleNet: Anchor-free Detection with Circle Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection networks are powerful in computer vision, but not\nnecessarily optimized for biomedical object detection. In this work, we propose\nCircleNet, a simple anchor-free detection method with circle representation for\ndetection of the ball-shaped glomerulus. Different from the traditional\nbounding box based detection method, the bounding circle (1) reduces the\ndegrees of freedom of detection representation, (2) is naturally rotation\ninvariant, (3) and optimized for ball-shaped objects. The key innovation to\nenable this representation is the anchor-free framework with the circle\ndetection head. We evaluate CircleNet in the context of detection of\nglomerulus. CircleNet increases average precision of the glomerulus detection\nfrom 0.598 to 0.647. Another key advantage is that CircleNet achieves better\nrotation consistency compared with bounding box representations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 18:31:51 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Yang", "Haichun", ""], ["Deng", "Ruining", ""], ["Lu", "Yuzhe", ""], ["Zhu", "Zheyu", ""], ["Chen", "Ye", ""], ["Roland", "Joseph T.", ""], ["Lu", "Le", ""], ["Landman", "Bennett A.", ""], ["Fogo", "Agnes B.", ""], ["Huo", "Yuankai", ""]]}, {"id": "2006.02535", "submitter": "Hamid Laga", "authors": "Hamid Laga, Laurent Valentin Jospin, Farid Boussaid, Mohammed\n  Bennamoun", "title": "A Survey on Deep Learning Techniques for Stereo-based Depth Estimation", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2020", "doi": "10.1109/TPAMI.2020.3032602", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating depth from RGB images is a long-standing ill-posed problem, which\nhas been explored for decades by the computer vision, graphics, and machine\nlearning communities. Among the existing techniques, stereo matching remains\none of the most widely used in the literature due to its strong connection to\nthe human binocular system. Traditionally, stereo-based depth estimation has\nbeen addressed through matching hand-crafted features across multiple images.\nDespite the extensive amount of research, these traditional techniques still\nsuffer in the presence of highly textured areas, large uniform regions, and\nocclusions. Motivated by their growing success in solving various 2D and 3D\nvision problems, deep learning for stereo-based depth estimation has attracted\ngrowing interest from the community, with more than 150 papers published in\nthis area between 2014 and 2019. This new generation of methods has\ndemonstrated a significant leap in performance, enabling applications such as\nautonomous driving and augmented reality. In this article, we provide a\ncomprehensive survey of this new and continuously growing field of research,\nsummarize the most commonly used pipelines, and discuss their benefits and\nlimitations. In retrospect of what has been achieved so far, we also conjecture\nwhat the future may hold for deep learning-based stereo for depth estimation\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 13:09:46 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Laga", "Hamid", ""], ["Jospin", "Laurent Valentin", ""], ["Boussaid", "Farid", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "2006.02536", "submitter": "Loris Nanni", "authors": "Luca Patarnello, Marco Celin, Loris Nanni", "title": "Phasic dopamine release identification using ensemble of AlexNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dopamine (DA) is an organic chemical that influences several parts of\nbehaviour and physical functions. Fast-scan cyclic voltammetry (FSCV) is a\ntechnique used for in vivo phasic dopamine release measurements. The analysis\nof such measurements, though, requires notable effort. In this paper, we\npresent the use of convolutional neural networks (CNNs) for the identification\nof phasic dopamine releases.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 21:13:05 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Patarnello", "Luca", ""], ["Celin", "Marco", ""], ["Nanni", "Loris", ""]]}, {"id": "2006.02569", "submitter": "Yukun Guo", "authors": "Yukun Guo, Tristan T. Hormel, Honglian Xiong, Jie Wang, Thomas S.\n  Hwang, Yali Jia", "title": "Automated segmentation of retinal fluid volumes from structural and\n  angiographic optical coherence tomography using deep learning", "comments": null, "journal-ref": null, "doi": "10.1167/tvst.9.2.54", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: We proposed a deep convolutional neural network (CNN), named Retinal\nFluid Segmentation Network (ReF-Net) to segment volumetric retinal fluid on\noptical coherence tomography (OCT) volume. Methods: 3 x 3-mm OCT scans were\nacquired on one eye by a 70-kHz OCT commercial AngioVue system (RTVue-XR;\nOptovue, Inc.) from 51 participants in a clinical diabetic retinopathy (DR)\nstudy (45 with retinal edema and 6 healthy controls). A CNN with U-Net-like\narchitecture was constructed to detect and segment the retinal fluid.\nCross-sectional OCT and angiography (OCTA) scans were used for training and\ntesting ReF-Net. The effect of including OCTA data for retinal fluid\nsegmentation was investigated in this study. Volumetric retinal fluid can be\nconstructed using the output of ReF-Net.\nArea-under-Receiver-Operating-Characteristic-curve (AROC),\nintersection-over-union (IoU), and F1-score were calculated to evaluate the\nperformance of ReF-Net. Results: ReF-Net shows high accuracy (F1 = 0.864 +/-\n0.084) in retinal fluid segmentation. The performance can be further improved\n(F1 = 0.892 +/- 0.038) by including information from both OCTA and structural\nOCT. ReF-Net also shows strong robustness to shadow artifacts. Volumetric\nretinal fluid can provide more comprehensive information than the 2D area,\nwhether cross-sectional or en face projections. Conclusions: A\ndeep-learning-based method can accurately segment retinal fluid volumetrically\non OCT/OCTA scans with strong robustness to shadow artifacts. OCTA data can\nimprove retinal fluid segmentation. Volumetric representations of retinal fluid\nare superior to 2D projections. Translational Relevance: Using a deep learning\nmethod to segment retinal fluid volumetrically has the potential to improve the\ndiagnostic accuracy of diabetic macular edema by OCT systems.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 22:55:47 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Guo", "Yukun", ""], ["Hormel", "Tristan T.", ""], ["Xiong", "Honglian", ""], ["Wang", "Jie", ""], ["Hwang", "Thomas S.", ""], ["Jia", "Yali", ""]]}, {"id": "2006.02570", "submitter": "Soumick Chatterjee", "authors": "Soumick Chatterjee, Fatima Saad, Chompunuch Sarasaen, Suhita Ghosh,\n  Rupali Khatun, Petia Radeva, Georg Rose, Sebastian Stober, Oliver Speck,\n  Andreas N\\\"urnberger", "title": "Exploration of Interpretability Techniques for Deep COVID-19\n  Classification using Chest X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of COVID-19 has shocked the entire world with its fairly rapid\nspread and has challenged different sectors. One of the most effective ways to\nlimit its spread is the early and accurate diagnosis of infected patients.\nMedical imaging such as X-ray and Computed Tomography (CT) combined with the\npotential of Artificial Intelligence (AI) plays an essential role in supporting\nthe medical staff in the diagnosis process. Thereby, the use of five different\ndeep learning models (ResNet18, ResNet34, InceptionV3, InceptionResNetV2, and\nDenseNet161) and their Ensemble have been used in this paper, to classify\nCOVID-19, pneumoni{\\ae} and healthy subjects using Chest X-Ray. Multi-label\nclassification was performed to predict multiple pathologies for each patient,\nif present. Foremost, the interpretability of each of the networks was\nthoroughly studied using techniques like occlusion, saliency, input X gradient,\nguided backpropagation, integrated gradients, and DeepLIFT. The mean Micro-F1\nscore of the models for COVID-19 classifications ranges from 0.66 to 0.875, and\nis 0.89 for the Ensemble of the network models. The qualitative results\ndepicted the ResNets to be the most interpretable model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 22:55:53 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 16:01:34 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Chatterjee", "Soumick", ""], ["Saad", "Fatima", ""], ["Sarasaen", "Chompunuch", ""], ["Ghosh", "Suhita", ""], ["Khatun", "Rupali", ""], ["Radeva", "Petia", ""], ["Rose", "Georg", ""], ["Stober", "Sebastian", ""], ["Speck", "Oliver", ""], ["N\u00fcrnberger", "Andreas", ""]]}, {"id": "2006.02578", "submitter": "Md. Kamrul Hasan Dr.", "authors": "Sabbir Ahmed, Uday Kamal, Md. Kamrul Hasan", "title": "DFR-TSD: A Deep Learning Based Framework for Robust Traffic Sign\n  Detection Under Challenging Weather Conditions", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.18341.86249", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust traffic sign detection and recognition (TSDR) is of paramount\nimportance for the successful realization of autonomous vehicle technology. The\nimportance of this task has led to a vast amount of research efforts and many\npromising methods have been proposed in the existing literature. However, the\nSOTA (SOTA) methods have been evaluated on clean and challenge-free datasets\nand overlooked the performance deterioration associated with different\nchallenging conditions (CCs) that obscure the traffic images captured in the\nwild. In this paper, we look at the TSDR problem under CCs and focus on the\nperformance degradation associated with them. To overcome this, we propose a\nConvolutional Neural Network (CNN) based TSDR framework with prior enhancement.\nOur modular approach consists of a CNN-based challenge classifier, Enhance-Net,\nan encoder-decoder CNN architecture for image enhancement, and two separate CNN\narchitectures for sign-detection and classification. We propose a novel\ntraining pipeline for Enhance-Net that focuses on the enhancement of the\ntraffic sign regions (instead of the whole image) in the challenging images\nsubject to their accurate detection. We used CURE-TSD dataset consisting of\ntraffic videos captured under different CCs to evaluate the efficacy of our\napproach. We experimentally show that our method obtains an overall precision\nand recall of 91.1% and 70.71% that is 7.58% and 35.90% improvement in\nprecision and recall, respectively, compared to the current benchmark.\nFurthermore, we compare our approach with SOTA object detection networks,\nFaster-RCNN and R-FCN, and show that our approach outperforms them by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 23:12:26 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Ahmed", "Sabbir", ""], ["Kamal", "Uday", ""], ["Hasan", "Md. Kamrul", ""]]}, {"id": "2006.02595", "submitter": "Zhengli Zhao", "authors": "Zhengli Zhao, Zizhao Zhang, Ting Chen, Sameer Singh, Han Zhang", "title": "Image Augmentations for GAN Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentations have been widely studied to improve the accuracy and\nrobustness of classifiers. However, the potential of image augmentation in\nimproving GAN models for image synthesis has not been thoroughly investigated\nin previous studies. In this work, we systematically study the effectiveness of\nvarious existing augmentation techniques for GAN training in a variety of\nsettings. We provide insights and guidelines on how to augment images for both\nvanilla GANs and GANs with regularizations, improving the fidelity of the\ngenerated images substantially. Surprisingly, we find that vanilla GANs attain\ngeneration quality on par with recent state-of-the-art results if we use\naugmentations on both real and generated images. When this GAN training is\ncombined with other augmentation-based regularization techniques, such as\ncontrastive loss and consistency regularization, the augmentations further\nimprove the quality of generated images. We provide new state-of-the-art\nresults for conditional generation on CIFAR-10 with both consistency loss and\ncontrastive loss as additional regularizations.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 00:16:02 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Zhao", "Zhengli", ""], ["Zhang", "Zizhao", ""], ["Chen", "Ting", ""], ["Singh", "Sameer", ""], ["Zhang", "Han", ""]]}, {"id": "2006.02597", "submitter": "Seyed Mojtaba Marvasti-Zadeh", "authors": "Seyed Mojtaba Marvasti-Zadeh, Javad Khaghani, Hossein Ghanei-Yakhdan,\n  Shohreh Kasaei, and Li Cheng", "title": "COMET: Context-Aware IoU-Guided Network for Small Object Tracking", "comments": "Accepted manuscript in ACCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of tracking an unknown small target from aerial\nvideos of medium to high altitudes. This is a challenging problem, which is\neven more pronounced in unavoidable scenarios of drastic camera motion and high\ndensity. To address this problem, we introduce a context-aware IoU-guided\ntracker (COMET) that exploits a multitask two-stream network and an offline\nreference proposal generation strategy. The proposed network fully exploits\ntarget-related information by multi-scale feature learning and attention\nmodules. The proposed strategy introduces an efficient sampling strategy to\ngeneralize the network on the target and its parts without imposing extra\ncomputational complexity during online tracking. These strategies contribute\nconsiderably in handling significant occlusions and viewpoint changes.\nEmpirically, COMET outperforms the state-of-the-arts in a range of aerial view\ndatasets that focusing on tracking small objects. Specifically, COMET\noutperforms the celebrated ATOM tracker by an average margin of 6.2% (and 7%)\nin precision (and success) score on challenging benchmarks of UAVDT,\nVisDrone-2019, and Small-90.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 00:28:45 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 19:56:58 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 15:04:26 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Marvasti-Zadeh", "Seyed Mojtaba", ""], ["Khaghani", "Javad", ""], ["Ghanei-Yakhdan", "Hossein", ""], ["Kasaei", "Shohreh", ""], ["Cheng", "Li", ""]]}, {"id": "2006.02598", "submitter": "Aditya Sanghi", "authors": "Aditya Sanghi", "title": "Info3D: Representation Learning on 3D Objects using Mutual Information\n  Maximization and Contrastive Learning", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major endeavor of computer vision is to represent, understand and extract\nstructure from 3D data. Towards this goal, unsupervised learning is a powerful\nand necessary tool. Most current unsupervised methods for 3D shape analysis use\ndatasets that are aligned, require objects to be reconstructed and suffer from\ndeteriorated performance on downstream tasks. To solve these issues, we propose\nto extend the InfoMax and contrastive learning principles on 3D shapes. We show\nthat we can maximize the mutual information between 3D objects and their\n\"chunks\" to improve the representations in aligned datasets. Furthermore, we\ncan achieve rotation invariance in SO${(3)}$ group by maximizing the mutual\ninformation between the 3D objects and their geometric transformed versions.\nFinally, we conduct several experiments such as clustering, transfer learning,\nshape retrieval, and achieve state of art results.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 00:30:26 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 22:12:57 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Sanghi", "Aditya", ""]]}, {"id": "2006.02609", "submitter": "Shyamgopal Karthik", "authors": "Shyamgopal Karthik, Ameya Prabhu, Vineet Gandhi", "title": "Simple Unsupervised Multi-Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-object tracking has seen a lot of progress recently, albeit with\nsubstantial annotation costs for developing better and larger labeled datasets.\nIn this work, we remove the need for annotated datasets by proposing an\nunsupervised re-identification network, thus sidestepping the labeling costs\nentirely, required for training. Given unlabeled videos, our proposed method\n(SimpleReID) first generates tracking labels using SORT and trains a ReID\nnetwork to predict the generated labels using crossentropy loss. We demonstrate\nthat SimpleReID performs substantially better than simpler alternatives, and we\nrecover the full performance of its supervised counterpart consistently across\ndiverse tracking frameworks. The observations are unusual because unsupervised\nReID is not expected to excel in crowded scenarios with occlusions, and drastic\nviewpoint changes. By incorporating our unsupervised SimpleReID with\nCenterTrack trained on augmented still images, we establish a new\nstate-of-the-art performance on popular datasets like MOT16/17 without using\ntracking supervision, beating current best (CenterTrack) by 0.2-0.3 MOTA and\n4.4-4.8 IDF1 scores. We further provide evidence for limited scope for\nimprovement in IDF1 scores beyond our unsupervised ReID in the studied\nsettings. Our investigation suggests reconsideration towards more\nsophisticated, supervised, end-to-end trackers by showing promise in simpler\nunsupervised alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 01:53:18 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Karthik", "Shyamgopal", ""], ["Prabhu", "Ameya", ""], ["Gandhi", "Vineet", ""]]}, {"id": "2006.02610", "submitter": "Xulei Yang", "authors": "Balagopal Unnikrishnan, Pranshu Ranjan Singh, Xulei Yang, and Matthew\n  Chin Heng Chua", "title": "Semi-supervised and Unsupervised Methods for Heart Sounds Classification\n  in Restricted Data Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated heart sounds classification is a much-required diagnostic tool in\nthe view of increasing incidences of heart related diseases worldwide. In this\nstudy, we conduct a comprehensive study of heart sounds classification by using\nvarious supervised, semi-supervised and unsupervised approaches on the\nPhysioNet/CinC 2016 Challenge dataset. Supervised approaches, including deep\nlearning and machine learning methods, require large amounts of labelled data\nto train the models, which are challenging to obtain in most practical\nscenarios. In view of the need to reduce the labelling burden for clinical\npractices, where human labelling is both expensive and time-consuming,\nsemi-supervised or even unsupervised approaches in restricted data setting are\ndesirable. A GAN based semi-supervised method is therefore proposed, which\nallows the usage of unlabelled data samples to boost the learning of data\ndistribution. It achieves a better performance in terms of AUROC over the\nsupervised baseline when limited data samples exist. Furthermore, several\nunsupervised methods are explored as an alternative approach by considering the\ngiven problem as an anomaly detection scenario. In particular, the unsupervised\nfeature extraction using 1D CNN Autoencoder coupled with one-class SVM obtains\ngood performance without any data labelling. The potential of the proposed\nsemi-supervised and unsupervised methods may lead to a workflow tool in the\nfuture for the creation of higher quality datasets.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 02:07:35 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Unnikrishnan", "Balagopal", ""], ["Singh", "Pranshu Ranjan", ""], ["Yang", "Xulei", ""], ["Chua", "Matthew Chin Heng", ""]]}, {"id": "2006.02620", "submitter": "Sai Hemanth Kasaraneni", "authors": "Sai Hemanth Kasaraneni, Abhishek Mishra", "title": "Image Completion and Extrapolation with Contextual Cycle Consistency", "comments": "This paper has been accepted to 2020 IEEE International Conference on\n  Image Processing (ICIP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Completion refers to the task of filling in the missing regions of an\nimage and Image Extrapolation refers to the task of extending an image at its\nboundaries while keeping it coherent. Many recent works based on GAN have shown\nprogress in addressing these problem statements but lack adaptability for these\ntwo cases, i.e. the neural network trained for the completion of interior\nmasked images does not generalize well for extrapolating over the boundaries\nand vice-versa. In this paper, we present a technique to train both completion\nand extrapolation networks concurrently while benefiting each other. We\ndemonstrate our method's efficiency in completing large missing regions and we\nshow the comparisons with the contemporary state of the art baseline.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 02:40:04 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Kasaraneni", "Sai Hemanth", ""], ["Mishra", "Abhishek", ""]]}, {"id": "2006.02627", "submitter": "Sara Ranjbar", "authors": "Sara Ranjbar (1), Kyle W. Singleton (1), Lee Curtin (1), Cassandra R.\n  Rickertsen (1), Lisa E. Paulson (1), Leland S. Hu (1,2), J. Ross Mitchell\n  (3), Kristin R. Swanson (1) ((1) Mathematical NeuroOncology Lab, Precision\n  Neurotherapeutics Innovation Program, Department of Neurological Surgery,\n  Mayo Clinic, Phoenix, AZ, USA, (2) Department of Diagnostic Imaging and\n  Interventional Radiology, Mayo Clinic, Phoenix, AZ, USA, (3) Department of\n  Biostatistics and Bioinformatics, Moffitt Cancer Center and Research\n  Institute, Tampa, Florida, USA)", "title": "Robust Automatic Whole Brain Extraction on Magnetic Resonance Imaging of\n  Brain Tumor Patients using Dense-Vnet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole brain extraction, also known as skull stripping, is a process in\nneuroimaging in which non-brain tissue such as skull, eyeballs, skin, etc. are\nremoved from neuroimages. Skull striping is a preliminary step in presurgical\nplanning, cortical reconstruction, and automatic tumor segmentation. Despite a\nplethora of skull stripping approaches in the literature, few are sufficiently\naccurate for processing pathology-presenting MRIs, especially MRIs with brain\ntumors. In this work we propose a deep learning approach for skull striping\ncommon MRI sequences in oncology such as T1-weighted with gadolinium contrast\n(T1Gd) and T2-weighted fluid attenuated inversion recovery (FLAIR) in patients\nwith brain tumors. We automatically created gray matter, white matter, and CSF\nprobability masks using SPM12 software and merged the masks into one for a\nfinal whole-brain mask for model training. Dice agreement, sensitivity, and\nspecificity of the model (referred herein as DeepBrain) was tested against\nmanual brain masks. To assess data efficiency, we retrained our models using\nprogressively fewer training data examples and calculated average dice scores\non the test set for the models trained in each round. Further, we tested our\nmodel against MRI of healthy brains from the LBP40A dataset. Overall, DeepBrain\nyielded an average dice score of 94.5%, sensitivity of 96.4%, and specificity\nof 98.5% on brain tumor data. For healthy brains, model performance improved to\na dice score of 96.2%, sensitivity of 96.6% and specificity of 99.2%. The data\nefficiency experiment showed that, for this specific task, comparable levels of\naccuracy could have been achieved with as few as 50 training samples. In\nconclusion, this study demonstrated that a deep learning model trained on\nminimally processed automatically-generated labels can generate more accurate\nbrain masks on MRI of brain tumor patients within seconds.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 03:18:43 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Ranjbar", "Sara", ""], ["Singleton", "Kyle W.", ""], ["Curtin", "Lee", ""], ["Rickertsen", "Cassandra R.", ""], ["Paulson", "Lisa E.", ""], ["Hu", "Leland S.", ""], ["Mitchell", "J. Ross", ""], ["Swanson", "Kristin R.", ""]]}, {"id": "2006.02631", "submitter": "He Lingxiao", "authors": "Lingxiao He, Xingyu Liao, Wu Liu, Xinchen Liu, Peng Cheng and Tao Mei", "title": "FastReID: A Pytorch Toolbox for General Instance Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General Instance Re-identification is a very important task in the computer\nvision, which can be widely used in many practical applications, such as\nperson/vehicle re-identification, face recognition, wildlife protection,\ncommodity tracing, and snapshop, etc.. To meet the increasing application\ndemand for general instance re-identification, we present FastReID as a widely\nused software system in JD AI Research. In FastReID, highly modular and\nextensible design makes it easy for the researcher to achieve new research\nideas. Friendly manageable system configuration and engineering deployment\nfunctions allow practitioners to quickly deploy models into productions. We\nhave implemented some state-of-the-art projects, including person re-id,\npartial re-id, cross-domain re-id and vehicle re-id, and plan to release these\npre-trained models on multiple benchmark datasets. FastReID is by far the most\ngeneral and high-performance toolbox that supports single and multiple GPU\nservers, you can reproduce our project results very easily and are very welcome\nto use it, the code and models are available at\nhttps://github.com/JDAI-CV/fast-reid.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 03:51:43 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 02:49:14 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 03:39:32 GMT"}, {"version": "v4", "created": "Wed, 15 Jul 2020 03:33:02 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["He", "Lingxiao", ""], ["Liao", "Xingyu", ""], ["Liu", "Wu", ""], ["Liu", "Xinchen", ""], ["Cheng", "Peng", ""], ["Mei", "Tao", ""]]}, {"id": "2006.02635", "submitter": "Haoyang Huang", "authors": "Minheng Ni, Haoyang Huang, Lin Su, Edward Cui, Taroon Bharti, Lijuan\n  Wang, Jianfeng Gao, Dongdong Zhang and Nan Duan", "title": "M3P: Learning Universal Representations via Multitask Multilingual\n  Multimodal Pre-training", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present M3P, a Multitask Multilingual Multimodal Pre-trained model that\ncombines multilingual pre-training and multimodal pre-training into a unified\nframework via multitask pre-training. Our goal is to learn universal\nrepresentations that can map objects occurred in different modalities or texts\nexpressed in different languages into a common semantic space. In addition, to\nexplicitly encourage fine-grained alignment between images and non-English\nlanguages, we also propose Multimodal Code-switched Training (MCT) to combine\nmonolingual pre-training and multimodal pre-training via a code-switch\nstrategy. Experiments are performed on the multilingual image retrieval task\nacross two benchmark datasets, including MSCOCO and Multi30K. M3P can achieve\ncomparable results for English and new state-of-the-art results for non-English\nlanguages.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 03:54:29 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 04:58:59 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 07:53:52 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 03:43:53 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Ni", "Minheng", ""], ["Huang", "Haoyang", ""], ["Su", "Lin", ""], ["Cui", "Edward", ""], ["Bharti", "Taroon", ""], ["Wang", "Lijuan", ""], ["Gao", "Jianfeng", ""], ["Zhang", "Dongdong", ""], ["Duan", "Nan", ""]]}, {"id": "2006.02636", "submitter": "Sergio Casas", "authors": "Sergio Casas, Cole Gulino, Simon Suo, Raquel Urtasun", "title": "The Importance of Prior Knowledge in Precise Multimodal Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roads have well defined geometries, topologies, and traffic rules. While this\nhas been widely exploited in motion planning methods to produce maneuvers that\nobey the law, little work has been devoted to utilize these priors in\nperception and motion forecasting methods. In this paper we propose to\nincorporate these structured priors as a loss function. In contrast to imposing\nhard constraints, this approach allows the model to handle non-compliant\nmaneuvers when those happen in the real world. Safe motion planning is the end\ngoal, and thus a probabilistic characterization of the possible future\ndevelopments of the scene is key to choose the plan with the lowest expected\ncost. Towards this goal, we design a framework that leverages REINFORCE to\nincorporate non-differentiable priors over sample trajectories from a\nprobabilistic model, thus optimizing the whole distribution. We demonstrate the\neffectiveness of our approach on real-world self-driving datasets containing\ncomplex road topologies and multi-agent interactions. Our motion forecasts not\nonly exhibit better precision and map understanding, but most importantly\nresult in safer motion plans taken by our self-driving vehicle. We emphasize\nthat despite the importance of this evaluation, it has been often overlooked by\nprevious perception and motion forecasting works.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 03:56:11 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Casas", "Sergio", ""], ["Gulino", "Cole", ""], ["Suo", "Simon", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2006.02659", "submitter": "Qing Yang", "authors": "Qing Yang, Xia Zhu, Jong-Kae Fwu, Yun Ye, Ganmei You and Yuan Zhu", "title": "MFPP: Morphological Fragmental Perturbation Pyramid for Black-Box Model\n  Explanations", "comments": "Accepted by 25th International Conference on Pattern Recognition\n  (ICPR2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks (DNNs) have recently been applied and used in many\nadvanced and diverse tasks, such as medical diagnosis, automatic driving, etc.\nDue to the lack of transparency of the deep models, DNNs are often criticized\nfor their prediction that cannot be explainable by human. In this paper, we\npropose a novel Morphological Fragmental Perturbation Pyramid (MFPP) method to\nsolve the Explainable AI problem. In particular, we focus on the black-box\nscheme, which can identify the input area that is responsible for the output of\nthe DNN without having to understand the internal architecture of the DNN. In\nthe MFPP method, we divide the input image into multi-scale fragments and\nrandomly mask out fragments as perturbation to generate a saliency map, which\nindicates the significance of each pixel for the prediction result of the black\nbox model. Compared with the existing input sampling perturbation method, the\npyramid structure fragment has proved to be more effective. It can better\nexplore the morphological information of the input image to match its semantic\ninformation, and does not need any value inside the DNN. We qualitatively and\nquantitatively prove that MFPP meets and exceeds the performance of\nstate-of-the-art (SOTA) black-box interpretation method on multiple DNN models\nand datasets.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 06:13:40 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 07:28:11 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 06:59:37 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Yang", "Qing", ""], ["Zhu", "Xia", ""], ["Fwu", "Jong-Kae", ""], ["Ye", "Yun", ""], ["You", "Ganmei", ""], ["Zhu", "Yuan", ""]]}, {"id": "2006.02662", "submitter": "Taimur Hassan", "authors": "Taimur Hassan, Muhammad Usman Akram and Naoufel Werghi", "title": "Exploiting the Transferability of Deep Learning Systems Across\n  Multi-modal Retinal Scans for Extracting Retinopathy Lesions", "comments": "Accepted in the 20th IEEE International Conference on BioInformatics\n  And BioEngineering (BIBE), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retinal lesions play a vital role in the accurate classification of retinal\nabnormalities. Many researchers have proposed deep lesion-aware screening\nsystems that analyze and grade the progression of retinopathy. However, to the\nbest of our knowledge, no literature exploits the tendency of these systems to\ngeneralize across multiple scanner specifications and multi-modal imagery.\nTowards this end, this paper presents a detailed evaluation of semantic\nsegmentation, scene parsing and hybrid deep learning systems for extracting the\nretinal lesions such as intra-retinal fluid, sub-retinal fluid, hard exudates,\ndrusen, and other chorioretinal anomalies from fused fundus and optical\ncoherence tomography (OCT) imagery. Furthermore, we present a novel strategy\nexploiting the transferability of these models across multiple retinal scanner\nspecifications. A total of 363 fundus and 173,915 OCT scans from seven publicly\navailable datasets were used in this research (from which 297 fundus and 59,593\nOCT scans were used for testing purposes). Overall, a hybrid retinal analysis\nand grading network (RAGNet), backboned through ResNet-50, stood first for\nextracting the retinal lesions, achieving a mean dice coefficient score of\n0.822. Moreover, the complete source code and its documentation are released\nat: http://biomisa.org/index.php/downloads/.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 06:25:25 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 15:48:51 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Hassan", "Taimur", ""], ["Akram", "Muhammad Usman", ""], ["Werghi", "Naoufel", ""]]}, {"id": "2006.02666", "submitter": "Ming Kong", "authors": "Yesheng Xu, Ming Kong, Wenjia Xie, Runping Duan, Zhengqing Fang,\n  Yuxiao Lin, Qiang Zhu, Siliang Tang, Fei Wu, Yu-Feng Yao", "title": "Deep Sequential Feature Learning in Clinical Image Classification of\n  Infectious Keratitis", "comments": "Accepted by Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infectious keratitis is the most common entities of corneal diseases, in\nwhich pathogen grows in the cornea leading to inflammation and destruction of\nthe corneal tissues. Infectious keratitis is a medical emergency, for which a\nrapid and accurate diagnosis is needed for speedy initiation of prompt and\nprecise treatment to halt the disease progress and to limit the extent of\ncorneal damage; otherwise it may develop sight-threatening and even\neye-globe-threatening condition. In this paper, we propose a sequential-level\ndeep learning model to effectively discriminate the distinction and subtlety of\ninfectious corneal disease via the classification of clinical images. In this\napproach, we devise an appropriate mechanism to preserve the spatial structures\nof clinical images and disentangle the informative features for clinical image\nclassification of infectious keratitis. In competition with 421\nophthalmologists, the performance of the proposed sequential-level deep model\nachieved 80.00% diagnostic accuracy, far better than the 49.27% diagnostic\naccuracy achieved by ophthalmologists over 120 test images.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 06:45:15 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Xu", "Yesheng", ""], ["Kong", "Ming", ""], ["Xie", "Wenjia", ""], ["Duan", "Runping", ""], ["Fang", "Zhengqing", ""], ["Lin", "Yuxiao", ""], ["Zhu", "Qiang", ""], ["Tang", "Siliang", ""], ["Wu", "Fei", ""], ["Yao", "Yu-Feng", ""]]}, {"id": "2006.02683", "submitter": "Raghavendra Selvan", "authors": "Raghavendra Selvan, Frederik Faye, Jon Middleton, Akshay Pai", "title": "Uncertainty quantification in medical image segmentation with\n  normalizing flows", "comments": "12 pages. Accepted to be presented at 11th International Workshop on\n  Machine Learning in Medical Imaging. Source code will be updated at\n  https://github.com/raghavian/cFlow", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation is inherently an ambiguous task due to factors\nsuch as partial volumes and variations in anatomical definitions. While in most\ncases the segmentation uncertainty is around the border of structures of\ninterest, there can also be considerable inter-rater differences. The class of\nconditional variational autoencoders (cVAE) offers a principled approach to\ninferring distributions over plausible segmentations that are conditioned on\ninput images. Segmentation uncertainty estimated from samples of such\ndistributions can be more informative than using pixel level probability\nscores. In this work, we propose a novel conditional generative model that is\nbased on conditional Normalizing Flow (cFlow). The basic idea is to increase\nthe expressivity of the cVAE by introducing a cFlow transformation step after\nthe encoder. This yields improved approximations of the latent posterior\ndistribution, allowing the model to capture richer segmentation variations.\nWith this we show that the quality and diversity of samples obtained from our\nconditional generative model is enhanced. Performance of our model, which we\ncall cFlow Net, is evaluated on two medical imaging datasets demonstrating\nsubstantial improvements in both qualitative and quantitative measures when\ncompared to a recent cVAE based model.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 07:56:46 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 10:40:10 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Selvan", "Raghavendra", ""], ["Faye", "Frederik", ""], ["Middleton", "Jon", ""], ["Pai", "Akshay", ""]]}, {"id": "2006.02692", "submitter": "Egor Ershov I", "authors": "E.I. Ershov, A.V. Belokopytov, A.V. Savchik", "title": "Problems of dataset creation for light source estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The paper describes our experience collecting a new dataset for the light\nsource estimation problem in a single image. The analysis of existing color\ntargets is presented along with various technical and scientific aspects\nessential for data collection. The paper also contains an announcement of an\nupcoming 2-nd International Illumination Estimation Challenge (IEC 2020).\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 08:20:30 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 07:51:53 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Ershov", "E. I.", ""], ["Belokopytov", "A. V.", ""], ["Savchik", "A. V.", ""]]}, {"id": "2006.02695", "submitter": "Changxing Ding", "authors": "Shengcong Chen, Changxing Ding, Dacheng Tao", "title": "Boundary-assisted Region Proposal Networks for Nucleus Segmentation", "comments": "Early Acception by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nucleus segmentation is an important task in medical image analysis. However,\nmachine learning models cannot perform well because there are large amount of\nclusters of crowded nuclei. To handle this problem, existing approaches\ntypically resort to sophisticated hand-crafted post-processing strategies;\ntherefore, they are vulnerable to the variation of post-processing\nhyper-parameters. Accordingly, in this paper, we devise a Boundary-assisted\nRegion Proposal Network (BRP-Net) that achieves robust instance-level nucleus\nsegmentation. First, we propose a novel Task-aware Feature Encoding (TAFE)\nnetwork that efficiently extracts respective high-quality features for semantic\nsegmentation and instance boundary detection tasks. This is achieved by\ncarefully considering the correlation and differences between the two tasks.\nSecond, coarse nucleus proposals are generated based on the predictions of the\nabove two tasks. Third, these proposals are fed into instance segmentation\nnetworks for more accurate prediction. Experimental results demonstrate that\nthe performance of BRP-Net is robust to the variation of post-processing\nhyper-parameters. Furthermore, BRP-Net achieves state-of-the-art performances\non both the Kumar and CPM17 datasets. The code of BRP-Net will be released at\nhttps://github.com/csccsccsccsc/brpnet.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 08:26:38 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Chen", "Shengcong", ""], ["Ding", "Changxing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2006.02706", "submitter": "Weihao Jiang", "authors": "Weihao Jiang and Zhaozhi Xie and Yaoyi Li and Chang Liu and Hongtao Lu", "title": "LRNNet: A Light-Weighted Network with Efficient Reduced Non-Local\n  Operation for Real-Time Semantic Segmentation", "comments": "To appear in icme2020workshop(MMC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent development of light-weighted neural networks has promoted the\napplications of deep learning under resource constraints and mobile\napplications. Many of these applications need to perform a real-time and\nefficient prediction for semantic segmentation with a light-weighted network.\nThis paper introduces a light-weighted network with an efficient reduced\nnon-local module (LRNNet) for efficient and realtime semantic segmentation. We\nproposed a factorized convolutional block in ResNet-Style encoder to achieve\nmore lightweighted, efficient and powerful feature extraction. Meanwhile, our\nproposed reduced non-local module utilizes spatial regional dominant singular\nvectors to achieve reduced and more representative non-local feature\nintegration with much lower computation and memory cost. Experiments\ndemonstrate our superior trade-off among light-weight, speed, computation and\naccuracy. Without additional processing and pretraining, LRNNet achieves 72.2%\nmIoU on Cityscapes test dataset only using the fine annotation data for\ntraining with only 0.68M parameters and with 71 FPS on a GTX 1080Ti card.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 08:55:15 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Jiang", "Weihao", ""], ["Xie", "Zhaozhi", ""], ["Li", "Yaoyi", ""], ["Liu", "Chang", ""], ["Lu", "Hongtao", ""]]}, {"id": "2006.02708", "submitter": "Jiawang Bian", "authors": "Jia-Wang Bian, Huangying Zhan, Naiyan Wang, Tat-Jun Chin, Chunhua\n  Shen, Ian Reid", "title": "Unsupervised Depth Learning in Challenging Indoor Video: Weak\n  Rectification to Rescue", "comments": "See codes, data, and demos in GitHub page\n  (https://github.com/JiawangBian/Unsupervised-Indoor-Depth)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-view depth estimation using CNNs trained from unlabelled videos has\nshown significant promise. However, the excellent results have mostly been\nobtained in street-scene driving scenarios, and such methods often fail in\nother settings, particularly indoor videos taken by handheld devices, in which\ncase the ego-motion is often degenerate, i.e., the rotation dominates the\ntranslation. In this work, we establish that the degenerate camera motions\nexhibited in handheld settings are a critical obstacle for unsupervised depth\nlearning. A main contribution of our work is fundamental analysis which shows\nthat the rotation behaves as noise during training, as opposed to the\ntranslation (baseline) which provides supervision signals. To capitalise on our\nfindings, we propose a novel data pre-processing method for effective training,\ni.e., we search for image pairs with modest translation and remove their\nrotation via the proposed weak image rectification. With our pre-processing,\nexisting unsupervised models can be trained well in challenging scenarios\n(e.g., NYUv2 dataset), and the results outperform the unsupervised SOTA by a\nlarge margin (0.147 vs. 0.189 in the AbsRel error).\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 08:59:17 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Bian", "Jia-Wang", ""], ["Zhan", "Huangying", ""], ["Wang", "Naiyan", ""], ["Chin", "Tat-Jun", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "2006.02713", "submitter": "Yixiao Ge", "authors": "Yixiao Ge, Feng Zhu, Dapeng Chen, Rui Zhao, Hongsheng Li", "title": "Self-paced Contrastive Learning with Hybrid Memory for Domain Adaptive\n  Object Re-ID", "comments": "Accepted in NeurIPS 2020. Code available:\n  https://github.com/yxgeee/SpCL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptive object re-ID aims to transfer the learned knowledge from the\nlabeled source domain to the unlabeled target domain to tackle the open-class\nre-identification problems. Although state-of-the-art pseudo-label-based\nmethods have achieved great success, they did not make full use of all valuable\ninformation because of the domain gap and unsatisfying clustering performance.\nTo solve these problems, we propose a novel self-paced contrastive learning\nframework with hybrid memory. The hybrid memory dynamically generates\nsource-domain class-level, target-domain cluster-level and un-clustered\ninstance-level supervisory signals for learning feature representations.\nDifferent from the conventional contrastive learning strategy, the proposed\nframework jointly distinguishes source-domain classes, and target-domain\nclusters and un-clustered instances. Most importantly, the proposed self-paced\nmethod gradually creates more reliable clusters to refine the hybrid memory and\nlearning targets, and is shown to be the key to our outstanding performance.\nOur method outperforms state-of-the-arts on multiple domain adaptation tasks of\nobject re-ID and even boosts the performance on the source domain without any\nextra annotations. Our generalized version on unsupervised object re-ID\nsurpasses state-of-the-art algorithms by considerable 16.7% and 7.9% on\nMarket-1501 and MSMT17 benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 09:12:44 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 13:10:31 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Ge", "Yixiao", ""], ["Zhu", "Feng", ""], ["Chen", "Dapeng", ""], ["Zhao", "Rui", ""], ["Li", "Hongsheng", ""]]}, {"id": "2006.02766", "submitter": "Yuhongze Zhou", "authors": "Yuhongze Zhou, Qinjie Xiao", "title": "GAN-Based Facial Attractiveness Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generative framework based on generative adversarial network\n(GAN) to enhance facial attractiveness while preserving facial identity and\nhigh-fidelity. Given a portrait image as input, having applied gradient descent\nto recover a latent vector that this generative framework can use to synthesize\nan image resemble to the input image, beauty semantic editing manipulation on\nthe corresponding recovered latent vector based on InterFaceGAN enables this\nframework to achieve facial image beautification. This paper compared our\nsystem with Beholder-GAN and our proposed result-enhanced version of\nBeholder-GAN. It turns out that our framework obtained state-of-art\nattractiveness enhancement results. The code is available at\nhttps://github.com/zoezhou1999/BeautifyBasedOnGAN.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 10:46:07 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Zhou", "Yuhongze", ""], ["Xiao", "Qinjie", ""]]}, {"id": "2006.02797", "submitter": "Vijay Pandey", "authors": "Vijay Pandey", "title": "Overcoming Overfitting and Large Weight Update Problem in Linear\n  Rectifiers: Thresholded Exponential Rectified Linear Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In past few years, linear rectified unit activation functions have shown its\nsignificance in the neural networks, surpassing the performance of sigmoid\nactivations. RELU (Nair & Hinton, 2010), ELU (Clevert et al., 2015), PRELU (He\net al., 2015), LRELU (Maas et al., 2013), SRELU (Jin et al., 2016),\nThresholdedRELU, all these linear rectified activation functions have its own\nsignificance over others in some aspect. Most of the time these activation\nfunctions suffer from bias shift problem due to non-zero output mean, and high\nweight update problem in deep complex networks due to unit gradient, which\nresults in slower training, and high variance in model prediction respectively.\nIn this paper, we propose, \"Thresholded exponential rectified linear unit\"\n(TERELU) activation function that works better in alleviating in overfitting:\nlarge weight update problem. Along with alleviating overfitting problem, this\nmethod also gives good amount of non-linearity as compared to other linear\nrectifiers. We will show better performance on the various datasets using\nneural networks, considering TERELU activation method compared to other\nactivations.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 11:55:47 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Pandey", "Vijay", ""]]}, {"id": "2006.02801", "submitter": "Yi Fang", "authors": "Xiang Li, Mingyang Wang, Yi Fang", "title": "Height estimation from single aerial images using a deep ordinal\n  regression network", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the 3D geometric structure of the Earth's surface has been an\nactive research topic in photogrammetry and remote sensing community for\ndecades, serving as an essential building block for various applications such\nas 3D digital city modeling, change detection, and city management. Previous\nresearches have extensively studied the problem of height estimation from\naerial images based on stereo or multi-view image matching. These methods\nrequire two or more images from different perspectives to reconstruct 3D\ncoordinates with camera information provided. In this paper, we deal with the\nambiguous and unsolved problem of height estimation from a single aerial image.\nDriven by the great success of deep learning, especially deep convolution\nneural networks (CNNs), some researches have proposed to estimate height\ninformation from a single aerial image by training a deep CNN model with\nlarge-scale annotated datasets. These methods treat height estimation as a\nregression problem and directly use an encoder-decoder network to regress the\nheight values. In this paper, we proposed to divide height values into\nspacing-increasing intervals and transform the regression problem into an\nordinal regression problem, using an ordinal loss for network training. To\nenable multi-scale feature extraction, we further incorporate an Atrous Spatial\nPyramid Pooling (ASPP) module to extract features from multiple dilated\nconvolution layers. After that, a post-processing technique is designed to\ntransform the predicted height map of each patch into a seamless height map.\nFinally, we conduct extensive experiments on ISPRS Vaihingen and Potsdam\ndatasets. Experimental results demonstrate significantly better performance of\nour method compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 12:03:51 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Li", "Xiang", ""], ["Wang", "Mingyang", ""], ["Fang", "Yi", ""]]}, {"id": "2006.02802", "submitter": "Satoshi Tsutsui", "authors": "Satoshi Tsutsui, Arjun Chandrasekaran, Md Alimoor Reza, David\n  Crandall, Chen Yu", "title": "A Computational Model of Early Word Learning from the Infant's Point of\n  View", "comments": "Accepted by Annual Conference of the Cognitive Science Society\n  (CogSci) 2020. (Oral Acceptance Rate = 177/811 = 22%)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human infants have the remarkable ability to learn the associations between\nobject names and visual objects from inherently ambiguous experiences.\nResearchers in cognitive science and developmental psychology have built formal\nmodels that implement in-principle learning algorithms, and then used\npre-selected and pre-cleaned datasets to test the abilities of the models to\nfind statistical regularities in the input data. In contrast to previous\nmodeling approaches, the present study used egocentric video and gaze data\ncollected from infant learners during natural toy play with their parents. This\nallowed us to capture the learning environment from the perspective of the\nlearner's own point of view. We then used a Convolutional Neural Network (CNN)\nmodel to process sensory data from the infant's point of view and learn\nname-object associations from scratch. As the first model that takes raw\negocentric video to simulate infant word learning, the present study provides a\nproof of principle that the problem of early word learning can be solved, using\nactual visual data perceived by infant learners. Moreover, we conducted\nsimulation experiments to systematically determine how visual, perceptual, and\nattentional properties of infants' sensory experiences may affect word\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 12:08:44 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Tsutsui", "Satoshi", ""], ["Chandrasekaran", "Arjun", ""], ["Reza", "Md Alimoor", ""], ["Crandall", "David", ""], ["Yu", "Chen", ""]]}, {"id": "2006.02813", "submitter": "Ruben Hemelings", "authors": "Ruben Hemelings, Bart Elen, Matthew B. Blaschko, Julie Jacob, Ingeborg\n  Stalmans, Patrick De Boever", "title": "Pathological myopia classification with simultaneous lesion segmentation\n  using deep learning", "comments": "18 pages, 2 figures, preprint to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This investigation reports on the results of convolutional neural networks\ndeveloped for the recently introduced PathologicAL Myopia (PALM) dataset, which\nconsists of 1200 fundus images. We propose a new Optic Nerve Head (ONH)-based\nprediction enhancement for the segmentation of atrophy and fovea. Models\ntrained with 400 available training images achieved an AUC of 0.9867 for\npathological myopia classification, and a Euclidean distance of 58.27 pixels on\nthe fovea localization task, evaluated on a test set of 400 images. Dice and F1\nmetrics for semantic segmentation of lesions scored 0.9303 and 0.9869 on optic\ndisc, 0.8001 and 0.9135 on retinal atrophy, and 0.8073 and 0.7059 on retinal\ndetachment, respectively. Our work was acknowledged with an award in the\ncontext of the \"PathologicAL Myopia detection from retinal images\" challenge\nheld during the IEEE International Symposium on Biomedical Imaging (April\n2019). Considering that (pathological) myopia cases are often identified as\nfalse positives and negatives in classification systems for glaucoma, we\nenvision that the current work could aid in future research to discriminate\nbetween glaucomatous and highly-myopic eyes, complemented by the localization\nand segmentation of landmarks such as fovea, optic disc and atrophy.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 12:21:06 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Hemelings", "Ruben", ""], ["Elen", "Bart", ""], ["Blaschko", "Matthew B.", ""], ["Jacob", "Julie", ""], ["Stalmans", "Ingeborg", ""], ["De Boever", "Patrick", ""]]}, {"id": "2006.02826", "submitter": "Tobias Fischer", "authors": "Tobias Fischer and Michael Milford", "title": "Event-based visual place recognition with ensembles of temporal windows", "comments": "8 pages, 8 figures, additional 8 pages supplementary material", "journal-ref": "IEEE Robotics and Automation Letters 2020", "doi": "10.1109/LRA.2020.3025505", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are bio-inspired sensors capable of providing a continuous\nstream of events with low latency and high dynamic range. As a single event\nonly carries limited information about the brightness change at a particular\npixel, events are commonly accumulated into spatio-temporal windows for further\nprocessing. However, the optimal window length varies depending on the scene,\ncamera motion, the task being performed, and other factors. In this research,\nwe develop a novel ensemble-based scheme for combining temporal windows of\nvarying lengths that are processed in parallel. For applications where the\nincreased computational requirements of this approach are not practical, we\nalso introduce a new \"approximate\" ensemble scheme that achieves significant\ncomputational efficiencies without unduly compromising the original performance\ngains provided by the ensemble approach. We demonstrate our ensemble scheme on\nthe visual place recognition (VPR) task, introducing a new Brisbane-Event-VPR\ndataset with annotated recordings captured using a DAVIS346 color event camera.\nWe show that our proposed ensemble scheme significantly outperforms all the\nsingle-window baselines and conventional model-based ensembles, irrespective of\nthe image reconstruction and feature extraction methods used in the VPR\npipeline, and evaluate which ensemble combination technique performs best.\nThese results demonstrate the significant benefits of ensemble schemes for\nevent camera processing in the VPR domain and may have relevance to other\nrelated processes, including feature tracking, visual-inertial odometry, and\nsteering prediction in driving.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 05:33:35 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 23:23:38 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Fischer", "Tobias", ""], ["Milford", "Michael", ""]]}, {"id": "2006.02834", "submitter": "Debayan Deb", "authors": "Debayan Deb, Anil K. Jain", "title": "Look Locally Infer Globally: A Generalizable Face Anti-Spoofing Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art spoof detection methods tend to overfit to the spoof types\nseen during training and fail to generalize to unknown spoof types. Given that\nface anti-spoofing is inherently a local task, we propose a face anti-spoofing\nframework, namely Self-Supervised Regional Fully Convolutional Network\n(SSR-FCN), that is trained to learn local discriminative cues from a face image\nin a self-supervised manner. The proposed framework improves generalizability\nwhile maintaining the computational efficiency of holistic face anti-spoofing\napproaches (< 4 ms on a Nvidia GTX 1080Ti GPU). The proposed method is\ninterpretable since it localizes which parts of the face are labeled as spoofs.\nExperimental results show that SSR-FCN can achieve TDR = 65% @ 2.0% FDR when\nevaluated on a dataset comprising of 13 different spoof types under unknown\nattacks while achieving competitive performances under standard benchmark\ndatasets (Oulu-NPU, CASIA-MFSD, and Replay-Attack).\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 13:11:17 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 11:01:46 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 19:04:10 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Deb", "Debayan", ""], ["Jain", "Anil K.", ""]]}, {"id": "2006.02901", "submitter": "Gang Liu", "authors": "Gang Liu and Jing Wang", "title": "A Polynomial Neural network with Controllable Precision and\n  Human-Readable Topology II: Accelerated Approach Based on Expanded Layer", "comments": "some studies attempted to explain the existing NNs using Taylor\n  series or polynomial.It is also troublesome. How about converting Taylor\n  series to a network?", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How about converting Taylor series to a network to solve the black-box nature\nof Neural Networks? Controllable and readable polynomial neural network (Gang\ntransform or CR-PNN) is the Taylor expansion in the form of network, which is\nabout ten times more efficient than typical BPNN for forward-propagation.\nAdditionally, we can control the approximation precision and explain the\ninternal structure of the network; thus, it is used for prediction and system\nidentification. However, as the network depth increases, the computational\ncomplexity increases. Here, we presented an accelerated method based on an\nexpanded order to optimize CR-PNN. The running speed of the structure of CR-PNN\nII is significantly higher than CR-PNN I under preserving the properties of\nCR-PNN I.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 17:56:24 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Liu", "Gang", ""], ["Wang", "Jing", ""]]}, {"id": "2006.02909", "submitter": "Nicholas Schaub", "authors": "Nicholas J. Schaub, Nathan Hotaling", "title": "Assessing Intelligence in Artificial Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this work was to develop of metrics to assess network\narchitectures that balance neural network size and task performance. To this\nend, the concept of neural efficiency is introduced to measure neural layer\nutilization, and a second metric called artificial intelligence quotient (aIQ)\nwas created to balance neural network performance and neural network\nefficiency. To study aIQ and neural efficiency, two simple neural networks were\ntrained on MNIST: a fully connected network (LeNet-300-100) and a convolutional\nneural network (LeNet-5). The LeNet-5 network with the highest aIQ was 2.32%\nless accurate but contained 30,912 times fewer parameters than the highest\naccuracy network. Both batch normalization and dropout layers were found to\nincrease neural efficiency. Finally, high aIQ networks are shown to be\nmemorization and overtraining resistant, capable of learning proper digit\nclassification with an accuracy of 92.51% even when 75% of the class labels are\nrandomized. These results demonstrate the utility of aIQ and neural efficiency\nas metrics for balancing network performance and size.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 16:45:42 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Schaub", "Nicholas J.", ""], ["Hotaling", "Nathan", ""]]}, {"id": "2006.02921", "submitter": "Samuel Dubuis", "authors": "Marc Bickel, Samuel Dubuis, S\\'ebastien Gachoud", "title": "Multiple Generative Adversarial Networks Analysis for Predicting\n  Photographers' Retouching", "comments": "15 pages, 34 figures https://github.com/MarcBickel/CS-413", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Anyone can take a photo, but not everybody has the ability to retouch their\npictures and obtain result close to professional. Since it is not possible to\nask experts to retouch thousands of pictures, we thought about teaching a piece\nof software how to reproduce the work of those said experts. This study aims to\nexplore the possibility to use deep learning methods and more specifically,\ngenerative adversarial networks (GANs), to mimic artists' retouching and find\nwhich one of the studied models provides the best results.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 10:10:01 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Bickel", "Marc", ""], ["Dubuis", "Samuel", ""], ["Gachoud", "S\u00e9bastien", ""]]}, {"id": "2006.02933", "submitter": "Ibon Merino", "authors": "Ibon Merino, Jon Azpiazu, Anthony Remazeilles, Basilio Sierra", "title": "2D Image Features Detector And Descriptor Selection Expert System", "comments": "10 pages, 5 figures, 5 tables", "journal-ref": "in 8th International Conference on Natural Language Processing\n  (NLP 2019), Sep. 2019, pp. 51-61", "doi": "10.5121/csit.2019.91206", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection and description of keypoints from an image is a well-studied\nproblem in Computer Vision. Some methods like SIFT, SURF or ORB are\ncomputationally really efficient. This paper proposes a solution for a\nparticular case study on object recognition of industrial parts based on\nhierarchical classification. Reducing the number of instances leads to better\nperformance, indeed, that is what the use of the hierarchical classification is\nlooking for. We demonstrate that this method performs better than using just\none method like ORB, SIFT or FREAK, despite being fairly slower.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 15:18:18 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Merino", "Ibon", ""], ["Azpiazu", "Jon", ""], ["Remazeilles", "Anthony", ""], ["Sierra", "Basilio", ""]]}, {"id": "2006.02963", "submitter": "Jacob Shermeyer", "authors": "Jacob Shermeyer, Thomas Hossler, Adam Van Etten, Daniel Hogan, Ryan\n  Lewis, Daeil Kim", "title": "RarePlanes: Synthetic Data Takes Flight", "comments": "To appear in WACV 2021 - 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RarePlanes is a unique open-source machine learning dataset that incorporates\nboth real and synthetically generated satellite imagery. The RarePlanes dataset\nspecifically focuses on the value of synthetic data to aid computer vision\nalgorithms in their ability to automatically detect aircraft and their\nattributes in satellite imagery. Although other synthetic/real combination\ndatasets exist, RarePlanes is the largest openly-available very-high resolution\ndataset built to test the value of synthetic data from an overhead perspective.\nPrevious research has shown that synthetic data can reduce the amount of real\ntraining data needed and potentially improve performance for many tasks in the\ncomputer vision domain. The real portion of the dataset consists of 253 Maxar\nWorldView-3 satellite scenes spanning 112 locations and 2,142 km^2 with 14,700\nhand-annotated aircraft. The accompanying synthetic dataset is generated via\nAI.Reverie's simulation platform and features 50,000 synthetic satellite images\nsimulating a total area of 9331.2 km^2 with ~630,000 aircraft annotations. Both\nthe real and synthetically generated aircraft feature 10 fine grain attributes\nincluding: aircraft length, wingspan, wing-shape, wing-position, wingspan\nclass, propulsion, number of engines, number of vertical-stabilizers, presence\nof canards, and aircraft role. Finally, we conduct extensive experiments to\nevaluate the real and synthetic datasets and compare performances. By doing so,\nwe show the value of synthetic data for the task of detecting and classifying\naircraft from an overhead perspective.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 15:46:43 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 17:17:01 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Shermeyer", "Jacob", ""], ["Hossler", "Thomas", ""], ["Van Etten", "Adam", ""], ["Hogan", "Daniel", ""], ["Lewis", "Ryan", ""], ["Kim", "Daeil", ""]]}, {"id": "2006.03001", "submitter": "Kexin Feng", "authors": "Kexin Feng, Theodora Chaspari", "title": "A Siamese Neural Network with Modified Distance Loss For Transfer\n  Learning in Speech Emotion Recognition", "comments": "AffCon@AAAI-20; Presented at AAAI-20 W1: Affective Content Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic emotion recognition plays a significant role in the process of\nhuman computer interaction and the design of Internet of Things (IOT)\ntechnologies. Yet, a common problem in emotion recognition systems lies in the\nscarcity of reliable labels. By modeling pairwise differences between samples\nof interest, a Siamese network can help to mitigate this challenge since it\nrequires fewer samples than traditional deep learning methods. In this paper,\nwe propose a distance loss, which can be applied on the Siamese network\nfine-tuning, by optimizing the model based on the relevant distance between\nsame and difference class pairs. Our system use samples from the source data to\npre-train the weights of proposed Siamese neural network, which are fine-tuned\nbased on the target data. We present an emotion recognition task that uses\nspeech, since it is one of the most ubiquitous and frequently used\nbio-behavioral signals. Our target data comes from the RAVDESS dataset, while\nthe CREMA-D and eNTERFACE'05 are used as source data, respectively. Our results\nindicate that the proposed distance loss is able to greatly benefit the\nfine-tuning process of Siamese network. Also, the selection of source data has\nmore effect on the Siamese network performance compared to the number of frozen\nlayers. These suggest the great potential of applying the Siamese network and\nmodelling pairwise differences in the field of transfer learning for automatic\nemotion recognition.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 16:44:33 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Feng", "Kexin", ""], ["Chaspari", "Theodora", ""]]}, {"id": "2006.03028", "submitter": "Lingyu Zhu", "authors": "Lingyu Zhu, Esa Rahtu", "title": "Visually Guided Sound Source Separation using Cascaded Opponent Filter\n  Network", "comments": "main paper 14 pages, ref 3 pages, and supp 7 pages. Revised argument\n  in section 3 and 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to recover the original component signals from\na mixture audio with the aid of visual cues of the sound sources. Such task is\nusually referred as visually guided sound source separation. The proposed\nCascaded Opponent Filter (COF) framework consists of multiple stages, which\nrecursively refine the source separation. A key element in COF is a novel\nopponent filter module that identifies and relocates residual components\nbetween sources. The system is guided by the appearance and motion of the\nsource, and, for this purpose, we study different representations based on\nvideo frames, optical flows, dynamic images, and their combinations. Finally,\nwe propose a Sound Source Location Masking (SSLM) technique, which, together\nwith COF, produces a pixel level mask of the source location. The entire system\nis trained end-to-end using a large set of unlabelled videos. We compare COF\nwith recent baselines and obtain the state-of-the-art performance in three\nchallenging datasets (MUSIC, A-MUSIC, and A-NATURAL). Project page:\nhttps://ly-zhu.github.io/cof-net.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 17:27:49 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 15:38:36 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Zhu", "Lingyu", ""], ["Rahtu", "Esa", ""]]}, {"id": "2006.03122", "submitter": "Mohammad Naser Sabet Jahromi", "authors": "Satya M. Muddamsetty, Mohammad N. S. Jahromi, Thomas B. Moeslund", "title": "SIDU: Similarity Difference and Uniqueness Method for Explainable AI", "comments": "Accepted manuscript in IEEE International Conference on Image\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new brand of technical artificial intelligence ( Explainable AI ) research\nhas focused on trying to open up the 'black box' and provide some\nexplainability. This paper presents a novel visual explanation method for deep\nlearning networks in the form of a saliency map that can effectively localize\nentire object regions. In contrast to the current state-of-the art methods, the\nproposed method shows quite promising visual explanations that can gain greater\ntrust of human expert. Both quantitative and qualitative evaluations are\ncarried out on both general and clinical data sets to confirm the effectiveness\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 20:33:40 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Muddamsetty", "Satya M.", ""], ["Jahromi", "Mohammad N. S.", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "2006.03156", "submitter": "Simone Parisotto Dr", "authors": "Simone Parisotto and Alessandro Launaro and Ninetta Leone and\n  Carola-Bibiane Sch\\\"onlieb", "title": "Unsupervised clustering of Roman pottery profiles from their SSAE\n  representation", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the ROman COmmonware POTtery (ROCOPOT) database,\nwhich comprises of more than 2000 black and white imaging profiles of pottery\nshapes extracted from 11 Roman catalogues and related to different excavation\nsites. The partiality and the handcrafted variance of the shape fragments\nwithin this new database make their unsupervised clustering a very challenging\nproblem: profile similarities are thus explored via the hierarchical clustering\nof non-linear features learned in the latent representation space of a stacked\nsparse autoencoder (SSAE) network, unveiling new profile matches. Results are\ncommented both from a mathematical and archaeological perspective so as to\nunlock new research directions in the respective communities.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 22:19:22 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Parisotto", "Simone", ""], ["Launaro", "Alessandro", ""], ["Leone", "Ninetta", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2006.03179", "submitter": "Garrett Bingham", "authors": "Garrett Bingham and Risto Miikkulainen", "title": "Discovering Parametric Activation Functions", "comments": "14 pages, 12 figures/tables, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that the choice of activation function can\nsignificantly affect the performance of deep learning networks. However, the\nbenefits of novel activation functions have been inconsistent and task\ndependent, and therefore the rectified linear unit (ReLU) is still the most\ncommonly used. This paper proposes a technique for customizing activation\nfunctions automatically, resulting in reliable improvements in performance.\nEvolutionary search is used to discover the general form of the function, and\ngradient descent to optimize its parameters for different parts of the network\nand over the learning process. Experiments with four different neural network\narchitectures on the CIFAR-10 and CIFAR-100 image classification datasets show\nthat this approach is effective. It discovers both general activation functions\nand specialized functions for different architectures, consistently improving\naccuracy over ReLU and other activation functions by significant margins. The\napproach can therefore be used as an automated optimization step in applying\ndeep learning to new tasks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 00:25:33 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 15:33:14 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 19:28:47 GMT"}, {"version": "v4", "created": "Sat, 30 Jan 2021 02:17:20 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Bingham", "Garrett", ""], ["Miikkulainen", "Risto", ""]]}, {"id": "2006.03182", "submitter": "Fan Yang", "authors": "Fan Yang and Xiao Xiao", "title": "MSDU-net: A Multi-Scale Dilated U-net for Blur Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blur detection is the separation of blurred and clear regions of an image,\nwhich is an important and challenging task in computer vision. In this work, we\nregard blur detection as an image segmentation problem. Inspired by the success\nof the U-net architecture for image segmentation, we design a Multi-Scale\nDilated convolutional neural network based on U-net, which we call MSDU-net.\nThe MSDU-net uses a group of multi-scale feature extractors with dilated\nconvolutions to extract texture information at different scales. The U-shape\narchitecture of the MSDU-net fuses the different-scale texture features and\ngenerates a semantic feature which allows us to achieve better results on the\nblur detection task. We show that using the MSDU-net we are able to outperform\nother state of the art blur detection methods on two publicly available\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 00:30:38 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Yang", "Fan", ""], ["Xiao", "Xiao", ""]]}, {"id": "2006.03184", "submitter": "Omid Mohamad Nezami", "authors": "Omid Mohamad Nezami, Akshay Chaturvedi, Mark Dras, Utpal Garain", "title": "Pick-Object-Attack: Type-Specific Adversarial Attack for Object\n  Detection", "comments": "The paper is under consideration at Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent studies have shown that deep neural models are vulnerable to\nadversarial samples: images with imperceptible perturbations, for example, can\nfool image classifiers. In this paper, we generate adversarial examples for\nobject detection, which entails detecting bounding boxes around multiple\nobjects present in the image and classifying them at the same time, making it a\nharder task than against image classification. We specifically aim to attack\nthe widely used Faster R-CNN by changing the predicted label for a particular\nobject in an image: where prior work has targeted one specific object (a stop\nsign), we generalise to arbitrary objects, with the key challenge being the\nneed to change the labels of all bounding boxes for all instances of that\nobject type. To do so, we propose a novel method, named Pick-Object-Attack.\nPick-Object-Attack successfully adds perturbations only to bounding boxes for\nthe targeted object, preserving the labels of other detected objects in the\nimage. In terms of perceptibility, the perturbations induced by the method are\nvery small. Furthermore, for the first time, we examine the effect of\nadversarial attacks on object detection in terms of a downstream task, image\ncaptioning; we show that where a method that can modify all object types leads\nto very obvious changes in captions, the changes from our constrained attack\nare much less apparent.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 00:37:09 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 07:27:02 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Nezami", "Omid Mohamad", ""], ["Chaturvedi", "Akshay", ""], ["Dras", "Mark", ""], ["Garain", "Utpal", ""]]}, {"id": "2006.03199", "submitter": "Chiranjibi Sitaula", "authors": "Chiranjibi Sitaula and Yong Xiang and Sunil Aryal and Xuequan Lu", "title": "Scene Image Representation by Foreground, Background and Hybrid Features", "comments": "Submitted to Expert Systems with Applications (ESWA), 28 pages and 17\n  images", "journal-ref": "Expert Systems with Applications (ESWA), 2021", "doi": "10.1016/j.eswa.2021.115285", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous methods for representing scene images based on deep learning\nprimarily consider either the foreground or background information as the\ndiscriminating clues for the classification task. However, scene images also\nrequire additional information (hybrid) to cope with the inter-class similarity\nand intra-class variation problems. In this paper, we propose to use hybrid\nfeatures in addition to foreground and background features to represent scene\nimages. We suppose that these three types of information could jointly help to\nrepresent scene image more accurately. To this end, we adopt three VGG-16\narchitectures pre-trained on ImageNet, Places, and Hybrid (both ImageNet and\nPlaces) datasets for the corresponding extraction of foreground, background and\nhybrid information. All these three types of deep features are further\naggregated to achieve our final features for the representation of scene\nimages. Extensive experiments on two large benchmark scene datasets (MIT-67 and\nSUN-397) show that our method produces the state-of-the-art classification\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 01:55:24 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sitaula", "Chiranjibi", ""], ["Xiang", "Yong", ""], ["Aryal", "Sunil", ""], ["Lu", "Xuequan", ""]]}, {"id": "2006.03201", "submitter": "Eadom Dessalene", "authors": "Eadom Dessalene, Michael Maynord, Chinmaya Devaraj, Cornelia Fermuller\n  and Yiannis Aloimonos", "title": "Egocentric Object Manipulation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Egocentric Object Manipulation Graphs (Ego-OMG) - a novel\nrepresentation for activity modeling and anticipation of near future actions\nintegrating three components: 1) semantic temporal structure of activities, 2)\nshort-term dynamics, and 3) representations for appearance. Semantic temporal\nstructure is modeled through a graph, embedded through a Graph Convolutional\nNetwork, whose states model characteristics of and relations between hands and\nobjects. These state representations derive from all three levels of\nabstraction, and span segments delimited by the making and breaking of\nhand-object contact. Short-term dynamics are modeled in two ways: A) through 3D\nconvolutions, and B) through anticipating the spatiotemporal end points of hand\ntrajectories, where hands come into contact with objects. Appearance is modeled\nthrough deep spatiotemporal features produced through existing methods. We note\nthat in Ego-OMG it is simple to swap these appearance features, and thus\nEgo-OMG is complementary to most existing action anticipation methods. We\nevaluate Ego-OMG on the EPIC Kitchens Action Anticipation Challenge. The\nconsistency of the egocentric perspective of EPIC Kitchens allows for the\nutilization of the hand-centric cues upon which Ego-OMG relies. We demonstrate\nstate-of-the-art performance, outranking all other previous published methods\nby large margins and ranking first on the unseen test set and second on the\nseen test set of the EPIC Kitchens Action Anticipation Challenge. We attribute\nthe success of Ego-OMG to the modeling of semantic structure captured over long\ntimespans. We evaluate the design choices made through several ablation\nstudies. Code will be released upon acceptance\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 02:03:25 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Dessalene", "Eadom", ""], ["Maynord", "Michael", ""], ["Devaraj", "Chinmaya", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "2006.03204", "submitter": "Vitali Petsiuk", "authors": "Vitali Petsiuk and Rajiv Jain and Varun Manjunatha and Vlad I. Morariu\n  and Ashutosh Mehra and Vicente Ordonez and Kate Saenko", "title": "Black-box Explanation of Object Detectors via Saliency Maps", "comments": "CVPR 2021 (oral). Project page\n  https://cs-people.bu.edu/vpetsiuk/drise/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose D-RISE, a method for generating visual explanations for the\npredictions of object detectors. Utilizing the proposed similarity metric that\naccounts for both localization and categorization aspects of object detection\nallows our method to produce saliency maps that show image areas that most\naffect the prediction. D-RISE can be considered \"black-box\" in the software\ntesting sense, as it only needs access to the inputs and outputs of an object\ndetector. Compared to gradient-based methods, D-RISE is more general and\nagnostic to the particular type of object detector being tested, and does not\nneed knowledge of the inner workings of the model. We show that D-RISE can be\neasily applied to different object detectors including one-stage detectors such\nas YOLOv3 and two-stage detectors such as Faster-RCNN. We present a detailed\nanalysis of the generated visual explanations to highlight the utilization of\ncontext and possible biases learned by object detectors.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 02:13:35 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 22:36:13 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Petsiuk", "Vitali", ""], ["Jain", "Rajiv", ""], ["Manjunatha", "Varun", ""], ["Morariu", "Vlad I.", ""], ["Mehra", "Ashutosh", ""], ["Ordonez", "Vicente", ""], ["Saenko", "Kate", ""]]}, {"id": "2006.03209", "submitter": "Chengtang Yao", "authors": "Chengtang Yao, Yunde Jia, Huijun Di, Yuwei Wu, Lidong Yu", "title": "Content-Aware Inter-Scale Cost Aggregation for Stereo Matching", "comments": "19 pages, 14 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cost aggregation is a key component of stereo matching for high-quality depth\nestimation. Most methods use multi-scale processing to downsample cost volume\nfor proper context information, but will cause loss of details when upsampling.\nIn this paper, we present a content-aware inter-scale cost aggregation method\nthat adaptively aggregates and upsamples the cost volume from coarse-scale to\nfine-scale by learning dynamic filter weights according to the content of the\nleft and right views on the two scales. Our method achieves reliable detail\nrecovery when upsampling through the aggregation of information across\ndifferent scales. Furthermore, a novel decomposition strategy is proposed to\nefficiently construct the 3D filter weights and aggregate the 3D cost volume,\nwhich greatly reduces the computation cost. We first learn the 2D similarities\nvia the feature maps on the two scales, and then build the 3D filter weights\nbased on the 2D similarities from the left and right views. After that, we\nsplit the aggregation in a full 3D spatial-disparity space into the aggregation\nin 1D disparity space and 2D spatial space. Experiment results on Scene Flow\ndataset, KITTI2015 and Middlebury demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 02:38:34 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Yao", "Chengtang", ""], ["Jia", "Yunde", ""], ["Di", "Huijun", ""], ["Wu", "Yuwei", ""], ["Yu", "Lidong", ""]]}, {"id": "2006.03217", "submitter": "Chiranjibi Sitaula", "authors": "Chiranjibi Sitaula and Sunil Aryal and Yong Xiang and Anish Basnet and\n  Xuequan Lu", "title": "Content and Context Features for Scene Image Representation", "comments": "Submitted to Knowledge-Based Systems (Elsevier) for consideration", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing research in scene image classification has focused on either content\nfeatures (e.g., visual information) or context features (e.g., annotations). As\nthey capture different information about images which can be complementary and\nuseful to discriminate images of different classes, we suppose the fusion of\nthem will improve classification results. In this paper, we propose new\ntechniques to compute content features and context features, and then fuse them\ntogether. For content features, we design multi-scale deep features based on\nbackground and foreground information in images. For context features, we use\nannotations of similar images available in the web to design a filter words\n(codebook). Our experiments in three widely used benchmark scene datasets using\nsupport vector machine classifier reveal that our proposed context and content\nfeatures produce better results than existing context and content features,\nrespectively. The fusion of the proposed two types of features significantly\noutperform numerous state-of-the-art features.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 03:19:13 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 06:25:35 GMT"}, {"version": "v3", "created": "Sat, 24 Apr 2021 05:37:31 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Sitaula", "Chiranjibi", ""], ["Aryal", "Sunil", ""], ["Xiang", "Yong", ""], ["Basnet", "Anish", ""], ["Lu", "Xuequan", ""]]}, {"id": "2006.03250", "submitter": "Jieru Zhao", "authors": "Jieru Zhao, Tingyuan Liang, Liang Feng, Wenchao Ding, Sharad Sinha,\n  Wei Zhang and Shaojie Shen", "title": "FP-Stereo: Hardware-Efficient Stereo Vision for Embedded Applications", "comments": "IEEE International Conference on Field Programmable Logic and\n  Applications (FPL), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and accurate depth estimation, or stereo matching, is essential in\nembedded stereo vision systems, requiring substantial design effort to achieve\nan appropriate balance among accuracy, speed and hardware cost. To reduce the\ndesign effort and achieve the right balance, we propose FP-Stereo for building\nhigh-performance stereo matching pipelines on FPGAs automatically. FP-Stereo\nconsists of an open-source hardware-efficient library, allowing designers to\nobtain the desired implementation instantly. Diverse methods are supported in\nour library for each stage of the stereo matching pipeline and a series of\ntechniques are developed to exploit the parallelism and reduce the resource\noverhead. To improve the usability, FP-Stereo can generate synthesizable C code\nof the FPGA accelerator with our optimized HLS templates automatically. To\nguide users for the right design choice meeting specific application\nrequirements, detailed comparisons are performed on various configurations of\nour library to investigate the accuracy/speed/cost trade-off. Experimental\nresults also show that FP-Stereo outperforms the state-of-the-art FPGA design\nfrom all aspects, including 6.08% lower error, 2x faster speed, 30% less\nresource usage and 40% less energy consumption. Compared to GPU designs,\nFP-Stereo achieves the same accuracy at a competitive speed while consuming\nmuch less energy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 06:17:43 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 07:44:10 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 07:04:33 GMT"}, {"version": "v4", "created": "Wed, 1 Jul 2020 09:20:48 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Zhao", "Jieru", ""], ["Liang", "Tingyuan", ""], ["Feng", "Liang", ""], ["Ding", "Wenchao", ""], ["Sinha", "Sharad", ""], ["Zhang", "Wei", ""], ["Shen", "Shaojie", ""]]}, {"id": "2006.03254", "submitter": "Honghu Pan", "authors": "Honghu Pan, Fanyang Meng, Zhenyu He, Yongsheng Liang, Wei Liu", "title": "TCDesc: Learning Topology Consistent Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triplet loss is widely used for learning local descriptors from image patch.\nHowever, triplet loss only minimizes the Euclidean distance between matching\ndescriptors and maximizes that between the non-matching descriptors, which\nneglects the topology similarity between two descriptor sets. In this paper, we\npropose topology measure besides Euclidean distance to learn topology\nconsistent descriptors by considering kNN descriptors of positive sample. First\nwe establish a novel topology vector for each descriptor followed by Locally\nLinear Embedding (LLE) to indicate the topological relation among the\ndescriptor and its kNN descriptors. Then we define topology distance between\ndescriptors as the difference of their topology vectors. Last we employ the\ndynamic weighting strategy to fuse Euclidean distance and topology distance of\nmatching descriptors and take the fusion result as the positive sample distance\nin the triplet loss. Experimental results on several benchmarks show that our\nmethod performs better than state-of-the-arts results and effectively improves\nthe performance of triplet loss.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 06:46:30 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Pan", "Honghu", ""], ["Meng", "Fanyang", ""], ["He", "Zhenyu", ""], ["Liang", "Yongsheng", ""], ["Liu", "Wei", ""]]}, {"id": "2006.03259", "submitter": "Xin Cheng", "authors": "Xin Cheng, Lei Zhang, Yin Tang, Yue Liu, Hao Wu and Jun He", "title": "Real-time Human Activity Recognition Using Conditionally Parametrized\n  Convolutions on Mobile and Wearable Devices", "comments": "10 pages,14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning has represented an important research trend in human\nactivity recognition (HAR). In particular, deep convolutional neural networks\n(CNNs) have achieved state-of-the-art performance on various HAR datasets. For\ndeep learning, improvements in performance have to heavily rely on increasing\nmodel size or capacity to scale to larger and larger datasets, which inevitably\nleads to the increase of operations. A high number of operations in deep\nleaning increases computational cost and is not suitable for real-time HAR\nusing mobile and wearable sensors. Though shallow learning techniques often are\nlightweight, they could not achieve good performance. Therefore, deep learning\nmethods that can balance the trade-off between accuracy and computation cost is\nhighly needed, which to our knowledge has seldom been researched. In this\npaper, we for the first time propose a computation efficient CNN using\nconditionally parametrized convolution for real-time HAR on mobile and wearable\ndevices. We evaluate the proposed method on four public benchmark HAR datasets\nconsisting of WISDM dataset, PAMAP2 dataset, UNIMIB-SHAR dataset, and\nOPPORTUNITY dataset, achieving state-of-the-art accuracy without compromising\ncomputation cost. Various ablation experiments are performed to show how such a\nnetwork with large capacity is clearly preferable to baseline while requiring a\nsimilar amount of operations. The method can be used as a drop-in replacement\nfor the existing deep HAR architectures and easily deployed onto mobile and\nwearable devices for real-time HAR applications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 07:06:42 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 07:55:34 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Cheng", "Xin", ""], ["Zhang", "Lei", ""], ["Tang", "Yin", ""], ["Liu", "Yue", ""], ["Wu", "Hao", ""], ["He", "Jun", ""]]}, {"id": "2006.03267", "submitter": "Christina Corbane", "authors": "Christina Corbane, Vasileios Syrris, Filip Sabo, Panagiotis Politis,\n  Michele Melchiorri, Martino Pesaresi, Pierre Soille, Thomas Kemper", "title": "Convolutional Neural Networks for Global Human Settlements Mapping from\n  Sentinel-2 Satellite Imagery", "comments": "51 pages including supplementary material, 13 Figures in the main\n  manuscript, under review in Neural Computing and Applications journal", "journal-ref": "Neural computing and Applications, 2020", "doi": "10.1007/s00521-020-05449-7", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially consistent and up-to-date maps of human settlements are crucial for\naddressing policies related to urbanization and sustainability, especially in\nthe era of an increasingly urbanized world.The availability of open and free\nSentinel-2 data of the Copernicus Earth Observation program offers a new\nopportunity for wall-to-wall mapping of human settlements at a global\nscale.This paper presents a deep-learning-based framework for a fully automated\nextraction of built-up areas at a spatial resolution of 10 m from a global\ncomposite of Sentinel-2 imagery.A multi-neuro modeling methodology building on\na simple Convolution Neural Networks architecture for pixel-wise image\nclassification of built-up areas is developed.The core features of the proposed\nmodel are the image patch of size 5 x 5 pixels adequate for describing built-up\nareas from Sentinel-2 imagery and the lightweight topology with a total number\nof 1,448,578 trainable parameters and 4 2D convolutional layers and 2 flattened\nlayers.The deployment of the model on the global Sentinel-2 image composite\nprovides the most detailed and complete map reporting about built-up areas for\nreference year 2018. The validation of the results with an independent\nreference data-set of building footprints covering 277 sites across the world\nestablishes the reliability of the built-up layer produced by the proposed\nframework and the model robustness.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 07:28:19 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 09:14:57 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Corbane", "Christina", ""], ["Syrris", "Vasileios", ""], ["Sabo", "Filip", ""], ["Politis", "Panagiotis", ""], ["Melchiorri", "Michele", ""], ["Pesaresi", "Martino", ""], ["Soille", "Pierre", ""], ["Kemper", "Thomas", ""]]}, {"id": "2006.03298", "submitter": "Javier Hernandez-Ortega", "authors": "Javier Hernandez-Ortega, Javier Galbally, Julian Fierrez, Laurent\n  Beslay", "title": "Biometric Quality: Review and Application to Face Recognition with\n  FaceQnet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"The output of a computerised system can only be as accurate as the\ninformation entered into it.\" This rather trivial statement is the basis behind\none of the driving concepts in biometric recognition: biometric quality.\nQuality is nowadays widely regarded as the number one factor responsible for\nthe good or bad performance of automated biometric systems. It refers to the\nability of a biometric sample to be used for recognition purposes and produce\nconsistent, accurate, and reliable results. Such a subjective term is\nobjectively estimated by the so-called biometric quality metrics. These\nalgorithms play nowadays a pivotal role in the correct functioning of systems,\nproviding feedback to the users and working as invaluable audit tools. In spite\nof their unanimously accepted relevance, some of the most used and deployed\nbiometric characteristics are lacking behind in the development of these\nmethods. This is the case of face recognition. After a gentle introduction to\nthe general topic of biometric quality and a review of past efforts in face\nquality metrics, in the present work, we address the need for better face\nquality metrics by developing FaceQnet. FaceQnet is a novel open-source face\nquality assessment tool, inspired and powered by deep learning technology,\nwhich assigns a scalar quality measure to facial images, as prediction of their\nrecognition accuracy. Two versions of FaceQnet have been thoroughly evaluated\nboth in this work and also independently by NIST, showing the soundness of the\napproach and its competitiveness with respect to current state-of-the-art\nmetrics. Even though our work is presented here particularly in the framework\nof face biometrics, the proposed methodology for building a fully automated\nquality metric can be very useful and easily adapted to other artificial\nintelligence tasks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 08:33:22 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 15:21:11 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 07:46:41 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hernandez-Ortega", "Javier", ""], ["Galbally", "Javier", ""], ["Fierrez", "Julian", ""], ["Beslay", "Laurent", ""]]}, {"id": "2006.03315", "submitter": "Ke Lin", "authors": "Ke Lin, Zhuoxin Gan and Liwei Wang", "title": "Multi-modal Feature Fusion with Feature Attention for VATEX Captioning\n  Challenge 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes our model for VATEX Captioning Challenge 2020. First,\nto gather information from multiple domains, we extract motion, appearance,\nsemantic and audio features. Then we design a feature attention module to\nattend on different feature when decoding. We apply two types of decoders,\ntop-down and X-LAN and ensemble these models to get the final result. The\nproposed method outperforms official baseline with a significant gap. We\nachieve 76.0 CIDEr and 50.0 CIDEr on English and Chinese private test set. We\nrank 2nd on both English and Chinese private test leaderboard.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 09:00:36 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Lin", "Ke", ""], ["Gan", "Zhuoxin", ""], ["Wang", "Liwei", ""]]}, {"id": "2006.03340", "submitter": "Federico Becattini", "authors": "Francesco Marchetti, Federico Becattini, Lorenzo Seidenari, Alberto\n  Del Bimbo", "title": "MANTRA: Memory Augmented Networks for Multiple Trajectory Prediction", "comments": "Accepted at CVPR20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles are expected to drive in complex scenarios with several\nindependent non cooperating agents. Path planning for safely navigating in such\nenvironments can not just rely on perceiving present location and motion of\nother agents. It requires instead to predict such variables in a far enough\nfuture. In this paper we address the problem of multimodal trajectory\nprediction exploiting a Memory Augmented Neural Network. Our method learns past\nand future trajectory embeddings using recurrent neural networks and exploits\nan associative external memory to store and retrieve such embeddings.\nTrajectory prediction is then performed by decoding in-memory future encodings\nconditioned with the observed past. We incorporate scene knowledge in the\ndecoding state by learning a CNN on top of semantic scene maps. Memory growth\nis limited by learning a writing controller based on the predictive capability\nof existing embeddings. We show that our method is able to natively perform\nmulti-modal trajectory prediction obtaining state-of-the art results on three\ndatasets. Moreover, thanks to the non-parametric nature of the memory module,\nwe show how once trained our system can continuously improve by ingesting novel\npatterns.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 09:49:59 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 22:52:06 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Marchetti", "Francesco", ""], ["Becattini", "Federico", ""], ["Seidenari", "Lorenzo", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2006.03347", "submitter": "Federico Becattini", "authors": "Luca Cultrera, Lorenzo Seidenari, Federico Becattini, Pietro Pala,\n  Alberto Del Bimbo", "title": "Explaining Autonomous Driving by Learning End-to-End Visual Attention", "comments": "accepted at CVPR20 Workshop on Safe Artificial Intelligence for\n  Automated Driving (SAIAD20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning based autonomous driving approaches yield impressive\nresults also leading to in-production deployment in certain controlled\nscenarios. One of the most popular and fascinating approaches relies on\nlearning vehicle controls directly from data perceived by sensors. This\nend-to-end learning paradigm can be applied both in classical supervised\nsettings and using reinforcement learning. Nonetheless the main drawback of\nthis approach as also in other learning problems is the lack of explainability.\nIndeed, a deep network will act as a black-box outputting predictions depending\non previously seen driving patterns without giving any feedback on why such\ndecisions were taken. While to obtain optimal performance it is not critical to\nobtain explainable outputs from a learned agent, especially in such a safety\ncritical field, it is of paramount importance to understand how the network\nbehaves. This is particularly relevant to interpret failures of such systems.\nIn this work we propose to train an imitation learning based agent equipped\nwith an attention model. The attention model allows us to understand what part\nof the image has been deemed most important. Interestingly, the use of\nattention also leads to superior performance in a standard benchmark using the\nCARLA driving simulator.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 10:12:31 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Cultrera", "Luca", ""], ["Seidenari", "Lorenzo", ""], ["Becattini", "Federico", ""], ["Pala", "Pietro", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2006.03349", "submitter": "Abdelrahman Eldesokey", "authors": "Abdelrahman Eldesokey, Michael Felsberg, Karl Holmquist, and Mikael\n  Persson", "title": "Uncertainty-Aware CNNs for Depth Completion: Uncertainty from Beginning\n  to End", "comments": "CVPR2020 (8 pages + supplementary)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus in deep learning research has been mostly to push the limits of\nprediction accuracy. However, this was often achieved at the cost of increased\ncomplexity, raising concerns about the interpretability and the reliability of\ndeep networks. Recently, an increasing attention has been given to untangling\nthe complexity of deep networks and quantifying their uncertainty for different\ncomputer vision tasks. Differently, the task of depth completion has not\nreceived enough attention despite the inherent noisy nature of depth sensors.\nIn this work, we thus focus on modeling the uncertainty of depth data in depth\ncompletion starting from the sparse noisy input all the way to the final\nprediction.\n  We propose a novel approach to identify disturbed measurements in the input\nby learning an input confidence estimator in a self-supervised manner based on\nthe normalized convolutional neural networks (NCNNs). Further, we propose a\nprobabilistic version of NCNNs that produces a statistically meaningful\nuncertainty measure for the final prediction. When we evaluate our approach on\nthe KITTI dataset for depth completion, we outperform all the existing Bayesian\nDeep Learning approaches in terms of prediction accuracy, quality of the\nuncertainty measure, and the computational efficiency. Moreover, our small\nnetwork with 670k parameters performs on-par with conventional approaches with\nmillions of parameters. These results give strong evidence that separating the\nnetwork into parallel uncertainty and prediction streams leads to\nstate-of-the-art performance with accurate uncertainty estimates.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 10:18:35 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Eldesokey", "Abdelrahman", ""], ["Felsberg", "Michael", ""], ["Holmquist", "Karl", ""], ["Persson", "Mikael", ""]]}, {"id": "2006.03361", "submitter": "Martin Wistuba", "authors": "Martin Wistuba and Tejaswini Pedapati", "title": "Learning to Rank Learning Curves", "comments": "Accepted at the International Conference on Machine Learning (ICML)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many automated machine learning methods, such as those for hyperparameter and\nneural architecture optimization, are computationally expensive because they\ninvolve training many different model configurations. In this work, we present\na new method that saves computational budget by terminating poor configurations\nearly on in the training. In contrast to existing methods, we consider this\ntask as a ranking and transfer learning problem. We qualitatively show that by\noptimizing a pairwise ranking loss and leveraging learning curves from other\ndatasets, our model is able to effectively rank learning curves without having\nto observe many or very long learning curves. We further demonstrate that our\nmethod can be used to accelerate a neural architecture search by a factor of up\nto 100 without a significant performance degradation of the discovered\narchitecture. In further experiments we analyze the quality of ranking, the\ninfluence of different model components as well as the predictive behavior of\nthe model.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 10:49:52 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Wistuba", "Martin", ""], ["Pedapati", "Tejaswini", ""]]}, {"id": "2006.03374", "submitter": "Vismay Agrawal", "authors": "Vismay Agrawal, Avinash Kori, Vikas Kumar Anand, and Ganapathy\n  Krishnamurthi", "title": "Structurally aware bidirectional unpaired image to image translation\n  between CT and MR", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Magnetic Resonance (MR) Imaging and Computed Tomography (CT) are the primary\ndiagnostic imaging modalities quite frequently used for surgical planning and\nanalysis. A general problem with medical imaging is that the acquisition\nprocess is quite expensive and time-consuming. Deep learning techniques like\ngenerative adversarial networks (GANs) can help us to leverage the possibility\nof an image to image translation between multiple imaging modalities, which in\nturn helps in saving time and cost. These techniques will help to conduct\nsurgical planning under CT with the feedback of MRI information. While previous\nstudies have shown paired and unpaired image synthesis from MR to CT, image\nsynthesis from CT to MR still remains a challenge, since it involves the\naddition of extra tissue information. In this manuscript, we have implemented\ntwo different variations of Generative Adversarial Networks exploiting the\ncycling consistency and structural similarity between both CT and MR image\nmodalities on a pelvis dataset, thus facilitating a bidirectional exchange of\ncontent and style between these image modalities. The proposed GANs translate\nthe input medical images by different mechanisms, and hence generated images\nnot only appears realistic but also performs well across various comparison\nmetrics, and these images have also been cross verified with a radiologist. The\nradiologist verification has shown that slight variations in generated MR and\nCT images may not be exactly the same as their true counterpart but it can be\nused for medical purposes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 11:21:56 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Agrawal", "Vismay", ""], ["Kori", "Avinash", ""], ["Anand", "Vikas Kumar", ""], ["Krishnamurthi", "Ganapathy", ""]]}, {"id": "2006.03394", "submitter": "Hans Pinckaers", "authors": "Hans Pinckaers, Wouter Bulten, Jeroen van der Laak, Geert Litjens", "title": "Detection of prostate cancer in whole-slide images through end-to-end\n  training with image-level labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prostate cancer is the most prevalent cancer among men in Western countries,\nwith 1.1 million new diagnoses every year. The gold standard for the diagnosis\nof prostate cancer is a pathologists' evaluation of prostate tissue.\n  To potentially assist pathologists deep-learning-based cancer detection\nsystems have been developed. Many of the state-of-the-art models are\npatch-based convolutional neural networks, as the use of entire scanned slides\nis hampered by memory limitations on accelerator cards. Patch-based systems\ntypically require detailed, pixel-level annotations for effective training.\nHowever, such annotations are seldom readily available, in contrast to the\nclinical reports of pathologists, which contain slide-level labels. As such,\ndeveloping algorithms which do not require manual pixel-wise annotations, but\ncan learn using only the clinical report would be a significant advancement for\nthe field.\n  In this paper, we propose to use a streaming implementation of convolutional\nlayers, to train a modern CNN (ResNet-34) with 21 million parameters end-to-end\non 4712 prostate biopsies. The method enables the use of entire biopsy images\nat high-resolution directly by reducing the GPU memory requirements by 2.4 TB.\nWe show that modern CNNs, trained using our streaming approach, can extract\nmeaningful features from high-resolution images without additional heuristics,\nreaching similar performance as state-of-the-art patch-based and\nmultiple-instance learning methods. By circumventing the need for manual\nannotations, this approach can function as a blueprint for other tasks in\nhistopathological diagnosis.\n  The source code to reproduce the streaming models is available at\nhttps://github.com/DIAGNijmegen/pathology-streaming-pipeline .\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 12:11:35 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Pinckaers", "Hans", ""], ["Bulten", "Wouter", ""], ["van der Laak", "Jeroen", ""], ["Litjens", "Geert", ""]]}, {"id": "2006.03427", "submitter": "Paul Sanzenbacher", "authors": "Paul Sanzenbacher, Lars Mescheder, Andreas Geiger", "title": "Learning Neural Light Transport", "comments": "31 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep generative models have gained significance due to their\nability to synthesize natural-looking images with applications ranging from\nvirtual reality to data augmentation for training computer vision models. While\nexisting models are able to faithfully learn the image distribution of the\ntraining set, they often lack controllability as they operate in 2D pixel space\nand do not model the physical image formation process. In this work, we\ninvestigate the importance of 3D reasoning for photorealistic rendering. We\npresent an approach for learning light transport in static and dynamic 3D\nscenes using a neural network with the goal of predicting photorealistic\nimages. In contrast to existing approaches that operate in the 2D image domain,\nour approach reasons in both 3D and 2D space, thus enabling global illumination\neffects and manipulation of 3D scene geometry. Experimentally, we find that our\nmodel is able to produce photorealistic renderings of static and dynamic\nscenes. Moreover, it compares favorably to baselines which combine path tracing\nand image denoising at the same computational budget.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 13:26:05 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Sanzenbacher", "Paul", ""], ["Mescheder", "Lars", ""], ["Geiger", "Andreas", ""]]}, {"id": "2006.03429", "submitter": "Robert M\\\"uller", "authors": "Robert M\\\"uller, Fabian Ritz, Steffen Illium and Claudia\n  Linnhoff-Popien", "title": "Acoustic Anomaly Detection for Machine Sounds based on Image Transfer\n  Learning", "comments": "ICAART 2021, 8 pages, 2 figures, 1 table", "journal-ref": null, "doi": "10.5220/0010185800490056", "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In industrial applications, the early detection of malfunctioning factory\nmachinery is crucial. In this paper, we consider acoustic malfunction detection\nvia transfer learning. Contrary to the majority of current approaches which are\nbased on deep autoencoders, we propose to extract features using neural\nnetworks that were pretrained on the task of image classification. We then use\nthese features to train a variety of anomaly detection models and show that\nthis improves results compared to convolutional autoencoders in recordings of\nfour different factory machines in noisy environments. Moreover, we find that\nfeatures extracted from ResNet based networks yield better results than those\nfrom AlexNet and Squeezenet. In our setting, Gaussian Mixture Models and\nOne-Class Support Vector Machines achieve the best anomaly detection\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 13:29:12 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 12:06:30 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["M\u00fcller", "Robert", ""], ["Ritz", "Fabian", ""], ["Illium", "Steffen", ""], ["Linnhoff-Popien", "Claudia", ""]]}, {"id": "2006.03434", "submitter": "Mathias Unberath", "authors": "Mathias Unberath and Kimia Ghobadi and Scott Levin and Jeremiah Hinson\n  and Gregory D Hager", "title": "Artificial Intelligence-based Clinical Decision Support for COVID-19 --\n  Where Art Thou?", "comments": "Invited perspective piece on AI in the fight against COVID-19 to\n  appear in Advanced Intelligent Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 crisis has brought about new clinical questions, new workflows,\nand accelerated distributed healthcare needs. While artificial intelligence\n(AI)-based clinical decision support seemed to have matured, the application of\nAI-based tools for COVID-19 has been limited to date. In this perspective\npiece, we identify opportunities and requirements for AI-based clinical\ndecision support systems and highlight challenges that impact \"AI readiness\"\nfor rapidly emergent healthcare challenges.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 13:34:47 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Unberath", "Mathias", ""], ["Ghobadi", "Kimia", ""], ["Levin", "Scott", ""], ["Hinson", "Jeremiah", ""], ["Hager", "Gregory D", ""]]}, {"id": "2006.03486", "submitter": "Inigo Azqueta-Gavaldon", "authors": "I\\~nigo Azqueta-Gavaldon, Florian Fr\\\"ohlich, Klaus Strobl and Rudolph\n  Triebel", "title": "Segmentation of Surgical Instruments for Minimally-Invasive\n  Robot-Assisted Procedures Using Generative Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proves that semantic segmentation on minimally invasive surgical\ninstruments can be improved by using training data that has been augmented\nthrough domain adaptation. The benefit of this method is twofold. Firstly, it\nsuppresses the need of manually labeling thousands of images by transforming\nsynthetic data into realistic-looking data. To achieve this, a CycleGAN model\nis used, which transforms a source dataset to approximate the domain\ndistribution of a target dataset. Secondly, this newly generated data with\nperfect labels is utilized to train a semantic segmentation neural network,\nU-Net. This method shows generalization capabilities on data with variability\nregarding its rotation- position- and lighting conditions. Nevertheless, one of\nthe caveats of this approach is that the model is unable to generalize well to\nother surgical instruments with a different shape from the one used for\ntraining. This is driven by the lack of a high variance in the geometric\ndistribution of the training data. Future work will focus on making the model\nmore scale-invariant and able to adapt to other types of surgical instruments\npreviously unseen by the training.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 14:39:41 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Azqueta-Gavaldon", "I\u00f1igo", ""], ["Fr\u00f6hlich", "Florian", ""], ["Strobl", "Klaus", ""], ["Triebel", "Rudolph", ""]]}, {"id": "2006.03492", "submitter": "Peter Ondruska", "authors": "Giacomo Dabisias, Emanuele Ruffaldi, Hugo Grimmett, Peter Ondruska", "title": "VALUE: Large Scale Voting-based Automatic Labelling for Urban\n  Environments", "comments": "Presented at ICRA-2018 conference, 20-25th May 2018, Brisbane,\n  Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a simple and robust method for the automatic localisation\nof static 3D objects in large-scale urban environments. By exploiting the\npotential to merge a large volume of noisy but accurately localised 2D image\ndata, we achieve superior performance in terms of both robustness and accuracy\nof the recovered 3D information. The method is based on a simple distributed\nvoting schema which can be fully distributed and parallelised to scale to\nlarge-scale scenarios. To evaluate the method we collected city-scale data sets\nfrom New York City and San Francisco consisting of almost 400k images spanning\nthe area of 40 km$^2$ and used it to accurately recover the 3D positions of\ntraffic lights. We demonstrate a robust performance and also show that the\nsolution improves in quality over time as the amount of data increases.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 14:47:03 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Dabisias", "Giacomo", ""], ["Ruffaldi", "Emanuele", ""], ["Grimmett", "Hugo", ""], ["Ondruska", "Peter", ""]]}, {"id": "2006.03531", "submitter": "Maell Cullen", "authors": "Maell Cullen, Jonathan Monney, M. Berk Mirza, Rosalyn Moran", "title": "A Meta-Bayesian Model of Intentional Visual Search", "comments": "Submitted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computational model of visual search that incorporates Bayesian\ninterpretations of the neural mechanisms that underlie categorical perception\nand saccade planning. To enable meaningful comparisons between simulated and\nhuman behaviours, we employ a gaze-contingent paradigm that required\nparticipants to classify occluded MNIST digits through a window that followed\ntheir gaze. The conditional independencies imposed by a separation of time\nscales in this task are embodied by constraints on the hierarchical structure\nof our model; planning and decision making are cast as a partially observable\nMarkov Decision Process while proprioceptive and exteroceptive signals are\nintegrated by a dynamic model that facilitates approximate inference on visual\ninformation and its latent causes. Our model is able to recapitulate human\nbehavioural metrics such as classification accuracy while retaining a high\ndegree of interpretability, which we demonstrate by recovering subject-specific\nparameters from observed human behaviour.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 16:10:35 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Cullen", "Maell", ""], ["Monney", "Jonathan", ""], ["Mirza", "M. Berk", ""], ["Moran", "Rosalyn", ""]]}, {"id": "2006.03586", "submitter": "Mohamed El Banani", "authors": "Mohamed El Banani, Jason J. Corso, David F. Fouhey", "title": "Novel Object Viewpoint Estimation through Reconstruction Alignment", "comments": "To appear at CVPR 2020. Project page:\n  https://mbanani.github.io/novelviewpoints/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to estimate the viewpoint for a novel object.\nStandard viewpoint estimation approaches generally fail on this task due to\ntheir reliance on a 3D model for alignment or large amounts of class-specific\ntraining data and their corresponding canonical pose. We overcome those\nlimitations by learning a reconstruct and align approach. Our key insight is\nthat although we do not have an explicit 3D model or a predefined canonical\npose, we can still learn to estimate the object's shape in the viewer's frame\nand then use an image to provide our reference model or canonical pose. In\nparticular, we propose learning two networks: the first maps images to a 3D\ngeometry-aware feature bottleneck and is trained via an image-to-image\ntranslation loss; the second learns whether two instances of features are\naligned. At test time, our model finds the relative transformation that best\naligns the bottleneck features of our test image to a reference image. We\nevaluate our method on novel object viewpoint estimation by generalizing across\ndifferent datasets, analyzing the impact of our different modules, and\nproviding a qualitative analysis of the learned features to identify what\nrepresentations are being learnt for alignment.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 17:58:14 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Banani", "Mohamed El", ""], ["Corso", "Jason J.", ""], ["Fouhey", "David F.", ""]]}, {"id": "2006.03622", "submitter": "Saman Motamed", "authors": "Saman Motamed and Patrik Rogalla and Farzad Khalvati", "title": "Data Augmentation using Generative Adversarial Networks (GANs) for\n  GAN-based Detection of Pneumonia and COVID-19 in Chest X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful training of convolutional neural networks (CNNs) requires a\nsubstantial amount of data. With small datasets networks generalize poorly.\nData Augmentation techniques improve the generalizability of neural networks by\nusing existing training data more effectively. Standard data augmentation\nmethods, however, produce limited plausible alternative data. Generative\nAdversarial Networks (GANs) have been utilized to generate new data and improve\nthe performance of CNNs. Nevertheless, data augmentation techniques for\ntraining GANs are under-explored compared to CNNs. In this work, we propose a\nnew GAN architecture for augmentation of chest X-rays for semi-supervised\ndetection of pneumonia and COVID-19 using generative models. We show that the\nproposed GAN can be used to effectively augment data and improve classification\naccuracy of disease in chest X-rays for pneumonia and COVID-19. We compare our\naugmentation GAN model with Deep Convolutional GAN and traditional augmentation\nmethods (rotate, zoom, etc) on two different X-ray datasets and show our\nGAN-based augmentation method surpasses other augmentation methods for training\na GAN in detecting anomalies in X-ray images.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 18:30:20 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 20:27:04 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Motamed", "Saman", ""], ["Rogalla", "Patrik", ""], ["Khalvati", "Farzad", ""]]}, {"id": "2006.03627", "submitter": "Siamak Ravanbakhsh", "authors": "Renhao Wang, Marjan Albooyeh, Siamak Ravanbakhsh", "title": "Equivariant Maps for Hierarchical Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While using invariant and equivariant maps, it is possible to apply deep\nlearning to a range of primitive data structures, a formalism for dealing with\nhierarchy is lacking. This is a significant issue because many practical\nstructures are hierarchies of simple building blocks; some examples include\nsequences of sets, graphs of graphs, or multiresolution images. Observing that\nthe symmetry of a hierarchical structure is the \"wreath product\" of symmetries\nof the building blocks, we express the equivariant map for the hierarchy using\nan intuitive combination of the equivariant linear layers of the building\nblocks. More generally, we show that any equivariant map for the hierarchy has\nthis form. To demonstrate the effectiveness of this approach to model design,\nwe consider its application in the semantic segmentation of point-cloud data.\nBy voxelizing the point cloud, we impose a hierarchy of translation and\npermutation symmetries on the data and report state-of-the-art on Semantic3D,\nS3DIS, and vKITTI, that include some of the largest real-world point-cloud\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 18:42:12 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 01:54:52 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Wang", "Renhao", ""], ["Albooyeh", "Marjan", ""], ["Ravanbakhsh", "Siamak", ""]]}, {"id": "2006.03629", "submitter": "Palash Goyal", "authors": "Palash Goyal and Shalini Ghosh", "title": "Hierarchical Class-Based Curriculum Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification algorithms in machine learning often assume a flat label\nspace. However, most real world data have dependencies between the labels,\nwhich can often be captured by using a hierarchy. Utilizing this relation can\nhelp develop a model capable of satisfying the dependencies and improving model\naccuracy and interpretability. Further, as different levels in the hierarchy\ncorrespond to different granularities, penalizing each label equally can be\ndetrimental to model learning. In this paper, we propose a loss function,\nhierarchical curriculum loss, with two properties: (i) satisfy hierarchical\nconstraints present in the label space, and (ii) provide non-uniform weights to\nlabels based on their levels in the hierarchy, learned implicitly by the\ntraining paradigm. We theoretically show that the proposed loss function is a\ntighter bound of 0-1 loss compared to any other loss satisfying the\nhierarchical constraints. We test our loss function on real world image data\nsets, and show that it significantly substantially outperforms multiple\nbaselines.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 18:48:57 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Goyal", "Palash", ""], ["Ghosh", "Shalini", ""]]}, {"id": "2006.03630", "submitter": "Sen Wang", "authors": "Xinxin Zuo and Sen Wang and Jiangbin Zheng and Weiwei Yu and Minglun\n  Gong and Ruigang Yang and Li Cheng", "title": "SparseFusion: Dynamic Human Avatar Modeling from Sparse RGBD Images", "comments": "Accepted by TMM", "journal-ref": null, "doi": "10.1109/TMM.2020.3001506", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach to reconstruct 3D human body\nshapes based on a sparse set of RGBD frames using a single RGBD camera. We\nspecifically focus on the realistic settings where human subjects move freely\nduring the capture. The main challenge is how to robustly fuse these sparse\nframes into a canonical 3D model, under pose changes and surface occlusions.\nThis is addressed by our new framework consisting of the following steps.\nFirst, based on a generative human template, for every two frames having\nsufficient overlap, an initial pairwise alignment is performed; It is followed\nby a global non-rigid registration procedure, in which partial results from\nRGBD frames are collected into a unified 3D shape, under the guidance of\ncorrespondences from the pairwise alignment; Finally, the texture map of the\nreconstructed human model is optimized to deliver a clear and spatially\nconsistent texture. Empirical evaluations on synthetic and real datasets\ndemonstrate both quantitatively and qualitatively the superior performance of\nour framework in reconstructing complete 3D human models with high fidelity. It\nis worth noting that our framework is flexible, with potential applications\ngoing beyond shape reconstruction. As an example, we showcase its use in\nreshaping and reposing to a new avatar.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 18:53:36 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zuo", "Xinxin", ""], ["Wang", "Sen", ""], ["Zheng", "Jiangbin", ""], ["Yu", "Weiwei", ""], ["Gong", "Minglun", ""], ["Yang", "Ruigang", ""], ["Cheng", "Li", ""]]}, {"id": "2006.03638", "submitter": "Marius Arvinte", "authors": "Marius Arvinte, Ahmed H. Tewfik and Sriram Vishwanath", "title": "Robust Face Verification via Disentangled Representations", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a robust algorithm for face verification, i.e., deciding whether\ntwoimages are of the same person or not. Our approach is a novel take on the\nidea ofusing deep generative networks for adversarial robustness. We use the\ngenerativemodel during training as an online augmentation method instead of a\ntest-timepurifier that removes adversarial noise. Our architecture uses a\ncontrastive loss termand a disentangled generative model to sample negative\npairs. Instead of randomlypairing two real images, we pair an image with its\nclass-modified counterpart whilekeeping its content (pose, head tilt, hair,\netc.) intact. This enables us to efficientlysample hard negative pairs for the\ncontrastive loss. We experimentally show that, when coupled with adversarial\ntraining, the proposed scheme converges with aweak inner solver and has a\nhigher clean and robust accuracy than state-of-the-art-methods when evaluated\nagainst white-box physical attacks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 19:17:02 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 16:43:02 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Arvinte", "Marius", ""], ["Tewfik", "Ahmed H.", ""], ["Vishwanath", "Sriram", ""]]}, {"id": "2006.03641", "submitter": "Jingxiao Liu", "authors": "Jingxiao Liu, Mario Berg\\'es, Jacobo Bielak, Hae Young Noh", "title": "Knowledge transfer between bridges for drive-by monitoring using\n  adversarial and multi-task learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring bridge health using the vibrations of drive-by vehicles has\nvarious benefits, such as low cost and no need for direct installation or\non-site maintenance of equipment on the bridge. However, many such approaches\nrequire labeled data from every bridge, which is expensive and time-consuming,\nif not impossible, to obtain. This is further exacerbated by having multiple\ndiagnostic tasks, such as damage quantification and localization. One way to\naddress this issue is to directly apply the supervised model trained for one\nbridge to other bridges, although this may significantly reduce the accuracy\nbecause of distribution mismatch between different bridges'data. To alleviate\nthese problems, we introduce a transfer learning framework using\ndomain-adversarial training and multi-task learning to detect, localize and\nquantify damage. Specifically, we train a deep network in an adversarial way to\nlearn features that are 1) sensitive to damage and 2) invariant to different\nbridges. In addition, to improve the error propagation from one task to the\nnext, our framework learns shared features for all the tasks using multi-task\nlearning. We evaluate our framework using lab-scale experiments with two\ndifferent bridges. On average, our framework achieves 94%, 97% and 84% accuracy\nfor damage detection, localization and quantification, respectively. within one\ndamage severity level.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 19:18:45 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Liu", "Jingxiao", ""], ["Berg\u00e9s", "Mario", ""], ["Bielak", "Jacobo", ""], ["Noh", "Hae Young", ""]]}, {"id": "2006.03642", "submitter": "Nitinraj Nair Mr.", "authors": "Nitinraj Nair, Rakshit Kothari, Aayush K. Chaudhary, Zhizhuo Yang,\n  Gabriel J. Diaz, Jeff B. Pelz, Reynold J. Bailey", "title": "RIT-Eyes: Rendering of near-eye images for eye-tracking applications", "comments": null, "journal-ref": null, "doi": "10.1145/3385955.3407935", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks for video-based eye tracking have demonstrated\nresilience to noisy environments, stray reflections, and low resolution.\nHowever, to train these networks, a large number of manually annotated images\nare required. To alleviate the cumbersome process of manual labeling, computer\ngraphics rendering is employed to automatically generate a large corpus of\nannotated eye images under various conditions. In this work, we introduce a\nsynthetic eye image generation platform that improves upon previous work by\nadding features such as an active deformable iris, an aspherical cornea,\nretinal retro-reflection, gaze-coordinated eye-lid deformations, and blinks. To\ndemonstrate the utility of our platform, we render images reflecting the\nrepresented gaze distributions inherent in two publicly available datasets,\nNVGaze and OpenEDS. We also report on the performance of two semantic\nsegmentation architectures (SegNet and RITnet) trained on rendered images and\ntested on the original datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 19:18:50 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Nair", "Nitinraj", ""], ["Kothari", "Rakshit", ""], ["Chaudhary", "Aayush K.", ""], ["Yang", "Zhizhuo", ""], ["Diaz", "Gabriel J.", ""], ["Pelz", "Jeff B.", ""], ["Bailey", "Reynold J.", ""]]}, {"id": "2006.03656", "submitter": "Xuanyi Dong", "authors": "Xuanyi Dong, Mingxing Tan, Adams Wei Yu, Daiyi Peng, Bogdan Gabrys,\n  Quoc V. Le", "title": "AutoHAS: Efficient Hyperparameter and Architecture Search", "comments": "Accepted to 2nd Workshop on Neural Architecture Search at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient hyperparameter or architecture search methods have shown remarkable\nresults, but each of them is only applicable to searching for either\nhyperparameters (HPs) or architectures. In this work, we propose a unified\npipeline, AutoHAS, to efficiently search for both architectures and\nhyperparameters. AutoHAS learns to alternately update the shared network\nweights and a reinforcement learning (RL) controller, which learns the\nprobability distribution for the architecture candidates and HP candidates. A\ntemporary weight is introduced to store the updated weight from the selected\nHPs (by the controller), and a validation accuracy based on this temporary\nweight serves as a reward to update the controller. In experiments, we show\nAutoHAS is efficient and generalizable to different search spaces, baselines\nand datasets. In particular, AutoHAS can improve the accuracy over popular\nnetwork architectures, such as ResNet and EfficientNet, on CIFAR-10/100,\nImageNet, and four more other datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 19:57:24 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 05:01:34 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 06:55:00 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Dong", "Xuanyi", ""], ["Tan", "Mingxing", ""], ["Yu", "Adams Wei", ""], ["Peng", "Daiyi", ""], ["Gabrys", "Bogdan", ""], ["Le", "Quoc V.", ""]]}, {"id": "2006.03677", "submitter": "Bichen Wu", "authors": "Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang,\n  Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, Peter Vajda", "title": "Visual Transformers: Token-based Image Representation and Processing for\n  Computer Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has achieved remarkable success by (a) representing images as\nuniformly-arranged pixel arrays and (b) convolving highly-localized features.\nHowever, convolutions treat all image pixels equally regardless of importance;\nexplicitly model all concepts across all images, regardless of content; and\nstruggle to relate spatially-distant concepts. In this work, we challenge this\nparadigm by (a) representing images as semantic visual tokens and (b) running\ntransformers to densely model token relationships. Critically, our Visual\nTransformer operates in a semantic token space, judiciously attending to\ndifferent image parts based on context. This is in sharp contrast to\npixel-space transformers that require orders-of-magnitude more compute. Using\nan advanced training recipe, our VTs significantly outperform their\nconvolutional counterparts, raising ResNet accuracy on ImageNet top-1 by 4.6 to\n7 points while using fewer FLOPs and parameters. For semantic segmentation on\nLIP and COCO-stuff, VT-based feature pyramid networks (FPN) achieve 0.35 points\nhigher mIoU while reducing the FPN module's FLOPs by 6.5x.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 20:49:49 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 23:35:53 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 18:55:40 GMT"}, {"version": "v4", "created": "Fri, 20 Nov 2020 00:10:51 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Wu", "Bichen", ""], ["Xu", "Chenfeng", ""], ["Dai", "Xiaoliang", ""], ["Wan", "Alvin", ""], ["Zhang", "Peizhao", ""], ["Yan", "Zhicheng", ""], ["Tomizuka", "Masayoshi", ""], ["Gonzalez", "Joseph", ""], ["Keutzer", "Kurt", ""], ["Vajda", "Peter", ""]]}, {"id": "2006.03680", "submitter": "Sharon Zhou", "authors": "Sharon Zhou, Eric Zelikman, Fred Lu, Andrew Y. Ng, Gunnar Carlsson,\n  Stefano Ermon", "title": "Evaluating the Disentanglement of Deep Generative Models through\n  Manifold Topology", "comments": "Published at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning disentangled representations is regarded as a fundamental task for\nimproving the generalization, robustness, and interpretability of generative\nmodels. However, measuring disentanglement has been challenging and\ninconsistent, often dependent on an ad-hoc external model or specific to a\ncertain dataset. To address this, we present a method for quantifying\ndisentanglement that only uses the generative model, by measuring the\ntopological similarity of conditional submanifolds in the learned\nrepresentation. This method showcases both unsupervised and supervised\nvariants. To illustrate the effectiveness and applicability of our method, we\nempirically evaluate several state-of-the-art models across multiple datasets.\nWe find that our method ranks models similarly to existing methods. We make\nourcode publicly available at\nhttps://github.com/stanfordmlgroup/disentanglement.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 20:54:11 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 01:59:24 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 14:51:35 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2021 20:34:57 GMT"}, {"version": "v5", "created": "Wed, 17 Mar 2021 21:46:59 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Zhou", "Sharon", ""], ["Zelikman", "Eric", ""], ["Lu", "Fred", ""], ["Ng", "Andrew Y.", ""], ["Carlsson", "Gunnar", ""], ["Ermon", "Stefano", ""]]}, {"id": "2006.03698", "submitter": "Jonathan Vacher", "authors": "Jonathan Vacher, Aida Davila, Adam Kohn, Ruben Coen-Cagli", "title": "Texture Interpolation for Probing Visual Perception", "comments": "Paper + ref: 12 pages and 7 figures | Supplementary: 16 pages and 16\n  figures Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Texture synthesis models are important tools for understanding visual\nprocessing. In particular, statistical approaches based on neurally relevant\nfeatures have been instrumental in understanding aspects of visual perception\nand of neural coding. New deep learning-based approaches further improve the\nquality of synthetic textures. Yet, it is still unclear why deep texture\nsynthesis performs so well, and applications of this new framework to probe\nvisual perception are scarce. Here, we show that distributions of deep\nconvolutional neural network (CNN) activations of a texture are well described\nby elliptical distributions and therefore, following optimal transport theory,\nconstraining their mean and covariance is sufficient to generate new texture\nsamples. Then, we propose the natural geodesics (ie the shortest path between\ntwo points) arising with the optimal transport metric to interpolate between\narbitrary textures. Compared to other CNN-based approaches, our interpolation\nmethod appears to match more closely the geometry of texture perception, and\nour mathematical framework is better suited to study its statistical nature. We\napply our method by measuring the perceptual scale associated to the\ninterpolation parameter in human observers, and the neural sensitivity of\ndifferent areas of visual cortex in macaque monkeys.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 21:28:36 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 18:05:27 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Vacher", "Jonathan", ""], ["Davila", "Aida", ""], ["Kohn", "Adam", ""], ["Coen-Cagli", "Ruben", ""]]}, {"id": "2006.03708", "submitter": "Yujiang Wang", "authors": "Yujiang Wang, Mingzhi Dong, Jie Shen, Yiming Lin, Maja Pantic", "title": "Dilated Convolutions with Lateral Inhibitions for Semantic Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dilated convolutions are widely used in deep semantic segmentation models as\nthey can enlarge the filters' receptive field without adding additional weights\nnor sacrificing spatial resolution. However, as dilated convolutional filters\ndo not possess positional knowledge about the pixels on semantically meaningful\ncontours, they could lead to ambiguous predictions on object boundaries. In\naddition, although dilating the filter can expand its receptive field, the\ntotal number of sampled pixels remains unchanged, which usually comprises a\nsmall fraction of the receptive field's total area. Inspired by the Lateral\nInhibition (LI) mechanisms in human visual systems, we propose the dilated\nconvolution with lateral inhibitions (LI-Convs) to overcome these limitations.\nIntroducing LI mechanisms improves the convolutional filter's sensitivity to\nsemantic object boundaries. Moreover, since LI-Convs also implicitly take the\npixels from the laterally inhibited zones into consideration, they can also\nextract features at a denser scale. By integrating LI-Convs into the Deeplabv3+\narchitecture, we propose the Lateral Inhibited Atrous Spatial Pyramid Pooling\n(LI-ASPP), the Lateral Inhibited MobileNet-V2 (LI-MNV2) and the Lateral\nInhibited ResNet (LI-ResNet). Experimental results on three benchmark datasets\n(PASCAL VOC 2012, CelebAMask-HQ and ADE20K) show that our LI-based segmentation\nmodels outperform the baseline on all of them, thus verify the effectiveness\nand generality of the proposed LI-Convs.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 21:55:43 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 11:18:41 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 13:08:28 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Wang", "Yujiang", ""], ["Dong", "Mingzhi", ""], ["Shen", "Jie", ""], ["Lin", "Yiming", ""], ["Pantic", "Maja", ""]]}, {"id": "2006.03725", "submitter": "Donald Beaver", "authors": "Donald Beaver", "title": "Applied Awareness: Test-Driven GUI Development using Computer Vision and\n  Cryptography", "comments": "8 pages. Submitted for conference publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical user interface testing is significantly challenging, and automating\nit even more so. Test-driven development is impractical: it generally requires\nan initial implementation of the GUI to generate golden images or to construct\ninteractive test scenarios, and subsequent maintenance is costly. While\ncomputer vision has been applied to several aspects of GUI testing, we\ndemonstrate a novel and immediately applicable approach of interpreting GUI\npresentation in terms of backend communications, modeling \"awareness\" in the\nfashion employed by cryptographic proofs of security. This focus on backend\ncommunication circumvents deficiencies in typical testing methodologies that\nrely on platform-dependent UI affordances or accessibility features. Our\ninterdisciplinary work is ready for off-the-shelf practice: we report\nself-contained, practical implementation with both online and offline\nvalidation, using simple designer specifications at the outset and specifically\navoiding any requirements for a bootstrap implementation or golden images. In\naddition to practical implementation, ties to formal verification methods in\ncryptography are explored and explained, providing fertile perspectives on\nassurance in UI and interpretability in AI.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 22:46:48 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Beaver", "Donald", ""]]}, {"id": "2006.03732", "submitter": "Mingfei Gao", "authors": "Mingfei Gao, Yingbo Zhou, Ran Xu, Richard Socher, Caiming Xiong", "title": "WOAD: Weakly Supervised Online Action Detection in Untrimmed Videos", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online action detection in untrimmed videos aims to identify an action as it\nhappens, which makes it very important for real-time applications. Previous\nmethods rely on tedious annotations of temporal action boundaries for training,\nwhich hinders the scalability of online action detection systems. We propose\nWOAD, a weakly supervised framework that can be trained using only video-class\nlabels. WOAD contains two jointly-trained modules, i.e., temporal proposal\ngenerator (TPG) and online action recognizer (OAR). Supervised by video-class\nlabels, TPG works offline and targets at accurately mining pseudo frame-level\nlabels for OAR. With the supervisory signals from TPG, OAR learns to conduct\naction detection in an online fashion. Experimental results on THUMOS'14,\nActivityNet1.2 and ActivityNet1.3 show that our weakly-supervised method\nlargely outperforms weakly-supervised baselines and achieves comparable\nperformance to the previous strongly-supervised methods. Beyond that, WOAD is\nflexible to leverage strong supervision when it is available. When strongly\nsupervised, our method obtains the state-of-the-art results in the tasks of\nboth online per-frame action recognition and online detection of action start.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 23:08:41 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 18:19:25 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Gao", "Mingfei", ""], ["Zhou", "Yingbo", ""], ["Xu", "Ran", ""], ["Socher", "Richard", ""], ["Xiong", "Caiming", ""]]}, {"id": "2006.03733", "submitter": "Sayeed Shafayet Chowdhury", "authors": "Sayeed Shafayet Chowdhury, Kazi Mejbaul Islam and Rouhan Noor", "title": "Unsupervised Abnormality Detection Using Heterogeneous Autonomous\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection (AD) in a surveillance scenario is an emerging and\nchallenging field of research. For autonomous vehicles like drones or cars, it\nis immensely important to distinguish between normal and abnormal states in\nreal-time. Additionally, we also need to detect any device malfunction. But the\nnature and degree of abnormality may vary depending upon the actual environment\nand adversary. As a result, it is impractical to model all cases a-priori and\nuse supervised methods to classify. Also, an autonomous vehicle provides\nvarious data types like images and other analog or digital sensor data, all of\nwhich can be useful in anomaly detection if leveraged fruitfully. To that\neffect, in this paper, a heterogeneous system is proposed which estimates the\ndegree of abnormality of an unmanned surveillance drone, analyzing real-time\nimage and IMU (Inertial Measurement Unit) sensor data in an unsupervised\nmanner. Here, we have demonstrated a Convolutional Neural Network (CNN)\narchitecture, named AngleNet to estimate the angle between a normal image and\nanother image under consideration, which provides us with a measure of anomaly\nof the device. Moreover, the IMU data are used in autoencoder to predict\nabnormality. Finally, the results from these two algorithms are ensembled to\nestimate the final degree of abnormality. The proposed method performs\nsatisfactorily on the IEEE SP Cup-2020 dataset with an accuracy of 97.3%.\nAdditionally, we have also tested this approach on an in-house dataset to\nvalidate its robustness.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 23:09:58 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 16:34:23 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Chowdhury", "Sayeed Shafayet", ""], ["Islam", "Kazi Mejbaul", ""], ["Noor", "Rouhan", ""]]}, {"id": "2006.03743", "submitter": "Han Gong", "authors": "Han Gong, Luwen Yu, Stephen Westland", "title": "Simple Primary Colour Editing for Consumer Product Images", "comments": "This is a working paper (pre-print). Code available at\n  https://github.com/hangong/prod_recolor/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple primary colour editing method for consumer product\nimages. We show that by using colour correction and colour blending, we can\nautomate the pain-staking colour editing task and save time for consumer colour\npreference researchers. To improve the colour harmony between the primary\ncolour and its complementary colours, our algorithm also tunes the other\ncolours in the image. Preliminary experiment has shown some promising results\ncompared with a state-of-the-art method and human editing.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 00:24:56 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Gong", "Han", ""], ["Yu", "Luwen", ""], ["Westland", "Stephen", ""]]}, {"id": "2006.03744", "submitter": "Mingjie Li", "authors": "Mingjie Li, Fuyu Wang, Xiaojun Chang and Xiaodan Liang", "title": "Auxiliary Signal-Guided Knowledge Encoder-Decoder for Medical Report\n  Generation", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beyond the common difficulties faced in the natural image captioning, medical\nreport generation specifically requires the model to describe a medical image\nwith a fine-grained and semantic-coherence paragraph that should satisfy both\nmedical commonsense and logic. Previous works generally extract the global\nimage features and attempt to generate a paragraph that is similar to\nreferenced reports; however, this approach has two limitations. Firstly, the\nregions of primary interest to radiologists are usually located in a small area\nof the global image, meaning that the remainder parts of the image could be\nconsidered as irrelevant noise in the training procedure. Secondly, there are\nmany similar sentences used in each medical report to describe the normal\nregions of the image, which causes serious data bias. This deviation is likely\nto teach models to generate these inessential sentences on a regular basis. To\naddress these problems, we propose an Auxiliary Signal-Guided Knowledge\nEncoder-Decoder (ASGK) to mimic radiologists' working patterns. In more detail,\nASGK integrates internal visual feature fusion and external medical linguistic\ninformation to guide medical knowledge transfer and learning. The core\nstructure of ASGK consists of a medical graph encoder and a natural language\ndecoder, inspired by advanced Generative Pre-Training (GPT). Experiments on the\nCX-CHR dataset and our COVID-19 CT Report dataset demonstrate that our proposed\nASGK is able to generate a robust and accurate report, and moreover outperforms\nstate-of-the-art methods on both medical terminology classification and\nparagraph generation metrics.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 01:00:15 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Li", "Mingjie", ""], ["Wang", "Fuyu", ""], ["Chang", "Xiaojun", ""], ["Liang", "Xiaodan", ""]]}, {"id": "2006.03761", "submitter": "Haozhe Xie", "authors": "Haozhe Xie, Hongxun Yao, Shangchen Zhou, Jiageng Mao, Shengping Zhang,\n  Wenxiu Sun", "title": "GRNet: Gridding Residual Network for Dense Point Cloud Completion", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Estimating the complete 3D point cloud from an incomplete one is a key\nproblem in many vision and robotics applications. Mainstream methods (e.g., PCN\nand TopNet) use Multi-layer Perceptrons (MLPs) to directly process point\nclouds, which may cause the loss of details because the structural and context\nof point clouds are not fully considered. To solve this problem, we introduce\n3D grids as intermediate representations to regularize unordered point clouds.\nWe therefore propose a novel Gridding Residual Network (GRNet) for point cloud\ncompletion. In particular, we devise two novel differentiable layers, named\nGridding and Gridding Reverse, to convert between point clouds and 3D grids\nwithout losing structural information. We also present the differentiable Cubic\nFeature Sampling layer to extract features of neighboring points, which\npreserves context information. In addition, we design a new loss function,\nnamely Gridding Loss, to calculate the L1 distance between the 3D grids of the\npredicted and ground truth point clouds, which is helpful to recover details.\nExperimental results indicate that the proposed GRNet performs favorably\nagainst state-of-the-art methods on the ShapeNet, Completion3D, and KITTI\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 02:46:39 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 00:46:08 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 07:39:51 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2020 11:22:05 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Xie", "Haozhe", ""], ["Yao", "Hongxun", ""], ["Zhou", "Shangchen", ""], ["Mao", "Jiageng", ""], ["Zhang", "Shengping", ""], ["Sun", "Wenxiu", ""]]}, {"id": "2006.03762", "submitter": "Peng-Shuai Wang", "authors": "Peng-Shuai Wang and Yang Liu and Xin Tong", "title": "Deep Octree-based CNNs with Output-Guided Skip Connections for 3D Shape\n  and Scene Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring complete and clean 3D shape and scene data is challenging due to\ngeometric occlusion and insufficient views during 3D capturing. We present a\nsimple yet effective deep learning approach for completing the input noisy and\nincomplete shapes or scenes. Our network is built upon the octree-based CNNs\n(O-CNN) with U-Net like structures, which enjoys high computational and memory\nefficiency and supports to construct a very deep network structure for 3D CNNs.\nA novel output-guided skip-connection is introduced to the network structure\nfor better preserving the input geometry and learning geometry prior from data\neffectively. We show that with these simple adaptions -- output-guided\nskip-connection and deeper O-CNN (up to 70 layers), our network achieves\nstate-of-the-art results in 3D shape completion and semantic scene computation.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 02:51:26 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Wang", "Peng-Shuai", ""], ["Liu", "Yang", ""], ["Tong", "Xin", ""]]}, {"id": "2006.03776", "submitter": "Amar Shrestha", "authors": "Amar Shrestha, Krittaphat Pugdeethosapol, Haowen Fang, Qinru Qiu", "title": "MAGNet: Multi-Region Attention-Assisted Grounding of Natural Language\n  Queries at Phrase Level", "comments": "Submitted to The 2020 European Conference on Computer Vision (ECCV\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding free-form textual queries necessitates an understanding of these\ntextual phrases and its relation to the visual cues to reliably reason about\nthe described locations. Spatial attention networks are known to learn this\nrelationship and focus its gaze on salient objects in the image. Thus, we\npropose to utilize spatial attention networks for image-level visual-textual\nfusion preserving local (word) and global (phrase) information to refine region\nproposals with an in-network Region Proposal Network (RPN) and detect single or\nmultiple regions for a phrase query. We focus only on the phrase query - ground\ntruth pair (referring expression) for a model independent of the constraints of\nthe datasets i.e. additional attributes, context etc. For such referring\nexpression dataset ReferIt game, our Multi-region Attention-assisted Grounding\nnetwork (MAGNet) achieves over 12\\% improvement over the state-of-the-art.\nWithout the context from image captions and attribute information in Flickr30k\nEntities, we still achieve competitive results compared to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 04:14:15 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Shrestha", "Amar", ""], ["Pugdeethosapol", "Krittaphat", ""], ["Fang", "Haowen", ""], ["Qiu", "Qinru", ""]]}, {"id": "2006.03783", "submitter": "S. Alireza Golestaneh", "authors": "S. Alireza Golestaneh, Kris Kitani", "title": "No-Reference Image Quality Assessment via Feature Fusion and Multi-Task\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blind or no-reference image quality assessment (NR-IQA) is a fundamental,\nunsolved, and yet challenging problem due to the unavailability of a reference\nimage. It is vital to the streaming and social media industries that impact\nbillions of viewers daily. Although previous NR-IQA methods leveraged different\nfeature extraction approaches, the performance bottleneck still exists. In this\npaper, we propose a simple and yet effective general-purpose no-reference (NR)\nimage quality assessment (IQA) framework based on multi-task learning. Our\nmodel employs distortion types as well as subjective human scores to predict\nimage quality. We propose a feature fusion method to utilize distortion\ninformation to improve the quality score estimation task. In our experiments,\nwe demonstrate that by utilizing multi-task learning and our proposed feature\nfusion method, our model yields better performance for the NR-IQA task. To\ndemonstrate the effectiveness of our approach, we test our approach on seven\nstandard datasets and show that we achieve state-of-the-art results on various\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 05:04:10 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Golestaneh", "S. Alireza", ""], ["Kitani", "Kris", ""]]}, {"id": "2006.03790", "submitter": "Xin Liu", "authors": "Xin Liu, Josh Fromm, Shwetak Patel, Daniel McDuff", "title": "Multi-Task Temporal Shift Attention Networks for On-Device Contactless\n  Vitals Measurement", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telehealth and remote health monitoring have become increasingly important\nduring the SARS-CoV-2 pandemic and it is widely expected that this will have a\nlasting impact on healthcare practices. These tools can help reduce the risk of\nexposing patients and medical staff to infection, make healthcare services more\naccessible, and allow providers to see more patients. However, objective\nmeasurement of vital signs is challenging without direct contact with a\npatient. We present a video-based and on-device optical cardiopulmonary vital\nsign measurement approach. It leverages a novel multi-task temporal shift\nconvolutional attention network (MTTS-CAN) and enables real-time cardiovascular\nand respiratory measurements on mobile platforms. We evaluate our system on an\nAdvanced RISC Machine (ARM) CPU and achieve state-of-the-art accuracy while\nrunning at over 150 frames per second which enables real-time applications.\nSystematic experimentation on large benchmark datasets reveals that our\napproach leads to substantial (20%-50%) reductions in error and generalizes\nwell across datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 06:31:24 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 20:50:15 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Liu", "Xin", ""], ["Fromm", "Josh", ""], ["Patel", "Shwetak", ""], ["McDuff", "Daniel", ""]]}, {"id": "2006.03796", "submitter": "Luyang Luo", "authors": "Luyang Luo, Lequan Yu, Hao Chen, Quande Liu, Xi Wang, Jiaqi Xu, and\n  Pheng-Ann Heng", "title": "Deep Mining External Imperfect Data for Chest X-ray Disease Screening", "comments": "Accepted to IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches have demonstrated remarkable progress in automatic\nChest X-ray analysis. The data-driven feature of deep models requires training\ndata to cover a large distribution. Therefore, it is substantial to integrate\nknowledge from multiple datasets, especially for medical images. However,\nlearning a disease classification model with extra Chest X-ray (CXR) data is\nyet challenging. Recent researches have demonstrated that performance\nbottleneck exists in joint training on different CXR datasets, and few made\nefforts to address the obstacle. In this paper, we argue that incorporating an\nexternal CXR dataset leads to imperfect training data, which raises the\nchallenges. Specifically, the imperfect data is in two folds: domain\ndiscrepancy, as the image appearances vary across datasets; and label\ndiscrepancy, as different datasets are partially labeled. To this end, we\nformulate the multi-label thoracic disease classification problem as weighted\nindependent binary tasks according to the categories. For common categories\nshared across domains, we adopt task-specific adversarial training to alleviate\nthe feature differences. For categories existing in a single dataset, we\npresent uncertainty-aware temporal ensembling of model predictions to mine the\ninformation from the missing labels further. In this way, our framework\nsimultaneously models and tackles the domain and label discrepancies, enabling\nsuperior knowledge mining ability. We conduct extensive experiments on three\ndatasets with more than 360,000 Chest X-ray images. Our method outperforms\nother competing models and sets state-of-the-art performance on the official\nNIH test set with 0.8349 AUC, demonstrating its effectiveness of utilizing the\nexternal dataset to improve the internal classification.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 06:48:40 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Luo", "Luyang", ""], ["Yu", "Lequan", ""], ["Chen", "Hao", ""], ["Liu", "Quande", ""], ["Wang", "Xi", ""], ["Xu", "Jiaqi", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2006.03800", "submitter": "Hanke Chen", "authors": "Hanke Chen", "title": "Extracting Cellular Location of Human Proteins Using Deep Learning", "comments": "5 page, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and extracting the patterns of microscopy images has been a\nmajor challenge in the biomedical field. Although trained scientists can locate\nthe proteins of interest within a human cell, this procedure is not efficient\nand accurate enough to process a large amount of data and it often leads to\nbias. To resolve this problem, we attempted to create an automatic image\nclassifier using Machine Learning to locate human proteins with higher speed\nand accuracy than human beings. We implemented a Convolution Neural Network\nwith Residue and Squeeze-Excitation layers classifier to locate given proteins\nof any type in a subcellular structure. After training the model using a series\nof techniques, it can locate thousands of proteins in 27 different human cell\ntypes into 28 subcellular locations, way significant than historical\napproaches. The model can classify 4,500 images per minute with an accuracy of\n63.07%, surpassing human performance in accuracy (by 35%) and speed. Because\nour system can be implemented on different cell types, it opens a new vision of\nunderstanding in the biomedical field. From the locational information of the\nhuman proteins, doctors can easily detect cell's abnormal behaviors including\nviral infection, pathogen invasion, and malignant tumor development. Given the\namount of data generalized by experiments are greater than that human can\nanalyze, the model cut down the human resources and time needed to analyze\ndata. Moreover, this locational information can be used in different scenarios\nlike subcellular engineering, medical care, and etiology inspection.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 07:15:11 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Chen", "Hanke", ""]]}, {"id": "2006.03810", "submitter": "Deepan Das", "authors": "Deepan Das, Haley Massa, Abhimanyu Kulkarni, Theodoros Rekatsinas", "title": "An Empirical Analysis of the Impact of Data Augmentation on Knowledge\n  Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Generalization Performance of Deep Learning models trained using Empirical\nRisk Minimization can be improved significantly by using Data Augmentation\nstrategies such as simple transformations, or using Mixed Samples. We attempt\nto empirically analyze the impact of such strategies on the transfer of\ngeneralization between teacher and student models in a distillation setup. We\nobserve that if a teacher is trained using any of the mixed sample augmentation\nstrategies, such as MixUp or CutMix, the student model distilled from it is\nimpaired in its generalization capabilities. We hypothesize that such\nstrategies limit a model's capability to learn example-specific features,\nleading to a loss in quality of the supervision signal during distillation. We\npresent a novel Class-Discrimination metric to quantitatively measure this\ndichotomy in performance and link it to the discriminative capacity induced by\nthe different strategies on a network's latent space.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 08:20:48 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 13:01:00 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Das", "Deepan", ""], ["Massa", "Haley", ""], ["Kulkarni", "Abhimanyu", ""], ["Rekatsinas", "Theodoros", ""]]}, {"id": "2006.03817", "submitter": "Benoit Guillard", "authors": "Benoit Guillard, Edoardo Remelli, Pascal Fua", "title": "UCLID-Net: Single View Reconstruction in Object Space", "comments": "Added supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art deep geometric learning single-view reconstruction\napproaches rely on encoder-decoder architectures that output either shape\nparametrizations or implicit representations. However, these representations\nrarely preserve the Euclidean structure of the 3D space objects exist in. In\nthis paper, we show that building a geometry preserving 3-dimensional latent\nspace helps the network concurrently learn global shape regularities and local\nreasoning in the object coordinate space and, as a result, boosts performance.\nWe demonstrate both on ShapeNet synthetic images, which are often used for\nbenchmarking purposes, and on real-world images that our approach outperforms\nstate-of-the-art ones. Furthermore, the single-view pipeline naturally extends\nto multi-view reconstruction, which we also show.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 09:15:56 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 12:11:18 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Guillard", "Benoit", ""], ["Remelli", "Edoardo", ""], ["Fua", "Pascal", ""]]}, {"id": "2006.03829", "submitter": "Aiham Taleb", "authors": "Aiham Taleb, Winfried Loetzsch, Noel Danz, Julius Severin, Thomas\n  Gaertner, Benjamin Bergner, and Christoph Lippert", "title": "3D Self-Supervised Methods for Medical Imaging", "comments": "Proceedings of NeurIPS 2020 (Please cite the proceedings version).\n  For source code, see https://github.com/HealthML/self-supervised-3d-tasks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning methods have witnessed a recent surge of interest\nafter proving successful in multiple application fields. In this work, we\nleverage these techniques, and we propose 3D versions for five different\nself-supervised methods, in the form of proxy tasks. Our methods facilitate\nneural network feature learning from unlabeled 3D images, aiming to reduce the\nrequired cost for expert annotation. The developed algorithms are 3D\nContrastive Predictive Coding, 3D Rotation prediction, 3D Jigsaw puzzles,\nRelative 3D patch location, and 3D Exemplar networks. Our experiments show that\npretraining models with our 3D tasks yields more powerful semantic\nrepresentations, and enables solving downstream tasks more accurately and\nefficiently, compared to training the models from scratch and to pretraining\nthem on 2D slices. We demonstrate the effectiveness of our methods on three\ndownstream tasks from the medical imaging domain: i) Brain Tumor Segmentation\nfrom 3D MRI, ii) Pancreas Tumor Segmentation from 3D CT, and iii) Diabetic\nRetinopathy Detection from 2D Fundus images. In each task, we assess the gains\nin data-efficiency, performance, and speed of convergence. Interestingly, we\nalso find gains when transferring the learned representations, by our methods,\nfrom a large unlabeled 3D corpus to a small downstream-specific dataset. We\nachieve results competitive to state-of-the-art solutions at a fraction of the\ncomputational expense. We publish our implementations for the developed\nalgorithms (both 3D and 2D versions) as an open-source library, in an effort to\nallow other researchers to apply and extend our methods on their datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 09:56:58 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 11:18:32 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 10:43:10 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Taleb", "Aiham", ""], ["Loetzsch", "Winfried", ""], ["Danz", "Noel", ""], ["Severin", "Julius", ""], ["Gaertner", "Thomas", ""], ["Bergner", "Benjamin", ""], ["Lippert", "Christoph", ""]]}, {"id": "2006.03840", "submitter": "Claudio Ferrari", "authors": "Claudio Ferrari, Stefano Berretti, Pietro Pala, Alberto Del Bimbo", "title": "A Sparse and Locally Coherent Morphable Face Model for Dense Semantic\n  Correspondence Across Heterogeneous 3D Faces", "comments": "Accepted for publication in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3090942", "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D Morphable Model (3DMM) is a powerful statistical tool for representing\n3D face shapes. To build a 3DMM, a training set of face scans in full\npoint-to-point correspondence is required, and its modeling capabilities\ndirectly depend on the variability contained in the training data. Thus, to\nincrease the descriptive power of the 3DMM, establishing a dense correspondence\nacross heterogeneous scans with sufficient diversity in terms of identities,\nethnicities, or expressions becomes essential. In this manuscript, we present a\nfully automatic approach that leverages a 3DMM to transfer its dense semantic\nannotation across raw 3D faces, establishing a dense correspondence between\nthem. We propose a novel formulation to learn a set of sparse deformation\ncomponents with local support on the face that, together with an original\nnon-rigid deformation algorithm, allow the 3DMM to precisely fit unseen faces\nand transfer its semantic annotation. We extensively experimented our approach,\nshowing it can effectively generalize to highly diverse samples and accurately\nestablish a dense correspondence even in presence of complex facial\nexpressions. The accuracy of the dense registration is demonstrated by building\na heterogeneous, large-scale 3DMM from more than 9,000 fully registered scans\nobtained by joining three large datasets together.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 10:52:07 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 15:24:54 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 13:52:18 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Ferrari", "Claudio", ""], ["Berretti", "Stefano", ""], ["Pala", "Pietro", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2006.03858", "submitter": "Qingyu Li", "authors": "Qingyu Li, Lichao Mou, Yuansheng Hua, Yao Sun, Pu Jin, Yilei Shi, Xiao\n  Xiang Zhu", "title": "Instance segmentation of buildings using keypoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building segmentation is of great importance in the task of remote sensing\nimagery interpretation. However, the existing semantic segmentation and\ninstance segmentation methods often lead to segmentation masks with blurred\nboundaries. In this paper, we propose a novel instance segmentation network for\nbuilding segmentation in high-resolution remote sensing images. More\nspecifically, we consider segmenting an individual building as detecting\nseveral keypoints. The detected keypoints are subsequently reformulated as a\nclosed polygon, which is the semantic boundary of the building. By doing so,\nthe sharp boundary of the building could be preserved. Experiments are\nconducted on selected Aerial Imagery for Roof Segmentation (AIRS) dataset, and\nour method achieves better performance in both quantitative and qualitative\nresults with comparison to the state-of-the-art methods. Our network is a\nbottom-up instance segmentation method that could well preserve geometric\ndetails.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 13:11:37 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Li", "Qingyu", ""], ["Mou", "Lichao", ""], ["Hua", "Yuansheng", ""], ["Sun", "Yao", ""], ["Jin", "Pu", ""], ["Shi", "Yilei", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2006.03870", "submitter": "Andrei Costin", "authors": "Hannu Turtiainen and Andrei Costin and Timo Hamalainen and Tuomo\n  Lahtinen", "title": "Towards large-scale, automated, accurate detection of CCTV camera\n  objects using computer vision. Applications and implications for privacy,\n  safety, and cybersecurity. (Preprint)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In order to withstand the ever-increasing invasion of privacy by CCTV cameras\nand technologies, on par CCTV-aware solutions must exist that provide privacy,\nsafety, and cybersecurity features. We argue that a first important step\ntowards such CCTV-aware solutions must be a mapping system (e.g., Google Maps,\nOpenStreetMap) that provides both privacy and safety routing and navigation\noptions. However, this in turn requires that the mapping system contains\nupdated information on CCTV cameras' exact geo-location, coverage area, and\npossibly other meta-data (e.g., resolution, facial recognition features,\noperator). Such information is however missing from current mapping systems,\nand there are several ways to fix this. One solution is to perform CCTV camera\ndetection on geo-location tagged images, e.g., street view imagery on various\nplatforms, user images publicly posted in image sharing platforms such as\nFlickr. Unfortunately, to the best of our knowledge, there are no computer\nvision models for CCTV camera object detection as well as no mapping system\nthat supports privacy and safety routing options.\n  To close these gaps, with this paper we introduce the first and only computer\nvision MS COCO-compatible models that are able to accurately detect CCTV and\nvideo surveillance cameras in images and video frames. To this end, our best\ndetectors were built using 8387 images that were manually reviewed and\nannotated to contain 10419 CCTV camera instances, and achieve an accuracy of up\nto 98.7%. Moreover, we build and evaluate multiple models, present a\ncomprehensive comparison of their performance, and outline core challenges\nassociated with such research.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 13:49:09 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 21:15:10 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Turtiainen", "Hannu", ""], ["Costin", "Andrei", ""], ["Hamalainen", "Timo", ""], ["Lahtinen", "Tuomo", ""]]}, {"id": "2006.03876", "submitter": "Yuecong Xu", "authors": "Yuecong Xu, Jianfei Yang, Haozhi Cao, Kezhi Mao, Jianxiong Yin and\n  Simon See", "title": "ARID: A Comprehensive Study on Recognizing Actions in the Dark and A New\n  Benchmark Dataset", "comments": "6 pages, 7 figures, Data available at https://xuyu0010.github.io/arid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The task of action recognition in dark videos is useful in various scenarios,\ne.g., night surveillance and self-driving at night. Though progress has been\nmade in the action recognition task for videos in normal illumination, few have\nstudied action recognition in the dark. This is partly due to the lack of\nsufficient datasets for such a task. In this paper, we explored the task of\naction recognition in dark videos. We bridge the gap of the lack of data for\nthis task by collecting a new dataset: the Action Recognition in the Dark\n(ARID) dataset. It consists of over 3,780 video clips with 11 action\ncategories. To the best of our knowledge, it is the first dataset focused on\nhuman actions in dark videos. To gain further understandings of our ARID\ndataset, we analyze the ARID dataset in detail and exhibited its necessity over\nsynthetic dark videos. Additionally, we benchmarked the performance of several\ncurrent action recognition models on our dataset and explored potential methods\nfor increasing their performances. Our results show that current action\nrecognition models and frame enhancement methods may not be effective solutions\nfor the task of action recognition in dark videos.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 14:25:52 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 02:34:52 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 15:40:15 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Xu", "Yuecong", ""], ["Yang", "Jianfei", ""], ["Cao", "Haozhi", ""], ["Mao", "Kezhi", ""], ["Yin", "Jianxiong", ""], ["See", "Simon", ""]]}, {"id": "2006.03895", "submitter": "Kevin Bowyer", "authors": "Kevin W. Bowyer, Michael King, Walter Scheirer and Kushal Vangara", "title": "The Criminality From Face Illusion", "comments": null, "journal-ref": "IEEE Transactions on Technology and Society, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic analysis of face images can generate predictions about a\nperson's gender, age, race, facial expression, body mass index, and various\nother indices and conditions. A few recent publications have claimed success in\nanalyzing an image of a person's face in order to predict the person's status\nas Criminal / Non-Criminal. Predicting criminality from face may initially seem\nsimilar to other facial analytics, but we argue that attempts to create a\ncriminality-from-face algorithm are necessarily doomed to fail, that apparently\npromising experimental results in recent publications are an illusion resulting\nfrom inadequate experimental design, and that there is potentially a large\nsocial cost to belief in the criminality from face illusion.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 15:45:05 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 16:13:49 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Bowyer", "Kevin W.", ""], ["King", "Michael", ""], ["Scheirer", "Walter", ""], ["Vangara", "Kushal", ""]]}, {"id": "2006.03898", "submitter": "Sachin Singh", "authors": "Sachin Singh, Victor Sanchez and Tanaya Guha", "title": "Ensemble Network for Ranking Images Based on Visual Appeal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computational framework for ranking images (group photos in\nparticular) taken at the same event within a short time span. The ranking is\nexpected to correspond with human perception of overall appeal of the images.\nWe hypothesize and provide evidence through subjective analysis that the\nfactors that appeal to humans are its emotional content, aesthetics and image\nquality. We propose a network which is an ensemble of three information\nchannels, each predicting a score corresponding to one of the three visual\nappeal factors. For group emotion estimation, we propose a convolutional neural\nnetwork (CNN) based architecture for predicting group emotion from images. This\nnew architecture enforces the network to put emphasis on the important regions\nin the images, and achieves comparable results to the state-of-the-art. Next,\nwe develop a network for the image ranking task that combines group emotion,\naesthetics and image quality scores. Owing to the unavailability of suitable\ndatabases, we created a new database of manually annotated group photos taken\nduring various social events. We present experimental results on this database\nand other benchmark databases whenever available. Overall, our experiments show\nthat the proposed framework can reliably predict the overall appeal of images\nwith results closely corresponding to human ranking.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 15:51:38 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Singh", "Sachin", ""], ["Sanchez", "Victor", ""], ["Guha", "Tanaya", ""]]}, {"id": "2006.03919", "submitter": "Linjiang Zhang", "authors": "Linjiang Zhang, Peng Wang, Hui Li, Zhen Li, Chunhua Shen, Yanning\n  Zhang", "title": "A Robust Attentional Framework for License Plate Recognition in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing car license plates in natural scene images is an important yet\nstill challenging task in realistic applications. Many existing approaches\nperform well for license plates collected under constrained conditions, eg,\nshooting in frontal and horizontal view-angles and under good lighting\nconditions. However, their performance drops significantly in an unconstrained\nenvironment that features rotation, distortion, occlusion, blurring, shading or\nextreme dark or bright conditions. In this work, we propose a robust framework\nfor license plate recognition in the wild. It is composed of a tailored\nCycleGAN model for license plate image generation and an elaborate designed\nimage-to-sequence network for plate recognition. On one hand, the CycleGAN\nbased plate generation engine alleviates the exhausting human annotation work.\nMassive amount of training data can be obtained with a more balanced character\ndistribution and various shooting conditions, which helps to boost the\nrecognition accuracy to a large extent. On the other hand, the 2D attentional\nbased license plate recognizer with an Xception-based CNN encoder is capable of\nrecognizing license plates with different patterns under various scenarios\naccurately and robustly. Without using any heuristics rule or post-processing,\nour method achieves the state-of-the-art performance on four public datasets,\nwhich demonstrates the generality and robustness of our framework. Moreover, we\nreleased a new license plate dataset, named \"CLPD\", with 1200 images from all\n31 provinces in mainland China. The dataset can be available from:\nhttps://github.com/wangpengnorman/CLPD_dataset.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 17:11:52 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 03:06:11 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zhang", "Linjiang", ""], ["Wang", "Peng", ""], ["Li", "Hui", ""], ["Li", "Zhen", ""], ["Shen", "Chunhua", ""], ["Zhang", "Yanning", ""]]}, {"id": "2006.03921", "submitter": "Marcin Plata", "authors": "Marcin Plata, Piotr Syga", "title": "Robust watermarking with double detector-discriminator approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel deep framework for a watermarking - a\ntechnique of embedding a transparent message into an image in a way that allows\nretrieving the message from a (perturbed) copy, so that copyright infringement\ncan be tracked. For this technique, it is essential to extract the information\nfrom the image even after imposing some digital processing operations on it.\nOur framework outperforms recent methods in the context of robustness against\nnot only spectrum of attacks (e.g. rotation, resizing, Gaussian smoothing) but\nalso against compression, especially JPEG. The bit accuracy of our method is at\nleast 0.86 for all types of distortions. We also achieved 0.90 bit accuracy for\nJPEG while recent methods provided at most 0.83. Our method retains high\ntransparency and capacity as well. Moreover, we present our double\ndetector-discriminator approach - a scheme to detect and discriminate if the\nimage contains the embedded message or not, which is crucial for real-life\nwatermarking systems and up to now was not investigated using neural networks.\nWith this, we design a testing formula to validate our extended approach and\ncompared it with a common procedure. We also present an alternative method of\nbalancing between image quality and robustness on attacks which is easily\napplicable to the framework.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 17:15:45 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Plata", "Marcin", ""], ["Syga", "Piotr", ""]]}, {"id": "2006.03926", "submitter": "Yixiao Ge", "authors": "Yixiao Ge, Haibo Wang, Feng Zhu, Rui Zhao, Hongsheng Li", "title": "Self-supervising Fine-grained Region Similarities for Large-scale Image\n  Localization", "comments": "Accepted in ECCV 2020 (Spotlight), code is available at\n  https://github.com/yxgeee/SFRS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of large-scale retrieval-based image localization is to estimate the\ngeographical location of a query image by recognizing its nearest reference\nimages from a city-scale dataset. However, the general public benchmarks only\nprovide noisy GPS labels associated with the training images, which act as weak\nsupervisions for learning image-to-image similarities. Such label noise\nprevents deep neural networks from learning discriminative features for\naccurate localization. To tackle this challenge, we propose to self-supervise\nimage-to-region similarities in order to fully explore the potential of\ndifficult positive images alongside their sub-regions. The estimated\nimage-to-region similarities can serve as extra training supervision for\nimproving the network in generations, which could in turn gradually refine the\nfine-grained similarities to achieve optimal performance. Our proposed\nself-enhanced image-to-region similarity labels effectively deal with the\ntraining bottleneck in the state-of-the-art pipelines without any additional\nparameters or manual annotations in both training and inference. Our method\noutperforms state-of-the-arts on the standard localization benchmarks by\nnoticeable margins and shows excellent generalization capability on multiple\nimage retrieval datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 17:31:52 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 06:21:08 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Ge", "Yixiao", ""], ["Wang", "Haibo", ""], ["Zhu", "Feng", ""], ["Zhao", "Rui", ""], ["Li", "Hongsheng", ""]]}, {"id": "2006.03936", "submitter": "Ranjan Maitra", "authors": "Karin S. Dorman and Ranjan Maitra", "title": "An Efficient $k$-modes Algorithm for Clustering Categorical Datasets", "comments": "16 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining clusters from data is an important endeavor in many applications. The\n$k$-means method is a popular, efficient, and distribution-free approach for\nclustering numerical-valued data, but does not apply for categorical-valued\nobservations. The $k$-modes method addresses this lacuna by replacing the\nEuclidean with the Hamming distance and the means with the modes in the\n$k$-means objective function. We provide a novel, computationally efficient\nimplementation of $k$-modes, called OTQT. We prove that OTQT finds updates to\nimprove the objective function that are undetectable to existing $k$-modes\nalgorithms. Although slightly slower per iteration due to algorithmic\ncomplexity, OTQT is always more accurate per iteration and almost always faster\n(and only barely slower on some datasets) to the final optimum. Thus, we\nrecommend OTQT as the preferred, default algorithm for $k$-modes optimization.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 18:41:36 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 05:32:31 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 20:18:20 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Dorman", "Karin S.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2006.03952", "submitter": "Tomer Cohen", "authors": "Tomer Cohen, Noy Shulman, Hai Morgenstern, Roey Mechrez, and Erez\n  Farhan", "title": "Self-Supervised Dynamic Networks for Covariate Shift Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As supervised learning still dominates most AI applications, test-time\nperformance is often unexpected. Specifically, a shift of the input covariates,\ncaused by typical nuisances like background-noise, illumination variations or\ntranscription errors, can lead to a significant decrease in prediction\naccuracy. Recently, it was shown that incorporating self-supervision can\nsignificantly improve covariate shift robustness. In this work, we propose\nSelf-Supervised Dynamic Networks (SSDN): an input-dependent mechanism, inspired\nby dynamic networks, that allows a self-supervised network to predict the\nweights of the main network, and thus directly handle covariate shifts at\ntest-time. We present the conceptual and empirical advantages of the proposed\nmethod on the problem of image classification under different covariate shifts,\nand show that it significantly outperforms comparable methods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 19:37:20 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Cohen", "Tomer", ""], ["Shulman", "Noy", ""], ["Morgenstern", "Hai", ""], ["Mechrez", "Roey", ""], ["Farhan", "Erez", ""]]}, {"id": "2006.03985", "submitter": "Markos Georgopoulos", "authors": "Markos Georgopoulos, James Oldfield, Mihalis A. Nicolaou, Yannis\n  Panagakis, Maja Pantic", "title": "Enhancing Facial Data Diversity with Style-based Face Aging", "comments": "IEEE CVPR 2020 WORKSHOP ON FAIR, DATA EFFICIENT AND TRUSTED COMPUTER\n  VISION", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant limiting factor in training fair classifiers relates to the\npresence of dataset bias. In particular, face datasets are typically biased in\nterms of attributes such as gender, age, and race. If not mitigated, bias leads\nto algorithms that exhibit unfair behaviour towards such groups. In this work,\nwe address the problem of increasing the diversity of face datasets with\nrespect to age. Concretely, we propose a novel, generative style-based\narchitecture for data augmentation that captures fine-grained aging patterns by\nconditioning on multi-resolution age-discriminative representations. By\nevaluating on several age-annotated datasets in both single- and cross-database\nexperiments, we show that the proposed method outperforms state-of-the-art\nalgorithms for age transfer, especially in the case of age groups that lie in\nthe tails of the label distribution. We further show significantly increased\ndiversity in the augmented datasets, outperforming all compared methods\naccording to established metrics.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 21:53:44 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Georgopoulos", "Markos", ""], ["Oldfield", "James", ""], ["Nicolaou", "Mihalis A.", ""], ["Panagakis", "Yannis", ""], ["Pantic", "Maja", ""]]}, {"id": "2006.03997", "submitter": "Artem Lukoianov", "authors": "Edoardo Remelli, Artem Lukoianov, Stephan R. Richter, Beno\\^it\n  Guillard, Timur Bagautdinov, Pierre Baque and Pascal Fua", "title": "MeshSDF: Differentiable Iso-Surface Extraction", "comments": "22 pages, 16 figures, Neural Information Processing Systems (NeurIPS\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric Deep Learning has recently made striking progress with the advent\nof continuous Deep Implicit Fields. They allow for detailed modeling of\nwatertight surfaces of arbitrary topology while not relying on a 3D Euclidean\ngrid, resulting in a learnable parameterization that is not limited in\nresolution.\n  Unfortunately, these methods are often not suitable for applications that\nrequire an explicit mesh-based surface representation because converting an\nimplicit field to such a representation relies on the Marching Cubes algorithm,\nwhich cannot be differentiated with respect to the underlying implicit field.\n  In this work, we remove this limitation and introduce a differentiable way to\nproduce explicit surface mesh representations from Deep Signed Distance\nFunctions. Our key insight is that by reasoning on how implicit field\nperturbations impact local surface geometry, one can ultimately differentiate\nthe 3D location of surface samples with respect to the underlying deep implicit\nfield. We exploit this to define MeshSDF, an end-to-end differentiable mesh\nrepresentation which can vary its topology.\n  We use two different applications to validate our theoretical insight:\nSingle-View Reconstruction via Differentiable Rendering and Physically-Driven\nShape Optimization. In both cases our differentiable parameterization gives us\nan edge over state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 23:44:05 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 15:45:02 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Remelli", "Edoardo", ""], ["Lukoianov", "Artem", ""], ["Richter", "Stephan R.", ""], ["Guillard", "Beno\u00eet", ""], ["Bagautdinov", "Timur", ""], ["Baque", "Pierre", ""], ["Fua", "Pascal", ""]]}, {"id": "2006.04005", "submitter": "David Mac\\^edo", "authors": "David Mac\\^edo, Tsang Ing Ren, Cleber Zanchettin, Adriano L. I.\n  Oliveira, Teresa Ludermir", "title": "Entropic Out-of-Distribution Detection: Seamless Detection of Unknown\n  Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we argue that the unsatisfactory out-of-distribution (OOD)\ndetection performance of neural networks is mainly due to the SoftMax loss\nanisotropy and propensity to produce low entropy probability distributions in\ndisagreement with the principle of maximum entropy. Current out-of-distribution\n(OOD) detection approaches usually do not directly fix the SoftMax loss\ndrawbacks but rather build techniques to circumvent it. Unfortunately, those\nmethods usually produce undesired side effects (e.g., classification accuracy\ndrop, additional hyperparameters, slower inferences, and collecting extra\ndata). In the opposite direction, we propose replacing SoftMax loss with a\nnovel loss function that does not suffer from the mentioned weaknesses. The\nproposed IsoMax loss is isotropic (exclusively distance-based) and provides\nhigh entropy posterior probability distributions. Replacing the SoftMax loss by\nIsoMax loss requires no model or training changes. Additionally, the models\ntrained with IsoMax loss produce as fast and energy-efficient inferences as\nthose trained using SoftMax loss. Further, no classification accuracy drop is\nobserved. The proposed method does not rely on outlier/background data,\nhyperparameter tuning, temperature calibration, feature extraction, metric\nlearning, adversarial training, ensemble procedures, or generative models. Our\nexperiments showed that IsoMax loss works as a seamless SoftMax loss drop-in\nreplacement that significantly improves neural networks' OOD detection\nperformance. Therefore, it may be used as a baseline OOD detection approach to\nbe combined with current or future OOD detection techniques to achieve even\nhigher results.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 00:34:57 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 03:31:42 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Mac\u00eado", "David", ""], ["Ren", "Tsang Ing", ""], ["Zanchettin", "Cleber", ""], ["Oliveira", "Adriano L. I.", ""], ["Ludermir", "Teresa", ""]]}, {"id": "2006.04026", "submitter": "Koutilya Pnvr", "authors": "Koutilya PNVR, Hao Zhou, David Jacobs", "title": "SharinGAN: Combining Synthetic and Real Data for Unsupervised Geometry\n  Estimation", "comments": "Accepted to CVPR 2020. Supplementary material added towards the end\n  instead of a separate file. A Github link to the code is also provided in\n  this submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for combining synthetic and real images when\ntraining networks to determine geometric information from a single image. We\nsuggest a method for mapping both image types into a single, shared domain.\nThis is connected to a primary network for end-to-end training. Ideally, this\nresults in images from two domains that present shared information to the\nprimary network. Our experiments demonstrate significant improvements over the\nstate-of-the-art in two important domains, surface normal estimation of human\nfaces and monocular depth estimation for outdoor scenes, both in an\nunsupervised setting.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 02:45:33 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["PNVR", "Koutilya", ""], ["Zhou", "Hao", ""], ["Jacobs", "David", ""]]}, {"id": "2006.04033", "submitter": "Huthaifa Ashqar", "authors": "Mohammed Hamad Almannaa, Huthaifa I. Ashqar, Mohammed Elhenawy,\n  Mahmoud Masoud, Andry Rakotonirainy, and Hesham Rakha", "title": "A Comparative Analysis of E-Scooter and E-Bike Usage Patterns: Findings\n  from the City of Austin, TX", "comments": "Submitted to the International Journal of Sustainable Transportation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-scooter-sharing and e-bike-sharing systems are accommodating and easing the\nincreased traffic in dense cities and are expanding considerably. However,\nthese new micro-mobility transportation modes raise numerous operational and\nsafety concerns. This study analyzes e-scooter and dockless e-bike sharing\nsystem user behavior. We investigate how average trip speed change depending on\nthe day of the week and the time of the day. We used a dataset from the city of\nAustin, TX from December 2018 to May 2019. Our results generally show that the\ntrip average speed for e-bikes ranges between 3.01 and 3.44 m/s, which is\nhigher than that for e-scooters (2.19 to 2.78 m/s). Results also show a similar\nusage pattern for the average speed of e-bikes and e-scooters throughout the\ndays of the week and a different usage pattern for the average speed of e-bikes\nand e-scooters over the hours of the day. We found that users tend to ride\ne-bikes and e-scooters with a slower average speed for recreational purposes\ncompared to when they are ridden for commuting purposes. This study is a\nbuilding block in this field, which serves as a first of its kind, and sheds\nthe light of significant new understanding of this emerging class of\nshared-road users.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 03:27:44 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Almannaa", "Mohammed Hamad", ""], ["Ashqar", "Huthaifa I.", ""], ["Elhenawy", "Mohammed", ""], ["Masoud", "Mahmoud", ""], ["Rakotonirainy", "Andry", ""], ["Rakha", "Hesham", ""]]}, {"id": "2006.04043", "submitter": "Qingdong He", "authors": "Qingdong He, Zhengning Wang, Hao Zeng, Yi Zeng, Shuaicheng Liu, Bing\n  Zeng", "title": "SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection\n  from Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate 3D object detection from point clouds has become a crucial component\nin autonomous driving. However, the volumetric representations and the\nprojection methods in previous works fail to establish the relationships\nbetween the local point sets. In this paper, we propose Sparse Voxel-Graph\nAttention Network (SVGA-Net), a novel end-to-end trainable network which mainly\ncontains voxel-graph module and sparse-to-dense regression module to achieve\ncomparable 3D detection tasks from raw LIDAR data. Specifically, SVGA-Net\nconstructs the local complete graph within each divided 3D spherical voxel and\nglobal KNN graph through all voxels. The local and global graphs serve as the\nattention mechanism to enhance the extracted features. In addition, the novel\nsparse-to-dense regression module enhances the 3D box estimation accuracy\nthrough feature maps aggregation at different levels. Experiments on KITTI\ndetection benchmark demonstrate the efficiency of extending the graph\nrepresentation to 3D object detection and the proposed SVGA-Net can achieve\ndecent detection accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 05:01:06 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["He", "Qingdong", ""], ["Wang", "Zhengning", ""], ["Zeng", "Hao", ""], ["Zeng", "Yi", ""], ["Liu", "Shuaicheng", ""], ["Zeng", "Bing", ""]]}, {"id": "2006.04045", "submitter": "Risheng Liu", "authors": "Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, Jin Zhang", "title": "A Generic First-Order Algorithmic Framework for Bi-Level Programming\n  Beyond Lower-Level Singleton", "comments": "Accepted at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a variety of gradient-based first-order methods have been\ndeveloped to solve bi-level optimization problems for learning applications.\nHowever, theoretical guarantees of these existing approaches heavily rely on\nthe simplification that for each fixed upper-level variable, the lower-level\nsolution must be a singleton (a.k.a., Lower-Level Singleton, LLS). In this\nwork, we first design a counter-example to illustrate the invalidation of such\nLLS condition. Then by formulating BLPs from the view point of optimistic\nbi-level and aggregating hierarchical objective information, we establish\nBi-level Descent Aggregation (BDA), a flexible and modularized algorithmic\nframework for generic bi-level optimization. Theoretically, we derive a new\nmethodology to prove the convergence of BDA without the LLS condition. Our\ninvestigations also demonstrate that BDA is indeed compatible to a verify of\nparticular first-order computation modules. Additionally, as an interesting\nbyproduct, we also improve these conventional first-order bi-level schemes\n(under the LLS simplification). Particularly, we establish their convergences\nwith weaker assumptions. Extensive experiments justify our theoretical results\nand demonstrate the superiority of the proposed BDA for different tasks,\nincluding hyper-parameter optimization and meta learning.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 05:18:50 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 10:26:42 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Liu", "Risheng", ""], ["Mu", "Pan", ""], ["Yuan", "Xiaoming", ""], ["Zeng", "Shangzhi", ""], ["Zhang", "Jin", ""]]}, {"id": "2006.04047", "submitter": "Shing Yan Loo", "authors": "Shing Yan Loo, Syamsiah Mashohor, Sai Hong Tang, Hong Zhang", "title": "DeepRelativeFusion: Dense Monocular SLAM using Single-Image Relative\n  Depth Prediction", "comments": "Accepted to be published in the Proceedings of the 2021 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a dense monocular SLAM system, named\nDeepRelativeFusion, that is capable to recover a globally consistent 3D\nstructure. To this end, we use a visual SLAM algorithm to reliably recover the\ncamera poses and semi-dense depth maps of the keyframes, and then use relative\ndepth prediction to densify the semi-dense depth maps and refine the keyframe\npose-graph. To improve the semi-dense depth maps, we propose an adaptive\nfiltering scheme, which is a structure-preserving weighted average smoothing\nfilter that takes into account the pixel intensity and depth of the\nneighbouring pixels, yielding substantial reconstruction accuracy gain in\ndensification. To perform densification, we introduce two incremental\nimprovements upon the energy minimization framework proposed by DeepFusion: (1)\nan improved cost function, and (2) the use of single-image relative depth\nprediction. After densification, we update the keyframes with two-view\nconsistent optimized semi-dense and dense depth maps to improve pose-graph\noptimization, providing a feedback loop to refine the keyframe poses for\naccurate scene reconstruction. Our system outperforms the state-of-the-art\ndense SLAM systems quantitatively in dense reconstruction accuracy by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 05:22:29 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 18:34:55 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 20:06:40 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Loo", "Shing Yan", ""], ["Mashohor", "Syamsiah", ""], ["Tang", "Sai Hong", ""], ["Zhang", "Hong", ""]]}, {"id": "2006.04057", "submitter": "Raghu Vamshi Namala", "authors": "Raghu Vamshi.N, Bharathi Raja S", "title": "Facial Expression Recognition using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the various ages, facial expressions have become one of the\nuniversal ways of non-verbal communication. The ability to recognize facial\nexpressions would pave the path for many novel applications. Despite the\nsuccess of traditional approaches in a controlled environment, these approaches\nfail on challenging datasets consisting of partial faces. In this paper, I take\none such dataset FER-2013 and will implement deep learning models that are able\nto achieve significant improvement over the previously used traditional\napproaches and even some of the deep learning models.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 06:32:05 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["N", "Raghu Vamshi.", ""], ["S", "Bharathi Raja", ""]]}, {"id": "2006.04058", "submitter": "Thoudam Doren Singh", "authors": "Alok Singh, Thoudam Doren Singh and Sivaji Bandyopadhyay", "title": "NITS-VC System for VATEX Video Captioning Challenge 2020", "comments": "Workshop on Language & Vision with applications to Video\n  Understanding (LVVU 2020) - In conjunction with CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video captioning is process of summarising the content, event and action of\nthe video into a short textual form which can be helpful in many research areas\nsuch as video guided machine translation, video sentiment analysis and\nproviding aid to needy individual. In this paper, a system description of the\nframework used for VATEX-2020 video captioning challenge is presented. We\nemploy an encoder-decoder based approach in which the visual features of the\nvideo are encoded using 3D convolutional neural network (C3D) and in the\ndecoding phase two Long Short Term Memory (LSTM) recurrent networks are used in\nwhich visual features and input captions are fused separately and final output\nis generated by performing element-wise product between the output of both\nLSTMs. Our model is able to achieve BLEU scores of 0.20 and 0.22 on public and\nprivate test data sets respectively.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 06:39:56 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 14:05:13 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Singh", "Alok", ""], ["Singh", "Thoudam Doren", ""], ["Bandyopadhyay", "Sivaji", ""]]}, {"id": "2006.04078", "submitter": "Qiang Li Capasso", "authors": "Qiang Li, Zekui Qin, Wenbo Zhang, and Wen Zheng", "title": "Siamese Keypoint Prediction Network for Visual Object Tracking", "comments": "Code: https://github.com/ZekuiQin/SiamKPN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object tracking aims to estimate the location of an arbitrary target\nin a video sequence given its initial bounding box. By utilizing offline\nfeature learning, the siamese paradigm has recently been the leading framework\nfor high performance tracking. However, current existing siamese trackers\neither heavily rely on complicated anchor-based detection networks or lack the\nability to resist to distractors. In this paper, we propose the Siamese\nkeypoint prediction network (SiamKPN) to address these challenges. Upon a\nSiamese backbone for feature embedding, SiamKPN benefits from a cascade heatmap\nstrategy for coarse-to-fine prediction modeling. In particular, the strategy is\nimplemented by sequentially shrinking the coverage of the label heatmap along\nthe cascade to apply loose-to-strict intermediate supervisions. During\ninference, we find the predicted heatmaps of successive stages to be gradually\nconcentrated to the target and reduced to the distractors. SiamKPN performs\nwell against state-of-the-art trackers for visual object tracking on four\nbenchmark datasets including OTB-100, VOT2018, LaSOT and GOT-10k, while running\nat real-time speed.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 08:11:06 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Li", "Qiang", ""], ["Qin", "Zekui", ""], ["Zhang", "Wenbo", ""], ["Zheng", "Wen", ""]]}, {"id": "2006.04080", "submitter": "Shubham Shrivastava", "authors": "Shubham Shrivastava and Punarjay Chakravarty", "title": "CubifAE-3D: Monocular Camera Space Cubification for Auto-Encoder based\n  3D Object Detection", "comments": "12 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for 3D object detection using a single monocular image.\nStarting from a synthetic dataset, we pre-train an RGB-to-Depth Auto-Encoder\n(AE). The embedding learnt from this AE is then used to train a 3D Object\nDetector (3DOD) CNN which is used to regress the parameters of 3D object poses\nafter the encoder from the AE generates a latent embedding from the RGB image.\nWe show that we can pre-train the AE using paired RGB and depth images from\nsimulation data once and subsequently only train the 3DOD network using real\ndata, comprising of RGB images and 3D object pose labels (without the\nrequirement of dense depth). Our 3DOD network utilizes a particular\n`cubification' of 3D space around the camera, where each cuboid is tasked with\npredicting N object poses, along with their class and confidence values. The AE\npre-training and this method of dividing the 3D space around the camera into\ncuboids give our method its name - CubifAE-3D. We demonstrate results for\nmonocular 3D object detection in the Autonomous Vehicle (AV) use-case with the\nVirtual KITTI 2 and the KITTI datasets.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 08:17:00 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 16:29:45 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Shrivastava", "Shubham", ""], ["Chakravarty", "Punarjay", ""]]}, {"id": "2006.04082", "submitter": "Zhenbo Song", "authors": "Zhenbo Song, Jianfeng Lu, Tong Zhang, Hongdong Li", "title": "End-to-end Learning for Inter-Vehicle Distance and Relative Velocity\n  Estimation in ADAS with a Monocular Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-vehicle distance and relative velocity estimations are two basic\nfunctions for any ADAS (Advanced driver-assistance systems). In this paper, we\npropose a monocular camera-based inter-vehicle distance and relative velocity\nestimation method based on end-to-end training of a deep neural network. The\nkey novelty of our method is the integration of multiple visual clues provided\nby any two time-consecutive monocular frames, which include deep feature clue,\nscene geometry clue, as well as temporal optical flow clue. We also propose a\nvehicle-centric sampling mechanism to alleviate the effect of perspective\ndistortion in the motion field (i.e. optical flow). We implement the method by\na light-weight deep neural network. Extensive experiments are conducted which\nconfirm the superior performance of our method over other state-of-the-art\nmethods, in terms of estimation accuracy, computational speed, and memory\nfootprint.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 08:18:31 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 07:40:51 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Song", "Zhenbo", ""], ["Lu", "Jianfeng", ""], ["Zhang", "Tong", ""], ["Li", "Hongdong", ""]]}, {"id": "2006.04093", "submitter": "Chuanguang Yang", "authors": "Chuanguang Yang, Zhulin An, Yongjun Xu", "title": "Multi-view Contrastive Learning for Online Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous Online Knowledge Distillation (OKD) often carries out mutually\nexchanging probability distributions, but neglects the useful representational\nknowledge. We therefore propose Multi-view Contrastive Learning (MCL) for OKD\nto implicitly capture correlations of feature embeddings encoded by multiple\npeer networks, which provide various views for understanding the input data\ninstances. Benefiting from MCL, we can learn a more discriminative\nrepresentation space for classification than previous OKD methods. Experimental\nresults on image classification demonstrate that our MCL-OKD outperforms other\nstate-of-the-art OKD methods by large margins without sacrificing additional\ninference cost. Codes are available at https://github.com/winycg/MCL-OKD.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 09:11:28 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 16:25:13 GMT"}, {"version": "v3", "created": "Sat, 10 Apr 2021 08:49:09 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Yang", "Chuanguang", ""], ["An", "Zhulin", ""], ["Xu", "Yongjun", ""]]}, {"id": "2006.04096", "submitter": "Amir Zamir", "authors": "Amir Zamir, Alexander Sax, Teresa Yeo, O\\u{g}uzhan Kar, Nikhil\n  Cheerla, Rohan Suri, Zhangjie Cao, Jitendra Malik, Leonidas Guibas", "title": "Robust Learning Through Cross-Task Consistency", "comments": "CVPR 2020 (Oral). Project website, models, live demo at\n  http://consistency.epfl.ch/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual perception entails solving a wide set of tasks, e.g., object\ndetection, depth estimation, etc. The predictions made for multiple tasks from\nthe same image are not independent, and therefore, are expected to be\nconsistent. We propose a broadly applicable and fully computational method for\naugmenting learning with Cross-Task Consistency. The proposed formulation is\nbased on inference-path invariance over a graph of arbitrary tasks. We observe\nthat learning with cross-task consistency leads to more accurate predictions\nand better generalization to out-of-distribution inputs. This framework also\nleads to an informative unsupervised quantity, called Consistency Energy, based\non measuring the intrinsic consistency of the system. Consistency Energy\ncorrelates well with the supervised error (r=0.67), thus it can be employed as\nan unsupervised confidence metric as well as for detection of\nout-of-distribution inputs (ROC-AUC=0.95). The evaluations are performed on\nmultiple datasets, including Taskonomy, Replica, CocoDoom, and ApolloScape, and\nthey benchmark cross-task consistency versus various baselines including\nconventional multi-task learning, cycle consistency, and analytical\nconsistency.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 09:24:33 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Zamir", "Amir", ""], ["Sax", "Alexander", ""], ["Yeo", "Teresa", ""], ["Kar", "O\u011fuzhan", ""], ["Cheerla", "Nikhil", ""], ["Suri", "Rohan", ""], ["Cao", "Zhangjie", ""], ["Malik", "Jitendra", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2006.04115", "submitter": "Moshe Eliasof", "authors": "Moshe Eliasof, Eran Treister", "title": "DiffGCN: Graph Convolutional Networks via Differential Operators and\n  Algebraic Multigrid Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have shown to be effective in handling\nunordered data like point clouds and meshes. In this work we propose novel\napproaches for graph convolution, pooling and unpooling, inspired from finite\ndifferences and algebraic multigrid frameworks. We form a parameterized\nconvolution kernel based on discretized differential operators, leveraging the\ngraph mass, gradient and Laplacian. This way, the parameterization does not\ndepend on the graph structure, only on the meaning of the network convolutions\nas differential operators. To allow hierarchical representations of the input,\nwe propose pooling and unpooling operations that are based on algebraic\nmultigrid methods, which are mainly used to solve partial differential\nequations on unstructured grids. To motivate and explain our method, we compare\nit to standard convolutional neural networks, and show their similarities and\nrelations in the case of a regular grid. Our proposed method is demonstrated in\nvarious experiments like classification and part-segmentation, achieving on par\nor better than state of the art results. We also analyze the computational cost\nof our method compared to other GCNs.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 11:08:37 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 15:36:52 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Eliasof", "Moshe", ""], ["Treister", "Eran", ""]]}, {"id": "2006.04127", "submitter": "Xiaoyu Feng", "authors": "Xiaoyu Feng, Zhuqing Yuan, Guijin Wang, Yongpan Liu", "title": "ADMP: An Adversarial Double Masks Based Pruning Framework For\n  Unsupervised Cross-Domain Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent progress of network pruning, directly applying it to the\nInternet of Things (IoT) applications still faces two challenges, i.e. the\ndistribution divergence between end and cloud data and the missing of data\nlabel on end devices. One straightforward solution is to combine the\nunsupervised domain adaptation (UDA) technique and pruning. For example, the\nmodel is first pruned on the cloud and then transferred from cloud to end by\nUDA. However, such a naive combination faces high performance degradation.\nHence this work proposes an Adversarial Double Masks based Pruning (ADMP) for\nsuch cross-domain compression. In ADMP, we construct a Knowledge Distillation\nframework not only to produce pseudo labels but also to provide a measurement\nof domain divergence as the output difference between the full-size teacher and\nthe pruned student. Unlike existing mask-based pruning works, two adversarial\nmasks, i.e. soft and hard masks, are adopted in ADMP. So ADMP can prune the\nmodel effectively while still allowing the model to extract strong\ndomain-invariant features and robust classification boundaries. During\ntraining, the Alternating Direction Multiplier Method is used to overcome the\nbinary constraint of {0,1}-masks. On Office31 and ImageCLEF-DA datasets, the\nproposed ADMP can prune 60% channels with only 0.2% and 0.3% average accuracy\nloss respectively. Compared with the state of art, we can achieve about 1.63x\nparameters reduction and 4.1% and 5.1% accuracy improvement.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 11:44:43 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Feng", "Xiaoyu", ""], ["Yuan", "Zhuqing", ""], ["Wang", "Guijin", ""], ["Liu", "Yongpan", ""]]}, {"id": "2006.04128", "submitter": "Erfan Ebrahim Esfahani", "authors": "Erfan Ebrahim Esfahani", "title": "A multi-channel framework for joint reconstruction of multi-contrast\n  parallel MRI", "comments": "13 pages, 9 figures, journal (under review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing, multi-contrast and parallel imaging have been\nindividually well developed in recent literature but the combination of the\nthree has not been equally well studied, much less the potential benefits of\nisotropy within such a setting. In this paper, a novel isotropic image\nregularizer is introduced to help develop a synergistic image reconstruction\nframework that exploits multi-contrast, multi-coil and compressed sensing\nredundancies in MRI. A convex optimization problem is introduced to model the\nnew framework and a first-order algorithm is developed to solve the problem.\nCompared to other state-of-the-art methods, image quality is significantly\nimproved thanks to guaranteed isotropy and retention of contrast-specific\nfeatures without leakage to other contrasts. The new method turns out to be a\nrobust and viable option for clinical protocols of fast multi-contrast parallel\nMRI, reducing scan times and patient discomfort.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 11:44:54 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 08:10:56 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 06:39:43 GMT"}, {"version": "v4", "created": "Tue, 26 Jan 2021 22:49:54 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Esfahani", "Erfan Ebrahim", ""]]}, {"id": "2006.04139", "submitter": "Fuzhi Yang", "authors": "Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, Baining Guo", "title": "Learning Texture Transformer Network for Image Super-Resolution", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study on image super-resolution (SR), which aims to recover realistic\ntextures from a low-resolution (LR) image. Recent progress has been made by\ntaking high-resolution images as references (Ref), so that relevant textures\ncan be transferred to LR images. However, existing SR approaches neglect to use\nattention mechanisms to transfer high-resolution (HR) textures from Ref images,\nwhich limits these approaches in challenging cases. In this paper, we propose a\nnovel Texture Transformer Network for Image Super-Resolution (TTSR), in which\nthe LR and Ref images are formulated as queries and keys in a transformer,\nrespectively. TTSR consists of four closely-related modules optimized for image\ngeneration tasks, including a learnable texture extractor by DNN, a relevance\nembedding module, a hard-attention module for texture transfer, and a\nsoft-attention module for texture synthesis. Such a design encourages joint\nfeature learning across LR and Ref images, in which deep feature\ncorrespondences can be discovered by attention, and thus accurate texture\nfeatures can be transferred. The proposed texture transformer can be further\nstacked in a cross-scale way, which enables texture recovery from different\nlevels (e.g., from 1x to 4x magnification). Extensive experiments show that\nTTSR achieves significant improvements over state-of-the-art approaches on both\nquantitative and qualitative evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 12:55:34 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 12:19:51 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Yang", "Fuzhi", ""], ["Yang", "Huan", ""], ["Fu", "Jianlong", ""], ["Lu", "Hongtao", ""], ["Guo", "Baining", ""]]}, {"id": "2006.04147", "submitter": "Guile Wu", "authors": "Guile Wu and Shaogang Gong", "title": "Peer Collaborative Learning for Online Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional knowledge distillation uses a two-stage training strategy to\ntransfer knowledge from a high-capacity teacher model to a compact student\nmodel, which relies heavily on the pre-trained teacher. Recent online knowledge\ndistillation alleviates this limitation by collaborative learning, mutual\nlearning and online ensembling, following a one-stage end-to-end training\nfashion. However, collaborative learning and mutual learning fail to construct\nan online high-capacity teacher, whilst online ensembling ignores the\ncollaboration among branches and its logit summation impedes the further\noptimisation of the ensemble teacher. In this work, we propose a novel Peer\nCollaborative Learning method for online knowledge distillation, which\nintegrates online ensembling and network collaboration into a unified\nframework. Specifically, given a target network, we construct a multi-branch\nnetwork for training, in which each branch is called a peer. We perform random\naugmentation multiple times on the inputs to peers and assemble feature\nrepresentations outputted from peers with an additional classifier as the peer\nensemble teacher. This helps to transfer knowledge from a high-capacity teacher\nto peers, and in turn further optimises the ensemble teacher. Meanwhile, we\nemploy the temporal mean model of each peer as the peer mean teacher to\ncollaboratively transfer knowledge among peers, which helps each peer to learn\nricher knowledge and facilitates to optimise a more stable model with better\ngeneralisation. Extensive experiments on CIFAR-10, CIFAR-100 and ImageNet show\nthat the proposed method significantly improves the generalisation of various\nbackbone networks and outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 13:21:52 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 15:00:39 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Wu", "Guile", ""], ["Gong", "Shaogang", ""]]}, {"id": "2006.04150", "submitter": "Guile Wu", "authors": "Guile Wu and Shaogang Gong", "title": "Decentralised Learning from Independent Multi-Domain Labels for Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been successful for many computer vision tasks due to the\navailability of shared and centralised large-scale training data. However,\nincreasing awareness of privacy concerns poses new challenges to deep learning,\nespecially for human subject related recognition such as person\nre-identification (Re-ID). In this work, we solve the Re-ID problem by\ndecentralised learning from non-shared private training data distributed at\nmultiple user sites of independent multi-domain label spaces. We propose a\nnovel paradigm called Federated Person Re-Identification (FedReID) to construct\na generalisable global model (a central server) by simultaneously learning with\nmultiple privacy-preserved local models (local clients). Specifically, each\nlocal client receives global model updates from the server and trains a local\nmodel using its local data independent from all the other clients. Then, the\ncentral server aggregates transferrable local model updates to construct a\ngeneralisable global feature embedding model without accessing local data so to\npreserve local privacy. This client-server collaborative learning process is\niteratively performed under privacy control, enabling FedReID to realise\ndecentralised learning without sharing distributed data nor collecting any\ncentralised data. Extensive experiments on ten Re-ID benchmarks show that\nFedReID achieves compelling generalisation performance beyond any locally\ntrained models without using shared training data, whilst inherently protects\nthe privacy of each local client. This is uniquely advantageous over\ncontemporary Re-ID methods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 13:32:33 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 19:26:54 GMT"}, {"version": "v3", "created": "Sat, 2 Jan 2021 10:44:12 GMT"}, {"version": "v4", "created": "Mon, 8 Mar 2021 18:12:35 GMT"}, {"version": "v5", "created": "Wed, 7 Jul 2021 07:39:18 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Wu", "Guile", ""], ["Gong", "Shaogang", ""]]}, {"id": "2006.04170", "submitter": "Arseny Nerinovsky", "authors": "Arseny Nerinovsky, Igor Buzhinsky, Andey Filchencov", "title": "Realistic text replacement with non-uniform style conditioning", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the possibility of realistic text replacement, the\ngoal of which is to replace text present in the image with user-supplied text.\nThe replacement should be performed in a way that will not allow distinguishing\nthe resulting image from the original one. We achieve this goal by developing a\nnovel non-uniform style conditioning layer and apply it to an encoder-decoder\nResNet based architecture. The resulting model is a single-stage model, with no\npost-processing. The proposed model achieves realistic text replacement and\noutperforms existing approaches on ICDAR MLT.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 15:05:42 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Nerinovsky", "Arseny", ""], ["Buzhinsky", "Igor", ""], ["Filchencov", "Andey", ""]]}, {"id": "2006.04171", "submitter": "Xilu Wang", "authors": "Xilu Wang", "title": "Learning pose variations within shape population by constrained mixtures\n  of factor analyzers", "comments": "25 Pages, 15 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining and learning the shape variability of underlying population has\nbenefited the applications including parametric shape modeling, 3D animation,\nand image segmentation. The current statistical shape modeling method works\nwell on learning unstructured shape variations without obvious pose changes\n(relative rotations of the body parts). Studying the pose variations within a\nshape population involves segmenting the shapes into different articulated\nparts and learning the transformations of the segmented parts. This paper\nformulates the pose learning problem as mixtures of factor analyzers. The\nsegmentation is obtained by components posterior probabilities and the\nrotations in pose variations are learned by the factor loading matrices. To\nguarantee that the factor loading matrices are composed by rotation matrices,\nconstraints are imposed and the corresponding closed form optimal solution is\nderived. Based on the proposed method, the pose variations are automatically\nlearned from the given shape populations. The method is applied in motion\nanimation where new poses are generated by interpolating the existing poses in\nthe training set. The obtained results are smooth and realistic.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 15:06:01 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Wang", "Xilu", ""]]}, {"id": "2006.04193", "submitter": "Raid Al-Nima", "authors": "Raid R. O. Al-Nima, Tingting Han, Taolue Chen, Satnam Dlay and\n  Jonathon Chambers", "title": "Finger Texture Biometric Characteristic: a Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\begin{abstract}\n  In recent years, the Finger Texture (FT) has attracted considerable attention\nas a biometric characteristic. It can provide efficient human recognition\nperformance, because it has different human-specific features of apparent\nlines, wrinkles and ridges distributed along the inner surface of all fingers.\nAlso, such pattern structures are reliable, unique and remain stable throughout\na human's life. Efficient biometric systems can be established based only on\nFTs. In this paper, a comprehensive survey of the relevant FT studies is\npresented. We also summarise the main drawbacks and obstacles of employing the\nFT as a biometric characteristic, and provide useful suggestions to further\nimprove the work on FT. \\end{abstract}\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 16:33:59 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Al-Nima", "Raid R. O.", ""], ["Han", "Tingting", ""], ["Chen", "Taolue", ""], ["Dlay", "Satnam", ""], ["Chambers", "Jonathon", ""]]}, {"id": "2006.04203", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Francine Chen, Yan-Ying Chen", "title": "Thoracic Disease Identification and Localization using Distance Learning\n  and Region Verification", "comments": "British Machine Vision Conference (BMVC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification and localization of diseases in medical images using deep\nlearning models have recently attracted significant interest. Existing methods\nonly consider training the networks with each image independently and most\nleverage an activation map for disease localization. In this paper, we propose\nan alternative approach that learns discriminative features among triplets of\nimages and cyclically trains on region features to verify whether attentive\nregions contain information indicative of a disease. Concretely, we adapt a\ndistance learning framework for multi-label disease classification to\ndifferentiate subtle disease features. Additionally, we feed back the features\nof the predicted class-specific regions to a separate classifier during\ntraining to better verify the localized diseases. Our model can achieve\nstate-of-the-art classification performance on the challenging ChestX-ray14\ndataset, and our ablation studies indicate that both distance learning and\nregion verification contribute to overall classification performance. Moreover,\nthe distance learning and region verification modules can capture essential\ninformation for better localization than baseline models without these modules.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 16:56:50 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 21:47:33 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Zhang", "Cheng", ""], ["Chen", "Francine", ""], ["Chen", "Yan-Ying", ""]]}, {"id": "2006.04224", "submitter": "Kumar Ayush", "authors": "Kumar Ayush, Burak Uzkent, Kumar Tanmay, Marshall Burke, David Lobell,\n  Stefano Ermon", "title": "Efficient Poverty Mapping using Deep Reinforcement Learning", "comments": "Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of high-resolution satellite imagery and machine learning\nhave proven useful in many sustainability-related tasks, including poverty\nprediction, infrastructure measurement, and forest monitoring. However, the\naccuracy afforded by high-resolution imagery comes at a cost, as such imagery\nis extremely expensive to purchase at scale. This creates a substantial hurdle\nto the efficient scaling and widespread adoption of high-resolution-based\napproaches. To reduce acquisition costs while maintaining accuracy, we propose\na reinforcement learning approach in which free low-resolution imagery is used\nto dynamically identify where to acquire costly high-resolution images, prior\nto performing a deep learning task on the high-resolution images. We apply this\napproach to the task of poverty prediction in Uganda, building on an earlier\napproach that used object detection to count objects and use these counts to\npredict poverty. Our approach exceeds previous performance benchmarks on this\ntask while using 80% fewer high-resolution images. Our approach could have\napplication in many sustainability domains that require high-resolution\nimagery.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 18:30:57 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 11:30:00 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Ayush", "Kumar", ""], ["Uzkent", "Burak", ""], ["Tanmay", "Kumar", ""], ["Burke", "Marshall", ""], ["Lobell", "David", ""], ["Ermon", "Stefano", ""]]}, {"id": "2006.04225", "submitter": "Sina Sharif Mansouri", "authors": "Sina Sharif Mansouri, Farhad Pourkamali-Anaraki, Miguel Castano\n  Arranz, Ali-akbar Agha-mohammadi, Joel Burdick, and George Nikolakopoulos", "title": "Unsupervised Learning for Subterranean Junction Recognition Based on 2D\n  Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a novel unsupervised learning framework for detecting\nthe number of tunnel junctions in subterranean environments based on acquired\n2D point clouds. The implementation of the framework provides valuable\ninformation for high level mission planners to navigate an aerial platform in\nunknown areas or robot homing missions. The framework utilizes spectral\nclustering, which is capable of uncovering hidden structures from connected\ndata points lying on non-linear manifolds. The spectral clustering algorithm\ncomputes a spectral embedding of the original 2D point cloud by utilizing the\neigen decomposition of a matrix that is derived from the pairwise similarities\nof these points. We validate the developed framework using multiple data-sets,\ncollected from multiple realistic simulations, as well as from real flights in\nunderground environments, demonstrating the performance and merits of the\nproposed methodology.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 18:36:56 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Mansouri", "Sina Sharif", ""], ["Pourkamali-Anaraki", "Farhad", ""], ["Arranz", "Miguel Castano", ""], ["Agha-mohammadi", "Ali-akbar", ""], ["Burdick", "Joel", ""], ["Nikolakopoulos", "George", ""]]}, {"id": "2006.04246", "submitter": "Chong You", "authors": "Chong You, Chi Li, Daniel P. Robinson, Rene Vidal", "title": "Self-Representation Based Unsupervised Exemplar Selection in a Union of\n  Subspaces", "comments": "In submission; conference version at ECCV'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a small set of representatives from an unlabeled dataset is a core\nproblem in a broad range of applications such as dataset summarization and\ninformation extraction. Classical exemplar selection methods such as\n$k$-medoids work under the assumption that the data points are close to a few\ncluster centroids, and cannot handle the case where data lie close to a union\nof subspaces. This paper proposes a new exemplar selection model that searches\nfor a subset that best reconstructs all data points as measured by the $\\ell_1$\nnorm of the representation coefficients. Geometrically, this subset best covers\nall the data points as measured by the Minkowski functional of the subset. To\nsolve our model efficiently, we introduce a farthest first search algorithm\nthat iteratively selects the worst represented point as an exemplar. When the\ndataset is drawn from a union of independent subspaces, our method is able to\nselect sufficiently many representatives from each subspace. We further develop\nan exemplar based subspace clustering method that is robust to imbalanced data\nand efficient for large scale data. Moreover, we show that a classifier trained\non the selected exemplars (when they are labeled) can correctly classify the\nrest of the data points.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 19:43:33 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["You", "Chong", ""], ["Li", "Chi", ""], ["Robinson", "Daniel P.", ""], ["Vidal", "Rene", ""]]}, {"id": "2006.04250", "submitter": "Luca Cavalli", "authors": "Luca Cavalli, Viktor Larsson, Martin Ralf Oswald, Torsten Sattler,\n  Marc Pollefeys", "title": "AdaLAM: Revisiting Handcrafted Outlier Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local feature matching is a critical component of many computer vision\npipelines, including among others Structure-from-Motion, SLAM, and Visual\nLocalization. However, due to limitations in the descriptors, raw matches are\noften contaminated by a majority of outliers. As a result, outlier detection is\na fundamental problem in computer vision, and a wide range of approaches have\nbeen proposed over the last decades. In this paper we revisit handcrafted\napproaches to outlier filtering. Based on best practices, we propose a\nhierarchical pipeline for effective outlier detection as well as integrate\nnovel ideas which in sum lead to AdaLAM, an efficient and competitive approach\nto outlier rejection. AdaLAM is designed to effectively exploit modern parallel\nhardware, resulting in a very fast, yet very accurate, outlier filter. We\nvalidate AdaLAM on multiple large and diverse datasets, and we submit to the\nImage Matching Challenge (CVPR2020), obtaining competitive results with simple\nbaseline descriptors. We show that AdaLAM is more than competitive to current\nstate of the art, both in terms of efficiency and effectiveness.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 20:16:36 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Cavalli", "Luca", ""], ["Larsson", "Viktor", ""], ["Oswald", "Martin Ralf", ""], ["Sattler", "Torsten", ""], ["Pollefeys", "Marc", ""]]}, {"id": "2006.04255", "submitter": "Koushik Nagasubramanian", "authors": "Koushik Nagasubramanian, Talukder Z. Jubery, Fateme Fotouhi Ardakani,\n  Seyed Vahid Mirnezami, Asheesh K. Singh, Arti Singh, Soumik Sarkar, and\n  Baskar Ganapathysubramanian", "title": "How useful is Active Learning for Image-based Plant Phenotyping?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have been successfully deployed for a diverse array of\nimage-based plant phenotyping applications including disease detection and\nclassification. However, successful deployment of supervised deep learning\nmodels requires large amount of labeled data, which is a significant challenge\nin plant science (and most biological) domains due to the inherent complexity.\nSpecifically, data annotation is costly, laborious, time consuming and needs\ndomain expertise for phenotyping tasks, especially for diseases. To overcome\nthis challenge, active learning algorithms have been proposed that reduce the\namount of labeling needed by deep learning models to achieve good predictive\nperformance. Active learning methods adaptively select samples to annotate\nusing an acquisition function to achieve maximum (classification) performance\nunder a fixed labeling budget. We report the performance of four different\nactive learning methods, (1) Deep Bayesian Active Learning (DBAL), (2) Entropy,\n(3) Least Confidence, and (4) Coreset, with conventional random sampling-based\nannotation for two different image-based classification datasets. The first\nimage dataset consists of soybean [Glycine max L. (Merr.)] leaves belonging to\neight different soybean stresses and a healthy class, and the second consists\nof nine different weed species from the field. For a fixed labeling budget, we\nobserved that the classification performance of deep learning models with\nactive learning-based acquisition strategies is better than random\nsampling-based acquisition for both datasets. The integration of active\nlearning strategies for data annotation can help mitigate labelling challenges\nin the plant sciences applications particularly where deep domain knowledge is\nrequired.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 20:32:42 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 08:20:24 GMT"}, {"version": "v3", "created": "Sat, 11 Jul 2020 07:07:28 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Nagasubramanian", "Koushik", ""], ["Jubery", "Talukder Z.", ""], ["Ardakani", "Fateme Fotouhi", ""], ["Mirnezami", "Seyed Vahid", ""], ["Singh", "Asheesh K.", ""], ["Singh", "Arti", ""], ["Sarkar", "Soumik", ""], ["Ganapathysubramanian", "Baskar", ""]]}, {"id": "2006.04270", "submitter": "Hojjat Salehinejad", "authors": "Hojjat Salehinejad and Shahrokh Valaee", "title": "EDropout: Energy-Based Dropout and Pruning of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is a well-known regularization method by sampling a sub-network from\na larger deep neural network and training different sub-networks on different\nsubsets of the data. Inspired by the dropout concept, we propose EDropout as an\nenergy-based framework for pruning neural networks in classification tasks. In\nthis approach, a set of binary pruning state vectors (population) represents a\nset of corresponding sub-networks from an arbitrary provided original neural\nnetwork. An energy loss function assigns a scalar energy loss value to each\npruning state. The energy-based model stochastically evolves the population to\nfind states with lower energy loss. The best pruning state is then selected and\napplied to the original network. Similar to dropout, the kept weights are\nupdated using backpropagation in a probabilistic model. The energy-based model\nagain searches for better pruning states and the cycle continuous. Indeed, this\nprocedure is in fact switching between the energy model, which manages the\npruning states, and the probabilistic model, which updates the temporarily\nunpruned weights, in each iteration. The population can dynamically converge to\na pruning state. This can be interpreted as dropout leading to pruning the\nnetwork. From an implementation perspective, EDropout can prune typical neural\nnetworks without modification of the network architecture. We evaluated the\nproposed method on different flavours of ResNets, AlexNet, and SqueezeNet on\nthe Kuzushiji, Fashion, CIFAR-10, CIFAR-100, and Flowers datasets, and compared\nthe pruning rate and classification performance of the models. On average the\nnetworks trained with \\textit{EDropout} achieved a pruning rate of more than\n$50\\%$ of the trainable parameters with approximately $<5\\%$ and $<1\\%$ drop of\nTop-1 and Top-5 classification accuracy, respectively.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 21:09:44 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 05:53:19 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 22:36:02 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Salehinejad", "Hojjat", ""], ["Valaee", "Shahrokh", ""]]}, {"id": "2006.04298", "submitter": "Jin-Hwa Kim", "authors": "Jin-Hwa Kim, Junyoung Park, Yongseok Choi", "title": "Multi-step Estimation for Gradient-based Meta-learning", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based meta-learning approaches have been successful in few-shot\nlearning, transfer learning, and a wide range of other domains. Despite its\nefficacy and simplicity, the burden of calculating the Hessian matrix with\nlarge memory footprints is the critical challenge in large-scale applications.\nTo tackle this issue, we propose a simple yet straightforward method to reduce\nthe cost by reusing the same gradient in a window of inner steps. We describe\nthe dynamics of the multi-step estimation in the Lagrangian formalism and\ndiscuss how to reduce evaluating second-order derivatives estimating the\ndynamics. To validate our method, we experiment on meta-transfer learning and\nfew-shot learning tasks for multiple settings. The experiment on meta-transfer\nemphasizes the applicability of training meta-networks, where other\napproximations are limited. For few-shot learning, we evaluate time and memory\ncomplexities compared with popular baselines. We show that our method\nsignificantly reduces training time and memory usage, maintaining competitive\naccuracies, or even outperforming in some cases.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 00:37:01 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Kim", "Jin-Hwa", ""], ["Park", "Junyoung", ""], ["Choi", "Yongseok", ""]]}, {"id": "2006.04305", "submitter": "Zobeir Raisi", "authors": "Zobeir Raisi, Mohamed A. Naiel, Paul Fieguth, Steven Wardell, and John\n  Zelek", "title": "Text Detection and Recognition in the Wild: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection and recognition of text in natural images are two main problems in\nthe field of computer vision that have a wide variety of applications in\nanalysis of sports videos, autonomous driving, industrial automation, to name a\nfew. They face common challenging problems that are factors in how text is\nrepresented and affected by several environmental conditions. The current\nstate-of-the-art scene text detection and/or recognition methods have exploited\nthe witnessed advancement in deep learning architectures and reported a\nsuperior accuracy on benchmark datasets when tackling multi-resolution and\nmulti-oriented text. However, there are still several remaining challenges\naffecting text in the wild images that cause existing methods to underperform\ndue to there models are not able to generalize to unseen data and the\ninsufficient labeled data. Thus, unlike previous surveys in this field, the\nobjectives of this survey are as follows: first, offering the reader not only a\nreview on the recent advancement in scene text detection and recognition, but\nalso presenting the results of conducting extensive experiments using a unified\nevaluation framework that assesses pre-trained models of the selected methods\non challenging cases, and applies the same evaluation criteria on these\ntechniques. Second, identifying several existing challenges for detecting or\nrecognizing text in the wild images, namely, in-plane-rotation, multi-oriented\nand multi-resolution text, perspective distortion, illumination reflection,\npartial occlusion, complex fonts, and special characters. Finally, the paper\nalso presents insight into the potential research directions in this field to\naddress some of the mentioned challenges that are still encountering scene text\ndetection and recognition techniques.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 01:08:04 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 22:23:08 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Raisi", "Zobeir", ""], ["Naiel", "Mohamed A.", ""], ["Fieguth", "Paul", ""], ["Wardell", "Steven", ""], ["Zelek", "John", ""]]}, {"id": "2006.04307", "submitter": "Biao Gao", "authors": "Biao Gao, Yancheng Pan, Chengkun Li, Sibo Geng, Huijing Zhao", "title": "Are We Hungry for 3D LiDAR Data for Semantic Segmentation? A Survey and\n  Experimental Study", "comments": "22 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D semantic segmentation is a fundamental task for robotic and autonomous\ndriving applications. Recent works have been focused on using deep learning\ntechniques, whereas developing fine-annotated 3D LiDAR datasets is extremely\nlabor intensive and requires professional skills. The performance limitation\ncaused by insufficient datasets is called data hunger problem. This research\nprovides a comprehensive survey and experimental study on the question: are we\nhungry for 3D LiDAR data for semantic segmentation? The studies are conducted\nat three levels. First, a broad review to the main 3D LiDAR datasets is\nconducted, followed by a statistical analysis on three representative datasets\nto gain an in-depth view on the datasets' size and diversity, which are the\ncritical factors in learning deep models. Second, a systematic review to the\nstate-of-the-art 3D semantic segmentation is conducted, followed by experiments\nand cross examinations of three representative deep learning methods to find\nout how the size and diversity of the datasets affect deep models' performance.\nFinally, a systematic survey to the existing efforts to solve the data hunger\nproblem is conducted on both methodological and dataset's viewpoints, followed\nby an insightful discussion of remaining problems and open questions To the\nbest of our knowledge, this is the first work to analyze the data hunger\nproblem for 3D semantic segmentation using deep learning techniques that are\naddressed in the literature review, statistical analysis, and cross-dataset and\ncross-algorithm experiments. We share findings and discussions, which may lead\nto potential topics in future works.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 01:20:59 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 12:58:20 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Gao", "Biao", ""], ["Pan", "Yancheng", ""], ["Li", "Chengkun", ""], ["Geng", "Sibo", ""], ["Zhao", "Huijing", ""]]}, {"id": "2006.04315", "submitter": "Yulei Niu", "authors": "Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua,\n  Ji-Rong Wen", "title": "Counterfactual VQA: A Cause-Effect Look at Language Bias", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VQA models may tend to rely on language bias as a shortcut and thus fail to\nsufficiently learn the multi-modal knowledge from both vision and language.\nRecent debiasing methods proposed to exclude the language prior during\ninference. However, they fail to disentangle the \"good\" language context and\n\"bad\" language bias from the whole. In this paper, we investigate how to\nmitigate language bias in VQA. Motivated by causal effects, we proposed a novel\ncounterfactual inference framework, which enables us to capture the language\nbias as the direct causal effect of questions on answers and reduce the\nlanguage bias by subtracting the direct language effect from the total causal\neffect. Experiments demonstrate that our proposed counterfactual inference\nframework 1) is general to various VQA backbones and fusion strategies, 2)\nachieves competitive performance on the language-bias sensitive VQA-CP dataset\nwhile performs robustly on the balanced VQA v2 dataset without any augmented\ndata. The code is available at https://github.com/yuleiniu/cfvqa.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 01:49:27 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 16:08:46 GMT"}, {"version": "v3", "created": "Mon, 28 Dec 2020 10:35:08 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 16:15:36 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Niu", "Yulei", ""], ["Tang", "Kaihua", ""], ["Zhang", "Hanwang", ""], ["Lu", "Zhiwu", ""], ["Hua", "Xian-Sheng", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2006.04323", "submitter": "Yuhong Guo", "authors": "Zhen Zhao, Bingyu Liu, Yuhong Guo, Jieping Ye", "title": "Ensemble Model with Batch Spectral Regularization and Data Blending for\n  Cross-Domain Few-Shot Learning with Unlabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present our proposed ensemble model with batch spectral\nregularization and data blending mechanisms for the Track 2 problem of the\ncross-domain few-shot learning (CD-FSL) challenge. We build a multi-branch\nensemble framework by using diverse feature transformation matrices, while\ndeploying batch spectral feature regularization on each branch to improve the\nmodel's transferability. Moreover, we propose a data blending method to exploit\nthe unlabeled data and augment the sparse support set in the target domain. Our\nproposed model demonstrates effective performance on the CD-FSL benchmark\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 02:27:34 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 07:52:51 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zhao", "Zhen", ""], ["Liu", "Bingyu", ""], ["Guo", "Yuhong", ""], ["Ye", "Jieping", ""]]}, {"id": "2006.04325", "submitter": "Yi Zhou", "authors": "Yi Zhou, Chenglei Wu, Zimo Li, Chen Cao, Yuting Ye, Jason Saragih, Hao\n  Li, Yaser Sheikh", "title": "Fully Convolutional Mesh Autoencoder using Efficient Spatially Varying\n  Kernels", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning latent representations of registered meshes is useful for many 3D\ntasks. Techniques have recently shifted to neural mesh autoencoders. Although\nthey demonstrate higher precision than traditional methods, they remain unable\nto capture fine-grained deformations. Furthermore, these methods can only be\napplied to a template-specific surface mesh, and is not applicable to more\ngeneral meshes, like tetrahedrons and non-manifold meshes. While more general\ngraph convolution methods can be employed, they lack performance in\nreconstruction precision and require higher memory usage. In this paper, we\npropose a non-template-specific fully convolutional mesh autoencoder for\narbitrary registered mesh data. It is enabled by our novel convolution and\n(un)pooling operators learned with globally shared weights and locally varying\ncoefficients which can efficiently capture the spatially varying contents\npresented by irregular mesh connections. Our model outperforms state-of-the-art\nmethods on reconstruction accuracy. In addition, the latent codes of our\nnetwork are fully localized thanks to the fully convolutional structure, and\nthus have much higher interpolation capability than many traditional 3D mesh\ngeneration models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 02:30:13 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 06:16:23 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Zhou", "Yi", ""], ["Wu", "Chenglei", ""], ["Li", "Zimo", ""], ["Cao", "Chen", ""], ["Ye", "Yuting", ""], ["Saragih", "Jason", ""], ["Li", "Hao", ""], ["Sheikh", "Yaser", ""]]}, {"id": "2006.04343", "submitter": "JongYoon Lim", "authors": "JongYoon Lim, Ho Seok Ahn, Mahla Nejati, Jamie Bell, Henry Williams,\n  Bruce A. MacDonald", "title": "Deep Neural Network Based Real-time Kiwi Fruit Flower Detection in an\n  Orchard Environment", "comments": "ACRA(Australian Robotics and Automation Association) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach to kiwi fruit flower detection\nusing Deep Neural Networks (DNNs) to build an accurate, fast, and robust\nautonomous pollination robot system. Recent work in deep neural networks has\nshown outstanding performance on object detection tasks in many areas. Inspired\nthis, we aim for exploiting DNNs for kiwi fruit flower detection and present\nintensive experiments and their analysis on two state-of-the-art object\ndetectors; Faster R-CNN and Single Shot Detector (SSD) Net, and feature\nextractors; Inception Net V2 and NAS Net with real-world orchard datasets. We\nalso compare those approaches to find an optimal model which is suitable for a\nreal-time agricultural pollination robot system in terms of accuracy and\nprocessing speed. We perform experiments with dataset collected from different\nseasons and locations (spatio-temporal consistency) in order to demonstrate the\nperformance of the generalized model. The proposed system demonstrates\npromising results of 0.919, 0.874, and 0.889 for precision, recall, and\nF1-score respectively on our real-world dataset, and the performance satisfies\nthe requirement for deploying the system onto an autonomous pollination\nrobotics system.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 03:53:54 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Lim", "JongYoon", ""], ["Ahn", "Ho Seok", ""], ["Nejati", "Mahla", ""], ["Bell", "Jamie", ""], ["Williams", "Henry", ""], ["MacDonald", "Bruce A.", ""]]}, {"id": "2006.04345", "submitter": "Mo Hossny", "authors": "Mohammed Hossny, Khaled Saleh, Mohammed Attia, Ahmed Abobakr, Julie\n  Iskander", "title": "Fast Synthetic LiDAR Rendering via Spherical UV Unwrapping of\n  Equirectangular Z-Buffer Images", "comments": "This version has been removed by arXiv administrators as the\n  submitter did not have the right to agree to the license at the time of\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR data is becoming increasingly essential with the rise of autonomous\nvehicles. Its ability to provide 360deg horizontal field of view of point\ncloud, equips self-driving vehicles with enhanced situational awareness\ncapabilities. While synthetic LiDAR data generation pipelines provide a good\nsolution to advance the machine learning research on LiDAR, they do suffer from\na major shortcoming, which is rendering time. Physically accurate LiDAR\nsimulators (e.g. Blensor) are computationally expensive with an average\nrendering time of 14-60 seconds per frame for urban scenes. This is often\ncompensated for via using 3D models with simplified polygon topology (low poly\nassets) as is the case of CARLA (Dosovitskiy et al., 2017). However, this comes\nat the price of having coarse grained unrealistic LiDAR point clouds. In this\npaper, we present a novel method to simulate LiDAR point cloud with faster\nrendering time of 1 sec per frame. The proposed method relies on spherical UV\nunwrapping of Equirectangular Z-Buffer images. We chose Blensor (Gschwandtner\net al., 2011) as the baseline method to compare the point clouds generated\nusing the proposed method. The reported error for complex urban landscapes is\n4.28cm for a scanning range between 2-120 meters with Velodyne HDL64-E2\nparameters. The proposed method reported a total time per frame to 3.2 +/- 0.31\nseconds per frame. In contrast, the BlenSor baseline method reported 16.2 +/-\n1.82 seconds.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 04:07:57 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Hossny", "Mohammed", ""], ["Saleh", "Khaled", ""], ["Attia", "Mohammed", ""], ["Abobakr", "Ahmed", ""], ["Iskander", "Julie", ""]]}, {"id": "2006.04356", "submitter": "Liang Du", "authors": "Liang Du and Xiaoqing Ye and Xiao Tan and Jianfeng Feng and Zhenbo Xu\n  and Errui Ding and Shilei Wen", "title": "Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud\n  Object Detection", "comments": "8 pages, 5 figures, CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection from 3D point clouds remains a challenging task, though\nrecent studies pushed the envelope with the deep learning techniques. Owing to\nthe severe spatial occlusion and inherent variance of point density with the\ndistance to sensors, appearance of a same object varies a lot in point cloud\ndata. Designing robust feature representation against such appearance changes\nis hence the key issue in a 3D object detection method. In this paper, we\ninnovatively propose a domain adaptation like approach to enhance the\nrobustness of the feature representation. More specifically, we bridge the gap\nbetween the perceptual domain where the feature comes from a real scene and the\nconceptual domain where the feature is extracted from an augmented scene\nconsisting of non-occlusion point cloud rich of detailed information. This\ndomain adaptation approach mimics the functionality of the human brain when\nproceeding object perception. Extensive experiments demonstrate that our simple\nyet effective approach fundamentally boosts the performance of 3D point cloud\nobject detection and achieves the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 05:15:06 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Du", "Liang", ""], ["Ye", "Xiaoqing", ""], ["Tan", "Xiao", ""], ["Feng", "Jianfeng", ""], ["Xu", "Zhenbo", ""], ["Ding", "Errui", ""], ["Wen", "Shilei", ""]]}, {"id": "2006.04357", "submitter": "Yuchen Fan", "authors": "Yuchen Fan, Jiahui Yu, Yiqun Mei, Yulun Zhang, Yun Fu, Ding Liu,\n  Thomas S. Huang", "title": "Neural Sparse Representation for Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the robustness and efficiency of sparse representation in sparse\ncoding based image restoration models, we investigate the sparsity of neurons\nin deep networks. Our method structurally enforces sparsity constraints upon\nhidden neurons. The sparsity constraints are favorable for gradient-based\nlearning algorithms and attachable to convolution layers in various networks.\nSparsity in neurons enables computation saving by only operating on non-zero\ncomponents without hurting accuracy. Meanwhile, our method can magnify\nrepresentation dimensionality and model capacity with negligible additional\ncomputation cost. Experiments show that sparse representation is crucial in\ndeep neural networks for multiple image restoration tasks, including image\nsuper-resolution, image denoising, and image compression artifacts removal.\nCode is available at https://github.com/ychfan/nsr\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 05:15:17 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Fan", "Yuchen", ""], ["Yu", "Jiahui", ""], ["Mei", "Yiqun", ""], ["Zhang", "Yulun", ""], ["Fu", "Yun", ""], ["Liu", "Ding", ""], ["Huang", "Thomas S.", ""]]}, {"id": "2006.04368", "submitter": "Da He", "authors": "Jiasheng Zhou, Da He, Xiaoyu Shang, Zhendong Guo, Sung-liang Chen,\n  Jiajia Luo", "title": "Photoacoustic Microscopy with Sparse Data Enabled by Convolutional\n  Neural Networks for Fast Imaging", "comments": "13 pages (including 2 pages of supplementary materials)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Photoacoustic microscopy (PAM) has been a promising biomedical imaging\ntechnology in recent years. However, the point-by-point scanning mechanism\nresults in low-speed imaging, which limits the application of PAM. Reducing\nsampling density can naturally shorten image acquisition time, which is at the\ncost of image quality. In this work, we propose a method using convolutional\nneural networks (CNNs) to improve the quality of sparse PAM images, thereby\nspeeding up image acquisition while keeping good image quality. The CNN model\nutilizes both squeeze-and-excitation blocks and residual blocks to achieve the\nenhancement, which is a mapping from a 1/4 or 1/16 low-sampling sparse PAM\nimage to a latent fully-sampled image. The perceptual loss function is applied\nto keep the fidelity of images. The model is mainly trained and validated on\nPAM images of leaf veins. The experiments show the effectiveness of our\nproposed method, which significantly outperforms existing methods\nquantitatively and qualitatively. Our model is also tested using in vivo PAM\nimages of blood vessels of mouse ears and eyes. The results show that the model\ncan enhance the image quality of the sparse PAM image of blood vessels from\nseveral aspects, which may help fast PAM and facilitate its clinical\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 05:49:32 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Zhou", "Jiasheng", ""], ["He", "Da", ""], ["Shang", "Xiaoyu", ""], ["Guo", "Zhendong", ""], ["Chen", "Sung-liang", ""], ["Luo", "Jiajia", ""]]}, {"id": "2006.04371", "submitter": "Xiaobin Wei", "authors": "Xiaobin Wei, Jianjiang Feng, Jie Zhou", "title": "Semantics-Driven Unsupervised Learning for Monocular Depth and\n  Ego-Motion Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semantics-driven unsupervised learning approach for monocular\ndepth and ego-motion estimation from videos in this paper. Recent unsupervised\nlearning methods employ photometric errors between synthetic view and actual\nimage as a supervision signal for training. In our method, we exploit semantic\nsegmentation information to mitigate the effects of dynamic objects and\nocclusions in the scene, and to improve depth prediction performance by\nconsidering the correlation between depth and semantics. To avoid costly\nlabeling process, we use noisy semantic segmentation results obtained by a\npre-trained semantic segmentation network. In addition, we minimize the\nposition error between the corresponding points of adjacent frames to utilize\n3D spatial information. Experimental results on the KITTI dataset show that our\nmethod achieves good performance in both depth and ego-motion estimation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 05:55:07 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Wei", "Xiaobin", ""], ["Feng", "Jianjiang", ""], ["Zhou", "Jie", ""]]}, {"id": "2006.04380", "submitter": "Zhi Li", "authors": "Zhi Li, Bo Wu, Qi Liu, Likang Wu, Hongke Zhao, Tao Mei", "title": "Learning the Compositional Visual Coherence for Complementary\n  Recommendations", "comments": "Early version accepted by IJCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complementary recommendations, which aim at providing users product\nsuggestions that are supplementary and compatible with their obtained items,\nhave become a hot topic in both academia and industry in recent years.\n%However, it is challenging due to its complexity and subjectivity. Existing\nwork mainly focused on modeling the co-purchased relations between two items,\nbut the compositional associations of item collections are largely unexplored.\nActually, when a user chooses the complementary items for the purchased\nproducts, it is intuitive that she will consider the visual semantic coherence\n(such as color collocations, texture compatibilities) in addition to global\nimpressions. Towards this end, in this paper, we propose a novel Content\nAttentive Neural Network (CANN) to model the comprehensive compositional\ncoherence on both global contents and semantic contents. Specifically, we first\npropose a \\textit{Global Coherence Learning} (GCL) module based on multi-heads\nattention to model the global compositional coherence. Then, we generate the\nsemantic-focal representations from different semantic regions and design a\n\\textit{Focal Coherence Learning} (FCL) module to learn the focal compositional\ncoherence from different semantic-focal representations. Finally, we optimize\nthe CANN in a novel compositional optimization strategy. Extensive experiments\non the large-scale real-world data clearly demonstrate the effectiveness of\nCANN compared with several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 06:57:18 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Li", "Zhi", ""], ["Wu", "Bo", ""], ["Liu", "Qi", ""], ["Wu", "Likang", ""], ["Zhao", "Hongke", ""], ["Mei", "Tao", ""]]}, {"id": "2006.04388", "submitter": "Xiang Li", "authors": "Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui\n  Tang and Jian Yang", "title": "Generalized Focal Loss: Learning Qualified and Distributed Bounding\n  Boxes for Dense Object Detection", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-stage detector basically formulates object detection as dense\nclassification and localization. The classification is usually optimized by\nFocal Loss and the box location is commonly learned under Dirac delta\ndistribution. A recent trend for one-stage detectors is to introduce an\nindividual prediction branch to estimate the quality of localization, where the\npredicted quality facilitates the classification to improve detection\nperformance. This paper delves into the representations of the above three\nfundamental elements: quality estimation, classification and localization. Two\nproblems are discovered in existing practices, including (1) the inconsistent\nusage of the quality estimation and classification between training and\ninference and (2) the inflexible Dirac delta distribution for localization when\nthere is ambiguity and uncertainty in complex scenes. To address the problems,\nwe design new representations for these elements. Specifically, we merge the\nquality estimation into the class prediction vector to form a joint\nrepresentation of localization quality and classification, and use a vector to\nrepresent arbitrary distribution of box locations. The improved representations\neliminate the inconsistency risk and accurately depict the flexible\ndistribution in real data, but contain continuous labels, which is beyond the\nscope of Focal Loss. We then propose Generalized Focal Loss (GFL) that\ngeneralizes Focal Loss from its discrete form to the continuous version for\nsuccessful optimization. On COCO test-dev, GFL achieves 45.0\\% AP using\nResNet-101 backbone, surpassing state-of-the-art SAPD (43.5\\%) and ATSS\n(43.6\\%) with higher or comparable inference speed, under the same backbone and\ntraining settings. Notably, our best model can achieve a single-model\nsingle-scale AP of 48.2\\%, at 10 FPS on a single 2080Ti GPU. Code and models\nare available at https://github.com/implus/GFocal.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 07:24:33 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Li", "Xiang", ""], ["Wang", "Wenhai", ""], ["Wu", "Lijun", ""], ["Chen", "Shuo", ""], ["Hu", "Xiaolin", ""], ["Li", "Jun", ""], ["Tang", "Jinhui", ""], ["Yang", "Jian", ""]]}, {"id": "2006.04390", "submitter": "Savas Ozkan", "authors": "Bora Baydar, Savas Ozkan, A. Emre Kavur, N. Sinem Gezer, M. Alper\n  Selver, Gozde Bozdagi Akar", "title": "Cross-Domain Segmentation with Adversarial Loss and Covariate Shift for\n  Biomedical Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the widespread use of deep learning methods for semantic segmentation\nof images that are acquired from a single source, clinicians often use\nmulti-domain data for a detailed analysis. For instance, CT and MRI have\nadvantages over each other in terms of imaging quality, artifacts, and output\ncharacteristics that lead to differential diagnosis. The capacity of current\nsegmentation techniques is only allow to work for an individual domain due to\ntheir differences. However, the models that are capable of working on all\nmodalities are essentially needed for a complete solution. Furthermore,\nrobustness is drastically affected by the number of samples in the training\nstep, especially for deep learning models. Hence, there is a necessity that all\navailable data regardless of data domain should be used for reliable methods.\nFor this purpose, this manuscript aims to implement a novel model that can\nlearn robust representations from cross-domain data by encapsulating distinct\nand shared patterns from different modalities. Precisely, covariate shift\nproperty is retained with structural modification and adversarial loss where\nsparse and rich representations are obtained. Hence, a single parameter set is\nused to perform cross-domain segmentation task. The superiority of the proposed\nmethod is that no information related to modalities are provided in either\ntraining or inference phase. The tests on CT and MRI liver data acquired in\nroutine clinical workflows show that the proposed model outperforms all other\nbaseline with a large margin. Experiments are also conducted on Covid-19\ndataset that it consists of CT data where significant intra-class visual\ndifferences are observed. Similarly, the proposed method achieves the best\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 07:35:55 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Baydar", "Bora", ""], ["Ozkan", "Savas", ""], ["Kavur", "A. Emre", ""], ["Gezer", "N. Sinem", ""], ["Selver", "M. Alper", ""], ["Akar", "Gozde Bozdagi", ""]]}, {"id": "2006.04406", "submitter": "Pravendra Singh", "authors": "Pravendra Singh, Pratik Mazumder, Vinay P. Namboodiri", "title": "Passive Batch Injection Training Technique: Boosting Network Performance\n  by Injecting Mini-Batches from a different Data Distribution", "comments": "Accepted in IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel training technique for deep neural networks that\nmakes use of additional data from a distribution that is different from that of\nthe original input data. This technique aims to reduce overfitting and improve\nthe generalization performance of the network. Our proposed technique, namely\nPassive Batch Injection Training Technique (PBITT), even reduces the level of\noverfitting in networks that already use the standard techniques for reducing\noverfitting such as $L_2$ regularization and batch normalization, resulting in\nsignificant accuracy improvements. Passive Batch Injection Training Technique\n(PBITT) introduces a few passive mini-batches into the training process that\ncontain data from a distribution that is different from the input data\ndistribution. This technique does not increase the number of parameters in the\nfinal model and also does not increase the inference (test) time but still\nimproves the performance of deep CNNs. To the best of our knowledge, this is\nthe first work that makes use of different data distribution to aid the\ntraining of convolutional neural networks (CNNs). We thoroughly evaluate the\nproposed approach on standard architectures: VGG, ResNet, and WideResNet, and\non several popular datasets: CIFAR-10, CIFAR-100, SVHN, and ImageNet. We\nobserve consistent accuracy improvement by using the proposed technique. We\nalso show experimentally that the model trained by our technique generalizes\nwell to other tasks such as object detection on the MS-COCO dataset using\nFaster R-CNN. We present extensive ablations to validate the proposed approach.\nOur approach improves the accuracy of VGG-16 by a significant margin of 2.1%\nover the CIFAR-100 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 08:17:32 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Singh", "Pravendra", ""], ["Mazumder", "Pratik", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2006.04436", "submitter": "Julius Ruseckas", "authors": "Eimantas Ledinauskas (1), Julius Ruseckas (1), Alfonsas Jur\\v{s}\\.enas\n  (1), Giedrius Bura\\v{c}as (2) ((1) Baltic Institute of Advanced Technology,\n  Lithuania, (2) SRI International, USA)", "title": "Training Deep Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation using brain-inspired spiking neural networks (SNNs) with\nneuromorphic hardware may offer orders of magnitude higher energy efficiency\ncompared to the current analog neural networks (ANNs). Unfortunately, training\nSNNs with the same number of layers as state of the art ANNs remains a\nchallenge. To our knowledge the only method which is successful in this regard\nis supervised training of ANN and then converting it to SNN. In this work we\ndirectly train deep SNNs using backpropagation with surrogate gradient and find\nthat due to implicitly recurrent nature of feed forward SNN's the exploding or\nvanishing gradient problem severely hinders their training. We show that this\nproblem can be solved by tuning the surrogate gradient function. We also\npropose using batch normalization from ANN literature on input currents of SNN\nneurons. Using these improvements we show that is is possible to train SNN with\nResNet50 architecture on CIFAR100 and Imagenette object recognition datasets.\nThe trained SNN falls behind in accuracy compared to analogous ANN but requires\nseveral orders of magnitude less inference time steps (as low as 10) to reach\ngood accuracy compared to SNNs obtained by conversion from ANN which require on\nthe order of 1000 time steps.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 09:47:05 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Ledinauskas", "Eimantas", ""], ["Ruseckas", "Julius", ""], ["Jur\u0161\u0117nas", "Alfonsas", ""], ["Bura\u010das", "Giedrius", ""]]}, {"id": "2006.04449", "submitter": "Sandesh Kamath K", "authors": "Sandesh Kamath, Amit Deshpande, K V Subrahmanyam", "title": "On Universalized Adversarial and Invariant Perturbations", "comments": "Some part of this work was presented in ICML 2018 Workshop on\n  \"Towards learning with limited labels: Equivariance, Invariance,and Beyond\"\n  as \"Understanding Adversarial Robustness of Symmetric Networks\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional neural networks or standard CNNs (StdCNNs) are\ntranslation-equivariant models that achieve translation invariance when trained\non data augmented with sufficient translations. Recent work on equivariant\nmodels for a given group of transformations (e.g., rotations) has lead to\ngroup-equivariant convolutional neural networks (GCNNs). GCNNs trained on data\naugmented with sufficient rotations achieve rotation invariance. Recent work by\nauthors arXiv:2002.11318 studies a trade-off between invariance and robustness\nto adversarial attacks. In another related work arXiv:2005.08632, given any\nmodel and any input-dependent attack that satisfies a certain spectral\nproperty, the authors propose a universalization technique called SVD-Universal\nto produce a universal adversarial perturbation by looking at very few test\nexamples. In this paper, we study the effectiveness of SVD-Universal on GCNNs\nas they gain rotation invariance through higher degree of training\naugmentation. We empirically observe that as GCNNs gain rotation invariance\nthrough training augmented with larger rotations, the fooling rate of\nSVD-Universal gets better. To understand this phenomenon, we introduce\nuniversal invariant directions and study their relation to the universal\nadversarial direction produced by SVD-Universal.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 10:08:20 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Kamath", "Sandesh", ""], ["Deshpande", "Amit", ""], ["Subrahmanyam", "K V", ""]]}, {"id": "2006.04451", "submitter": "Kuo-Liang Chung", "authors": "Kuo-Liang Chung, Yu-Lun Chang, and Bo-Wei Tsai", "title": "Novel Adaptive Binary Search Strategy-First Hybrid Pyramid- and\n  Clustering-Based CNN Filter Pruning Method without Parameters Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning redundant filters in CNN models has received growing attention. In\nthis paper, we propose an adaptive binary search-first hybrid pyramid- and\nclustering-based (ABSHPC-based) method for pruning filters automatically. In\nour method, for each convolutional layer, initially a hybrid pyramid data\nstructure is constructed to store the hierarchical information of each filter.\nGiven a tolerant accuracy loss, without parameters setting, we begin from the\nlast convolutional layer to the first layer; for each considered layer with\nless or equal pruning rate relative to its previous layer, our ABSHPC-based\nprocess is applied to optimally partition all filters to clusters, where each\ncluster is thus represented by the filter with the median root mean of the\nhybrid pyramid, leading to maximal removal of redundant filters. Based on the\npractical dataset and the CNN models, with higher accuracy, the thorough\nexperimental results demonstrated the significant parameters and floating-point\noperations reduction merits of the proposed filter pruning method relative to\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 10:09:43 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 07:33:00 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Chung", "Kuo-Liang", ""], ["Chang", "Yu-Lun", ""], ["Tsai", "Bo-Wei", ""]]}, {"id": "2006.04455", "submitter": "Bo Zhao", "authors": "Bo Zhao, Shixiang Tang, Dapeng Chen, Hakan Bilen, Rui Zhao", "title": "Continual Representation Learning for Biometric Identification", "comments": null, "journal-ref": "Proceedings of the IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV), 2021, pp. 1198-1208", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the explosion of digital data in recent years, continuously learning new\ntasks from a stream of data without forgetting previously acquired knowledge\nhas become increasingly important. In this paper, we propose a new continual\nlearning (CL) setting, namely ``continual representation learning'', which\nfocuses on learning better representation in a continuous way. We also provide\ntwo large-scale multi-step benchmarks for biometric identification, where the\nvisual appearance of different classes are highly relevant. In contrast to\nrequiring the model to recognize more learned classes, we aim to learn feature\nrepresentation that can be better generalized to not only previously unseen\nimages but also unseen classes/identities. For the new setting, we propose a\nnovel approach that performs the knowledge distillation over a large number of\nidentities by applying the neighbourhood selection and consistency relaxation\nstrategies to improve scalability and flexibility of the continual learning\nmodel. We demonstrate that existing CL methods can improve the representation\nin the new setting, and our method achieves better results than the\ncompetitors.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 10:18:06 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 19:54:51 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhao", "Bo", ""], ["Tang", "Shixiang", ""], ["Chen", "Dapeng", ""], ["Bilen", "Hakan", ""], ["Zhao", "Rui", ""]]}, {"id": "2006.04473", "submitter": "Hichem Sahbi", "authors": "Ahmed Mazari and Hichem Sahbi", "title": "Deep hierarchical pooling design for cross-granularity action\n  recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel hierarchical aggregation design that\ncaptures different levels of temporal granularity in action recognition. Our\ndesign principle is coarse-to-fine and achieved using a tree-structured\nnetwork; as we traverse this network top-down, pooling operations are getting\nless invariant but timely more resolute and well localized. Learning the\ncombination of operations in this network -- which best fits a given\nground-truth -- is obtained by solving a constrained minimization problem whose\nsolution corresponds to the distribution of weights that capture the\ncontribution of each level (and thereby temporal granularity) in the global\nhierarchical pooling process. Besides being principled and well grounded, the\nproposed hierarchical pooling is also video-length agnostic and resilient to\nmisalignments in actions. Extensive experiments conducted on the challenging\nUCF-101 database corroborate these statements.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 11:03:54 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Mazari", "Ahmed", ""], ["Sahbi", "Hichem", ""]]}, {"id": "2006.04489", "submitter": "Hichem Sahbi", "authors": "Ahmed Mazari and Hichem Sahbi", "title": "Action Recognition with Deep Multiple Aggregation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the current action recognition algorithms are based on deep networks\nwhich stack multiple convolutional, pooling and fully connected layers. While\nconvolutional and fully connected operations have been widely studied in the\nliterature, the design of pooling operations that handle action recognition,\nwith different sources of temporal granularity in action categories, has\ncomparatively received less attention, and existing solutions rely mainly on\nmax or averaging operations. The latter are clearly powerless to fully exhibit\nthe actual temporal granularity of action categories and thereby constitute a\nbottleneck in classification performances. In this paper, we introduce a novel\nhierarchical pooling design that captures different levels of temporal\ngranularity in action recognition. Our design principle is coarse-to-fine and\nachieved using a tree-structured network; as we traverse this network top-down,\npooling operations are getting less invariant but timely more resolute and well\nlocalized. Learning the combination of operations in this network -- which best\nfits a given ground-truth -- is obtained by solving a constrained minimization\nproblem whose solution corresponds to the distribution of weights that capture\nthe contribution of each level (and thereby temporal granularity) in the global\nhierarchical pooling process. Besides being principled and well grounded, the\nproposed hierarchical pooling is also video-length and resolution agnostic.\nExtensive experiments conducted on the challenging UCF-101, HMDB-51 and\nJHMDB-21 databases corroborate all these statements.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 11:37:38 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Mazari", "Ahmed", ""], ["Sahbi", "Hichem", ""]]}, {"id": "2006.04518", "submitter": "Ying Huang", "authors": "Ying Huang, Shangfeng Qiu, Wenwei Zhang, Xianghui Luo, Jinzhuo Wang", "title": "More Information Supervised Probabilistic Deep Face Embedding Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researches using margin based comparison loss demonstrate the effectiveness\nof penalizing the distance between face feature and their corresponding class\ncenters. Despite their popularity and excellent performance, they do not\nexplicitly encourage the generic embedding learning for an open set recognition\nproblem. In this paper, we analyse margin based softmax loss in probability\nview. With this perspective, we propose two general principles: 1) monotonic\ndecreasing and 2) margin probability penalty, for designing new margin loss\nfunctions. Unlike methods optimized with single comparison metric, we provide a\nnew perspective to treat open set face recognition as a problem of information\ntransmission. And the generalization capability for face embedding is gained\nwith more clean information. An auto-encoder architecture called\nLinear-Auto-TS-Encoder(LATSE) is proposed to corroborate this finding.\nExtensive experiments on several benchmarks demonstrate that LATSE help face\nembedding to gain more generalization capability and it boosted the single\nmodel performance with open training dataset to more than $99\\%$ on MegaFace\ntest.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 12:33:32 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 02:25:56 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Huang", "Ying", ""], ["Qiu", "Shangfeng", ""], ["Zhang", "Wenwei", ""], ["Luo", "Xianghui", ""], ["Wang", "Jinzhuo", ""]]}, {"id": "2006.04523", "submitter": "Zheng Dang", "authors": "Zheng Dang, Fei Wang and Mathieu Salzmann", "title": "Learning 3D-3D Correspondences for One-shot Partial-to-partial\n  Registration", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  While 3D-3D registration is traditionally tacked by optimization-based\nmethods, recent work has shown that learning-based techniques could achieve\nfaster and more robust results. In this context, however, only PRNet can handle\nthe partial-to-partial registration scenario. Unfortunately, this is achieved\nat the cost of relying on an iterative procedure, with a complex network\narchitecture. Here, we show that learning-based partial-to-partial registration\ncan be achieved in a one-shot manner, jointly reducing network complexity and\nincreasing registration accuracy. To this end, we propose an Optimal Transport\nlayer able to account for occluded points thanks to the use of outlier bins.\nThe resulting OPRNet framework outperforms the state of the art on standard\nbenchmarks, demonstrating better robustness and generalization ability than\nexisting techniques.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 12:35:47 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 15:57:22 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Dang", "Zheng", ""], ["Wang", "Fei", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2006.04552", "submitter": "Max Frei", "authors": "Max Frei, Frank Einar Kruis", "title": "FibeR-CNN: Expanding Mask R-CNN to Improve Image-Based Fiber Analysis", "comments": "21 pages, 31 figures, 5 tables, 1 algorithm", "journal-ref": "Powder Technology, vol. 377, pp. 974-991", "doi": "10.1016/j.powtec.2020.08.034", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fiber-shaped materials (e.g. carbon nano tubes) are of great relevance, due\nto their unique properties but also the health risk they can impose.\nUnfortunately, image-based analysis of fibers still involves manual annotation,\nwhich is a time-consuming and costly process. We therefore propose the use of\nregion-based convolutional neural networks (R-CNNs) to automate this task. Mask\nR-CNN, the most widely used R-CNN for semantic segmentation tasks, is prone to\nerrors when it comes to the analysis of fiber-shaped objects. Hence, a new\narchitecture - FibeR-CNN - is introduced and validated. FibeR-CNN combines two\nestablished R-CNN architectures (Mask and Keypoint R-CNN) and adds additional\nnetwork heads for the prediction of fiber widths and lengths. As a result,\nFibeR-CNN is able to surpass the mean average precision of Mask R-CNN by 33 %\n(11 percentage points) on a novel test data set of fiber images.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 13:03:09 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 08:45:14 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Frei", "Max", ""], ["Kruis", "Frank Einar", ""]]}, {"id": "2006.04569", "submitter": "Zhedong Zheng", "authors": "Zhedong Zheng, Yi Yang", "title": "Parameter-Efficient Person Re-identification in the 3D Space", "comments": "The code is available at https://github.com/layumi/person-reid-3d", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People live in a 3D world. However, existing works on person\nre-identification (re-id) mostly consider the semantic representation learning\nin a 2D space, intrinsically limiting the understanding of people. In this\nwork, we address this limitation by exploring the prior knowledge of the 3D\nbody structure. Specifically, we project 2D images to a 3D space and introduce\na novel parameter-efficient Omni-scale Graph Network (OG-Net) to learn the\npedestrian representation directly from 3D point clouds. OG-Net effectively\nexploits the local information provided by sparse 3D points and takes advantage\nof the structure and appearance information in a coherent manner. With the help\nof 3D geometry information, we can learn a new type of deep re-id feature free\nfrom noisy variants, such as scale and viewpoint. To our knowledge, we are\namong the first attempts to conduct person re-identification in the 3D space.\nWe demonstrate through extensive experiments that the proposed method (1) eases\nthe matching difficulty in the traditional 2D space, (2) exploits the\ncomplementary information of 2D appearance and 3D structure, (3) achieves\ncompetitive results with limited parameters on four large-scale person re-id\ndatasets, and (4) has good scalability to unseen datasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 13:20:33 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 08:59:48 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Zheng", "Zhedong", ""], ["Yang", "Yi", ""]]}, {"id": "2006.04570", "submitter": "Vijay Pandey", "authors": "Vijay Pandey, Shashi Bhushan Jha", "title": "Incorporating Image Gradients as Secondary Input Associated with Input\n  Image to Improve the Performance of the CNN Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  CNN is very popular neural network architecture in modern days. It is\nprimarily most used tool for vision related task to extract the important\nfeatures from the given image. Moreover, CNN works as a filter to extract the\nimportant features using convolutional operation in distinct layers. In\nexisting CNN architectures, to train the network on given input, only single\nform of given input is fed to the network. In this paper, new architecture has\nbeen proposed where given input is passed in more than one form to the network\nsimultaneously by sharing the layers with both forms of input. We incorporate\nimage gradient as second form of the input associated with the original input\nimage and allowing both inputs to flow in the network using same number of\nparameters to improve the performance of the model for better generalization.\nThe results of the proposed CNN architecture, applying on diverse set of\ndatasets such as MNIST, CIFAR10 and CIFAR100 show superior result compared to\nthe benchmark CNN architecture considering inputs in single form.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 14:01:52 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Pandey", "Vijay", ""], ["Jha", "Shashi Bhushan", ""]]}, {"id": "2006.04603", "submitter": "Alberto Signoroni", "authors": "Alberto Signoroni, Mattia Savardi, Sergio Benini, Nicola Adami,\n  Riccardo Leonardi, Paolo Gibellini, Filippo Vaccher, Marco Ravanelli, Andrea\n  Borghesi, Roberto Maroldi, Davide Farina (University of Brescia)", "title": "BS-Net: learning COVID-19 pneumonia severity on a large Chest X-Ray\n  dataset", "comments": "28 pages, 11 figures, preprint of accepted paper to Medical Image\n  Analysis, Project page with Code and Dataset Available at\n  https://brixia.github.io/", "journal-ref": null, "doi": "10.1016/j.media.2021.102046", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work we design an end-to-end deep learning architecture for\npredicting, on Chest X-rays images (CXR), a multi-regional score conveying the\ndegree of lung compromise in COVID-19 patients. Such semi-quantitative scoring\nsystem, namely Brixia~score, is applied in serial monitoring of such patients,\nshowing significant prognostic value, in one of the hospitals that experienced\none of the highest pandemic peaks in Italy. To solve such a challenging visual\ntask, we adopt a weakly supervised learning strategy structured to handle\ndifferent tasks (segmentation, spatial alignment, and score estimation) trained\nwith a \"from-the-part-to-the-whole\" procedure involving different datasets. In\nparticular, we exploit a clinical dataset of almost 5,000 CXR annotated images\ncollected in the same hospital. Our BS-Net demonstrates self-attentive behavior\nand a high degree of accuracy in all processing stages. Through inter-rater\nagreement tests and a gold standard comparison, we show that our solution\noutperforms single human annotators in rating accuracy and consistency, thus\nsupporting the possibility of using this tool in contexts of computer-assisted\nmonitoring. Highly resolved (super-pixel level) explainability maps are also\ngenerated, with an original technique, to visually help the understanding of\nthe network activity on the lung areas. We also consider other scores proposed\nin literature and provide a comparison with a recently proposed non-specific\napproach. We eventually test the performance robustness of our model on an\nassorted public COVID-19 dataset, for which we also provide Brixia~score\nannotations, observing good direct generalization and fine-tuning capabilities\nthat highlight the portability of BS-Net in other clinical settings. The CXR\ndataset along with the source code and the trained model are publicly released\nfor research purposes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 13:55:58 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 15:06:57 GMT"}, {"version": "v3", "created": "Sat, 3 Apr 2021 08:44:53 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Signoroni", "Alberto", "", "University of Brescia"], ["Savardi", "Mattia", "", "University of Brescia"], ["Benini", "Sergio", "", "University of Brescia"], ["Adami", "Nicola", "", "University of Brescia"], ["Leonardi", "Riccardo", "", "University of Brescia"], ["Gibellini", "Paolo", "", "University of Brescia"], ["Vaccher", "Filippo", "", "University of Brescia"], ["Ravanelli", "Marco", "", "University of Brescia"], ["Borghesi", "Andrea", "", "University of Brescia"], ["Maroldi", "Roberto", "", "University of Brescia"], ["Farina", "Davide", "", "University of Brescia"]]}, {"id": "2006.04604", "submitter": "Hyeongju Kim", "authors": "Hyeongju Kim, Hyeonseung Lee, Woo Hyun Kang, Joun Yeop Lee, Nam Soo\n  Kim", "title": "SoftFlow: Probabilistic Framework for Normalizing Flow on Manifolds", "comments": "17 pages, 15figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow-based generative models are composed of invertible transformations\nbetween two random variables of the same dimension. Therefore, flow-based\nmodels cannot be adequately trained if the dimension of the data distribution\ndoes not match that of the underlying target distribution. In this paper, we\npropose SoftFlow, a probabilistic framework for training normalizing flows on\nmanifolds. To sidestep the dimension mismatch problem, SoftFlow estimates a\nconditional distribution of the perturbed input data instead of learning the\ndata distribution directly. We experimentally show that SoftFlow can capture\nthe innate structure of the manifold data and generate high-quality samples\nunlike the conventional flow-based models. Furthermore, we apply the proposed\nframework to 3D point clouds to alleviate the difficulty of forming thin\nstructures for flow-based models. The proposed model for 3D point clouds,\nnamely SoftPointFlow, can estimate the distribution of various shapes more\naccurately and achieves state-of-the-art performance in point cloud generation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 13:56:07 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 18:22:03 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 09:53:56 GMT"}, {"version": "v4", "created": "Sun, 15 Nov 2020 11:18:29 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Kim", "Hyeongju", ""], ["Lee", "Hyeonseung", ""], ["Kang", "Woo Hyun", ""], ["Lee", "Joun Yeop", ""], ["Kim", "Nam Soo", ""]]}, {"id": "2006.04647", "submitter": "Elliot J. Crowley", "authors": "Joseph Mellor, Jack Turner, Amos Storkey, Elliot J. Crowley", "title": "Neural Architecture Search without Training", "comments": "Accepted at ICML 2021 for a long presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The time and effort involved in hand-designing deep neural networks is\nimmense. This has prompted the development of Neural Architecture Search (NAS)\ntechniques to automate this design. However, NAS algorithms tend to be slow and\nexpensive; they need to train vast numbers of candidate networks to inform the\nsearch process. This could be alleviated if we could partially predict a\nnetwork's trained accuracy from its initial state. In this work, we examine the\noverlap of activations between datapoints in untrained networks and motivate\nhow this can give a measure which is usefully indicative of a network's trained\nperformance. We incorporate this measure into a simple algorithm that allows us\nto search for powerful networks without any training in a matter of seconds on\na single GPU, and verify its effectiveness on NAS-Bench-101, NAS-Bench-201,\nNATS-Bench, and Network Design Spaces. Our approach can be readily combined\nwith more expensive search methods; we examine a simple adaptation of\nregularised evolutionary search. Code for reproducing our experiments is\navailable at https://github.com/BayesWatch/nas-without-training.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 14:53:56 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 11:36:56 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 14:31:02 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Mellor", "Joseph", ""], ["Turner", "Jack", ""], ["Storkey", "Amos", ""], ["Crowley", "Elliot J.", ""]]}, {"id": "2006.04648", "submitter": "Yang Hu Dr.", "authors": "Yang Hu, Guihua Wen, Adriane Chapman, Pei Yang, Mingnan Luo, Yingxue\n  Xu, Dan Dai, Wendy Hall", "title": "Graph-based Visual-Semantic Entanglement Network for Zero-shot Image\n  Recognition", "comments": "15 pages, 11 figures, on IEEE Transactions on Multimedia", "journal-ref": "[J]. IEEE Transactions on Multimedia, 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Zero-shot learning uses semantic attributes to connect the search space of\nunseen objects. In recent years, although the deep convolutional network brings\npowerful visual modeling capabilities to the ZSL task, its visual features have\nsevere pattern inertia and lack of representation of semantic relationships,\nwhich leads to severe bias and ambiguity. In response to this, we propose the\nGraph-based Visual-Semantic Entanglement Network to conduct graph modeling of\nvisual features, which is mapped to semantic attributes by using a knowledge\ngraph, it contains several novel designs: 1. it establishes a multi-path\nentangled network with the convolutional neural network (CNN) and the graph\nconvolutional network (GCN), which input the visual features from CNN to GCN to\nmodel the implicit semantic relations, then GCN feedback the graph modeled\ninformation to CNN features; 2. it uses attribute word vectors as the target\nfor the graph semantic modeling of GCN, which forms a self-consistent\nregression for graph modeling and supervise GCN to learn more personalized\nattribute relations; 3. it fuses and supplements the hierarchical\nvisual-semantic features refined by graph modeling into visual embedding. Our\nmethod outperforms state-of-the-art approaches on multiple representative ZSL\ndatasets: AwA2, CUB, and SUN by promoting the semantic linkage modelling of\nvisual features.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 14:54:08 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 01:21:22 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hu", "Yang", ""], ["Wen", "Guihua", ""], ["Chapman", "Adriane", ""], ["Yang", "Pei", ""], ["Luo", "Mingnan", ""], ["Xu", "Yingxue", ""], ["Dai", "Dan", ""], ["Hall", "Wendy", ""]]}, {"id": "2006.04691", "submitter": "Yinbo Liu", "authors": "Yin-Bo Liu, Ming Zeng, Qing-Hao Meng", "title": "Unstructured Road Vanishing Point Detection Using the Convolutional\n  Neural Network and Heatmap Regression", "comments": "8 pages, 6 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unstructured road vanishing point (VP) detection is a challenging problem,\nespecially in the field of autonomous driving. In this paper, we proposed a\nnovel solution combining the convolutional neural network (CNN) and heatmap\nregression to detect unstructured road VP. The proposed algorithm firstly\nadopts a lightweight backbone, i.e., depthwise convolution modified HRNet, to\nextract hierarchical features of the unstructured road image. Then, three\nadvanced strategies, i.e., multi-scale supervised learning, heatmap\nsuper-resolution, and coordinate regression techniques are utilized to achieve\nfast and high-precision unstructured road VP detection. The empirical results\non Kong's dataset show that our proposed approach enjoys the highest detection\naccuracy compared with state-of-the-art methods under various conditions in\nreal-time, achieving the highest speed of 33 fps.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 15:44:37 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Liu", "Yin-Bo", ""], ["Zeng", "Ming", ""], ["Meng", "Qing-Hao", ""]]}, {"id": "2006.04700", "submitter": "Osama Makansi", "authors": "Osama Makansi, \\\"Ozg\\\"un Cicek, Kevin Buchicchio, Thomas Brox", "title": "Multimodal Future Localization and Emergence Prediction for Objects in\n  Egocentric View with a Reachability Prior", "comments": "In CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of anticipating future dynamics,\nparticularly the future location of other vehicles and pedestrians, in the view\nof a moving vehicle. We approach two fundamental challenges: (1) the partial\nvisibility due to the egocentric view with a single RGB camera and considerable\nfield-of-view change due to the egomotion of the vehicle; (2) the multimodality\nof the distribution of future states. In contrast to many previous works, we do\nnot assume structural knowledge from maps. We rather estimate a reachability\nprior for certain classes of objects from the semantic map of the present image\nand propagate it into the future using the planned egomotion. Experiments show\nthat the reachability prior combined with multi-hypotheses learning improves\nmultimodal prediction of the future location of tracked objects and, for the\nfirst time, the emergence of new objects. We also demonstrate promising\nzero-shot transfer to unseen datasets. Source code is available at\n$\\href{https://github.com/lmb-freiburg/FLN-EPN-RPN}{\\text{this https URL.}}$\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 15:57:26 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Makansi", "Osama", ""], ["Cicek", "\u00d6zg\u00fcn", ""], ["Buchicchio", "Kevin", ""], ["Brox", "Thomas", ""]]}, {"id": "2006.04719", "submitter": "Xi Li", "authors": "Xuewei Li, Songyuan Li, Bourahla Omar, Fei Wu, and Xi Li", "title": "ResKD: Residual-Guided Knowledge Distillation", "comments": "The first two authors (Xuewei Li and Songyuan Li) contribute equally.\n  Accepted to IEEE TRANSACTIONS ON IMAGE PROCESSING (TIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation, aimed at transferring the knowledge from a heavy\nteacher network to a lightweight student network, has emerged as a promising\ntechnique for compressing neural networks. However, due to the capacity gap\nbetween the heavy teacher and the lightweight student, there still exists a\nsignificant performance gap between them. In this paper, we see knowledge\ndistillation in a fresh light, using the knowledge gap, or the residual,\nbetween a teacher and a student as guidance to train a much more lightweight\nstudent, called a res-student. We combine the student and the res-student into\na new student, where the res-student rectifies the errors of the former\nstudent. Such a residual-guided process can be repeated until the user strikes\nthe balance between accuracy and cost. At inference time, we propose a\nsample-adaptive strategy to decide which res-students are not necessary for\neach sample, which can save computational cost. Experimental results show that\nwe achieve competitive performance with 18.04$\\%$, 23.14$\\%$, 53.59$\\%$, and\n56.86$\\%$ of the teachers' computational costs on the CIFAR-10, CIFAR-100,\nTiny-ImageNet, and ImageNet datasets. Finally, we do thorough theoretical and\nempirical analysis for our method.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 16:18:45 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 11:39:18 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 17:21:09 GMT"}, {"version": "v4", "created": "Tue, 9 Mar 2021 03:35:20 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Li", "Xuewei", ""], ["Li", "Songyuan", ""], ["Omar", "Bourahla", ""], ["Wu", "Fei", ""], ["Li", "Xi", ""]]}, {"id": "2006.04725", "submitter": "Chen Qin", "authors": "Chen Qin, Shuo Wang, Chen Chen, Huaqi Qiu, Wenjia Bai and Daniel\n  Rueckert", "title": "Biomechanics-informed Neural Networks for Myocardial Motion Tracking in\n  MRI", "comments": "The paper is early accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration is an ill-posed inverse problem which often requires\nregularisation on the solution space. In contrast to most of the current\napproaches which impose explicit regularisation terms such as smoothness, in\nthis paper we propose a novel method that can implicitly learn\nbiomechanics-informed regularisation. Such an approach can incorporate\napplication-specific prior knowledge into deep learning based registration.\nParticularly, the proposed biomechanics-informed regularisation leverages a\nvariational autoencoder (VAE) to learn a manifold for biomechanically plausible\ndeformations and to implicitly capture their underlying properties via\nreconstructing biomechanical simulations. The learnt VAE regulariser then can\nbe coupled with any deep learning based registration network to regularise the\nsolution space to be biomechanically plausible. The proposed method is\nvalidated in the context of myocardial motion tracking on 2D stacks of cardiac\nMRI data from two different datasets. The results show that it can achieve\nbetter performance against other competing methods in terms of motion tracking\naccuracy and has the ability to learn biomechanical properties such as\nincompressibility and strains. The method has also been shown to have better\ngeneralisability to unseen domains compared with commonly used L2\nregularisation schemes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 16:29:13 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 11:24:17 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 09:42:11 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Qin", "Chen", ""], ["Wang", "Shuo", ""], ["Chen", "Chen", ""], ["Qiu", "Huaqi", ""], ["Bai", "Wenjia", ""], ["Rueckert", "Daniel", ""]]}, {"id": "2006.04737", "submitter": "Jiabo Huang", "authors": "Jiabo Huang and Shaogang Gong", "title": "Unsupervised Transfer Learning with Self-Supervised Remedy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalising deep networks to novel domains without manual labels is\nchallenging to deep learning. This problem is intrinsically difficult due to\nunpredictable changing nature of imagery data distributions in novel domains.\nPre-learned knowledge does not transfer well without making strong assumptions\nabout the learned and the novel domains. Different methods have been studied to\naddress the underlying problem based on different assumptions, e.g. from domain\nadaptation to zero-shot and few-shot learning. In this work, we address this\nproblem by transfer clustering that aims to learn a discriminative latent space\nof the unlabelled target data in a novel domain by knowledge transfer from\nlabelled related domains. Specifically, we want to leverage relative (pairwise)\nimagery information, which is freely available and intrinsic to a target\ndomain, to model the target domain image distribution characteristics as well\nas the prior-knowledge learned from related labelled domains to enable more\ndiscriminative clustering of unlabelled target data. Our method mitigates\nnontransferrable prior-knowledge by self-supervision, benefiting from both\ntransfer and self-supervised learning. Extensive experiments on four datasets\nfor image clustering tasks reveal the superiority of our model over the\nstate-of-the-art transfer clustering techniques. We further demonstrate its\ncompetitive transferability on four zero-shot learning benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 16:42:17 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Huang", "Jiabo", ""], ["Gong", "Shaogang", ""]]}, {"id": "2006.04767", "submitter": "Elena Corina Grigore", "authors": "Freddy A. Boulton and Elena Corina Grigore and Eric M. Wolff", "title": "Motion Prediction using Trajectory Sets and Self-Driving Domain\n  Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future motion of vehicles has been studied using various\ntechniques, including stochastic policies, generative models, and regression.\nRecent work has shown that classification over a trajectory set, which\napproximates possible motions, achieves state-of-the-art performance and avoids\nissues like mode collapse. However, map information and the physical\nrelationships between nearby trajectories is not fully exploited in this\nformulation. We build on classification-based approaches to motion prediction\nby adding an auxiliary loss that penalizes off-road predictions. This auxiliary\nloss can easily be pretrained using only map information (e.g., off-road area),\nwhich significantly improves performance on small datasets. We also investigate\nweighted cross-entropy losses to capture spatial-temporal relationships among\ntrajectories. Our final contribution is a detailed comparison of classification\nand ordinal regression on two public self-driving datasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 17:37:15 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 20:41:54 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Boulton", "Freddy A.", ""], ["Grigore", "Elena Corina", ""], ["Wolff", "Eric M.", ""]]}, {"id": "2006.04859", "submitter": "Suryansh Saxena", "authors": "Suryansh Saxena and Isaac K Isukapati", "title": "Novel Perception Algorithmic Framework For Object Identification and\n  Tracking In Autonomous Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel perception framework that has the ability to\nidentify and track objects in autonomous vehicle's field of view. The proposed\nalgorithms don't require any training for achieving this goal. The framework\nmakes use of ego-vehicle's pose estimation and a KD-Tree-based segmentation\nalgorithm to generate object clusters. In turn, using a VFH technique, the\ngeometry of each identified object cluster is translated into a multi-modal PDF\nand a motion model is initiated with every new object cluster for the purpose\nof robust spatio-temporal tracking. The methodology further uses statistical\nproperties of high-dimensional probability density functions and Bayesian\nmotion model estimates to identify and track objects from frame to frame. The\neffectiveness of the methodology is tested on a KITTI dataset. The results show\nthat the median tracking accuracy is around 91% with an end-to-end\ncomputational time of 153 milliseconds\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 18:21:40 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Saxena", "Suryansh", ""], ["Isukapati", "Isaac K", ""]]}, {"id": "2006.04868", "submitter": "Debesh Jha", "authors": "Debesh Jha, Michael A. Riegler, Dag Johansen, P{\\aa}l Halvorsen,\n  H{\\aa}vard D. Johansen", "title": "DoubleU-Net: A Deep Convolutional Neural Network for Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image segmentation is the process of labeling each pixel of an image\nwith its corresponding class. An encoder-decoder based approach, like U-Net and\nits variants, is a popular strategy for solving medical image segmentation\ntasks. To improve the performance of U-Net on various segmentation tasks, we\npropose a novel architecture called DoubleU-Net, which is a combination of two\nU-Net architectures stacked on top of each other. The first U-Net uses a\npre-trained VGG-19 as the encoder, which has already learned features from\nImageNet and can be transferred to another task easily. To capture more\nsemantic information efficiently, we added another U-Net at the bottom. We also\nadopt Atrous Spatial Pyramid Pooling (ASPP) to capture contextual information\nwithin the network. We have evaluated DoubleU-Net using four medical\nsegmentation datasets, covering various imaging modalities such as colonoscopy,\ndermoscopy, and microscopy. Experiments on the MICCAI 2015 segmentation\nchallenge, the CVC-ClinicDB, the 2018 Data Science Bowl challenge, and the\nLesion boundary segmentation datasets demonstrate that the DoubleU-Net\noutperforms U-Net and the baseline models. Moreover, DoubleU-Net produces more\naccurate segmentation masks, especially in the case of the CVC-ClinicDB and\nMICCAI 2015 segmentation challenge datasets, which have challenging images such\nas smaller and flat polyps. These results show the improvement over the\nexisting U-Net model. The encouraging results, produced on various medical\nimage segmentation datasets, show that DoubleU-Net can be used as a strong\nbaseline for both medical image segmentation and cross-dataset evaluation\ntesting to measure the generalizability of Deep Learning (DL) models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 18:38:24 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 15:40:40 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Jha", "Debesh", ""], ["Riegler", "Michael A.", ""], ["Johansen", "Dag", ""], ["Halvorsen", "P\u00e5l", ""], ["Johansen", "H\u00e5vard D.", ""]]}, {"id": "2006.04874", "submitter": "Jane Wu", "authors": "Jane Wu, Zhenglin Geng, Hui Zhou, Ronald Fedkiw", "title": "Skinning a Parameterization of Three-Dimensional Space for Neural\n  Network Cloth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel learning framework for cloth deformation by embedding\nvirtual cloth into a tetrahedral mesh that parametrizes the volumetric region\nof air surrounding the underlying body. In order to maintain this volumetric\nparameterization during character animation, the tetrahedral mesh is\nconstrained to follow the body surface as it deforms. We embed the cloth mesh\nvertices into this parameterization of three-dimensional space in order to\nautomatically capture much of the nonlinear deformation due to both joint\nrotations and collisions. We then train a convolutional neural network to\nrecover ground truth deformation by learning cloth embedding offsets for each\nskeletal pose. Our experiments show significant improvement over learning cloth\noffsets from body surface parameterizations, both quantitatively and visually,\nwith prior state of the art having a mean error five standard deviations higher\nthan ours. Moreover, our results demonstrate the efficacy of a general learning\nparadigm where high-frequency details can be embedded into low-frequency\nparameterizations.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 18:53:03 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Wu", "Jane", ""], ["Geng", "Zhenglin", ""], ["Zhou", "Hui", ""], ["Fedkiw", "Ronald", ""]]}, {"id": "2006.04878", "submitter": "Valanarasu Jeya Maria Jose", "authors": "Jeya Maria Jose, Vishwanath Sindagi, Ilker Hacihaliloglu, Vishal M.\n  Patel", "title": "KiU-Net: Towards Accurate Segmentation of Biomedical Images using\n  Over-complete Representations", "comments": "Accepted at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its excellent performance, U-Net is the most widely used backbone\narchitecture for biomedical image segmentation in the recent years. However, in\nour studies, we observe that there is a considerable performance drop in the\ncase of detecting smaller anatomical landmarks with blurred noisy boundaries.\nWe analyze this issue in detail, and address it by proposing an over-complete\narchitecture (Ki-Net) which involves projecting the data onto higher dimensions\n(in the spatial sense). This network, when augmented with U-Net, results in\nsignificant improvements in the case of segmenting small anatomical landmarks\nand blurred noisy boundaries while obtaining better overall performance.\nFurthermore, the proposed network has additional benefits like faster\nconvergence and fewer number of parameters. We evaluate the proposed method on\nthe task of brain anatomy segmentation from 2D Ultrasound (US) of preterm\nneonates, and achieve an improvement of around 4% in terms of the DICE accuracy\nand Jaccard index as compared to the standard-U-Net, while outperforming the\nrecent best methods by 2%. Code:\nhttps://github.com/jeya-maria-jose/KiU-Net-pytorch .\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 18:59:24 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 21:20:48 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Jose", "Jeya Maria", ""], ["Sindagi", "Vishwanath", ""], ["Hacihaliloglu", "Ilker", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2006.04894", "submitter": "David Paz", "authors": "David Paz, Hengyuan Zhang, Qinru Li, Hao Xiang, Henrik Christensen", "title": "Probabilistic Semantic Mapping for Urban Autonomous Driving Applications", "comments": "6 pages, 7 figures, IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in statistical learning and computational abilities have\nenabled autonomous vehicle technology to develop at a much faster rate. While\nmany of the architectures previously introduced are capable of operating under\nhighly dynamic environments, many of these are constrained to smaller-scale\ndeployments, require constant maintenance due to the associated scalability\ncost with high-definition (HD) maps, and involve tedious manual labeling. As an\nattempt to tackle this problem, we propose to fuse image and pre-built point\ncloud map information to perform automatic and accurate labeling of static\nlandmarks such as roads, sidewalks, crosswalks, and lanes. The method performs\nsemantic segmentation on 2D images, associates the semantic labels with point\ncloud maps to accurately localize them in the world, and leverages the\nconfusion matrix formulation to construct a probabilistic semantic map in\nbird's eye view from semantic point clouds. Experiments from data collected in\nan urban environment show that this model is able to predict most road features\nand can be extended for automatically incorporating road features into HD maps\nwith potential future work directions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 19:29:09 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 17:29:49 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Paz", "David", ""], ["Zhang", "Hengyuan", ""], ["Li", "Qinru", ""], ["Xiang", "Hao", ""], ["Christensen", "Henrik", ""]]}, {"id": "2006.04898", "submitter": "Markus Knoche", "authors": "Markus Knoche, Istv\\'an S\\'ar\\'andi, Bastian Leibe", "title": "Reposing Humans by Warping 3D Features", "comments": "Accepted at CVPR 2020 Workshop on Human-Centric Image/Video Synthesis", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR) Workshops, 2020, pp. 1044-1045", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of reposing an image of a human into any desired novel\npose. This conditional image-generation task requires reasoning about the 3D\nstructure of the human, including self-occluded body parts. Most prior works\nare either based on 2D representations or require fitting and manipulating an\nexplicit 3D body mesh. Based on the recent success in deep learning-based\nvolumetric representations, we propose to implicitly learn a dense feature\nvolume from human images, which lends itself to simple and intuitive\nmanipulation through explicit geometric warping. Once the latent feature volume\nis warped according to the desired pose change, the volume is mapped back to\nRGB space by a convolutional decoder. Our state-of-the-art results on the\nDeepFashion and the iPER benchmarks indicate that dense volumetric human\nrepresentations are worth investigating in more detail.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 19:31:02 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Knoche", "Markus", ""], ["S\u00e1r\u00e1ndi", "Istv\u00e1n", ""], ["Leibe", "Bastian", ""]]}, {"id": "2006.04902", "submitter": "Rico Jonschkowski", "authors": "Rico Jonschkowski, Austin Stone, Jonathan T. Barron, Ariel Gordon,\n  Kurt Konolige, Anelia Angelova", "title": "What Matters in Unsupervised Optical Flow", "comments": "Accepted at ECCV 2020 (Oral). Source code is available at\n  https://github.com/google-research/google-research/tree/master/uflow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We systematically compare and analyze a set of key components in unsupervised\noptical flow to identify which photometric loss, occlusion handling, and\nsmoothness regularization is most effective. Alongside this investigation we\nconstruct a number of novel improvements to unsupervised flow models, such as\ncost volume normalization, stopping the gradient at the occlusion mask,\nencouraging smoothness before upsampling the flow field, and continual\nself-supervision with image resizing. By combining the results of our\ninvestigation with our improved model components, we are able to present a new\nunsupervised flow technique that significantly outperforms the previous\nunsupervised state-of-the-art and performs on par with supervised FlowNet2 on\nthe KITTI 2015 dataset, while also being significantly simpler than related\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 19:36:26 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 13:39:34 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Jonschkowski", "Rico", ""], ["Stone", "Austin", ""], ["Barron", "Jonathan T.", ""], ["Gordon", "Ariel", ""], ["Konolige", "Kurt", ""], ["Angelova", "Anelia", ""]]}, {"id": "2006.04924", "submitter": "Muzammal Naseer", "authors": "Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Fatih\n  Porikli", "title": "A Self-supervised Approach for Adversarial Robustness", "comments": "CVPR-2020 (Oral). Code this http\n  https://github.com/Muzammal-Naseer/NRP}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples can cause catastrophic mistakes in Deep Neural Network\n(DNNs) based vision systems e.g., for classification, segmentation and object\ndetection. The vulnerability of DNNs against such attacks can prove a major\nroadblock towards their real-world deployment. Transferability of adversarial\nexamples demand generalizable defenses that can provide cross-task protection.\nAdversarial training that enhances robustness by modifying target model's\nparameters lacks such generalizability. On the other hand, different input\nprocessing based defenses fall short in the face of continuously evolving\nattacks. In this paper, we take the first step to combine the benefits of both\napproaches and propose a self-supervised adversarial training mechanism in the\ninput space. By design, our defense is a generalizable approach and provides\nsignificant robustness against the \\textbf{unseen} adversarial attacks (\\eg by\nreducing the success rate of translation-invariant \\textbf{ensemble} attack\nfrom 82.6\\% to 31.9\\% in comparison to previous state-of-the-art). It can be\ndeployed as a plug-and-play solution to protect a variety of vision systems, as\nwe demonstrate for the case of classification, segmentation and detection. Code\nis available at: {\\small\\url{https://github.com/Muzammal-Naseer/NRP}}.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 20:42:39 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Naseer", "Muzammal", ""], ["Khan", "Salman", ""], ["Hayat", "Munawar", ""], ["Khan", "Fahad Shahbaz", ""], ["Porikli", "Fatih", ""]]}, {"id": "2006.04973", "submitter": "Manikandasriram Srinivasan Ramanagopal", "authors": "Manikandasriram Srinivasan Ramanagopal, Zixu Zhang, Ram Vasudevan,\n  Matthew Johnson-Roberson", "title": "Pixel-Wise Motion Deblurring of Thermal Videos", "comments": "10 pages, 8 figures, Accepted to Robotics: Science and Systems 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncooled microbolometers can enable robots to see in the absence of visible\nillumination by imaging the \"heat\" radiated from the scene. Despite this\nability to see in the dark, these sensors suffer from significant motion blur.\nThis has limited their application on robotic systems. As described in this\npaper, this motion blur arises due to the thermal inertia of each pixel. This\nhas meant that traditional motion deblurring techniques, which rely on\nidentifying an appropriate spatial blur kernel to perform spatial\ndeconvolution, are unable to reliably perform motion deblurring on thermal\ncamera images. To address this problem, this paper formulates reversing the\neffect of thermal inertia at a single pixel as a Least Absolute Shrinkage and\nSelection Operator (LASSO) problem which we can solve rapidly using a quadratic\nprogramming solver. By leveraging sparsity and a high frame rate, this\npixel-wise LASSO formulation is able to recover motion deblurred frames of\nthermal videos without using any spatial information. To compare its quality\nagainst state-of-the-art visible camera based deblurring methods, this paper\nevaluated the performance of a family of pre-trained object detectors on a set\nof images restored by different deblurring algorithms. All evaluated object\ndetectors performed systematically better on images restored by the proposed\nalgorithm rather than any other tested, state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 22:35:12 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Ramanagopal", "Manikandasriram Srinivasan", ""], ["Zhang", "Zixu", ""], ["Vasudevan", "Ram", ""], ["Johnson-Roberson", "Matthew", ""]]}, {"id": "2006.04988", "submitter": "Andrey Voynov", "authors": "Andrey Voynov, Stanislav Morozov, Artem Babenko", "title": "Object Segmentation Without Labels with Large-Scale Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent rise of unsupervised and self-supervised learning has dramatically\nreduced the dependency on labeled data, providing effective image\nrepresentations for transfer to downstream vision tasks. Furthermore, recent\nworks employed these representations in a fully unsupervised setup for image\nclassification, reducing the need for human labels on the fine-tuning stage as\nwell. This work demonstrates that large-scale unsupervised models can also\nperform a more challenging object segmentation task, requiring neither\npixel-level nor image-level labeling. Namely, we show that recent unsupervised\nGANs allow to differentiate between foreground/background pixels, providing\nhigh-quality saliency masks. By extensive comparison on standard benchmarks, we\noutperform existing unsupervised alternatives for object segmentation,\nachieving new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 23:30:43 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 09:49:40 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Voynov", "Andrey", ""], ["Morozov", "Stanislav", ""], ["Babenko", "Artem", ""]]}, {"id": "2006.04991", "submitter": "Zhizheng Zhang", "authors": "Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Zhibo Chen, Shif-Fu Chang", "title": "Rethinking Classification Loss Designs for Person Re-identification with\n  a Unified View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-identification (ReID) aims at matching a person of interest across\nimages. In convolutional neural networks (CNNs) based approaches, loss design\nplays a role of metric learning which guides the feature learning process to\npull closer features of the same identity and to push far apart features of\ndifferent identities. In recent years, the combination of classification loss\nand triplet loss achieves superior performance and is predominant in ReID. In\nthis paper, we rethink these loss functions within a generalized formulation\nand argue that triplet-based optimization can be viewed as a two-class\nsubsampling classification, which performs classification over two sampled\ncategories based on instance similarities. Furthermore, we present a case study\nwhich demonstrates that increasing the number of simultaneously considered\ninstance classes significantly improves the ReID performance, since it is\naligned better with the ReID test/inference process. With the multi-class\nsubsampling classification incorporated, we provide a strong baseline which\nachieves the state-of-the-art performance on the benchmark person ReID\ndatasets. Finally, we propose a new meta prototypical N-tuple loss for more\nefficient multi-class subsampling classification. We aim to inspire more new\nloss designs in the person ReID field.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 23:34:08 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zhang", "Zhizheng", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Chen", "Zhibo", ""], ["Chang", "Shif-Fu", ""]]}, {"id": "2006.04996", "submitter": "Xiang Jiang", "authors": "Xiang Jiang, Qicheng Lao, Stan Matwin, Mohammad Havaei", "title": "Implicit Class-Conditioned Domain Alignment for Unsupervised Domain\n  Adaptation", "comments": "Accepted at ICML2020. For code, see\n  https://github.com/xiangdal/implicit_alignment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for unsupervised domain adaptation---with a strong\nfocus on practical considerations of within-domain class imbalance and\nbetween-domain class distribution shift---from a class-conditioned domain\nalignment perspective. Current methods for class-conditioned domain alignment\naim to explicitly minimize a loss function based on pseudo-label estimations of\nthe target domain. However, these methods suffer from pseudo-label bias in the\nform of error accumulation. We propose a method that removes the need for\nexplicit optimization of model parameters from pseudo-labels directly. Instead,\nwe present a sampling-based implicit alignment approach, where the sample\nselection procedure is implicitly guided by the pseudo-labels. Theoretical\nanalysis reveals the existence of a domain-discriminator shortcut in misaligned\nclasses, which is addressed by the proposed implicit alignment approach to\nfacilitate domain-adversarial learning. Empirical results and ablation studies\nconfirm the effectiveness of the proposed approach, especially in the presence\nof within-domain class imbalance and between-domain class distribution shift.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 00:20:21 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Jiang", "Xiang", ""], ["Lao", "Qicheng", ""], ["Matwin", "Stan", ""], ["Havaei", "Mohammad", ""]]}, {"id": "2006.04998", "submitter": "Shikha Chaganti", "authors": "Eduardo Jose Mortani Barbosa Jr., Bogdan Georgescu, Shikha Chaganti,\n  Gorka Bastarrika Aleman, Jordi Broncano Cabrero, Guillaume Chabin, Thomas\n  Flohr, Philippe Grenier, Sasa Grbic, Nakul Gupta, Fran\\c{c}ois Mellot, Savvas\n  Nicolaou, Thomas Re, Pina Sanelli, Alexander W. Sauter, Youngjin Yoo,\n  Valentin Ziebandt, Dorin Comaniciu", "title": "Machine Learning Automatically Detects COVID-19 using Chest CTs in a\n  Large Multicenter Cohort", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: To investigate machine-learning classifiers and interpretable\nmodels using chest CT for detection of COVID-19 and differentiation from other\npneumonias, ILD and normal CTs.\n  Methods: Our retrospective multi-institutional study obtained 2096 chest CTs\nfrom 16 institutions (including 1077 COVID-19 patients). Training/testing\ncohorts included 927/100 COVID-19, 388/33 ILD, 189/33 other pneumonias, and\n559/34 normal (no pathologies) CTs. A metric-based approach for classification\nof COVID-19 used interpretable features, relying on logistic regression and\nrandom forests. A deep learning-based classifier differentiated COVID-19 via 3D\nfeatures extracted directly from CT attenuation and probability distribution of\nairspace opacities.\n  Results: Most discriminative features of COVID-19 are percentage of airspace\nopacity and peripheral and basal predominant opacities, concordant with the\ntypical characterization of COVID-19 in the literature. Unsupervised\nhierarchical clustering compares feature distribution across COVID-19 and\ncontrol cohorts. The metrics-based classifier achieved AUC=0.83,\nsensitivity=0.74, and specificity=0.79 of versus respectively 0.93, 0.90, and\n0.83 for the DL-based classifier. Most of ambiguity comes from non-COVID-19\npneumonia with manifestations that overlap with COVID-19, as well as mild\nCOVID-19 cases. Non-COVID-19 classification performance is 91% for ILD, 64% for\nother pneumonias and 94% for no pathologies, which demonstrates the robustness\nof our method against different compositions of control groups.\n  Conclusions: Our new method accurately discriminates COVID-19 from other\ntypes of pneumonia, ILD, and no pathologies CTs, using quantitative imaging\nfeatures derived from chest CT, while balancing interpretability of results and\nclassification performance, and therefore may be useful to facilitate diagnosis\nof COVID-19.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 00:40:35 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 13:24:04 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2020 00:53:14 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Barbosa", "Eduardo Jose Mortani", "Jr."], ["Georgescu", "Bogdan", ""], ["Chaganti", "Shikha", ""], ["Aleman", "Gorka Bastarrika", ""], ["Cabrero", "Jordi Broncano", ""], ["Chabin", "Guillaume", ""], ["Flohr", "Thomas", ""], ["Grenier", "Philippe", ""], ["Grbic", "Sasa", ""], ["Gupta", "Nakul", ""], ["Mellot", "Fran\u00e7ois", ""], ["Nicolaou", "Savvas", ""], ["Re", "Thomas", ""], ["Sanelli", "Pina", ""], ["Sauter", "Alexander W.", ""], ["Yoo", "Youngjin", ""], ["Ziebandt", "Valentin", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "2006.05011", "submitter": "Etienne Dubeau", "authors": "Etienne Dubeau, Mathieu Garon, Benoit Debaque, Raoul de Charette,\n  Jean-Fran\\c{c}ois Lalonde", "title": "RGB-D-E: Event Camera Calibration for Fast 6-DOF Object Tracking", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality devices require multiple sensors to perform various tasks\nsuch as localization and tracking. Currently, popular cameras are mostly\nframe-based (e.g. RGB and Depth) which impose a high data bandwidth and power\nusage. With the necessity for low power and more responsive augmented reality\nsystems, using solely frame-based sensors imposes limits to the various\nalgorithms that needs high frequency data from the environement. As such,\nevent-based sensors have become increasingly popular due to their low power,\nbandwidth and latency, as well as their very high frequency data acquisition\ncapabilities. In this paper, we propose, for the first time, to use an\nevent-based camera to increase the speed of 3D object tracking in 6 degrees of\nfreedom. This application requires handling very high object speed to convey\ncompelling AR experiences. To this end, we propose a new system which combines\na recent RGB-D sensor (Kinect Azure) with an event camera (DAVIS346). We\ndevelop a deep learning approach, which combines an existing RGB-D network\nalong with a novel event-based network in a cascade fashion, and demonstrate\nthat our approach significantly improves the robustness of a state-of-the-art\nframe-based 6-DOF object tracker using our RGB-D-E pipeline.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 01:55:48 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 20:41:29 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Dubeau", "Etienne", ""], ["Garon", "Mathieu", ""], ["Debaque", "Benoit", ""], ["de Charette", "Raoul", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "2006.05015", "submitter": "Weixing Liu", "authors": "Weixing Liu, Jun Liu and Bin Luo", "title": "Can Synthetic Data Improve Object Detection Results for Remote Sensing\n  Images?", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches require enough training samples to perform well, but\nit is a challenge to collect enough real training data and label them manually.\nIn this letter, we propose the use of realistic synthetic data with a wide\ndistribution to improve the performance of remote sensing image aircraft\ndetection. Specifically, to increase the variability of synthetic data, we\nrandomly set the parameters during rendering, such as the size of the instance\nand the class of background images. In order to make the synthetic images more\nrealistic, we then refine the synthetic images at the pixel level using\nCycleGAN with real unlabeled images. We also fine-tune the model with a small\namount of real data, to obtain a higher accuracy. Experiments on NWPU VHR-10,\nUCAS-AOD and DIOR datasets demonstrate that the proposed method can be applied\nfor augmenting insufficient real data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 02:23:22 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Liu", "Weixing", ""], ["Liu", "Jun", ""], ["Luo", "Bin", ""]]}, {"id": "2006.05018", "submitter": "Xukun Li", "authors": "Wei Wu, Yu Shi, Xukun Li, Yukun Zhou, Peng Du, Shuangzhi Lv, Tingbo\n  Liang, Jifang Sheng", "title": "Deep learning to estimate the physical proportion of infected region of\n  lung for COVID-19 pneumonia with CT image set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utilizing computed tomography (CT) images to quickly estimate the severity of\ncases with COVID-19 is one of the most straightforward and efficacious methods.\nTwo tasks were studied in this present paper. One was to segment the mask of\nintact lung in case of pneumonia. Another was to generate the masks of regions\ninfected by COVID-19. The masks of these two parts of images then were\nconverted to corresponding volumes to calculate the physical proportion of\ninfected region of lung. A total of 129 CT image set were herein collected and\nstudied. The intrinsic Hounsfiled value of CT images was firstly utilized to\ngenerate the initial dirty version of labeled masks both for intact lung and\ninfected regions. Then, the samples were carefully adjusted and improved by two\nprofessional radiologists to generate the final training set and test\nbenchmark. Two deep learning models were evaluated: UNet and 2.5D UNet. For the\nsegment of infected regions, a deep learning based classifier was followed to\nremove unrelated blur-edged regions that were wrongly segmented out such as air\ntube and blood vessel tissue etc. For the segmented masks of intact lung and\ninfected regions, the best method could achieve 0.972 and 0.757 measure in mean\nDice similarity coefficient on our test benchmark. As the overall proportion of\ninfected region of lung, the final result showed 0.961 (Pearson's correlation\ncoefficient) and 11.7% (mean absolute percent error). The instant proportion of\ninfected regions of lung could be used as a visual evidence to assist clinical\nphysician to determine the severity of the case. Furthermore, a quantified\nreport of infected regions can help predict the prognosis for COVID-19 cases\nwhich were scanned periodically within the treatment cycle.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 02:38:40 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Wu", "Wei", ""], ["Shi", "Yu", ""], ["Li", "Xukun", ""], ["Zhou", "Yukun", ""], ["Du", "Peng", ""], ["Lv", "Shuangzhi", ""], ["Liang", "Tingbo", ""], ["Sheng", "Jifang", ""]]}, {"id": "2006.05030", "submitter": "Mohammad Hamghalam", "authors": "Mohammad Hamghalam, Baiying Lei, Tianfu Wang", "title": "High Tissue Contrast MRI Synthesis Using Multi-Stage Attention-GAN for\n  Glioma Segmentation", "comments": "Will be published in Thirty-Fourth AAAI Conference on Artificial\n  Intelligence (AAAI-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) provides varying tissue contrast images of\ninternal organs based on a strong magnetic field. Despite the non-invasive\nadvantage of MRI in frequent imaging, the low contrast MR images in the target\narea make tissue segmentation a challenging problem. This paper demonstrates\nthe potential benefits of image-to-image translation techniques to generate\nsynthetic high tissue contrast (HTC) images. Notably, we adopt a new cycle\ngenerative adversarial network (CycleGAN) with an attention mechanism to\nincrease the contrast within underlying tissues. The attention block, as well\nas training on HTC images, guides our model to converge on certain tissues. To\nincrease the resolution of HTC images, we employ multi-stage architecture to\nfocus on one particular tissue as a foreground and filter out the irrelevant\nbackground in each stage. This multi-stage structure also alleviates the common\nartifacts of the synthetic images by decreasing the gap between source and\ntarget domains. We show the application of our method for synthesizing HTC\nimages on brain MR scans, including glioma tumor. We also employ HTC MR images\nin both the end-to-end and two-stage segmentation structure to confirm the\neffectiveness of these images. The experiments over three competitive\nsegmentation baselines on BraTS 2018 dataset indicate that incorporating the\nsynthetic HTC images in the multi-modal segmentation framework improves the\naverage Dice scores 0.8%, 0.6%, and 0.5% on the whole tumor, tumor core, and\nenhancing tumor, respectively, while eliminating one real MRI sequence from the\nsegmentation procedure.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 03:21:30 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Hamghalam", "Mohammad", ""], ["Lei", "Baiying", ""], ["Wang", "Tianfu", ""]]}, {"id": "2006.05049", "submitter": "Bo Pang", "authors": "Bo Pang, Deming Zhai, Junjun Jiang, Xianming Liu", "title": "Single Image Deraining via Scale-space Invariant Attention Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image enhancement from degradation of rainy artifacts plays a critical role\nin outdoor visual computing systems. In this paper, we tackle the notion of\nscale that deals with visual changes in appearance of rain steaks with respect\nto the camera. Specifically, we revisit multi-scale representation by\nscale-space theory, and propose to represent the multi-scale correlation in\nconvolutional feature domain, which is more compact and robust than that in\npixel domain. Moreover, to improve the modeling ability of the network, we do\nnot treat the extracted multi-scale features equally, but design a novel\nscale-space invariant attention mechanism to help the network focus on parts of\nthe features. In this way, we summarize the most activated presence of feature\nmaps as the salient features. Extensive experiments results on synthetic and\nreal rainy scenes demonstrate the superior performance of our scheme over the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 04:59:26 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 01:35:10 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Pang", "Bo", ""], ["Zhai", "Deming", ""], ["Jiang", "Junjun", ""], ["Liu", "Xianming", ""]]}, {"id": "2006.05066", "submitter": "Woochul Kang", "authors": "Woochul Kang, Daeyeon Kim", "title": "Deeply Shared Filter Bases for Parameter-Efficient Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern convolutional neural networks (CNNs) have massive identical\nconvolution blocks, and, hence, recursive sharing of parameters across these\nblocks has been proposed to reduce the amount of parameters. However, naive\nsharing of parameters poses many challenges such as limited representational\npower and the vanishing/exploding gradients problem of recursively shared\nparameters. In this paper, we present a recursive convolution block design and\ntraining method, in which a recursively shareable part, or a filter basis, is\nseparated and learned while effectively avoiding the vanishing/exploding\ngradients problem during training. We show that the unwieldy\nvanishing/exploding gradients problem can be controlled by enforcing the\nelements of the filter basis orthonormal, and empirically demonstrate that the\nproposed orthogonality regularization improves the flow of gradients during\ntraining. Experimental results on image classification and object detection\nshow that our approach, unlike previous parameter-sharing approaches, does not\ntrade performance to save parameters and consistently outperforms\noverparameterized counterpart networks. This superior performance demonstrates\nthat the proposed recursive convolution block design and the orthogonality\nregularization not only prevent performance degradation, but also consistently\nimprove the representation capability while a significant amount of parameters\nare recursively shared.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 06:09:42 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 06:04:54 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 02:27:18 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Kang", "Woochul", ""], ["Kim", "Daeyeon", ""]]}, {"id": "2006.05074", "submitter": "Christian Rathgeb", "authors": "Christian Rathgeb, Pawel Drozdowski, Christoph Busch", "title": "Detection of Makeup Presentation Attacks based on Deep Face\n  Representations", "comments": "published at 25th International Conference on Pattern Recognition\n  (ICPR'2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial cosmetics have the ability to substantially alter the facial\nappearance, which can negatively affect the decisions of a face recognition. In\naddition, it was recently shown that the application of makeup can be abused to\nlaunch so-called makeup presentation attacks. In such attacks, the attacker\nmight apply heavy makeup in order to achieve the facial appearance of a target\nsubject for the purpose of impersonation. In this work, we assess the\nvulnerability of a COTS face recognition system to makeup presentation attacks\nemploying the publicly available Makeup Induced Face Spoofing (MIFS) database.\nIt is shown that makeup presentation attacks might seriously impact the\nsecurity of the face recognition system. Further, we propose an attack\ndetection scheme which distinguishes makeup presentation attacks from genuine\nauthentication attempts by analysing differences in deep face representations\nobtained from potential makeup presentation attacks and corresponding target\nface images. The proposed detection system employs a machine learning-based\nclassifier, which is trained with synthetically generated makeup presentation\nattacks utilizing a generative adversarial network for facial makeup transfer\nin conjunction with image warping. Experimental evaluations conducted using the\nMIFS database reveal a detection equal error rate of 0.7% for the task of\nseparating genuine authentication attempts from makeup presentation attacks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 06:53:58 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 11:19:14 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Rathgeb", "Christian", ""], ["Drozdowski", "Pawel", ""], ["Busch", "Christoph", ""]]}, {"id": "2006.05077", "submitter": "Yafei Song", "authors": "Yafei Song, Ling Cai, Jia Li, Yonghong Tian, Mingyang Li", "title": "SEKD: Self-Evolving Keypoint Detection and Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have attempted utilizing deep neural network (DNN) to learn novel\nlocal features from images inspired by its recent successes on a variety of\nvision tasks. However, existing DNN-based algorithms have not achieved such\nremarkable progress that could be partly attributed to insufficient utilization\nof the interactive characters between local feature detector and descriptor. To\nalleviate these difficulties, we emphasize two desired properties, i.e.,\nrepeatability and reliability, to simultaneously summarize the inherent and\ninteractive characters of local feature detector and descriptor. Guided by\nthese properties, a self-supervised framework, namely self-evolving keypoint\ndetection and description (SEKD), is proposed to learn an advanced local\nfeature model from unlabeled natural images. Additionally, to have performance\nguarantees, novel training strategies have also been dedicatedly designed to\nminimize the gap between the learned feature and its properties. We benchmark\nthe proposed method on homography estimation, relative pose estimation, and\nstructure-from-motion tasks. Extensive experimental results demonstrate that\nthe proposed method outperforms popular hand-crafted and DNN-based methods by\nremarkable margins. Ablation studies also verify the effectiveness of each\ncritical training strategy. We will release our code along with the trained\nmodel publicly.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 06:56:50 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Song", "Yafei", ""], ["Cai", "Ling", ""], ["Li", "Jia", ""], ["Tian", "Yonghong", ""], ["Li", "Mingyang", ""]]}, {"id": "2006.05091", "submitter": "Yuecong Xu", "authors": "Yuecong Xu, Haozhi Cao, Jianfei Yang, Kezhi Mao, Jianxiong Yin and\n  Simon See", "title": "PNL: Efficient Long-Range Dependencies Extraction with Pyramid Non-Local\n  Module for Action Recognition", "comments": "Single column, 26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-range spatiotemporal dependencies capturing plays an essential role in\nimproving video features for action recognition. The non-local block inspired\nby the non-local means is designed to address this challenge and have shown\nexcellent performance. However, the non-local block brings significant increase\nin computation cost to the original network. It also lacks the ability to model\nregional correlation in videos. To address the above limitations, we propose\nPyramid Non-Local (PNL) module, which extends the non-local block by\nincorporating regional correlation at multiple scales through a pyramid\nstructured module. This extension upscales the effectiveness of non-local\noperation by attending to the interaction between different regions. Empirical\nresults prove the effectiveness and efficiency of our PNL module, which\nachieves state-of-the-art performance of 83.09% on the Mini-Kinetics dataset,\nwith decreased computation cost compared to the non-local block.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 07:40:23 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Xu", "Yuecong", ""], ["Cao", "Haozhi", ""], ["Yang", "Jianfei", ""], ["Mao", "Kezhi", ""], ["Yin", "Jianxiong", ""], ["See", "Simon", ""]]}, {"id": "2006.05095", "submitter": "Th\\'eo Giraudon", "authors": "Th\\'eo Giraudon, Vincent Gripon, Matthias L\\\"owe, Franck Vermet", "title": "Towards an Intrinsic Definition of Robustness for a Classifier", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robustness of classifiers has become a question of paramount importance\nin the past few years. Indeed, it has been shown that state-of-the-art deep\nlearning architectures can easily be fooled with imperceptible changes to their\ninputs. Therefore, finding good measures of robustness of a trained classifier\nis a key issue in the field. In this paper, we point out that averaging the\nradius of robustness of samples in a validation set is a statistically weak\nmeasure. We propose instead to weight the importance of samples depending on\ntheir difficulty. We motivate the proposed score by a theoretical case study\nusing logistic regression, where we show that the proposed score is independent\nof the choice of the samples it is evaluated upon. We also empirically\ndemonstrate the ability of the proposed score to measure robustness of\nclassifiers with little dependence on the choice of samples in more complex\nsettings, including deep convolutional neural networks and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 07:47:21 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 12:40:07 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Giraudon", "Th\u00e9o", ""], ["Gripon", "Vincent", ""], ["L\u00f6we", "Matthias", ""], ["Vermet", "Franck", ""]]}, {"id": "2006.05097", "submitter": "YueFeng Chen", "authors": "Xiaofeng Mao, Yuefeng Chen, Yuhong Li, Yuan He, Hui Xue", "title": "GAP++: Learning to generate target-conditioned adversarial examples", "comments": "Accepted to IJCAI 2019 AIBS Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are perturbed inputs which can cause a serious threat\nfor machine learning models. Finding these perturbations is such a hard task\nthat we can only use the iterative methods to traverse. For computational\nefficiency, recent works use adversarial generative networks to model the\ndistribution of both the universal or image-dependent perturbations directly.\nHowever, these methods generate perturbations only rely on input images. In\nthis work, we propose a more general-purpose framework which infers\ntarget-conditioned perturbations dependent on both input image and target\nlabel. Different from previous single-target attack models, our model can\nconduct target-conditioned attacks by learning the relations of attack target\nand the semantics in image. Using extensive experiments on the datasets of\nMNIST and CIFAR10, we show that our method achieves superior performance with\nsingle target attack models and obtains high fooling rates with small\nperturbation norms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 07:49:49 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Mao", "Xiaofeng", ""], ["Chen", "Yuefeng", ""], ["Li", "Yuhong", ""], ["He", "Yuan", ""], ["Xue", "Hui", ""]]}, {"id": "2006.05121", "submitter": "Corentin Kervadec", "authors": "Corentin Kervadec (LIRIS), Grigory Antipov (Orange), Moez Baccouche\n  (Orange), Christian Wolf (LIRIS)", "title": "Roses Are Red, Violets Are Blue... but Should Vqa Expect Them To?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for Visual Question Answering (VQA) are notorious for their tendency\nto rely on dataset biases, as the large and unbalanced diversity of questions\nand concepts involved and tends to prevent models from learning to reason,\nleading them to perform educated guesses instead. In this paper, we claim that\nthe standard evaluation metric, which consists in measuring the overall\nin-domain accuracy, is misleading. Since questions and concepts are unbalanced,\nthis tends to favor models which exploit subtle training set statistics.\nAlternatively, naively introducing artificial distribution shifts between train\nand test splits is also not completely satisfying. First, the shifts do not\nreflect real-world tendencies, resulting in unsuitable models; second, since\nthe shifts are handcrafted, trained models are specifically designed for this\nparticular setting, and do not generalize to other configurations. We propose\nthe GQA-OOD benchmark designed to overcome these concerns: we measure and\ncompare accuracy over both rare and frequent question-answer pairs, and argue\nthat the former is better suited to the evaluation of reasoning abilities,\nwhich we experimentally validate with models trained to more or less exploit\nbiases. In a large-scale study involving 7 VQA models and 3 bias reduction\ntechniques, we also experimentally demonstrate that these models fail to\naddress questions involving infrequent concepts and provide recommendations for\nfuture directions of research.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 08:50:39 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 11:19:06 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 14:13:35 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Kervadec", "Corentin", "", "LIRIS"], ["Antipov", "Grigory", "", "Orange"], ["Baccouche", "Moez", "", "Orange"], ["Wolf", "Christian", "", "LIRIS"]]}, {"id": "2006.05123", "submitter": "Georgia Chalvatzaki", "authors": "Georgia Chalvatzaki, Nikolaos Gkanatsios, Petros Maragos, Jan Peters", "title": "Orientation Attentive Robotic Grasp Synthesis with Augmented Grasp Map\n  Representation", "comments": "7 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inherent morphological characteristics in objects may offer a wide range of\nplausible grasping orientations that obfuscates the visual learning of robotic\ngrasping. Existing grasp generation approaches are cursed to construct\ndiscontinuous grasp maps by aggregating annotations for drastically different\norientations per grasping point. Moreover, current methods generate grasp\ncandidates across a single direction in the robot's viewpoint, ignoring its\nfeasibility constraints. In this paper, we propose a novel augmented grasp map\nrepresentation, suitable for pixel-wise synthesis, that locally disentangles\ngrasping orientations by partitioning the angle space into multiple bins.\nFurthermore, we introduce the ORientation AtteNtive Grasp synthEsis (ORANGE)\nframework, that jointly addresses classification into orientation bins and\nangle-value regression. The bin-wise orientation maps further serve as an\nattention mechanism for areas with higher graspability, i.e. probability of\nbeing an actual grasp point. We report new state-of-the-art 94.71% performance\non Jacquard, with a simple U-Net using only depth images, outperforming even\nmulti-modal approaches. Subsequent qualitative results with a real bi-manual\nrobot validate ORANGE's effectiveness in generating grasps for multiple\norientations, hence allowing planning grasps that are feasible.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 08:54:54 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 10:26:55 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Chalvatzaki", "Georgia", ""], ["Gkanatsios", "Nikolaos", ""], ["Maragos", "Petros", ""], ["Peters", "Jan", ""]]}, {"id": "2006.05127", "submitter": "Wenxi Liu", "authors": "Yuzhen Niu, Weifeng Shi, Wenxi Liu, Shengfeng He, Jia Pan, Antoni B.\n  Chan", "title": "Over-crowdedness Alert! Forecasting the Future Crowd Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, vision-based crowd analysis has been studied extensively due\nto its practical applications in real world. In this paper, we formulate a\nnovel crowd analysis problem, in which we aim to predict the crowd distribution\nin the near future given sequential frames of a crowd video without any\nidentity annotations. Studying this research problem will benefit applications\nconcerned with forecasting crowd dynamics. To solve this problem, we propose a\nglobal-residual two-stream recurrent network, which leverages the consecutive\ncrowd video frames as inputs and their corresponding density maps as auxiliary\ninformation to predict the future crowd distribution. Moreover, to strengthen\nthe capability of our network, we synthesize scene-specific crowd density maps\nusing simulated data for pretraining. Finally, we demonstrate that our\nframework is able to predict the crowd distribution for different crowd\nscenarios and we delve into applications including predicting future crowd\ncount, forecasting high-density region, etc.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 08:59:54 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Niu", "Yuzhen", ""], ["Shi", "Weifeng", ""], ["Liu", "Wenxi", ""], ["He", "Shengfeng", ""], ["Pan", "Jia", ""], ["Chan", "Antoni B.", ""]]}, {"id": "2006.05132", "submitter": "Xi Li", "authors": "Abdul Jabbar, Xi Li, and Bourahla Omar", "title": "A Survey on Generative Adversarial Networks: Variants, Applications, and\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Generative Models have gained considerable attention in the field of\nunsupervised learning via a new and practical framework called Generative\nAdversarial Networks (GAN) due to its outstanding data generation capability.\nMany models of GAN have proposed, and several practical applications emerged in\nvarious domains of computer vision and machine learning. Despite GAN's\nexcellent success, there are still obstacles to stable training. The problems\nare due to Nash-equilibrium, internal covariate shift, mode collapse, vanishing\ngradient, and lack of proper evaluation metrics. Therefore, stable training is\na crucial issue in different applications for the success of GAN. Herein, we\nsurvey several training solutions proposed by different researchers to\nstabilize GAN training. We survey, (I) the original GAN model and its modified\nclassical versions, (II) detail analysis of various GAN applications in\ndifferent domains, (III) detail study about the various GAN training obstacles\nas well as training solutions. Finally, we discuss several new issues as well\nas research outlines to the topic.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 09:04:41 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Jabbar", "Abdul", ""], ["Li", "Xi", ""], ["Omar", "Bourahla", ""]]}, {"id": "2006.05142", "submitter": "David Varas", "authors": "Carlos Roig and David Varas and Issey Masuda and Juan Carlos Riveiro\n  and Elisenda Bou-Balust", "title": "Smooth Proxy-Anchor Loss for Noisy Metric Learning", "comments": "The 4th Workshop on Visual Understanding by Learning from Web Data\n  (CVPR 2020)", "journal-ref": "The 4th Workshop on Visual Understanding by Learning from Web Data\n  (CVPR 2020)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many industrial applications use Metric Learning as a way to circumvent\nscalability issues when designing systems with a high number of classes.\nBecause of this, this field of research is attracting a lot of interest from\nthe academic and non-academic communities. Such industrial applications require\nlarge-scale datasets, which are usually generated with web data and, as a\nresult, often contain a high number of noisy labels. While Metric Learning\nsystems are sensitive to noisy labels, this is usually not tackled in the\nliterature, that relies on manually annotated datasets.\n  In this work, we propose a Metric Learning method that is able to overcome\nthe presence of noisy labels using our novel Smooth Proxy-Anchor Loss. We also\npresent an architecture that uses the aforementioned loss with a two-phase\nlearning procedure. First, we train a confidence module that computes sample\nclass confidences. Second, these confidences are used to weight the influence\nof each sample for the training of the embeddings. This results in a system\nthat is able to provide robust sample embeddings.\n  We compare the performance of the described method with current\nstate-of-the-art Metric Learning losses (proxy-based and pair-based), when\ntrained with a dataset containing noisy labels. The results showcase an\nimprovement of 2.63 and 3.29 in Recall@1 with respect to MultiSimilarity and\nProxy-Anchor Loss respectively, proving that our method outperforms the\nstate-of-the-art of Metric Learning in noisy labeling conditions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 09:33:04 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Roig", "Carlos", ""], ["Varas", "David", ""], ["Masuda", "Issey", ""], ["Riveiro", "Juan Carlos", ""], ["Bou-Balust", "Elisenda", ""]]}, {"id": "2006.05159", "submitter": "Albert Dulian", "authors": "Albert Dulian and John C. Murray", "title": "Physically constrained short-term vehicle trajectory forecasting with\n  naive semantic maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban environments manifest a high level of complexity, and therefore it is\nof vital importance for safety systems embedded within autonomous vehicles\n(AVs) to be able to accurately predict the short-term future motion of nearby\nagents. This problem can be further understood as generating a sequence of\nfuture coordinates for a given agent based on its past motion data e.g.\nposition, velocity, acceleration etc, and whilst current approaches demonstrate\nplausible results they have a propensity to neglect a scene's physical\nconstrains. In this paper we propose the model based on a combination of the\nCNN and LSTM encoder-decoder architecture that learns to extract a relevant\nroad features from semantic maps as well as general motion of agents and uses\nthis learned representation to predict their short-term future trajectories. We\ntrain and validate the model on the publicly available dataset that provides\ndata from urban areas, allowing us to examine it in challenging and uncertain\nscenarios. We show that our model is not only capable of anticipating future\nmotion whilst taking into consideration road boundaries, but can also\neffectively and precisely predict trajectories for a longer time horizon than\ninitially trained for.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 09:52:44 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Dulian", "Albert", ""], ["Murray", "John C.", ""]]}, {"id": "2006.05179", "submitter": "Jinkui Hao", "authors": "Jinkui Hao, Huazhu Fu, Yanwu Xu, Yan Hu, Fei Li, Xiulan Zhang, Jiang\n  Liu, Yitian Zhao", "title": "Reconstruction and Quantification of 3D Iris Surface for Angle-Closure\n  Glaucoma Detection in Anterior Segment OCT", "comments": "has been accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise characterization and analysis of iris shape from Anterior Segment OCT\n(AS-OCT) are of great importance in facilitating diagnosis of\nangle-closure-related diseases. Existing methods focus solely on analyzing\nstructural properties identified from the 2D slice, while accurate\ncharacterization of morphological changes of iris shape in 3D AS-OCT may be\nable to reveal in addition the risk of disease progression. In this paper, we\npropose a novel framework for reconstruction and quantification of 3D iris\nsurface from AS-OCT imagery. We consider it to be the first work to detect\nangle-closure glaucoma by means of 3D representation. An iris segmentation\nnetwork with wavelet refinement block (WRB) is first proposed to generate the\ninitial shape of the iris from single AS-OCT slice. The 3D iris surface is then\nreconstructed using a guided optimization method with Poisson-disk sampling.\nFinally, a set of surface-based features are extracted, which are used in\ndetecting of angle-closure glaucoma. Experimental results demonstrate that our\nmethod is highly effective in iris segmentation and surface reconstruction.\nMoreover, we show that 3D-based representation achieves better performance in\nangle-closure glaucoma detection than does 2D-based feature.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 10:56:50 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Hao", "Jinkui", ""], ["Fu", "Huazhu", ""], ["Xu", "Yanwu", ""], ["Hu", "Yan", ""], ["Li", "Fei", ""], ["Zhang", "Xiulan", ""], ["Liu", "Jiang", ""], ["Zhao", "Yitian", ""]]}, {"id": "2006.05180", "submitter": "Naoto Yokoya", "authors": "Naoto Yokoya, Kazuki Yamanoi, Wei He, Gerald Baier, Bruno Adriano,\n  Hiroyuki Miura, Satoru Oishi", "title": "Breaking the Limits of Remote Sensing by Simulation and Deep Learning\n  for Flood and Debris Flow Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework that estimates inundation depth (maximum water level)\nand debris-flow-induced topographic deformation from remote sensing imagery by\nintegrating deep learning and numerical simulation. A water and debris flow\nsimulator generates training data for various artificial disaster scenarios. We\nshow that regression models based on Attention U-Net and LinkNet architectures\ntrained on such synthetic data can predict the maximum water level and\ntopographic deformation from a remote sensing-derived change detection map and\na digital elevation model. The proposed framework has an inpainting capability,\nthus mitigating the false negatives that are inevitable in remote sensing image\nanalysis. Our framework breaks the limits of remote sensing and enables rapid\nestimation of inundation depth and topographic deformation, essential\ninformation for emergency response, including rescue and relief activities. We\nconduct experiments with both synthetic and real data for two disaster events\nthat caused simultaneous flooding and debris flows and demonstrate the\neffectiveness of our approach quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 10:59:15 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Yokoya", "Naoto", ""], ["Yamanoi", "Kazuki", ""], ["He", "Wei", ""], ["Baier", "Gerald", ""], ["Adriano", "Bruno", ""], ["Miura", "Hiroyuki", ""], ["Oishi", "Satoru", ""]]}, {"id": "2006.05181", "submitter": "Miguel de Prado", "authors": "Miguel de Prado, Andrew Mundy, Rabia Saeed, Maurizio Denna, Nuria\n  Pazos and Luca Benini", "title": "Automated Design Space Exploration for optimised Deployment of DNN on\n  Arm Cortex-A CPUs", "comments": null, "journal-ref": null, "doi": "10.1109/TCAD.2020.3046568", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of deep learning on embedded devices has prompted the development\nof numerous methods to optimise the deployment of deep neural networks (DNN).\nWorks have mainly focused on: i) efficient DNN architectures, ii) network\noptimisation techniques such as pruning and quantisation, iii) optimised\nalgorithms to speed up the execution of the most computational intensive layers\nand, iv) dedicated hardware to accelerate the data flow and computation.\nHowever, there is a lack of research on cross-level optimisation as the space\nof approaches becomes too large to test and obtain a globally optimised\nsolution. Thus, leading to suboptimal deployment in terms of latency, accuracy,\nand memory. In this work, we first detail and analyse the methods to improve\nthe deployment of DNNs across the different levels of software optimisation.\nBuilding on this knowledge, we present an automated exploration framework to\nease the deployment of DNNs. The framework relies on a Reinforcement Learning\nsearch that, combined with a deep learning inference framework, automatically\nexplores the design space and learns an optimised solution that speeds up the\nperformance and reduces the memory on embedded CPU platforms. Thus, we present\na set of results for state-of-the-art DNNs on a range of Arm Cortex-A CPU\nplatforms achieving up to 4x improvement in performance and over 2x reduction\nin memory with negligible loss in accuracy with respect to the BLAS\nfloating-point implementation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 11:00:06 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 19:30:11 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["de Prado", "Miguel", ""], ["Mundy", "Andrew", ""], ["Saeed", "Rabia", ""], ["Denna", "Maurizio", ""], ["Pazos", "Nuria", ""], ["Benini", "Luca", ""]]}, {"id": "2006.05183", "submitter": "Piotr Kawa", "authors": "Piotr Kawa and Piotr Syga", "title": "A Note on Deepfake Detection with Low-Resources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deepfakes are videos that include changes, quite often substituting face of a\nportrayed individual with a different face using neural networks. Even though\nthe technology gained its popularity as a carrier of jokes and parodies it\nraises a serious threat to ones security - via biometric impersonation or\nbesmearing. In this paper we present two methods that allow detecting Deepfakes\nfor a user without significant computational power. In particular, we enhance\nMesoNet by replacing the original activation functions allowing a nearly 1%\nimprovement as well as increasing the consistency of the results. Moreover, we\nintroduced and verified a new activation function - Pish that at the cost of\nslight time overhead allows even higher consistency.\n  Additionally, we present a preliminary results of Deepfake detection method\nbased on Local Feature Descriptors (LFD), that allows setting up the system\neven faster and without resorting to GPU computation. Our method achieved Equal\nError Rate of 0.28, with both accuracy and recall exceeding 0.7.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 11:07:08 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Kawa", "Piotr", ""], ["Syga", "Piotr", ""]]}, {"id": "2006.05187", "submitter": "Qingdong He", "authors": "Qingdong He, Zhengning Wang, Hao Zeng, Yijun Liu, Shuaicheng Liu, Bing\n  Zeng", "title": "Stereo RGB and Deeper LIDAR Based Network for 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection has become an emerging task in autonomous driving\nscenarios. Previous works process 3D point clouds using either projection-based\nor voxel-based models. However, both approaches contain some drawbacks. The\nvoxel-based methods lack semantic information, while the projection-based\nmethods suffer from numerous spatial information loss when projected to\ndifferent views. In this paper, we propose the Stereo RGB and Deeper LIDAR\n(SRDL) framework which can utilize semantic and spatial information\nsimultaneously such that the performance of network for 3D object detection can\nbe improved naturally. Specifically, the network generates candidate boxes from\nstereo pairs and combines different region-wise features using a deep fusion\nscheme. The stereo strategy offers more information for prediction compared\nwith prior works. Then, several local and global feature extractors are stacked\nin the segmentation module to capture richer deep semantic geometric features\nfrom point clouds. After aligning the interior points with fused features, the\nproposed network refines the prediction in a more accurate manner and encodes\nthe whole box in a novel compact method. The decent experimental results on the\nchallenging KITTI detection benchmark demonstrate the effectiveness of\nutilizing both stereo images and point clouds for 3D object detection.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 11:19:24 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["He", "Qingdong", ""], ["Wang", "Zhengning", ""], ["Zeng", "Hao", ""], ["Liu", "Yijun", ""], ["Liu", "Shuaicheng", ""], ["Zeng", "Bing", ""]]}, {"id": "2006.05196", "submitter": "Jin Keong", "authors": "Jin Keong, Xingbo Dong, Zhe Jin, Khawla Mallat, Jean-Luc Dugelay", "title": "Multi-spectral Facial Landmark Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal face image analysis is favorable for certain circumstances. For\nexample, illumination-sensitive applications, like nighttime surveillance; and\nprivacy-preserving demanded access control. However, the inadequate study on\nthermal face image analysis calls for attention in responding to the industry\nrequirements. Detecting facial landmark points are important for many face\nanalysis tasks, such as face recognition, 3D face reconstruction, and face\nexpression recognition. In this paper, we propose a robust neural network\nenabled facial landmark detection, namely Deep Multi-Spectral Learning (DMSL).\nBriefly, DMSL consists of two sub-models, i.e. face boundary detection, and\nlandmark coordinates detection. Such an architecture demonstrates the\ncapability of detecting the facial landmarks on both visible and thermal\nimages. Particularly, the proposed DMSL model is robust in facial landmark\ndetection where the face is partially occluded, or facing different directions.\nThe experiment conducted on Eurecom's visible and thermal paired database shows\nthe superior performance of DMSL over the state-of-the-art for thermal facial\nlandmark detection. In addition to that, we have annotated a thermal face\ndataset with their respective facial landmark for the purpose of\nexperimentation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 11:43:46 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Keong", "Jin", ""], ["Dong", "Xingbo", ""], ["Jin", "Zhe", ""], ["Mallat", "Khawla", ""], ["Dugelay", "Jean-Luc", ""]]}, {"id": "2006.05210", "submitter": "Xichuan Zhou", "authors": "Xichuan Zhou, Kui Liu, Cong Shi, Haijun Liu, Ji Liu", "title": "Neural Network Activation Quantization with Bitwise Information\n  Bottlenecks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent researches on information bottleneck shed new light on the continuous\nattempts to open the black box of neural signal encoding. Inspired by the\nproblem of lossy signal compression for wireless communication, this paper\npresents a Bitwise Information Bottleneck approach for quantizing and encoding\nneural network activations. Based on the rate-distortion theory, the Bitwise\nInformation Bottleneck attempts to determine the most significant bits in\nactivation representation by assigning and approximating the sparse coefficient\nassociated with each bit. Given the constraint of a limited average code rate,\nthe information bottleneck minimizes the rate-distortion for optimal activation\nquantization in a flexible layer-by-layer manner. Experiments over ImageNet and\nother datasets show that, by minimizing the quantization rate-distortion of\neach layer, the neural network with information bottlenecks achieves the\nstate-of-the-art accuracy with low-precision activation. Meanwhile, by reducing\nthe code rate, the proposed method can improve the memory and computational\nefficiency by over six times compared with the deep neural network with\nstandard single-precision representation. Codes will be available on GitHub\nwhen the paper is accepted \\url{https://github.com/BitBottleneck/PublicCode}.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 12:10:04 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zhou", "Xichuan", ""], ["Liu", "Kui", ""], ["Shi", "Cong", ""], ["Liu", "Haijun", ""], ["Liu", "Ji", ""]]}, {"id": "2006.05218", "submitter": "Ioannis Gatopoulos", "authors": "Ioannis Gatopoulos, Maarten Stol, Jakub M. Tomczak", "title": "Super-resolution Variational Auto-Encoders", "comments": "13 pages, 11 figures, 3 tables. Code available at:\n  https://github.com/ioangatop/srVAE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of variational autoencoders (VAEs) provides a principled method\nfor jointly learning latent-variable models and corresponding inference models.\nHowever, the main drawback of this approach is the blurriness of the generated\nimages. Some studies link this effect to the objective function, namely, the\n(negative) log-likelihood. Here, we propose to enhance VAEs by adding a random\nvariable that is a downscaled version of the original image and still use the\nlog-likelihood function as the learning objective. Further, by providing the\ndownscaled image as an input to the decoder, it can be used in a manner similar\nto the super-resolution. We present empirically that the proposed approach\nperforms comparably to VAEs in terms of the negative log-likelihood, but it\nobtains a better FID score in data synthesis.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 12:32:16 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 13:06:43 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Gatopoulos", "Ioannis", ""], ["Stol", "Maarten", ""], ["Tomczak", "Jakub M.", ""]]}, {"id": "2006.05220", "submitter": "Xiaolin Zhang", "authors": "Xiaolin Zhang, Yunchao Wei, Yi Yang, Fei Wu", "title": "Rethinking Localization Map: Towards Accurate Object Perception with\n  Self-Enhancement Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, remarkable progress has been made in weakly supervised object\nlocalization (WSOL) to promote object localization maps. The common practice of\nevaluating these maps applies an indirect and coarse way, i.e., obtaining tight\nbounding boxes which can cover high-activation regions and calculating\nintersection-over-union (IoU) scores between the predicted and ground-truth\nboxes. This measurement can evaluate the ability of localization maps to some\nextent, but we argue that the maps should be measured directly and delicately,\ni.e., comparing the maps with the ground-truth object masks pixel-wisely. To\nfulfill the direct evaluation, we annotate pixel-level object masks on the\nILSVRC validation set. We propose to use IoU-Threshold curves for evaluating\nthe real quality of localization maps. Beyond the amended evaluation metric and\nannotated object masks, this work also introduces a novel self-enhancement\nmethod to harvest accurate object localization maps and object boundaries with\nonly category labels as supervision. We propose a two-stage approach to\ngenerate the localization maps by simply comparing the similarity of point-wise\nfeatures between the high-activation and the rest pixels. Based on the\npredicted localization maps, we explore to estimate object boundaries on a very\nlarge dataset. A hard-negative suppression loss is proposed for obtaining fine\nboundaries. We conduct extensive experiments on the ILSVRC and CUB benchmarks.\nIn particular, the proposed Self-Enhancement Maps achieve the state-of-the-art\nlocalization accuracy of 54.88% on ILSVRC. The code and the annotated masks are\nreleased at https://github.com/xiaomengyc/SEM.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 12:35:55 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 04:13:23 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhang", "Xiaolin", ""], ["Wei", "Yunchao", ""], ["Yang", "Yi", ""], ["Wu", "Fei", ""]]}, {"id": "2006.05245", "submitter": "Delong Chen", "authors": "Delong Chen, Shunhui Ji, Fan Liu, Zewen Li, Xinyu Zhou", "title": "A Review of Automated Diagnosis of COVID-19 Based on Scanning Images", "comments": "In ICRAI 2020: 2020 6th International Conference on Robotics and\n  Artificial Intelligence", "journal-ref": null, "doi": "10.1145/3449301.3449778", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The pandemic of COVID-19 has caused millions of infections, which has led to\na great loss all over the world, socially and economically. Due to the\nfalse-negative rate and the time-consuming of the conventional Reverse\nTranscription Polymerase Chain Reaction (RT-PCR) tests, diagnosing based on\nX-ray images and Computed Tomography (CT) images has been widely adopted.\nTherefore, researchers of the computer vision area have developed many\nautomatic diagnosing models based on machine learning or deep learning to\nassist the radiologists and improve the diagnosing accuracy. In this paper, we\npresent a review of these recently emerging automatic diagnosing models. 70\nmodels proposed from February 14, 2020, to July 21, 2020, are involved. We\nanalyzed the models from the perspective of preprocessing, feature extraction,\nclassification, and evaluation. Based on the limitation of existing models, we\npointed out that domain adaption in transfer learning and interpretability\npromotion would be the possible future directions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 13:29:15 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 02:20:19 GMT"}, {"version": "v3", "created": "Sat, 24 Jul 2021 05:18:17 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chen", "Delong", ""], ["Ji", "Shunhui", ""], ["Liu", "Fan", ""], ["Li", "Zewen", ""], ["Zhou", "Xinyu", ""]]}, {"id": "2006.05249", "submitter": "Daniel Harari", "authors": "Hanna Benoni, Daniel Harari and Shimon Ullman", "title": "What takes the brain so long: Object recognition at the level of minimal\n  images develops for up to seconds of presentation time", "comments": "7 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rich empirical evidence has shown that visual object recognition in the brain\nis fast and effortless, with relevant brain signals reported to start as early\nas 80 ms. Here we study the time trajectory of the recognition process at the\nlevel of minimal recognizable images (termed MIRC). These are images that can\nbe recognized reliably, but in which a minute change of the image (reduction by\neither size or resolution) has a drastic effect on recognition. Subjects were\nassigned to one of nine exposure conditions: 200, 500, 1000, 2000 ms with or\nwithout masking, as well as unlimited time. The subjects were not limited in\ntime to respond after presentation. The results show that in the masked\nconditions, recognition rates develop gradually over an extended period, e.g.\naverage of 18% for 200 ms exposure and 45% for 500 ms, increasing significantly\nwith longer exposure even above 2 secs. When presented for unlimited time\n(until response), MIRC recognition rates were equivalent to the rates of\nfull-object images presented for 50 ms followed by masking. What takes the\nbrain so long to recognize such images? We discuss why processes involving\neye-movements, perceptual decision-making and pattern completion are unlikely\nexplanations. Alternatively, we hypothesize that MIRC recognition requires an\nextended top-down process complementing the feed-forward phase.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 13:33:04 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Benoni", "Hanna", ""], ["Harari", "Daniel", ""], ["Ullman", "Shimon", ""]]}, {"id": "2006.05274", "submitter": "German Gonzalez", "authors": "Germ\\'an Gonz\\'alez, Aurelia Bustos, Jos\\'e Mar\\'ia Salinas, Mar\\'ia\n  de la Iglesia-Vaya, Joaqu\\'in Galant, Carlos Cano-Espinosa, Xavier Barber,\n  Domingo Orozco-Beltr\\'an, Miguel Cazorla and Antonio Pertusa", "title": "UMLS-ChestNet: A deep convolutional neural network for radiological\n  findings, differential diagnoses and localizations of COVID-19 in chest\n  x-rays", "comments": "17 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a method for the detection of radiological findings,\ntheir location and differential diagnoses from chest x-rays. Unlike prior works\nthat focus on the detection of few pathologies, we use a hierarchical taxonomy\nmapped to the Unified Medical Language System (UMLS) terminology to identify\n189 radiological findings, 22 differential diagnosis and 122 anatomic\nlocations, including ground glass opacities, infiltrates, consolidations and\nother radiological findings compatible with COVID-19. We train the system on\none large database of 92,594 frontal chest x-rays (AP or PA, standing, supine\nor decubitus) and a second database of 2,065 frontal images of COVID-19\npatients identified by at least one positive Polymerase Chain Reaction (PCR)\ntest. The reference labels are obtained through natural language processing of\nthe radiological reports. On 23,159 test images, the proposed neural network\nobtains an AUC of 0.94 for the diagnosis of COVID-19. To our knowledge, this\nwork uses the largest chest x-ray dataset of COVID-19 positive cases to date\nand is the first one to use a hierarchical labeling schema and to provide\ninterpretability of the results, not only by using network attention methods,\nbut also by indicating the radiological findings that have led to the\ndiagnosis.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 19:24:35 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Gonz\u00e1lez", "Germ\u00e1n", ""], ["Bustos", "Aurelia", ""], ["Salinas", "Jos\u00e9 Mar\u00eda", ""], ["de la Iglesia-Vaya", "Mar\u00eda", ""], ["Galant", "Joaqu\u00edn", ""], ["Cano-Espinosa", "Carlos", ""], ["Barber", "Xavier", ""], ["Orozco-Beltr\u00e1n", "Domingo", ""], ["Cazorla", "Miguel", ""], ["Pertusa", "Antonio", ""]]}, {"id": "2006.05325", "submitter": "Orhan Akal", "authors": "Orhan Akal, Zhigang Peng and Gerardo Hermosillo Valadez", "title": "ComboNet: Combined 2D & 3D Architecture for Aorta Segmentation", "comments": "9 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D segmentation with deep learning if trained with full resolution is the\nideal way of achieving the best accuracy. Unlike in 2D, 3D segmentation\ngenerally does not have sparse outliers, prevents leakage to surrounding soft\ntissues, at the very least it is generally more consistent than 2D\nsegmentation. However, GPU memory is generally the bottleneck for such an\napplication. Thus, most of the 3D segmentation applications handle sub-sampled\ninput instead of full resolution, which comes with the cost of losing precision\nat the boundary. In order to maintain precision at the boundary and prevent\nsparse outliers and leakage, we designed ComboNet. ComboNet is designed in an\nend to end fashion with three sub-network structures. The first two are\nparallel: 2D UNet with full resolution and 3D UNet with four times sub-sampled\ninput. The last stage is the concatenation of 2D and 3D outputs along with a\nfull-resolution input image which is followed by two convolution layers either\nwith 2D or 3D convolutions. With ComboNet we have achieved $92.1\\%$ dice\naccuracy for aorta segmentation. With Combonet, we have observed up to $2.3\\%$\nimprovement of dice accuracy as opposed to 2D UNet with the full-resolution\ninput image.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 15:02:55 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Akal", "Orhan", ""], ["Peng", "Zhigang", ""], ["Valadez", "Gerardo Hermosillo", ""]]}, {"id": "2006.05327", "submitter": "Roberto Daza", "authors": "Roberto Daza, Aythami Morales, Julian Fierrez, Ruben Tolosana", "title": "mEBAL: A Multimodal Database for Eye Blink Detection and Attention Level\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents mEBAL, a multimodal database for eye blink detection and\nattention level estimation. The eye blink frequency is related to the cognitive\nactivity and automatic detectors of eye blinks have been proposed for many\ntasks including attention level estimation, analysis of neuro-degenerative\ndiseases, deception recognition, drive fatigue detection, or face\nanti-spoofing. However, most existing databases and algorithms in this area are\nlimited to experiments involving only a few hundred samples and individual\nsensors like face cameras. The proposed mEBAL improves previous databases in\nterms of acquisition sensors and samples. In particular, three different\nsensors are simultaneously considered: Near Infrared (NIR) and RGB cameras to\ncapture the face gestures and an Electroencephalography (EEG) band to capture\nthe cognitive activity of the user and blinking events. Regarding the size of\nmEBAL, it comprises 6,000 samples and the corresponding attention level from 38\ndifferent students while conducting a number of e-learning tasks of varying\ndifficulty. In addition to presenting mEBAL, we also include preliminary\nexperiments on: i) eye blink detection using Convolutional Neural Networks\n(CNN) with the facial images, and ii) attention level estimation of the\nstudents based on their eye blink frequency.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 15:05:08 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 11:11:33 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Daza", "Roberto", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""], ["Tolosana", "Ruben", ""]]}, {"id": "2006.05332", "submitter": "Mete Ahishali", "authors": "Mete Ahishali, Aysen Degerli, Mehmet Yamac, Serkan Kiranyaz, Muhammad\n  E. H. Chowdhury, Khalid Hameed, Tahir Hamid, Rashid Mazhar, Moncef Gabbouj", "title": "Advance Warning Methodologies for COVID-19 using Chest X-Ray Images", "comments": "12 pages", "journal-ref": "in IEEE Access, vol. 9, pp. 41052-41065, 2021", "doi": "10.1109/ACCESS.2021.3064927", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) has rapidly become a global health\nconcern after its first known detection in December 2019. As a result, accurate\nand reliable advance warning system for the early diagnosis of COVID-19 has now\nbecome a priority. The detection of COVID-19 in early stages is not a\nstraightforward task from chest X-ray images according to expert medical\ndoctors because the traces of the infection are visible only when the disease\nhas progressed to a moderate or severe stage. In this study, our first aim is\nto evaluate the ability of recent \\textit{state-of-the-art} Machine Learning\ntechniques for the early detection of COVID-19 from chest X-ray images. Both\ncompact classifiers and deep learning approaches are considered in this study.\nFurthermore, we propose a recent compact classifier, Convolutional Support\nEstimator Network (CSEN) approach for this purpose since it is well-suited for\na scarce-data classification task. Finally, this study introduces a new\nbenchmark dataset called Early-QaTa-COV19, which consists of 1065 early-stage\nCOVID-19 pneumonia samples (very limited or no infection signs) labelled by the\nmedical doctors and 12 544 samples for control (normal) class. A detailed set\nof experiments shows that the CSEN achieves the top (over 97%) sensitivity with\nover 95.5% specificity. Moreover, DenseNet-121 network produces the leading\nperformance among other deep networks with 95% sensitivity and 99.74%\nspecificity.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 20:42:25 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 14:36:39 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 22:12:58 GMT"}, {"version": "v4", "created": "Fri, 4 Sep 2020 17:46:51 GMT"}, {"version": "v5", "created": "Sun, 31 Jan 2021 18:49:03 GMT"}, {"version": "v6", "created": "Thu, 18 Mar 2021 11:39:17 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ahishali", "Mete", ""], ["Degerli", "Aysen", ""], ["Yamac", "Mehmet", ""], ["Kiranyaz", "Serkan", ""], ["Chowdhury", "Muhammad E. H.", ""], ["Hameed", "Khalid", ""], ["Hamid", "Tahir", ""], ["Mazhar", "Rashid", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2006.05338", "submitter": "Ngoc-Trung Tran", "authors": "Ngoc-Trung Tran, Viet-Hung Tran, Ngoc-Bao Nguyen, Trung-Kien Nguyen,\n  Ngai-Man Cheung", "title": "On Data Augmentation for GAN Training", "comments": "Accepted in IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2021.3049346", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes in Generative Adversarial Networks (GAN) have affirmed the\nimportance of using more data in GAN training. Yet it is expensive to collect\ndata in many domains such as medical applications. Data Augmentation (DA) has\nbeen applied in these applications. In this work, we first argue that the\nclassical DA approach could mislead the generator to learn the distribution of\nthe augmented data, which could be different from that of the original data. We\nthen propose a principled framework, termed Data Augmentation Optimized for GAN\n(DAG), to enable the use of augmented data in GAN training to improve the\nlearning of the original distribution. We provide theoretical analysis to show\nthat using our proposed DAG aligns with the original GAN in minimizing the\nJensen-Shannon (JS) divergence between the original distribution and model\ndistribution. Importantly, the proposed DAG effectively leverages the augmented\ndata to improve the learning of discriminator and generator. We conduct\nexperiments to apply DAG to different GAN models: unconditional GAN,\nconditional GAN, self-supervised GAN and CycleGAN using datasets of natural\nimages and medical images. The results show that DAG achieves consistent and\nconsiderable improvements across these models. Furthermore, when DAG is used in\nsome GAN models, the system establishes state-of-the-art Frechet Inception\nDistance (FID) scores. Our code is available.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 15:19:26 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 14:00:58 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2020 08:34:10 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Tran", "Ngoc-Trung", ""], ["Tran", "Viet-Hung", ""], ["Nguyen", "Ngoc-Bao", ""], ["Nguyen", "Trung-Kien", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "2006.05353", "submitter": "Alon Lahav", "authors": "Alon Lahav, Ayellet Tal", "title": "MeshWalker: Deep Mesh Understanding by Random Walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most attempts to represent 3D shapes for deep learning have focused on\nvolumetric grids, multi-view images and point clouds. In this paper we look at\nthe most popular representation of 3D shapes in computer graphics - a\ntriangular mesh - and ask how it can be utilized within deep learning. The few\nattempts to answer this question propose to adapt convolutions & pooling to\nsuit Convolutional Neural Networks (CNNs). This paper proposes a very different\napproach, termed MeshWalker, to learn the shape directly from a given mesh. The\nkey idea is to represent the mesh by random walks along the surface, which\n\"explore\" the mesh's geometry and topology. Each walk is organized as a list of\nvertices, which in some manner imposes regularity on the mesh. The walk is fed\ninto a Recurrent Neural Network (RNN) that \"remembers\" the history of the walk.\nWe show that our approach achieves state-of-the-art results for two fundamental\nshape analysis tasks: shape classification and semantic segmentation.\nFurthermore, even a very small number of examples suffices for learning. This\nis highly important, since large datasets of meshes are difficult to acquire.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 15:35:41 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 05:39:22 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 15:39:51 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Lahav", "Alon", ""], ["Tal", "Ayellet", ""]]}, {"id": "2006.05355", "submitter": "Erdem Akag\\\"und\\\"uz", "authors": "Alper Kaplan and Erdem Akagunduz", "title": "A Hybrid Framework for Matching Printing Design Files to Product Photos", "comments": null, "journal-ref": "published in Balkan Journal of Electrical and Computer\n  Engineering, Volume 8 - Issue 2 - Apr 30, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a real-time image matching framework, which is hybrid in the sense\nthat it uses both hand-crafted features and deep features obtained from a\nwell-tuned deep convolutional network. The matching problem, which we\nconcentrate on, is specific to a certain application, that is, printing design\nto product photo matching. Printing designs are any kind of template image\nfiles, created using a design tool, thus are perfect image signals. However,\nphotographs of a printed product suffer many unwanted effects, such as\nuncontrolled shooting angle, uncontrolled illumination, occlusions, printing\ndeficiencies in color, camera noise, optic blur, et cetera. For this purpose,\nwe create an image set that includes printing design and corresponding product\nphoto pairs with collaboration of an actual printing facility. Using this image\nset, we benchmark various hand-crafted and deep features for matching\nperformance and propose a framework in which deep learning is utilized with\nhighest contribution, but without disabling real-time operation using an\nordinary desktop computer.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 15:39:14 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Kaplan", "Alper", ""], ["Akagunduz", "Erdem", ""]]}, {"id": "2006.05367", "submitter": "Huaying Hao", "authors": "Huaying Hao, Huazhu Fu, Yanwu Xu, Jianlong Yang, Fei Li, Xiulan Zhang,\n  Jiang Liu, Yitian Zhao", "title": "Open-Narrow-Synechiae Anterior Chamber Angle Classification in AS-OCT\n  Sequences", "comments": "Accepted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anterior chamber angle (ACA) classification is a key step in the diagnosis of\nangle-closure glaucoma in Anterior Segment Optical Coherence Tomography\n(AS-OCT). Existing automated analysis methods focus on a binary classification\nsystem (i.e., open angle or angle-closure) in a 2D AS-OCT slice. However,\nclinical diagnosis requires a more discriminating ACA three-class system (i.e.,\nopen, narrow, or synechiae angles) for the benefit of clinicians who seek\nbetter to understand the progression of the spectrum of angle-closure glaucoma\ntypes. To address this, we propose a novel sequence multi-scale aggregation\ndeep network (SMA-Net) for open-narrow-synechiae ACA classification based on an\nAS-OCT sequence. In our method, a Multi-Scale Discriminative Aggregation (MSDA)\nblock is utilized to learn the multi-scale representations at slice level,\nwhile a ConvLSTM is introduced to study the temporal dynamics of these\nrepresentations at sequence level. Finally, a multi-level loss function is used\nto combine the slice-based and sequence-based losses. The proposed method is\nevaluated across two AS-OCT datasets. The experimental results show that the\nproposed method outperforms existing state-of-the-art methods in applicability,\neffectiveness, and accuracy. We believe this work to be the first attempt to\nclassify ACAs into open, narrow, or synechia types grading using AS-OCT\nsequences.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 16:00:00 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Hao", "Huaying", ""], ["Fu", "Huazhu", ""], ["Xu", "Yanwu", ""], ["Yang", "Jianlong", ""], ["Li", "Fei", ""], ["Zhang", "Xiulan", ""], ["Liu", "Jiang", ""], ["Zhao", "Yitian", ""]]}, {"id": "2006.05389", "submitter": "Niccol\\`o Antonello", "authors": "Niccol\\`o Antonello, Philip N. Garner", "title": "A t-distribution based operator for enhancing out of distribution\n  robustness of neural network classifiers", "comments": "5 pages, 5 figures, to be published in IEEE Signal Processing\n  Letters, reproducible code https://github.com/idiap/tsoftmax", "journal-ref": null, "doi": "10.1109/LSP.2020.3001843", "report-no": null, "categories": "eess.SP cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Network (NN) classifiers can assign extreme probabilities to samples\nthat have not appeared during training (out-of-distribution samples) resulting\nin erroneous and unreliable predictions. One of the causes for this unwanted\nbehaviour lies in the use of the standard softmax operator which pushes the\nposterior probabilities to be either zero or unity hence failing to model\nuncertainty. The statistical derivation of the softmax operator relies on the\nassumption that the distributions of the latent variables for a given class are\nGaussian with known variance. However, it is possible to use different\nassumptions in the same derivation and attain from other families of\ndistributions as well. This allows derivation of novel operators with more\nfavourable properties. Here, a novel operator is proposed that is derived using\n$t$-distributions which are capable of providing a better description of\nuncertainty. It is shown that classifiers that adopt this novel operator can be\nmore robust to out of distribution samples, often outperforming NNs that use\nthe standard softmax operator. These enhancements can be reached with minimal\nchanges to the NN architecture.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 16:39:07 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 07:40:10 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2020 12:15:02 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Antonello", "Niccol\u00f2", ""], ["Garner", "Philip N.", ""]]}, {"id": "2006.05398", "submitter": "Danny Driess", "authors": "Danny Driess, Jung-Su Ha, Marc Toussaint", "title": "Deep Visual Reasoning: Learning to Predict Action Sequences for Task and\n  Motion Planning from an Initial Scene Image", "comments": "Robotics: Science and Systems (R:SS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep convolutional recurrent neural network that\npredicts action sequences for task and motion planning (TAMP) from an initial\nscene image. Typical TAMP problems are formalized by combining reasoning on a\nsymbolic, discrete level (e.g. first-order logic) with continuous motion\nplanning such as nonlinear trajectory optimization. Due to the great\ncombinatorial complexity of possible discrete action sequences, a large number\nof optimization/motion planning problems have to be solved to find a solution,\nwhich limits the scalability of these approaches.\n  To circumvent this combinatorial complexity, we develop a neural network\nwhich, based on an initial image of the scene, directly predicts promising\ndiscrete action sequences such that ideally only one motion planning problem\nhas to be solved to find a solution to the overall TAMP problem. A key aspect\nis that our method generalizes to scenes with many and varying number of\nobjects, although being trained on only two objects at a time. This is possible\nby encoding the objects of the scene in images as input to the neural network,\ninstead of a fixed feature vector. Results show runtime improvements of several\nmagnitudes. Video: https://youtu.be/i8yyEbbvoEk\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 16:52:02 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Driess", "Danny", ""], ["Ha", "Jung-Su", ""], ["Toussaint", "Marc", ""]]}, {"id": "2006.05400", "submitter": "Matan Atzmon", "authors": "Matan Atzmon and Yaron Lipman", "title": "SALD: Sign Agnostic Learning with Derivatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning 3D geometry directly from raw data, such as point clouds, triangle\nsoups, or unoriented meshes is still a challenging task that feeds many\ndownstream computer vision and graphics applications.\n  In this paper, we introduce SALD: a method for learning implicit neural\nrepresentations of shapes directly from raw data. We generalize sign agnostic\nlearning (SAL) to include derivatives: given an unsigned distance function to\nthe input raw data, we advocate a novel sign agnostic regression loss,\nincorporating both pointwise values and gradients of the unsigned distance\nfunction. Optimizing this loss leads to a signed implicit function solution,\nthe zero level set of which is a high quality and valid manifold approximation\nto the input 3D data. The motivation behind SALD is that incorporating\nderivatives in a regression loss leads to a lower sample complexity, and\nconsequently better fitting. In addition, we prove that SAL enjoys a minimal\nlength property in 2D, favoring minimal length solutions. More importantly, we\nare able to show that this property still holds for SALD, i.e., with\nderivatives included.\n  We demonstrate the efficacy of SALD for shape space learning on two\nchallenging datasets: ShapeNet that contains inconsistent orientation and\nnon-manifold meshes, and D-Faust that contains raw 3D scans (triangle soups).\nOn both these datasets, we present state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 16:54:57 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 17:24:48 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Atzmon", "Matan", ""], ["Lipman", "Yaron", ""]]}, {"id": "2006.05407", "submitter": "Yinbo Liu", "authors": "Yin-Bo Liu, Ming Zeng, Qing-Hao Meng", "title": "D-VPnet: A Network for Real-time Dominant Vanishing Point Detection in\n  Natural Scenes", "comments": "18 pages, 6 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important part of linear perspective, vanishing points (VPs) provide\nuseful clues for mapping objects from 2D photos to 3D space. Existing methods\nare mainly focused on extracting structural features such as lines or contours\nand then clustering these features to detect VPs. However, these techniques\nsuffer from ambiguous information due to the large number of line segments and\ncontours detected in outdoor environments. In this paper, we present a new\nconvolutional neural network (CNN) to detect dominant VPs in natural scenes,\ni.e., the Dominant Vanishing Point detection Network (D-VPnet). The key\ncomponent of our method is the feature line-segment proposal unit (FLPU), which\ncan be directly utilized to predict the location of the dominant VP. Moreover,\nthe model also uses the two main parallel lines as an assistant to determine\nthe position of the dominant VP. The proposed method was tested using a public\ndataset and a Parallel Line based Vanishing Point (PLVP) dataset. The\nexperimental results suggest that the detection accuracy of our approach\noutperforms those of state-of-the-art methods under various conditions in\nreal-time, achieving rates of 115fps.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 17:12:27 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Liu", "Yin-Bo", ""], ["Zeng", "Ming", ""], ["Meng", "Qing-Hao", ""]]}, {"id": "2006.05415", "submitter": "Edgar Galvan", "authors": "Edgar Galv\\'an and Peter Mooney", "title": "Neuroevolution in Deep Neural Networks: Current Trends and Future\n  Challenges", "comments": "20 pages (double column), 2 figures, 3 tables, 157 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of methods have been applied to the architectural configuration and\nlearning or training of artificial deep neural networks (DNN). These methods\nplay a crucial role in the success or failure of the DNN for most problems and\napplications. Evolutionary Algorithms (EAs) are gaining momentum as a\ncomputationally feasible method for the automated optimisation and training of\nDNNs. Neuroevolution is a term which describes these processes of automated\nconfiguration and training of DNNs using EAs. While many works exist in the\nliterature, no comprehensive surveys currently exist focusing exclusively on\nthe strengths and limitations of using neuroevolution approaches in DNNs.\nProlonged absence of such surveys can lead to a disjointed and fragmented field\npreventing DNNs researchers potentially adopting neuroevolutionary methods in\ntheir own research, resulting in lost opportunities for improving performance\nand wider application within real-world deep learning problems. This paper\npresents a comprehensive survey, discussion and evaluation of the\nstate-of-the-art works on using EAs for architectural configuration and\ntraining of DNNs. Based on this survey, the paper highlights the most pertinent\ncurrent issues and challenges in neuroevolution and identifies multiple\npromising future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 17:28:25 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Galv\u00e1n", "Edgar", ""], ["Mooney", "Peter", ""]]}, {"id": "2006.05456", "submitter": "Aishwarya Padmakumar", "authors": "Aishwarya Padmakumar and Raymond J. Mooney", "title": "Dialog Policy Learning for Joint Clarification and Active Learning\n  Queries", "comments": "AAAI 2020 Camera Ready", "journal-ref": "Proceedings of 2021 AAAI Conference on Artificial Intelligence\n  (AAAI-2021)", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Intelligent systems need to be able to recover from mistakes, resolve\nuncertainty, and adapt to novel concepts not seen during training. Dialog\ninteraction can enable this by the use of clarifications for correction and\nresolving uncertainty, and active learning queries to learn new concepts\nencountered during operation. Prior work on dialog systems has either focused\non exclusively learning how to perform clarification/ information seeking, or\nto perform active learning. In this work, we train a hierarchical dialog policy\nto jointly perform both clarification and active learning in the context of an\ninteractive language-based image retrieval task motivated by an online shopping\napplication, and demonstrate that jointly learning dialog policies for\nclarification and active learning is more effective than the use of static\ndialog policies for one or both of these functions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 18:53:21 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 18:03:22 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 03:31:36 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Padmakumar", "Aishwarya", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "2006.05467", "submitter": "Hidenori Tanaka", "authors": "Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, Surya Ganguli", "title": "Pruning neural networks without any data by iteratively conserving\n  synaptic flow", "comments": "NeurIPS 2020, 18 pages, 10 figures", "journal-ref": "Advances in Neural Information Processing Systems 2020", "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning the parameters of deep neural networks has generated intense interest\ndue to potential savings in time, memory and energy both during training and at\ntest time. Recent works have identified, through an expensive sequence of\ntraining and pruning cycles, the existence of winning lottery tickets or sparse\ntrainable subnetworks at initialization. This raises a foundational question:\ncan we identify highly sparse trainable subnetworks at initialization, without\never training, or indeed without ever looking at the data? We provide an\naffirmative answer to this question through theory driven algorithm design. We\nfirst mathematically formulate and experimentally verify a conservation law\nthat explains why existing gradient-based pruning algorithms at initialization\nsuffer from layer-collapse, the premature pruning of an entire layer rendering\na network untrainable. This theory also elucidates how layer-collapse can be\nentirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow\nPruning (SynFlow). This algorithm can be interpreted as preserving the total\nflow of synaptic strengths through the network at initialization subject to a\nsparsity constraint. Notably, this algorithm makes no reference to the training\ndata and consistently competes with or outperforms existing state-of-the-art\npruning algorithms at initialization over a range of models (VGG and ResNet),\ndatasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to\n99.99 percent). Thus our data-agnostic pruning algorithm challenges the\nexisting paradigm that, at initialization, data must be used to quantify which\nsynapses are important.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 19:21:57 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 11:05:52 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 03:54:34 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Tanaka", "Hidenori", ""], ["Kunin", "Daniel", ""], ["Yamins", "Daniel L. K.", ""], ["Ganguli", "Surya", ""]]}, {"id": "2006.05470", "submitter": "Alex Zwanenburg", "authors": "Adrien Depeursinge, Vincent Andrearczyk, Philip Whybra, Joost van\n  Griethuysen, Henning M\\\"uller, Roger Schaer, Martin Valli\\`eres, Alex\n  Zwanenburg (for the Image Biomarker Standardisation Initiative)", "title": "Standardised convolutional filtering for radiomics", "comments": "54 pages. For additional information see https://theibsi.github.io/\n  Changes in v3: * Clarified how to scale Laplacian-of-Gaussian filter kernels.\n  * Added 2D filter tests. Changes in v2: * Clarified how to reach rotational\n  invariance for convolutional filtering. * Reworked description of Simoncelli\n  filter. * Added intended filter dimensionality as a test parameter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Image Biomarker Standardisation Initiative (IBSI) aims to improve\nreproducibility of radiomics studies by standardising the computational process\nof extracting image biomarkers (features) from images. We have previously\nestablished reference values for 169 commonly used features, created a standard\nradiomics image processing scheme, and developed reporting guidelines for\nradiomic studies. However, several aspects are not standardised.\n  Here we present a preliminary version of a reference manual on the use of\nconvolutional image filters in radiomics. Filters, such as wavelets or\nLaplacian of Gaussian filters, play an important part in emphasising specific\nimage characteristics such as edges and blobs. Features derived from filter\nresponse maps have been found to be poorly reproducible. This reference manual\nforms the basis of ongoing work on standardising convolutional filters in\nradiomics, and will be updated as this work progresses.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 19:29:49 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 19:52:43 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 18:37:19 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Depeursinge", "Adrien", "", "for the Image Biomarker Standardisation Initiative"], ["Andrearczyk", "Vincent", "", "for the Image Biomarker Standardisation Initiative"], ["Whybra", "Philip", "", "for the Image Biomarker Standardisation Initiative"], ["van Griethuysen", "Joost", "", "for the Image Biomarker Standardisation Initiative"], ["M\u00fcller", "Henning", "", "for the Image Biomarker Standardisation Initiative"], ["Schaer", "Roger", "", "for the Image Biomarker Standardisation Initiative"], ["Valli\u00e8res", "Martin", "", "for the Image Biomarker Standardisation Initiative"], ["Zwanenburg", "Alex", "", "for the Image Biomarker Standardisation Initiative"]]}, {"id": "2006.05480", "submitter": "Pengxiao Zang", "authors": "Pengxiao Zang, Liqin Gao, Tristan T. Hormel, Jie Wang, Qisheng You,\n  Thomas S. Hwang and Yali Jia", "title": "DcardNet: Diabetic Retinopathy Classification at Multiple Levels Based\n  on Structural and Angiographic Optical Coherence Tomography", "comments": "Accepted for publication by IEEE Transactions on Biomedical\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Optical coherence tomography (OCT) and its angiography (OCTA) have\nseveral advantages for the early detection and diagnosis of diabetic\nretinopathy (DR). However, automated, complete DR classification frameworks\nbased on both OCT and OCTA data have not been proposed. In this study, a\nconvolutional neural network (CNN) based method is proposed to fulfill a DR\nclassification framework using en face OCT and OCTA. Methods: A densely and\ncontinuously connected neural network with adaptive rate dropout (DcardNet) is\ndesigned for the DR classification. In addition, adaptive label smoothing was\nproposed and used to suppress overfitting. Three separate classification levels\nare generated for each case based on the International Clinical Diabetic\nRetinopathy scale. At the highest level the network classifies scans as\nreferable or non-referable for DR. The second level classifies the eye as\nnon-DR, non-proliferative DR (NPDR), or proliferative DR (PDR). The last level\nclassifies the case as no DR, mild and moderate NPDR, severe NPDR, and PDR.\nResults: We used 10-fold cross-validation with 10% of the data to assess the\nnetworks performance. The overall classification accuracies of the three levels\nwere 95.7%, 85.0%, and 71.0% respectively. Conclusion/Significance: A reliable,\nsensitive and specific automated classification framework for referral to an\nophthalmologist can be a key technology for reducing vision loss related to DR.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 19:44:10 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 22:03:58 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Zang", "Pengxiao", ""], ["Gao", "Liqin", ""], ["Hormel", "Tristan T.", ""], ["Wang", "Jie", ""], ["You", "Qisheng", ""], ["Hwang", "Thomas S.", ""], ["Jia", "Yali", ""]]}, {"id": "2006.05485", "submitter": "Nicolas Scheiner", "authors": "Nicolas Scheiner, Ole Schumann, Florian Kraus, Nils Appenrodt,\n  J\\\"urgen Dickmann, Bernhard Sick", "title": "Off-the-shelf sensor vs. experimental radar -- How much resolution is\n  necessary in automotive radar classification?", "comments": "Accepted @ 23rd International Conference on Information Fusion\n  (FUSION)", "journal-ref": "2020 IEEE 23rd International Conference on Information Fusion\n  (FUSION)", "doi": "10.23919/FUSION45008.2020.9190338", "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radar-based road user detection is an important topic in the context of\nautonomous driving applications. The resolution of conventional automotive\nradar sensors results in a sparse data representation which is tough to refine\nduring subsequent signal processing. On the other hand, a new sensor generation\nis waiting in the wings for its application in this challenging field. In this\narticle, two sensors of different radar generations are evaluated against each\nother. The evaluation criterion is the performance on moving road user object\ndetection and classification tasks. To this end, two data sets originating from\nan off-the-shelf radar and a high resolution next generation radar are\ncompared. Special attention is given on how the two data sets are assembled in\norder to make them comparable. The utilized object detector consists of a\nclustering algorithm, a feature extraction module, and a recurrent neural\nnetwork ensemble for classification. For the assessment, all components are\nevaluated both individually and, for the first time, as a whole. This allows\nfor indicating where overall performance improvements have their origin in the\npipeline. Furthermore, the generalization capabilities of both data sets are\nevaluated and important comparison metrics for automotive radar object\ndetection are discussed. Results show clear benefits of the next generation\nradar. Interestingly, those benefits do not actually occur due to better\nperformance at the classification stage, but rather because of the vast\nimprovements at the clustering stage.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 19:51:34 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Scheiner", "Nicolas", ""], ["Schumann", "Ole", ""], ["Kraus", "Florian", ""], ["Appenrodt", "Nils", ""], ["Dickmann", "J\u00fcrgen", ""], ["Sick", "Bernhard", ""]]}, {"id": "2006.05509", "submitter": "Jacob Creswell", "authors": "Zhi Zhen Qin, Shahriar Ahmed, Mohammad Shahnewaz Sarker, Kishor Paul,\n  Ahammad Shafiq Sikder Adel, Tasneem Naheyan, Rachael Barrett, Sayera Banu,\n  Jacob Creswell", "title": "Can artificial intelligence (AI) be used to accurately detect\n  tuberculosis (TB) from chest X-rays? An evaluation of five AI products for TB\n  screening and triaging in a high TB burden setting", "comments": "43 pages, 3 Tables 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) products can be trained to recognize\ntuberculosis (TB)-related abnormalities on chest radiographs. Various AI\nproducts are available commercially, yet there is lack of evidence on how their\nperformance compared with each other and with radiologists. We evaluated five\nAI software products for screening and triaging TB using a large dataset that\nhad not been used to train any commercial AI products. Individuals (>=15 years\nold) presenting to three TB screening centers in Dhaka, Bangladesh, were\nrecruited consecutively. All CXR were read independently by a group of three\nBangladeshi registered radiologists and five commercial AI products: CAD4TB\n(v7), InferReadDR (v2), Lunit INSIGHT CXR (v4.9.0), JF CXR-1 (v2), and qXR\n(v3). All five AI products significantly outperformed the Bangladeshi\nradiologists. The areas under the receiver operating characteristic curve are\nqXR: 90.81% (95% CI:90.33-91.29%), CAD4TB: 90.34% (95% CI:89.81-90.87), Lunit\nINSIGHT CXR: 88.61% (95% CI:88.03%-89.20%), InferReadDR: 84.90% (95% CI:\n84.27-85.54%) and JF CXR-1: 84.89% (95% CI:84.26-85.53%). Only qXR met the TPP\nwith 74.3% specificity at 90% sensitivity. Five AI algorithms can reduce the\nnumber of Xpert tests required by 50%, while maintaining a sensitivity above\n90%. All AI algorithms performed worse among the older age and people with\nprior TB history. AI products can be highly accurate and useful screening and\ntriage tools for TB detection in high burden regions and outperform human\nreaders.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 21:06:46 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 08:35:52 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 15:28:07 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Qin", "Zhi Zhen", ""], ["Ahmed", "Shahriar", ""], ["Sarker", "Mohammad Shahnewaz", ""], ["Paul", "Kishor", ""], ["Adel", "Ahammad Shafiq Sikder", ""], ["Naheyan", "Tasneem", ""], ["Barrett", "Rachael", ""], ["Banu", "Sayera", ""], ["Creswell", "Jacob", ""]]}, {"id": "2006.05513", "submitter": "Chen Zhao", "authors": "Chen Zhao, Joyce H. Keyak, Jinshan Tang, Tadashi S. Kaneko, Sundeep\n  Khosla, Shreyasee Amin, Elizabeth J. Atkinson, Lan-Juan Zhao, Michael J.\n  Serou, Chaoyang Zhang, Hui Shen, Hong-Wen Deng, Weihua Zhou", "title": "A Deep Learning-Based Method for Automatic Segmentation of Proximal\n  Femur from Quantitative Computed Tomography Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Proximal femur image analyses based on quantitative computed\ntomography (QCT) provide a method to quantify the bone density and evaluate\nosteoporosis and risk of fracture. We aim to develop a deep-learning-based\nmethod for automatic proximal femur segmentation. Methods and Materials: We\ndeveloped a 3D image segmentation method based on V-Net, an end-to-end fully\nconvolutional neural network (CNN), to extract the proximal femur QCT images\nautomatically. The proposed V-net methodology adopts a compound loss function,\nwhich includes a Dice loss and a L2 regularizer. We performed experiments to\nevaluate the effectiveness of the proposed segmentation method. In the\nexperiments, a QCT dataset which included 397 QCT subjects was used. For the\nQCT image of each subject, the ground truth for the proximal femur was\ndelineated by a well-trained scientist. During the experiments for the entire\ncohort then for male and female subjects separately, 90% of the subjects were\nused in 10-fold cross-validation for training and internal validation, and to\nselect the optimal parameters of the proposed models; the rest of the subjects\nwere used to evaluate the performance of models. Results: Visual comparison\ndemonstrated high agreement between the model prediction and ground truth\ncontours of the proximal femur portion of the QCT images. In the entire cohort,\nthe proposed model achieved a Dice score of 0.9815, a sensitivity of 0.9852 and\na specificity of 0.9992. In addition, an R2 score of 0.9956 (p<0.001) was\nobtained when comparing the volumes measured by our model prediction with the\nground truth. Conclusion: This method shows a great promise for clinical\napplication to QCT and QCT-based finite element analysis of the proximal femur\nfor evaluating osteoporosis and hip fracture risk.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 21:16:47 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 18:08:39 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 13:04:52 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Zhao", "Chen", ""], ["Keyak", "Joyce H.", ""], ["Tang", "Jinshan", ""], ["Kaneko", "Tadashi S.", ""], ["Khosla", "Sundeep", ""], ["Amin", "Shreyasee", ""], ["Atkinson", "Elizabeth J.", ""], ["Zhao", "Lan-Juan", ""], ["Serou", "Michael J.", ""], ["Zhang", "Chaoyang", ""], ["Shen", "Hui", ""], ["Deng", "Hong-Wen", ""], ["Zhou", "Weihua", ""]]}, {"id": "2006.05518", "submitter": "Nikolai Smolyanskiy", "authors": "Ke Chen, Ryan Oldja, Nikolai Smolyanskiy, Stan Birchfield, Alexander\n  Popov, David Wehr, Ibrahim Eden, Joachim Pehserl", "title": "MVLidarNet: Real-Time Multi-Class Scene Understanding for Autonomous\n  Driving Using Multiple Views", "comments": "IROS 2020 conference (submitted March 1st, 2020). For accompanying\n  video, see https://youtu.be/2ck5_sToayc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving requires the inference of actionable information such as\ndetecting and classifying objects, and determining the drivable space. To this\nend, we present Multi-View LidarNet (MVLidarNet), a two-stage deep neural\nnetwork for multi-class object detection and drivable space segmentation using\nmultiple views of a single LiDAR point cloud. The first stage processes the\npoint cloud projected onto a perspective view in order to semantically segment\nthe scene. The second stage then processes the point cloud (along with semantic\nlabels from the first stage) projected onto a bird's eye view, to detect and\nclassify objects. Both stages use an encoder-decoder architecture. We show that\nour multi-view, multi-stage, multi-class approach is able to detect and\nclassify objects while simultaneously determining the drivable space using a\nsingle LiDAR scan as input, in challenging scenes with more than one hundred\nvehicles and pedestrians at a time. The system operates efficiently at 150 fps\non an embedded GPU designed for a self-driving car, including a postprocessing\nstep to maintain identities over time. We show results on both KITTI and a much\nlarger internal dataset, thus demonstrating the method's ability to scale by an\norder of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 21:28:17 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 03:09:18 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Chen", "Ke", ""], ["Oldja", "Ryan", ""], ["Smolyanskiy", "Nikolai", ""], ["Birchfield", "Stan", ""], ["Popov", "Alexander", ""], ["Wehr", "David", ""], ["Eden", "Ibrahim", ""], ["Pehserl", "Joachim", ""]]}, {"id": "2006.05521", "submitter": "Michael McCann", "authors": "Michael T. McCann, Saiprasad Ravishankar", "title": "Supervised Learning of Sparsity-Promoting Regularizers for Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for supervised learning of sparsity-promoting\nregularizers for image denoising. Sparsity-promoting regularization is a key\ningredient in solving modern image reconstruction problems; however, the\noperators underlying these regularizers are usually either designed by hand or\nlearned from data in an unsupervised way. The recent success of supervised\nlearning (mainly convolutional neural networks) in solving image reconstruction\nproblems suggests that it could be a fruitful approach to designing\nregularizers. As a first experiment in this direction, we propose to denoise\nimages using a variational formulation with a parametric, sparsity-promoting\nregularizer, where the parameters of the regularizer are learned to minimize\nthe mean squared error of reconstructions on a training set of (ground truth\nimage, measurement) pairs. Training involves solving a challenging bilievel\noptimization problem; we derive an expression for the gradient of the training\nloss using Karush-Kuhn-Tucker conditions and provide an accompanying gradient\ndescent algorithm to minimize it. Our experiments on a simple synthetic,\ndenoising problem show that the proposed method can learn an operator that\noutperforms well-known regularizers (total variation, DCT-sparsity, and\nunsupervised dictionary learning) and collaborative filtering. While the\napproach we present is specific to denoising, we believe that it can be adapted\nto the whole class of inverse problems with linear measurement models, giving\nit applicability to a wide range of image reconstruction problems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 21:38:05 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["McCann", "Michael T.", ""], ["Ravishankar", "Saiprasad", ""]]}, {"id": "2006.05538", "submitter": "Bin Li", "authors": "Bin Li, Kevin W. Eliceiri", "title": "Dual-stream Maximum Self-attention Multi-instance Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-instance learning (MIL) is a form of weakly supervised learning where a\nsingle class label is assigned to a bag of instances while the instance-level\nlabels are not available. Training classifiers to accurately determine the bag\nlabel and instance labels is a challenging but critical task in many practical\nscenarios, such as computational histopathology. Recently, MIL models fully\nparameterized by neural networks have become popular due to the high\nflexibility and superior performance. Most of these models rely on attention\nmechanisms that assign attention scores across the instance embeddings in a bag\nand produce the bag embedding using an aggregation operator. In this paper, we\nproposed a dual-stream maximum self-attention MIL model (DSMIL) parameterized\nby neural networks. The first stream deploys a simple MIL max-pooling while the\ntop-activated instance embedding is determined and used to obtain\nself-attention scores across instance embeddings in the second stream.\nDifferent from most of the previous methods, the proposed model jointly learns\nan instance classifier and a bag classifier based on the same instance\nembeddings. The experiments results show that our method achieves superior\nperformance compared to the best MIL methods and demonstrates state-of-the-art\nperformance on benchmark MIL datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 22:44:58 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Li", "Bin", ""], ["Eliceiri", "Kevin W.", ""]]}, {"id": "2006.05544", "submitter": "Daniel Martinez", "authors": "Daniel Enrique Martinez, Waiman Meinhold, John Oshinski, Ai-Ping Hu,\n  and Jun Ueda", "title": "Resolution-Enhanced MRI-Guided Navigation of Spinal Cellular Injection\n  Robot", "comments": "6 pages, 10 figures, 3 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method of navigating a surgical robot beyond the\nresolution of magnetic resonance imaging (MRI) by using a resolution\nenhancement technique enabled by high-precision piezoelectric actuation. The\nsurgical robot was specifically designed for injecting stem cells into the\nspinal cord. This particular therapy can be performed in a shorter time by\nusing a MRI-compatible robotic platform than by using a manual needle\npositioning platform. Imaging resolution of fiducial markers attached to the\nneedle guide tubing was enhanced by reconstructing a high-resolution image from\nmultiple images with sub-pixel movements of the robot. The parallel-plane\ndirect-drive needle positioning mechanism positioned the needle guide with a\nhigh spatial precision that is two orders of magnitude higher than typical MRI\nresolution up to 1 mm. Reconstructed resolution enhanced images were used to\nnavigate the robot precisely that would not have been possible by using\nstandard MRI. Experiments were conducted to verify the effectiveness of the\nproposed enhanced-resolution image-guided intervention.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 23:07:55 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Martinez", "Daniel Enrique", ""], ["Meinhold", "Waiman", ""], ["Oshinski", "John", ""], ["Hu", "Ai-Ping", ""], ["Ueda", "Jun", ""]]}, {"id": "2006.05548", "submitter": "Ananya Gupta", "authors": "Ananya Gupta, Simon Watson, Hujun Yin", "title": "3D Point Cloud Feature Explanations Using Gradient-Based Methods", "comments": "Accepted for IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainability is an important factor to drive user trust in the use of\nneural networks for tasks with material impact. However, most of the work done\nin this area focuses on image analysis and does not take into account 3D data.\nWe extend the saliency methods that have been shown to work on image data to\ndeal with 3D data. We analyse the features in point clouds and voxel spaces and\nshow that edges and corners in 3D data are deemed as important features while\nplanar surfaces are deemed less important. The approach is model-agnostic and\ncan provide useful information about learnt features. Driven by the insight\nthat 3D data is inherently sparse, we visualise the features learnt by a\nvoxel-based classification network and show that these features are also sparse\nand can be pruned relatively easily, leading to more efficient neural networks.\nOur results show that the Voxception-ResNet model can be pruned down to 5\\% of\nits parameters with negligible loss in accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 23:17:24 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Gupta", "Ananya", ""], ["Watson", "Simon", ""], ["Yin", "Hujun", ""]]}, {"id": "2006.05560", "submitter": "Ananya Gupta", "authors": "Ananya Gupta, Jonathan Byrne, David Moloney, Simon Watson, Hujun Yin", "title": "Tree Annotations in LiDAR Data Using Point Densities and Convolutional\n  Neural Networks", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, vol. 58, no.\n  2, pp. 971-981, Feb. 2020", "doi": "10.1109/TGRS.2019.2942201", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiDAR provides highly accurate 3D point clouds. However, data needs to be\nmanually labelled in order to provide subsequent useful information. Manual\nannotation of such data is time consuming, tedious and error prone, and hence\nin this paper we present three automatic methods for annotating trees in LiDAR\ndata. The first method requires high density point clouds and uses certain\nLiDAR data attributes for the purpose of tree identification, achieving almost\n90% accuracy. The second method uses a voxel-based 3D Convolutional Neural\nNetwork on low density LiDAR datasets and is able to identify most large trees\naccurately but struggles with smaller ones due to the voxelisation process. The\nthird method is a scaled version of the PointNet++ method and works directly on\noutdoor point clouds and achieves an F_score of 82.1% on the ISPRS benchmark\ndataset, comparable to the state-of-the-art methods but with increased\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 23:50:40 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Gupta", "Ananya", ""], ["Byrne", "Jonathan", ""], ["Moloney", "David", ""], ["Watson", "Simon", ""], ["Yin", "Hujun", ""]]}, {"id": "2006.05569", "submitter": "Michel Melo Silva", "authors": "Alan Carvalho Neves, Michel Melo Silva, Mario Fernando Montenegro\n  Campos, Erickson Rangel Nascimento", "title": "A gaze driven fast-forward method for first-person videos", "comments": "Accepted for presentation at EPIC@CVPR2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing data sharing and life-logging cultures are driving an\nunprecedented increase in the amount of unedited First-Person Videos. In this\npaper, we address the problem of accessing relevant information in First-Person\nVideos by creating an accelerated version of the input video and emphasizing\nthe important moments to the recorder. Our method is based on an attention\nmodel driven by gaze and visual scene analysis that provides a semantic score\nof each frame of the input video. We performed several experimental evaluations\non publicly available First-Person Videos datasets. The results show that our\nmethodology can fast-forward videos emphasizing moments when the recorder\nvisually interact with scene components while not including monotonous clips.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 00:08:42 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Neves", "Alan Carvalho", ""], ["Silva", "Michel Melo", ""], ["Campos", "Mario Fernando Montenegro", ""], ["Nascimento", "Erickson Rangel", ""]]}, {"id": "2006.05575", "submitter": "Ananya Gupta", "authors": "Ananya Gupta, Simon Watson, Hujun Yin", "title": "Deep Learning-based Aerial Image Segmentation with Open Data for\n  Disaster Impact Assessment", "comments": "Accepted in Neurocomputing, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite images are an extremely valuable resource in the aftermath of\nnatural disasters such as hurricanes and tsunamis where they can be used for\nrisk assessment and disaster management. In order to provide timely and\nactionable information for disaster response, in this paper a framework\nutilising segmentation neural networks is proposed to identify impacted areas\nand accessible roads in post-disaster scenarios. The effectiveness of\npretraining with ImageNet on the task of aerial image segmentation has been\nanalysed and performances of popular segmentation models compared. Experimental\nresults show that pretraining on ImageNet usually improves the segmentation\nperformance for a number of models. Open data available from OpenStreetMap\n(OSM) is used for training, forgoing the need for time-consuming manual\nannotation. The method also makes use of graph theory to update road network\ndata available from OSM and to detect the changes caused by a natural disaster.\nExtensive experiments on data from the 2018 tsunami that struck Palu, Indonesia\nshow the effectiveness of the proposed framework. ENetSeparable, with 30% fewer\nparameters compared to ENet, achieved comparable segmentation results to that\nof the state-of-the-art networks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 00:19:58 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Gupta", "Ananya", ""], ["Watson", "Simon", ""], ["Yin", "Hujun", ""]]}, {"id": "2006.05580", "submitter": "Rajeev Yasarla", "authors": "Rajeev Yasarla, Vishwanath A. Sindagi, Vishal M. Patel", "title": "Syn2Real Transfer Learning for Image Deraining using Gaussian Processes", "comments": "Accepted at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent CNN-based methods for image deraining have achieved excellent\nperformance in terms of reconstruction error as well as visual quality.\nHowever, these methods are limited in the sense that they can be trained only\non fully labeled data. Due to various challenges in obtaining real world\nfully-labeled image deraining datasets, existing methods are trained only on\nsynthetically generated data and hence, generalize poorly to real-world images.\nThe use of real-world data in training image deraining networks is relatively\nless explored in the literature. We propose a Gaussian Process-based\nsemi-supervised learning framework which enables the network in learning to\nderain using synthetic dataset while generalizing better using unlabeled\nreal-world images. Through extensive experiments and ablations on several\nchallenging datasets (such as Rain800, Rain200H and DDN-SIRR), we show that the\nproposed method, when trained on limited labeled data, achieves on-par\nperformance with fully-labeled training. Additionally, we demonstrate that\nusing unlabeled real-world images in the proposed GP-based framework results in\nsuperior performance as compared to existing methods. Code is available at:\nhttps://github.com/rajeevyasarla/Syn2Real\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 00:33:18 GMT"}], "update_date": "2020-09-27", "authors_parsed": [["Yasarla", "Rajeev", ""], ["Sindagi", "Vishwanath A.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2006.05586", "submitter": "Lei Zhu", "authors": "Lei Zhu, Hui Cui, Zhiyong Cheng, Jingjing Li, Zheng Zhang", "title": "Dual-level Semantic Transfer Deep Hashing for Efficient Social Image\n  Retrieval", "comments": "Accepted by IEEE TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network stores and disseminates a tremendous amount of user shared\nimages. Deep hashing is an efficient indexing technique to support large-scale\nsocial image retrieval, due to its deep representation capability, fast\nretrieval speed and low storage cost. Particularly, unsupervised deep hashing\nhas well scalability as it does not require any manually labelled data for\ntraining. However, owing to the lacking of label guidance, existing methods\nsuffer from severe semantic shortage when optimizing a large amount of deep\nneural network parameters. Differently, in this paper, we propose a Dual-level\nSemantic Transfer Deep Hashing (DSTDH) method to alleviate this problem with a\nunified deep hash learning framework. Our model targets at learning the\nsemantically enhanced deep hash codes by specially exploiting the\nuser-generated tags associated with the social images. Specifically, we design\na complementary dual-level semantic transfer mechanism to efficiently discover\nthe potential semantics of tags and seamlessly transfer them into binary hash\ncodes. On the one hand, instance-level semantics are directly preserved into\nhash codes from the associated tags with adverse noise removing. Besides, an\nimage-concept hypergraph is constructed for indirectly transferring the latent\nhigh-order semantic correlations of images and tags into hash codes. Moreover,\nthe hash codes are obtained simultaneously with the deep representation\nlearning by the discrete hash optimization strategy. Extensive experiments on\ntwo public social image retrieval datasets validate the superior performance of\nour method compared with state-of-the-art hashing methods. The source codes of\nour method can be obtained at https://github.com/research2020-1/DSTDH\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 01:03:09 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zhu", "Lei", ""], ["Cui", "Hui", ""], ["Cheng", "Zhiyong", ""], ["Li", "Jingjing", ""], ["Zhang", "Zheng", ""]]}, {"id": "2006.05589", "submitter": "Ananya Gupta", "authors": "Ananya Gupta, Elisabeth Welburn, Simon Watson, Hujun Yin", "title": "CNN-Based Semantic Change Detection in Satellite Imagery", "comments": null, "journal-ref": "Proceedings of International Conference on Artificial Neural\n  Networks , 2019. pg-669-684", "doi": "10.1007/978-3-030-30493-5_61", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timely disaster risk management requires accurate road maps and prompt damage\nassessment. Currently, this is done by volunteers manually marking satellite\nimagery of affected areas but this process is slow and often error-prone.\nSegmentation algorithms can be applied to satellite images to detect road\nnetworks. However, existing methods are unsuitable for disaster-struck areas as\nthey make assumptions about the road network topology which may no longer be\nvalid in these scenarios. Herein, we propose a CNN-based framework for\nidentifying accessible roads in post-disaster imagery by detecting changes from\npre-disaster imagery. Graph theory is combined with the CNN output for\ndetecting semantic changes in road networks with OpenStreetMap data. Our\nresults are validated with data of a tsunami-affected region in Palu, Indonesia\nacquired from DigitalGlobe.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 01:06:03 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Gupta", "Ananya", ""], ["Welburn", "Elisabeth", ""], ["Watson", "Simon", ""], ["Yin", "Hujun", ""]]}, {"id": "2006.05597", "submitter": "Zhe Chen", "authors": "Zhe Chen, Jing Zhang, Dacheng Tao", "title": "Condensing Two-stage Detection with Automatic Object Key Part Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern two-stage object detectors generally require excessively large models\nfor their detection heads to achieve high accuracy. To address this problem, we\npropose that the model parameters of two-stage detection heads can be condensed\nand reduced by concentrating on object key parts. To this end, we first\nintroduce an automatic object key part discovery task to make neural networks\ndiscover representative sub-parts in each foreground object. With these\ndiscovered key parts, we then decompose the object appearance modeling into a\nkey part modeling process and a global modeling process for detection. Key part\nmodeling encodes fine and detailed features from the discovered key parts, and\nglobal modeling encodes rough and holistic object characteristics. In practice,\nsuch decomposition allows us to significantly abridge model parameters without\nsacrificing much detection accuracy. Experiments on popular datasets illustrate\nthat our proposed technique consistently maintains original performance while\nwaiving around 50% of the model parameters of common two-stage detection heads,\nwith the performance only deteriorating by 1.5% when waiving around 96% of the\noriginal model parameters. Codes are released on:\nhttps://github.com/zhechen/Condensing2stageDetection.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 01:20:47 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 05:18:56 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 01:08:50 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Chen", "Zhe", ""], ["Zhang", "Jing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2006.05612", "submitter": "Lazhar Khelifi", "authors": "Lazhar Khelifi and Max Mignotte", "title": "Deep Learning for Change Detection in Remote Sensing Images:\n  Comprehensive Review and Meta-Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) algorithms are considered as a methodology of choice for\nremote-sensing image analysis over the past few years. Due to its effective\napplications, deep learning has also been introduced for automatic change\ndetection and achieved great success. The present study attempts to provide a\ncomprehensive review and a meta-analysis of the recent progress in this\nsubfield. Specifically, we first introduce the fundamentals of deep learning\nmethods which arefrequently adopted for change detection. Secondly, we present\nthe details of the meta-analysis conducted to examine the status of change\ndetection DL studies. Then, we focus on deep learning-based change detection\nmethodologies for remote sensing images by giving a general overview of the\nexisting methods. Specifically, these deep learning-based methods were\nclassified into three groups; fully supervised learning-based methods, fully\nunsupervised learning-based methods and transfer learning-based techniques. As\na result of these investigations, promising new directions were identified for\nfuture research. This study will contribute in several ways to our\nunderstanding of deep learning for change detection and will provide a basis\nfor further research.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 02:14:08 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Khelifi", "Lazhar", ""], ["Mignotte", "Max", ""]]}, {"id": "2006.05627", "submitter": "Xiaopeng Zhang", "authors": "Xiaopeng Zhang", "title": "A survey on deep hashing for image retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Hashing has been widely used in approximate nearest search for large-scale\ndatabase retrieval for its computation and storage efficiency. Deep hashing,\nwhich devises convolutional neural network architecture to exploit and extract\nthe semantic information or feature of images, has received increasing\nattention recently. In this survey, several deep supervised hashing methods for\nimage retrieval are evaluated and I conclude three main different directions\nfor deep supervised hashing methods. Several comments are made at the end.\nMoreover, to break through the bottleneck of the existing hashing methods, I\npropose a Shadow Recurrent Hashing(SRH) method as a try. Specifically, I devise\na CNN architecture to extract the semantic features of images and design a loss\nfunction to encourage similar images projected close. To this end, I propose a\nconcept: shadow of the CNN output. During optimization process, the CNN output\nand its shadow are guiding each other so as to achieve the optimal solution as\nmuch as possible. Several experiments on dataset CIFAR-10 show the satisfying\nperformance of SRH.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 03:01:59 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zhang", "Xiaopeng", ""]]}, {"id": "2006.05646", "submitter": "Haripriya Harikumar", "authors": "Haripriya Harikumar, Vuong Le, Santu Rana, Sourangshu Bhattacharya,\n  Sunil Gupta, and Svetha Venkatesh", "title": "Scalable Backdoor Detection in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it has been shown that deep learning models are vulnerable to\nTrojan attacks, where an attacker can install a backdoor during training time\nto make the resultant model misidentify samples contaminated with a small\ntrigger patch. Current backdoor detection methods fail to achieve good\ndetection performance and are computationally expensive. In this paper, we\npropose a novel trigger reverse-engineering based approach whose computational\ncomplexity does not scale with the number of labels, and is based on a measure\nthat is both interpretable and universal across different network and patch\ntypes. In experiments, we observe that our method achieves a perfect score in\nseparating Trojaned models from pure models, which is an improvement over the\ncurrent state-of-the art method.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 04:12:53 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Harikumar", "Haripriya", ""], ["Le", "Vuong", ""], ["Rana", "Santu", ""], ["Bhattacharya", "Sourangshu", ""], ["Gupta", "Sunil", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "2006.05652", "submitter": "Tiago Buarque Assun\\c{c}\\~ao De Carvalho", "authors": "Tiago Buarque Assun\\c{c}\\~ao de Carvalho", "title": "Agrupamento de Pixels para o Reconhecimento de Faces", "comments": "21 pages, in Portuguese, 5 figures, book chapter, recortado\n  (adapatado) da tese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research starts with the observation that face recognition can suffer a\nlow impact from significant image shrinkage. To explain this fact, we proposed\nthe Pixel Clustering methodology. It defines regions in the image in which its\npixels are very similar to each other. We extract features from each region. We\nused three face databases in the experiments. We noticed that 512 is the\nmaximum number of features needed for high accuracy image recognition. The\nproposed method is also robust, even if only it uses a few classes from the\ntraining set.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 04:42:07 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["de Carvalho", "Tiago Buarque Assun\u00e7\u00e3o", ""]]}, {"id": "2006.05674", "submitter": "Leonid  Bedratyuk", "authors": "Leonid Bedratyuk", "title": "3D geometric moment invariants from the point of view of the classical\n  invariant theory", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to clear up the problem of the connection between\nthe 3D geometric moments invariants and the invariant theory, considering a\nproblem of describing of the 3D geometric moments invariants as a problem of\nthe classical invariant theory. Using the remarkable fact that the groups\n$SO(3)$ and $SL(2)$ are locally isomorphic, we reduced the problem of deriving\n3D geometric moments invariants to the well-known problem of the classical\ninvariant theory. We give a precise statement of the 3D geometric invariant\nmoments computation, introducing the notions of the algebras of simultaneous 3D\ngeometric moment invariants, and prove that they are isomorphic to the algebras\nof joint $SL(2)$-invariants of several binary forms. To simplify the\ncalculating of the invariants we proceed from an action of Lie group $SO(3)$ to\nan action of its Lie algebra $\\mathfrak{sl}_2$. The author hopes that the\nresults will be useful to the researchers in the fields of image analysis and\npattern recognition.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 15:40:53 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bedratyuk", "Leonid", ""]]}, {"id": "2006.05675", "submitter": "Hyeokhyen Kwon", "authors": "Hyeokhyen Kwon, Catherine Tong, Harish Haresamudram, Yan Gao, Gregory\n  D. Abowd, Nicholas D. Lane, Thomas Ploetz", "title": "IMUTube: Automatic Extraction of Virtual on-body Accelerometry from\n  Video for Human Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of large-scale, labeled data sets impedes progress in developing\nrobust and generalized predictive models for on-body sensor-based human\nactivity recognition (HAR). Labeled data in human activity recognition is\nscarce and hard to come by, as sensor data collection is expensive, and the\nannotation is time-consuming and error-prone. To address this problem, we\nintroduce IMUTube, an automated processing pipeline that integrates existing\ncomputer vision and signal processing techniques to convert videos of human\nactivity into virtual streams of IMU data. These virtual IMU streams represent\naccelerometry at a wide variety of locations on the human body. We show how the\nvirtually-generated IMU data improves the performance of a variety of models on\nknown HAR datasets. Our initial results are very promising, but the greater\npromise of this work lies in a collective approach by the computer vision,\nsignal processing, and activity recognition communities to extend this work in\nways that we outline. This should lead to on-body, sensor-based HAR becoming\nyet another success story in large-dataset breakthroughs in recognition.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 21:50:38 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 15:21:46 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Kwon", "Hyeokhyen", ""], ["Tong", "Catherine", ""], ["Haresamudram", "Harish", ""], ["Gao", "Yan", ""], ["Abowd", "Gregory D.", ""], ["Lane", "Nicholas D.", ""], ["Ploetz", "Thomas", ""]]}, {"id": "2006.05682", "submitter": "Zaiwei Zhang", "authors": "Zaiwei Zhang, Bo Sun, Haitao Yang, Qixing Huang", "title": "H3DNet: 3D Object Detection Using Hybrid Geometric Primitives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce H3DNet, which takes a colorless 3D point cloud as input and\noutputs a collection of oriented object bounding boxes (or BB) and their\nsemantic labels. The critical idea of H3DNet is to predict a hybrid set of\ngeometric primitives, i.e., BB centers, BB face centers, and BB edge centers.\nWe show how to convert the predicted geometric primitives into object proposals\nby defining a distance function between an object and the geometric primitives.\nThis distance function enables continuous optimization of object proposals, and\nits local minimums provide high-fidelity object proposals. H3DNet then utilizes\na matching and refinement module to classify object proposals into detected\nobjects and fine-tune the geometric parameters of the detected objects. The\nhybrid set of geometric primitives not only provides more accurate signals for\nobject detection than using a single type of geometric primitives, but it also\nprovides an overcomplete set of constraints on the resulting 3D layout.\nTherefore, H3DNet can tolerate outliers in predicted geometric primitives. Our\nmodel achieves state-of-the-art 3D detection results on two large datasets with\nreal 3D scans, ScanNet and SUN RGB-D.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 06:44:53 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 23:37:57 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 19:16:39 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Zhang", "Zaiwei", ""], ["Sun", "Bo", ""], ["Yang", "Haitao", ""], ["Huang", "Qixing", ""]]}, {"id": "2006.05683", "submitter": "Bo Pang", "authors": "Bo Pang, Yizhuo Li, Yifan Zhang, Muchen Li, Cewu Lu", "title": "TubeTK: Adopting Tubes to Track Multi-Object in a One-Step Training\n  Model", "comments": "CVPR-2020 oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-object tracking is a fundamental vision problem that has been studied\nfor a long time. As deep learning brings excellent performances to object\ndetection algorithms, Tracking by Detection (TBD) has become the mainstream\ntracking framework. Despite the success of TBD, this two-step method is too\ncomplicated to train in an end-to-end manner and induces many challenges as\nwell, such as insufficient exploration of video spatial-temporal information,\nvulnerability when facing object occlusion, and excessive reliance on detection\nresults. To address these challenges, we propose a concise end-to-end model\nTubeTK which only needs one step training by introducing the ``bounding-tube\"\nto indicate temporal-spatial locations of objects in a short video clip. TubeTK\nprovides a novel direction of multi-object tracking, and we demonstrate its\npotential to solve the above challenges without bells and whistles. We analyze\nthe performance of TubeTK on several MOT benchmarks and provide empirical\nevidence to show that TubeTK has the ability to overcome occlusions to some\nextent without any ancillary technologies like Re-ID. Compared with other\nmethods that adopt private detection results, our one-stage end-to-end model\nachieves state-of-the-art performances even if it adopts no ready-made\ndetection results. We hope that the proposed TubeTK model can serve as a simple\nbut strong alternative for video-based MOT task. The code and models are\navailable at https://github.com/BoPang1996/TubeTK.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 06:45:05 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Pang", "Bo", ""], ["Li", "Yizhuo", ""], ["Zhang", "Yifan", ""], ["Li", "Muchen", ""], ["Lu", "Cewu", ""]]}, {"id": "2006.05698", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Jagruti Patel, Radu Timofte", "title": "Rendering Natural Camera Bokeh Effect with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bokeh is an important artistic effect used to highlight the main object of\ninterest on the photo by blurring all out-of-focus areas. While DSLR and system\ncamera lenses can render this effect naturally, mobile cameras are unable to\nproduce shallow depth-of-field photos due to a very small aperture diameter of\ntheir optics. Unlike the current solutions simulating bokeh by applying\nGaussian blur to image background, in this paper we propose to learn a\nrealistic shallow focus technique directly from the photos produced by DSLR\ncameras. For this, we present a large-scale bokeh dataset consisting of 5K\nshallow / wide depth-of-field image pairs captured using the Canon 7D DSLR with\n50mm f/1.8 lenses. We use these images to train a deep learning model to\nreproduce a natural bokeh effect based on a single narrow-aperture image. The\nexperimental results show that the proposed approach is able to render a\nplausible non-uniform bokeh even in case of complex input data with multiple\nobjects. The dataset, pre-trained models and codes used in this paper are\navailable on the project website.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 07:28:06 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Ignatov", "Andrey", ""], ["Patel", "Jagruti", ""], ["Timofte", "Radu", ""]]}, {"id": "2006.05700", "submitter": "Sourav Garg", "authors": "Sourav Garg, Ben Harwood, Gaurangi Anand and Michael Milford", "title": "Delta Descriptors: Change-Based Place Representation for Robust Visual\n  Localization", "comments": "8 pages and 7 figures. Published in 2020 IEEE Robotics and Automation\n  Letters (RA-L)", "journal-ref": null, "doi": "10.1109/LRA.2020.3005627", "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual place recognition is challenging because there are so many factors\nthat can cause the appearance of a place to change, from day-night cycles to\nseasonal change to atmospheric conditions. In recent years a large range of\napproaches have been developed to address this challenge including deep-learnt\nimage descriptors, domain translation, and sequential filtering, all with\nshortcomings including generality and velocity-sensitivity. In this paper we\npropose a novel descriptor derived from tracking changes in any learned global\ndescriptor over time, dubbed Delta Descriptors. Delta Descriptors mitigate the\noffsets induced in the original descriptor matching space in an unsupervised\nmanner by considering temporal differences across places observed along a\nroute. Like all other approaches, Delta Descriptors have a shortcoming -\nvolatility on a frame to frame basis - which can be overcome by combining them\nwith sequential filtering methods. Using two benchmark datasets, we first\ndemonstrate the high performance of Delta Descriptors in isolation, before\nshowing new state-of-the-art performance when combined with sequence-based\nmatching. We also present results demonstrating the approach working with four\ndifferent underlying descriptor types, and two other beneficial properties of\nDelta Descriptors in comparison to existing techniques: their increased\ninherent robustness to variations in camera motion and a reduced rate of\nperformance degradation as dimensional reduction is applied. Source code is\nmade available at https://github.com/oravus/DeltaDescriptors.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 07:37:29 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 07:24:52 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Garg", "Sourav", ""], ["Harwood", "Ben", ""], ["Anand", "Gaurangi", ""], ["Milford", "Michael", ""]]}, {"id": "2006.05713", "submitter": "Jiahao Huo", "authors": "Jiahao Huo and Terence L van Zyl", "title": "Unique Faces Recognition in Videos", "comments": "Paper was accepted into Fusion 2020 conference but will only be\n  published after the virtual conference in July 2020. 7 pages long", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles face recognition in videos employing metric learning\nmethods and similarity ranking models. The paper compares the use of the\nSiamese network with contrastive loss and Triplet Network with triplet loss\nimplementing the following architectures: Google/Inception architecture, 3D\nConvolutional Network (C3D), and a 2-D Long short-term memory (LSTM) Recurrent\nNeural Network. We make use of still images and sequences from videos for\ntraining the networks and compare the performances implementing the above\narchitectures. The dataset used was the YouTube Face Database designed for\ninvestigating the problem of face recognition in videos. The contribution of\nthis paper is two-fold: to begin, the experiments have established 3-D\nConvolutional networks and 2-D LSTMs with the contrastive loss on image\nsequences do not outperform Google/Inception architecture with contrastive loss\nin top $n$ rank face retrievals with still images. However, the 3-D Convolution\nnetworks and 2-D LSTM with triplet Loss outperform the Google/Inception with\ntriplet loss in top $n$ rank face retrievals on the dataset; second, a Support\nVector Machine (SVM) was used in conjunction with the CNNs' learned feature\nrepresentations for facial identification. The results show that feature\nrepresentation learned with triplet loss is significantly better for n-shot\nfacial identification compared to contrastive loss. The most useful feature\nrepresentations for facial identification are from the 2-D LSTM with triplet\nloss. The experiments show that learning spatio-temporal features from video\nsequences is beneficial for facial recognition in videos.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 08:08:26 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Huo", "Jiahao", ""], ["van Zyl", "Terence L", ""]]}, {"id": "2006.05724", "submitter": "Matteo Poggi", "authors": "Filippo Aleotti, Giulio Zaccaroni, Luca Bartolomei, Matteo Poggi,\n  Fabio Tosi, Stefano Mattoccia", "title": "Real-time single image depth perception in the wild with handheld\n  devices", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth perception is paramount to tackle real-world problems, ranging from\nautonomous driving to consumer applications. For the latter, depth estimation\nfrom a single image represents the most versatile solution, since a standard\ncamera is available on almost any handheld device. Nonetheless, two main issues\nlimit its practical deployment: i) the low reliability when deployed\nin-the-wild and ii) the demanding resource requirements to achieve real-time\nperformance, often not compatible with such devices. Therefore, in this paper,\nwe deeply investigate these issues showing how they are both addressable\nadopting appropriate network design and training strategies -- also outlining\nhow to map the resulting networks on handheld devices to achieve real-time\nperformance. Our thorough evaluation highlights the ability of such fast\nnetworks to generalize well to new environments, a crucial feature required to\ntackle the extremely varied contexts faced in real applications. Indeed, to\nfurther support this evidence, we report experimental results concerning\nreal-time depth-aware augmented reality and image blurring with smartphones\nin-the-wild.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 08:30:20 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Aleotti", "Filippo", ""], ["Zaccaroni", "Giulio", ""], ["Bartolomei", "Luca", ""], ["Poggi", "Matteo", ""], ["Tosi", "Fabio", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "2006.05726", "submitter": "Corentin Kervadec", "authors": "Corentin Kervadec (imagine), Grigory Antipov, Moez Baccouche,\n  Christian Wolf (imagine)", "title": "Estimating semantic structure for the VQA answer space", "comments": "[WARNING] We want to notice the reader that additional experiments\n  (not in the paper) have shown that using a `random' semantic space performs\n  as much as the proposed semantic loss. This additional result question the\n  effectiveness of our method", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its appearance, Visual Question Answering (VQA, i.e. answering a\nquestion posed over an image), has always been treated as a classification\nproblem over a set of predefined answers. Despite its convenience, this\nclassification approach poorly reflects the semantics of the problem limiting\nthe answering to a choice between independent proposals, without taking into\naccount the similarity between them (e.g. equally penalizing for answering cat\nor German shepherd instead of dog). We address this issue by proposing (1) two\nmeasures of proximity between VQA classes, and (2) a corresponding loss which\ntakes into account the estimated proximity. This significantly improves the\ngeneralization of VQA models by reducing their language bias. In particular, we\nshow that our approach is completely model-agnostic since it allows consistent\nimprovements with three different VQA models. Finally, by combining our method\nwith a language bias reduction approach, we report SOTA-level performance on\nthe challenging VQAv2-CP dataset.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 08:32:56 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 10:33:21 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kervadec", "Corentin", "", "imagine"], ["Antipov", "Grigory", "", "imagine"], ["Baccouche", "Moez", "", "imagine"], ["Wolf", "Christian", "", "imagine"]]}, {"id": "2006.05728", "submitter": "Mert Kilickaya", "authors": "Mert Kilickaya and Arnold Smeulders", "title": "Diagnosing Rarity in Human-Object Interaction Detection", "comments": "Accepted at CVPR'20 Workshop on Learning from Limited Labels", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-object interaction (HOI) detection is a core task in computer vision.\nThe goal is to localize all human-object pairs and recognize their\ninteractions. An interaction defined by a <verb, noun> tuple leads to a\nlong-tailed visual recognition challenge since many combinations are rarely\nrepresented. The performance of the proposed models is limited especially for\nthe tail categories, but little has been done to understand the reason. To that\nend, in this paper, we propose to diagnose rarity in HOI detection. We propose\na three-step strategy, namely Detection, Identification and Recognition where\nwe carefully analyse the limiting factors by studying state-of-the-art models.\nOur findings indicate that detection and identification steps are altered by\nthe interaction signals like occlusion and relative location, as a result\nlimiting the recognition accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 08:35:29 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Kilickaya", "Mert", ""], ["Smeulders", "Arnold", ""]]}, {"id": "2006.05732", "submitter": "Benjamin Deguerre", "authors": "Benjamin Deguerre, Clement Chatelain, Gilles Gasso", "title": "Object Detection in the DCT Domain: is Luminance the Solution?", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in images has reached unprecedented performances. The\nstate-of-the-art methods rely on deep architectures that extract salient\nfeatures and predict bounding boxes enclosing the objects of interest. These\nmethods essentially run on RGB images. However, the RGB images are often\ncompressed by the acquisition devices for storage purpose and transfer\nefficiency. Hence, their decompression is required for object detectors. To\ngain in efficiency, this paper proposes to take advantage of the compressed\nrepresentation of images to carry out object detection usable in constrained\nresources conditions.\n  Specifically, we focus on JPEG images and propose a thorough analysis of\ndetection architectures newly designed in regard of the peculiarities of the\nJPEG norm. This leads to a $\\times 1.7$ speed up in comparison with a standard\nRGB-based architecture, while only reducing the detection performance by 5.5%.\nAdditionally, our empirical findings demonstrate that only part of the\ncompressed JPEG information, namely the luminance component, may be required to\nmatch detection accuracy of the full input methods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 08:43:40 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 17:39:03 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 08:09:24 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Deguerre", "Benjamin", ""], ["Chatelain", "Clement", ""], ["Gasso", "Gilles", ""]]}, {"id": "2006.05734", "submitter": "Wang Zeng", "authors": "Wang Zeng, Wanli Ouyang, Ping Luo, Wentao Liu, Xiaogang Wang", "title": "3D Human Mesh Regression with Dense Correspondence", "comments": "To appear at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D mesh of the human body from a single 2D image is an important\ntask with many applications such as augmented reality and Human-Robot\ninteraction. However, prior works reconstructed 3D mesh from global image\nfeature extracted by using convolutional neural network (CNN), where the dense\ncorrespondences between the mesh surface and the image pixels are missing,\nleading to suboptimal solution. This paper proposes a model-free 3D human mesh\nestimation framework, named DecoMR, which explicitly establishes the dense\ncorrespondence between the mesh and the local image features in the UV space\n(i.e. a 2D space used for texture mapping of 3D mesh). DecoMR first predicts\npixel-to-surface dense correspondence map (i.e., IUV image), with which we\ntransfer local features from the image space to the UV space. Then the\ntransferred local image features are processed in the UV space to regress a\nlocation map, which is well aligned with transferred features. Finally we\nreconstruct 3D human mesh from the regressed location map with a predefined\nmapping function. We also observe that the existing discontinuous UV map are\nunfriendly to the learning of network. Therefore, we propose a novel UV map\nthat maintains most of the neighboring relations on the original mesh surface.\nExperiments demonstrate that our proposed local feature alignment and\ncontinuous UV map outperforms existing 3D mesh based methods on multiple public\nbenchmarks. Code will be made available at\nhttps://github.com/zengwang430521/DecoMR\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 08:50:53 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 13:00:47 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zeng", "Wang", ""], ["Ouyang", "Wanli", ""], ["Luo", "Ping", ""], ["Liu", "Wentao", ""], ["Wang", "Xiaogang", ""]]}, {"id": "2006.05782", "submitter": "Yu Tian", "authors": "Yu Tian and Gaofeng Pan and Mohamed-Slim Alouini", "title": "Applying Deep-Learning-Based Computer Vision to Wireless Communications:\n  Methodologies, Opportunities, and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Deep learning (DL) has seen great success in the computer vision (CV) field,\nand related techniques have been used in security, healthcare, remote sensing,\nand many other fields. As a parallel development, visual data has become\nuniversal in daily life, easily generated by ubiquitous low-cost cameras.\nTherefore, exploring DL-based CV may yield useful information about objects,\nsuch as their number, locations, distribution, motion, etc. Intuitively,\nDL-based CV can also facilitate and improve the designs of wireless\ncommunications, especially in dynamic network scenarios. However, so far, such\nwork is rare in the literature. The primary purpose of this article, then, is\nto introduce ideas about applying DL-based CV in wireless communications to\nbring some novel degrees of freedom to both theoretical research and\nengineering applications. To illustrate how DL-based CV can be applied in\nwireless communications, an example of using a DL-based CV with a\nmillimeter-wave (mmWave) system is given to realize optimal mmWave\nmultiple-input and multiple-output (MIMO) beamforming in mobile scenarios. In\nthis example, we propose a framework to predict future beam indices from\npreviously observed beam indices and images of street views using ResNet,\n3-dimensional ResNext, and a long short-term memory network. The experimental\nresults show that our frameworks achieve much higher accuracy than the baseline\nmethod, and that visual data can significantly improve the performance of the\nMIMO beamforming system. Finally, we discuss the opportunities and challenges\nof applying DL-based CV in wireless communications.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 11:37:49 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 11:50:38 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 18:12:30 GMT"}, {"version": "v4", "created": "Wed, 2 Dec 2020 12:25:26 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Tian", "Yu", ""], ["Pan", "Gaofeng", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "2006.05787", "submitter": "Aayush Kafle", "authors": "Aashish Bhandari, Aayush Kafle, Pranjal Dhakal, Prateek Raj Joshi,\n  Dinesh Baniya Kshatri", "title": "Image Enhancement and Object Recognition for Night Vision Surveillance", "comments": "International Conference on Recent Trends in Computational\n  Engineering and Technologies, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition is a critical part of any surveillance system. It is the\nmatter of utmost concern to identify intruders and foreign objects in the area\nwhere surveillance is done. The performance of surveillance system using the\ntraditional camera in daylight is vastly superior as compared to night. The\nmain problem for surveillance during the night is the objects captured by\ntraditional cameras have low contrast against the background because of the\nabsence of ambient light in the visible spectrum. Due to that reason, the image\nis taken in low light condition using an Infrared Camera and the image is\nenhanced to obtain an image with higher contrast using different enhancing\nalgorithms based on the spatial domain. The enhanced image is then sent to the\nclassification process. The classification is done by using convolutional\nneural network followed by a fully connected layer of neurons. The accuracy of\nclassification after implementing different enhancement algorithms is compared\nin this paper.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 11:57:56 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bhandari", "Aashish", ""], ["Kafle", "Aayush", ""], ["Dhakal", "Pranjal", ""], ["Joshi", "Prateek Raj", ""], ["Kshatri", "Dinesh Baniya", ""]]}, {"id": "2006.05798", "submitter": "Jiuwen Zhu", "authors": "Jiuwen Zhu, Yuexiang Li, Yifan Hu, S. Kevin Zhou", "title": "Embedding Task Knowledge into 3D Neural Networks via Self-supervised\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning highly relies on the amount of annotated data. However,\nannotating medical images is extremely laborious and expensive. To this end,\nself-supervised learning (SSL), as a potential solution for deficient annotated\ndata, attracts increasing attentions from the community. However, SSL\napproaches often design a proxy task that is not necessarily related to target\ntask. In this paper, we propose a novel SSL approach for 3D medical image\nclassification, namely Task-related Contrastive Prediction Coding (TCPC), which\nembeds task knowledge into training 3D neural networks. The proposed TCPC first\nlocates the initial candidate lesions via supervoxel estimation using simple\nlinear iterative clustering. Then, we extract features from the sub-volume\ncropped around potential lesion areas, and construct a calibrated contrastive\npredictive coding scheme for self-supervised learning. Extensive experiments\nare conducted on public and private datasets. The experimental results\ndemonstrate the effectiveness of embedding lesion-related prior-knowledge into\nneural networks for 3D medical image classification.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 12:37:39 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zhu", "Jiuwen", ""], ["Li", "Yuexiang", ""], ["Hu", "Yifan", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2006.05838", "submitter": "Arnab Kumar Mondal", "authors": "Arnab Kumar Mondal, Himanshu Asnani, Parag Singla, Prathosh AP", "title": "To Regularize or Not To Regularize? The Bias Variance Trade-off in\n  Regularized AEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized Auto-Encoders (RAEs) form a rich class of neural generative\nmodels. They effectively model the joint-distribution between the data and the\nlatent space using an Encoder-Decoder combination, with regularization imposed\nin terms of a prior over the latent space. Despite their advantages, such as\nstability in training, the performance of AE based models has not reached the\nsuperior standards as that of the other generative models such as Generative\nAdversarial Networks (GANs). Motivated by this, we examine the effect of the\nlatent prior on the generation quality of deterministic AE models in this\npaper. Specifically, we consider the class of RAEs with deterministic\nEncoder-Decoder pairs, Wasserstein Auto-Encoders (WAE), and show that having a\nfixed prior distribution, \\textit{a priori}, oblivious to the dimensionality of\nthe `true' latent space, will lead to the infeasibility of the optimization\nproblem considered. Further, we show that, in the finite data regime, despite\nknowing the correct latent dimensionality, there exists a bias-variance\ntrade-off with any arbitrary prior imposition. As a remedy to both the issues\nmentioned above, we introduce an additional state space in the form of flexibly\nlearnable latent priors, in the optimization objective of the WAEs. We\nimplicitly learn the distribution of the latent prior jointly with the AE\ntraining, which not only makes the learning objective feasible but also\nfacilitates operation on different points of the bias-variance curve. We show\nthe efficacy of our model, called FlexAE, through several experiments on\nmultiple datasets, and demonstrate that it is the new state-of-the-art for the\nAE based generative models.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 14:00:14 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 10:56:48 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Mondal", "Arnab Kumar", ""], ["Asnani", "Himanshu", ""], ["Singla", "Parag", ""], ["AP", "Prathosh", ""]]}, {"id": "2006.05847", "submitter": "Dong Yang", "authors": "Dong Yang, Holger Roth, Ziyue Xu, Fausto Milletari, Ling Zhang,\n  Daguang Xu", "title": "Searching Learning Strategy with Reinforcement Learning for 3D Medical\n  Image Segmentation", "comments": "9 pages, 1 figures", "journal-ref": "Published at MICCAI 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) based approaches have been widely investigated and\ndeployed in medical image analysis. For example, fully convolutional neural\nnetworks (FCN) achieve the state-of-the-art performance in several applications\nof 2D/3D medical image segmentation. Even the baseline neural network models\n(U-Net, V-Net, etc.) have been proven to be very effective and efficient when\nthe training process is set up properly. Nevertheless, to fully exploit the\npotentials of neural networks, we propose an automated searching approach for\nthe optimal training strategy with reinforcement learning. The proposed\napproach can be utilized for tuning hyper-parameters, and selecting necessary\ndata augmentation with certain probabilities. The proposed approach is\nvalidated on several tasks of 3D medical image segmentation. The performance of\nthe baseline model is boosted after searching, and it can achieve comparable\naccuracy to other manually-tuned state-of-the-art segmentation approaches.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 14:24:06 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Yang", "Dong", ""], ["Roth", "Holger", ""], ["Xu", "Ziyue", ""], ["Milletari", "Fausto", ""], ["Zhang", "Ling", ""], ["Xu", "Daguang", ""]]}, {"id": "2006.05848", "submitter": "Yi Fang", "authors": "Xiang Li, Lingjing Wang, Yi Fang", "title": "Geometry-Aware Segmentation of Remote Sensing Images via Implicit Height\n  Estimation", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown the benefits of using additional elevation data\n(e.g., DSM) for enhancing the performance of the semantic segmentation of\naerial images. However, previous methods mostly adopt 3D elevation information\nas additional inputs. While in many real-world applications, one does not have\nthe corresponding DSM information at hand and the spatial resolution of\nacquired DSM images usually do not match the aerial images. To alleviate this\ndata constraint and also take advantage of 3D elevation information, in this\npaper, we introduce a geometry-aware segmentation model that achieves accurate\nsemantic labeling of aerial images via joint height estimation. Instead of\nusing a single-stream encoder-decoder network for semantic labeling, we design\na separate decoder branch to predict the height map and use the DSM images as\nside supervision to train this newly designed decoder branch. In this way, our\nmodel does not require DSM as model input and still benefits from the helpful\n3D geometric information during training. Moreover, we develop a new\ngeometry-aware convolution module that fuses the 3D geometric features from the\nheight decoder branch and the 2D contextual features from the semantic\nsegmentation branch. The fused feature embeddings can produce geometry-aware\nsegmentation maps with enhanced performance. Our model is trained with DSM\nimages as side supervision, while in the inference stage, it does not require\nDSM data and directly predicts the semantic labels in an end-to-end fashion.\nExperiments on ISPRS Vaihingen and Potsdam datasets demonstrate the\neffectiveness of the proposed method for the semantic segmentation of aerial\nimages. The proposed model achieves remarkable performance on both datasets\nwithout using any hand-crafted features or post-processing.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 14:24:10 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 01:48:22 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Li", "Xiang", ""], ["Wang", "Lingjing", ""], ["Fang", "Yi", ""]]}, {"id": "2006.05861", "submitter": "Fatemeh Abdolali", "authors": "Fatemeh Abdolali, Atefeh Shahroudnejad, Abhilash Rakkunedeth\n  Hareendranathan, Jacob L Jaremko, Michelle Noga, Kumaradevan Punithakumar", "title": "A systematic review on the role of artificial intelligence in\n  sonographic diagnosis of thyroid cancer: Past, present and future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thyroid cancer is common worldwide, with a rapid increase in prevalence\nacross North America in recent years. While most patients present with palpable\nnodules through physical examination, a large number of small and medium-sized\nnodules are detected by ultrasound examination. Suspicious nodules are then\nsent for biopsy through fine needle aspiration. Since biopsies are invasive and\nsometimes inconclusive, various research groups have tried to develop\ncomputer-aided diagnosis systems. Earlier approaches along these lines relied\non clinically relevant features that were manually identified by radiologists.\nWith the recent success of artificial intelligence (AI), various new methods\nare being developed to identify these features in thyroid ultrasound\nautomatically. In this paper, we present a systematic review of\nstate-of-the-art on AI application in sonographic diagnosis of thyroid cancer.\nThis review follows a methodology-based classification of the different\ntechniques available for thyroid cancer diagnosis. With more than 50 papers\nincluded in this review, we reflect on the trends and challenges of the field\nof sonographic diagnosis of thyroid malignancies and potential of\ncomputer-aided diagnosis to increase the impact of ultrasound applications on\nthe future of thyroid cancer diagnosis. Machine learning will continue to play\na fundamental role in the development of future thyroid cancer diagnosis\nframeworks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 14:38:05 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Abdolali", "Fatemeh", ""], ["Shahroudnejad", "Atefeh", ""], ["Hareendranathan", "Abhilash Rakkunedeth", ""], ["Jaremko", "Jacob L", ""], ["Noga", "Michelle", ""], ["Punithakumar", "Kumaradevan", ""]]}, {"id": "2006.05873", "submitter": "Gary White", "authors": "Gary White, Christian Cabrera, Andrei Palade, Fan Li, Siobhan Clarke", "title": "WasteNet: Waste Classification at the Edge for Smart Bins", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smart Bins have become popular in smart cities and campuses around the world.\nThese bins have a compaction mechanism that increases the bins' capacity as\nwell as automated real-time collection notifications. In this paper, we propose\nWasteNet, a waste classification model based on convolutional neural networks\nthat can be deployed on a low power device at the edge of the network, such as\na Jetson Nano. The problem of segregating waste is a big challenge for many\ncountries around the world. Automated waste classification at the edge allows\nfor fast intelligent decisions in smart bins without needing access to the\ncloud. Waste is classified into six categories: paper, cardboard, glass, metal,\nplastic and other. Our model achieves a 97\\% prediction accuracy on the test\ndataset. This level of classification accuracy will help to alleviate some\ncommon smart bin problems, such as recycling contamination, where different\ntypes of waste become mixed with recycling waste causing the bin to be\ncontaminated. It also makes the bins more user friendly as citizens do not have\nto worry about disposing their rubbish in the correct bin as the smart bin will\nbe able to make the decision for them.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 14:57:58 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["White", "Gary", ""], ["Cabrera", "Christian", ""], ["Palade", "Andrei", ""], ["Li", "Fan", ""], ["Clarke", "Siobhan", ""]]}, {"id": "2006.05888", "submitter": "Yeqi Bai", "authors": "Yeqi Bai, Tao Ma, Lipo Wang, Zhenjie Zhang", "title": "Speech Fusion to Face: Bridging the Gap Between Human's Vocal\n  Characteristics and Facial Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning technologies are now capable of generating realistic\nimages confusing humans, the research efforts are turning to the synthesis of\nimages for more concrete and application-specific purposes. Facial image\ngeneration based on vocal characteristics from speech is one of such important\nyet challenging tasks. It is the key enabler to influential use cases of image\ngeneration, especially for business in public security and entertainment.\nExisting solutions to the problem of speech2face renders limited image quality\nand fails to preserve facial similarity due to the lack of quality dataset for\ntraining and appropriate integration of vocal features. In this paper, we\ninvestigate these key technical challenges and propose Speech Fusion to Face,\nor SF2F in short, attempting to address the issue of facial image quality and\nthe poor connection between vocal feature domain and modern image generation\nmodels. By adopting new strategies on data model and training, we demonstrate\ndramatic performance boost over state-of-the-art solution, by doubling the\nrecall of individual identity, and lifting the quality score from 15 to 19\nbased on the mutual information score with VGGFace classifier.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 15:19:31 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bai", "Yeqi", ""], ["Ma", "Tao", ""], ["Wang", "Lipo", ""], ["Zhang", "Zhenjie", ""]]}, {"id": "2006.05895", "submitter": "Vishaal Udandarao", "authors": "Sarthak Bhagat, Vishaal Udandarao, Shagun Uppal", "title": "DisCont: Self-Supervised Visual Attribute Disentanglement using Context\n  Vectors", "comments": "Published at the 37th International Conference on Machine Learning\n  (ICML 2020) Workshop on ML Interpretability for Scientific Discovery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentangling the underlying feature attributes within an image with no prior\nsupervision is a challenging task. Models that can disentangle attributes well\nprovide greater interpretability and control. In this paper, we propose a\nself-supervised framework DisCont to disentangle multiple attributes by\nexploiting the structural inductive biases within images. Motivated by the\nrecent surge in contrastive learning paradigms, our model bridges the gap\nbetween self-supervised contrastive learning algorithms and unsupervised\ndisentanglement. We evaluate the efficacy of our approach, both qualitatively\nand quantitatively, on four benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 15:29:20 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 23:23:12 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Bhagat", "Sarthak", ""], ["Udandarao", "Vishaal", ""], ["Uppal", "Shagun", ""]]}, {"id": "2006.05918", "submitter": "Abenezer Girma Mr", "authors": "Abenezer Girma, Seifemichael Amsalu, Abrham Workineh, Mubbashar Khan,\n  Abdollah Homaifar", "title": "Deep Learning with Attention Mechanism for Predicting Driver Intention\n  at Intersection", "comments": "IEEE Intelligent Vehicles Symposium 2020 (IEEE IV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a driver's intention prediction near a road intersection is\nproposed. Our approach uses a deep bidirectional Long Short-Term Memory (LSTM)\nwith an attention mechanism model based on a hybrid-state system (HSS)\nframework. As intersection is considered to be as one of the major source of\nroad accidents, predicting a driver's intention at an intersection is very\ncrucial. Our method uses a sequence to sequence modeling with an attention\nmechanism to effectively exploit temporal information out of the time-series\nvehicular data including velocity and yaw-rate. The model then predicts ahead\nof time whether the target vehicle/driver will go straight, stop, or take right\nor left turn. The performance of the proposed approach is evaluated on a\nnaturalistic driving dataset and results show that our method achieves high\naccuracy as well as outperforms other methods. The proposed solution is\npromising to be applied in advanced driver assistance systems (ADAS) and as\npart of active safety system of autonomous vehicles.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 16:12:00 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Girma", "Abenezer", ""], ["Amsalu", "Seifemichael", ""], ["Workineh", "Abrham", ""], ["Khan", "Mubbashar", ""], ["Homaifar", "Abdollah", ""]]}, {"id": "2006.05926", "submitter": "Gil Ben-Artzi", "authors": "Gil Ben-Artzi", "title": "Separable Four Points Fundamental Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for RANSAC-based computation of the fundamental\nmatrix based on epipolar homography decomposition. We analyze the geometrical\nmeaning of the decomposition-based representation and show that it directly\ninduces a consecutive sampling strategy of two independent sets of\ncorrespondences. We show that our method guarantees a minimal number of\nevaluated hypotheses with respect to current minimal approaches, on the\ncondition that there are four correspondences on an image line. We validate our\napproach on real-world image pairs, providing fast and accurate results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 16:21:17 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 22:56:12 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Ben-Artzi", "Gil", ""]]}, {"id": "2006.05927", "submitter": "Vincent Lepetit", "authors": "Vincent Lepetit", "title": "Recent Advances in 3D Object and Hand Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object and hand pose estimation have huge potentials for Augmented\nReality, to enable tangible interfaces, natural interfaces, and blurring the\nboundaries between the real and virtual worlds. In this chapter, we present the\nrecent developments for 3D object and hand pose estimation using cameras, and\ndiscuss their abilities and limitations and the possible future development of\nthe field.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 16:25:28 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Lepetit", "Vincent", ""]]}, {"id": "2006.05929", "submitter": "Bo Zhao", "authors": "Bo Zhao, Konda Reddy Mopuri, Hakan Bilen", "title": "Dataset Condensation with Gradient Matching", "comments": null, "journal-ref": "International Conference on Learning Representations 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As the state-of-the-art machine learning methods in many fields rely on\nlarger datasets, storing datasets and training models on them become\nsignificantly more expensive. This paper proposes a training set synthesis\ntechnique for data-efficient learning, called Dataset Condensation, that learns\nto condense large dataset into a small set of informative synthetic samples for\ntraining deep neural networks from scratch. We formulate this goal as a\ngradient matching problem between the gradients of deep neural network weights\nthat are trained on the original and our synthetic data. We rigorously evaluate\nits performance in several computer vision benchmarks and demonstrate that it\nsignificantly outperforms the state-of-the-art methods. Finally we explore the\nuse of our method in continual learning and neural architecture search and\nreport promising gains when limited memory and computations are available.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 16:30:52 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 12:49:58 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 13:31:22 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhao", "Bo", ""], ["Mopuri", "Konda Reddy", ""], ["Bilen", "Hakan", ""]]}, {"id": "2006.05938", "submitter": "Shiqi Yang", "authors": "Shiqi Yang, Kai Wang, Luis Herranz, Joost van de Weijer", "title": "Simple and effective localized attribute representations for zero-shot\n  learning", "comments": "A journal version of the paper is arXiv:arXiv:2103.04704", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to discriminate images from unseen classes by\nexploiting relations to seen classes via their semantic descriptions. Some\nrecent papers have shown the importance of localized features together with\nfine-tuning the feature extractor to obtain discriminative and transferable\nfeatures. However, these methods require complex attention or part detection\nmodules to perform explicit localization in the visual space. In contrast, in\nthis paper we propose localizing representations in the semantic/attribute\nspace, with a simple but effective pipeline where localization is implicit.\nFocusing on attribute representations, we show that our method obtains\nstate-of-the-art performance on CUB and SUN datasets, and also achieves\ncompetitive results on AWA2 dataset, outperforming generally more complex\nmethods with explicit localization in the visual space. Our method can be\nimplemented easily, which can be used as a new baseline for zero shot-learning.\nIn addition, our localized representations are highly interpretable as\nattribute-specific heatmaps.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 16:46:12 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 18:07:23 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 09:44:15 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Yang", "Shiqi", ""], ["Wang", "Kai", ""], ["Herranz", "Luis", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2006.05941", "submitter": "Fan Zhang", "authors": "Fan Zhang, Licheng Jiao, Lingling Li, Fang Liu, and Xu Liu", "title": "MultiResolution Attention Extractor for Small Object Detection", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small objects are difficult to detect because of their low resolution and\nsmall size. The existing small object detection methods mainly focus on data\npreprocessing or narrowing the differences between large and small objects.\nInspired by human vision \"attention\" mechanism, we exploit two feature\nextraction methods to mine the most useful information of small objects. Both\nmethods are based on multiresolution feature extraction. We initially design\nand explore the soft attention method, but we find that its convergence speed\nis slow. Then we present the second method, an attention-based feature\ninteraction method, called a MultiResolution Attention Extractor (MRAE),\nshowing significant improvement as a generic feature extractor in small object\ndetection. After each building block in the vanilla feature extractor, we\nappend a small network to generate attention weights followed by a weighted-sum\noperation to get the final attention maps. Our attention-based feature\nextractor is 2.0 times the AP of the \"hard\" attention counterpart (plain\narchitecture) on the COCO small object detection benchmark, proving that MRAE\ncan capture useful location and contextual information through adaptive\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 16:47:56 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zhang", "Fan", ""], ["Jiao", "Licheng", ""], ["Li", "Lingling", ""], ["Liu", "Fang", ""], ["Liu", "Xu", ""]]}, {"id": "2006.06015", "submitter": "Miguel Monteiro", "authors": "Miguel Monteiro, Lo\\\"ic Le Folgoc, Daniel Coelho de Castro, Nick\n  Pawlowski, Bernardo Marques, Konstantinos Kamnitsas, Mark van der Wilk, Ben\n  Glocker", "title": "Stochastic Segmentation Networks: Modelling Spatially Correlated\n  Aleatoric Uncertainty", "comments": "Published at Neurips2020. 17 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image segmentation, there is often more than one plausible solution for a\ngiven input. In medical imaging, for example, experts will often disagree about\nthe exact location of object boundaries. Estimating this inherent uncertainty\nand predicting multiple plausible hypotheses is of great interest in many\napplications, yet this ability is lacking in most current deep learning\nmethods. In this paper, we introduce stochastic segmentation networks (SSNs),\nan efficient probabilistic method for modelling aleatoric uncertainty with any\nimage segmentation network architecture. In contrast to approaches that produce\npixel-wise estimates, SSNs model joint distributions over entire label maps and\nthus can generate multiple spatially coherent hypotheses for a single image. By\nusing a low-rank multivariate normal distribution over the logit space to model\nthe probability of the label map given the image, we obtain a spatially\nconsistent probability distribution that can be efficiently computed by a\nneural network without any changes to the underlying architecture. We tested\nour method on the segmentation of real-world medical data, including lung\nnodules in 2D CT and brain tumours in 3D multimodal MRI scans. SSNs outperform\nstate-of-the-art for modelling correlated uncertainty in ambiguous images while\nbeing much simpler, more flexible, and more efficient.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 18:06:41 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 16:28:58 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Monteiro", "Miguel", ""], ["Folgoc", "Lo\u00efc Le", ""], ["de Castro", "Daniel Coelho", ""], ["Pawlowski", "Nick", ""], ["Marques", "Bernardo", ""], ["Kamnitsas", "Konstantinos", ""], ["van der Wilk", "Mark", ""], ["Glocker", "Ben", ""]]}, {"id": "2006.06017", "submitter": "Georgios Evangelidis", "authors": "Georgios Evangelidis, Branislav Micusik", "title": "Revisiting visual-inertial structure from motion for odometry and SLAM\n  initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an efficient closed-form solution for the state initialization\nin visual-inertial odometry (VIO) and simultaneous localization and mapping\n(SLAM) is presented. Unlike the state-of-the-art, we do not derive linear\nequations from triangulating pairs of point observations. Instead, we build on\na direct triangulation of the unknown $3D$ point paired with each of its\nobservations. We show and validate the high impact of such a simple difference.\nThe resulting linear system has a simpler structure and the solution through\nanalytic elimination only requires solving a $6\\times 6$ linear system (or $9\n\\times 9$ when accelerometer bias is included). In addition, all the\nobservations of every scene point are jointly related, thereby leading to a\nless biased and more robust solution. The proposed formulation attains up to\n$50$ percent decreased velocity and point reconstruction error compared to the\nstandard closed-form solver, while it is $4\\times$ faster for a $7$-frame set.\nApart from the inherent efficiency, fewer iterations are needed by any further\nnon-linear refinement thanks to better parameter initialization. In this\ncontext, we provide the analytic Jacobians for a non-linear optimizer that\noptionally refines the initial parameters. The superior performance of the\nproposed solver is established by quantitative comparisons with the\nstate-of-the-art solver.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 18:08:22 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 17:58:46 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Evangelidis", "Georgios", ""], ["Micusik", "Branislav", ""]]}, {"id": "2006.06028", "submitter": "Krishna Kanth Nakka", "authors": "Krishna Kanth Nakka and Mathieu Salzmann", "title": "Towards Robust Fine-grained Recognition by Maximal Separation of\n  Discriminative Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks have been widely studied for general classification\ntasks, but remain unexplored in the context of fine-grained recognition, where\nthe inter-class similarities facilitate the attacker's task. In this paper, we\nidentify the proximity of the latent representations of different classes in\nfine-grained recognition networks as a key factor to the success of adversarial\nattacks. We therefore introduce an attention-based regularization mechanism\nthat maximally separates the discriminative latent features of different\nclasses while minimizing the contribution of the non-discriminative regions to\nthe final class prediction. As evidenced by our experiments, this allows us to\nsignificantly improve robustness to adversarial attacks, to the point of\nmatching or even surpassing that of adversarial training, but without requiring\naccess to adversarial samples.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 18:34:45 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Nakka", "Krishna Kanth", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2006.06038", "submitter": "Yuankai Huo", "authors": "Ruining Deng, Haichun Yang, Aadarsh Jha, Yuzhe Lu, Peng Chu, Agnes B.\n  Fogo, Yuankai Huo", "title": "Map3D: Registration Based Multi-Object Tracking on 3D Serial Whole Slide\n  Images", "comments": "Accepted by IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a long pursuit for precise and reproducible glomerular\nquantification on renal pathology to leverage both research and practice. When\ndigitizing the biopsy tissue samples using whole slide imaging (WSI), a set of\nserial sections from the same tissue can be acquired as a stack of images,\nsimilar to frames in a video. In radiology, the stack of images (e.g., computed\ntomography) are naturally used to provide 3D context for organs, tissues, and\ntumors. In pathology, it is appealing to do a similar 3D assessment. However,\nthe 3D identification and association of large-scale glomeruli on renal\npathology is challenging due to large tissue deformation, missing tissues, and\nartifacts from WSI. In this paper, we propose a novel Multi-object Association\nfor Pathology in 3D (Map3D) method for automatically identifying and\nassociating large-scale cross-sections of 3D objects from routine serial\nsectioning and WSI. The innovations of the Map3D method are three-fold: (1) the\nlarge-scale glomerular association is formed as a new multi-object tracking\n(MOT) perspective; (2) the quality-aware whole series registration is proposed\nto not only provide affinity estimation but also offer automatic kidney-wise\nquality assurance (QA) for registration; (3) a dual-path association method is\nproposed to tackle the large deformation, missing tissues, and artifacts during\ntracking. To the best of our knowledge, the Map3D method is the first approach\nthat enables automatic and large-scale glomerular association across 3D serial\nsectioning using WSI. Our proposed method Map3D achieved MOTA= 44.6, which is\n12.1% higher than the non deep learning benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 19:31:02 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 19:28:44 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Deng", "Ruining", ""], ["Yang", "Haichun", ""], ["Jha", "Aadarsh", ""], ["Lu", "Yuzhe", ""], ["Chu", "Peng", ""], ["Fogo", "Agnes B.", ""], ["Huo", "Yuankai", ""]]}, {"id": "2006.06059", "submitter": "Tian Han", "authors": "Tian Han, Erik Nijkamp, Linqi Zhou, Bo Pang, Song-Chun Zhu, Ying Nian\n  Wu", "title": "Joint Training of Variational Auto-Encoder and Latent Energy-Based Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a joint training method to learn both the variational\nauto-encoder (VAE) and the latent energy-based model (EBM). The joint training\nof VAE and latent EBM are based on an objective function that consists of three\nKullback-Leibler divergences between three joint distributions on the latent\nvector and the image, and the objective function is of an elegant symmetric and\nanti-symmetric form of divergence triangle that seamlessly integrates\nvariational and adversarial learning. In this joint training scheme, the latent\nEBM serves as a critic of the generator model, while the generator model and\nthe inference model in VAE serve as the approximate synthesis sampler and\ninference sampler of the latent EBM. Our experiments show that the joint\ntraining greatly improves the synthesis quality of the VAE. It also enables\nlearning of an energy function that is capable of detecting out of sample\nexamples for anomaly detection.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 20:32:25 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Han", "Tian", ""], ["Nijkamp", "Erik", ""], ["Zhou", "Linqi", ""], ["Pang", "Bo", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "2006.06061", "submitter": "Chris Finlay", "authors": "Ryan Campbell, Chris Finlay, Adam M Oberman", "title": "Deterministic Gaussian Averaged Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic method to compute the Gaussian average of neural\nnetworks used in regression and classification. Our method is based on an\nequivalence between training with a particular regularized loss, and the\nexpected values of Gaussian averages. We use this equivalence to certify models\nwhich perform well on clean data but are not robust to adversarial\nperturbations. In terms of certified accuracy and adversarial robustness, our\nmethod is comparable to known stochastic methods such as randomized smoothing,\nbut requires only a single model evaluation during inference.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 20:53:31 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Campbell", "Ryan", ""], ["Finlay", "Chris", ""], ["Oberman", "Adam M", ""]]}, {"id": "2006.06072", "submitter": "Florian Jug", "authors": "Mangal Prakash, Alexander Krull, Florian Jug", "title": "Fully Unsupervised Diversity Denoising with Convolutional Variational\n  Autoencoders", "comments": "44 pages including supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning based methods have emerged as the indisputable leaders for\nvirtually all image restoration tasks. Especially in the domain of microscopy\nimages, various content-aware image restoration (CARE) approaches are now used\nto improve the interpretability of acquired data. Naturally, there are\nlimitations to what can be restored in corrupted images, and like for all\ninverse problems, many potential solutions exist, and one of them must be\nchosen. Here, we propose DivNoising, a denoising approach based on fully\nconvolutional variational autoencoders (VAEs), overcoming the problem of having\nto choose a single solution by predicting a whole distribution of denoised\nimages. First we introduce a principled way of formulating the unsupervised\ndenoising problem within the VAE framework by explicitly incorporating imaging\nnoise models into the decoder. Our approach is fully unsupervised, only\nrequiring noisy images and a suitable description of the imaging noise\ndistribution. We show that such a noise model can either be measured,\nbootstrapped from noisy data, or co-learned during training. If desired,\nconsensus predictions can be inferred from a set of DivNoising predictions,\nleading to competitive results with other unsupervised methods and, on\noccasion, even with the supervised state-of-the-art. DivNoising samples from\nthe posterior enable a plethora of useful applications. We are (i) showing\ndenoising results for 13 datasets, (ii) discussing how optical character\nrecognition (OCR) applications can benefit from diverse predictions, and are\n(iii) demonstrating how instance cell segmentation improves when using diverse\nDivNoising predictions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 21:28:13 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 12:28:08 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Prakash", "Mangal", ""], ["Krull", "Alexander", ""], ["Jug", "Florian", ""]]}, {"id": "2006.06091", "submitter": "Yu Huang", "authors": "Yu Huang and Yue Chen", "title": "Autonomous Driving with Deep Learning: A Survey of State-of-Art\n  Technologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007,\nautonomous driving has been the most active field of AI applications. Almost at\nthe same time, deep learning has made breakthrough by several pioneers, three\nof them (also called fathers of deep learning), Hinton, Bengio and LeCun, won\nACM Turin Award in 2019. This is a survey of autonomous driving technologies\nwith deep learning methods. We investigate the major fields of self-driving\nsystems, such as perception, mapping and localization, prediction, planning and\ncontrol, simulation, V2X and safety etc. Due to the limited space, we focus the\nanalysis on several key areas, i.e. 2D and 3D object detection in perception,\ndepth estimation from cameras, multiple sensor fusion on the data, feature and\ntask level respectively, behavior modelling and prediction of vehicle driving\nand pedestrian trajectories.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 22:21:57 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 17:19:48 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 04:38:43 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Huang", "Yu", ""], ["Chen", "Yue", ""]]}, {"id": "2006.06113", "submitter": "Nikhil Churamani", "authors": "Nikhil Churamani", "title": "Continual Learning for Affective Computing", "comments": "Accepted at the Doctoral Consortium for the IEEE International\n  Conference on Automatic Face and Gesture Recognition (FG), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world application requires affect perception models to be sensitive to\nindividual differences in expression. As each user is different and expresses\ndifferently, these models need to personalise towards each individual to\nadequately capture their expressions and thus, model their affective state.\nDespite high performance on benchmarks, current approaches fall short in such\nadaptation. In this work, we propose the use of Continual Learning (CL) for\naffective computing as a paradigm for developing personalised affect\nperception.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 23:36:06 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 06:42:26 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Churamani", "Nikhil", ""]]}, {"id": "2006.06119", "submitter": "Huang Hu", "authors": "Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang and Daxin Jiang", "title": "Dance Revolution: Long-Term Dance Generation with Music via Curriculum\n  Learning", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dancing to music is one of human's innate abilities since ancient times. In\nmachine learning research, however, synthesizing dance movements from music is\na challenging problem. Recently, researchers synthesize human motion sequences\nthrough autoregressive models like recurrent neural network (RNN). Such an\napproach often generates short sequences due to an accumulation of prediction\nerrors that are fed back into the neural network. This problem becomes even\nmore severe in the long motion sequence generation. Besides, the consistency\nbetween dance and music in terms of style, rhythm and beat is yet to be taken\ninto account during modeling. In this paper, we formalize the music-conditioned\ndance generation as a sequence-to-sequence learning problem and devise a novel\nseq2seq architecture to efficiently process long sequences of music features\nand capture the fine-grained correspondence between music and dance.\nFurthermore, we propose a novel curriculum learning strategy to alleviate error\naccumulation of autoregressive models in long motion sequence generation, which\ngently changes the training process from a fully guided teacher-forcing scheme\nusing the previous ground-truth movements, towards a less guided autoregressive\nscheme mostly using the generated movements instead. Extensive experiments show\nthat our approach significantly outperforms the existing state-of-the-arts on\nautomatic metrics and human evaluation. We also make a demo video to\ndemonstrate the superior performance of our proposed approach at\nhttps://www.youtube.com/watch?v=lmE20MEheZ8.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 00:08:25 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 06:20:54 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 09:02:18 GMT"}, {"version": "v4", "created": "Thu, 15 Oct 2020 11:33:39 GMT"}, {"version": "v5", "created": "Sun, 17 Jan 2021 04:21:31 GMT"}, {"version": "v6", "created": "Sun, 7 Feb 2021 09:35:07 GMT"}, {"version": "v7", "created": "Sun, 14 Mar 2021 07:56:13 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Huang", "Ruozi", ""], ["Hu", "Huang", ""], ["Wu", "Wei", ""], ["Sawada", "Kei", ""], ["Zhang", "Mi", ""], ["Jiang", "Daxin", ""]]}, {"id": "2006.06134", "submitter": "Mohib Ullah", "authors": "Mohib Ullah, Maqsood Mahmud, Habib Ullah, Kashif Ahmad, Ali Shariq\n  Imran, Faouzi Alaya Cheikh", "title": "Kalman Filter Based Multiple Person Head Tracking", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For multi-target tracking, target representation plays a crucial rule in\nperformance. State-of-the-art approaches rely on the deep learning-based visual\nrepresentation that gives an optimal performance at the cost of high\ncomputational complexity. In this paper, we come up with a simple yet effective\ntarget representation for human tracking. Our inspiration comes from the fact\nthat the human body goes through severe deformation and inter/intra occlusion\nover the passage of time. So, instead of tracking the whole body part, a\nrelative rigid organ tracking is selected for tracking the human over an\nextended period of time. Hence, we followed the tracking-by-detection paradigm\nand generated the target hypothesis of only the spatial locations of heads in\nevery frame. After the localization of head location, a Kalman filter with a\nconstant velocity motion model is instantiated for each target that follows the\ntemporal evolution of the targets in the scene. For associating the targets in\nthe consecutive frames, combinatorial optimization is used that associates the\ncorresponding targets in a greedy fashion. Qualitative results are evaluated on\nfour challenging video surveillance dataset and promising results has been\nachieved.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 00:54:45 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Ullah", "Mohib", ""], ["Mahmud", "Maqsood", ""], ["Ullah", "Habib", ""], ["Ahmad", "Kashif", ""], ["Imran", "Ali Shariq", ""], ["Cheikh", "Faouzi Alaya", ""]]}, {"id": "2006.06154", "submitter": "John Tencer", "authors": "John Tencer and Kevin Potter", "title": "A Tailored Convolutional Neural Network for Nonlinear Manifold Learning\n  of Computational Physics Data using Unstructured Spatial Discretizations", "comments": "Preprint", "journal-ref": null, "doi": "10.1137/20M1344263", "report-no": null, "categories": "physics.comp-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonlinear manifold learning technique based on deep\nconvolutional autoencoders that is appropriate for model order reduction of\nphysical systems in complex geometries. Convolutional neural networks have\nproven to be highly advantageous for compressing data arising from systems\ndemonstrating a slow-decaying Kolmogorov n-width. However, these networks are\nrestricted to data on structured meshes. Unstructured meshes are often required\nfor performing analyses of real systems with complex geometry. Our custom graph\nconvolution operators based on the available differential operators for a given\nspatial discretization effectively extend the application space of deep\nconvolutional autoencoders to systems with arbitrarily complex geometry that\nare typically discretized using unstructured meshes. We propose sets of\nconvolution operators based on the spatial derivative operators for the\nunderlying spatial discretization, making the method particularly well suited\nto data arising from the solution of partial differential equations. We\ndemonstrate the method using examples from heat transfer and fluid mechanics\nand show better than an order of magnitude improvement in accuracy over linear\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 02:19:34 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 18:04:19 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 23:47:43 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Tencer", "John", ""], ["Potter", "Kevin", ""]]}, {"id": "2006.06156", "submitter": "Loic Royer", "authors": "Hirofumi Kobayashi, Ahmet Can Solak, Joshua Batson, Loic A. Royer", "title": "Image Deconvolution via Noise-Tolerant Self-Supervised Inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for solving inverse problems in the presence\nof noise that requires no signal prior, no noise estimate, and no clean\ntraining data. We only require that the forward model be available and that the\nnoise be statistically independent across measurement dimensions. We build upon\nthe theory of $\\mathcal{J}$-invariant functions (Batson & Royer 2019,\narXiv:1901.11365) and show how self-supervised denoising \\emph{\\`a la}\nNoise2Self is a special case of learning a noise-tolerant pseudo-inverse of the\nidentity. We demonstrate our approach by showing how a convolutional neural\nnetwork can be taught in a self-supervised manner to deconvolve images and\nsurpass in image quality classical inversion schemes such as Lucy-Richardson\ndeconvolution.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 02:27:23 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Kobayashi", "Hirofumi", ""], ["Solak", "Ahmet Can", ""], ["Batson", "Joshua", ""], ["Royer", "Loic A.", ""]]}, {"id": "2006.06158", "submitter": "Chethan M Parameshwara", "authors": "Chethan M. Parameshwara, Nitin J. Sanket, Chahat Deep Singh, Cornelia\n  Ferm\\\"uller, and Yiannis Aloimonos", "title": "0-MMS: Zero-Shot Multi-Motion Segmentation With A Monocular Event Camera", "comments": "7 pages, 6 figures, 4 tables, Under review ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of moving objects in dynamic scenes is a key process in scene\nunderstanding for navigation tasks. Classical cameras suffer from motion blur\nin such scenarios rendering them effete. On the contrary, event cameras,\nbecause of their high temporal resolution and lack of motion blur, are\ntailor-made for this problem. We present an approach for monocular multi-motion\nsegmentation, which combines bottom-up feature tracking and top-down motion\ncompensation into a unified pipeline, which is the first of its kind to our\nknowledge. Using the events within a time-interval, our method segments the\nscene into multiple motions by splitting and merging. We further speed up our\nmethod by using the concept of motion propagation and cluster keyslices.\n  The approach was successfully evaluated on both challenging real-world and\nsynthetic scenarios from the EV-IMO, EED, and MOD datasets and outperformed the\nstate-of-the-art detection rate by 12\\%, achieving a new state-of-the-art\naverage detection rate of 81.06%, 94.2% and 82.35% on the aforementioned\ndatasets. To enable further research and systematic evaluation of multi-motion\nsegmentation, we present and open-source a new dataset/benchmark called MOD++,\nwhich includes challenging sequences and extensive data stratification in-terms\nof camera and object motion, velocity magnitudes, direction, and rotational\nspeeds.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 02:34:29 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 01:58:31 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Parameshwara", "Chethan M.", ""], ["Sanket", "Nitin J.", ""], ["Singh", "Chahat Deep", ""], ["Ferm\u00fcller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "2006.06175", "submitter": "Karren Yang", "authors": "Karren Yang, Bryan Russell, Justin Salamon", "title": "Telling Left from Right: Learning Spatial Correspondence of Sight and\n  Sound", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised audio-visual learning aims to capture useful representations\nof video by leveraging correspondences between visual and audio inputs.\nExisting approaches have focused primarily on matching semantic information\nbetween the sensory streams. We propose a novel self-supervised task to\nleverage an orthogonal principle: matching spatial information in the audio\nstream to the positions of sound sources in the visual stream. Our approach is\nsimple yet effective. We train a model to determine whether the left and right\naudio channels have been flipped, forcing it to reason about spatial\nlocalization across the visual and audio streams. To train and evaluate our\nmethod, we introduce a large-scale video dataset, YouTube-ASMR-300K, with\nspatial audio comprising over 900 hours of footage. We demonstrate that\nunderstanding spatial correspondence enables models to perform better on three\naudio-visual tasks, achieving quantitative gains over supervised and\nself-supervised baselines that do not leverage spatial audio cues. We also show\nhow to extend our self-supervised approach to 360 degree videos with ambisonic\naudio.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 04:00:24 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 03:12:16 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Yang", "Karren", ""], ["Russell", "Bryan", ""], ["Salamon", "Justin", ""]]}, {"id": "2006.06177", "submitter": "Yifan Peng", "authors": "Yifan Peng, Yu-Xing Tang, Sungwon Lee, Yingying Zhu, Ronald M.\n  Summers, Zhiyong Lu", "title": "COVID-19-CT-CXR: a freely accessible and weakly labeled chest X-ray and\n  CT image collection on COVID-19 from biomedical literature", "comments": "Accepted by IEEE Transactions on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest threat to global health is the COVID-19 outbreak. Although there\nexist large datasets of chest X-rays (CXR) and computed tomography (CT) scans,\nfew COVID-19 image collections are currently available due to patient privacy.\nAt the same time, there is a rapid growth of COVID-19-relevant articles in the\nbiomedical literature. Here, we present COVID-19-CT-CXR, a public database of\nCOVID-19 CXR and CT images, which are automatically extracted from\nCOVID-19-relevant articles from the PubMed Central Open Access (PMC-OA) Subset.\nWe extracted figures, associated captions, and relevant figure descriptions in\nthe article and separated compound figures into subfigures. We also designed a\ndeep-learning model to distinguish them from other figure types and to classify\nthem accordingly. The final database includes 1,327 CT and 263 CXR images (as\nof May 9, 2020) with their relevant text. To demonstrate the utility of\nCOVID-19-CT-CXR, we conducted four case studies. (1) We show that\nCOVID-19-CT-CXR, when used as additional training data, is able to contribute\nto improved DL performance for the classification of COVID-19 and non-COVID-19\nCT. (2) We collected CT images of influenza and trained a DL baseline to\ndistinguish a diagnosis of COVID-19, influenza, or normal or other types of\ndiseases on CT. (3) We trained an unsupervised one-class classifier from\nnon-COVID-19 CXR and performed anomaly detection to detect COVID-19 CXR. (4)\nFrom text-mined captions and figure descriptions, we compared clinical symptoms\nand clinical findings of COVID-19 vs. those of influenza to demonstrate the\ndisease differences in the scientific publications. We believe that our work is\ncomplementary to existing resources and hope that it will contribute to medical\nimage analysis of the COVID-19 pandemic. The dataset, code, and DL models are\npublicly available at https://github.com/ncbi-nlp/COVID-19-CT-CXR.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 04:00:56 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 03:03:55 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Peng", "Yifan", ""], ["Tang", "Yu-Xing", ""], ["Lee", "Sungwon", ""], ["Zhu", "Yingying", ""], ["Summers", "Ronald M.", ""], ["Lu", "Zhiyong", ""]]}, {"id": "2006.06185", "submitter": "Joseph Chuang", "authors": "Jo Chuang, Qian Dong", "title": "JIT-Masker: Efficient Online Distillation for Background Matting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a real-time portrait matting pipeline for everyday use,\nparticularly for \"virtual backgrounds\" in video conferences. Existing\nsegmentation and matting methods prioritize accuracy and quality over\nthroughput and efficiency, and our pipeline enables trading off a controllable\namount of accuracy for better throughput by leveraging online distillation on\nthe input video stream. We construct our own dataset of simulated video calls\nin various scenarios, and show that our approach delivers a 5x speedup over a\nsaliency detection based pipeline in a non-GPU accelerated setting while\ndelivering higher quality results. We demonstrate that an online distillation\napproach can feasibly work as part of a general, consumer level product as a\n\"virtual background\" tool. Our public implementation is at\nhttps://github.com/josephch405/jit-masker.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 04:28:09 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Chuang", "Jo", ""], ["Dong", "Qian", ""]]}, {"id": "2006.06195", "submitter": "Zhe Gan", "authors": "Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, Jingjing Liu", "title": "Large-Scale Adversarial Training for Vision-and-Language Representation\n  Learning", "comments": "NeurIPS 2020 Spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present VILLA, the first known effort on large-scale adversarial training\nfor vision-and-language (V+L) representation learning. VILLA consists of two\ntraining stages: (i) task-agnostic adversarial pre-training; followed by (ii)\ntask-specific adversarial finetuning. Instead of adding adversarial\nperturbations on image pixels and textual tokens, we propose to perform\nadversarial training in the embedding space of each modality. To enable\nlarge-scale training, we adopt the \"free\" adversarial training strategy, and\ncombine it with KL-divergence-based regularization to promote higher invariance\nin the embedding space. We apply VILLA to current best-performing V+L models,\nand achieve new state of the art on a wide range of tasks, including Visual\nQuestion Answering, Visual Commonsense Reasoning, Image-Text Retrieval,\nReferring Expression Comprehension, Visual Entailment, and NLVR2.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 05:14:35 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 18:12:53 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Gan", "Zhe", ""], ["Chen", "Yen-Chun", ""], ["Li", "Linjie", ""], ["Zhu", "Chen", ""], ["Cheng", "Yu", ""], ["Liu", "Jingjing", ""]]}, {"id": "2006.06196", "submitter": "Huali Xu", "authors": "Huali Xu, Xiangdong Su, Meng Wang, Xiang Hao, Guanglai Gao", "title": "An Edge Information and Mask Shrinking Based Image Inpainting Approach", "comments": "Accepted by ICME2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the image inpainting task, the ability to repair both high-frequency and\nlow-frequency information in the missing regions has a substantial influence on\nthe quality of the restored image. However, existing inpainting methods usually\nfail to consider both high-frequency and low-frequency information\nsimultaneously. To solve this problem, this paper proposes edge information and\nmask shrinking based image inpainting approach, which consists of two models.\nThe first model is an edge generation model used to generate complete edge\ninformation from the damaged image, and the second model is an image completion\nmodel used to fix the missing regions with the generated edge information and\nthe valid contents of the damaged image. The mask shrinking strategy is\nemployed in the image completion model to track the areas to be repaired. The\nproposed approach is evaluated qualitatively and quantitatively on the dataset\nPlaces2. The result shows our approach outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 05:15:52 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Xu", "Huali", ""], ["Su", "Xiangdong", ""], ["Wang", "Meng", ""], ["Hao", "Xiang", ""], ["Gao", "Guanglai", ""]]}, {"id": "2006.06200", "submitter": "Yi Fang", "authors": "Lingjing Wang, Xiang Li, Yi Fang", "title": "Unsupervised Learning of 3D Point Set Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud registration is the process of aligning a pair of point sets via\nsearching for a geometric transformation. Recent works leverage the power of\ndeep learning for registering a pair of point sets. However, unfortunately,\ndeep learning models often require a large number of ground truth labels for\ntraining. Moreover, for a pair of source and target point sets, existing deep\nlearning mechanisms require explicitly designed encoders to extract both deep\nspatial features from unstructured point clouds and their spatial correlation\nrepresentation, which is further fed to a decoder to regress the desired\ngeometric transformation for point set alignment. To further enhance deep\nlearning models for point set registration, this paper proposes Deep-3DAligner,\na novel unsupervised registration framework based on a newly introduced deep\nSpatial Correlation Representation (SCR) feature. The SCR feature describes the\ngeometric essence of the spatial correlation between source and target point\nsets in an encoding-free manner. More specifically, our method starts with\noptimizing a randomly initialized latent SCR feature, which is then decoded to\na geometric transformation (i.e., rotation and translation) to align source and\ntarget point sets. Our Deep-3DAligner jointly updates the SCR feature and\nweights of the transformation decoder towards the minimization of an\nunsupervised alignment loss. We conducted experiments on the ModelNet40\ndatasets to validate the performance of our unsupervised Deep-3DAligner for\npoint set registration. The results demonstrated that, even without ground\ntruth and any assumption of a direct correspondence between source and target\npoint sets for training, our proposed approach achieved comparative performance\ncompared to most recent supervised state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 05:21:38 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Wang", "Lingjing", ""], ["Li", "Xiang", ""], ["Fang", "Yi", ""]]}, {"id": "2006.06201", "submitter": "Paul Peyramaure", "authors": "Alexy Carlier (IETR), Paul Peyramaure (IETR), Ketty Favre (UR1),\n  Muriel Pressigout (IETR)", "title": "Fall Detector Adapted to Nursing Home Needs through an Optical-Flow\n  based CNN", "comments": null, "journal-ref": "42nd Annual International Conference of the IEEE Engineering in\n  Medicine and Biology Society - EMBC2020, Jul 2020, Montreal, Canada", "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fall detection in specialized homes for the elderly is challenging.\nVision-based fall detection solutions have a significant advantage over\nsensor-based ones as they do not instrument the resident who can suffer from\nmental diseases. This work is part of a project intended to deploy fall\ndetection solutions in nursing homes. The proposed solution, based on Deep\nLearning, is built on a Convolutional Neural Network (CNN) trained to maximize\na sensitivity-based metric. This work presents the requirements from the\nmedical side and how it impacts the tuning of a CNN. Results highlight the\nimportance of the temporal aspect of a fall. Therefore, a custom metric adapted\nto this use case and an implementation of a decision-making process are\nproposed in order to best meet the medical teams requirements. Clinical\nrelevance This work presents a fall detection solution enabled to detect 86.2%\nof falls while producing only 11.6% of false alarms in average on the\nconsidered databases.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 05:23:12 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Carlier", "Alexy", "", "IETR"], ["Peyramaure", "Paul", "", "IETR"], ["Favre", "Ketty", "", "UR1"], ["Pressigout", "Muriel", "", "IETR"]]}, {"id": "2006.06244", "submitter": "Youngmin Baek", "authors": "Youngmin Baek, Daehyun Nam, Sungrae Park, Junyeop Lee, Seung Shin,\n  Jeonghun Baek, Chae Young Lee, Hwalsuk Lee", "title": "CLEval: Character-Level Evaluation for Text Detection and Recognition\n  Tasks", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of text detection and recognition methods,\nexisting evaluation metrics fail to provide a fair and reliable comparison\namong those methods. In addition, there exists no end-to-end evaluation metric\nthat takes characteristics of OCR tasks into account. Previous end-to-end\nmetric contains cascaded errors from the binary scoring process applied in both\ndetection and recognition tasks. Ignoring partially correct results raises a\ngap between quantitative and qualitative analysis, and prevents fine-grained\nassessment. Based on the fact that character is a key element of text, we\nhereby propose a Character-Level Evaluation metric (CLEval). In CLEval, the\n\\textit{instance matching} process handles split and merge detection cases, and\nthe \\textit{scoring process} conducts character-level evaluation. By\naggregating character-level scores, the CLEval metric provides a fine-grained\nevaluation of end-to-end results composed of the detection and recognition as\nwell as individual evaluations for each module from the end-performance\nperspective. We believe that our metrics can play a key role in developing and\nanalyzing state-of-the-art text detection and recognition methods. The\nevaluation code is publicly available at https://github.com/clovaai/CLEval.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 08:12:39 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Baek", "Youngmin", ""], ["Nam", "Daehyun", ""], ["Park", "Sungrae", ""], ["Lee", "Junyeop", ""], ["Shin", "Seung", ""], ["Baek", "Jeonghun", ""], ["Lee", "Chae Young", ""], ["Lee", "Hwalsuk", ""]]}, {"id": "2006.06246", "submitter": "Taufiq Hasan", "authors": "Partho Ghosh, Md. Abrar Istiak, Nayeeb Rashid, Ahsan Habib Akash,\n  Ridwan Abrar, Ankan Ghosh Dastider, Asif Shahriyar Sushmit, Taufiq Hasan", "title": "Privacy-Aware Activity Classification from First Person Office Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the advent of wearable body-cameras, human activity classification from\nFirst-Person Videos (FPV) has become a topic of increasing importance for\nvarious applications, including in life-logging, law-enforcement, sports,\nworkplace, and healthcare. One of the challenging aspects of FPV is its\nexposure to potentially sensitive objects within the user's field of view. In\nthis work, we developed a privacy-aware activity classification system focusing\non office videos. We utilized a Mask-RCNN with an Inception-ResNet hybrid as a\nfeature extractor for detecting, and then blurring out sensitive objects (e.g.,\ndigital screens, human face, paper) from the videos. For activity\nclassification, we incorporate an ensemble of Recurrent Neural Networks (RNNs)\nwith ResNet, ResNext, and DenseNet based feature extractors. The proposed\nsystem was trained and evaluated on the FPV office video dataset that includes\n18-classes made available through the IEEE Video and Image Processing (VIP) Cup\n2019 competition. On the original unprotected FPVs, the proposed activity\nclassifier ensemble reached an accuracy of 85.078% with precision, recall, and\nF1 scores of 0.88, 0.85 & 0.86, respectively. On privacy protected videos, the\nperformances were slightly degraded, with accuracy, precision, recall, and F1\nscores at 73.68%, 0.79, 0.75, and 0.74, respectively. The presented system won\nthe 3rd prize in the IEEE VIP Cup 2019 competition.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 08:13:15 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Ghosh", "Partho", ""], ["Istiak", "Md. Abrar", ""], ["Rashid", "Nayeeb", ""], ["Akash", "Ahsan Habib", ""], ["Abrar", "Ridwan", ""], ["Dastider", "Ankan Ghosh", ""], ["Sushmit", "Asif Shahriyar", ""], ["Hasan", "Taufiq", ""]]}, {"id": "2006.06277", "submitter": "Lei Liu", "authors": "Hongwei Zhao, Chengtao Peng, Lei Liu and Bin Li", "title": "W-net: Simultaneous segmentation of multi-anatomical retinal structures\n  using a multi-task deep neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of multiple anatomical structures is of great importance in\nmedical image analysis. In this study, we proposed a $\\mathcal{W}$-net to\nsimultaneously segment both the optic disc (OD) and the exudates in retinal\nimages based on the multi-task learning (MTL) scheme. We introduced a\nclass-balanced loss and a multi-task weighted loss to alleviate the imbalanced\nproblem and to improve the robustness and generalization property of the\n$\\mathcal{W}$-net. We demonstrated the effectiveness of our approach by\napplying five-fold cross-validation experiments on two public datasets\ne\\_ophtha\\_EX and DiaRetDb1. We achieved F1-score of 94.76\\% and 95.73\\% for OD\nsegmentation, and 92.80\\% and 94.14\\% for exudates segmentation. To further\nprove the generalization property of the proposed method, we applied the\ntrained model on the DRIONS-DB dataset for OD segmentation and on the MESSIDOR\ndataset for exudate segmentation. Our results demonstrated that by choosing the\noptimal weights of each task, the MTL based $\\mathcal{W}$-net outperformed\nseparate models trained individually on each task. Code and pre-trained models\nwill be available at:\n\\url{https://github.com/FundusResearch/MTL_for_OD_and_exudates.git}.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 09:33:33 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Zhao", "Hongwei", ""], ["Peng", "Chengtao", ""], ["Liu", "Lei", ""], ["Li", "Bin", ""]]}, {"id": "2006.06278", "submitter": "Pin Tang", "authors": "Pin Tang, Chen Zu, Mei Hong, Rui Yan, Xingchen Peng, Jianghong Xiao,\n  Xi Wu, Jiliu Zhou, Luping Zhou, and Yan Wang", "title": "DSU-net: Dense SegU-net for automatic head-and-neck tumor segmentation\n  in MR images", "comments": "This research needs to be advanced in the future", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise and accurate segmentation of the most common head-and-neck tumor,\nnasopharyngeal carcinoma (NPC), in MRI sheds light on treatment and regulatory\ndecisions making. However, the large variations in the lesion size and shape of\nNPC, boundary ambiguity, as well as the limited available annotated samples\nconspire NPC segmentation in MRI towards a challenging task. In this paper, we\npropose a Dense SegU-net (DSU-net) framework for automatic NPC segmentation in\nMRI. Our contribution is threefold. First, different from the traditional\ndecoder in U-net using upconvolution for upsamling, we argue that the\nrestoration from low resolution features to high resolution output should be\ncapable of preserving information significant for precise boundary\nlocalization. Hence, we use unpooling to unsample and propose SegU-net. Second,\nto combat the potential vanishing-gradient problem, we introduce dense blocks\nwhich can facilitate feature propagation and reuse. Third, using only cross\nentropy (CE) as loss function may bring about troubles such as miss-prediction,\ntherefore we propose to use a loss function comprised of both CE loss and Dice\nloss to train the network. Quantitative and qualitative comparisons are carried\nout extensively on in-house datasets, the experimental results show that our\nproposed architecture outperforms the existing state-of-the-art segmentation\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 09:33:41 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 10:30:10 GMT"}, {"version": "v3", "created": "Sun, 20 Dec 2020 03:15:56 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Tang", "Pin", ""], ["Zu", "Chen", ""], ["Hong", "Mei", ""], ["Yan", "Rui", ""], ["Peng", "Xingchen", ""], ["Xiao", "Jianghong", ""], ["Wu", "Xi", ""], ["Zhou", "Jiliu", ""], ["Zhou", "Luping", ""], ["Wang", "Yan", ""]]}, {"id": "2006.06281", "submitter": "Xiangwei Feng", "authors": "Xiang-Wei Feng, Da-Zheng Feng, Yun Zhu", "title": "Fast Coherent Point Drift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonrigid point set registration is widely applied in the tasks of computer\nvision and pattern recognition. Coherent point drift (CPD) is a classical\nmethod for nonrigid point set registration. However, to solve spatial\ntransformation functions, CPD has to compute inversion of a M*M matrix per\niteration with time complexity O(M3). By introducing a simple corresponding\nconstraint, we develop a fast implementation of CPD. The most advantage of our\nmethod is to avoid matrix-inverse operation. Before the iteration begins, our\nmethod requires to take eigenvalue decomposition of a M*M matrix once. After\niteration begins, our method only needs to update a diagonal matrix with linear\ncomputational complexity, and perform matrix multiplication operation with time\ncomplexity approximately O(M2) in each iteration. Besides, our method can be\nfurther accelerated by the low-rank matrix approximation. Experimental results\nin 3D point cloud data show that our method can significantly reduce\ncomputation burden of the registration process, and keep comparable performance\nwith CPD on accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 09:35:23 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Feng", "Xiang-Wei", ""], ["Feng", "Da-Zheng", ""], ["Zhu", "Yun", ""]]}, {"id": "2006.06316", "submitter": "John Pavlopoulos", "authors": "Vasiliki Kougia and John Pavlopoulos and Panagiotis Papapetrou and Max\n  Gordon", "title": "RTEX: A novel methodology for Ranking, Tagging, and Explanatory\n  diagnostic captioning of radiography exams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces RTEx, a novel methodology for a) ranking radiography\nexams based on their probability to contain an abnormality, b) generating\nabnormality tags for abnormal exams, and c) providing a diagnostic explanation\nin natural language for each abnormal exam. The task of ranking radiography\nexams is an important first step for practitioners who want to identify and\nprioritize those radiography exams that are more likely to contain\nabnormalities, for example, to avoid mistakes due to tiredness or to manage\nheavy workload (e.g., during a pandemic). We used two publicly available\ndatasets to assess our methodology and demonstrate that for the task of ranking\nit outperforms its competitors in terms of NDCG@k. For each abnormal\nradiography exam RTEx generates a set of abnormality tags alongside an\nexplanatory diagnostic text to explain the tags and guide the medical expert.\nOur tagging component outperforms two strong competitor methods in terms of F1.\nMoreover, the diagnostic captioning component of RTEx, which exploits the\nalready extracted tags to constrain the captioning process, outperforms all\ncompetitors with respect to clinical precision and recall.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 10:29:44 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Kougia", "Vasiliki", ""], ["Pavlopoulos", "John", ""], ["Papapetrou", "Panagiotis", ""], ["Gordon", "Max", ""]]}, {"id": "2006.06320", "submitter": "Che-Han Chang", "authors": "Chih-Yang Chen, Che-Han Chang, Edward Y. Chang", "title": "Hypernetwork-Based Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is an effective technique to improve the generalization of\ndeep neural networks. Recently, AutoAugment proposed a well-designed search\nspace and a search algorithm that automatically finds augmentation policies in\na data-driven manner. However, AutoAugment is computationally intensive. In\nthis paper, we propose an efficient gradient-based search algorithm, called\nHypernetwork-Based Augmentation (HBA), which simultaneously learns model\nparameters and augmentation hyperparameters in a single training. Our HBA uses\na hypernetwork to approximate a population-based training algorithm, which\nenables us to tune augmentation hyperparameters by gradient descent. Besides,\nwe introduce a weight sharing strategy that simplifies our hypernetwork\narchitecture and speeds up our search algorithm. We conduct experiments on\nCIFAR-10, CIFAR-100, SVHN, and ImageNet. Our results demonstrate that HBA is\nsignificantly faster than state-of-the-art methods while achieving competitive\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 10:36:39 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Chen", "Chih-Yang", ""], ["Chang", "Che-Han", ""], ["Chang", "Edward Y.", ""]]}, {"id": "2006.06321", "submitter": "Osama Mazhar", "authors": "Osama Mazhar, Sofiane Ramdani, and Andrea Cherubini", "title": "A Deep Learning Framework for Recognizing both Static and Dynamic\n  Gestures", "comments": "19 pages - Accepted in MDPI Sensors: Sensors and Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intuitive user interfaces are indispensable to interact with the human\ncentric smart environments. In this paper, we propose a unified framework that\nrecognizes both static and dynamic gestures, using simple RGB vision (without\ndepth sensing). This feature makes it suitable for inexpensive human-robot\ninteraction in social or industrial settings. We employ a pose-driven spatial\nattention strategy, which guides our proposed Static and Dynamic gestures\nNetwork - StaDNet. From the image of the human upper body, we estimate his/her\ndepth, along with the region-of-interest around his/her hands. The\nConvolutional Neural Network in StaDNet is fine-tuned on a\nbackground-substituted hand gestures dataset. It is utilized to detect 10\nstatic gestures for each hand as well as to obtain the hand image-embeddings.\nThese are subsequently fused with the augmented pose vector and then passed to\nthe stacked Long Short-Term Memory blocks. Thus, human-centred frame-wise\ninformation from the augmented pose vector and from the left/right hands\nimage-embeddings are aggregated in time to predict the dynamic gestures of the\nperforming person. In a number of experiments, we show that the proposed\napproach surpasses the state-of-the-art results on the large-scale Chalearn\n2016 dataset. Moreover, we transfer the knowledge learned through the proposed\nmethodology to the Praxis gestures dataset, and the obtained results also\noutscore the state-of-the-art on this dataset.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 10:39:02 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 10:31:16 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Mazhar", "Osama", ""], ["Ramdani", "Sofiane", ""], ["Cherubini", "Andrea", ""]]}, {"id": "2006.06325", "submitter": "Nicolas Pielawski", "authors": "Nicolas Pielawski, Elisabeth Wetzer, Johan \\\"Ofverstedt, Jiahao Lu,\n  Carolina W\\\"ahlby, Joakim Lindblad and Nata\\v{s}a Sladoje", "title": "CoMIR: Contrastive Multimodal Image Representation for Registration", "comments": "21 pages, 11 figures", "journal-ref": "NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose contrastive coding to learn shared, dense image representations,\nreferred to as CoMIRs (Contrastive Multimodal Image Representations). CoMIRs\nenable the registration of multimodal images where existing registration\nmethods often fail due to a lack of sufficiently similar image structures.\nCoMIRs reduce the multimodal registration problem to a monomodal one, in which\ngeneral intensity-based, as well as feature-based, registration algorithms can\nbe applied. The method involves training one neural network per modality on\naligned images, using a contrastive loss based on noise-contrastive estimation\n(InfoNCE). Unlike other contrastive coding methods, used for, e.g.,\nclassification, our approach generates image-like representations that contain\nthe information shared between modalities. We introduce a novel,\nhyperparameter-free modification to InfoNCE, to enforce rotational equivariance\nof the learnt representations, a property essential to the registration task.\nWe assess the extent of achieved rotational equivariance and the stability of\nthe representations with respect to weight initialization, training set, and\nhyperparameter settings, on a remote sensing dataset of RGB and near-infrared\nimages. We evaluate the learnt representations through registration of a\nbiomedical dataset of bright-field and second-harmonic generation microscopy\nimages; two modalities with very little apparent correlation. The proposed\napproach based on CoMIRs significantly outperforms registration of\nrepresentations created by GAN-based image-to-image translation, as well as a\nstate-of-the-art, application-specific method which takes additional knowledge\nabout the data into account. Code is available at:\nhttps://github.com/MIDA-group/CoMIR.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 10:51:33 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 08:54:38 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Pielawski", "Nicolas", ""], ["Wetzer", "Elisabeth", ""], ["\u00d6fverstedt", "Johan", ""], ["Lu", "Jiahao", ""], ["W\u00e4hlby", "Carolina", ""], ["Lindblad", "Joakim", ""], ["Sladoje", "Nata\u0161a", ""]]}, {"id": "2006.06356", "submitter": "Suzanne Wetstein", "authors": "Gerda Bortsova, Cristina Gonz\\'alez-Gonzalo, Suzanne C. Wetstein,\n  Florian Dubost, Ioannis Katramados, Laurens Hogeweg, Bart Liefers, Bram van\n  Ginneken, Josien P.W. Pluim, Mitko Veta, Clara I. S\\'anchez, and Marleen de\n  Bruijne", "title": "Adversarial Attack Vulnerability of Medical Image Analysis Systems:\n  Unexplored Factors", "comments": "First three authors contributed equally", "journal-ref": "Medical Image Analysis. Available online 18 Jun 2021", "doi": "10.1016/j.media.2021.102141", "report-no": null, "categories": "cs.CR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks are considered a potentially serious security threat for\nmachine learning systems. Medical image analysis (MedIA) systems have recently\nbeen argued to be vulnerable to adversarial attacks due to strong financial\nincentives and the associated technological infrastructure.\n  In this paper, we study previously unexplored factors affecting adversarial\nattack vulnerability of deep learning MedIA systems in three medical domains:\nophthalmology, radiology, and pathology. We focus on adversarial black-box\nsettings, in which the attacker does not have full access to the target model\nand usually uses another model, commonly referred to as surrogate model, to\ncraft adversarial examples. We consider this to be the most realistic scenario\nfor MedIA systems.\n  Firstly, we study the effect of weight initialization (ImageNet vs. random)\non the transferability of adversarial attacks from the surrogate model to the\ntarget model. Secondly, we study the influence of differences in development\ndata between target and surrogate models. We further study the interaction of\nweight initialization and data differences with differences in model\narchitecture. All experiments were done with a perturbation degree tuned to\nensure maximal transferability at minimal visual perceptibility of the attacks.\n  Our experiments show that pre-training may dramatically increase the\ntransferability of adversarial examples, even when the target and surrogate's\narchitectures are different: the larger the performance gain using\npre-training, the larger the transferability. Differences in the development\ndata between target and surrogate models considerably decrease the performance\nof the attack; this decrease is further amplified by difference in the model\narchitecture. We believe these factors should be considered when developing\nsecurity-critical MedIA systems planned to be deployed in clinical practice.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 12:19:39 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 08:36:29 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 12:50:56 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Bortsova", "Gerda", ""], ["Gonz\u00e1lez-Gonzalo", "Cristina", ""], ["Wetstein", "Suzanne C.", ""], ["Dubost", "Florian", ""], ["Katramados", "Ioannis", ""], ["Hogeweg", "Laurens", ""], ["Liefers", "Bart", ""], ["van Ginneken", "Bram", ""], ["Pluim", "Josien P. W.", ""], ["Veta", "Mitko", ""], ["S\u00e1nchez", "Clara I.", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "2006.06385", "submitter": "Mira Park Dr.", "authors": "Heemoon Yoon, Sang-Hee Lee, Mira Park", "title": "TensorFlow with user friendly Graphical Framework for object detection\n  API", "comments": "\"The code of TF-GraF for TensorFlow object detection API is opened at\n  https://github.com/boguss1225/ObjectDetectionGUI\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TensorFlow is an open-source framework for deep learning dataflow and\ncontains application programming interfaces (APIs) of voice analysis, natural\nlanguage process, and computer vision. Especially, TensorFlow object detection\nAPI in computer vision field has been widely applied to technologies of\nagriculture, engineering, and medicine but barriers to entry of the framework\nusage is still high through command-line interface (CLI) and code for amateurs\nand beginners of information technology (IT) field. Therefore, this is aim to\ndevelop an user friendly Graphical Framework for object detection API on\nTensorFlow which is called TensorFlow Graphical Framework (TF-GraF). The\nTF-GraF provides independent virtual environments according to user accounts in\nserver-side, additionally, execution of data preprocessing, training, and\nevaluation without CLI in client-side. Furthermore, hyperparameter setting,\nreal-time observation of training process, object visualization of test images,\nand metrics evaluations of test data can also be operated via TF-GraF.\nEspecially, TF-GraF supports flexible model selection of SSD, Faster-RCNN,\nRFCN, and Mask-RCNN including convolutional neural networks (inceptions and\nResNets) through GUI environment. Consequently, TF-GraF allows anyone, even\nwithout any previous knowledge of deep learning frameworks, to design, train\nand deploy machine intelligence models without coding. Since TF-GraF takes care\nof setting and configuration, it allows anyone to use deep learning technology\nfor their project without spending time to install complex software and\nenvironment.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 13:00:02 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Yoon", "Heemoon", ""], ["Lee", "Sang-Hee", ""], ["Park", "Mira", ""]]}, {"id": "2006.06392", "submitter": "Luka Murn", "authors": "Luka Murn, Saverio Blasi, Alan F. Smeaton, Noel E. O'Connor, Marta\n  Mrak", "title": "Interpreting CNN for Low Complexity Learned Sub-pixel Motion\n  Compensation in Video Coding", "comments": "27th IEEE International Conference on Image Processing, 25-28 Oct\n  2020, Abu Dhabi, United Arab Emirates", "journal-ref": "2020 IEEE International Conference on Image Processing (ICIP),\n  2020, pp. 798-802", "doi": "10.1109/ICIP40778.2020.9191193", "report-no": null, "categories": "eess.IV cs.CC cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning has shown great potential in image and video compression tasks.\nHowever, it brings bit savings at the cost of significant increases in coding\ncomplexity, which limits its potential for implementation within practical\napplications. In this paper, a novel neural network-based tool is presented\nwhich improves the interpolation of reference samples needed for fractional\nprecision motion compensation. Contrary to previous efforts, the proposed\napproach focuses on complexity reduction achieved by interpreting the\ninterpolation filters learned by the networks. When the approach is implemented\nin the Versatile Video Coding (VVC) test model, up to 4.5% BD-rate saving for\nindividual sequences is achieved compared with the baseline VVC, while the\ncomplexity of learned interpolation is significantly reduced compared to the\napplication of full neural network.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 13:10:20 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Murn", "Luka", ""], ["Blasi", "Saverio", ""], ["Smeaton", "Alan F.", ""], ["O'Connor", "Noel E.", ""], ["Mrak", "Marta", ""]]}, {"id": "2006.06432", "submitter": "Fahdi Kanavati", "authors": "Fahdi Kanavati, Shah Islam, Zohaib Arain, Eric O. Aboagye, Andrea\n  Rockall", "title": "Fully-automated deep learning slice-based muscle estimation from CT\n  images for sarcopenia assessment", "comments": "arXiv admin note: substantial text overlap with arXiv:1811.09244", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To demonstrate the effectiveness of using a deep learning-based\napproach for a fully automated slice-based measurement of muscle mass for\nassessing sarcopenia on CT scans of the abdomen without any case exclusion\ncriteria.\n  Materials and Methods: This retrospective study was conducted using a\ncollection of public and privately available CT images (n = 1070). The method\nconsisted of two stages: slice detection from a CT volume and single-slice CT\nsegmentation. Both stages used Fully Convolutional Neural Networks (FCNN) and\nwere based on a UNet-like architecture. Input data consisted of CT volumes with\na variety of fields of view. The output consisted of a segmented muscle mass on\na CT slice at the level of L3 vertebra. The muscle mass is segmented into\nerector spinae, psoas, and rectus abdominus muscle groups. The output was\ntested against manual ground-truth segmentation by an expert annotator.\n  Results: 3-fold cross validation was used to evaluate the proposed method.\nThe slice detection cross validation error was 1.41+-5.02 (in slices). The\nsegmentation cross validation Dice overlaps were 0.97+-0.02, 0.95+-0.04,\n0.94+-0.04 for erector spinae, psoas, and rectus abdominus, respectively, and\n0.96+-0.02 for the combined muscle mass.\n  Conclusion: A deep learning approach to detect CT slices and segment muscle\nmass to perform slice-based analysis of sarcopenia is an effective and\npromising approach. The use of FCNN to accurately and efficiently detect a\nslice in CT volumes with a variety of fields of view, occlusions, and slice\nthicknesses was demonstrated.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 12:05:55 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Kanavati", "Fahdi", ""], ["Islam", "Shah", ""], ["Arain", "Zohaib", ""], ["Aboagye", "Eric O.", ""], ["Rockall", "Andrea", ""]]}, {"id": "2006.06443", "submitter": "Pavel Kaloshin", "authors": "Pavel Kaloshin", "title": "Convolutional neural networks compression with low rank and sparse\n  tensor decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks show outstanding results in a variety of\ncomputer vision tasks. However, a neural network architecture design usually\nfaces a trade-off between model performance and computational/memory\ncomplexity. For some real-world applications, it is crucial to develop models,\nwhich can be fast and light enough to run on edge systems and mobile devices.\nHowever, many modern architectures that demonstrate good performance don't\nsatisfy inference time and storage limitation requirements. Thus, arises a\nproblem of neural network compression to obtain a smaller and faster model,\nwhich is on par with the initial one.\n  In this work, we consider a neural network compression method based on tensor\ndecompositions. Namely, we propose to approximate the convolutional layer\nweight with a tensor, which can be represented as a sum of low-rank and sparse\ncomponents. The motivation for such approximation is based on the assumption\nthat low-rank and sparse terms allow eliminating two different types of\nredundancy and thus yield a better compression rate. An efficient CPU\nimplementation for the proposed method has been developed. Our algorithm has\ndemonstrated up to 3.5x CPU layer speedup and 11x layer size reduction when\ncompressing Resnet50 architecture for the image classification task.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 13:53:18 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Kaloshin", "Pavel", ""]]}, {"id": "2006.06458", "submitter": "Kiran Raja Dr", "authors": "Kiran Raja, Matteo Ferrara, Annalisa Franco, Luuk Spreeuwers, Illias\n  Batskos, Florens de Wit Marta Gomez-Barrero, Ulrich Scherhag, Daniel Fischer,\n  Sushma Venkatesh, Jag Mohan Singh, Guoqiang Li, Lo\\\"ic Bergeron, Sergey\n  Isadskiy, Raghavendra Ramachandra, Christian Rathgeb, Dinusha Frings, Uwe\n  Seidel, Fons Knopjes, Raymond Veldhuis, Davide Maltoni, Christoph Busch", "title": "Morphing Attack Detection -- Database, Evaluation Platform and\n  Benchmarking", "comments": "This paper is a pre-print. The article is accepted for publication in\n  IEEE Transactions on Information Forensics and Security (TIFS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Morphing attacks have posed a severe threat to Face Recognition System (FRS).\nDespite the number of advancements reported in recent works, we note serious\nopen issues such as independent benchmarking, generalizability challenges and\nconsiderations to age, gender, ethnicity that are inadequately addressed.\nMorphing Attack Detection (MAD) algorithms often are prone to generalization\nchallenges as they are database dependent. The existing databases, mostly of\nsemi-public nature, lack in diversity in terms of ethnicity, various morphing\nprocess and post-processing pipelines. Further, they do not reflect a realistic\noperational scenario for Automated Border Control (ABC) and do not provide a\nbasis to test MAD on unseen data, in order to benchmark the robustness of\nalgorithms. In this work, we present a new sequestered dataset for facilitating\nthe advancements of MAD where the algorithms can be tested on unseen data in an\neffort to better generalize. The newly constructed dataset consists of facial\nimages from 150 subjects from various ethnicities, age-groups and both genders.\nIn order to challenge the existing MAD algorithms, the morphed images are with\ncareful subject pre-selection created from the contributing images, and further\npost-processed to remove morphing artifacts. The images are also printed and\nscanned to remove all digital cues and to simulate a realistic challenge for\nMAD algorithms. Further, we present a new online evaluation platform to test\nalgorithms on sequestered data. With the platform we can benchmark the morph\ndetection performance and study the generalization ability. This work also\npresents a detailed analysis on various subsets of sequestered data and\noutlines open challenges for future directions in MAD research.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 14:11:09 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 11:35:07 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 16:35:11 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Raja", "Kiran", ""], ["Ferrara", "Matteo", ""], ["Franco", "Annalisa", ""], ["Spreeuwers", "Luuk", ""], ["Batskos", "Illias", ""], ["Gomez-Barrero", "Florens de Wit Marta", ""], ["Scherhag", "Ulrich", ""], ["Fischer", "Daniel", ""], ["Venkatesh", "Sushma", ""], ["Singh", "Jag Mohan", ""], ["Li", "Guoqiang", ""], ["Bergeron", "Lo\u00efc", ""], ["Isadskiy", "Sergey", ""], ["Ramachandra", "Raghavendra", ""], ["Rathgeb", "Christian", ""], ["Frings", "Dinusha", ""], ["Seidel", "Uwe", ""], ["Knopjes", "Fons", ""], ["Veldhuis", "Raymond", ""], ["Maltoni", "Davide", ""], ["Busch", "Christoph", ""]]}, {"id": "2006.06460", "submitter": "Zijie Wu", "authors": "Zijie Wu, Yaonan Wang, Qing Zhu, Jianxu Mao, Haotian Wu, Mingtao Feng\n  and Ajmal Mian", "title": "Minimum Potential Energy of Point Cloud for Robust Global Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel minimum gravitational potential energy\n(MPE)-based algorithm for global point set registration. The feature\ndescriptors extraction algorithms have emerged as the standard approach to\nalign point sets in the past few decades. However, the alignment can be\nchallenging to take effect when the point set suffers from raw point data\nproblems such as noises (Gaussian and Uniformly). Different from the most\nexisting point set registration methods which usually extract the descriptors\nto find correspondences between point sets, our proposed MPE alignment method\nis able to handle large scale raw data offset without depending on traditional\ndescriptors extraction, whether for the local or global registration methods.\nWe decompose the solution into a global optimal convex approximation and the\nfast descent process to a local minimum. For the approximation step, the\nproposed minimum potential energy (MPE) approach consists of two main steps.\nFirstly, according to the construction of the force traction operator, we could\nsimply compute the position of the potential energy minimum; Secondly, with\nrespect to the finding of the MPE point, we propose a new theory that employs\nthe two flags to observe the status of the registration procedure. The method\nof fast descent process to the minimum that we employed is the iterative\nclosest point algorithm; it can achieve the global minimum. We demonstrate the\nperformance of the proposed algorithm on synthetic data as well as on real\ndata. The proposed method outperforms the other global methods in terms of both\nefficiency, accuracy and noise resistance.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 14:13:40 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 02:41:13 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Wu", "Zijie", ""], ["Wang", "Yaonan", ""], ["Zhu", "Qing", ""], ["Mao", "Jianxu", ""], ["Wu", "Haotian", ""], ["Feng", "Mingtao", ""], ["Mian", "Ajmal", ""]]}, {"id": "2006.06493", "submitter": "Nataniel Ruiz", "authors": "Nataniel Ruiz, Sarah Adel Bargal, Stan Sclaroff", "title": "Protecting Against Image Translation Deepfakes by Leaking Universal\n  Perturbations from Black-Box Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop efficient disruptions of black-box image translation\ndeepfake generation systems. We are the first to demonstrate black-box deepfake\ngeneration disruption by presenting image translation formulations of attacks\ninitially proposed for classification models. Nevertheless, a naive adaptation\nof classification black-box attacks results in a prohibitive number of queries\nfor image translation systems in the real-world. We present a frustratingly\nsimple yet highly effective algorithm Leaking Universal Perturbations (LUP),\nthat significantly reduces the number of queries needed to attack an image. LUP\nconsists of two phases: (1) a short leaking phase where we attack the network\nusing traditional black-box attacks and gather information on successful\nattacks on a small dataset and (2) and an exploitation phase where we leverage\nsaid information to subsequently attack the network with improved efficiency.\nOur attack reduces the total number of queries necessary to attack GANimation\nand StarGAN by 30%.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 15:02:27 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Ruiz", "Nataniel", ""], ["Bargal", "Sarah Adel", ""], ["Sclaroff", "Stan", ""]]}, {"id": "2006.06500", "submitter": "Hyunjung Shim Dr.", "authors": "Kyungjune Baek, Yunjey Choi, Youngjung Uh, Jaejun Yoo, Hyunjung Shim", "title": "Rethinking the Truly Unsupervised Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every recent image-to-image translation model uses either image-level (i.e.\ninput-output pairs) or set-level (i.e. domain labels) supervision at minimum.\nHowever, even the set-level supervision can be a serious bottleneck for data\ncollection in practice. In this paper, we tackle image-to-image translation in\na fully unsupervised setting, i.e., neither paired images nor domain labels. To\nthis end, we propose the truly unsupervised image-to-image translation method\n(TUNIT) that simultaneously learns to separate image domains via an\ninformation-theoretic approach and generate corresponding images using the\nestimated domain labels. Experimental results on various datasets show that the\nproposed method successfully separates domains and translates images across\nthose domains. In addition, our model outperforms existing set-level supervised\nmethods under a semi-supervised setting, where a subset of domain labels is\nprovided. The source code is available at https://github.com/clovaai/tunit\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 15:15:12 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Baek", "Kyungjune", ""], ["Choi", "Yunjey", ""], ["Uh", "Youngjung", ""], ["Yoo", "Jaejun", ""], ["Shim", "Hyunjung", ""]]}, {"id": "2006.06525", "submitter": "Wenhao Wang", "authors": "Wenhao Wang, Fang Zhao, Shengcai Liao, Ling Shao", "title": "Attentive WaveBlock: Complementarity-enhanced Mutual Networks for\n  Unsupervised Domain Adaptation in Person Re-identification and Beyond", "comments": "Our codes and models are available at\n  https://github.com/WangWenhao0716/Attentive-WaveBlock", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised domain adaptation (UDA) for person re-identification is\nchallenging because of the huge gap between the source and target domain. A\ntypical self-training method is to use pseudo-labels generated by clustering\nalgorithms to iteratively optimize the model on the target domain. However, a\ndrawback to this is that noisy pseudo-labels generally cause trouble in\nlearning. To address this problem, a mutual learning method by dual networks\nhas been developed to produce reliable soft labels. However, as the two neural\nnetworks gradually converge, their complementarity is weakened and they likely\nbecome biased towards the same kind of noise. This paper proposes a novel\nlight-weight module, the Attentive WaveBlock (AWB), which can be integrated\ninto the dual networks of mutual learning to enhance the complementarity and\nfurther depress noise in the pseudo-labels. Specifically, we first introduce a\nparameter-free module, the WaveBlock, which creates a difference between\nfeatures learned by two networks by waving blocks of feature maps differently.\nThen, an attention mechanism is leveraged to enlarge the difference created and\ndiscover more complementary features. Furthermore, two kinds of combination\nstrategies, i.e. pre-attention and post-attention, are explored. Experiments\ndemonstrate that the proposed method achieves state-of-the-art performance with\nsignificant improvements on multiple UDA person re-identification tasks. We\nalso prove the generality of the proposed method by applying it to vehicle\nre-identification and image classification tasks. Our codes and models are\navailable at https://github.com/WangWenhao0716/Attentive-WaveBlock.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 15:40:40 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 16:45:50 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Wang", "Wenhao", ""], ["Zhao", "Fang", ""], ["Liao", "Shengcai", ""], ["Shao", "Ling", ""]]}, {"id": "2006.06527", "submitter": "Zhennan Wang", "authors": "Zhennan Wang, Canqun Xiang, Wenbin Zou, Chen Xu", "title": "MMA Regularization: Decorrelating Weights of Neural Networks by\n  Maximizing the Minimal Angles", "comments": "NeurIPS2020", "journal-ref": "https://proceedings.neurips.cc/paper/2020/file/dcd2f3f312b6705fb06f4f9f1b55b55c-Paper.pdf", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strong correlation between neurons or filters can significantly weaken\nthe generalization ability of neural networks. Inspired by the well-known\nTammes problem, we propose a novel diversity regularization method to address\nthis issue, which makes the normalized weight vectors of neurons or filters\ndistributed on a hypersphere as uniformly as possible, through maximizing the\nminimal pairwise angles (MMA). This method can easily exert its effect by\nplugging the MMA regularization term into the loss function with negligible\ncomputational overhead. The MMA regularization is simple, efficient, and\neffective. Therefore, it can be used as a basic regularization method in neural\nnetwork training. Extensive experiments demonstrate that MMA regularization is\nable to enhance the generalization ability of various modern models and\nachieves considerable performance improvements on CIFAR100 and TinyImageNet\ndatasets. In addition, experiments on face verification show that MMA\nregularization is also effective for feature learning. Code is available at:\nhttps://github.com/wznpub/MMA_Regularization.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 14:03:16 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 09:14:49 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wang", "Zhennan", ""], ["Xiang", "Canqun", ""], ["Zou", "Wenbin", ""], ["Xu", "Chen", ""]]}, {"id": "2006.06563", "submitter": "Cristian Jes\\'us Vaca Rubio", "authors": "Cristian J. Vaca-Rubio, Pablo Ramirez-Espinosa, Robin Jess Williams,\n  Kimmo Kansanen, Zheng-Hua Tan, Elisabeth de Carvalho and Petar Popovski", "title": "A Primer on Large Intelligent Surface (LIS) for Wireless Sensing in an\n  Industrial Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the beyond-5G developments that is often highlighted is the\nintegration of wireless communication and radio sensing. This paper addresses\nthe potential of communication-sensing integration of Large Intelligent\nSurfaces (LIS) in an exemplary Industry 4.0 scenario. Besides the potential for\nhigh throughput and efficient multiplexing of wireless links, an LIS can offer\na high-resolution rendering of the propagation environment. This is because, in\nan indoor setting, it can be placed in proximity to the sensed phenomena, while\nthe high resolution is offered by densely spaced tiny antennas deployed over a\nlarge area. By treating an LIS as a radio image of the environment, we develop\nsensing techniques that leverage the usage of computer vision combined with\nmachine learning. We test these methods for a scenario where we need to detect\nwhether an industrial robot deviates from a predefined route. The results show\nthat the LIS-based sensing offers high precision and has a high application\npotential in indoor industrial environments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 16:15:50 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 11:13:27 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 15:21:42 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Vaca-Rubio", "Cristian J.", ""], ["Ramirez-Espinosa", "Pablo", ""], ["Williams", "Robin Jess", ""], ["Kansanen", "Kimmo", ""], ["Tan", "Zheng-Hua", ""], ["de Carvalho", "Elisabeth", ""], ["Popovski", "Petar", ""]]}, {"id": "2006.06567", "submitter": "Ting Yao", "authors": "Yingwei Pan and Ting Yao and Yehao Li and Chong-Wah Ngo and Tao Mei", "title": "Exploring Category-Agnostic Clusters for Open-Set Domain Adaptation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation has received significant attention in recent\nyears. Most of existing works tackle the closed-set scenario, assuming that the\nsource and target domains share the exactly same categories. In practice,\nnevertheless, a target domain often contains samples of classes unseen in\nsource domain (i.e., unknown class). The extension of domain adaptation from\nclosed-set to such open-set situation is not trivial since the target samples\nin unknown class are not expected to align with the source. In this paper, we\naddress this problem by augmenting the state-of-the-art domain adaptation\ntechnique, Self-Ensembling, with category-agnostic clusters in target domain.\nSpecifically, we present Self-Ensembling with Category-agnostic Clusters\n(SE-CC) -- a novel architecture that steers domain adaptation with the\nadditional guidance of category-agnostic clusters that are specific to target\ndomain. These clustering information provides domain-specific visual cues,\nfacilitating the generalization of Self-Ensembling for both closed-set and\nopen-set scenarios. Technically, clustering is firstly performed over all the\nunlabeled target samples to obtain the category-agnostic clusters, which reveal\nthe underlying data space structure peculiar to target domain. A clustering\nbranch is capitalized on to ensure that the learnt representation preserves\nsuch underlying structure by matching the estimated assignment distribution\nover clusters to the inherent cluster distribution for each target sample.\nFurthermore, SE-CC enhances the learnt representation with mutual information\nmaximization. Extensive experiments are conducted on Office and VisDA datasets\nfor both open-set and closed-set domain adaptation, and superior results are\nreported when comparing to the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 16:19:02 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Li", "Yehao", ""], ["Ngo", "Chong-Wah", ""], ["Mei", "Tao", ""]]}, {"id": "2006.06568", "submitter": "Ting Yao", "authors": "Qi Cai and Yingwei Pan and Yu Wang and Jingen Liu and Ting Yao and Tao\n  Mei", "title": "Learning a Unified Sample Weighting Network for Object Detection", "comments": "CVPR 2020; The source code and model are publicly available at:\n  \\url{https://github.com/caiqi/sample-weighting-network}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region sampling or weighting is significantly important to the success of\nmodern region-based object detectors. Unlike some previous works, which only\nfocus on \"hard\" samples when optimizing the objective function, we argue that\nsample weighting should be data-dependent and task-dependent. The importance of\na sample for the objective function optimization is determined by its\nuncertainties to both object classification and bounding box regression tasks.\nTo this end, we devise a general loss function to cover most region-based\nobject detectors with various sampling strategies, and then based on it we\npropose a unified sample weighting network to predict a sample's task weights.\nOur framework is simple yet effective. It leverages the samples' uncertainty\ndistributions on classification loss, regression loss, IoU, and probability\nscore, to predict sample weights. Our approach has several advantages: (i). It\njointly learns sample weights for both classification and regression tasks,\nwhich differentiates it from most previous work. (ii). It is a data-driven\nprocess, so it avoids some manual parameter tuning. (iii). It can be\neffortlessly plugged into most object detectors and achieves noticeable\nperformance improvements without affecting their inference time. Our approach\nhas been thoroughly evaluated with recent object detection frameworks and it\ncan consistently boost the detection accuracy. Code has been made available at\n\\url{https://github.com/caiqi/sample-weighting-network}.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 16:19:16 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 05:30:43 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Cai", "Qi", ""], ["Pan", "Yingwei", ""], ["Wang", "Yu", ""], ["Liu", "Jingen", ""], ["Yao", "Ting", ""], ["Mei", "Tao", ""]]}, {"id": "2006.06570", "submitter": "Ting Yao", "authors": "Yiheng Zhang and Zhaofan Qiu and Ting Yao and Chong-Wah Ngo and Dong\n  Liu and Tao Mei", "title": "Transferring and Regularizing Prediction for Semantic Segmentation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation often requires a large set of images with pixel-level\nannotations. In the view of extremely expensive expert labeling, recent\nresearch has shown that the models trained on photo-realistic synthetic data\n(e.g., computer games) with computer-generated annotations can be adapted to\nreal images. Despite this progress, without constraining the prediction on real\nimages, the models will easily overfit on synthetic data due to severe domain\nmismatch. In this paper, we novelly exploit the intrinsic properties of\nsemantic segmentation to alleviate such problem for model transfer.\nSpecifically, we present a Regularizer of Prediction Transfer (RPT) that\nimposes the intrinsic properties as constraints to regularize model transfer in\nan unsupervised fashion. These constraints include patch-level, cluster-level\nand context-level semantic prediction consistencies at different levels of\nimage formation. As the transfer is label-free and data-driven, the robustness\nof prediction is addressed by selectively involving a subset of image regions\nfor model regularization. Extensive experiments are conducted to verify the\nproposal of RPT on the transfer of models trained on GTA5 and SYNTHIA\n(synthetic data) to Cityscapes dataset (urban street scenes). RPT shows\nconsistent improvements when injecting the constraints on several neural\nnetworks for semantic segmentation. More remarkably, when integrating RPT into\nthe adversarial-based segmentation framework, we report to-date the best\nresults: mIoU of 53.2%/51.7% when transferring from GTA5/SYNTHIA to Cityscapes,\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 16:19:41 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Zhang", "Yiheng", ""], ["Qiu", "Zhaofan", ""], ["Yao", "Ting", ""], ["Ngo", "Chong-Wah", ""], ["Liu", "Dong", ""], ["Mei", "Tao", ""]]}, {"id": "2006.06573", "submitter": "Pedro Felzenszwalb", "authors": "Jeova F. S. Rocha Neto and Pedro F. Felzenszwalb", "title": "Spectral Image Segmentation with Global Appearance Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new spectral method for image segmentation that incorporates\nlong range relationships for global appearance modeling. The approach combines\ntwo different graphs, one is a sparse graph that captures spatial relationships\nbetween nearby pixels and another is a dense graph that captures pairwise\nsimilarity between all pairs of pixels. We extend the spectral method for\nNormalized Cuts to this setting by combining the transition matrices of Markov\nchains associated with each graph. We also derive an efficient method that uses\nimportance sampling for sparsifying the dense graph of appearance\nrelationships. This leads to a practical algorithm for segmenting\nhigh-resolution images. The resulting method can segment challenging images\nwithout any filtering or pre-processing.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 16:21:54 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Neto", "Jeova F. S. Rocha", ""], ["Felzenszwalb", "Pedro F.", ""]]}, {"id": "2006.06606", "submitter": "Nanxuan Zhao", "authors": "Nanxuan Zhao and Zhirong Wu and Rynson W.H. Lau and Stephen Lin", "title": "What makes instance discrimination good for transfer learning?", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive visual pretraining based on the instance discrimination pretext\ntask has made significant progress. Notably, recent work on unsupervised\npretraining has shown to surpass the supervised counterpart for finetuning\ndownstream applications such as object detection and segmentation. It comes as\na surprise that image annotations would be better left unused for transfer\nlearning. In this work, we investigate the following problems: What makes\ninstance discrimination pretraining good for transfer learning? What knowledge\nis actually learned and transferred from these models? From this understanding\nof instance discrimination, how can we better exploit human annotation labels\nfor pretraining? Our findings are threefold. First, what truly matters for the\ntransfer is low-level and mid-level representations, not high-level\nrepresentations. Second, the intra-category invariance enforced by the\ntraditional supervised model weakens transferability by increasing task\nmisalignment. Finally, supervised pretraining can be strengthened by following\nan exemplar-based approach without explicit constraints among the instances\nwithin the same category.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 16:55:07 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 15:45:44 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Zhao", "Nanxuan", ""], ["Wu", "Zhirong", ""], ["Lau", "Rynson W. H.", ""], ["Lin", "Stephen", ""]]}, {"id": "2006.06611", "submitter": "Pierre Jacob", "authors": "Pierre Jacob and David Picard and Aymeric Histace and Edouard Klein", "title": "Improving Deep Metric Learning with Virtual Classes and Examples Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In deep metric learning, the training procedure relies on sampling\ninformative tuples. However, as the training procedure progresses, it becomes\nnearly impossible to sample relevant hard negative examples without proper\nmining strategies or generation-based methods. Recent work on hard negative\ngeneration have shown great promises to solve the mining problem. However, this\ngeneration process is difficult to tune and often leads to incorrectly labelled\nexamples. To tackle this issue, we introduce MIRAGE, a generation-based method\nthat relies on virtual classes entirely composed of generated examples that act\nas buffer areas between the training classes. We empirically show that virtual\nclasses significantly improve the results on popular datasets (Cub-200-2011,\nCars-196 and Stanford Online Products) compared to other generation methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:09:43 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Jacob", "Pierre", ""], ["Picard", "David", ""], ["Histace", "Aymeric", ""], ["Klein", "Edouard", ""]]}, {"id": "2006.06614", "submitter": "Jiaze Sun", "authors": "Jiaze Sun, Binod Bhattarai, Tae-Kyun Kim", "title": "MatchGAN: A Self-Supervised Semi-Supervised Conditional Generative\n  Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel self-supervised learning approach for conditional\ngenerative adversarial networks (GANs) under a semi-supervised setting. Unlike\nprior self-supervised approaches which often involve geometric augmentations on\nthe image space such as predicting rotation angles, our pretext task leverages\nthe label space. We perform augmentation by randomly sampling sensible labels\nfrom the label space of the few labelled examples available and assigning them\nas target labels to the abundant unlabelled examples from the same distribution\nas that of the labelled ones. The images are then translated and grouped into\npositive and negative pairs by their target labels, acting as training examples\nfor our pretext task which involves optimising an auxiliary match loss on the\ndiscriminator's side. We tested our method on two challenging benchmarks,\nCelebA and RaFD, and evaluated the results using standard metrics including\nFr\\'{e}chet Inception Distance, Inception Score, and Attribute Classification\nRate. Extensive empirical evaluation demonstrates the effectiveness of our\nproposed method over competitive baselines and existing arts. In particular,\nour method surpasses the baseline with only 20% of the labelled examples used\nto train the baseline.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:14:55 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 18:57:44 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Sun", "Jiaze", ""], ["Bhattarai", "Binod", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "2006.06624", "submitter": "Jonathan Williams", "authors": "Jonathan Williams, Carola-Bibiane Sch\\\"onlieb, Tom Swinfield, Bambang\n  Irawan, Eva Achmad, Muhammad Zudhi, Habibi, Elva Gemita, David A. Coomes", "title": "SLIC-UAV: A Method for monitoring recovery in tropical restoration\n  projects through identification of signature species using UAVs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logged forests cover four million square kilometres of the tropics and\nrestoring these forests is essential if we are to avoid the worst impacts of\nclimate change, yet monitoring recovery is challenging. Tracking the abundance\nof visually identifiable, early-successional species enables successional\nstatus and thereby restoration progress to be evaluated. Here we present a new\npipeline, SLIC-UAV, for processing Unmanned Aerial Vehicle (UAV) imagery to map\nearly-successional species in tropical forests. The pipeline is novel because\nit comprises: (a) a time-efficient approach for labelling crowns from UAV\nimagery; (b) machine learning of species based on spectral and textural\nfeatures within individual tree crowns, and (c) automatic segmentation of\northomosaiced UAV imagery into 'superpixels', using Simple Linear Iterative\nClustering (SLIC). Creating superpixels reduces the dataset's dimensionality\nand focuses prediction onto clusters of pixels, greatly improving accuracy. To\ndemonstrate SLIC-UAV, support vector machines and random forests were used to\npredict the species of hand-labelled crowns in a restoration concession in\nIndonesia. Random forests were most accurate at discriminating species for\nwhole crowns, with accuracy ranging from 79.3% when mapping five common\nspecies, to 90.5% when mapping the three most visually-distinctive species. In\ncontrast, support vector machines proved better for labelling automatically\nsegmented superpixels, with accuracy ranging from 74.3% to 91.7% for the same\nspecies. Models were extended to map species across 100 hectares of forest. The\nstudy demonstrates the power of SLIC-UAV for mapping characteristic\nearly-successional tree species as an indicator of successional stage within\ntropical forest restoration areas. Continued effort is needed to develop\neasy-to-implement and low-cost technology to improve the affordability of\nproject management.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:22:56 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Williams", "Jonathan", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Swinfield", "Tom", ""], ["Irawan", "Bambang", ""], ["Achmad", "Eva", ""], ["Zudhi", "Muhammad", ""], ["Habibi", "", ""], ["Gemita", "Elva", ""], ["Coomes", "David A.", ""]]}, {"id": "2006.06627", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari", "title": "Diagnosis and Analysis of Celiac Disease and Environmental Enteropathy\n  on Biopsy Images using Deep Learning Approaches", "comments": "PhD dissertation, Univ Virginia (May 2020)", "journal-ref": null, "doi": "10.18130/v3-837s-3a79", "report-no": null, "categories": "cs.LG cs.CV eess.IV q-bio.TO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Celiac Disease (CD) and Environmental Enteropathy (EE) are common causes of\nmalnutrition and adversely impact normal childhood development. Both conditions\nrequire a tissue biopsy for diagnosis and a major challenge of interpreting\nclinical biopsy images to differentiate between these gastrointestinal diseases\nis striking histopathologic overlap between them. In the current study, we\npropose four diagnosis techniques for these diseases and address their\nlimitations and advantages. First, the diagnosis between CD, EE, and Normal\nbiopsies is considered, but the main challenge with this diagnosis technique is\nthe staining problem. The dataset used in this research is collected from\ndifferent centers with different staining standards. To solve this problem, we\nuse color balancing in order to train our model with a varying range of colors.\nRandom Multimodel Deep Learning (RMDL) architecture has been used as another\napproach to mitigate the effects of the staining problem. RMDL combines\ndifferent architectures and structures of deep learning and the final output of\nthe model is based on the majority vote. CD is a chronic autoimmune disease\nthat affects the small intestine genetically predisposed children and adults.\nTypically, CD rapidly progress from Marsh I to IIIa. Marsh III is sub-divided\ninto IIIa (partial villus atrophy), Marsh IIIb (subtotal villous atrophy), and\nMarsh IIIc (total villus atrophy) to explain the spectrum of villus atrophy\nalong with crypt hypertrophy and increased intraepithelial lymphocytes. In the\nsecond part of this study, we proposed two ways for diagnosing different stages\nof CD. Finally, in the third part of this study, these two steps are combined\nas Hierarchical Medical Image Classification (HMIC) to have a model to diagnose\nthe disease data hierarchically.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:25:29 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Kowsari", "Kamran", ""]]}, {"id": "2006.06628", "submitter": "Mehrdad Khani", "authors": "Mehrdad Khani, Pouya Hamadanian, Arash Nasr-Esfahany, Mohammad\n  Alizadeh", "title": "Real-Time Video Inference on Edge Devices via Adaptive Model Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NI eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time video inference on edge devices like mobile phones and drones is\nchallenging due to the high computation cost of Deep Neural Networks. We\npresent Adaptive Model Streaming (AMS), a new approach to improving performance\nof efficient lightweight models for video inference on edge devices. AMS uses a\nremote server to continually train and adapt a small model running on the edge\ndevice, boosting its performance on the live video using online knowledge\ndistillation from a large, state-of-the-art model. We discuss the challenges of\nover-the-network model adaptation for video inference, and present several\ntechniques to reduce communication cost of this approach: avoiding excessive\noverfitting, updating a small fraction of important model parameters, and\nadaptive sampling of training frames at edge devices. On the task of video\nsemantic segmentation, our experimental results show 0.4--17.8 percent mean\nIntersection-over-Union improvement compared to a pre-trained model across\nseveral video datasets. Our prototype can perform video segmentation at 30\nframes-per-second with 40 milliseconds camera-to-label latency on a Samsung\nGalaxy S10+ mobile phone, using less than 300 Kbps uplink and downlink\nbandwidth on the device.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:25:44 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 23:29:53 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Khani", "Mehrdad", ""], ["Hamadanian", "Pouya", ""], ["Nasr-Esfahany", "Arash", ""], ["Alizadeh", "Mohammad", ""]]}, {"id": "2006.06634", "submitter": "Mihai Dusmanu", "authors": "Mihai Dusmanu, Johannes L. Sch\\\"onberger, Sudipta N. Sinha, Marc\n  Pollefeys", "title": "Privacy-Preserving Image Features via Adversarial Affine Subspace\n  Embeddings", "comments": "Accepted at CVPR 2021. 16 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision systems require users to upload image features to the\ncloud for processing and storage. These features can be exploited to recover\nsensitive information about the scene or subjects, e.g., by reconstructing the\nappearance of the original image. To address this privacy concern, we propose a\nnew privacy-preserving feature representation. The core idea of our work is to\ndrop constraints from each feature descriptor by embedding it within an affine\nsubspace containing the original feature as well as adversarial feature\nsamples. Feature matching on the privacy-preserving representation is enabled\nbased on the notion of subspace-to-subspace distance. We experimentally\ndemonstrate the effectiveness of our method and its high practical relevance\nfor the applications of visual localization and mapping as well as face\nauthentication. Compared to the original features, our approach makes it\nsignificantly more difficult for an adversary to recover private information.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:29:48 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 18:09:49 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 09:46:40 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Dusmanu", "Mihai", ""], ["Sch\u00f6nberger", "Johannes L.", ""], ["Sinha", "Sudipta N.", ""], ["Pollefeys", "Marc", ""]]}, {"id": "2006.06637", "submitter": "Shaunak Halbe", "authors": "Shaunak Halbe", "title": "Exploring Weaknesses of VQA Models through Attribution Driven Insights", "comments": "Second Grand-Challenge and Workshop on Multimodal Language, ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have been successfully used for the task of Visual\nQuestion Answering for the past few years owing to the availability of relevant\nlarge scale datasets. However these datasets are created in artificial settings\nand rarely reflect the real world scenario. Recent research effectively applies\nthese VQA models for answering visual questions for the blind. Despite\nachieving high accuracy these models appear to be susceptible to variation in\ninput questions.We analyze popular VQA models through the lens of attribution\n(input's influence on predictions) to gain valuable insights. Further, We use\nthese insights to craft adversarial attacks which inflict significant damage to\nthese systems with negligible change in meaning of the input questions. We\nbelieve this will enhance development of systems more robust to the possible\nvariations in inputs when deployed to assist the visually impaired.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:30:07 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 12:01:03 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Halbe", "Shaunak", ""]]}, {"id": "2006.06640", "submitter": "Isaac Robinson", "authors": "Isaac Robinson", "title": "Interpretable Visualizations with Differentiating Embedding Networks", "comments": "10 pages, 4 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a visualization algorithm based on a novel unsupervised Siamese\nneural network training regime and loss function, called Differentiating\nEmbedding Networks (DEN). The Siamese neural network finds differentiating or\nsimilar features between specific pairs of samples in a dataset, and uses these\nfeatures to embed the dataset in a lower dimensional space where it can be\nvisualized. Unlike existing visualization algorithms such as UMAP or $t$-SNE,\nDEN is parametric, meaning it can be interpreted by techniques such as SHAP. To\ninterpret DEN, we create an end-to-end parametric clustering algorithm on top\nof the visualization, and then leverage SHAP scores to determine which features\nin the sample space are important for understanding the structures shown in the\nvisualization based on the clusters found. We compare DEN visualizations with\nexisting techniques on a variety of datasets, including image and scRNA-seq\ndata. We then show that our clustering algorithm performs similarly to the\nstate of the art despite not having prior knowledge of the number of clusters,\nand sets a new state of the art on FashionMNIST. Finally, we demonstrate\nfinding differentiating features of a dataset. Code available at\nhttps://github.com/isaacrob/DEN\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:30:44 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Robinson", "Isaac", ""]]}, {"id": "2006.06649", "submitter": "Qing Li", "authors": "Qing Li, Siyuan Huang, Yining Hong, Yixin Chen, Ying Nian Wu,\n  Song-Chun Zhu", "title": "Closed Loop Neural-Symbolic Learning via Integrating Neural Perception,\n  Grammar Parsing, and Symbolic Reasoning", "comments": "ICML 2020. Project page: https://liqing-ustc.github.io/NGS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of neural-symbolic computation is to integrate the connectionist and\nsymbolist paradigms. Prior methods learn the neural-symbolic models using\nreinforcement learning (RL) approaches, which ignore the error propagation in\nthe symbolic reasoning module and thus converge slowly with sparse rewards. In\nthis paper, we address these issues and close the loop of neural-symbolic\nlearning by (1) introducing the \\textbf{grammar} model as a \\textit{symbolic\nprior} to bridge neural perception and symbolic reasoning, and (2) proposing a\nnovel \\textbf{back-search} algorithm which mimics the top-down human-like\nlearning procedure to propagate the error through the symbolic reasoning module\nefficiently. We further interpret the proposed learning framework as maximum\nlikelihood estimation using Markov chain Monte Carlo sampling and the\nback-search algorithm as a Metropolis-Hastings sampler. The experiments are\nconducted on two weakly-supervised neural-symbolic tasks: (1) handwritten\nformula recognition on the newly introduced HWF dataset; (2) visual question\nanswering on the CLEVR dataset. The results show that our approach\nsignificantly outperforms the RL methods in terms of performance, converging\nspeed, and data efficiency. Our code and data are released at\n\\url{https://liqing-ustc.github.io/NGS}.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:42:49 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 22:17:10 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Li", "Qing", ""], ["Huang", "Siyuan", ""], ["Hong", "Yining", ""], ["Chen", "Yixin", ""], ["Wu", "Ying Nian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2006.06658", "submitter": "Yunpeng Shi", "authors": "Yunpeng Shi, Shaohan Li and Gilad Lerman", "title": "Robust Multi-object Matching via Iterative Reweighting of the Graph\n  Connection Laplacian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient and robust iterative solution to the multi-object\nmatching problem. We first clarify serious limitations of current methods as\nwell as the inappropriateness of the standard iteratively reweighted least\nsquares procedure. In view of these limitations, we suggest a novel and more\nreliable iterative reweighting strategy that incorporates information from\nhigher-order neighborhoods by exploiting the graph connection Laplacian. We\ndemonstrate the superior performance of our procedure over state-of-the-art\nmethods using both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:53:01 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 20:54:12 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Shi", "Yunpeng", ""], ["Li", "Shaohan", ""], ["Lerman", "Gilad", ""]]}, {"id": "2006.06664", "submitter": "Fisher Yu", "authors": "Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor\n  Darrell, Fisher Yu", "title": "Quasi-Dense Similarity Learning for Multiple Object Tracking", "comments": "Multiple object tracking on large-scale datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity learning has been recognized as a crucial step for object\ntracking. However, existing multiple object tracking methods only use sparse\nground truth matching as the training objective, while ignoring the majority of\nthe informative regions on the images. In this paper, we present Quasi-Dense\nSimilarity Learning, which densely samples hundreds of region proposals on a\npair of images for contrastive learning. We can directly combine this\nsimilarity learning with existing detection methods to build Quasi-Dense\nTracking (QDTrack) without turning to displacement regression or motion priors.\nWe also find that the resulting distinctive feature space admits a simple\nnearest neighbor search at the inference time. Despite its simplicity, QDTrack\noutperforms all existing methods on MOT, BDD100K, Waymo, and TAO tracking\nbenchmarks. It achieves 68.7 MOTA at 20.3 FPS on MOT17 without using external\ntraining data. Compared to methods with similar detectors, it boosts almost 10\npoints of MOTA and significantly decreases the number of ID switches on BDD100K\nand Waymo datasets. The code is available at https://github.com/SysCV/qdtrack\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:57:12 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 09:00:01 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 21:54:02 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Pang", "Jiangmiao", ""], ["Qiu", "Linlu", ""], ["Li", "Xia", ""], ["Chen", "Haofeng", ""], ["Li", "Qi", ""], ["Darrell", "Trevor", ""], ["Yu", "Fisher", ""]]}, {"id": "2006.06666", "submitter": "Karan Desai", "authors": "Karan Desai, Justin Johnson", "title": "VirTex: Learning Visual Representations from Textual Annotations", "comments": "Code available at https://github.com/kdexd/virtex", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The de-facto approach to many vision tasks is to start from pretrained visual\nrepresentations, typically learned via supervised training on ImageNet. Recent\nmethods have explored unsupervised pretraining to scale to vast quantities of\nunlabeled images. In contrast, we aim to learn high-quality visual\nrepresentations from fewer images. To this end, we revisit supervised\npretraining, and seek data-efficient alternatives to classification-based\npretraining. We propose VirTex -- a pretraining approach using semantically\ndense captions to learn visual representations. We train convolutional networks\nfrom scratch on COCO Captions, and transfer them to downstream recognition\ntasks including image classification, object detection, and instance\nsegmentation. On all tasks, VirTex yields features that match or exceed those\nlearned on ImageNet -- supervised or unsupervised -- despite using up to ten\ntimes fewer images.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:58:48 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 12:03:24 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Desai", "Karan", ""], ["Johnson", "Justin", ""]]}, {"id": "2006.06668", "submitter": "Han Hu", "authors": "Minghao Yin and Zhuliang Yao and Yue Cao and Xiu Li and Zheng Zhang\n  and Stephen Lin and Han Hu", "title": "Disentangled Non-Local Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The non-local block is a popular module for strengthening the context\nmodeling ability of a regular convolutional neural network. This paper first\nstudies the non-local block in depth, where we find that its attention\ncomputation can be split into two terms, a whitened pairwise term accounting\nfor the relationship between two pixels and a unary term representing the\nsaliency of every pixel. We also observe that the two terms trained alone tend\nto model different visual clues, e.g. the whitened pairwise term learns\nwithin-region relationships while the unary term learns salient boundaries.\nHowever, the two terms are tightly coupled in the non-local block, which\nhinders the learning of each. Based on these findings, we present the\ndisentangled non-local block, where the two terms are decoupled to facilitate\nlearning for both terms. We demonstrate the effectiveness of the decoupled\ndesign on various tasks, such as semantic segmentation on Cityscapes, ADE20K\nand PASCAL Context, object detection on COCO, and action recognition on\nKinetics.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:59:22 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 14:12:09 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Yin", "Minghao", ""], ["Yao", "Zhuliang", ""], ["Cao", "Yue", ""], ["Li", "Xiu", ""], ["Zhang", "Zheng", ""], ["Lin", "Stephen", ""], ["Hu", "Han", ""]]}, {"id": "2006.06669", "submitter": "Dandan Shan", "authors": "Dandan Shan, Jiaqi Geng, Michelle Shu, David F. Fouhey", "title": "Understanding Human Hands in Contact at Internet Scale", "comments": "To appear at CVPR 2020 (Oral). Project and dataset webpage:\n  http://fouheylab.eecs.umich.edu/~dandans/projects/100DOH/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hands are the central means by which humans manipulate their world and being\nable to reliably extract hand state information from Internet videos of humans\nengaged in their hands has the potential to pave the way to systems that can\nlearn from petabytes of video data. This paper proposes steps towards this by\ninferring a rich representation of hands engaged in interaction method that\nincludes: hand location, side, contact state, and a box around the object in\ncontact. To support this effort, we gather a large-scale dataset of hands in\ncontact with objects consisting of 131 days of footage as well as a 100K\nannotated hand-contact video frame dataset. The learned model on this dataset\ncan serve as a foundation for hand-contact understanding in videos. We\nquantitatively evaluate it both on its own and in service of predicting and\nlearning from 3D meshes of human hands.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:59:30 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Shan", "Dandan", ""], ["Geng", "Jiaqi", ""], ["Shu", "Michelle", ""], ["Fouhey", "David F.", ""]]}, {"id": "2006.06676", "submitter": "Samuli Laine", "authors": "Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko\n  Lehtinen, Timo Aila", "title": "Training Generative Adversarial Networks with Limited Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training generative adversarial networks (GAN) using too little data\ntypically leads to discriminator overfitting, causing training to diverge. We\npropose an adaptive discriminator augmentation mechanism that significantly\nstabilizes training in limited data regimes. The approach does not require\nchanges to loss functions or network architectures, and is applicable both when\ntraining from scratch and when fine-tuning an existing GAN on another dataset.\nWe demonstrate, on several datasets, that good results are now possible using\nonly a few thousand training images, often matching StyleGAN2 results with an\norder of magnitude fewer images. We expect this to open up new application\ndomains for GANs. We also find that the widely used CIFAR-10 is, in fact, a\nlimited data benchmark, and improve the record FID from 5.59 to 2.42.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:06:34 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 17:09:24 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Karras", "Tero", ""], ["Aittala", "Miika", ""], ["Hellsten", "Janne", ""], ["Laine", "Samuli", ""], ["Lehtinen", "Jaakko", ""], ["Aila", "Timo", ""]]}, {"id": "2006.06715", "submitter": "Kecheng Xu", "authors": "Kecheng Xu, Xiangquan Xiao, Jinghao Miao, Qi Luo", "title": "Data Driven Prediction Architecture for Autonomous Driving and its\n  Application on Apollo Platform", "comments": "Accepted by the 31st IEEE Intelligent Vehicles Symposium (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous Driving vehicles (ADV) are on road with large scales. For safe and\nefficient operations, ADVs must be able to predict the future states and\niterative with road entities in complex, real-world driving scenarios. How to\nmigrate a well-trained prediction model from one geo-fenced area to another is\nessential in scaling the ADV operation and is difficult most of the time since\nthe terrains, traffic rules, entities distributions, driving/walking patterns\nwould be largely different in different geo-fenced operation areas. In this\npaper, we introduce a highly automated learning-based prediction model\npipeline, which has been deployed on Baidu Apollo self-driving platform, to\nsupport different prediction learning sub-modules' data annotation, feature\nextraction, model training/tuning and deployment. This pipeline is completely\nautomatic without any human intervention and shows an up to 400\\% efficiency\nincrease in parameter tuning, when deployed at scale in different scenarios\nacross nations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 18:16:12 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Xu", "Kecheng", ""], ["Xiao", "Xiangquan", ""], ["Miao", "Jinghao", ""], ["Luo", "Qi", ""]]}, {"id": "2006.06740", "submitter": "Gonzalo Garde", "authors": "Gonzalo Garde, Andoni Larumbe-Bergera, Beno\\^it Bossavit, Rafael\n  Cabeza, Sonia Porta and Arantxa Villanueva", "title": "Gaze estimation problem tackled through synthetic images", "comments": "https://dl.acm.org/doi/abs/10.1145/3379156.3391368", "journal-ref": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and\n  Applications; June 2020; Article No.: 16; Pages 1 to 5", "doi": "10.1145/3379156.3391368", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we evaluate a synthetic framework to be used in the field of\ngaze estimation employing deep learning techniques. The lack of sufficient\nannotated data could be overcome by the utilization of a synthetic evaluation\nframework as far as it resembles the behavior of a real scenario. In this work,\nwe use U2Eyes synthetic environment employing I2Head datataset as real\nbenchmark for comparison based on alternative training and testing strategies.\nThe results obtained show comparable average behavior between both frameworks\nalthough significantly more robust and stable performance is retrieved by the\nsynthetic images. Additionally, the potential of synthetically pretrained\nmodels in order to be applied in user's specific calibration strategies is\nshown with outstanding performances.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 18:53:51 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Garde", "Gonzalo", ""], ["Larumbe-Bergera", "Andoni", ""], ["Bossavit", "Beno\u00eet", ""], ["Cabeza", "Rafael", ""], ["Porta", "Sonia", ""], ["Villanueva", "Arantxa", ""]]}, {"id": "2006.06746", "submitter": "Reza Jalil Mozhdehi", "authors": "Reza Jalil Mozhdehi and Henry Medeiros", "title": "Deep Convolutional Likelihood Particle Filter for Visual Tracking", "comments": "Accepted in Transactions on Computational Science & Computational\n  Intelligence, 11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel particle filter for convolutional-correlation visual\ntrackers. Our method uses correlation response maps to estimate likelihood\ndistributions and employs these likelihoods as proposal densities to sample\nparticles. Likelihood distributions are more reliable than proposal densities\nbased on target transition distributions because correlation response maps\nprovide additional information regarding the target's location. Additionally,\nour particle filter searches for multiple modes in the likelihood distribution,\nwhich improves performance in target occlusion scenarios while decreasing\ncomputational costs by more efficiently sampling particles. In other\nchallenging scenarios such as those involving motion blur, where only one mode\nis present but a larger search area may be necessary, our particle filter\nallows for the variance of the likelihood distribution to increase. We tested\nour algorithm on the Visual Tracker Benchmark v1.1 (OTB100) and our\nexperimental results demonstrate that our framework outperforms\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 19:02:27 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Mozhdehi", "Reza Jalil", ""], ["Medeiros", "Henry", ""]]}, {"id": "2006.06752", "submitter": "Johannes Ball\\'e", "authors": "Sangnie Bhardwaj, Ian Fischer, Johannes Ball\\'e, Troy Chinen", "title": "An Unsupervised Information-Theoretic Perceptual Quality Metric", "comments": "19 pages, 10 figures. Presented at NeurIPS 2020. Code available at\n  https://github.com/google-research/perceptual-quality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tractable models of human perception have proved to be challenging to build.\nHand-designed models such as MS-SSIM remain popular predictors of human image\nquality judgements due to their simplicity and speed. Recent modern deep\nlearning approaches can perform better, but they rely on supervised data which\ncan be costly to gather: large sets of class labels such as ImageNet, image\nquality ratings, or both. We combine recent advances in information-theoretic\nobjective functions with a computational architecture informed by the\nphysiology of the human visual system and unsupervised training on pairs of\nvideo frames, yielding our Perceptual Information Metric (PIM). We show that\nPIM is competitive with supervised metrics on the recent and challenging BAPPS\nimage quality assessment dataset and outperforms them in predicting the ranking\nof image compression methods in CLIC 2020. We also perform qualitative\nexperiments using the ImageNet-C dataset, and establish that PIM is robust with\nrespect to architectural details.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 19:11:28 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 01:33:55 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 19:28:57 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bhardwaj", "Sangnie", ""], ["Fischer", "Ian", ""], ["Ball\u00e9", "Johannes", ""], ["Chinen", "Troy", ""]]}, {"id": "2006.06753", "submitter": "Nitin J. Sanket", "authors": "Nitin J. Sanket, Chahat Deep Singh, Cornelia Ferm\\\"uller, Yiannis\n  Aloimonos", "title": "PRGFlow: Benchmarking SWAP-Aware Unified Deep Visual Inertial Odometry", "comments": "16 pages, 13 figures, 10 tables. Under review T-RO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Odometry on aerial robots has to be of low latency and high robustness whilst\nalso respecting the Size, Weight, Area and Power (SWAP) constraints as demanded\nby the size of the robot. A combination of visual sensors coupled with Inertial\nMeasurement Units (IMUs) has proven to be the best combination to obtain robust\nand low latency odometry on resource-constrained aerial robots. Recently, deep\nlearning approaches for Visual Inertial fusion have gained momentum due to\ntheir high accuracy and robustness. However, the remarkable advantages of these\ntechniques are their inherent scalability (adaptation to different sized aerial\nrobots) and unification (same method works on different sized aerial robots) by\nutilizing compression methods and hardware acceleration, which have been\nlacking from previous approaches.\n  To this end, we present a deep learning approach for visual translation\nestimation and loosely fuse it with an Inertial sensor for full 6DoF odometry\nestimation. We also present a detailed benchmark comparing different\narchitectures, loss functions and compression methods to enable scalability. We\nevaluate our network on the MSCOCO dataset and evaluate the VI fusion on\nmultiple real-flight trajectories.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 19:12:54 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Sanket", "Nitin J.", ""], ["Singh", "Chahat Deep", ""], ["Ferm\u00fcller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "2006.06756", "submitter": "Xiang Xu", "authors": "Xiang Xu and Yuanjun Xiong and Wei Xia", "title": "On Improving Temporal Consistency for Online Face Liveness Detection", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on improving the online face liveness detection\nsystem to enhance the security of the downstream face recognition system. Most\nof the existing frame-based methods are suffering from the prediction\ninconsistency across time. To address the issue, a simple yet effective\nsolution based on temporal consistency is proposed. Specifically, in the\ntraining stage, to integrate the temporal consistency constraint, a temporal\nself-supervision loss and a class consistency loss are proposed in addition to\nthe softmax cross-entropy loss. In the deployment stage, a training-free\nnon-parametric uncertainty estimation module is developed to smooth the\npredictions adaptively. Beyond the common evaluation approach, a video\nsegment-based evaluation is proposed to accommodate more practical scenarios.\nExtensive experiments demonstrated that our solution is more robust against\nseveral presentation attacks in various scenarios, and significantly\noutperformed the state-of-the-art on multiple public datasets by at least 40%\nin terms of ACER. Besides, with much less computational complexity (33% fewer\nFLOPs), it provides great potential for low-latency online applications.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 19:19:47 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Xu", "Xiang", ""], ["Xiong", "Yuanjun", ""], ["Xia", "Wei", ""]]}, {"id": "2006.06769", "submitter": "Heng Yang", "authors": "Heng Yang, Luca Carlone", "title": "One Ring to Rule Them All: Certifiably Robust Geometric Perception with\n  Outliers", "comments": "NeurIPS 2020. 9 pages main results, 34 pages total. Code available at\n  https://github.com/MIT-SPARK/CertifiablyRobustPerception", "journal-ref": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020)", "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first general and practical framework to design certifiable\nalgorithms for robust geometric perception in the presence of a large amount of\noutliers. We investigate the use of a truncated least squares (TLS) cost\nfunction, which is known to be robust to outliers, but leads to hard,\nnonconvex, and nonsmooth optimization problems. Our first contribution is to\nshow that -for a broad class of geometric perception problems- TLS estimation\ncan be reformulated as an optimization over the ring of polynomials and\nLasserre's hierarchy of convex moment relaxations is empirically tight at the\nminimum relaxation order (i.e., certifiably obtains the global minimum of the\nnonconvex TLS problem). Our second contribution is to exploit the structural\nsparsity of the objective and constraint polynomials and leverage basis\nreduction to significantly reduce the size of the semidefinite program (SDP)\nresulting from the moment relaxation, without compromising its tightness. Our\nthird contribution is to develop scalable dual optimality certifiers from the\nlens of sums-of-squares (SOS) relaxation, that can compute the suboptimality\ngap and possibly certify global optimality of any candidate solution (e.g.,\nreturned by fast heuristics such as RANSAC or graduated non-convexity). Our\ndual certifiers leverage Douglas-Rachford Splitting to solve a convex\nfeasibility SDP. Numerical experiments across different perception problems,\nincluding single rotation averaging, shape alignment, 3D point cloud and mesh\nregistration, and high-integrity satellite pose estimation, demonstrate the\ntightness of our relaxations, the correctness of the certification, and the\nscalability of the proposed dual certifiers to large problems, beyond the reach\nof current SDP solvers.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 19:46:42 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 14:27:57 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yang", "Heng", ""], ["Carlone", "Luca", ""]]}, {"id": "2006.06787", "submitter": "Xiang Xu", "authors": "Xiang Xu, Nikolaos Sarafianos, Ioannis A. Kakadiaris", "title": "On Improving the Generalization of Face Recognition in the Presence of\n  Occlusions", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address a key limitation of existing 2D face recognition\nmethods: robustness to occlusions. To accomplish this task, we systematically\nanalyzed the impact of facial attributes on the performance of a\nstate-of-the-art face recognition method and through extensive experimentation,\nquantitatively analyzed the performance degradation under different types of\nocclusion. Our proposed Occlusion-aware face REcOgnition (OREO) approach\nlearned discriminative facial templates despite the presence of such\nocclusions. First, an attention mechanism was proposed that extracted local\nidentity-related region. The local features were then aggregated with the\nglobal representations to form a single template. Second, a simple, yet\neffective, training strategy was introduced to balance the non-occluded and\noccluded facial images. Extensive experiments demonstrated that OREO improved\nthe generalization ability of face recognition under occlusions by (10.17%) in\na single-image-based setting and outperformed the baseline by approximately\n(2%) in terms of rank-1 accuracy in an image-set-based scenario.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 20:17:23 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Xu", "Xiang", ""], ["Sarafianos", "Nikolaos", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "2006.06799", "submitter": "Moshe Eliasof", "authors": "Jonathan Ephrath, Lars Ruthotto, Eran Treister", "title": "Multigrid-in-Channels Architectures for Wide Convolutional Neural\n  Networks", "comments": "This paper has been withdrawn by the authors. This paper has been\n  superseded by arXiv:2011.09128", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multigrid approach that combats the quadratic growth of the\nnumber of parameters with respect to the number of channels in standard\nconvolutional neural networks (CNNs). It has been shown that there is a\nredundancy in standard CNNs, as networks with much sparser convolution\noperators can yield similar performance to full networks. The sparsity patterns\nthat lead to such behavior, however, are typically random, hampering hardware\nefficiency. In this work, we present a multigrid-in-channels approach for\nbuilding CNN architectures that achieves full coupling of the channels, and\nwhose number of parameters is linearly proportional to the width of the\nnetwork. To this end, we replace each convolution layer in a generic CNN with a\nmultilevel layer consisting of structured (i.e., grouped) convolutions. Our\nexamples from supervised image classification show that applying this strategy\nto residual networks and MobileNetV2 considerably reduces the number of\nparameters without negatively affecting accuracy. Therefore, we can widen\nnetworks without dramatically increasing the number of parameters or\noperations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 20:28:36 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 18:30:01 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Ephrath", "Jonathan", ""], ["Ruthotto", "Lars", ""], ["Treister", "Eran", ""]]}, {"id": "2006.06805", "submitter": "Adora DSouza", "authors": "Adora M. DSouza, Anas Z. Abidin, and Axel Wism\\\"uller", "title": "Automated Identification of Thoracic Pathology from Chest Radiographs\n  with Enhanced Training Pipeline", "comments": "6 pages, 1 figure, 2 tables", "journal-ref": "Proc. SPIE 10950, Medical Imaging 2019: Computer-Aided Diagnosis,\n  vol. 10950, p. 109503F, (2019)", "doi": "10.1117/12.2512600", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest x-rays are the most common radiology studies for diagnosing lung and\nheart disease. Hence, a system for automated pre-reporting of pathologic\nfindings on chest x-rays would greatly enhance radiologists' productivity. To\nthis end, we investigate a deep-learning framework with novel training schemes\nfor classification of different thoracic pathology labels from chest x-rays. We\nuse the currently largest publicly available annotated dataset ChestX-ray14 of\n112,120 chest radiographs of 30,805 patients. Each image was annotated with\neither a 'NoFinding' class, or one or more of 14 thoracic pathology labels.\nSubjects can have multiple pathologies, resulting in a multi-class, multi-label\nproblem. We encoded labels as binary vectors using k-hot encoding. We study the\nResNet34 architecture, pre-trained on ImageNet, where two key modifications\nwere incorporated into the training framework: (1) Stochastic gradient descent\nwith momentum and with restarts using cosine annealing, (2) Variable image\nsizes for fine-tuning to prevent overfitting. Additionally, we use a heuristic\nalgorithm to select a good learning rate. Learning with restarts was used to\navoid local minima. Area Under receiver operating characteristics Curve (AUC)\nwas used to quantitatively evaluate diagnostic quality. Our results are\ncomparable to, or outperform the best results of current state-of-the-art\nmethods with AUCs as follows: Atelectasis:0.81, Cardiomegaly:0.91,\nConsolidation:0.81, Edema:0.92, Effusion:0.89, Emphysema: 0.92, Fibrosis:0.81,\nHernia:0.84, Infiltration:0.73, Mass:0.85, Nodule:0.76, Pleural\nThickening:0.81, Pneumonia:0.77, Pneumothorax:0.89 and NoFinding:0.79. Our\nresults suggest that, in addition to using sophisticated network architectures,\na good learning rate, scheduler and a robust optimizer can boost performance.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 20:43:09 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["DSouza", "Adora M.", ""], ["Abidin", "Anas Z.", ""], ["Wism\u00fcller", "Axel", ""]]}, {"id": "2006.06823", "submitter": "Monica Hernandez", "authors": "Monica Hernandez", "title": "Combining the band-limited parameterization and Semi-Lagrangian\n  Runge--Kutta integration for efficient PDE-constrained LDDMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The family of PDE-constrained LDDMM methods is emerging as a particularly\ninteresting approach for physically meaningful diffeomorphic transformations.\nThe original combination of Gauss--Newton--Krylov optimization and Runge--Kutta\nintegration, shows excellent numerical accuracy and fast convergence rate.\nHowever, its most significant limitation is the huge computational complexity,\nhindering its extensive use in Computational Anatomy applied studies. This\nlimitation has been treated independently by the problem formulation in the\nspace of band-limited vector fields and Semi-Lagrangian integration. The\npurpose of this work is to combine both in three variants of band-limited\nPDE-constrained LDDMM for further increasing their computational efficiency.\nThe accuracy of the resulting methods is evaluated extensively. For all the\nvariants, the proposed combined approach shows a significant increment of the\ncomputational efficiency. In addition, the variant based on the deformation\nstate equation is positioned consistently as the best performing method across\nall the evaluation frameworks in terms of accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 07:12:18 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Hernandez", "Monica", ""]]}, {"id": "2006.06868", "submitter": "Alvin Wan", "authors": "Alvin Wan, Daniel Ho, Younjin Song, Henk Tillman, Sarah Adel Bargal,\n  Joseph E. Gonzalez", "title": "SegNBDT: Visual Decision Rules for Segmentation", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The black-box nature of neural networks limits model decision\ninterpretability, in particular for high-dimensional inputs in computer vision\nand for dense pixel prediction tasks like segmentation. To address this, prior\nwork combines neural networks with decision trees. However, such models (1)\nperform poorly when compared to state-of-the-art segmentation models or (2)\nfail to produce decision rules with spatially-grounded semantic meaning. In\nthis work, we build a hybrid neural-network and decision-tree model for\nsegmentation that (1) attains neural network segmentation accuracy and (2)\nprovides semi-automatically constructed visual decision rules such as \"Is there\na window?\". We obtain semantic visual meaning by extending saliency methods to\nsegmentation and attain accuracy by leveraging insights from neural-backed\ndecision trees, a deep learning analog of decision trees for image\nclassification. Our model SegNBDT attains accuracy within ~2-4% of the\nstate-of-the-art HRNetV2 segmentation model while also retaining\nexplainability; we achieve state-of-the-art performance for explainable models\non three benchmark datasets -- Pascal-Context (49.12%), Cityscapes (79.01%),\nand Look Into Person (51.64%). Furthermore, user studies suggest visual\ndecision rules are more interpretable, particularly for incorrect predictions.\nCode and pretrained models can be found at\nhttps://github.com/daniel-ho/SegNBDT.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 23:10:02 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Wan", "Alvin", ""], ["Ho", "Daniel", ""], ["Song", "Younjin", ""], ["Tillman", "Henk", ""], ["Bargal", "Sarah Adel", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "2006.06869", "submitter": "Faith Johnson", "authors": "Faith Johnson, Kristin Dana", "title": "Feudal Steering: Hierarchical Learning for Steering Angle Prediction", "comments": "InThe IEEE/CVFConference on Computer Vision and Pattern\n  Recognition(CVPR) Workshops, June 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the challenge of automated steering angle prediction for self\ndriving cars using egocentric road images. In this work, we explore the use of\nfeudal networks, used in hierarchical reinforcement learning (HRL), to devise a\nvehicle agent to predict steering angles from first person, dash-cam images of\nthe Udacity driving dataset. Our method, Feudal Steering, is inspired by recent\nwork in HRL consisting of a manager network and a worker network that operate\non different temporal scales and have different goals. The manager works at a\ntemporal scale that is relatively coarse compared to the worker and has a\nhigher level, task-oriented goal space. Using feudal learning to divide the\ntask into manager and worker sub-networks provides more accurate and robust\nprediction. Temporal abstraction in driving allows more complex primitives than\nthe steering angle at a single time instance. Composite actions comprise a\nsubroutine or skill that can be re-used throughout the driving sequence. The\nassociated subroutine id is the manager network's goal, so that the manager\nseeks to succeed at the high level task (e.g. a sharp right turn, a slight\nright turn, moving straight in traffic, or moving straight unencumbered by\ntraffic). The steering angle at a particular time instance is the worker\nnetwork output which is regulated by the manager's high level task. We\ndemonstrate state-of-the art steering angle prediction results on the Udacity\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 23:17:55 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Johnson", "Faith", ""], ["Dana", "Kristin", ""]]}, {"id": "2006.06880", "submitter": "Alexander Shekhovtsov", "authors": "Alexander Shekhovtsov, Viktor Yanush", "title": "Reintroducing Straight-Through Estimators as Principled Methods for\n  Stochastic Binary Networks", "comments": "30 pages, ICLR version (rejected)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training neural networks with binary weights and activations is a challenging\nproblem due to the lack of gradients and difficulty of optimization over\ndiscrete weights. Many successful experimental results have been achieved with\nempirical straight-through (ST) approaches, proposing a variety of ad-hoc rules\nfor propagating gradients through non-differentiable activations and updating\ndiscrete weights. At the same time, ST methods can be truly derived as\nestimators in the stochastic binary network (SBN) model with Bernoulli weights.\nWe advance these derivations to a more complete and systematic study. We\nanalyze properties, estimation accuracy, obtain different forms of correct ST\nestimators for activations and weights, explain existing empirical approaches\nand their shortcomings, explain how latent weights arise from the mirror\ndescent method when optimizing over probabilities. This allows to reintroduce,\nonce empirical, ST methods as sound approximations, apply them with clarity and\ndevelop further improvements.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 23:58:18 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 15:48:44 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Shekhovtsov", "Alexander", ""], ["Yanush", "Viktor", ""]]}, {"id": "2006.06882", "submitter": "Barret Zoph", "authors": "Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin\n  D. Cubuk, Quoc V. Le", "title": "Rethinking Pre-training and Self-training", "comments": "Accepted for publication at the Thirty-fourth Conference on Neural\n  Information Processing Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training is a dominant paradigm in computer vision. For example,\nsupervised ImageNet pre-training is commonly used to initialize the backbones\nof object detection and segmentation models. He et al., however, show a\nsurprising result that ImageNet pre-training has limited impact on COCO object\ndetection. Here we investigate self-training as another method to utilize\nadditional data on the same setup and contrast it against ImageNet\npre-training. Our study reveals the generality and flexibility of self-training\nwith three additional insights: 1) stronger data augmentation and more labeled\ndata further diminish the value of pre-training, 2) unlike pre-training,\nself-training is always helpful when using stronger data augmentation, in both\nlow-data and high-data regimes, and 3) in the case that pre-training is\nhelpful, self-training improves upon pre-training. For example, on the COCO\nobject detection dataset, pre-training benefits when we use one fifth of the\nlabeled data, and hurts accuracy when we use all labeled data. Self-training,\non the other hand, shows positive improvements from +1.3 to +3.4AP across all\ndataset sizes. In other words, self-training works well exactly on the same\nsetup that pre-training does not work (using ImageNet to help COCO). On the\nPASCAL segmentation dataset, which is a much smaller dataset than COCO, though\npre-training does help significantly, self-training improves upon the\npre-trained model. On COCO object detection, we achieve 54.3AP, an improvement\nof +1.5AP over the strongest SpineNet model. On PASCAL segmentation, we achieve\n90.5 mIOU, an improvement of +1.5% mIOU over the previous state-of-the-art\nresult by DeepLabv3+.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 23:59:16 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 19:41:27 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Zoph", "Barret", ""], ["Ghiasi", "Golnaz", ""], ["Lin", "Tsung-Yi", ""], ["Cui", "Yin", ""], ["Liu", "Hanxiao", ""], ["Cubuk", "Ekin D.", ""], ["Le", "Quoc V.", ""]]}, {"id": "2006.06888", "submitter": "Leo F. Isikdogan", "authors": "Leo F Isikdogan, Bhavin V Nayak, Chyuan-Tyng Wu, Joao Peralta Moreira,\n  Sushma Rao, Gilad Michael", "title": "SemifreddoNets: Partially Frozen Neural Networks for Efficient Computer\n  Vision Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a system comprised of fixed-topology neural networks having\npartially frozen weights, named SemifreddoNets. SemifreddoNets work as\nfully-pipelined hardware blocks that are optimized to have an efficient\nhardware implementation. Those blocks freeze a certain portion of the\nparameters at every layer and replace the corresponding multipliers with fixed\nscalers. Fixing the weights reduces the silicon area, logic delay, and memory\nrequirements, leading to significant savings in cost and power consumption.\nUnlike traditional layer-wise freezing approaches, SemifreddoNets make a\nprofitable trade between the cost and flexibility by having some of the weights\nconfigurable at different scales and levels of abstraction in the model.\nAlthough fixing the topology and some of the weights somewhat limits the\nflexibility, we argue that the efficiency benefits of this strategy outweigh\nthe advantages of a fully configurable model for many use cases. Furthermore,\nour system uses repeatable blocks, therefore it has the flexibility to adjust\nmodel complexity without requiring any hardware change. The hardware\nimplementation of SemifreddoNets provides up to an order of magnitude reduction\nin silicon area and power consumption as compared to their equivalent\nimplementation on a general-purpose accelerator.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 00:31:54 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Isikdogan", "Leo F", ""], ["Nayak", "Bhavin V", ""], ["Wu", "Chyuan-Tyng", ""], ["Moreira", "Joao Peralta", ""], ["Rao", "Sushma", ""], ["Michael", "Gilad", ""]]}, {"id": "2006.06893", "submitter": "Chandra Swarathesh Addanki", "authors": "Chandra Swarathesh Addanki", "title": "Online Sequential Extreme Learning Machines: Features Combined From\n  Hundreds of Midlayers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop an algorithm called hierarchal online sequential\nlearning algorithm (H-OS-ELM) for single feed feedforward network with features\ncombined from hundreds of midlayers, the algorithm can learn chunk by chunk\nwith fixed or varying block size, we believe that the diverse selectivity of\nneurons in top layers which consists of encoded distributed information\nproduced by the other neurons offers better computational advantage over\ninference accuracy. Thus this paper proposes a Hierarchical model framework\ncombined with Online-Sequential learning algorithm, Firstly the model consists\nof subspace feature extractor which consists of subnetwork neuron, using the\nsub-features which is result of the feature extractor in first layer of the\nhierarchy we get rid of irrelevant factors which are of no use for the learning\nand iterate this process so that to recast the the subfeatures into the\nhierarchical model to be processed into more acceptable cognition. Secondly by\nusing OS-Elm we are using non-iterative style for learning we are implementing\na network which is wider and shallow which plays a important role in\ngeneralizing the overall performance which in turn boosts up the learning speed\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 00:50:04 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Addanki", "Chandra Swarathesh", ""]]}, {"id": "2006.06911", "submitter": "Eli Shlizerman", "authors": "Jingyuan Li, Eli Shlizerman", "title": "Iterate & Cluster: Iterative Semi-Supervised Action Recognition", "comments": "for associated video, see https://www.youtube.com/watch?v=ewuoz2tt73E", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel system for active semi-supervised feature-based action\nrecognition. Given time sequences of features tracked during movements our\nsystem clusters the sequences into actions. Our system is based on\nencoder-decoder unsupervised methods shown to perform clustering by\nself-organization of their latent representation through the auto-regression\ntask. These methods were tested on human action recognition benchmarks and\noutperformed non-feature based unsupervised methods and achieved comparable\naccuracy to skeleton-based supervised methods. However, such methods rely on\nK-Nearest Neighbours (KNN) associating sequences to actions, and general\nfeatures with no annotated data would correspond to approximate clusters which\ncould be further enhanced. Our system proposes an iterative semi-supervised\nmethod to address this challenge and to actively learn the association of\nclusters and actions. The method utilizes latent space embedding and clustering\nof the unsupervised encoder-decoder to guide the selection of sequences to be\nannotated in each iteration. Each iteration, the selection aims to enhance\naction recognition accuracy while choosing a small number of sequences for\nannotation. We test the approach on human skeleton-based action recognition\nbenchmarks assuming that only annotations chosen by our method are available\nand on mouse movements videos recorded in lab experiments. We show that our\nsystem can boost recognition performance with only a small percentage of\nannotations. The system can be used as an interactive annotation tool to guide\nlabeling efforts for 'in the wild' videos of various objects and actions to\nreach robust recognition.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 02:19:39 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Li", "Jingyuan", ""], ["Shlizerman", "Eli", ""]]}, {"id": "2006.06923", "submitter": "Weiya Ren", "authors": "Weiya Ren", "title": "Potential Field Guided Actor-Critic Reinforcement Learning", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of actor-critic reinforcement\nlearning. Firstly, we extend the actor-critic architecture to actor-critic-N\narchitecture by introducing more critics beyond rewards. Secondly, we combine\nthe reward-based critic with a potential-field-based critic to formulate the\nproposed potential field guided actor-critic reinforcement learning approach\n(actor-critic-2). This can be seen as a combination of the model-based\ngradients and the model-free gradients in policy improvement. State with large\npotential field often contains a strong prior information, such as pointing to\nthe target at a long distance or avoiding collision by the side of an obstacle.\nIn this situation, we should trust potential-field-based critic more as policy\nevaluation to accelerate policy improvement, where action policy tends to be\nguided. For example, in practical application, learning to avoid obstacles\nshould be guided rather than learned by trial and error. State with small\npotential filed is often lack of information, for example, at the local minimum\npoint or around the moving target. At this time, we should trust reward-based\ncritic as policy evaluation more to evaluate the long-term return. In this\ncase, action policy tends to explore. In addition, potential field evaluation\ncan be combined with planning to estimate a better state value function. In\nthis way, reward design can focus more on the final stage of reward, rather\nthan reward shaping or phased reward. Furthermore, potential field evaluation\ncan make up for the lack of communication in multi-agent cooperation problem,\ni.e., multi-agent each has a reward-based critic and a relative unified\npotential-field-based critic with prior information. Thirdly, simplified\nexperiments on predator-prey game demonstrate the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 03:09:25 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Ren", "Weiya", ""]]}, {"id": "2006.06930", "submitter": "Qingyu Zhao", "authors": "Qingyu Zhao, Zixuan Liu, Ehsan Adeli, Kilian M. Pohl", "title": "Longitudinal Self-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning analysis of longitudinal neuroimaging data is typically\nbased on supervised learning, which requires a large number of ground-truth\nlabels to be informative. As ground-truth labels are often missing or expensive\nto obtain in neuroscience, we avoid them in our analysis by combing factor\ndisentanglement with self-supervised learning to identify changes and\nconsistencies across the multiple MRIs acquired of each individual over time.\nSpecifically, we propose a new definition of disentanglement by formulating a\nmultivariate mapping between factors (e.g., brain age) associated with an MRI\nand a latent image representation. Then, factors that evolve across\nacquisitions of longitudinal sequences are disentangled from that mapping by\nself-supervised learning in such a way that changes in a single factor induce\nchange along one direction in the representation space. We implement this\nmodel, named Longitudinal Self-Supervised Learning (LSSL), via a standard\nautoencoding structure with a cosine loss to disentangle brain age from the\nimage representation. We apply LSSL to two longitudinal neuroimaging studies to\nhighlight its strength in extracting the brain-age information from MRI and\nrevealing informative characteristics associated with neurodegenerative and\nneuropsychological disorders. Moreover, the representations learned by LSSL\nfacilitate supervised classification by recording faster convergence and higher\n(or similar) prediction accuracy compared to several other representation\nlearning techniques.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 03:35:17 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 04:20:17 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhao", "Qingyu", ""], ["Liu", "Zixuan", ""], ["Adeli", "Ehsan", ""], ["Pohl", "Kilian M.", ""]]}, {"id": "2006.06936", "submitter": "Shen Yan", "authors": "Shen Yan, Yu Zheng, Wei Ao, Xiao Zeng, Mi Zhang", "title": "Does Unsupervised Architecture Representation Learning Help Neural\n  Architecture Search?", "comments": "NeurIPS 2020 camera-ready. Code:\n  https://github.com/MSU-MLSys-Lab/arch2vec", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Neural Architecture Search (NAS) methods either encode neural\narchitectures using discrete encodings that do not scale well, or adopt\nsupervised learning-based methods to jointly learn architecture representations\nand optimize architecture search on such representations which incurs search\nbias. Despite the widespread use, architecture representations learned in NAS\nare still poorly understood. We observe that the structural properties of\nneural architectures are hard to preserve in the latent space if architecture\nrepresentation learning and search are coupled, resulting in less effective\nsearch performance. In this work, we find empirically that pre-training\narchitecture representations using only neural architectures without their\naccuracies as labels considerably improve the downstream architecture search\nefficiency. To explain these observations, we visualize how unsupervised\narchitecture representation learning better encourages neural architectures\nwith similar connections and operators to cluster together. This helps to map\nneural architectures with similar performance to the same regions in the latent\nspace and makes the transition of architectures in the latent space relatively\nsmooth, which considerably benefits diverse downstream search strategies.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 04:15:34 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 21:54:36 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Yan", "Shen", ""], ["Zheng", "Yu", ""], ["Ao", "Wei", ""], ["Zeng", "Xiao", ""], ["Zhang", "Mi", ""]]}, {"id": "2006.06961", "submitter": "Ramanathan Subramanian", "authors": "Parul Gupta, Komal Chugh, Abhinav Dhall, Ramanathan Subramanian", "title": "The eyes know it: FakeET -- An Eye-tracking Database to Understand\n  Deepfake Perception", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present \\textbf{FakeET}-- an eye-tracking database to understand human\nvisual perception of \\emph{deepfake} videos. Given that the principal purpose\nof deepfakes is to deceive human observers, FakeET is designed to understand\nand evaluate the ease with which viewers can detect synthetic video artifacts.\nFakeET contains viewing patterns compiled from 40 users via the \\emph{Tobii}\ndesktop eye-tracker for 811 videos from the \\textit{Google Deepfake} dataset,\nwith a minimum of two viewings per video. Additionally, EEG responses acquired\nvia the \\emph{Emotiv} sensor are also available. The compiled data confirms (a)\ndistinct eye movement characteristics for \\emph{real} vs \\emph{fake} videos;\n(b) utility of the eye-track saliency maps for spatial forgery localization and\ndetection, and (c) Error Related Negativity (ERN) triggers in the EEG\nresponses, and the ability of the \\emph{raw} EEG signal to distinguish between\n\\emph{real} and \\emph{fake} videos.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 06:14:49 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 22:02:13 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Gupta", "Parul", ""], ["Chugh", "Komal", ""], ["Dhall", "Abhinav", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "2006.06968", "submitter": "Guan Wang", "authors": "Xin Guo, Yusuke Kikuchi, Guan Wang, Jinglin Yi, Qiong Zou, and Rui\n  Zhou", "title": "Early Detection of Retinopathy of Prematurity (ROP) in Retinal Fundus\n  Images Via Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinopathy of prematurity (ROP) is an abnormal blood vessel development in\nthe retina of a prematurely-born infant or an infant with low birth weight. ROP\nis one of the leading causes for infant blindness globally. Early detection of\nROP is critical to slow down and avert the progression to vision impairment\ncaused by ROP. Yet there is limited awareness of ROP even among medical\nprofessionals. Consequently, dataset for ROP is limited if ever available, and\nis in general extremely imbalanced in terms of the ratio between negative\nimages and positive ones. In this study, we formulate the problem of detecting\nROP in retinal fundus images in an optimization framework, and apply\nstate-of-art convolutional neural network techniques to solve this problem.\nExperimental results based on our models achieve 100 percent sensitivity, 96\npercent specificity, 98 percent accuracy, and 96 percent precision. In\naddition, our study shows that as the network gets deeper, more significant\nfeatures can be extracted for better understanding of ROP.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 07:04:13 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Guo", "Xin", ""], ["Kikuchi", "Yusuke", ""], ["Wang", "Guan", ""], ["Yi", "Jinglin", ""], ["Zou", "Qiong", ""], ["Zhou", "Rui", ""]]}, {"id": "2006.06969", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl and Enkelejda Kasneci", "title": "Multi Layer Neural Networks as Replacement for Pooling Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooling operations, which can be calculated at low cost and serve as a linear\nor nonlinear transfer function for data reduction, are found in almost every\nmodern neural network. Countless modern approaches have already tackled\nreplacing the common maximum value selection and mean value operations, not to\nmention providing a function that allows different functions to be selected\nthrough changing parameters. Additional neural networks are used to estimate\nthe parameters of these pooling functions.Consequently, pooling layers may\nrequire supplementary parameters to increase the complexity of the whole model.\nIn this work, we show that one perceptron can already be used effectively as a\npooling operation without increasing the complexity of the model. This kind of\npooling allows for the integration of multi-layer neural networks directly into\na model as a pooling operation by restructuring the data and, as a result,\nlearnin complex pooling operations. We compare our approach to tensor\nconvolution with strides as a pooling operation and show that our approach is\nboth effective and reduces complexity. The restructuring of the data in\ncombination with multiple perceptrons allows for our approach to be used for\nupscaling, which can then be utilized for transposed convolutions in semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 07:08:38 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 07:03:37 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 14:23:00 GMT"}, {"version": "v4", "created": "Sun, 17 Jan 2021 12:02:52 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2006.06976", "submitter": "Xu-Yao Zhang", "authors": "Xu-Yao Zhang, Cheng-Lin Liu, Ching Y. Suen", "title": "Towards Robust Pattern Recognition: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracies for many pattern recognition tasks have increased rapidly year\nby year, achieving or even outperforming human performance. From the\nperspective of accuracy, pattern recognition seems to be a nearly-solved\nproblem. However, once launched in real applications, the high-accuracy pattern\nrecognition systems may become unstable and unreliable, due to the lack of\nrobustness in open and changing environments. In this paper, we present a\ncomprehensive review of research towards robust pattern recognition from the\nperspective of breaking three basic and implicit assumptions: closed-world\nassumption, independent and identically distributed assumption, and clean and\nbig data assumption, which form the foundation of most pattern recognition\nmodels. Actually, our brain is robust at learning concepts continually and\nincrementally, in complex, open and changing environments, with different\ncontexts, modalities and tasks, by showing only a few examples, under weak or\nnoisy supervision. These are the major differences between human intelligence\nand machine intelligence, which are closely related to the above three\nassumptions. After witnessing the significant progress in accuracy improvement\nnowadays, this review paper will enable us to analyze the shortcomings and\nlimitations of current methods and identify future research directions for\nrobust pattern recognition.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 07:24:27 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Zhang", "Xu-Yao", ""], ["Liu", "Cheng-Lin", ""], ["Suen", "Ching Y.", ""]]}, {"id": "2006.06979", "submitter": "Masahiro Kato", "authors": "Masahiro Kato, Takeshi Teshima", "title": "Non-Negative Bregman Divergence Minimization for Deep Direct Density\n  Ratio Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density ratio estimation (DRE) is at the core of various machine learning\ntasks such as anomaly detection and domain adaptation. In existing studies on\nDRE, methods based on Bregman divergence (BD) minimization have been\nextensively studied. However, BD minimization when applied with highly flexible\nmodels, such as deep neural networks, tends to suffer from what we call\ntrain-loss hacking, which is a source of overfitting caused by a typical\ncharacteristic of empirical BD estimators. In this paper, to mitigate\ntrain-loss hacking, we propose a non-negative correction for empirical BD\nestimators. Theoretically, we confirm the soundness of the proposed method\nthrough a generalization error bound. Through our experiments, the proposed\nmethods show a favorable performance in inlier-based outlier detection.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 07:39:03 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 09:32:41 GMT"}, {"version": "v3", "created": "Sat, 17 Jul 2021 09:25:37 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Kato", "Masahiro", ""], ["Teshima", "Takeshi", ""]]}, {"id": "2006.06986", "submitter": "Shin-Fang Chng", "authors": "Tat-Jun Chin, David Suter, Shin-Fang Chng, James Quach", "title": "Quantum Robust Fitting", "comments": "Appears in: Asian Conference on Computer Vision 2020 (ACCV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision applications need to recover structure from imperfect\nmeasurements of the real world. The task is often solved by robustly fitting a\ngeometric model onto noisy and outlier-contaminated data. However, recent\ntheoretical analyses indicate that many commonly used formulations of robust\nfitting in computer vision are not amenable to tractable solution and\napproximation. In this paper, we explore the usage of quantum computers for\nrobust fitting. To do so, we examine and establish the practical usefulness of\na robust fitting formulation inspired by Fourier analysis of Boolean functions.\nWe then investigate a quantum algorithm to solve the formulation and analyse\nthe computational speed-up possible over the classical algorithm. Our work thus\nproposes one of the first quantum treatments of robust fitting for computer\nvision.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 08:00:55 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 02:50:08 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2020 11:02:05 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Chin", "Tat-Jun", ""], ["Suter", "David", ""], ["Chng", "Shin-Fang", ""], ["Quach", "James", ""]]}, {"id": "2006.07006", "submitter": "Pilhyeon Lee", "authors": "Pilhyeon Lee, Jinglu Wang, Yan Lu, Hyeran Byun", "title": "Weakly-supervised Temporal Action Localization by Uncertainty Modeling", "comments": "Accepted by the 35th AAAI Conference on Artificial Intelligence (AAAI\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised temporal action localization aims to learn detecting\ntemporal intervals of action classes with only video-level labels. To this end,\nit is crucial to separate frames of action classes from the background frames\n(i.e., frames not belonging to any action classes). In this paper, we present a\nnew perspective on background frames where they are modeled as\nout-of-distribution samples regarding their inconsistency. Then, background\nframes can be detected by estimating the probability of each frame being\nout-of-distribution, known as uncertainty, but it is infeasible to directly\nlearn uncertainty without frame-level labels. To realize the uncertainty\nlearning in the weakly-supervised setting, we leverage the multiple instance\nlearning formulation. Moreover, we further introduce a background entropy loss\nto better discriminate background frames by encouraging their in-distribution\n(action) probabilities to be uniformly distributed over all action classes.\nExperimental results show that our uncertainty modeling is effective at\nalleviating the interference of background frames and brings a large\nperformance gain without bells and whistles. We demonstrate that our model\nsignificantly outperforms state-of-the-art methods on the benchmarks, THUMOS'14\nand ActivityNet (1.2 & 1.3). Our code is available at\nhttps://github.com/Pilhyeon/WTAL-Uncertainty-Modeling.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 08:54:35 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 07:39:30 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 07:12:38 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Lee", "Pilhyeon", ""], ["Wang", "Jinglu", ""], ["Lu", "Yan", ""], ["Byun", "Hyeran", ""]]}, {"id": "2006.07029", "submitter": "He Wang", "authors": "He Wang, Zetian Jiang, Li Yi, Kaichun Mo, Hao Su, Leonidas J. Guibas", "title": "Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the long-neglected yet important effects of point\nsampling patterns in point cloud GANs. Through extensive experiments, we show\nthat sampling-insensitive discriminators (e.g.PointNet-Max) produce shape point\nclouds with point clustering artifacts while sampling-oversensitive\ndiscriminators (e.g.PointNet++, DGCNN) fail to guide valid shape generation. We\npropose the concept of sampling spectrum to depict the different sampling\nsensitivities of discriminators. We further study how different evaluation\nmetrics weigh the sampling pattern against the geometry and propose several\nperceptual metrics forming a sampling spectrum of metrics. Guided by the\nproposed sampling spectrum, we discover a middle-point sampling-aware baseline\ndiscriminator, PointNet-Mix, which improves all existing point cloud generators\nby a large margin on sampling-related metrics. We point out that, though recent\nresearch has been focused on the generator design, the main bottleneck of point\ncloud GAN actually lies in the discriminator design. Our work provides both\nsuggestions and tools for building future discriminators. We will release the\ncode to facilitate future research.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 09:29:24 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Wang", "He", ""], ["Jiang", "Zetian", ""], ["Yi", "Li", ""], ["Mo", "Kaichun", ""], ["Su", "Hao", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "2006.07034", "submitter": "Marissa Weis", "authors": "Marissa A. Weis, Kashyap Chitta, Yash Sharma, Wieland Brendel,\n  Matthias Bethge, Andreas Geiger and Alexander S. Ecker", "title": "Benchmarking Unsupervised Object Representations for Video Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceiving the world in terms of objects and tracking them through time is a\ncrucial prerequisite for reasoning and scene understanding. Recently, several\nmethods have been proposed for unsupervised learning of object-centric\nrepresentations. However, since these models were evaluated on different\ndownstream tasks, it remains unclear how they compare in terms of basic\nperceptual abilities such as detection, figure-ground segmentation and tracking\nof objects. To close this gap, we design a benchmark with four data sets of\nvarying complexity and seven additional test sets featuring challenging\ntracking scenarios relevant for natural videos. Using this benchmark, we\ncompare the perceptual abilities of four object-centric approaches: ViMON, a\nvideo-extension of MONet, based on recurrent spatial attention, OP3, which\nexploits clustering via spatial mixture models, as well as TBA and SCALOR,\nwhich use explicit factorization via spatial transformers. Our results suggest\nthat the architectures with unconstrained latent representations learn more\npowerful representations in terms of object detection, segmentation and\ntracking than the spatial transformer based architectures. We also observe that\nnone of the methods are able to gracefully handle the most challenging tracking\nscenarios despite their synthetic nature, suggesting that our benchmark may\nprovide fruitful guidance towards learning more robust object-centric video\nrepresentations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 09:37:24 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 07:24:56 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Weis", "Marissa A.", ""], ["Chitta", "Kashyap", ""], ["Sharma", "Yash", ""], ["Brendel", "Wieland", ""], ["Bethge", "Matthias", ""], ["Geiger", "Andreas", ""], ["Ecker", "Alexander S.", ""]]}, {"id": "2006.07084", "submitter": "Polychronis Charitidis", "authors": "Polychronis Charitidis, Giorgos Kordopatis-Zilos, Symeon Papadopoulos,\n  Ioannis Kompatsiaris", "title": "Investigating the Impact of Pre-processing and Prediction Aggregation on\n  the DeepFake Detection Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in content generation technologies (widely known as\nDeepFakes) along with the online proliferation of manipulated media content\nrender the detection of such manipulations a task of increasing importance.\nEven though there are many DeepFake detection methods, only a few focus on the\nimpact of dataset preprocessing and the aggregation of frame-level to\nvideo-level prediction on model performance. In this paper, we propose a\npre-processing step to improve the training data quality and examine its effect\non the performance of DeepFake detection. We also propose and evaluate the\neffect of video-level prediction aggregation approaches. Experimental results\nshow that the proposed pre-processing approach leads to considerable\nimprovements in the performance of detection models, and the proposed\nprediction aggregation scheme further boosts the detection efficiency in cases\nwhere there are multiple faces in a video.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 11:16:02 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 08:22:44 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 10:22:15 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Charitidis", "Polychronis", ""], ["Kordopatis-Zilos", "Giorgos", ""], ["Papadopoulos", "Symeon", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "2006.07114", "submitter": "Guodong Xu", "authors": "Guodong Xu, Ziwei Liu, Xiaoxiao Li, Chen Change Loy", "title": "Knowledge Distillation Meets Self-Supervision", "comments": "To appear in ECCV 2020. Code is available at:\n  https://github.com/xuguodong03/SSKD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation, which involves extracting the \"dark knowledge\" from a\nteacher network to guide the learning of a student network, has emerged as an\nimportant technique for model compression and transfer learning. Unlike\nprevious works that exploit architecture-specific cues such as activation and\nattention for distillation, here we wish to explore a more general and\nmodel-agnostic approach for extracting \"richer dark knowledge\" from the\npre-trained teacher model. We show that the seemingly different\nself-supervision task can serve as a simple yet powerful solution. For example,\nwhen performing contrastive learning between transformed entities, the noisy\npredictions of the teacher network reflect its intrinsic composition of\nsemantic and pose information. By exploiting the similarity between those\nself-supervision signals as an auxiliary task, one can effectively transfer the\nhidden information from the teacher to the student. In this paper, we discuss\npractical ways to exploit those noisy self-supervision signals with selective\ntransfer for distillation. We further show that self-supervision signals\nimprove conventional distillation with substantial gains under few-shot and\nnoisy-label scenarios. Given the richer knowledge mined from self-supervision,\nour knowledge distillation approach achieves state-of-the-art performance on\nstandard benchmarks, i.e., CIFAR100 and ImageNet, under both\nsimilar-architecture and cross-architecture settings. The advantage is even\nmore pronounced under the cross-architecture setting, where our method\noutperforms the state of the art CRD by an average of 2.3% in accuracy rate on\nCIFAR100 across six different teacher-student pairs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 12:18:52 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 09:14:27 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Xu", "Guodong", ""], ["Liu", "Ziwei", ""], ["Li", "Xiaoxiao", ""], ["Loy", "Chen Change", ""]]}, {"id": "2006.07139", "submitter": "Suncheng Xiang", "authors": "Suncheng Xiang, Yuzhuo Fu, Guanjie You, Ting Liu", "title": "Attribute analysis with synthetic dataset for person re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) plays an important role in applications such\nas public security and video surveillance. Recently, learning from synthetic\ndata, which benefits from the popularity of synthetic data engine, have\nachieved remarkable performance. However, existing synthetic datasets are in\nsmall size and lack of diversity, which hinders the development of person re-ID\nin real-world scenarios. To address this problem, firstly, we develop a\nlarge-scale synthetic data engine, the salient characteristic of this engine is\ncontrollable. Based on it, we build a large-scale synthetic dataset, which are\ndiversified and customized from different attributes, such as illumination and\nviewpoint. Secondly, we quantitatively analyze the influence of dataset\nattributes on re-ID system. To our best knowledge, this is the first attempt to\nexplicitly dissect person re-ID from the aspect of attribute on synthetic\ndataset. Comprehensive experiments help us have a deeper understanding of the\nfundamental problems in person re-ID. Our research also provides useful\ninsights for dataset building and future practical usage.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 12:51:47 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 14:41:39 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Xiang", "Suncheng", ""], ["Fu", "Yuzhuo", ""], ["You", "Guanjie", ""], ["Liu", "Ting", ""]]}, {"id": "2006.07152", "submitter": "Akka Zemmari", "authors": "Miltiadis Poursanidis (LaBRI), Jenny Benois-Pineau (LaBRI), Akka\n  Zemmari (LaBRI), Boris Mansenca (LaBRI), Aymar de Rugy (INCIA)", "title": "Move-to-Data: A new Continual Learning approach with Deep CNNs,\n  Application for image-class recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-life tasks of application of supervised learning approaches, all\nthe training data are not available at the same time. The examples are lifelong\nimage classification or recognition of environmental objects during interaction\nof instrumented persons with their environment, enrichment of an\nonline-database with more images. It is necessary to pre-train the model at a\n\"training recording phase\" and then adjust it to the new coming data. This is\nthe task of incremental/continual learning approaches. Amongst different\nproblems to be solved by these approaches such as introduction of new\ncategories in the model, refining existing categories to sub-categories and\nextending trained classifiers over them, ... we focus on the problem of\nadjusting pre-trained model with new additional training data for existing\ncategories. We propose a fast continual learning layer at the end of the\nneuronal network. Obtained results are illustrated on the opensource CIFAR\nbenchmark dataset. The proposed scheme yields similar performances as\nretraining but with drastically lower computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 13:04:58 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Poursanidis", "Miltiadis", "", "LaBRI"], ["Benois-Pineau", "Jenny", "", "LaBRI"], ["Zemmari", "Akka", "", "LaBRI"], ["Mansenca", "Boris", "", "LaBRI"], ["de Rugy", "Aymar", "", "INCIA"]]}, {"id": "2006.07159", "submitter": "Lucas Beyer", "authors": "Lucas Beyer and Olivier J. H\\'enaff and Alexander Kolesnikov and\n  Xiaohua Zhai and A\\\"aron van den Oord", "title": "Are we done with ImageNet?", "comments": "All five authors contributed equally. New labels at\n  https://github.com/google-research/reassessed-imagenet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yes, and no. We ask whether recent progress on the ImageNet classification\nbenchmark continues to represent meaningful generalization, or whether the\ncommunity has started to overfit to the idiosyncrasies of its labeling\nprocedure. We therefore develop a significantly more robust procedure for\ncollecting human annotations of the ImageNet validation set. Using these new\nlabels, we reassess the accuracy of recently proposed ImageNet classifiers, and\nfind their gains to be substantially smaller than those reported on the\noriginal labels. Furthermore, we find the original ImageNet labels to no longer\nbe the best predictors of this independently-collected set, indicating that\ntheir usefulness in evaluating vision models may be nearing an end.\nNevertheless, we find our annotation procedure to have largely remedied the\nerrors in the original labels, reinforcing ImageNet as a powerful benchmark for\nfuture research in visual recognition.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 13:17:25 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Beyer", "Lucas", ""], ["H\u00e9naff", "Olivier J.", ""], ["Kolesnikov", "Alexander", ""], ["Zhai", "Xiaohua", ""], ["Oord", "A\u00e4ron van den", ""]]}, {"id": "2006.07164", "submitter": "Gurkirt Singh", "authors": "Vivek Singh Bawa, Gurkirt Singh, Francis KapingA, Inna\n  Skarga-Bandurova, Alice Leporini, Carmela Landolfo, Armando Stabile,\n  Francesco Setti, Riccardo Muradore, Elettra Oleari, Fabio Cuzzolin", "title": "ESAD: Endoscopic Surgeon Action Detection Dataset", "comments": "In context of SARAS ESAD Challeneg at MIDL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we take aim towards increasing the effectiveness of surgical\nassistant robots. We intended to make assistant robots safer by making them\naware about the actions of surgeon, so it can take appropriate assisting\nactions. In other words, we aim to solve the problem of surgeon action\ndetection in endoscopic videos. To this, we introduce a challenging dataset for\nsurgeon action detection in real-world endoscopic videos. Action classes are\npicked based on the feedback of surgeons and annotated by medical professional.\nGiven a video frame, we draw bounding box around surgical tool which is\nperforming action and label it with action label. Finally, we presenta\nframe-level action detection baseline model based on recent advances in ob-ject\ndetection. Results on our new dataset show that our presented dataset provides\nenough interesting challenges for future method and it can serveas strong\nbenchmark corresponding research in surgeon action detection in endoscopic\nvideos.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 13:22:41 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Bawa", "Vivek Singh", ""], ["Singh", "Gurkirt", ""], ["KapingA", "Francis", ""], ["Skarga-Bandurova", "Inna", ""], ["Leporini", "Alice", ""], ["Landolfo", "Carmela", ""], ["Stabile", "Armando", ""], ["Setti", "Francesco", ""], ["Muradore", "Riccardo", ""], ["Oleari", "Elettra", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "2006.07187", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Rasoul Sali, Lubaina Ehsan, William Adorno, Asad Ali,\n  Sean Moore, Beatrice Amadi, Paul Kelly, Sana Syed, Donald Brown", "title": "HMIC: Hierarchical Medical Image Classification, A Deep Learning\n  Approach", "comments": null, "journal-ref": "Information 11, no. 6 (2020): 318", "doi": "10.3390/info11060318", "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image classification is central to the big data revolution in medicine.\nImproved information processing methods for diagnosis and classification of\ndigital medical images have shown to be successful via deep learning\napproaches. As this field is explored, there are limitations to the performance\nof traditional supervised classifiers. This paper outlines an approach that is\ndifferent from the current medical image classification tasks that view the\nissue as multi-class classification. We performed a hierarchical classification\nusing our Hierarchical Medical Image classification (HMIC) approach. HMIC uses\nstacks of deep learning models to give particular comprehension at each level\nof the clinical picture hierarchy. For testing our performance, we use biopsy\nof the small bowel images that contain three categories in the parent level\n(Celiac Disease, Environmental Enteropathy, and histologically normal\ncontrols). For the child level, Celiac Disease Severity is classified into 4\nclasses (I, IIIa, IIIb, and IIIC).\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 15:15:29 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 22:59:38 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Kowsari", "Kamran", ""], ["Sali", "Rasoul", ""], ["Ehsan", "Lubaina", ""], ["Adorno", "William", ""], ["Ali", "Asad", ""], ["Moore", "Sean", ""], ["Amadi", "Beatrice", ""], ["Kelly", "Paul", ""], ["Syed", "Sana", ""], ["Brown", "Donald", ""]]}, {"id": "2006.07203", "submitter": "Bruno Korbar", "authors": "Bruno Korbar, Fabio Petroni, Rohit Girdhar, Lorenzo Torresani", "title": "Video Understanding as Machine Translation", "comments": "The authors have temporarily withdrawn this paper to reassess some of\n  the experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of large-scale multimodal video datasets, especially\nsequences with audio or transcribed speech, there has been a growing interest\nin self-supervised learning of video representations. Most prior work\nformulates the objective as a contrastive metric learning problem between the\nmodalities. To enable effective learning, however, these strategies require a\ncareful selection of positive and negative samples often combined with\nhand-designed curriculum policies. In this work we remove the need for negative\nsampling by taking a generative modeling approach that poses the objective as a\ntranslation problem between modalities. Such a formulation allows us to tackle\na wide variety of downstream video understanding tasks by means of a single\nunified framework, without the need for large batches of negative samples\ncommon in contrastive metric learning. We experiment with the large-scale\nHowTo100M dataset for training, and report performance gains over the\nstate-of-the-art on several downstream tasks including video classification\n(EPIC-Kitchens), question answering (TVQA), captioning (TVC, YouCook2, and\nMSR-VTT), and text-based clip retrieval (YouCook2 and MSR-VTT).\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 14:07:04 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 19:41:31 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Korbar", "Bruno", ""], ["Petroni", "Fabio", ""], ["Girdhar", "Rohit", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "2006.07206", "submitter": "Lei Zhang", "authors": "Lei Zhang, Xiaofu Wu, Suofei Zhang and Zirui Yin", "title": "Branch-Cooperative OSNet for Person Re-Identification", "comments": "7 pages, 3 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-branch is extensively studied for learning rich feature representation\nfor person re-identification (Re-ID). In this paper, we propose a\nbranch-cooperative architecture over OSNet, termed BC-OSNet, for person Re-ID.\nBy stacking four cooperative branches, namely, a global branch, a local branch,\na relational branch and a contrastive branch, we obtain powerful feature\nrepresentation for person Re-ID. Extensive experiments show that the proposed\nBC-OSNet achieves state-of-art performance on the three popular datasets,\nincluding Market-1501, DukeMTMC-reID and CUHK03. In particular, it achieves mAP\nof 84.0% and rank-1 accuracy of 87.1% on the CUHK03_labeled.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 14:09:23 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Zhang", "Lei", ""], ["Wu", "Xiaofu", ""], ["Zhang", "Suofei", ""], ["Yin", "Zirui", ""]]}, {"id": "2006.07214", "submitter": "Andre Martins", "authors": "Andr\\'e F. T. Martins, Ant\\'onio Farinhas, Marcos Treviso, Vlad\n  Niculae, Pedro M. Q. Aguiar, M\\'ario A. T. Figueiredo", "title": "Sparse and Continuous Attention Mechanisms", "comments": "Accepted for spotlight presentation at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential families are widely used in machine learning; they include many\ndistributions in continuous and discrete domains (e.g., Gaussian, Dirichlet,\nPoisson, and categorical distributions via the softmax transformation).\nDistributions in each of these families have fixed support. In contrast, for\nfinite domains, there has been recent work on sparse alternatives to softmax\n(e.g. sparsemax and alpha-entmax), which have varying support, being able to\nassign zero probability to irrelevant categories. This paper expands that work\nin two directions: first, we extend alpha-entmax to continuous domains,\nrevealing a link with Tsallis statistics and deformed exponential families.\nSecond, we introduce continuous-domain attention mechanisms, deriving efficient\ngradient backpropagation algorithms for alpha in {1,2}. Experiments on\nattention-based text classification, machine translation, and visual question\nanswering illustrate the use of continuous attention in 1D and 2D, showing that\nit allows attending to time intervals and compact regions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 14:16:48 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 22:22:38 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 08:39:54 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Martins", "Andr\u00e9 F. T.", ""], ["Farinhas", "Ant\u00f3nio", ""], ["Treviso", "Marcos", ""], ["Niculae", "Vlad", ""], ["Aguiar", "Pedro M. Q.", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "2006.07226", "submitter": "Qendrim Bytyqi", "authors": "Qendrim Bytyqi and Nicola Wolpert and Elmar Sch\\\"omer", "title": "Local-Area-Learning Network: Meaningful Local Areas for Efficient Point\n  Cloud Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in point cloud analysis with deep neural networks has made rapid\nprogress in recent years. The pioneering work PointNet offered a direct\nanalysis of point clouds. However, due to its architecture PointNet is not able\nto capture local structures. To overcome this drawback, the same authors have\ndeveloped PointNet++ by applying PointNet to local areas. The local areas are\ndefined by center points and their neighbors. In PointNet++ and its further\ndevelopments the center points are determined with a Farthest Point Sampling\n(FPS) algorithm. This has the disadvantage that the center points in general do\nnot have meaningful local areas. In this paper, we introduce the neural\nLocal-Area-Learning Network (LocAL-Net) which places emphasis on the selection\nand characterization of the local areas. Our approach learns critical points\nthat we use as center points. In order to strengthen the recognition of local\nstructures, the points are given additional metric properties depending on the\nlocal areas. Finally, we derive and combine two global feature vectors, one\nfrom the whole point cloud and one from all local areas. Experiments on the\ndatasets ModelNet10/40 and ShapeNet show that LocAL-Net is competitive for part\nsegmentation. For classification LocAL-Net outperforms the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 14:32:28 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Bytyqi", "Qendrim", ""], ["Wolpert", "Nicola", ""], ["Sch\u00f6mer", "Elmar", ""]]}, {"id": "2006.07228", "submitter": "Tao Sun", "authors": "Mohammad Rasouli, Tao Sun, Ram Rajagopal", "title": "FedGAN: Federated Generative Adversarial Networks for Distributed Data", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Federated Generative Adversarial Network (FedGAN) for training a\nGAN across distributed sources of non-independent-and-identically-distributed\ndata sources subject to communication and privacy constraints. Our algorithm\nuses local generators and discriminators which are periodically synced via an\nintermediary that averages and broadcasts the generator and discriminator\nparameters. We theoretically prove the convergence of FedGAN with both equal\nand two time-scale updates of generator and discriminator, under standard\nassumptions, using stochastic approximations and communication efficient\nstochastic gradient descents. We experiment FedGAN on toy examples (2D system,\nmixed Gaussian, and Swiss role), image datasets (MNIST, CIFAR-10, and CelebA),\nand time series datasets (household electricity consumption and electric\nvehicle charging sessions). We show FedGAN converges and has similar\nperformance to general distributed GAN, while reduces communication complexity.\nWe also show its robustness to reduced communications.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 14:36:43 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 06:38:12 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Rasouli", "Mohammad", ""], ["Sun", "Tao", ""], ["Rajagopal", "Ram", ""]]}, {"id": "2006.07229", "submitter": "Kenneth Vanhoey", "authors": "Eric Heitz and Kenneth Vanhoey and Thomas Chambon and Laurent Belcour", "title": "A Sliced Wasserstein Loss for Neural Texture Synthesis", "comments": "9 pages, 14 figures, accepted at CVPR 2021", "journal-ref": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We address the problem of computing a textural loss based on the statistics\nextracted from the feature activations of a convolutional neural network\noptimized for object recognition (e.g. VGG-19). The underlying mathematical\nproblem is the measure of the distance between two distributions in feature\nspace. The Gram-matrix loss is the ubiquitous approximation for this problem\nbut it is subject to several shortcomings. Our goal is to promote the Sliced\nWasserstein Distance as a replacement for it. It is theoretically\nproven,practical, simple to implement, and achieves results that are visually\nsuperior for texture synthesis by optimization or training generative neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 14:37:11 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 10:44:45 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 09:21:28 GMT"}, {"version": "v4", "created": "Thu, 11 Mar 2021 14:53:09 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Heitz", "Eric", ""], ["Vanhoey", "Kenneth", ""], ["Chambon", "Thomas", ""], ["Belcour", "Laurent", ""]]}, {"id": "2006.07309", "submitter": "Fateme Bafghi", "authors": "Fateme Bafghi, Bijan Shoushtarian", "title": "Multiple-Vehicle Tracking in the Highway Using Appearance Model and\n  Visual Object Tracking", "comments": null, "journal-ref": null, "doi": "10.1109/MVIP49855.2020.9116905", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent decades, due to the groundbreaking improvements in machine vision,\nmany daily tasks are performed by computers. One of these tasks is\nmultiple-vehicle tracking, which is widely used in different areas such as\nvideo surveillance and traffic monitoring. This paper focuses on introducing an\nefficient novel approach with acceptable accuracy. This is achieved through an\nefficient appearance and motion model based on the features extracted from each\nobject. For this purpose, two different approaches have been used to extract\nfeatures, i.e. features extracted from a deep neural network, and traditional\nfeatures. Then the results from these two approaches are compared with\nstate-of-the-art trackers. The results are obtained by executing the methods on\nthe UA-DETRACK benchmark. The first method led to 58.9% accuracy while the\nsecond method caused up to 15.9%. The proposed methods can still be improved by\nextracting more distinguishable features.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 16:46:12 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Bafghi", "Fateme", ""], ["Shoushtarian", "Bijan", ""]]}, {"id": "2006.07326", "submitter": "Sungmin Cha", "authors": "Sungmin Cha, Hsiang Hsu, Taebaek Hwang, Flavio P. Calmon and Taesup\n  Moon", "title": "CPR: Classifier-Projection Regularization for Continual Learning", "comments": "ICLR 2021 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general, yet simple patch that can be applied to existing\nregularization-based continual learning methods called classifier-projection\nregularization (CPR). Inspired by both recent results on neural networks with\nwide local minima and information theory, CPR adds an additional regularization\nterm that maximizes the entropy of a classifier's output probability. We\ndemonstrate that this additional term can be interpreted as a projection of the\nconditional probability given by a classifier's output to the uniform\ndistribution. By applying the Pythagorean theorem for KL divergence, we then\nprove that this projection may (in theory) improve the performance of continual\nlearning methods. In our extensive experimental results, we apply CPR to\nseveral state-of-the-art regularization-based continual learning methods and\nbenchmark performance on popular image recognition datasets. Our results\ndemonstrate that CPR indeed promotes a wide local minima and significantly\nimproves both accuracy and plasticity while simultaneously mitigating the\ncatastrophic forgetting of baseline continual learning methods. The codes and\nscripts for this work are available at https://github.com/csm9493/CPR_CL.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 17:07:37 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 09:30:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cha", "Sungmin", ""], ["Hsu", "Hsiang", ""], ["Hwang", "Taebaek", ""], ["Calmon", "Flavio P.", ""], ["Moon", "Taesup", ""]]}, {"id": "2006.07327", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng, Yongxin Wang, Yunze Man, Kris Kitani", "title": "GNN3DMOT: Graph Neural Network for 3D Multi-Object Tracking with\n  Multi-Feature Learning", "comments": "CVPR 2020. My website for all my research works:\n  http://www.xinshuoweng.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Multi-object tracking (MOT) is crucial to autonomous systems. Recent work\nuses a standard tracking-by-detection pipeline, where feature extraction is\nfirst performed independently for each object in order to compute an affinity\nmatrix. Then the affinity matrix is passed to the Hungarian algorithm for data\nassociation. A key process of this standard pipeline is to learn discriminative\nfeatures for different objects in order to reduce confusion during data\nassociation. In this work, we propose two techniques to improve the\ndiscriminative feature learning for MOT: (1) instead of obtaining features for\neach object independently, we propose a novel feature interaction mechanism by\nintroducing the Graph Neural Network. As a result, the feature of one object is\ninformed of the features of other objects so that the object feature can lean\ntowards the object with similar feature (i.e., object probably with a same ID)\nand deviate from objects with dissimilar features (i.e., object probably with\ndifferent IDs), leading to a more discriminative feature for each object; (2)\ninstead of obtaining the feature from either 2D or 3D space in prior work, we\npropose a novel joint feature extractor to learn appearance and motion features\nfrom 2D and 3D space simultaneously. As features from different modalities\noften have complementary information, the joint feature can be more\ndiscriminate than feature from each individual modality. To ensure that the\njoint feature extractor does not heavily rely on one modality, we also propose\nan ensemble training paradigm. Through extensive evaluation, our proposed\nmethod achieves state-of-the-art performance on KITTI and nuScenes 3D MOT\nbenchmarks. Our code will be made available at\nhttps://github.com/xinshuoweng/GNN3DMOT\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 17:08:14 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Weng", "Xinshuo", ""], ["Wang", "Yongxin", ""], ["Man", "Yunze", ""], ["Kitani", "Kris", ""]]}, {"id": "2006.07345", "submitter": "Kashif Inayat", "authors": "Shahbano, Muhammad Abdullah and Kashif Inayat", "title": "Robust Baggage Detection and Classification Based on Local\n  Tri-directional Pattern", "comments": null, "journal-ref": "International Journal of Internet Technology and Secured\n  Transactions (2021)", "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent decades, the automatic video surveillance system has gained\nsignificant importance in computer vision community. The crucial objective of\nsurveillance is monitoring and security in public places. In the traditional\nLocal Binary Pattern, the feature description is somehow inaccurate, and the\nfeature size is large enough. Therefore, to overcome these shortcomings, our\nresearch proposed a detection algorithm for a human with or without carrying\nbaggage. The Local tri-directional pattern descriptor is exhibited to extract\nfeatures of different human body parts including head, trunk, and limbs. Then\nwith the help of support vector machine, extracted features are trained and\nevaluated. Experimental results on INRIA and MSMT17 V1 datasets show that\nLtriDP outperforms several state-of-the-art feature descriptors and validate\nits effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 17:33:21 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 13:41:09 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 04:14:21 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Shahbano", "", ""], ["Abdullah", "Muhammad", ""], ["Inayat", "Kashif", ""]]}, {"id": "2006.07364", "submitter": "Ye Yuan", "authors": "Ye Yuan, Kris Kitani", "title": "Residual Force Control for Agile Human Behavior Imitation and Extended\n  Motion Synthesis", "comments": "NeurIPS 2020. Code: https://github.com/Khrylx/RFC. Project page:\n  https://www.ye-yuan.com/rfc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning has shown great promise for synthesizing realistic\nhuman behaviors by learning humanoid control policies from motion capture data.\nHowever, it is still very challenging to reproduce sophisticated human skills\nlike ballet dance, or to stably imitate long-term human behaviors with complex\ntransitions. The main difficulty lies in the dynamics mismatch between the\nhumanoid model and real humans. That is, motions of real humans may not be\nphysically possible for the humanoid model. To overcome the dynamics mismatch,\nwe propose a novel approach, residual force control (RFC), that augments a\nhumanoid control policy by adding external residual forces into the action\nspace. During training, the RFC-based policy learns to apply residual forces to\nthe humanoid to compensate for the dynamics mismatch and better imitate the\nreference motion. Experiments on a wide range of dynamic motions demonstrate\nthat our approach outperforms state-of-the-art methods in terms of convergence\nspeed and the quality of learned motions. Notably, we showcase a physics-based\nvirtual character empowered by RFC that can perform highly agile ballet dance\nmoves such as pirouette, arabesque and jet\\'e. Furthermore, we propose a\ndual-policy control framework, where a kinematic policy and an RFC-based policy\nwork in tandem to synthesize multi-modal infinite-horizon human motions without\nany task guidance or user input. Our approach is the first humanoid control\nmethod that successfully learns from a large-scale human motion dataset\n(Human3.6M) and generates diverse long-term motions. Code and videos are\navailable at https://www.ye-yuan.com/rfc.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 17:56:16 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 17:57:12 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Yuan", "Ye", ""], ["Kitani", "Kris", ""]]}, {"id": "2006.07397", "submitter": "Cristian Canton Ferrer", "authors": "Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes,\n  Menglin Wang, Cristian Canton Ferrer", "title": "The DeepFake Detection Challenge (DFDC) Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deepfakes are a recent off-the-shelf manipulation technique that allows\nanyone to swap two identities in a single video. In addition to Deepfakes, a\nvariety of GAN-based face swapping methods have also been published with\naccompanying code. To counter this emerging threat, we have constructed an\nextremely large face swap video dataset to enable the training of detection\nmodels, and organized the accompanying DeepFake Detection Challenge (DFDC)\nKaggle competition. Importantly, all recorded subjects agreed to participate in\nand have their likenesses modified during the construction of the face-swapped\ndataset. The DFDC dataset is by far the largest currently and publicly\navailable face swap video dataset, with over 100,000 total clips sourced from\n3,426 paid actors, produced with several Deepfake, GAN-based, and non-learned\nmethods. In addition to describing the methods used to construct the dataset,\nwe provide a detailed analysis of the top submissions from the Kaggle contest.\nWe show although Deepfake detection is extremely difficult and still an\nunsolved problem, a Deepfake detection model trained only on the DFDC can\ngeneralize to real \"in-the-wild\" Deepfake videos, and such a model can be a\nvaluable analysis tool when analyzing potentially Deepfaked videos. Training,\nvalidation and testing corpuses can be downloaded from\nhttps://ai.facebook.com/datasets/dfdc.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 18:15:55 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 04:28:03 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 01:22:11 GMT"}, {"version": "v4", "created": "Wed, 28 Oct 2020 03:48:28 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Dolhansky", "Brian", ""], ["Bitton", "Joanna", ""], ["Pflaum", "Ben", ""], ["Lu", "Jikuo", ""], ["Howes", "Russ", ""], ["Wang", "Menglin", ""], ["Ferrer", "Cristian Canton", ""]]}, {"id": "2006.07412", "submitter": "Eli Shlizerman", "authors": "Yang Zheng, Jinlin Xiang, Kun Su, Eli Shlizerman", "title": "BI-MAML: Balanced Incremental Approach for Meta Learning", "comments": "Please see associated video at: https://youtu.be/4qlb-iG5SFo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Balanced Incremental Model Agnostic Meta Learning system\n(BI-MAML) for learning multiple tasks. Our method implements a meta-update rule\nto incrementally adapt its model to new tasks without forgetting old tasks.\nSuch a capability is not possible in current state-of-the-art MAML approaches.\nThese methods effectively adapt to new tasks, however, suffer from\n'catastrophic forgetting' phenomena, in which new tasks that are streamed into\nthe model degrade the performance of the model on previously learned tasks. Our\nsystem performs the meta-updates with only a few-shots and can successfully\naccomplish them. Our key idea for achieving this is the design of balanced\nlearning strategy for the baseline model. The strategy sets the baseline model\nto perform equally well on various tasks and incorporates time efficiency. The\nbalanced learning strategy enables BI-MAML to both outperform other\nstate-of-the-art models in terms of classification accuracy for existing tasks\nand also accomplish efficient adaption to similar new tasks with less required\nshots. We evaluate BI-MAML by conducting comparisons on two common benchmark\ndatasets with multiple number of image classification tasks. BI-MAML\nperformance demonstrates advantages in both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 18:28:48 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zheng", "Yang", ""], ["Xiang", "Jinlin", ""], ["Su", "Kun", ""], ["Shlizerman", "Eli", ""]]}, {"id": "2006.07421", "submitter": "Chaofei Yang", "authors": "Chaofei Yang, Lei Ding, Yiran Chen, Hai Li", "title": "Defending against GAN-based Deepfake Attacks via Transformation-aware\n  Adversarial Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deepfake represents a category of face-swapping attacks that leverage machine\nlearning models such as autoencoders or generative adversarial networks.\nAlthough the concept of the face-swapping is not new, its recent technical\nadvances make fake content (e.g., images, videos) more realistic and\nimperceptible to Humans. Various detection techniques for Deepfake attacks have\nbeen explored. These methods, however, are passive measures against Deepfakes\nas they are mitigation strategies after the high-quality fake content is\ngenerated. More importantly, we would like to think ahead of the attackers with\nrobust defenses. This work aims to take an offensive measure to impede the\ngeneration of high-quality fake images or videos. Specifically, we propose to\nuse novel transformation-aware adversarially perturbed faces as a defense\nagainst GAN-based Deepfake attacks. Different from the naive adversarial faces,\nour proposed approach leverages differentiable random image transformations\nduring the generation. We also propose to use an ensemble-based approach to\nenhance the defense robustness against GAN-based Deepfake variants under the\nblack-box setting. We show that training a Deepfake model with adversarial\nfaces can lead to a significant degradation in the quality of synthesized\nfaces. This degradation is twofold. On the one hand, the quality of the\nsynthesized faces is reduced with more visual artifacts such that the\nsynthesized faces are more obviously fake or less convincing to human\nobservers. On the other hand, the synthesized faces can easily be detected\nbased on various metrics.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 18:51:57 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Yang", "Chaofei", ""], ["Ding", "Lei", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "2006.07469", "submitter": "Luca Biferale", "authors": "L. Biferale and F. Bonaccorso and M. Buzzicotti and P. Clark di Leoni", "title": "TURB-Rot. A large database of 3d and 2d snapshots from turbulent\n  rotating flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.CV physics.data-an physics.geo-ph stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present TURB-Rot, a new open database of 3d and 2d snapshots of turbulent\nvelocity fields, obtained by Direct Numerical Simulations (DNS) of the original\nNavier-Stokes equations in the presence of rotation. The aim is to provide the\ncommunity interested in data-assimilation and/or computer vision with a new\ntesting-ground made of roughly 300K complex images and fields. TURB-Rot data\nare characterized by multi-scales strongly non-Gaussian features and rough,\nnon-differentiable, fields over almost two decades of scales. In addition,\ncoming from fully resolved numerical simulations of the original partial\ndifferential equations, they offer the possibility to apply a wide range of\napproaches, from equation-free to physics-based models. TURB-Rot data are\nreachable at http://smart-turb.roma2.infn.it\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 15:52:43 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Biferale", "L.", ""], ["Bonaccorso", "F.", ""], ["Buzzicotti", "M.", ""], ["di Leoni", "P. Clark", ""]]}, {"id": "2006.07472", "submitter": "Anjana Wijekoon", "authors": "Anjana Wijekoon, Nirmalie Wiratunga", "title": "Learning-to-Learn Personalised Human Activity Recognition Models", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human Activity Recognition~(HAR) is the classification of human movement,\ncaptured using one or more sensors either as wearables or embedded in the\nenvironment~(e.g. depth cameras, pressure mats). State-of-the-art methods of\nHAR rely on having access to a considerable amount of labelled data to train\ndeep architectures with many train-able parameters. This becomes prohibitive\nwhen tasked with creating models that are sensitive to personal nuances in\nhuman movement, explicitly present when performing exercises. In addition, it\nis not possible to collect training data to cover all possible subjects in the\ntarget population. Accordingly, learning personalised models with few data\nremains an interesting challenge for HAR research. We present a meta-learning\nmethodology for learning to learn personalised HAR models for HAR; with the\nexpectation that the end-user need only provides a few labelled data but can\nbenefit from the rapid adaptation of a generic meta-model. We introduce two\nalgorithms, Personalised MAML and Personalised Relation Networks inspired by\nexisting Meta-Learning algorithms but optimised for learning HAR models that\nare adaptable to any person in health and well-being applications. A\ncomparative study shows significant performance improvements against the\nstate-of-the-art Deep Learning algorithms and the Few-shot Meta-Learning\nalgorithms in multiple HAR domains.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 21:11:59 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Wijekoon", "Anjana", ""], ["Wiratunga", "Nirmalie", ""]]}, {"id": "2006.07475", "submitter": "Niloy Sikder", "authors": "Niloy Sikder, Md. Sanaullah Chowdhury, Abu Shamim Mohammad Arif, and\n  Abdullah-Al Nahid", "title": "Early Blindness Detection Based on Retinal Images Using Ensemble\n  Learning", "comments": "6 pages, 22nd International Conference of Computer and Information\n  Technology (ICCIT), 18-20 December, 2019", "journal-ref": null, "doi": "10.1109/ICCIT48885.2019.9038439", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Diabetic retinopathy (DR) is the primary cause of vision loss among grownup\npeople around the world. In four out of five cases having diabetes for a\nprolonged period leads to DR. If detected early, more than 90 percent of the\nnew DR occurrences can be prevented from turning into blindness through proper\ntreatment. Despite having multiple treatment procedures available that are well\ncapable to deal with DR, the negligence and failure of early detection cost\nmost of the DR patients their precious eyesight. The recent developments in the\nfield of Digital Image Processing (DIP) and Machine Learning (ML) have paved\nthe way to use machines in this regard. The contemporary technologies allow us\nto develop devices capable of automatically detecting the condition of a\npersons eyes based on their retinal images. However, in practice, several\nfactors hinder the quality of the captured images and impede the detection\noutcome. In this study, a novel early blind detection method has been proposed\nbased on the color information extracted from retinal images using an ensemble\nlearning algorithm. The method has been tested on a set of retinal images\ncollected from people living in the rural areas of South Asia, which resulted\nin a 91 percent classification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 21:16:21 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Sikder", "Niloy", ""], ["Chowdhury", "Md. Sanaullah", ""], ["Arif", "Abu Shamim Mohammad", ""], ["Nahid", "Abdullah-Al", ""]]}, {"id": "2006.07489", "submitter": "Leonidas Spinoulas", "authors": "Leonidas Spinoulas, Mohamed Hussein, David Geissb\\\"uhler, Joe Mathai,\n  Oswin G.Almeida, Guillaume Clivaz, S\\'ebastien Marcel, and Wael AbdAlmageed", "title": "Multispectral Biometrics System Framework: Application to Presentation\n  Attack Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a general framework for building a biometrics system\ncapable of capturing multispectral data from a series of sensors synchronized\nwith active illumination sources. The framework unifies the system design for\ndifferent biometric modalities and its realization on face, finger and iris\ndata is described in detail. To the best of our knowledge, the presented design\nis the first to employ such a diverse set of electromagnetic spectrum bands,\nranging from visible to long-wave-infrared wavelengths, and is capable of\nacquiring large volumes of data in seconds. Having performed a series of data\ncollections, we run a comprehensive analysis on the captured data using a\ndeep-learning classifier for presentation attack detection. Our study follows a\ndata-centric approach attempting to highlight the strengths and weaknesses of\neach spectral band at distinguishing live from fake samples.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 22:09:35 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Spinoulas", "Leonidas", ""], ["Hussein", "Mohamed", ""], ["Geissb\u00fchler", "David", ""], ["Mathai", "Joe", ""], ["Almeida", "Oswin G.", ""], ["Clivaz", "Guillaume", ""], ["Marcel", "S\u00e9bastien", ""], ["AbdAlmageed", "Wael", ""]]}, {"id": "2006.07491", "submitter": "Mohamed Yousef", "authors": "Mohamed Yousef, Tom E. Bishop", "title": "OrigamiNet: Weakly-Supervised, Segmentation-Free, One-Step, Full Page\n  Text Recognition by learning to unfold", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text recognition is a major computer vision task with a big set of associated\nchallenges. One of those traditional challenges is the coupled nature of text\nrecognition and segmentation. This problem has been progressively solved over\nthe past decades, going from segmentation based recognition to segmentation\nfree approaches, which proved more accurate and much cheaper to annotate data\nfor. We take a step from segmentation-free single line recognition towards\nsegmentation-free multi-line / full page recognition. We propose a novel and\nsimple neural network module, termed \\textbf{OrigamiNet}, that can augment any\nCTC-trained, fully convolutional single line text recognizer, to convert it\ninto a multi-line version by providing the model with enough spatial capacity\nto be able to properly collapse a 2D input signal into 1D without losing\ninformation. Such modified networks can be trained using exactly their same\nsimple original procedure, and using only \\textbf{unsegmented} image and text\npairs. We carry out a set of interpretability experiments that show that our\ntrained models learn an accurate implicit line segmentation. We achieve\nstate-of-the-art character error rate on both IAM \\& ICDAR 2017 HTR benchmarks\nfor handwriting recognition, surpassing all other methods in the literature. On\nIAM we even surpass single line methods that use accurate localization\ninformation during training. Our code is available online at\n\\url{https://github.com/IntuitionMachines/OrigamiNet}.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 22:18:02 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Yousef", "Mohamed", ""], ["Bishop", "Tom E.", ""]]}, {"id": "2006.07498", "submitter": "Leonidas Spinoulas", "authors": "Leonidas Spinoulas, Hengameh Mirzaalian, Mohamed Hussein, and Wael\n  AbdAlmageed", "title": "Multi-Modal Fingerprint Presentation Attack Detection: Evaluation On A\n  New Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint presentation attack detection is becoming an increasingly\nchallenging problem due to the continuous advancement of attack preparation\ntechniques, which generate realistic-looking fake fingerprint presentations. In\nthis work, rather than relying on legacy fingerprint images, which are widely\nused in the community, we study the usefulness of multiple recently introduced\nsensing modalities. Our study covers front-illumination imaging using\nshort-wave-infrared, near-infrared, and laser illumination; and\nback-illumination imaging using near-infrared light. Toward studying the\neffectiveness of each of these unconventional sensing modalities and their\nfusion for liveness detection, we conducted a comprehensive analysis using a\nfully convolutional deep neural network framework. Our evaluation compares\ndifferent combination of the new sensing modalities to legacy data from one of\nour collections as well as the public LivDet2015 dataset, showing the\nsuperiority of the new sensing modalities in most cases. It also covers the\ncases of known and unknown attacks and the cases of intra-dataset and\ninter-dataset evaluations. Our results indicate that the power of our approach\nstems from the nature of the captured data rather than the employed\nclassification framework, which justifies the extra cost for hardware-based (or\nhybrid) solutions. We plan to publicly release one of our dataset collections.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 22:38:23 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 08:09:44 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Spinoulas", "Leonidas", ""], ["Mirzaalian", "Hengameh", ""], ["Hussein", "Mohamed", ""], ["AbdAlmageed", "Wael", ""]]}, {"id": "2006.07502", "submitter": "Siddhesh Khandelwal", "authors": "Siddhesh Khandelwal, Raghav Goyal, Leonid Sigal", "title": "UniT: Unified Knowledge Transfer for Any-shot Object Detection and\n  Segmentation", "comments": "22 Pages, 8 Figures, 13 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for object detection and segmentation rely on large scale\ninstance-level annotations for training, which are difficult and time-consuming\nto collect. Efforts to alleviate this look at varying degrees and quality of\nsupervision. Weakly-supervised approaches draw on image-level labels to build\ndetectors/segmentors, while zero/few-shot methods assume abundant\ninstance-level data for a set of base classes, and none to a few examples for\nnovel classes. This taxonomy has largely siloed algorithmic designs. In this\nwork, we aim to bridge this divide by proposing an intuitive and unified\nsemi-supervised model that is applicable to a range of supervision: from zero\nto a few instance-level samples per novel class. For base classes, our model\nlearns a mapping from weakly-supervised to fully-supervised\ndetectors/segmentors. By learning and leveraging visual and lingual\nsimilarities between the novel and base classes, we transfer those mappings to\nobtain detectors/segmentors for novel classes; refining them with a few novel\nclass instance-level annotated samples, if available. The overall model is\nend-to-end trainable and highly flexible. Through extensive experiments on\nMS-COCO and Pascal VOC benchmark datasets we show improved performance in a\nvariety of settings.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 22:45:47 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 17:38:45 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 20:34:48 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Khandelwal", "Siddhesh", ""], ["Goyal", "Raghav", ""], ["Sigal", "Leonid", ""]]}, {"id": "2006.07520", "submitter": "Qing Zhiwu", "authors": "Zhiwu Qing, Xiang Wang, Yongpeng Sang, Changxin Gao, Shiwei Zhang,\n  Nong Sang", "title": "Temporal Fusion Network for Temporal Action Localization:Submission to\n  ActivityNet Challenge 2020 (Task E)", "comments": "To appear on CVPR 2020 HACS Workshop (Rank 1st)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report analyzes a temporal action localization method we used\nin the HACS competition which is hosted in Activitynet Challenge 2020.The goal\nof our task is to locate the start time and end time of the action in the\nuntrimmed video, and predict action category.Firstly, we utilize the\nvideo-level feature information to train multiple video-level action\nclassification models. In this way, we can get the category of action in the\nvideo.Secondly, we focus on generating high quality temporal proposals.For this\npurpose, we apply BMN to generate a large number of proposals to obtain high\nrecall rates. We then refine these proposals by employing a cascade structure\nnetwork called Refine Network, which can predict position offset and new IOU\nunder the supervision of ground truth.To make the proposals more accurate, we\nuse bidirectional LSTM, Nonlocal and Transformer to capture temporal\nrelationships between local features of each proposal and global features of\nthe video data.Finally, by fusing the results of multiple models, our method\nobtains 40.55% on the validation set and 40.53% on the test set in terms of\nmAP, and achieves Rank 1 in this challenge.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 00:33:00 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Qing", "Zhiwu", ""], ["Wang", "Xiang", ""], ["Sang", "Yongpeng", ""], ["Gao", "Changxin", ""], ["Zhang", "Shiwei", ""], ["Sang", "Nong", ""]]}, {"id": "2006.07525", "submitter": "Riddhish Bhalodia", "authors": "Riddhish Bhalodia and Ladislav Kavan and Ross Whitaker", "title": "Self-Supervised Discovery of Anatomical Shape Landmarks", "comments": "Early accept at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical shape analysis is a very useful tool in a wide range of medical\nand biological applications. However, it typically relies on the ability to\nproduce a relatively small number of features that can capture the relevant\nvariability in a population. State-of-the-art methods for obtaining such\nanatomical features rely on either extensive preprocessing or segmentation\nand/or significant tuning and post-processing. These shortcomings limit the\nwidespread use of shape statistics. We propose that effective shape\nrepresentations should provide sufficient information to align/register images.\nUsing this assumption we propose a self-supervised, neural network approach for\nautomatically positioning and detecting landmarks in images that can be used\nfor subsequent analysis. The network discovers the landmarks corresponding to\nanatomical shape features that promote good image registration in the context\nof a particular class of transformations. In addition, we also propose a\nregularization for the proposed network which allows for a uniform distribution\nof these discovered landmarks. In this paper, we present a complete framework,\nwhich only takes a set of input images and produces landmarks that are\nimmediately usable for statistical shape analysis. We evaluate the performance\non a phantom dataset as well as 2D and 3D images.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 00:56:33 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Bhalodia", "Riddhish", ""], ["Kavan", "Ladislav", ""], ["Whitaker", "Ross", ""]]}, {"id": "2006.07526", "submitter": "Xiang Wang", "authors": "Xiang Wang, Baiteng Ma, Zhiwu Qing, Yongpeng Sang, Changxin Gao,\n  Shiwei Zhang, Nong Sang", "title": "CBR-Net: Cascade Boundary Refinement Network for Action Detection:\n  Submission to ActivityNet Challenge 2020 (Task 1)", "comments": "ActivityNet Challenge 2020 Temporal Action Localization (Task 1)\n  Champion Solution (Rank 1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we present our solution for the task of temporal action\nlocalization (detection) (task 1) in ActivityNet Challenge 2020. The purpose of\nthis task is to temporally localize intervals where actions of interest occur\nand predict the action categories in a long untrimmed video. Our solution\nmainly includes three components: 1) feature encoding: we apply three kinds of\nbackbones, including TSN [7], Slowfast[3] and I3d[1], which are both pretrained\non Kinetics dataset[2]. Applying these models, we can extract snippet-level\nvideo representations; 2) proposal generation: we choose BMN [5] as our\nbaseline, base on which we design a Cascade Boundary Refinement Network\n(CBR-Net) to conduct proposal detection. The CBR-Net mainly contains two\nmodules: temporal feature encoding, which applies BiLSTM to encode long-term\ntemporal information; CBR module, which targets to refine the proposal\nprecision under different parameter settings; 3) action localization: In this\nstage, we combine the video-level classification results obtained by the fine\ntuning networks to predict the category of each proposal. Moreover, we also\napply to different ensemble strategies to improve the performance of the\ndesigned solution, by which we achieve 42.788% on the testing set of\nActivityNet v1.3 dataset in terms of mean Average Precision metrics.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 01:05:51 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 04:22:52 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Wang", "Xiang", ""], ["Ma", "Baiteng", ""], ["Qing", "Zhiwu", ""], ["Sang", "Yongpeng", ""], ["Gao", "Changxin", ""], ["Zhang", "Shiwei", ""], ["Sang", "Nong", ""]]}, {"id": "2006.07529", "submitter": "Yuzhe Yang", "authors": "Yuzhe Yang, Zhi Xu", "title": "Rethinking the Value of Labels for Improving Class-Imbalanced Learning", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world data often exhibits long-tailed distributions with heavy class\nimbalance, posing great challenges for deep recognition models. We identify a\npersisting dilemma on the value of labels in the context of imbalanced\nlearning: on the one hand, supervision from labels typically leads to better\nresults than its unsupervised counterparts; on the other hand, heavily\nimbalanced data naturally incurs \"label bias\" in the classifier, where the\ndecision boundary can be drastically altered by the majority classes. In this\nwork, we systematically investigate these two facets of labels. We demonstrate,\ntheoretically and empirically, that class-imbalanced learning can significantly\nbenefit in both semi-supervised and self-supervised manners. Specifically, we\nconfirm that (1) positively, imbalanced labels are valuable: given more\nunlabeled data, the original labels can be leveraged with the extra data to\nreduce label bias in a semi-supervised manner, which greatly improves the final\nclassifier; (2) negatively however, we argue that imbalanced labels are not\nuseful always: classifiers that are first pre-trained in a self-supervised\nmanner consistently outperform their corresponding baselines. Extensive\nexperiments on large-scale imbalanced datasets verify our theoretically\ngrounded strategies, showing superior performance over previous\nstate-of-the-arts. Our intriguing findings highlight the need to rethink the\nusage of imbalanced labels in realistic long-tailed tasks. Code is available at\nhttps://github.com/YyzHarry/imbalanced-semi-self.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 01:35:58 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2020 20:05:13 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Yang", "Yuzhe", ""], ["Xu", "Zhi", ""]]}, {"id": "2006.07533", "submitter": "Yihao Huang", "authors": "Yihao Huang, Felix Juefei-Xu, Run Wang, Qing Guo, Lei Ma, Xiaofei Xie,\n  Jianwen Li, Weikai Miao, Yang Liu, Geguang Pu", "title": "FakePolisher: Making DeepFakes More Detection-Evasive by Shallow\n  Reconstruction", "comments": "9 pages, accepted by ACM MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At this moment, GAN-based image generation methods are still imperfect, whose\nupsampling design has limitations in leaving some certain artifact patterns in\nthe synthesized image. Such artifact patterns can be easily exploited (by\nrecent methods) for difference detection of real and GAN-synthesized images.\nHowever, the existing detection methods put much emphasis on the artifact\npatterns, which can become futile if such artifact patterns were reduced.\nTowards reducing the artifacts in the synthesized images, in this paper, we\ndevise a simple yet powerful approach termed FakePolisher that performs shallow\nreconstruction of fake images through a learned linear dictionary, intending to\neffectively and efficiently reduce the artifacts introduced during image\nsynthesis. The comprehensive evaluation on 3 state-of-the-art DeepFake\ndetection methods and fake images generated by 16 popular GAN-based fake image\ngeneration techniques, demonstrates the effectiveness of our technique.Overall,\nthrough reducing artifact patterns, our technique significantly reduces the\naccuracy of the 3 state-of-the-art fake image detection methods, i.e., 47% on\naverage and up to 93% in the worst case.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 01:48:15 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 12:10:08 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 07:27:28 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Huang", "Yihao", ""], ["Juefei-Xu", "Felix", ""], ["Wang", "Run", ""], ["Guo", "Qing", ""], ["Ma", "Lei", ""], ["Xie", "Xiaofei", ""], ["Li", "Jianwen", ""], ["Miao", "Weikai", ""], ["Liu", "Yang", ""], ["Pu", "Geguang", ""]]}, {"id": "2006.07543", "submitter": "Miaoyun Zhao", "authors": "Yulai Cong, Miaoyun Zhao, Jianqiao Li, Sijia Wang, Lawrence Carin", "title": "GAN Memory with No Forgetting", "comments": "NeurIPS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a fundamental issue in lifelong learning, catastrophic forgetting is\ndirectly caused by inaccessible historical data; accordingly, if the data\n(information) were memorized perfectly, no forgetting should be expected.\nMotivated by that, we propose a GAN memory for lifelong learning, which is\ncapable of remembering a stream of datasets via generative processes, with\n\\emph{no} forgetting. Our GAN memory is based on recognizing that one can\nmodulate the \"style\" of a GAN model to form perceptually-distant targeted\ngeneration. Accordingly, we propose to do sequential style modulations atop a\nwell-behaved base GAN model, to form sequential targeted generative models,\nwhile simultaneously benefiting from the transferred base knowledge. The GAN\nmemory -- that is motivated by lifelong learning -- is therefore itself\nmanifested by a form of lifelong learning, via forward transfer and modulation\nof information from prior tasks. Experiments demonstrate the superiority of our\nmethod over existing approaches and its effectiveness in alleviating\ncatastrophic forgetting for lifelong classification problems. Code is available\nat https://github.com/MiaoyunZhao/GANmemory_LifelongLearning.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 03:19:54 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 05:31:26 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Cong", "Yulai", ""], ["Zhao", "Miaoyun", ""], ["Li", "Jianqiao", ""], ["Wang", "Sijia", ""], ["Carin", "Lawrence", ""]]}, {"id": "2006.07553", "submitter": "Nicolas Gillis", "authors": "Nicolas Nadisic, Arnaud Vandaele, Jeremy E. Cohen, Nicolas Gillis", "title": "Sparse Separable Nonnegative Matrix Factorization", "comments": "20 pages, accepted in ECML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new variant of nonnegative matrix factorization (NMF), combining\nseparability and sparsity assumptions. Separability requires that the columns\nof the first NMF factor are equal to columns of the input matrix, while\nsparsity requires that the columns of the second NMF factor are sparse. We call\nthis variant sparse separable NMF (SSNMF), which we prove to be NP-complete, as\nopposed to separable NMF which can be solved in polynomial time. The main\nmotivation to consider this new model is to handle underdetermined blind source\nseparation problems, such as multispectral image unmixing. We introduce an\nalgorithm to solve SSNMF, based on the successive nonnegative projection\nalgorithm (SNPA, an effective algorithm for separable NMF), and an exact sparse\nnonnegative least squares solver. We prove that, in noiseless settings and\nunder mild assumptions, our algorithm recovers the true underlying sources.\nThis is illustrated by experiments on synthetic data sets and the unmixing of a\nmultispectral image.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 03:52:29 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Nadisic", "Nicolas", ""], ["Vandaele", "Arnaud", ""], ["Cohen", "Jeremy E.", ""], ["Gillis", "Nicolas", ""]]}, {"id": "2006.07560", "submitter": "Shengyun Peng", "authors": "Shengyun Peng and Yunxuan Yu and Kun Wang and Lei He", "title": "Accurate Anchor Free Tracking", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual object tracking is an important application of computer vision.\nRecently, Siamese based trackers have achieved good accuracy. However, most of\nSiamese based trackers are not efficient, as they exhaustively search potential\nobject locations to define anchors and then classify each anchor (i.e., a\nbounding box). This paper develops the first Anchor Free Siamese Network\n(AFSN). Specifically, a target object is defined by a bounding box center,\ntracking offset, and object size. All three are regressed by Siamese network\nwith no additional classification or regional proposal, and performed once for\neach frame. We also tune the stride and receptive field for Siamese network,\nand further perform ablation experiments to quantitatively illustrate the\neffectiveness of our AFSN. We evaluate AFSN using five most commonly used\nbenchmarks and compare to the best anchor-based trackers with source codes\navailable for each benchmark. AFSN is 3-425 times faster than these best anchor\nbased trackers. AFSN is also 5.97% to 12.4% more accurate in terms of all\nmetrics for benchmark sets OTB2015, VOT2015, VOT2016, VOT2018 and TrackingNet,\nexcept that SiamRPN++ is 4% better than AFSN in terms of Expected Average\nOverlap (EAO) on VOT2018 (but SiamRPN++ is 3.9 times slower).\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 04:42:32 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Peng", "Shengyun", ""], ["Yu", "Yunxuan", ""], ["Wang", "Kun", ""], ["He", "Lei", ""]]}, {"id": "2006.07567", "submitter": "Ligong Han", "authors": "Ligong Han, Anastasis Stathopoulos, Tao Xue, Dimitris Metaxas", "title": "Unbiased Auxiliary Classifier GANs with MINE", "comments": "Accepted at CVPRW-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Auxiliary Classifier GANs (AC-GANs) are widely used conditional generative\nmodels and are capable of generating high-quality images. Previous work has\npointed out that AC-GAN learns a biased distribution. To remedy this, Twin\nAuxiliary Classifier GAN (TAC-GAN) introduces a twin classifier to the min-max\ngame. However, it has been reported that using a twin auxiliary classifier may\ncause instability in training. To this end, we propose an Unbiased Auxiliary\nGANs (UAC-GAN) that utilizes the Mutual Information Neural Estimator (MINE) to\nestimate the mutual information between the generated data distribution and\nlabels. To further improve the performance, we also propose a novel\nprojection-based statistics network architecture for MINE. Experimental results\non three datasets, including Mixture of Gaussian (MoG), MNIST and CIFAR10\ndatasets, show that our UAC-GAN performs better than AC-GAN and TAC-GAN. Code\ncan be found on the project website.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 05:51:51 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Han", "Ligong", ""], ["Stathopoulos", "Anastasis", ""], ["Xue", "Tao", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "2006.07576", "submitter": "Sixue Gong Miss", "authors": "Sixue Gong, Xiaoming Liu, and Anil K. Jain", "title": "Mitigating Face Recognition Bias via Group Adaptive Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition is known to exhibit bias - subjects in a certain demographic\ngroup can be better recognized than other groups. This work aims to learn a\nfair face representation, where faces of every group could be more equally\nrepresented. Our proposed group adaptive classifier mitigates bias by using\nadaptive convolution kernels and attention mechanisms on faces based on their\ndemographic attributes. The adaptive module comprises kernel masks and\nchannel-wise attention maps for each demographic group so as to activate\ndifferent facial regions for identification, leading to more discriminative\nfeatures pertinent to their demographics. Our introduced automated adaptation\nstrategy determines whether to apply adaptation to a certain layer by\niteratively computing the dissimilarity among demographic-adaptive parameters.\nA new de-biasing loss function is proposed to mitigate the gap of average\nintra-class distance between demographic groups. Experiments on face benchmarks\n(RFW, LFW, IJB-A, and IJB-C) show that our work is able to mitigate face\nrecognition bias across demographic groups while maintaining the competitive\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 06:43:37 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 04:18:39 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Gong", "Sixue", ""], ["Liu", "Xiaoming", ""], ["Jain", "Anil K.", ""]]}, {"id": "2006.07585", "submitter": "Tao He", "authors": "Tao He, Lianli Gao, Jingkuan Song, Jianfei Cai, Yuan-Fang Li", "title": "Learning from the Scene and Borrowing from the Rich: Tackling the Long\n  Tail in Scene Graph Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the huge progress in scene graph generation in recent years, its\nlong-tail distribution in object relationships remains a challenging and\npestering issue. Existing methods largely rely on either external knowledge or\nstatistical bias information to alleviate this problem. In this paper, we\ntackle this issue from another two aspects: (1) scene-object interaction aiming\nat learning specific knowledge from a scene via an additive attention\nmechanism; and (2) long-tail knowledge transfer which tries to transfer the\nrich knowledge learned from the head into the tail. Extensive experiments on\nthe benchmark dataset Visual Genome on three tasks demonstrate that our method\noutperforms current state-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 07:43:40 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["He", "Tao", ""], ["Gao", "Lianli", ""], ["Song", "Jingkuan", ""], ["Cai", "Jianfei", ""], ["Li", "Yuan-Fang", ""]]}, {"id": "2006.07587", "submitter": "Man M. Ho", "authors": "Man M. Ho, Lu Zhang, Alexander Raake, Jinjia Zhou", "title": "Semantic-driven Colorization", "comments": "This work is available at\n  https://minhmanho.github.io/semantic-driven_colorization/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep colorization works predict the semantic information implicitly\nwhile learning to colorize black-and-white photographic images. As a\nconsequence, the generated color is easier to be overflowed, and the semantic\nfaults are invisible. As human experience in coloring, the human first\nrecognize which objects and their location in the photo, imagine which color is\nplausible for the objects as in real life, then colorize it. In this study, we\nsimulate that human-like action to firstly let our network learn to segment\nwhat is in the photo, then colorize it. Therefore, our network can choose a\nplausible color under semantic constraint for specific objects, and give\ndiscriminative colors between them. Moreover, the segmentation map becomes\nunderstandable and interactable for the user. Our models are trained on\nPASCAL-Context and evaluated on selected images from the public domain and\nCOCO-Stuff, which has several unseen categories compared to training data. As\nseen from the experimental results, our colorization system can provide\nplausible colors for specific objects and generate harmonious colors\ncompetitive with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 08:13:30 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 11:32:24 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Ho", "Man M.", ""], ["Zhang", "Lu", ""], ["Raake", "Alexander", ""], ["Zhou", "Jinjia", ""]]}, {"id": "2006.07589", "submitter": "Minseon Kim", "authors": "Minseon Kim, Jihoon Tack, Sung Ju Hwang", "title": "Adversarial Self-Supervised Contrastive Learning", "comments": "NeurIPS 2020. Code: https://github.com/Kim-Minseon/RoCL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing adversarial learning approaches mostly use class labels to generate\nadversarial samples that lead to incorrect predictions, which are then used to\naugment the training of the model for improved robustness. While some recent\nworks propose semi-supervised adversarial learning methods that utilize\nunlabeled data, they still require class labels. However, do we really need\nclass labels at all, for adversarially robust training of deep neural networks?\nIn this paper, we propose a novel adversarial attack for unlabeled data, which\nmakes the model confuse the instance-level identities of the perturbed data\nsamples. Further, we present a self-supervised contrastive learning framework\nto adversarially train a robust neural network without labeled data, which aims\nto maximize the similarity between a random augmentation of a data sample and\nits instance-wise adversarial perturbation. We validate our method, Robust\nContrastive Learning (RoCL), on multiple benchmark datasets, on which it\nobtains comparable robust accuracy over state-of-the-art supervised adversarial\nlearning methods, and significantly improved robustness against the black box\nand unseen types of attacks. Moreover, with further joint fine-tuning with\nsupervised adversarial loss, RoCL obtains even higher robust accuracy over\nusing self-supervised learning alone. Notably, RoCL also demonstrate impressive\nresults in robust transfer learning.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 08:24:33 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 14:04:48 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kim", "Minseon", ""], ["Tack", "Jihoon", ""], ["Hwang", "Sung Ju", ""]]}, {"id": "2006.07597", "submitter": "Zhiyuan Chen", "authors": "Zhiyuan Chen, Annan Li, Shilu Jiang, Yunhong Wang", "title": "Attribute-aware Identity-hard Triplet Loss for Video-based Person\n  Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification (Re-ID) is an important computer vision\ntask. The batch-hard triplet loss frequently used in video-based person Re-ID\nsuffers from the Distance Variance among Different Positives (DVDP) problem. In\nthis paper, we address this issue by introducing a new metric learning method\ncalled Attribute-aware Identity-hard Triplet Loss (AITL), which reduces the\nintra-class variation among positive samples via calculating attribute\ndistance. To achieve a complete model of video-based person Re-ID, a multi-task\nframework with Attribute-driven Spatio-Temporal Attention (ASTA) mechanism is\nalso proposed. Extensive experiments on MARS and DukeMTMC-VID datasets shows\nthat both the AITL and ASTA are very effective. Enhanced by them, even a simple\nlight-weighted video-based person Re-ID baseline can outperform existing\nstate-of-the-art approaches. The codes has been published on\nhttps://github.com/yuange250/Video-based-person-ReID-with-Attribute-information.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 09:15:38 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Chen", "Zhiyuan", ""], ["Li", "Annan", ""], ["Jiang", "Shilu", ""], ["Wang", "Yunhong", ""]]}, {"id": "2006.07601", "submitter": "Mariia Dobko", "authors": "Mariia Dobko, Ostap Viniavskyi, Oles Dobosevych", "title": "NoPeopleAllowed: The Three-Step Approach to Weakly Supervised Semantic\n  Segmentation", "comments": "This short-paper was submitted to Learning from Imperfect Data\n  workshop at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to weakly supervised semantic segmentation, which\nconsists of three consecutive steps. The first two steps extract high-quality\npseudo masks from image-level annotated data, which are then used to train a\nsegmentation model on the third step. The presented approach also addresses two\nproblems in the data: class imbalance and missing labels. Using only\nimage-level annotations as supervision, our method is capable of segmenting\nvarious classes and complex objects. It achieves 37.34 mean IoU on the test\nset, placing 3rd at the LID Challenge in the task of weakly supervised semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 09:56:18 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Dobko", "Mariia", ""], ["Viniavskyi", "Ostap", ""], ["Dobosevych", "Oles", ""]]}, {"id": "2006.07604", "submitter": "Cheng Zhang", "authors": "Cheng Zhang", "title": "Dynamic gesture retrieval: searching videos by human pose sequence", "comments": "The problem proposed in this article should be classified as \"gesture\n  retrieval\" or \"gesture detection\", and there are already better algorithms to\n  deal with the proposed problem, for example Dynamic Time Warping (DTW) based\n  methods. The solution in this work gives little contribution to the field, so\n  I decided to withdraw it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of static human poses is limited, it is hard to retrieve the exact\nvideos using one single pose as the clue. However, with a pose sequence or a\ndynamic gesture as the keyword, retrieving specific videos becomes more\nfeasible. We propose a novel method for querying videos containing a designated\nsequence of human poses, whereas previous works only designate a single static\npose. The proposed method takes continuous 3d human poses from keyword gesture\nvideo and video candidates, then converts each pose in individual frames into\nbone direction descriptors, which describe the direction of each natural\nconnection in articulated pose. A temporal pyramid sliding window is then\napplied to find matches between designated gesture and video candidates, which\nensures that same gestures with different duration can be matched.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 10:11:22 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 03:53:15 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Zhang", "Cheng", ""]]}, {"id": "2006.07606", "submitter": "Tianren Wang", "authors": "Tianren Wang, Teng Zhang, Brian Lovell", "title": "Faces \\`a la Carte: Text-to-Face Generation via Attribute\n  Disentanglement", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-Face (TTF) synthesis is a challenging task with great potential for\ndiverse computer vision applications. Compared to Text-to-Image (TTI) synthesis\ntasks, the textual description of faces can be much more complicated and\ndetailed due to the variety of facial attributes and the parsing of high\ndimensional abstract natural language. In this paper, we propose a Text-to-Face\nmodel that not only produces images in high resolution (1024x1024) with\ntext-to-image consistency, but also outputs multiple diverse faces to cover a\nwide range of unspecified facial features in a natural way. By fine-tuning the\nmulti-label classifier and image encoder, our model obtains the vectors and\nimage embeddings which are used to transform the input noise vector sampled\nfrom the normal distribution. Afterwards, the transformed noise vector is fed\ninto a pre-trained high-resolution image generator to produce a set of faces\nwith the desired facial attributes. We refer to our model as TTF-HD.\nExperimental results show that TTF-HD generates high-quality faces with\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 10:24:31 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 07:21:45 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Wang", "Tianren", ""], ["Zhang", "Teng", ""], ["Lovell", "Brian", ""]]}, {"id": "2006.07607", "submitter": "Ziming Liu", "authors": "Ziming Liu and Guangyu Gao and Lin Sun and Zhiyuan Fang", "title": "HRDNet: High-resolution Detection Network for Small Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small object detection is challenging because small objects do not contain\ndetailed information and may even disappear in the deep network. Usually,\nfeeding high-resolution images into a network can alleviate this issue.\nHowever, simply enlarging the resolution will cause more problems, such as\nthat, it aggravates the large variant of object scale and introduces unbearable\ncomputation cost. To keep the benefits of high-resolution images without\nbringing up new problems, we proposed the High-Resolution Detection Network\n(HRDNet). HRDNet takes multiple resolution inputs using multi-depth backbones.\nTo fully take advantage of multiple features, we proposed Multi-Depth Image\nPyramid Network (MD-IPN) and Multi-Scale Feature Pyramid Network (MS-FPN) in\nHRDNet. MD-IPN maintains multiple position information using multiple depth\nbackbones. Specifically, high-resolution input will be fed into a shallow\nnetwork to reserve more positional information and reducing the computational\ncost while low-resolution input will be fed into a deep network to extract more\nsemantics. By extracting various features from high to low resolutions, the\nMD-IPN is able to improve the performance of small object detection as well as\nmaintaining the performance of middle and large objects. MS-FPN is proposed to\nalign and fuse multi-scale feature groups generated by MD-IPN to reduce the\ninformation imbalance between these multi-scale multi-level features. Extensive\nexperiments and ablation studies are conducted on the standard benchmark\ndataset MS COCO2017, Pascal VOC2007/2012 and a typical small object dataset,\nVisDrone 2019. Notably, our proposed HRDNet achieves the state-of-the-art on\nthese datasets and it performs better on small objects.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 10:25:35 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Liu", "Ziming", ""], ["Gao", "Guangyu", ""], ["Sun", "Lin", ""], ["Fang", "Zhiyuan", ""]]}, {"id": "2006.07609", "submitter": "Ziming Liu", "authors": "Ziming Liu and Guangyu Gao and A. K. Qin and Jinyang Li", "title": "DTG-Net: Differentiated Teachers Guided Self-Supervised Video Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art video action recognition models with complex network\narchitecture have archived significant improvements, but these models heavily\ndepend on large-scale well-labeled datasets. To reduce such dependency, we\npropose a self-supervised teacher-student architecture, i.e., the\nDifferentiated Teachers Guided self-supervised Network (DTG-Net). In DTG-Net,\nexcept for reducing labeled data dependency by self-supervised learning (SSL),\npre-trained action related models are used as teacher guidance providing prior\nknowledge to alleviate the demand for a large number of unlabeled videos in\nSSL. Specifically, leveraging the years of effort in action-related tasks,\ne.g., image classification, image-based action recognition, the DTG-Net learns\nthe self-supervised video representation under various teacher guidance, i.e.,\nthose well-trained models of action-related tasks. Meanwhile, the DTG-Net is\noptimized in the way of contrastive self-supervised learning. When two image\nsequences are randomly sampled from the same video or different videos as the\npositive or negative pairs, respectively, they are then sent to the teacher and\nstudent networks for feature embedding. After that, the contrastive feature\nconsistency is defined between features embedding of each pair, i.e.,\nconsistent for positive pair and inconsistent for negative pairs. Meanwhile, to\nreflect various teacher tasks' different guidance, we also explore different\nweighted guidance on teacher tasks. Finally, the DTG-Net is evaluated in two\nways: (i) the self-supervised DTG-Net to pre-train the supervised action\nrecognition models with only unlabeled videos; (ii) the supervised DTG-Net to\nbe jointly trained with the supervised action networks in an end-to-end way.\nIts performance is better than most pre-training methods but also has excellent\ncompetitiveness compared to supervised action recognition methods.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 10:40:31 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Liu", "Ziming", ""], ["Gao", "Guangyu", ""], ["Qin", "A. K.", ""], ["Li", "Jinyang", ""]]}, {"id": "2006.07630", "submitter": "Emilien Dupont", "authors": "Emilien Dupont, Miguel Angel Bautista, Alex Colburn, Aditya Sankar,\n  Carlos Guestrin, Josh Susskind, Qi Shan", "title": "Equivariant Neural Rendering", "comments": "Add link to code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for learning neural scene representations directly\nfrom images, without 3D supervision. Our key insight is that 3D structure can\nbe imposed by ensuring that the learned representation transforms like a real\n3D scene. Specifically, we introduce a loss which enforces equivariance of the\nscene representation with respect to 3D transformations. Our formulation allows\nus to infer and render scenes in real time while achieving comparable results\nto models requiring minutes for inference. In addition, we introduce two\nchallenging new datasets for scene representation and neural rendering,\nincluding scenes with complex lighting and backgrounds. Through experiments, we\nshow that our model achieves compelling results on these datasets as well as on\nstandard ShapeNet benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 12:25:07 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 11:28:31 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Dupont", "Emilien", ""], ["Bautista", "Miguel Angel", ""], ["Colburn", "Alex", ""], ["Sankar", "Aditya", ""], ["Guestrin", "Carlos", ""], ["Susskind", "Josh", ""], ["Shan", "Qi", ""]]}, {"id": "2006.07634", "submitter": "Qing Guo", "authors": "Hua Qi and Qing Guo and Felix Juefei-Xu and Xiaofei Xie and Lei Ma and\n  Wei Feng and Yang Liu and Jianjun Zhao", "title": "DeepRhythm: Exposing DeepFakes with Attentional Visual Heartbeat Rhythms", "comments": "11 pages, 7 figures; This paper has been accepted to ACM-MM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the GAN-based face image and video generation techniques, widely known as\nDeepFakes, have become more and more matured and realistic, there comes a\npressing and urgent demand for effective DeepFakes detectors. Motivated by the\nfact that remote visual photoplethysmography (PPG) is made possible by\nmonitoring the minuscule periodic changes of skin color due to blood pumping\nthrough the face, we conjecture that normal heartbeat rhythms found in the real\nface videos will be disrupted or even entirely broken in a DeepFake video,\nmaking it a potentially powerful indicator for DeepFake detection. In this\nwork, we propose DeepRhythm, a DeepFake detection technique that exposes\nDeepFakes by monitoring the heartbeat rhythms. DeepRhythm utilizes\ndual-spatial-temporal attention to adapt to dynamically changing face and fake\ntypes. Extensive experiments on FaceForensics++ and DFDC-preview datasets have\nconfirmed our conjecture and demonstrated not only the effectiveness, but also\nthe generalization capability of \\emph{DeepRhythm} over different datasets by\nvarious DeepFakes generation techniques and multifarious challenging\ndegradations.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 12:56:46 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 06:45:10 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Qi", "Hua", ""], ["Guo", "Qing", ""], ["Juefei-Xu", "Felix", ""], ["Xie", "Xiaofei", ""], ["Ma", "Lei", ""], ["Feng", "Wei", ""], ["Liu", "Yang", ""], ["Zhao", "Jianjun", ""]]}, {"id": "2006.07644", "submitter": "Lin Bai", "authors": "Lin Bai, Yecheng Lyu and Xinming Huang", "title": "RoadNet-RT: High Throughput CNN Architecture and SoC Design for\n  Real-Time Road Segmentation", "comments": null, "journal-ref": "in IEEE Transactions on Circuits and Systems I: Regular Papers,\n  vol. 68, no. 2, pp. 704-714, Feb. 2021", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, convolutional neural network has gained popularity in many\nengineering applications especially for computer vision. In order to achieve\nbetter performance, often more complex structures and advanced operations are\nincorporated into the neural networks, which results very long inference time.\nFor time-critical tasks such as autonomous driving and virtual reality,\nreal-time processing is fundamental. In order to reach real-time process speed,\na light-weight, high-throughput CNN architecture namely RoadNet-RT is proposed\nfor road segmentation in this paper. It achieves 90.33% MaxF score on test set\nof KITTI road segmentation task and 8 ms per frame when running on GTX 1080\nGPU. Comparing to the state-of-the-art network, RoadNet-RT speeds up the\ninference time by a factor of 20 at the cost of only 6.2% accuracy loss. For\nhardware design optimization, several techniques such as depthwise separable\nconvolution and non-uniformed kernel size convolution are customized designed\nto further reduce the processing time. The proposed CNN architecture has been\nsuccessfully implemented on an FPGA ZCU102 MPSoC platform that achieves the\ncomputation capability of 83.05 GOPS. The system throughput reaches 327.9\nframes per second with image size 1216x176.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 14:12:23 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 13:59:45 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bai", "Lin", ""], ["Lyu", "Yecheng", ""], ["Huang", "Xinming", ""]]}, {"id": "2006.07660", "submitter": "Dario Pavllo", "authors": "Dario Pavllo, Graham Spinks, Thomas Hofmann, Marie-Francine Moens,\n  Aurelien Lucchi", "title": "Convolutional Generation of Textured 3D Meshes", "comments": "NeurIPS 2020, Oral presentation. Code at\n  https://github.com/dariopavllo/convmesh", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent generative models for 2D images achieve impressive visual\nresults, they clearly lack the ability to perform 3D reasoning. This heavily\nrestricts the degree of control over generated objects as well as the possible\napplications of such models. In this work, we bridge this gap by leveraging\nrecent advances in differentiable rendering. We design a framework that can\ngenerate triangle meshes and associated high-resolution texture maps, using\nonly 2D supervision from single-view natural images. A key contribution of our\nwork is the encoding of the mesh and texture as 2D representations, which are\nsemantically aligned and can be easily modeled by a 2D convolutional GAN. We\ndemonstrate the efficacy of our method on Pascal3D+ Cars and CUB, both in an\nunconditional setting and in settings where the model is conditioned on class\nlabels, attributes, and text. Finally, we propose an evaluation methodology\nthat assesses the mesh and texture quality separately.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 15:23:29 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 10:21:45 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Pavllo", "Dario", ""], ["Spinks", "Graham", ""], ["Hofmann", "Thomas", ""], ["Moens", "Marie-Francine", ""], ["Lucchi", "Aurelien", ""]]}, {"id": "2006.07665", "submitter": "Yansong Tang", "authors": "Yansong Tang, Zanlin Ni, Jiahuan Zhou, Danyang Zhang, Jiwen Lu, Ying\n  Wu, Jie Zhou", "title": "Uncertainty-aware Score Distribution Learning for Action Quality\n  Assessment", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing action quality from videos has attracted growing attention in\nrecent years. Most existing approaches usually tackle this problem based on\nregression algorithms, which ignore the intrinsic ambiguity in the score labels\ncaused by multiple judges or their subjective appraisals. To address this\nissue, we propose an uncertainty-aware score distribution learning (USDL)\napproach for action quality assessment (AQA). Specifically, we regard an action\nas an instance associated with a score distribution, which describes the\nprobability of different evaluated scores. Moreover, under the circumstance\nwhere fine-grained score labels are available (e.g., difficulty degree of an\naction or multiple scores from different judges), we further devise a\nmulti-path uncertainty-aware score distributions learning (MUSDL) method to\nexplore the disentangled components of a score. We conduct experiments on three\nAQA datasets containing various Olympic actions and surgical activities, where\nour approaches set new state-of-the-arts under the Spearman's Rank Correlation.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 15:41:29 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Tang", "Yansong", ""], ["Ni", "Zanlin", ""], ["Zhou", "Jiahuan", ""], ["Zhang", "Danyang", ""], ["Lu", "Jiwen", ""], ["Wu", "Ying", ""], ["Zhou", "Jie", ""]]}, {"id": "2006.07694", "submitter": "Hengtao Guo", "authors": "Hengtao Guo, Sheng Xu, Bradford Wood, Pingkun Yan", "title": "Sensorless Freehand 3D Ultrasound Reconstruction via Deep Contextual\n  Learning", "comments": "Provisionally accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transrectal ultrasound (US) is the most commonly used imaging modality to\nguide prostate biopsy and its 3D volume provides even richer context\ninformation. Current methods for 3D volume reconstruction from freehand US\nscans require external tracking devices to provide spatial position for every\nframe. In this paper, we propose a deep contextual learning network (DCL-Net),\nwhich can efficiently exploit the image feature relationship between US frames\nand reconstruct 3D US volumes without any tracking device. The proposed DCL-Net\nutilizes 3D convolutions over a US video segment for feature extraction. An\nembedded self-attention module makes the network focus on the speckle-rich\nareas for better spatial movement prediction. We also propose a novel case-wise\ncorrelation loss to stabilize the training process for improved accuracy.\nHighly promising results have been obtained by using the developed method. The\nexperiments with ablation studies demonstrate superior performance of the\nproposed method by comparing against other state-of-the-art methods. Source\ncode of this work is publicly available at\nhttps://github.com/DIAL-RPI/FreehandUSRecon.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 18:37:30 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Guo", "Hengtao", ""], ["Xu", "Sheng", ""], ["Wood", "Bradford", ""], ["Yan", "Pingkun", ""]]}, {"id": "2006.07722", "submitter": "Yuhuang Hu", "authors": "Yuhuang Hu and Shih-Chii Liu and Tobi Delbruck", "title": "v2e: From Video Frames to Realistic DVS Events", "comments": "Accepted at 2021 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW); Third International Workshop on Event-Based\n  Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To help meet the increasing need for dynamic vision sensor (DVS) event camera\ndata, this paper proposes the v2e toolbox that generates realistic synthetic\nDVS events from intensity frames. It also clarifies incorrect claims about DVS\nmotion blur and latency characteristics in recent literature. Unlike other\ntoolboxes, v2e includes pixel-level Gaussian event threshold mismatch, finite\nintensity-dependent bandwidth, and intensity-dependent noise. Realistic DVS\nevents are useful in training networks for uncontrolled lighting conditions.\nThe use of v2e synthetic events is demonstrated in two experiments. The first\nexperiment is object recognition with N-Caltech 101 dataset. Results show that\npretraining on various v2e lighting conditions improves generalization when\ntransferred on real DVS data for a ResNet model. The second experiment shows\nthat for night driving, a car detector trained with v2e events shows an average\naccuracy improvement of 40% compared to the YOLOv3 trained on intensity frames.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 21:06:12 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 08:30:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hu", "Yuhuang", ""], ["Liu", "Shih-Chii", ""], ["Delbruck", "Tobi", ""]]}, {"id": "2006.07733", "submitter": "Michal Valko", "authors": "Jean-Bastien Grill, Florian Strub, Florent Altch\\'e, Corentin Tallec,\n  Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires,\n  Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu,\n  R\\'emi Munos, Michal Valko", "title": "Bootstrap your own latent: A new approach to self-supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Bootstrap Your Own Latent (BYOL), a new approach to\nself-supervised image representation learning. BYOL relies on two neural\nnetworks, referred to as online and target networks, that interact and learn\nfrom each other. From an augmented view of an image, we train the online\nnetwork to predict the target network representation of the same image under a\ndifferent augmented view. At the same time, we update the target network with a\nslow-moving average of the online network. While state-of-the art methods rely\non negative pairs, BYOL achieves a new state of the art without them. BYOL\nreaches $74.3\\%$ top-1 classification accuracy on ImageNet using a linear\nevaluation with a ResNet-50 architecture and $79.6\\%$ with a larger ResNet. We\nshow that BYOL performs on par or better than the current state of the art on\nboth transfer and semi-supervised benchmarks. Our implementation and pretrained\nmodels are given on GitHub.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 22:35:21 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 13:38:14 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 09:46:02 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Grill", "Jean-Bastien", ""], ["Strub", "Florian", ""], ["Altch\u00e9", "Florent", ""], ["Tallec", "Corentin", ""], ["Richemond", "Pierre H.", ""], ["Buchatskaya", "Elena", ""], ["Doersch", "Carl", ""], ["Pires", "Bernardo Avila", ""], ["Guo", "Zhaohan Daniel", ""], ["Azar", "Mohammad Gheshlaghi", ""], ["Piot", "Bilal", ""], ["Kavukcuoglu", "Koray", ""], ["Munos", "R\u00e9mi", ""], ["Valko", "Michal", ""]]}, {"id": "2006.07739", "submitter": "Guohao Li", "authors": "Guohao Li, Chenxin Xiong, Ali Thabet, Bernard Ghanem", "title": "DeeperGCN: All You Need to Train Deeper GCNs", "comments": "This work is still working in process. More results will be updated\n  in the future version. Project website: https://www.deepgcns.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have been drawing significant attention\nwith the power of representation learning on graphs. Unlike Convolutional\nNeural Networks (CNNs), which are able to take advantage of stacking very deep\nlayers, GCNs suffer from vanishing gradient, over-smoothing and over-fitting\nissues when going deeper. These challenges limit the representation power of\nGCNs on large-scale graphs. This paper proposes DeeperGCN that is capable of\nsuccessfully and reliably training very deep GCNs. We define differentiable\ngeneralized aggregation functions to unify different message aggregation\noperations (e.g. mean, max). We also propose a novel normalization layer namely\nMsgNorm and a pre-activation version of residual connections for GCNs.\nExtensive experiments on Open Graph Benchmark (OGB) show DeeperGCN\nsignificantly boosts performance over the state-of-the-art on the large scale\ngraph learning tasks of node property prediction and graph property prediction.\nPlease visit https://www.deepgcns.org for more information.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 23:00:22 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Li", "Guohao", ""], ["Xiong", "Chenxin", ""], ["Thabet", "Ali", ""], ["Ghanem", "Bernard", ""]]}, {"id": "2006.07742", "submitter": "Omid Hosseini Jafari", "authors": "Omid Hosseini Jafari, Carsten Rother", "title": "Split-Merge Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are a variety of approaches to obtain a vast receptive field with\nconvolutional neural networks (CNNs), such as pooling or striding convolutions.\nMost of these approaches were initially designed for image classification and\nlater adapted to dense prediction tasks, such as semantic segmentation.\nHowever, the major drawback of this adaptation is the loss of spatial\ninformation. Even the popular dilated convolution approach, which in theory is\nable to operate with full spatial resolution, needs to subsample features for\nlarge image sizes in order to make the training and inference tractable. In\nthis work, we introduce Split-Merge pooling to fully preserve the spatial\ninformation without any subsampling. By applying Split-Merge pooling to deep\nnetworks, we achieve, at the same time, a very large receptive field. We\nevaluate our approach for dense semantic segmentation of large image sizes\ntaken from the Cityscapes and GTA-5 datasets. We demonstrate that by replacing\nmax-pooling and striding convolutions with our split-merge pooling, we are able\nto improve the accuracy of different variations of ResNet significantly.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 23:20:30 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Jafari", "Omid Hosseini", ""], ["Rother", "Carsten", ""]]}, {"id": "2006.07743", "submitter": "David Fuentes-Jimenez", "authors": "Adrian Sanchez-Caballero, Sergio de L\\'opez-Diz, David\n  Fuentes-Jimenez, Cristina Losada-Guti\\'errez, Marta Marr\\'on-Romera, David\n  Casillas-Perez, Mohammad Ibrahim Sarker", "title": "3DFCNN: Real-Time Action Recognition using 3D Deep Neural Networks with\n  Raw Depth Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human actions recognition is a fundamental task in artificial vision, that\nhas earned a great importance in recent years due to its multiple applications\nin different areas. %, such as the study of human behavior, security or video\nsurveillance. In this context, this paper describes an approach for real-time\nhuman action recognition from raw depth image-sequences, provided by an RGB-D\ncamera. The proposal is based on a 3D fully convolutional neural network, named\n3DFCNN, which automatically encodes spatio-temporal patterns from depth\nsequences without %any costly pre-processing. Furthermore, the described 3D-CNN\nallows %automatic features extraction and actions classification from the\nspatial and temporal encoded information of depth sequences. The use of depth\ndata ensures that action recognition is carried out protecting people's\nprivacy% allows recognizing the actions carried out by people, protecting their\nprivacy%\\sout{of them} , since their identities can not be recognized from\nthese data. %\\st{ from depth images.} 3DFCNN has been evaluated and its results\ncompared to those from other state-of-the-art methods within three widely used\n%large-scale NTU RGB+D datasets, with different characteristics (resolution,\nsensor type, number of views, camera location, etc.). The obtained results\nallows validating the proposal, concluding that it outperforms several\nstate-of-the-art approaches based on classical computer vision techniques.\nFurthermore, it achieves action recognition accuracy comparable to deep\nlearning based state-of-the-art methods with a lower computational cost, which\nallows its use in real-time applications.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 23:24:07 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Sanchez-Caballero", "Adrian", ""], ["de L\u00f3pez-Diz", "Sergio", ""], ["Fuentes-Jimenez", "David", ""], ["Losada-Guti\u00e9rrez", "Cristina", ""], ["Marr\u00f3n-Romera", "Marta", ""], ["Casillas-Perez", "David", ""], ["Sarker", "Mohammad Ibrahim", ""]]}, {"id": "2006.07744", "submitter": "David Fuentes-Jimenez", "authors": "Adrian Sanchez-Caballero, David Fuentes-Jimenez, Cristina\n  Losada-Guti\\'errez", "title": "Exploiting the ConvLSTM: Human Action Recognition using Raw Depth\n  Video-Based Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As in many other different fields, deep learning has become the main approach\nin most computer vision applications, such as scene understanding, object\nrecognition, computer-human interaction or human action recognition (HAR).\nResearch efforts within HAR have mainly focused on how to efficiently extract\nand process both spatial and temporal dependencies of video sequences. In this\npaper, we propose and compare, two neural networks based on the convolutional\nlong short-term memory unit, namely ConvLSTM, with differences in the\narchitecture and the long-term learning strategy. The former uses a\nvideo-length adaptive input data generator (\\emph{stateless}) whereas the\nlatter explores the \\emph{stateful} ability of general recurrent neural\nnetworks but applied in the particular case of HAR. This stateful property\nallows the model to accumulate discriminative patterns from previous frames\nwithout compromising computer memory. Experimental results on the large-scale\nNTU RGB+D dataset show that the proposed models achieve competitive recognition\naccuracies with lower computational cost compared with state-of-the-art methods\nand prove that, in the particular case of videos, the rarely-used stateful mode\nof recurrent neural networks significantly improves the accuracy obtained with\nthe standard mode. The recognition accuracies obtained are 75.26\\% (CS) and\n75.45\\% (CV) for the stateless model, with an average time consumption per\nvideo of 0.21 s, and 80.43\\% (CS) and 79.91\\%(CV) with 0.89 s for the stateful\nversion.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 23:35:59 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Sanchez-Caballero", "Adrian", ""], ["Fuentes-Jimenez", "David", ""], ["Losada-Guti\u00e9rrez", "Cristina", ""]]}, {"id": "2006.07752", "submitter": "Anh Thai", "authors": "Anh Thai, Stefan Stojanov, Vijay Upadhya, James M. Rehg", "title": "3D Reconstruction of Novel Object Shapes from Single Images", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key challenge in single image 3D shape reconstruction is to ensure that\ndeep models can generalize to shapes which were not part of the training set.\nThis is challenging because the algorithm must infer the occluded portion of\nthe surface by leveraging a representation learned based on the shape\ncharacteristics of the training data, and is therefore vulnerable to\noverfitting. Such generalization to unseen categories of objects is a function\nof both architecture design and training approaches. This paper introduces\nSDFNet, a novel shape prediction architecture and training approach which\nsupports effective generalization. We provide an extensive investigation of the\nfactors which influence generalization accuracy and its measurement, ranging\nfrom the consistent use of 3D shape metrics to the choice of rendering approach\nand the large-scale evaluation on unseen shapes using ShapeNetCore.v2 and ABC.\nWe show that SDFNet provides state-of-the-art performance on seen and unseen\nshapes relative to existing baseline methods GenRe and OccNet. We provide the\nfirst large-scale experimental evaluation of generalization performance. The\ncodebase released with this article will allow for the consistent evaluation\nand comparison of methods for single image shape reconstruction.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 00:34:26 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 02:12:45 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 19:41:48 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Thai", "Anh", ""], ["Stojanov", "Stefan", ""], ["Upadhya", "Vijay", ""], ["Rehg", "James M.", ""]]}, {"id": "2006.07755", "submitter": "Wenxi Liu", "authors": "Yue Gu, Wenxi Liu", "title": "Recurrent Distillation based Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the progress of deep learning technologies, crowd\ncounting has been rapidly developed. In this work, we propose a simple yet\neffective crowd counting framework that is able to achieve the state-of-the-art\nperformance on various crowded scenes. In particular, we first introduce a\nperspective-aware density map generation method that is able to produce\nground-truth density maps from point annotations to train crowd counting model\nto accomplish superior performance than prior density map generation\ntechniques. Besides, leveraging our density map generation method, we propose\nan iterative distillation algorithm to progressively enhance our model with\nidentical network structures, without significantly sacrificing the dimension\nof the output density maps. In experiments, we demonstrate that, with our\nsimple convolutional neural network architecture strengthened by our proposed\ntraining algorithm, our model is able to outperform or be comparable with the\nstate-of-the-art methods. Furthermore, we also evaluate our density map\ngeneration approach and distillation algorithm in ablation studies.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 01:04:52 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Gu", "Yue", ""], ["Liu", "Wenxi", ""]]}, {"id": "2006.07776", "submitter": "Pengfei Ge", "authors": "Pengfei Ge, Chuan-Xian Ren, Dao-Qing Dai, Hong Yan", "title": "Domain Adaptation and Image Classification via Deep Conditional\n  Adaptation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation aims to generalize the supervised model\ntrained on a source domain to an unlabeled target domain. Marginal distribution\nalignment of feature spaces is widely used to reduce the domain discrepancy\nbetween the source and target domains. However, it assumes that the source and\ntarget domains share the same label distribution, which limits their\napplication scope. In this paper, we consider a more general application\nscenario where the label distributions of the source and target domains are not\nthe same. In this scenario, marginal distribution alignment-based methods will\nbe vulnerable to negative transfer. To address this issue, we propose a novel\nunsupervised domain adaptation method, Deep Conditional Adaptation Network\n(DCAN), based on conditional distribution alignment of feature spaces. To be\nspecific, we reduce the domain discrepancy by minimizing the Conditional\nMaximum Mean Discrepancy between the conditional distributions of deep features\non the source and target domains, and extract the discriminant information from\ntarget domain by maximizing the mutual information between samples and the\nprediction labels. In addition, DCAN can be used to address a special scenario,\nPartial unsupervised domain adaptation, where the target domain category is a\nsubset of the source domain category. Experiments on both unsupervised domain\nadaptation and Partial unsupervised domain adaptation show that DCAN achieves\nsuperior classification performance over state-of-the-art methods. In\nparticular, DCAN achieves great improvement in the tasks with large difference\nin label distributions (6.1\\% on SVHN to MNIST, 5.4\\% in UDA tasks on\nOffice-Home and 4.5\\% in Partial UDA tasks on Office-Home).\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 02:56:01 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Ge", "Pengfei", ""], ["Ren", "Chuan-Xian", ""], ["Dai", "Dao-Qing", ""], ["Yan", "Hong", ""]]}, {"id": "2006.07778", "submitter": "Shichao Li", "authors": "Shichao Li, Lei Ke, Kevin Pratama, Yu-Wing Tai, Chi-Keung Tang,\n  Kwang-Ting Cheng", "title": "Cascaded deep monocular 3D human pose estimation with evolutionary\n  training data", "comments": "Accepted to CVPR 2020 as Oral Presentation", "journal-ref": null, "doi": "10.1109/CVPR42600.2020.00621", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end deep representation learning has achieved remarkable accuracy for\nmonocular 3D human pose estimation, yet these models may fail for unseen poses\nwith limited and fixed training data. This paper proposes a novel data\naugmentation method that: (1) is scalable for synthesizing massive amount of\ntraining data (over 8 million valid 3D human poses with corresponding 2D\nprojections) for training 2D-to-3D networks, (2) can effectively reduce dataset\nbias. Our method evolves a limited dataset to synthesize unseen 3D human\nskeletons based on a hierarchical human representation and heuristics inspired\nby prior knowledge. Extensive experiments show that our approach not only\nachieves state-of-the-art accuracy on the largest public benchmark, but also\ngeneralizes significantly better to unseen and rare poses. Code, pre-trained\nmodels and tools are available at this HTTPS URL.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 03:09:52 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 08:08:15 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 02:37:06 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Li", "Shichao", ""], ["Ke", "Lei", ""], ["Pratama", "Kevin", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""], ["Cheng", "Kwang-Ting", ""]]}, {"id": "2006.07789", "submitter": "Myung-Hwan Jeon", "authors": "Myung-Hwan Jeon and Ayoung Kim", "title": "PrimA6D: Rotational Primitive Reconstruction for Enhanced and Robust 6D\n  Pose Estimation", "comments": "RA-L and IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a rotational primitive prediction based 6D object\npose estimation using a single image as an input. We solve for the 6D object\npose of a known object relative to the camera using a single image with\nocclusion. Many recent state-of-the-art (SOTA) two-step approaches have\nexploited image keypoints extraction followed by PnP regression for pose\nestimation. Instead of relying on bounding box or keypoints on the object, we\npropose to learn orientation-induced primitive so as to achieve the pose\nestimation accuracy regardless of the object size. We leverage a Variational\nAutoEncoder (VAE) to learn this underlying primitive and its associated\nkeypoints. The keypoints inferred from the reconstructed primitive image are\nthen used to regress the rotation using PnP. Lastly, we compute the translation\nin a separate localization module to complete the entire 6D pose estimation.\nWhen evaluated over public datasets, the proposed method yields a notable\nimprovement over the LINEMOD, the Occlusion LINEMOD, and the YCB-Video dataset.\nWe further provide a synthetic-only trained case presenting comparable\nperformance to the existing methods which require real images in the training\nphase.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 03:55:42 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 10:39:13 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Jeon", "Myung-Hwan", ""], ["Kim", "Ayoung", ""]]}, {"id": "2006.07793", "submitter": "Qingnan Fan", "authors": "Jialei Huang, Guanqi Zhan, Qingnan Fan, Kaichun Mo, Lin Shao, Baoquan\n  Chen, Leonidas Guibas, Hao Dong", "title": "Generative 3D Part Assembly via Dynamic Graph Learning", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous part assembly is a challenging yet crucial task in 3D computer\nvision and robotics. Analogous to buying an IKEA furniture, given a set of 3D\nparts that can assemble a single shape, an intelligent agent needs to perceive\nthe 3D part geometry, reason to propose pose estimations for the input parts,\nand finally call robotic planning and control routines for actuation. In this\npaper, we focus on the pose estimation subproblem from the vision side\ninvolving geometric and relational reasoning over the input part geometry.\nEssentially, the task of generative 3D part assembly is to predict a 6-DoF part\npose, including a rigid rotation and translation, for each input part that\nassembles a single 3D shape as the final output. To tackle this problem, we\npropose an assembly-oriented dynamic graph learning framework that leverages an\niterative graph neural network as a backbone. It explicitly conducts sequential\npart assembly refinements in a coarse-to-fine manner, exploits a pair of part\nrelation reasoning module and part aggregation module for dynamically adjusting\nboth part features and their relations in the part graph. We conduct extensive\nexperiments and quantitative comparisons to three strong baseline methods,\ndemonstrating the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 04:26:42 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 04:49:25 GMT"}, {"version": "v3", "created": "Wed, 23 Dec 2020 05:50:35 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Huang", "Jialei", ""], ["Zhan", "Guanqi", ""], ["Fan", "Qingnan", ""], ["Mo", "Kaichun", ""], ["Shao", "Lin", ""], ["Chen", "Baoquan", ""], ["Guibas", "Leonidas", ""], ["Dong", "Hao", ""]]}, {"id": "2006.07795", "submitter": "Yi Zhang", "authors": "Zerui Shao, Yifei Pu, Jiliu Zhou, Bihan Wen and Yi Zhang", "title": "Hyper RPCA: Joint Maximum Correntropy Criterion and Laplacian Scale\n  Mixture Modeling On-the-Fly for Moving Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving object detection is critical for automated video analysis in many\nvision-related tasks, such as surveillance tracking, video compression coding,\netc. Robust Principal Component Analysis (RPCA), as one of the most popular\nmoving object modelling methods, aims to separate the temporally varying (i.e.,\nmoving) foreground objects from the static background in video, assuming the\nbackground frames to be low-rank while the foreground to be spatially sparse.\nClassic RPCA imposes sparsity of the foreground component using l1-norm, and\nminimizes the modeling error via 2-norm. We show that such assumptions can be\ntoo restrictive in practice, which limits the effectiveness of the classic\nRPCA, especially when processing videos with dynamic background, camera jitter,\ncamouflaged moving object, etc. In this paper, we propose a novel RPCA-based\nmodel, called Hyper RPCA, to detect moving objects on the fly. Different from\nclassic RPCA, the proposed Hyper RPCA jointly applies the maximum correntropy\ncriterion (MCC) for the modeling error, and Laplacian scale mixture (LSM) model\nfor foreground objects. Extensive experiments have been conducted, and the\nresults demonstrate that the proposed Hyper RPCA has competitive performance\nfor foreground detection to the state-of-the-art algorithms on several\nwell-known benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 04:35:45 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Shao", "Zerui", ""], ["Pu", "Yifei", ""], ["Zhou", "Jiliu", ""], ["Wen", "Bihan", ""], ["Zhang", "Yi", ""]]}, {"id": "2006.07796", "submitter": "Felix Leeb", "authors": "Felix Leeb, Guilia Lanzillotta, Yashas Annadani, Michel Besserve,\n  Stefan Bauer, Bernhard Sch\\\"olkopf", "title": "Structure by Architecture: Disentangled Representations without\n  Regularization", "comments": "Under review at NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of self-supervised structured representation learning\nusing autoencoders for generative modeling. Unlike most methods which rely on\nmatching an arbitrary, relatively unstructured, prior distribution for\nsampling, we propose a sampling technique that relies solely on the\nindependence of latent variables, thereby avoiding the trade-off between\nreconstruction quality and generative performance inherent to VAEs. We design a\nnovel autoencoder architecture capable of learning a structured representation\nwithout the need for aggressive regularization. Our structural decoders learn a\nhierarchy of latent variables, akin to structural causal models, thereby\nordering the information without any additional regularization. We demonstrate\nhow these models learn a representation that improves results in a variety of\ndownstream tasks including generation, disentanglement, and extrapolation using\nseveral challenging and natural image datasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 04:37:08 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 13:52:33 GMT"}, {"version": "v3", "created": "Sun, 4 Jul 2021 11:59:35 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Leeb", "Felix", ""], ["Lanzillotta", "Guilia", ""], ["Annadani", "Yashas", ""], ["Besserve", "Michel", ""], ["Bauer", "Stefan", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2006.07802", "submitter": "Cho-Ying Wu", "authors": "Cho-Ying Wu, Xiaoyan Hu, Michael Happold, Qiangeng Xu, Ulrich Neumann", "title": "Geometry-Aware Instance Segmentation with Disparity Maps", "comments": "CVPR 2020 Workshop of Scalability in Autonomous Driving (WSAD).\n  Please refer to WSAD site for details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most previous works of outdoor instance segmentation for images only use\ncolor information. We explore a novel direction of sensor fusion to exploit\nstereo cameras. Geometric information from disparities helps separate\noverlapping objects of the same or different classes. Moreover, geometric\ninformation penalizes region proposals with unlikely 3D shapes thus suppressing\nfalse positive detections. Mask regression is based on 2D, 2.5D, and 3D ROI\nusing the pseudo-lidar and image-based representations. These mask predictions\nare fused by a mask scoring process. However, public datasets only adopt stereo\nsystems with shorter baseline and focal legnth, which limit measuring ranges of\nstereo cameras. We collect and utilize High-Quality Driving Stereo (HQDS)\ndataset, using much longer baseline and focal length with higher resolution.\nOur performance attains state of the art. Please refer to our project page. The\nfull paper is available here.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 05:03:52 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Wu", "Cho-Ying", ""], ["Hu", "Xiaoyan", ""], ["Happold", "Michael", ""], ["Xu", "Qiangeng", ""], ["Neumann", "Ulrich", ""]]}, {"id": "2006.07807", "submitter": "Yuchao Dai Dr.", "authors": "Ke Wang, Bin Fan, and Yuchao Dai", "title": "Relative Pose Estimation for Stereo Rolling Shutter Cameras", "comments": "Accepted by International Conference on Image Processing (ICIP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel linear algorithm to estimate the 6 DoF\nrelative pose from consecutive frames of stereo rolling shutter (RS) cameras.\nOur method is derived based on the assumption that stereo cameras undergo\nmotion with constant velocity around the center of the baseline, which needs 9\npairs of correspondences on both left and right consecutive frames. The stereo\nRS images enable the recovery of depth maps from the semi-global matching (SGM)\nalgorithm. With the estimated camera motion and depth map, we can correct the\nRS images to get the undistorted images without any scene structure assumption.\nExperiments on both simulated points and synthetic RS images demonstrate the\neffectiveness of our algorithm in relative pose estimation.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 05:58:39 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Wang", "Ke", ""], ["Fan", "Bin", ""], ["Dai", "Yuchao", ""]]}, {"id": "2006.07809", "submitter": "Chiranjib Sur", "authors": "Chiranjib Sur", "title": "ReLGAN: Generalization of Consistency for GAN with Disjoint Constraints\n  and Relative Learning of Generative Processes for Multiple Transformation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image to image transformation has gained popularity from different research\ncommunities due to its enormous impact on different applications, including\nmedical. In this work, we have introduced a generalized scheme for consistency\nfor GAN architectures with two new concepts of Transformation Learning (TL) and\nRelative Learning (ReL) for enhanced learning image transformations.\nConsistency for GAN architectures suffered from inadequate constraints and\nfailed to learn multiple and multi-modal transformations, which is inevitable\nfor many medical applications. The main drawback is that it focused on creating\nan intermediate and workable hybrid, which is not permissible for the medical\napplications which focus on minute details. Another drawback is the weak\ninterrelation between the two learning phases and TL and ReL have introduced\nimproved coordination among them. We have demonstrated the capability of the\nnovel network framework on public datasets. We emphasized that our novel\narchitecture produced an improved neural image transformation version for the\nimage, which is more acceptable to the medical community. Experiments and\nresults demonstrated the effectiveness of our framework with enhancement\ncompared to the previous works.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 06:03:30 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Sur", "Chiranjib", ""]]}, {"id": "2006.07810", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu", "title": "Disentanglement for Discriminative Visual Recognition", "comments": "Manuscript for book \"Recognition and perception of images\" Willy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes of deep learning-based recognition rely on maintaining the\ncontent related to the main-task label. However, how to explicitly dispel the\nnoisy signals for better generalization in a controllable manner remains an\nopen issue. For instance, various factors such as identity-specific attributes,\npose, illumination and expression affect the appearance of face images.\nDisentangling the identity-specific factors is potentially beneficial for\nfacial expression recognition (FER). This chapter systematically summarize the\ndetrimental factors as task-relevant/irrelevant semantic variations and\nunspecified latent variation. In this chapter, these problems are casted as\neither a deep metric learning problem or an adversarial minimax game in the\nlatent space. For the former choice, a generalized adaptive (N+M)-tuplet\nclusters loss function together with the identity-aware hard-negative mining\nand online positive mining scheme can be used for identity-invariant FER. The\nbetter FER performance can be achieved by combining the deep metric loss and\nsoftmax loss in a unified two fully connected layer branches framework via\njoint optimization. For the latter solution, it is possible to equipping an\nend-to-end conditional adversarial network with the ability to decompose an\ninput sample into three complementary parts. The discriminative representation\ninherits the desired invariance property guided by prior knowledge of the task,\nwhich is marginal independent to the task-relevant/irrelevant semantic and\nlatent variations. The framework achieves top performance on a serial of tasks,\nincluding lighting, makeup, disguise-tolerant face recognition and facial\nattributes recognition. This chapter systematically summarize the popular and\npractical solution for disentanglement to achieve more discriminative visual\nrecognition.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 06:10:51 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Liu", "Xiaofeng", ""]]}, {"id": "2006.07816", "submitter": "Erick Maraz", "authors": "Paul Gafton and Erick Maraz", "title": "2D Image Relighting with Image-to-Image Translation", "comments": "12 pages, 52 Postscript figures, uses cvpr_eso.sty eso-pic.sty\n  ruler.sty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of Generative Adversarial Networks (GANs), a finer level of\ncontrol in manipulating various features of an image has become possible. One\nexample of such fine manipulation is changing the position of the light source\nin a scene. This is fundamentally an ill-posed problem, since it requires\nunderstanding the scene geometry to generate proper lighting effects. This\nproblem is not a trivial one and can become even more complicated if we want to\nchange the direction of the light source from any direction to a specific one.\nHere we provide our attempt to solve this problem using GANs. Specifically,\npix2pix [arXiv:1611.07004] trained with the dataset VIDIT [arXiv:2005.05460]\nwhich contains images of the same scene with different types of light\ntemperature and 8 different light source positions (N, NE, E, SE, S, SW, W,\nNW). The results are 8 neural networks trained to be able to change the\ndirection of the light source from any direction to one of the 8 previously\nmentioned. Additionally, we provide, as a tool, a simple CNN trained to\nidentify the direction of the light source in an image.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 06:39:53 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 23:35:24 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Gafton", "Paul", ""], ["Maraz", "Erick", ""]]}, {"id": "2006.07818", "submitter": "Congyue Deng", "authors": "Congyue Deng, Tai-Jiang Mu, Shi-Min Hu", "title": "Alternating ConvLSTM: Learning Force Propagation with Alternate State\n  Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven simulation is an important step-forward in computational physics\nwhen traditional numerical methods meet their limits. Learning-based simulators\nhave been widely studied in past years; however, most previous works view\nsimulation as a general spatial-temporal prediction problem and take little\nphysical guidance in designing their neural network architectures. In this\npaper, we introduce the alternating convolutional Long Short-Term Memory\n(Alt-ConvLSTM) that models the force propagation mechanisms in a deformable\nobject with near-uniform material properties. Specifically, we propose an\naccumulation state, and let the network update its cell state and the\naccumulation state alternately. We demonstrate how this novel scheme imitates\nthe alternate updates of the first and second-order terms in the forward Euler\nmethod of numerical PDE solvers. Benefiting from this, our network only\nrequires a small number of parameters, independent of the number of the\nsimulated particles, and also retains the essential features in ConvLSTM,\nmaking it naturally applicable to sequential data with spatial inputs and\noutputs. We validate our Alt-ConvLSTM on human soft tissue simulation with\nthousands of particles and consistent body pose changes. Experimental results\nshow that Alt-ConvLSTM efficiently models the material kinetic features and\ngreatly outperforms vanilla ConvLSTM with only the single state update.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 06:43:33 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Deng", "Congyue", ""], ["Mu", "Tai-Jiang", ""], ["Hu", "Shi-Min", ""]]}, {"id": "2006.07820", "submitter": "Minda Zhao", "authors": "Minda Zhao, Qiang Ling", "title": "Adaptively Meshed Video Stabilization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video stabilization is essential for improving visual quality of shaky\nvideos. The current video stabilization methods usually take feature\ntrajectories in the background to estimate one global transformation matrix or\nseveral transformation matrices based on a fixed mesh, and warp shaky frames\ninto their stabilized views. However, these methods may not model the shaky\ncamera motion well in complicated scenes, such as scenes containing large\nforeground objects or strong parallax, and may result in notable visual\nartifacts in the stabilized videos. To resolve the above issues, this paper\nproposes an adaptively meshed method to stabilize a shaky video based on all of\nits feature trajectories and an adaptive blocking strategy. More specifically,\nwe first extract feature trajectories of the shaky video and then generate a\ntriangle mesh according to the distribution of the feature trajectories in each\nframe. Then transformations between shaky frames and their stabilized views\nover all triangular grids of the mesh are calculated to stabilize the shaky\nvideo. Since more feature trajectories can usually be extracted from all\nregions, including both background and foreground regions, a finer mesh will be\nobtained and provided for camera motion estimation and frame warping. We\nestimate the mesh-based transformations of each frame by solving a two-stage\noptimization problem. Moreover, foreground and background feature trajectories\nare no longer distinguished and both contribute to the estimation of the camera\nmotion in the proposed optimization problem, which yields better estimation\nperformance than previous works, particularly in challenging videos with large\nforeground objects or strong parallax.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 06:51:23 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhao", "Minda", ""], ["Ling", "Qiang", ""]]}, {"id": "2006.07825", "submitter": "Artem Kozlov", "authors": "Artem Kozlov", "title": "Working with scale: 2nd place solution to Product Detection in Densely\n  Packed Scenes [Technical Report]", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This report describes a 2nd place solution of the detection challenge which\nis held within CVPR 2020 Retail-Vision workshop. Instead of going further\nconsidering previous results this work mainly aims to verify previously\nobserved takeaways by re-experimenting. The reliability and reproducibility of\nthe results are reached by incorporating a popular object detection toolbox -\nMMDetection. In this report, I firstly represent the results received for\nFaster-RCNN and RetinaNet models, which were taken for comparison in the\noriginal work. Then I describe the experiment results with more advanced\nmodels. The final section reviews two simple tricks for Faster-RCNN model that\nwere used for my final submission: changing default anchor scale parameter and\ntrain-time image tiling. The source code is available at\nhttps://github.com/tyomj/product_detection.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 07:17:21 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Kozlov", "Artem", ""]]}, {"id": "2006.07826", "submitter": "Yi Fang", "authors": "Jingyu Deng, Xiang Li, Yi Fang", "title": "Few-shot Object Detection on Remote Sensing Images", "comments": "12pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we deal with the problem of object detection on remote sensing\nimages. Previous methods have developed numerous deep CNN-based methods for\nobject detection on remote sensing images and the report remarkable\nachievements in detection performance and efficiency. However, current\nCNN-based methods mostly require a large number of annotated samples to train\ndeep neural networks and tend to have limited generalization abilities for\nunseen object categories. In this paper, we introduce a few-shot learning-based\nmethod for object detection on remote sensing images where only a few annotated\nsamples are provided for the unseen object categories. More specifically, our\nmodel contains three main components: a meta feature extractor that learns to\nextract feature representations from input images, a reweighting module that\nlearn to adaptively assign different weights for each feature representation\nfrom the support images, and a bounding box prediction module that carries out\nobject detection on the reweighted feature maps. We build our few-shot object\ndetection model upon YOLOv3 architecture and develop a multi-scale object\ndetection framework. Experiments on two benchmark datasets demonstrate that\nwith only a few annotated samples our model can still achieve a satisfying\ndetection performance on remote sensing images and the performance of our model\nis significantly better than the well-established baseline models.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 07:18:10 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 03:55:42 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Deng", "Jingyu", ""], ["Li", "Xiang", ""], ["Fang", "Yi", ""]]}, {"id": "2006.07827", "submitter": "Chi-Hieu Pham", "authors": "Chi-Hieu Pham and Sa\\\"id Ladjal and Alasdair Newson", "title": "PCAAE: Principal Component Analysis Autoencoder for organising the\n  latent space of generative networks", "comments": "Preprint with Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders and generative models produce some of the most spectacular deep\nlearning results to date. However, understanding and controlling the latent\nspace of these models presents a considerable challenge. Drawing inspiration\nfrom principal component analysis and autoencoder, we propose the Principal\nComponent Analysis Autoencoder (PCAAE). This is a novel autoencoder whose\nlatent space verifies two properties. Firstly, the dimensions are organised in\ndecreasing importance with respect to the data at hand. Secondly, the\ncomponents of the latent space are statistically independent. We achieve this\nby progressively increasing the latent space during training, and with a\ncovariance loss applied to the latent codes. The resulting autoencoder produces\na latent space which separates the intrinsic attributes of the data into\ndifferent components of the latent space, in a completely unsupervised manner.\nWe also describe an extension of our approach to the case of powerful,\npre-trained GANs. We show results on both synthetic examples of shapes and on a\nstate-of-the-art GAN. For example, we are able to separate the color shade\nscale of hair and skin, pose of faces and the gender in the CelebA, without\naccessing any labels. We compare the PCAAE with other state-of-the-art\napproaches, in particular with respect to the ability to disentangle attributes\nin the latent space. We hope that this approach will contribute to better\nunderstanding of the intrinsic latent spaces of powerful deep generative\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 07:40:45 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Pham", "Chi-Hieu", ""], ["Ladjal", "Sa\u00efd", ""], ["Newson", "Alasdair", ""]]}, {"id": "2006.07828", "submitter": "Puneet Mangla", "authors": "Puneet Mangla, Vedant Singh, Vineeth N Balasubramanian", "title": "On Saliency Maps and Adversarial Robustness", "comments": "Accepted at ECML-PKDD 2020, Acknowledgements added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Very recent trend has emerged to couple the notion of interpretability and\nadversarial robustness, unlike earlier efforts which solely focused on good\ninterpretations or robustness against adversaries. Works have shown that\nadversarially trained models exhibit more interpretable saliency maps than\ntheir non-robust counterparts, and that this behavior can be quantified by\nconsidering the alignment between input image and saliency map. In this work,\nwe provide a different perspective to this coupling, and provide a method,\nSaliency based Adversarial training (SAT), to use saliency maps to improve\nadversarial robustness of a model. In particular, we show that using\nannotations such as bounding boxes and segmentation masks, already provided\nwith a dataset, as weak saliency maps, suffices to improve adversarial\nrobustness with no additional effort to generate the perturbations themselves.\nOur empirical results on CIFAR-10, CIFAR-100, Tiny ImageNet and Flower-17\ndatasets consistently corroborate our claim, by showing improved adversarial\nrobustness using our method. saliency maps. We also show how using finer and\nstronger saliency maps leads to more robust models, and how integrating SAT\nwith existing adversarial training methods, further boosts performance of these\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 07:41:53 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 15:02:59 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Mangla", "Puneet", ""], ["Singh", "Vedant", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2006.07834", "submitter": "Kuangqi Zhou", "authors": "Kuangqi Zhou, Qibin Hou, Zun Li, Jiashi Feng", "title": "Multi-Miner: Object-Adaptive Region Mining for Weakly-Supervised\n  Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object region mining is a critical step for weakly-supervised semantic\nsegmentation. Most recent methods mine the object regions by expanding the seed\nregions localized by class activation maps. They generally do not consider the\nsizes of objects and apply a monotonous procedure to mining all the object\nregions. Thus their mined regions are often insufficient in number and scale\nfor large objects, and on the other hand easily contaminated by surrounding\nbackgrounds for small objects. In this paper, we propose a novel multi-miner\nframework to perform a region mining process that adapts to diverse object\nsizes and is thus able to mine more integral and finer object regions.\nSpecifically, our multi-miner leverages a parallel modulator to check whether\nthere are remaining object regions for each single object, and guide a\ncategory-aware generator to mine the regions of each object independently. In\nthis way, the multi-miner adaptively takes more steps for large objects and\nfewer steps for small objects. Experiment results demonstrate that the\nmulti-miner offers better region mining results and helps achieve better\nsegmentation performance than state-of-the-art weakly-supervised semantic\nsegmentation methods.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 08:00:42 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhou", "Kuangqi", ""], ["Hou", "Qibin", ""], ["Li", "Zun", ""], ["Feng", "Jiashi", ""]]}, {"id": "2006.07839", "submitter": "Da Chen", "authors": "Da Chen, Jack Spencer, Jean-Marie Mirebeau, Ke Chen, Minglei Shu and\n  Laurent D. Cohen", "title": "A Generalized Asymmetric Dual-front Model for Active Contours and Image\n  Segmentation", "comments": "Published in IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2021.3078102", "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Voronoi diagram-based dual-front active contour models are known as a\npowerful and efficient way for addressing the image segmentation and domain\npartitioning problems. In the basic formulation of the dual-front models, the\nevolving contours can be considered as the interfaces of adjacent Voronoi\nregions. Among these dual-front models, a crucial ingredient is regarded as the\ngeodesic metrics by which the geodesic distances and the corresponding Voronoi\ndiagram can be estimated. In this paper, we introduce a type of asymmetric\nquadratic metrics dual-front model. The metrics considered are built by the\nintegration of the image features and a vector field derived from the evolving\ncontours. The use of the asymmetry enhancement can reduce the risk of contour\nshortcut or leakage problems especially when the initial contours are far away\nfrom the target boundaries or the images have complicated intensity\ndistributions. Moreover, the proposed dual-front model can be applied for image\nsegmentation in conjunction with various region-based homogeneity terms. The\nnumerical experiments on both synthetic and real images show that the proposed\ndual-front model indeed achieves encouraging results.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 08:24:01 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 13:25:28 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 09:19:18 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Chen", "Da", ""], ["Spencer", "Jack", ""], ["Mirebeau", "Jean-Marie", ""], ["Chen", "Ke", ""], ["Shu", "Minglei", ""], ["Cohen", "Laurent D.", ""]]}, {"id": "2006.07845", "submitter": "Prithviraj Dhar", "authors": "Prithviraj Dhar, Joshua Gleason, Hossein Souri, Carlos D. Castillo,\n  Rama Chellappa", "title": "Towards Gender-Neutral Face Descriptors for Mitigating Bias in Face\n  Recognition", "comments": "Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep networks implicitly encode gender information while\nbeing trained for face recognition. Gender is often viewed as an important\nattribute with respect to identifying faces. However, the implicit encoding of\ngender information in face descriptors has two major issues: (a.) It makes the\ndescriptors susceptible to privacy leakage, i.e. a malicious agent can be\ntrained to predict the face gender from such descriptors. (b.) It appears to\ncontribute to gender bias in face recognition, i.e. we find a significant\ndifference in the recognition accuracy of DCNNs on male and female faces.\nTherefore, we present a novel `Adversarial Gender De-biasing algorithm\n(AGENDA)' to reduce the gender information present in face descriptors obtained\nfrom previously trained face recognition networks. We show that AGENDA\nsignificantly reduces gender predictability of face descriptors. Consequently,\nwe are also able to reduce gender bias in face verification while maintaining\nreasonable recognition performance.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 08:54:03 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 06:25:31 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Dhar", "Prithviraj", ""], ["Gleason", "Joshua", ""], ["Souri", "Hossein", ""], ["Castillo", "Carlos D.", ""], ["Chellappa", "Rama", ""]]}, {"id": "2006.07853", "submitter": "Danilo Vasconcellos  Vargas", "authors": "Danilo Vasconcellos Vargas and Toshitake Asabuki", "title": "Continual General Chunking Problem and SyncMap", "comments": null, "journal-ref": "AAAI2021", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans possess an inherent ability to chunk sequences into their constituent\nparts. In fact, this ability is thought to bootstrap language skills and\nlearning of image patterns which might be a key to a more animal-like type of\nintelligence. Here, we propose a continual generalization of the chunking\nproblem (an unsupervised problem), encompassing fixed and probabilistic chunks,\ndiscovery of temporal and causal structures and their continual variations.\nAdditionally, we propose an algorithm called SyncMap that can learn and adapt\nto changes in the problem by creating a dynamic map which preserves the\ncorrelation between variables. Results of SyncMap suggest that the proposed\nalgorithm learn near optimal solutions, despite the presence of many types of\nstructures and their continual variation. When compared to Word2vec, PARSER and\nMRIL, SyncMap surpasses or ties with the best algorithm on $66\\%$ of the\nscenarios while being the second best in the remaining $34\\%$. SyncMap's\nmodel-free simple dynamics and the absence of loss functions reveal that,\nperhaps surprisingly, much can be done with self-organization alone. Code\navailable at https://github.com/zweifel/SyncMap.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 09:39:56 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 02:00:10 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 07:17:42 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 09:40:22 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Vargas", "Danilo Vasconcellos", ""], ["Asabuki", "Toshitake", ""]]}, {"id": "2006.07864", "submitter": "Nils G\\\"ahlert", "authors": "Nils G\\\"ahlert, Nicolas Jourdan, Marius Cordts, Uwe Franke, Joachim\n  Denzler", "title": "Cityscapes 3D: Dataset and Benchmark for 9 DoF Vehicle Detection", "comments": "2020 \"Scalability in Autonomous Driving\" CVPR Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting vehicles and representing their position and orientation in the\nthree dimensional space is a key technology for autonomous driving. Recently,\nmethods for 3D vehicle detection solely based on monocular RGB images gained\npopularity. In order to facilitate this task as well as to compare and drive\nstate-of-the-art methods, several new datasets and benchmarks have been\npublished. Ground truth annotations of vehicles are usually obtained using\nlidar point clouds, which often induces errors due to imperfect calibration or\nsynchronization between both sensors. To this end, we propose Cityscapes 3D,\nextending the original Cityscapes dataset with 3D bounding box annotations for\nall types of vehicles. In contrast to existing datasets, our 3D annotations\nwere labeled using stereo RGB images only and capture all nine degrees of\nfreedom. This leads to a pixel-accurate reprojection in the RGB image and a\nhigher range of annotations compared to lidar-based approaches. In order to\nease multitask learning, we provide a pairing of 2D instance segments with 3D\nbounding boxes. In addition, we complement the Cityscapes benchmark suite with\n3D vehicle detection based on the new annotations as well as metrics presented\nin this work. Dataset and benchmark are available online.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 10:56:27 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["G\u00e4hlert", "Nils", ""], ["Jourdan", "Nicolas", ""], ["Cordts", "Marius", ""], ["Franke", "Uwe", ""], ["Denzler", "Joachim", ""]]}, {"id": "2006.07872", "submitter": "Andong Tan", "authors": "Andong Tan, Duc Tam Nguyen, Maximilian Dax, Matthias Nie{\\ss}ner,\n  Thomas Brox", "title": "Explicitly Modeled Attention Maps for Image Classification", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-attention networks have shown remarkable progress in computer vision\ntasks such as image classification. The main benefit of the self-attention\nmechanism is the ability to capture long-range feature interactions in\nattention-maps. However, the computation of attention-maps requires a learnable\nkey, query, and positional encoding, whose usage is often not intuitive and\ncomputationally expensive. To mitigate this problem, we propose a novel\nself-attention module with explicitly modeled attention-maps using only a\nsingle learnable parameter for low computational overhead. The design of\nexplicitly modeled attention-maps using geometric prior is based on the\nobservation that the spatial context for a given pixel within an image is\nmostly dominated by its neighbors, while more distant pixels have a minor\ncontribution. Concretely, the attention-maps are parametrized via simple\nfunctions (e.g., Gaussian kernel) with a learnable radius, which is modeled\nindependently of the input content. Our evaluation shows that our method\nachieves an accuracy improvement of up to 2.2% over the ResNet-baselines in\nImageNet ILSVRC and outperforms other self-attention methods such as\nAA-ResNet152 in accuracy by 0.9% with 6.4% fewer parameters and 6.7% fewer\nGFLOPs. This result empirically indicates the value of incorporating geometric\nprior into self-attention mechanism when applied in image classification.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 11:47:09 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 14:18:57 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Tan", "Andong", ""], ["Nguyen", "Duc Tam", ""], ["Dax", "Maximilian", ""], ["Nie\u00dfner", "Matthias", ""], ["Brox", "Thomas", ""]]}, {"id": "2006.07877", "submitter": "Pu Li", "authors": "Pu Li, Xiangyang Li, Xiang Long", "title": "FenceMask: A Data Augmentation Approach for Pre-extracted Image Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel data augmentation method named 'FenceMask' that exhibits\noutstanding performance in various computer vision tasks. It is based on the\n'simulation of object occlusion' strategy, which aim to achieve the balance\nbetween object occlusion and information retention of the input data. By\nenhancing the sparsity and regularity of the occlusion block, our augmentation\nmethod overcome the difficulty of small object augmentation and notably improve\nperformance over baselines. Sufficient experiments prove the performance of our\nmethod is better than other simulate object occlusion approaches. We tested it\non CIFAR10, CIFAR100 and ImageNet datasets for Coarse-grained classification,\nCOCO2017 and VisDrone datasets for detection, Oxford Flowers, Cornel Leaf and\nStanford Dogs datasets for Fine-Grained Visual Categorization. Our method\nachieved significant performance improvement on Fine-Grained Visual\nCategorization task and VisDrone dataset.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 12:16:16 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Li", "Pu", ""], ["Li", "Xiangyang", ""], ["Long", "Xiang", ""]]}, {"id": "2006.07885", "submitter": "Elona Shatri Miss", "authors": "Elona Shatri and Gy\\\"orgy Fazekas", "title": "Optical Music Recognition: State of the Art and Major Challenges", "comments": "Author manuscript for TENOR 2020 conference. 11 pages with\n  references, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optical Music Recognition (OMR) is concerned with transcribing sheet music\ninto a machine-readable format. The transcribed copy should allow musicians to\ncompose, play and edit music by taking a picture of a music sheet. Complete\ntranscription of sheet music would also enable more efficient archival. OMR\nfacilitates examining sheet music statistically or searching for patterns of\nnotations, thus helping use cases in digital musicology too. Recently, there\nhas been a shift in OMR from using conventional computer vision techniques\ntowards a deep learning approach. In this paper, we review relevant works in\nOMR, including fundamental methods and significant outcomes, and highlight\ndifferent stages of the OMR pipeline. These stages often lack standard input\nand output representation and standardised evaluation. Therefore, comparing\ndifferent approaches and evaluating the impact of different processing methods\ncan become rather complex. This paper provides recommendations for future work,\naddressing some of the highlighted issues and represents a position in\nfurthering this important field of research.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 12:40:17 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 16:33:59 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Shatri", "Elona", ""], ["Fazekas", "Gy\u00f6rgy", ""]]}, {"id": "2006.07896", "submitter": "Yuqing Song", "authors": "Yuqing Song, Shizhe Chen, Yida Zhao, Qin Jin", "title": "Team RUC_AIM3 Technical Report at Activitynet 2020 Task 2: Exploring\n  Sequential Events Detection for Dense Video Captioning", "comments": "Winner solution in CVPR 2020 Activitynet Dense Video Captioning\n  challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting meaningful events in an untrimmed video is essential for dense\nvideo captioning. In this work, we propose a novel and simple model for event\nsequence generation and explore temporal relationships of the event sequence in\nthe video. The proposed model omits inefficient two-stage proposal generation\nand directly generates event boundaries conditioned on bi-directional temporal\ndependency in one pass. Experimental results show that the proposed event\nsequence generation model can generate more accurate and diverse events within\na small number of proposals. For the event captioning, we follow our previous\nwork to employ the intra-event captioning models into our pipeline system. The\noverall system achieves state-of-the-art performance on the dense-captioning\nevents in video task with 9.894 METEOR score on the challenge testing set.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 13:21:37 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Song", "Yuqing", ""], ["Chen", "Shizhe", ""], ["Zhao", "Yida", ""], ["Jin", "Qin", ""]]}, {"id": "2006.07909", "submitter": "Selvan Sunitha Ravi", "authors": "Anumeha Agrawal, Rosa Anil George, Selvan Sunitha Ravi, Sowmya Kamath\n  S, Anand Kumar M", "title": "Leveraging Multimodal Behavioral Analytics for Automated Job Interview\n  Performance Assessment and Feedback", "comments": "9 pages, ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Behavioral cues play a significant part in human communication and cognitive\nperception. In most professional domains, employee recruitment policies are\nframed such that both professional skills and personality traits are adequately\nassessed. Hiring interviews are structured to evaluate expansively a potential\nemployee's suitability for the position - their professional qualifications,\ninterpersonal skills, ability to perform in critical and stressful situations,\nin the presence of time and resource constraints, etc. Therefore, candidates\nneed to be aware of their positive and negative attributes and be mindful of\nbehavioral cues that might have adverse effects on their success. We propose a\nmultimodal analytical framework that analyzes the candidate in an interview\nscenario and provides feedback for predefined labels such as engagement,\nspeaking rate, eye contact, etc. We perform a comprehensive analysis that\nincludes the interviewee's facial expressions, speech, and prosodic\ninformation, using the video, audio, and text transcripts obtained from the\nrecorded interview. We use these multimodal data sources to construct a\ncomposite representation, which is used for training machine learning\nclassifiers to predict the class labels. Such analysis is then used to provide\nconstructive feedback to the interviewee for their behavioral cues and body\nlanguage. Experimental validation showed that the proposed methodology achieved\npromising results.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 14:20:42 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 14:18:05 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Agrawal", "Anumeha", ""], ["George", "Rosa Anil", ""], ["Ravi", "Selvan Sunitha", ""], ["S", "Sowmya Kamath", ""], ["M", "Anand Kumar", ""]]}, {"id": "2006.07920", "submitter": "Renato J Cintra", "authors": "H. M. de Oliveira, V. V. Vermehren, R. J. Cintra", "title": "Multidimensional Wavelets for Scalable Image Decomposition: Orbital\n  Wavelets", "comments": "Fixed typo in Figure 2(b) and Figure 3(b); 9 pages, 4 figures.\n  International Journal of Wavelets, Multiresolution and Information\n  Processing, 2020", "journal-ref": "ijwmip vol.18, issue N 05, 2020", "doi": "10.1142/S0219691320500381", "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wavelets are closely related to the Schr\\\"odinger's wave functions and the\ninterpretation of Born. Similarly to the appearance of atomic orbital, it is\nproposed to combine anti-symmetric wavelets into orbital wavelets. The proposed\napproach allows the increase of the dimension of wavelets through this process.\nNew orbital 2D-wavelets are introduced for the decomposition of still images,\nshowing that it is possible to perform an analysis simultaneous in two distinct\nscales. An example of such an image analysis is shown.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 14:55:55 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["de Oliveira", "H. M.", ""], ["Vermehren", "V. V.", ""], ["Cintra", "R. J.", ""]]}, {"id": "2006.07965", "submitter": "Ryuichiro Hataya", "authors": "Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, Hideki Nakayama", "title": "Meta Approach to Data Augmentation Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation policies drastically improve the performance of image\nrecognition tasks, especially when the policies are optimized for the target\ndata and tasks. In this paper, we propose to optimize image recognition models\nand data augmentation policies simultaneously to improve the performance using\ngradient descent. Unlike prior methods, our approach avoids using proxy tasks\nor reducing search space, and can directly improve the validation performance.\nOur method achieves efficient and scalable training by approximating the\ngradient of policies by implicit gradient with Neumann series approximation. We\ndemonstrate that our approach can improve the performance of various image\nclassification tasks, including ImageNet classification and fine-grained\nrecognition, without using dataset-specific hyperparameter tuning.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 18:11:52 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Hataya", "Ryuichiro", ""], ["Zdenek", "Jan", ""], ["Yoshizoe", "Kazuki", ""], ["Nakayama", "Hideki", ""]]}, {"id": "2006.07976", "submitter": "Junting Pan", "authors": "Junting Pan, Siyu Chen, Mike Zheng Shou, Yu Liu, Jing Shao, Hongsheng\n  Li", "title": "Actor-Context-Actor Relation Network for Spatio-Temporal Action\n  Localization", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing persons and recognizing their actions from videos is a challenging\ntask towards high-level video understanding. Recent advances have been achieved\nby modeling direct pairwise relations between entities. In this paper, we take\none step further, not only model direct relations between pairs but also take\ninto account indirect higher-order relations established upon multiple\nelements. We propose to explicitly model the Actor-Context-Actor Relation,\nwhich is the relation between two actors based on their interactions with the\ncontext. To this end, we design an Actor-Context-Actor Relation Network\n(ACAR-Net) which builds upon a novel High-order Relation Reasoning Operator and\nan Actor-Context Feature Bank to enable indirect relation reasoning for\nspatio-temporal action localization. Experiments on AVA and UCF101-24 datasets\nshow the advantages of modeling actor-context-actor relations, and\nvisualization of attention maps further verifies that our model is capable of\nfinding relevant higher-order relations to support action detection. Notably,\nour method ranks first in the AVA-Kineticsaction localization task of\nActivityNet Challenge 2020, out-performing other entries by a significant\nmargin (+6.71mAP). Training code and models will be available at\nhttps://github.com/Siyu-C/ACAR-Net.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 18:51:49 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 19:12:35 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 20:30:27 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Pan", "Junting", ""], ["Chen", "Siyu", ""], ["Shou", "Mike Zheng", ""], ["Liu", "Yu", ""], ["Shao", "Jing", ""], ["Li", "Hongsheng", ""]]}, {"id": "2006.07981", "submitter": "Ziyun Wang", "authors": "Ziyun Wang, Eric A. Mitchell, Volkan Isler, Daniel D. Lee", "title": "Geodesic-HOF: 3D Reconstruction Without Cutting Corners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-view 3D object reconstruction is a challenging fundamental problem in\ncomputer vision, largely due to the morphological diversity of objects in the\nnatural world. In particular, high curvature regions are not always captured\neffectively by methods trained using only set-based loss functions, resulting\nin reconstructions short-circuiting the surface or cutting corners. In\nparticular, high curvature regions are not always captured effectively by\nmethods trained using only set-based loss functions, resulting in\nreconstructions short-circuiting the surface or cutting corners. To address\nthis issue, we propose learning an image-conditioned mapping function from a\ncanonical sampling domain to a high dimensional space where the Euclidean\ndistance is equal to the geodesic distance on the object. The first three\ndimensions of a mapped sample correspond to its 3D coordinates. The additional\nlifted components contain information about the underlying geodesic structure.\nOur results show that taking advantage of these learned lifted coordinates\nyields better performance for estimating surface normals and generating\nsurfaces than using point cloud reconstructions alone. Further, we find that\nthis learned geodesic embedding space provides useful information for\napplications such as unsupervised object decomposition.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 18:59:06 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Wang", "Ziyun", ""], ["Mitchell", "Eric A.", ""], ["Isler", "Volkan", ""], ["Lee", "Daniel D.", ""]]}, {"id": "2006.07982", "submitter": "Chiyu Jiang", "authors": "Chiyu \"Max\" Jiang, Jingwei Huang, Andrea Tagliasacchi, Leonidas Guibas", "title": "ShapeFlow: Learnable Deformations Among 3D Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ShapeFlow, a flow-based model for learning a deformation space for\nentire classes of 3D shapes with large intra-class variations. ShapeFlow allows\nlearning a multi-template deformation space that is agnostic to shape topology,\nyet preserves fine geometric details. Different from a generative space where a\nlatent vector is directly decoded into a shape, a deformation space decodes a\nvector into a continuous flow that can advect a source shape towards a target.\nSuch a space naturally allows the disentanglement of geometric style (coming\nfrom the source) and structural pose (conforming to the target). We parametrize\nthe deformation between geometries as a learned continuous flow field via a\nneural network and show that such deformations can be guaranteed to have\ndesirable properties, such as be bijectivity, freedom from self-intersections,\nor volume preservation. We illustrate the effectiveness of this learned\ndeformation space for various downstream applications, including shape\ngeneration via deformation, geometric style transfer, unsupervised learning of\na consistent parameterization for entire classes of shapes, and shape\ninterpolation.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 19:03:35 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 22:06:18 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Jiang", "Chiyu \"Max\"", ""], ["Huang", "Jingwei", ""], ["Tagliasacchi", "Andrea", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2006.07989", "submitter": "Taojiannan Yang", "authors": "Taojiannan Yang, Sijie Zhu, Chen Chen", "title": "GradAug: A New Regularization Method for Deep Neural Networks", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new regularization method to alleviate over-fitting in deep\nneural networks. The key idea is utilizing randomly transformed training\nsamples to regularize a set of sub-networks, which are originated by sampling\nthe width of the original network, in the training process. As such, the\nproposed method introduces self-guided disturbances to the raw gradients of the\nnetwork and therefore is termed as Gradient Augmentation (GradAug). We\ndemonstrate that GradAug can help the network learn well-generalized and more\ndiverse representations. Moreover, it is easy to implement and can be applied\nto various structures and applications. GradAug improves ResNet-50 to 78.79% on\nImageNet classification, which is a new state-of-the-art accuracy. By combining\nwith CutMix, it further boosts the performance to 79.67%, which outperforms an\nensemble of advanced training tricks. The generalization ability is evaluated\non COCO object detection and instance segmentation where GradAug significantly\nsurpasses other state-of-the-art methods. GradAug is also robust to image\ndistortions and FGSM adversarial attacks and is highly effective in low data\nregimes. Code is available at https://github.com/taoyang1122/GradAug\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 19:30:34 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 18:20:51 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Yang", "Taojiannan", ""], ["Zhu", "Sijie", ""], ["Chen", "Chen", ""]]}, {"id": "2006.07991", "submitter": "Arturo Deza", "authors": "Arturo Deza and Talia Konkle", "title": "Emergent Properties of Foveated Perceptual Systems", "comments": "An updated pre-print. Currently under review at NeurIPS 2021. Themes:\n  Foveation, Machine Perception & Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to characterize the representational impact that\nfoveation operations have for machine vision systems, inspired by the foveated\nhuman visual system, which has higher acuity at the center of gaze and\ntexture-like encoding in the periphery. To do so, we introduce models\nconsisting of a first-stage \\textit{fixed} image transform followed by a\nsecond-stage \\textit{learnable} convolutional neural network, and we varied the\nfirst stage component. The primary model has a foveated-textural input stage,\nwhich we compare to a model with foveated-blurred input and a model with\nspatially-uniform blurred input (both matched for perceptual compression), and\na final reference model with minimal input-based compression. We find that: 1)\nthe foveated-texture model shows similar scene classification accuracy as the\nreference model despite its compressed input, with greater i.i.d.\ngeneralization than the other models; 2) the foveated-texture model has greater\nsensitivity to high-spatial frequency information and greater robustness to\nocclusion, w.r.t the comparison models; 3) both the foveated systems, show a\nstronger center image-bias relative to the spatially-uniform systems even with\na weight sharing constraint. Critically, these results are preserved over\ndifferent classical CNN architectures throughout their learning dynamics.\nAltogether, this suggests that foveation with peripheral texture-based\ncomputations yields an efficient, distinct, and robust representational format\nof scene information, and provides symbiotic computational insight into the\nrepresentational consequences that texture-based peripheral encoding may have\nfor processing in the human visual system, while also potentially inspiring the\nnext generation of computer vision models via spatially-adaptive computation.\nCode + Data available here: https://github.com/ArturoDeza/EmergentProperties\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 19:34:44 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 20:23:00 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 21:21:08 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Deza", "Arturo", ""], ["Konkle", "Talia", ""]]}, {"id": "2006.07993", "submitter": "Benjamin Choi", "authors": "John Kamalu, Benjamin Choi", "title": "Road Mapping in Low Data Environments with OpenStreetMap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roads are among the most essential components of any country's\ninfrastructure. By facilitating the movement and exchange of people, ideas, and\ngoods, they support economic and cultural activity both within and across local\nand international borders. A comprehensive, up-to-date mapping of the\ngeographical distribution of roads and their quality thus has the potential to\nact as an indicator for broader economic development. Such an indicator has a\nvariety of high-impact applications, particularly in the planning of rural\ndevelopment projects where up-to-date infrastructure information is not\navailable. This work investigates the viability of high resolution satellite\nimagery and crowd-sourced resources like OpenStreetMap in the construction of\nsuch a mapping. We experiment with state-of-the-art deep learning methods to\nexplore the utility of OpenStreetMap data in road classification and\nsegmentation tasks. We also compare the performance of models in different mask\nocclusion scenarios as well as out-of-country domains. Our comparison raises\nimportant pitfalls to consider in image-based infrastructure classification\ntasks, and shows the need for local training data specific to regions of\ninterest for reliable performance.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 19:39:57 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Kamalu", "John", ""], ["Choi", "Benjamin", ""]]}, {"id": "2006.07995", "submitter": "Jesper Christensen", "authors": "Jesper Haahr Christensen, Sascha Hornauer, Stella Yu", "title": "BatVision with GCC-PHAT Features for Better Sound to Vision Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by sophisticated echolocation abilities found in nature, we train a\ngenerative adversarial network to predict plausible depth maps and grayscale\nlayouts from sound. To achieve this, our sound-to-vision model processes\nbinaural echo-returns from chirping sounds. We build upon previous work with\nBatVision that consists of a sound-to-vision model and a self-collected dataset\nusing our mobile robot and low-cost hardware. We improve on the previous model\nby introducing several changes to the model, which leads to a better depth and\ngrayscale estimation, and increased perceptual quality. Rather than using raw\nbinaural waveforms as input, we generate generalized cross-correlation (GCC)\nfeatures and use these as input instead. In addition, we change the model\ngenerator and base it on residual learning and use spectral normalization in\nthe discriminator. We compare and present both quantitative and qualitative\nimprovements over our previous BatVision model.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 19:49:58 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Christensen", "Jesper Haahr", ""], ["Hornauer", "Sascha", ""], ["Yu", "Stella", ""]]}, {"id": "2006.08003", "submitter": "Shubham Dash", "authors": "Suraj Kiran Raman (1), Aditya Ramesh (1), Vijayakrishna Naganoor (1),\n  Shubham Dash (1), Giridharan Kumaravelu (1), Honglak Lee (1) ((1) University\n  of Michigan, Ann Arbor)", "title": "CompressNet: Generative Compression at Extremely Low Bitrates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressing images at extremely low bitrates (< 0.1 bpp) has always been a\nchallenging task since the quality of reconstruction significantly reduces due\nto the strong imposed constraint on the number of bits allocated for the\ncompressed data. With the increasing need to transfer large amounts of images\nwith limited bandwidth, compressing images to very low sizes is a crucial task.\nHowever, the existing methods are not effective at extremely low bitrates. To\naddress this need, we propose a novel network called CompressNet which augments\na Stacked Autoencoder with a Switch Prediction Network (SAE-SPN). This helps in\nthe reconstruction of visually pleasing images at these low bitrates (< 0.1\nbpp). We benchmark the performance of our proposed method on the Cityscapes\ndataset, evaluating over different metrics at extremely low bitrates to show\nthat our method outperforms the other state-of-the-art. In particular, at a\nbitrate of 0.07, CompressNet achieves 22% lower Perceptual Loss and 55% lower\nFrechet Inception Distance (FID) compared to the deep learning SOTA methods.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 20:03:12 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Raman", "Suraj Kiran", ""], ["Ramesh", "Aditya", ""], ["Naganoor", "Vijayakrishna", ""], ["Dash", "Shubham", ""], ["Kumaravelu", "Giridharan", ""], ["Lee", "Honglak", ""]]}, {"id": "2006.08021", "submitter": "Armin Hadzic", "authors": "Armin Hadzic, Hunter Blanton, Weilian Song, Mei Chen, Scott Workman,\n  Nathan Jacobs", "title": "RasterNet: Modeling Free-Flow Speed using LiDAR and Overhead Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roadway free-flow speed captures the typical vehicle speed in low traffic\nconditions. Modeling free-flow speed is an important problem in transportation\nengineering with applications to a variety of design, operation, planning, and\npolicy decisions of highway systems. Unfortunately, collecting large-scale\nhistorical traffic speed data is expensive and time consuming. Traditional\napproaches for estimating free-flow speed use geometric properties of the\nunderlying road segment, such as grade, curvature, lane width, lateral\nclearance and access point density, but for many roads such features are\nunavailable. We propose a fully automated approach, RasterNet, for estimating\nfree-flow speed without the need for explicit geometric features. RasterNet is\na neural network that fuses large-scale overhead imagery and aerial LiDAR point\nclouds using a geospatially consistent raster structure. To support training\nand evaluation, we introduce a novel dataset combining free-flow speeds of road\nsegments, overhead imagery, and LiDAR point clouds across the state of\nKentucky. Our method achieves state-of-the-art results on a benchmark dataset.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 21:03:52 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Hadzic", "Armin", ""], ["Blanton", "Hunter", ""], ["Song", "Weilian", ""], ["Chen", "Mei", ""], ["Workman", "Scott", ""], ["Jacobs", "Nathan", ""]]}, {"id": "2006.08070", "submitter": "Xianhang Cheng", "authors": "Xianhang Cheng and Zhenzhong Chen", "title": "Multiple Video Frame Interpolation via Enhanced Deformable Separable\n  Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating non-existing frames from a consecutive video sequence has been an\ninteresting and challenging problem in the video processing field. Typical\nkernel-based interpolation methods predict pixels with a single convolution\nprocess that convolves source frames with spatially adaptive local kernels,\nwhich circumvents the time-consuming, explicit motion estimation in the form of\noptical flow. However, when scene motion is larger than the pre-defined kernel\nsize, these methods are prone to yield less plausible results. In addition,\nthey cannot directly generate a frame at an arbitrary temporal position because\nthe learned kernels are tied to the midpoint in time between the input frames.\nIn this paper, we try to solve these problems and propose a novel non-flow\nkernel-based approach that we refer to as enhanced deformable separable\nconvolution (EDSC) to estimate not only adaptive kernels, but also offsets,\nmasks and biases to make the network obtain information from non-local\nneighborhood. During the learning process, different intermediate time step can\nbe involved as a control variable by means of an extension of coord-conv trick,\nallowing the estimated components to vary with different input temporal\ninformation. This makes our method capable to produce multiple in-between\nframes. Furthermore, we investigate the relationships between our method and\nother typical kernel- and flow-based methods. Experimental results show that\nour method performs favorably against the state-of-the-art methods across a\nbroad range of datasets. Code will be publicly available on URL:\n\\url{https://github.com/Xianhang/EDSC-pytorch}.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 01:10:59 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 09:10:57 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Cheng", "Xianhang", ""], ["Chen", "Zhenzhong", ""]]}, {"id": "2006.08072", "submitter": "Tong He", "authors": "Tong He, John Collomosse, Hailin Jin, Stefano Soatto", "title": "Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view\n  Human Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Geo-PIFu, a method to recover a 3D mesh from a monocular color\nimage of a clothed person. Our method is based on a deep implicit\nfunction-based representation to learn latent voxel features using a\nstructure-aware 3D U-Net, to constrain the model in two ways: first, to resolve\nfeature ambiguities in query point encoding, second, to serve as a coarse human\nshape proxy to regularize the high-resolution mesh and encourage global shape\nregularity. We show that, by both encoding query points and constraining global\nshape using latent voxel features, the reconstruction we obtain for clothed\nhuman meshes exhibits less shape distortion and improved surface details\ncompared to competing methods. We evaluate Geo-PIFu on a recent human mesh\npublic dataset that is $10 \\times$ larger than the private commercial dataset\nused in PIFu and previous derivative work. On average, we exceed the state of\nthe art by $42.7\\%$ reduction in Chamfer and Point-to-Surface Distances, and\n$19.4\\%$ reduction in normal estimation errors.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 01:11:48 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 00:20:35 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["He", "Tong", ""], ["Collomosse", "John", ""], ["Jin", "Hailin", ""], ["Soatto", "Stefano", ""]]}, {"id": "2006.08089", "submitter": "Homanga Bharadhwaj", "authors": "Yatin Dandi, Homanga Bharadhwaj, Abhishek Kumar, Piyush Rai", "title": "Generalized Adversarially Learned Inference", "comments": "AAAI 2021 (accepted for publication)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allowing effective inference of latent vectors while training GANs can\ngreatly increase their applicability in various downstream tasks. Recent\napproaches, such as ALI and BiGAN frameworks, develop methods of inference of\nlatent variables in GANs by adversarially training an image generator along\nwith an encoder to match two joint distributions of image and latent vector\npairs. We generalize these approaches to incorporate multiple layers of\nfeedback on reconstructions, self-supervision, and other forms of supervision\nbased on prior or learned knowledge about the desired solutions. We achieve\nthis by modifying the discriminator's objective to correctly identify more than\ntwo joint distributions of tuples of an arbitrary number of random variables\nconsisting of images, latent vectors, and other variables generated through\nauxiliary tasks, such as reconstruction and inpainting or as outputs of\nsuitable pre-trained models. We design a non-saturating maximization objective\nfor the generator-encoder pair and prove that the resulting adversarial game\ncorresponds to a global optimum that simultaneously matches all the\ndistributions. Within our proposed framework, we introduce a novel set of\ntechniques for providing self-supervised feedback to the model based on\nproperties, such as patch-level correspondence and cycle consistency of\nreconstructions. Through comprehensive experiments, we demonstrate the\nefficacy, scalability, and flexibility of the proposed approach for a variety\nof tasks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 02:18:13 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 22:28:20 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 15:34:08 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Dandi", "Yatin", ""], ["Bharadhwaj", "Homanga", ""], ["Kumar", "Abhishek", ""], ["Rai", "Piyush", ""]]}, {"id": "2006.08129", "submitter": "Mandeep Singh", "authors": "Mandeep Singh and Yuan Fang", "title": "Emotion Recognition in Audio and Video Using Deep Neural Networks", "comments": "9 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are able to comprehend information from multiple domains for e.g.\nspeech, text and visual. With advancement of deep learning technology there has\nbeen significant improvement of speech recognition. Recognizing emotion from\nspeech is important aspect and with deep learning technology emotion\nrecognition has improved in accuracy and latency. There are still many\nchallenges to improve accuracy. In this work, we attempt to explore different\nneural networks to improve accuracy of emotion recognition. With different\narchitectures explored, we find (CNN+RNN) + 3DCNN multi-model architecture\nwhich processes audio spectrograms and corresponding video frames giving\nemotion prediction accuracy of 54.0% among 4 emotions and 71.75% among 3\nemotions using IEMOCAP[2] dataset.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 04:50:18 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Singh", "Mandeep", ""], ["Fang", "Yuan", ""]]}, {"id": "2006.08143", "submitter": "Harpreet Singh", "authors": "Harpreet Singh, Emily M. Hand, Kostas Alexis", "title": "Anomalous Motion Detection on Highway Using Deep Learning", "comments": "to be published in IEEE ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in visual anomaly detection draws much interest due to its\napplications in surveillance. Common datasets for evaluation are constructed\nusing a stationary camera overlooking a region of interest. Previous research\nhas shown promising results in detecting spatial as well as temporal anomalies\nin these settings. The advent of self-driving cars provides an opportunity to\napply visual anomaly detection in a more dynamic application yet no dataset\nexists in this type of environment. This paper presents a new anomaly detection\ndataset - the Highway Traffic Anomaly (HTA) dataset - for the problem of\ndetecting anomalous traffic patterns from dash cam videos of vehicles on\nhighways. We evaluate state-of-the-art deep learning anomaly detection models\nand propose novel variations to these methods. Our results show that\nstate-of-the-art models built for settings with a stationary camera do not\ntranslate well to a more dynamic environment. The proposed variations to these\nSoTA methods show promising results on the new HTA dataset.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 05:40:11 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Singh", "Harpreet", ""], ["Hand", "Emily M.", ""], ["Alexis", "Kostas", ""]]}, {"id": "2006.08145", "submitter": "Kazuki Endo", "authors": "Kazuki Endo, Masayuki Tanaka, Masatoshi Okutomi", "title": "Classifying degraded images over various levels of degradation", "comments": "Accepted by the 27th IEEE International Conference on Image\n  Processing (ICIP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification for degraded images having various levels of degradation is\nvery important in practical applications. This paper proposes a convolutional\nneural network to classify degraded images by using a restoration network and\nan ensemble learning. The results demonstrate that the proposed network can\nclassify degraded images over various levels of degradation well. This paper\nalso reveals how the image-quality of training data for a classification\nnetwork affects the classification performance of degraded images.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 05:43:07 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Endo", "Kazuki", ""], ["Tanaka", "Masayuki", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "2006.08159", "submitter": "Yang Wang", "authors": "Yang Wang", "title": "Survey on Deep Multi-modal Data Analytics: Collaboration, Rivalry and\n  Fusion", "comments": "Appearing at ACM TOMM, 26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of web technology, multi-modal or multi-view data has\nsurged as a major stream for big data, where each modal/view encodes individual\nproperty of data objects. Often, different modalities are complementary to each\nother. Such fact motivated a lot of research attention on fusing the\nmulti-modal feature spaces to comprehensively characterize the data objects.\nMost of the existing state-of-the-art focused on how to fuse the energy or\ninformation from multi-modal spaces to deliver a superior performance over\ntheir counterparts with single modal. Recently, deep neural networks have\nexhibited as a powerful architecture to well capture the nonlinear distribution\nof high-dimensional multimedia data, so naturally does for multi-modal data.\nSubstantial empirical studies are carried out to demonstrate its advantages\nthat are benefited from deep multi-modal methods, which can essentially deepen\nthe fusion from multi-modal deep feature spaces. In this paper, we provide a\nsubstantial overview of the existing state-of-the-arts on the filed of\nmulti-modal data analytics from shallow to deep spaces. Throughout this survey,\nwe further indicate that the critical components for this field go to\ncollaboration, adversarial competition and fusion over multi-modal spaces.\nFinally, we share our viewpoints regarding some future directions on this\nfield.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 06:42:04 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Wang", "Yang", ""]]}, {"id": "2006.08162", "submitter": "Erdem Akag\\\"und\\\"uz", "authors": "H. Se\\c{c}kin Demir and Erdem Akagunduz", "title": "Filter design for small target detection on infrared imagery using\n  normalized-cross-correlation layer", "comments": null, "journal-ref": "published in Turkish Journal of Electrical Engineering and\n  Computer Sciences, vol.28, 302:317, 2020", "doi": "10.3906/elk-1807-287", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a machine learning approach to the problem of\ninfrared small target detection filter design. For this purpose, similarly to a\nconvolutional layer of a neural network, the normalized-cross-correlational\n(NCC) layer, which we utilize for designing a target detection/recognition\nfilter bank, is proposed. By employing the NCC layer in a neural network\nstructure, we introduce a framework, in which supervised training is used to\ncalculate the optimal filter shape and the optimum number of filters required\nfor a specific target detection/recognition task on infrared images. We also\npropose the mean-absolute-deviation NCC (MAD-NCC) layer, an efficient\nimplementation of the proposed NCC layer, designed especially for FPGA systems,\nin which square root operations are avoided for real-time computation. As a\ncase study we work on dim-target detection on mid-wave infrared imagery and\nobtain the filters that can discriminate a dim target from various types of\nbackground clutter, specific to our operational concept.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 06:46:08 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Demir", "H. Se\u00e7kin", ""], ["Akagunduz", "Erdem", ""]]}, {"id": "2006.08173", "submitter": "Brian Chmiel", "authors": "Brian Chmiel, Liad Ben-Uri, Moran Shkolnik, Elad Hoffer, Ron Banner,\n  Daniel Soudry", "title": "Neural gradients are near-lognormal: improved quantized and sparse\n  training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While training can mostly be accelerated by reducing the time needed to\npropagate neural gradients back throughout the model, most previous works focus\non the quantization/pruning of weights and activations. These methods are often\nnot applicable to neural gradients, which have very different statistical\nproperties. Distinguished from weights and activations, we find that the\ndistribution of neural gradients is approximately lognormal. Considering this,\nwe suggest two closed-form analytical methods to reduce the computational and\nmemory burdens of neural gradients. The first method optimizes the\nfloating-point format and scale of the gradients. The second method accurately\nsets sparsity thresholds for gradient pruning. Each method achieves\nstate-of-the-art results on ImageNet. To the best of our knowledge, this paper\nis the first to (1) quantize the gradients to 6-bit floating-point formats, or\n(2) achieve up to 85% gradient sparsity -- in each case without accuracy\ndegradation. Reference implementation accompanies the paper.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 07:00:15 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 15:02:50 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 14:18:25 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chmiel", "Brian", ""], ["Ben-Uri", "Liad", ""], ["Shkolnik", "Moran", ""], ["Hoffer", "Elad", ""], ["Banner", "Ron", ""], ["Soudry", "Daniel", ""]]}, {"id": "2006.08177", "submitter": "Juan Sebasti\\'an Lara Ram\\'irez", "authors": "Juan S. Lara, Fabio A. Gonz\\'alez", "title": "Dissimilarity Mixture Autoencoder for Deep Clustering", "comments": "20 pages (2.5 pages of references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dissimilarity mixture autoencoder (DMAE) is a neural network model for\nfeature-based clustering that incorporates a flexible dissimilarity function\nand can be integrated into any kind of deep learning architecture. It\ninternally represents a dissimilarity mixture model (DMM) that extends\nclassical methods like K-Means, Gaussian mixture models, or Bregman clustering\nto any convex and differentiable dissimilarity function through the\nreinterpretation of probabilities as neural network representations. DMAE can\nbe integrated with deep learning architectures into end-to-end models, allowing\nthe simultaneous estimation of the clustering and neural network's parameters.\nExperimental evaluation was performed on image and text clustering benchmark\ndatasets showing that DMAE is competitive in terms of unsupervised\nclassification accuracy and normalized mutual information. The source code with\nthe implementation of DMAE is publicly available at:\nhttps://github.com/juselara1/dmae\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 07:08:59 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 08:16:47 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 10:15:23 GMT"}, {"version": "v4", "created": "Thu, 15 Jul 2021 16:33:02 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Lara", "Juan S.", ""], ["Gonz\u00e1lez", "Fabio A.", ""]]}, {"id": "2006.08178", "submitter": "Manoj Rohit Vemparla", "authors": "Alexander Frickenstein and Manoj Rohit Vemparala and Jakob Mayr and\n  Naveen Shankar Nagaraja and Christian Unger and Federico Tombari and Walter\n  Stechele", "title": "Binary DAD-Net: Binarized Driveable Area Detection Network for\n  Autonomous Driving", "comments": "IEEE International Conference on Robotics and Automation (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driveable area detection is a key component for various applications in the\nfield of autonomous driving (AD), such as ground-plane detection, obstacle\ndetection and maneuver planning. Additionally, bulky and over-parameterized\nnetworks can be easily forgone and replaced with smaller networks for faster\ninference on embedded systems. The driveable area detection, posed as a two\nclass segmentation task, can be efficiently modeled with slim binary networks.\nThis paper proposes a novel binarized driveable area detection network (binary\nDAD-Net), which uses only binary weights and activations in the encoder, the\nbottleneck, and the decoder part. The latent space of the bottleneck is\nefficiently increased (x32 -> x16 downsampling) through binary dilated\nconvolutions, learning more complex features. Along with automatically\ngenerated training data, the binary DAD-Net outperforms state-of-the-art\nsemantic segmentation networks on public datasets. In comparison to a\nfull-precision model, our approach has a x14.3 reduced compute complexity on an\nFPGA and it requires only 0.9MB memory resources. Therefore, commodity\nSIMD-based AD-hardware is capable of accelerating the binary DAD-Net.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 07:09:01 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Frickenstein", "Alexander", ""], ["Vemparala", "Manoj Rohit", ""], ["Mayr", "Jakob", ""], ["Nagaraja", "Naveen Shankar", ""], ["Unger", "Christian", ""], ["Tombari", "Federico", ""], ["Stechele", "Walter", ""]]}, {"id": "2006.08184", "submitter": "Giorgio Roffo", "authors": "Giorgio Roffo, Simone Melzi, Umberto Castellani, Alessandro\n  Vinciarelli, Marco Cristani", "title": "Infinite Feature Selection: A Graph-based Feature Filtering Approach", "comments": "TPAMI PREPRINT 2020", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (2020),", "doi": "10.1109/TPAMI.2020.3002843", "report-no": "TPAMI-2019-08-0679.R1", "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a filtering feature selection framework that considers subsets of\nfeatures as paths in a graph, where a node is a feature and an edge indicates\npairwise (customizable) relations among features, dealing with relevance and\nredundancy principles. By two different interpretations (exploiting properties\nof power series of matrices and relying on Markov chains fundamentals) we can\nevaluate the values of paths (i.e., feature subsets) of arbitrary lengths,\neventually go to infinite, from which we dub our framework Infinite Feature\nSelection (Inf-FS). Going to infinite allows to constrain the computational\ncomplexity of the selection process, and to rank the features in an elegant\nway, that is, considering the value of any path (subset) containing a\nparticular feature. We also propose a simple unsupervised strategy to cut the\nranking, so providing the subset of features to keep. In the experiments, we\nanalyze diverse settings with heterogeneous features, for a total of 11\nbenchmarks, comparing against 18 widely-known comparative approaches. The\nresults show that Inf-FS behaves better in almost any situation, that is, when\nthe number of features to keep are fixed a priori, or when the decision of the\nsubset cardinality is part of the process.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 07:20:40 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Roffo", "Giorgio", ""], ["Melzi", "Simone", ""], ["Castellani", "Umberto", ""], ["Vinciarelli", "Alessandro", ""], ["Cristani", "Marco", ""]]}, {"id": "2006.08198", "submitter": "Yonggan Fu", "authors": "Yonggan Fu, Wuyang Chen, Haotao Wang, Haoran Li, Yingyan Lin,\n  Zhangyang Wang", "title": "AutoGAN-Distiller: Searching to Compress Generative Adversarial Networks", "comments": "Accepted at ICML2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The compression of Generative Adversarial Networks (GANs) has lately drawn\nattention, due to the increasing demand for deploying GANs into mobile devices\nfor numerous applications such as image translation, enhancement and editing.\nHowever, compared to the substantial efforts to compressing other deep models,\nthe research on compressing GANs (usually the generators) remains at its\ninfancy stage. Existing GAN compression algorithms are limited to handling\nspecific GAN architectures and losses. Inspired by the recent success of AutoML\nin deep compression, we introduce AutoML to GAN compression and develop an\nAutoGAN-Distiller (AGD) framework. Starting with a specifically designed\nefficient search space, AGD performs an end-to-end discovery for new efficient\ngenerators, given the target computational resource constraints. The search is\nguided by the original GAN model via knowledge distillation, therefore\nfulfilling the compression. AGD is fully automatic, standalone (i.e., needing\nno trained discriminators), and generically applicable to various GAN models.\nWe evaluate AGD in two representative GAN tasks: image translation and super\nresolution. Without bells and whistles, AGD yields remarkably lightweight yet\nmore competitive compressed models, that largely outperform existing\nalternatives. Our codes and pretrained models are available at\nhttps://github.com/TAMU-VITA/AGD.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 07:56:24 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 15:41:44 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Fu", "Yonggan", ""], ["Chen", "Wuyang", ""], ["Wang", "Haotao", ""], ["Li", "Haoran", ""], ["Lin", "Yingyan", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2006.08217", "submitter": "Byeongho Heo", "authors": "Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun,\n  Gyuwan Kim, Youngjung Uh, Jung-Woo Ha", "title": "AdamP: Slowing Down the Slowdown for Momentum Optimizers on\n  Scale-invariant Weights", "comments": "Accepted at ICLR 2021. First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization techniques are a boon for modern deep learning. They let\nweights converge more quickly with often better generalization performances. It\nhas been argued that the normalization-induced scale invariance among the\nweights provides an advantageous ground for gradient descent (GD) optimizers:\nthe effective step sizes are automatically reduced over time, stabilizing the\noverall training procedure. It is often overlooked, however, that the\nadditional introduction of momentum in GD optimizers results in a far more\nrapid reduction in effective step sizes for scale-invariant weights, a\nphenomenon that has not yet been studied and may have caused unwanted side\neffects in the current practice. This is a crucial issue because arguably the\nvast majority of modern deep neural networks consist of (1) momentum-based GD\n(e.g. SGD or Adam) and (2) scale-invariant parameters. In this paper, we verify\nthat the widely-adopted combination of the two ingredients lead to the\npremature decay of effective step sizes and sub-optimal model performances. We\npropose a simple and effective remedy, SGDP and AdamP: get rid of the radial\ncomponent, or the norm-increasing direction, at each optimizer step. Because of\nthe scale invariance, this modification only alters the effective step sizes\nwithout changing the effective update directions, thus enjoying the original\nconvergence properties of GD optimizers. Given the ubiquity of momentum GD and\nscale invariance in machine learning, we have evaluated our methods against the\nbaselines on 13 benchmarks. They range from vision tasks like classification\n(e.g. ImageNet), retrieval (e.g. CUB and SOP), and detection (e.g. COCO) to\nlanguage modelling (e.g. WikiText) and audio classification (e.g. DCASE) tasks.\nWe verify that our solution brings about uniform gains in those benchmarks.\nSource code is available at https://github.com/clovaai/AdamP.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 08:35:15 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 04:20:54 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 14:36:15 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Heo", "Byeongho", ""], ["Chun", "Sanghyuk", ""], ["Oh", "Seong Joon", ""], ["Han", "Dongyoon", ""], ["Yun", "Sangdoo", ""], ["Kim", "Gyuwan", ""], ["Uh", "Youngjung", ""], ["Ha", "Jung-Woo", ""]]}, {"id": "2006.08231", "submitter": "Heung-Chang Lee", "authors": "Do-Guk Kim, Heung-Chang Lee", "title": "Differentiable Neural Architecture Transformation for Reproducible\n  Architecture Improvement", "comments": null, "journal-ref": "CVPR2020-NAS Workshop", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Neural Architecture Search (NAS) methods are introduced and show\nimpressive performance on many benchmarks. Among those NAS studies, Neural\nArchitecture Transformer (NAT) aims to improve the given neural architecture to\nhave better performance while maintaining computational costs. However, NAT has\nlimitations about a lack of reproducibility. In this paper, we propose\ndifferentiable neural architecture transformation that is reproducible and\nefficient. The proposed method shows stable performance on various\narchitectures. Extensive reproducibility experiments on two datasets, i.e.,\nCIFAR-10 and Tiny Imagenet, present that the proposed method definitely\noutperforms NAT and be applicable to other models and datasets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 09:03:48 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Kim", "Do-Guk", ""], ["Lee", "Heung-Chang", ""]]}, {"id": "2006.08247", "submitter": "Alexandros Stergiou MSc", "authors": "Alexandros Stergiou and Ronald Poppe", "title": "Learn to cycle: Time-consistent feature discovery for action recognition", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2020.11.012", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalizing over temporal variations is a prerequisite for effective action\nrecognition in videos. Despite significant advances in deep neural networks, it\nremains a challenge to focus on short-term discriminative motions in relation\nto the overall performance of an action. We address this challenge by allowing\nsome flexibility in discovering relevant spatio-temporal features. We introduce\nSqueeze and Recursion Temporal Gates (SRTG), an approach that favors inputs\nwith similar activations with potential temporal variations. We implement this\nidea with a novel CNN block that uses an LSTM to encapsulate feature dynamics,\nin conjunction with a temporal gate that is responsible for evaluating the\nconsistency of the discovered dynamics and the modeled features. We show\nconsistent improvement when using SRTG blocks, with only a minimal increase in\nthe number of GFLOPs. On Kinetics-700, we perform on par with current\nstate-of-the-art models, and outperform these on HACS, Moments in Time, UCF-101\nand HMDB-51.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 09:36:28 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 14:06:36 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Stergiou", "Alexandros", ""], ["Poppe", "Ronald", ""]]}, {"id": "2006.08254", "submitter": "Kaushil Mangaroliya", "authors": "Kaushil Mangaroliya and Mitt Shah", "title": "Dermatologist vs Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer, in general, is very deadly. Timely treatment of any cancer is the key\nto saving a life. Skin cancer is no exception. There have been thousands of\nSkin Cancer cases registered per year all over the world. There have been\n123,000 deadly melanoma cases detected in a single year. This huge number is\nproven to be a cause of a high amount of UV rays present in the sunlight due to\nthe degradation of the Ozone layer. If not detected at an early stage, skin\ncancer can lead to the death of the patient. Unavailability of proper resources\nsuch as expert dermatologists, state of the art testing facilities, and quick\nbiopsy results have led researchers to develop a technology that can solve the\nabove problem. Deep Learning is one such method that has offered extraordinary\nresults. The Convolutional Neural Network proposed in this study out performs\nevery pretrained models. We trained our model on the HAM10000 dataset which\noffers 10015 images belonging to 7 classes of skin disease. The model we\nproposed gave an accuracy of 89%. This model can predict deadly melanoma skin\ncancer with a great accuracy. Hopefully, this study can help save people's life\nwhere there is the unavailability of proper dermatological resources by\nbridging the gap using our proposed study.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 09:44:57 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Mangaroliya", "Kaushil", ""], ["Shah", "Mitt", ""]]}, {"id": "2006.08264", "submitter": "Wentong Liao", "authors": "Hao Cheng, Wentong Liao, Michael Ying Yang, Bodo Rosenhahn, Monika\n  Sester", "title": "AMENet: Attentive Maps Encoder Network for Trajectory Prediction", "comments": "Accepted by ISPRS Journal of Photogrammetry and Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory prediction is critical for applications of planning safe future\nmovements and remains challenging even for the next few seconds in urban mixed\ntraffic. How an agent moves is affected by the various behaviors of its\nneighboring agents in different environments. To predict movements, we propose\nan end-to-end generative model named Attentive Maps Encoder Network (AMENet)\nthat encodes the agent's motion and interaction information for accurate and\nrealistic multi-path trajectory prediction. A conditional variational\nauto-encoder module is trained to learn the latent space of possible future\npaths based on attentive dynamic maps for interaction modeling and then is used\nto predict multiple plausible future trajectories conditioned on the observed\npast trajectories. The efficacy of AMENet is validated using two public\ntrajectory prediction benchmarks Trajnet and InD.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 10:00:07 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 15:57:30 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Cheng", "Hao", ""], ["Liao", "Wentong", ""], ["Yang", "Michael Ying", ""], ["Rosenhahn", "Bodo", ""], ["Sester", "Monika", ""]]}, {"id": "2006.08296", "submitter": "Mahdi Rezaei", "authors": "Zahra Noury and Mahdi Rezaei", "title": "Deep-CAPTCHA: a deep learning based CAPTCHA solver for vulnerability\n  assessment", "comments": "Version 2.0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CAPTCHA is a human-centred test to distinguish a human operator from bots,\nattacking programs, or other computerised agents that tries to imitate human\nintelligence. In this research, we investigate a way to crack visual CAPTCHA\ntests by an automated deep learning based solution. The goal of this research\nis to investigate the weaknesses and vulnerabilities of the CAPTCHA generator\nsystems; hence, developing more robust CAPTCHAs, without taking the risks of\nmanual try and fail efforts. We develop a Convolutional Neural Network called\nDeep-CAPTCHA to achieve this goal. The proposed platform is able to investigate\nboth numerical and alphanumerical CAPTCHAs. To train and develop an efficient\nmodel, we have generated a dataset of 500,000 CAPTCHAs to train our model. In\nthis paper, we present our customised deep neural network model, we review the\nresearch gaps, the existing challenges, and the solutions to cope with the\nissues. Our network's cracking accuracy leads to a high rate of 98.94% and\n98.31% for the numerical and the alpha-numerical test datasets, respectively.\nThat means more works is required to develop robust CAPTCHAs, to be\nnon-crackable against automated artificial agents. As the outcome of this\nresearch, we identify some efficient techniques to improve the security of the\nCAPTCHAs, based on the performance analysis conducted on the Deep-CAPTCHA\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 11:44:43 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 19:55:33 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Noury", "Zahra", ""], ["Rezaei", "Mahdi", ""]]}, {"id": "2006.08315", "submitter": "Ruixiang Tang", "authors": "Ruixiang Tang, Mengnan Du, Yuening Li, Zirui Liu, Na Zou, Xia Hu", "title": "Mitigating Gender Bias in Captioning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning has made substantial progress with huge supporting image\ncollections sourced from the web. However, recent studies have pointed out that\ncaptioning datasets, such as COCO, contain gender bias found in web corpora. As\na result, learning models could heavily rely on the learned priors and image\ncontext for gender identification, leading to incorrect or even offensive\nerrors. To encourage models to learn correct gender features, we reorganize the\nCOCO dataset and present two new splits COCO-GB V1 and V2 datasets where the\ntrain and test sets have different gender-context joint distribution. Models\nrelying on contextual cues will suffer from huge gender prediction errors on\nthe anti-stereotypical test data. Benchmarking experiments reveal that most\ncaptioning models learn gender bias, leading to high gender prediction errors,\nespecially for women. To alleviate the unwanted bias, we propose a new Guided\nAttention Image Captioning model (GAIC) which provides self-guidance on visual\nattention to encourage the model to capture correct gender visual evidence.\nExperimental results validate that GAIC can significantly reduce gender\nprediction errors with a competitive caption quality. Our codes and the\ndesigned benchmark datasets are available at\nhttps://github.com/datamllab/Mitigating_Gender_Bias_In_Captioning_System.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 12:16:19 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 21:27:12 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2021 22:01:37 GMT"}, {"version": "v4", "created": "Tue, 19 Jan 2021 15:49:55 GMT"}, {"version": "v5", "created": "Fri, 22 Jan 2021 03:09:07 GMT"}, {"version": "v6", "created": "Mon, 15 Feb 2021 08:21:06 GMT"}, {"version": "v7", "created": "Tue, 20 Apr 2021 21:48:29 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Tang", "Ruixiang", ""], ["Du", "Mengnan", ""], ["Li", "Yuening", ""], ["Liu", "Zirui", ""], ["Zou", "Na", ""], ["Hu", "Xia", ""]]}, {"id": "2006.08321", "submitter": "Yi\\u{g}it Oktar", "authors": "Yigit Oktar, Mehmet Turkan", "title": "On the Preservation of Spatio-temporal Information in Machine Learning\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conventional machine learning applications, each data attribute is assumed\nto be orthogonal to others. Namely, every pair of dimension is orthogonal to\neach other and thus there is no distinction of in-between relations of\ndimensions. However, this is certainly not the case in real world signals which\nnaturally originate from a spatio-temporal configuration. As a result, the\nconventional vectorization process disrupts all of the spatio-temporal\ninformation about the order/place of data whether it be $1$D, $2$D, $3$D, or\n$4$D. In this paper, the problem of orthogonality is first investigated through\nconventional $k$-means of images, where images are to be processed as vectors.\nAs a solution, shift-invariant $k$-means is proposed in a novel framework with\nthe help of sparse representations. A generalization of shift-invariant\n$k$-means, convolutional dictionary learning, is then utilized as an\nunsupervised feature extraction method for classification. Experiments suggest\nthat Gabor feature extraction as a simulation of shallow convolutional neural\nnetworks provides a little better performance compared to convolutional\ndictionary learning. Many alternatives of convolutional-logic are also\ndiscussed for spatio-temporal information preservation, including a\nspatio-temporal hypercomplex encoding scheme.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 12:22:36 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Oktar", "Yigit", ""], ["Turkan", "Mehmet", ""]]}, {"id": "2006.08322", "submitter": "Ziwei Wang", "authors": "Ziwei Wang, Zi Huang, Yadan Luo, Huimin Lu", "title": "ORD: Object Relationship Discovery for Visual Dialogue Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid advancement of image captioning and visual question answering\nat single-round level, the question of how to generate multi-round dialogue\nabout visual content has not yet been well explored.Existing visual dialogue\nmethods encode the image into a fixed feature vector directly, concatenated\nwith the question and history embeddings to predict the response.Some recent\nmethods tackle the co-reference resolution problem using co-attention mechanism\nto cross-refer relevant elements from the image, history, and the target\nquestion.However, it remains challenging to reason visual relationships, since\nthe fine-grained object-level information is omitted before co-attentive\nreasoning. In this paper, we propose an object relationship discovery (ORD)\nframework to preserve the object interactions for visual dialogue generation.\nSpecifically, a hierarchical graph convolutional network (HierGCN) is proposed\nto retain the object nodes and neighbour relationships locally, and then\nrefines the object-object connections globally to obtain the final graph\nembeddings. A graph attention is further incorporated to dynamically attend to\nthis graph-structured representation at the response reasoning stage. Extensive\nexperiments have proved that the proposed method can significantly improve the\nquality of dialogue by utilising the contextual information of visual\nrelationships. The model achieves superior performance over the\nstate-of-the-art methods on the Visual Dialog dataset, increasing MRR from\n0.6222 to 0.6447, and recall@1 from 48.48% to 51.22%.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 12:25:40 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Wang", "Ziwei", ""], ["Huang", "Zi", ""], ["Luo", "Yadan", ""], ["Lu", "Huimin", ""]]}, {"id": "2006.08335", "submitter": "Bofan Xue", "authors": "Bofan Xue, David Chan, John Canny", "title": "A Dataset and Benchmarks for Multimedia Social Analysis", "comments": "Published as a workshop paper at \"Multimodality Learning\" (CVPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new publicly available dataset with the goal of advancing\nmulti-modality learning by offering vision and language data within the same\ncontext. This is achieved by obtaining data from a social media website with\nposts containing multiple paired images/videos and text, along with comment\ntrees containing images/videos and/or text. With a total of 677k posts, 2.9\nmillion post images, 488k post videos, 1.4 million comment images, 4.6 million\ncomment videos, and 96.9 million comments, data from different modalities can\nbe jointly used to improve performances for a variety of tasks such as image\ncaptioning, image classification, next frame prediction, sentiment analysis,\nand language modeling. We present a wide range of statistics for our dataset.\nFinally, we provide baseline performance analysis for one of the regression\ntasks using pre-trained models and several fully connected networks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 11:33:01 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Xue", "Bofan", ""], ["Chan", "David", ""], ["Canny", "John", ""]]}, {"id": "2006.08346", "submitter": "Hadi Shafiee Dr.", "authors": "Manoj Kumar Kanakasabapathy, Prudhvi Thirumalaraju, Charles L Bormann,\n  Raghav Gupta, Rohan Pooniwala, Hemanth Kandula, Irene Souter, Irene\n  Dimitriadis, Hadi Shafiee", "title": "Deep learning mediated single time-point image-based prediction of\n  embryo developmental outcome at the cleavage stage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In conventional clinical in-vitro fertilization practices embryos are\ntransferred either at the cleavage or blastocyst stages of development.\nCleavage stage transfers, particularly, are beneficial for patients with\nrelatively poor prognosis and at fertility centers in resource-limited settings\nwhere there is a higher chance of developmental failure in embryos in-vitro.\nHowever, one of the major limitations of embryo selections at the cleavage\nstage is the availability of very low number of manually discernable features\nto predict developmental outcomes. Although, time-lapse imaging systems have\nbeen proposed as possible solutions, they are cost-prohibitive and require\nbulky and expensive hardware, and labor-intensive. Advances in convolutional\nneural networks (CNNs) have been utilized to provide accurate classifications\nacross many medical and non-medical object categories. Here, we report an\nautomated system for classification and selection of human embryos at the\ncleavage stage using a trained CNN combined with a genetic algorithm. The\nsystem selected the cleavage stage embryo at 70 hours post insemination (hpi)\nthat ultimately developed into top-quality blastocyst at 70 hpi with 64%\naccuracy, outperforming the abilities of embryologists in identifying embryos\nwith the highest developmental potential. Such systems can have a significant\nimpact on IVF procedures by empowering embryologists for accurate and\nconsistent embryo assessment in both resource-poor and resource-rich settings.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 21:21:15 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Kanakasabapathy", "Manoj Kumar", ""], ["Thirumalaraju", "Prudhvi", ""], ["Bormann", "Charles L", ""], ["Gupta", "Raghav", ""], ["Pooniwala", "Rohan", ""], ["Kandula", "Hemanth", ""], ["Souter", "Irene", ""], ["Dimitriadis", "Irene", ""], ["Shafiee", "Hadi", ""]]}, {"id": "2006.08357", "submitter": "Zhen Dong", "authors": "Zhen Dong, Dequan Wang, Qijing Huang, Yizhao Gao, Yaohui Cai, Tian Li,\n  Bichen Wu, Kurt Keutzer, John Wawrzynek", "title": "CoDeNet: Efficient Deployment of Input-Adaptive Object Detection on\n  Embedded FPGAs", "comments": "Github repo: https://github.com/DequanWang/CoDeNet arXiv:2002.08357\n  is the preliminary version of this paper", "journal-ref": "FPGA 2021", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying deep learning models on embedded systems has been challenging due\nto limited computing resources. The majority of existing work focuses on\naccelerating image classification, while other fundamental vision problems,\nsuch as object detection, have not been adequately addressed. Compared with\nimage classification, detection problems are more sensitive to the spatial\nvariance of objects, and therefore, require specialized convolutions to\naggregate spatial information. To address this need, recent work introduces\ndynamic deformable convolution to augment regular convolutions. However, this\nwill lead to inefficient memory accesses of inputs with existing hardware. In\nthis work, we harness the flexibility of FPGAs to develop a novel object\ndetection pipeline with deformable convolutions. We show the speed-accuracy\ntradeoffs for a set of algorithm modifications including irregular-access\nversus limited-range and fixed-shape. We then Co-Design a Network CoDeNet with\nthe modified deformable convolution and quantize it to 4-bit weights and 8-bit\nactivations. With our high-efficiency implementation, our solution reaches 26.9\nframes per second with a tiny model size of 0.76 MB while achieving 61.7 AP50\non the standard object detection dataset, Pascal VOC. With our higher accuracy\nimplementation, our model gets to 67.1 AP50 on Pascal VOC with only 2.9 MB of\nparameters-20.9x smaller but 10% more accurate than Tiny-YOLO.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 17:56:47 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 22:35:57 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Dong", "Zhen", ""], ["Wang", "Dequan", ""], ["Huang", "Qijing", ""], ["Gao", "Yizhao", ""], ["Cai", "Yaohui", ""], ["Li", "Tian", ""], ["Wu", "Bichen", ""], ["Keutzer", "Kurt", ""], ["Wawrzynek", "John", ""]]}, {"id": "2006.08367", "submitter": "Muthiah Annamalai", "authors": "Muthiah Annamalai", "title": "Tamil Vowel Recognition With Augmented MNIST-like Data Set", "comments": "8 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We report generation of a MNIST [4] compatible data set [1] for Tamil vowels\nto enable building a classification DNN or other such ML/AI deep learning [2]\nmodels for Tamil OCR/Handwriting applications. We report the capability of the\n60,000 grayscale, 28x28 pixel dataset to build a 92% accuracy (training) and\n82% cross-validation 4-layer CNN, with 100,000+ parameters, in TensorFlow. We\nalso report a top-1 classification accuracy of 70% and top-2 classification\naccuracy of 92% on handwritten vowels showing, for the same network.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 19:17:30 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 19:20:09 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Annamalai", "Muthiah", ""]]}, {"id": "2006.08376", "submitter": "Hong Huy Nguyen", "authors": "Huy H. Nguyen, Junichi Yamagishi, Isao Echizen, S\\'ebastien Marcel", "title": "Generating Master Faces for Use in Performing Wolf Attacks on Face\n  Recognition Systems", "comments": "Accepted to be Published in Proceedings of the 2020 International\n  Joint Conference on Biometrics (IJCB 2020), Houston, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its convenience, biometric authentication, especial face\nauthentication, has become increasingly mainstream and thus is now a prime\ntarget for attackers. Presentation attacks and face morphing are typical types\nof attack. Previous research has shown that finger-vein- and fingerprint-based\nauthentication methods are susceptible to wolf attacks, in which a wolf sample\nmatches many enrolled user templates. In this work, we demonstrated that wolf\n(generic) faces, which we call \"master faces,\" can also compromise face\nrecognition systems and that the master face concept can be generalized in some\ncases. Motivated by recent similar work in the fingerprint domain, we generated\nhigh-quality master faces by using the state-of-the-art face generator StyleGAN\nin a process called latent variable evolution. Experiments demonstrated that\neven attackers with limited resources using only pre-trained models available\non the Internet can initiate master face attacks. The results, in addition to\ndemonstrating performance from the attacker's point of view, can also be used\nto clarify and improve the performance of face recognition systems and harden\nface authentication systems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 12:59:49 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Nguyen", "Huy H.", ""], ["Yamagishi", "Junichi", ""], ["Echizen", "Isao", ""], ["Marcel", "S\u00e9bastien", ""]]}, {"id": "2006.08383", "submitter": "Yongxin Wang", "authors": "Yongxin Wang and Duminda Wijesekera", "title": "Pixel Invisibility: Detecting Objects Invisible in Color Images", "comments": "8 pages, 7 figures, submitted to NIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent success of object detectors using deep neural networks, their\ndeployment on safety-critical applications such as self-driving cars remains\nquestionable. This is partly due to the absence of reliable estimation for\ndetectors' failure under operational conditions such as night, fog, dusk, dawn\nand glare. Such unquantifiable failures could lead to safety violations. In\norder to solve this problem, we created an algorithm that predicts a\npixel-level invisibility map for color images that does not require manual\nlabeling - that computes the probability that a pixel/region contains objects\nthat are invisible in color domain, during various lighting conditions such as\nday, night and fog. We propose a novel use of cross modal knowledge\ndistillation from color to infra-red domain using weakly-aligned image pairs\nfrom the day and construct indicators for the pixel-level invisibility based on\nthe distances of their intermediate-level features. Quantitative experiments\nshow the great performance of our pixel-level invisibility mask and also the\neffectiveness of distilled mid-level features on object detection in infra-red\nimagery.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 13:10:17 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 02:46:52 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Wang", "Yongxin", ""], ["Wijesekera", "Duminda", ""]]}, {"id": "2006.08419", "submitter": "Ruosi Wan", "authors": "Ruosi Wan, Zhanxing Zhu, Xiangyu Zhang, Jian Sun", "title": "Spherical Motion Dynamics: Learning Dynamics of Neural Network with\n  Normalization, Weight Decay, and SGD", "comments": "Theoretical analysis on joint effect of normalization and weight\n  decay", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we comprehensively reveal the learning dynamics of neural\nnetwork with normalization, weight decay (WD), and SGD (with momentum), named\nas Spherical Motion Dynamics (SMD). Most related works study SMD by focusing on\n\"effective learning rate\" in \"equilibrium\" condition, where weight norm remains\nunchanged. However, their discussions on why equilibrium condition can be\nreached in SMD is either absent or less convincing. Our work investigates SMD\nby directly exploring the cause of equilibrium condition. Specifically, 1) we\nintroduce the assumptions that can lead to equilibrium condition in SMD, and\nprove that weight norm can converge at linear rate with given assumptions; 2)\nwe propose \"angular update\" as a substitute for effective learning rate to\nmeasure the evolving of neural network in SMD, and prove angular update can\nalso converge to its theoretical value at linear rate; 3) we verify our\nassumptions and theoretical results on various computer vision tasks including\nImageNet and MSCOCO with standard settings. Experiment results show our\ntheoretical findings agree well with empirical observations.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 14:16:33 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 07:22:03 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 13:09:16 GMT"}, {"version": "v4", "created": "Fri, 27 Nov 2020 06:10:50 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wan", "Ruosi", ""], ["Zhu", "Zhanxing", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Jian", ""]]}, {"id": "2006.08432", "submitter": "Gencer Sumbul", "authors": "Gencer Sumbul, Sonali Nayak, Beg\\\"um Demir", "title": "SD-RSIC: Summarization Driven Deep Remote Sensing Image Captioning", "comments": "Accepted in the IEEE Transactions on Geoscience and Remote Sensing.\n  For code visit: https://gitlab.tubit.tu-berlin.de/rsim/SD-RSIC", "journal-ref": null, "doi": "10.1109/TGRS.2020.3031111", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been recently found popular for image\ncaptioning problems in remote sensing (RS). Existing DNN based approaches rely\non the availability of a training set made up of a high number of RS images\nwith their captions. However, captions of training images may contain redundant\ninformation (they can be repetitive or semantically similar to each other),\nresulting in information deficiency while learning a mapping from the image\ndomain to the language domain. To overcome this limitation, in this paper, we\npresent a novel Summarization Driven Remote Sensing Image Captioning (SD-RSIC)\napproach. The proposed approach consists of three main steps. The first step\nobtains the standard image captions by jointly exploiting convolutional neural\nnetworks (CNNs) with long short-term memory (LSTM) networks. The second step,\nunlike the existing RS image captioning methods, summarizes the ground-truth\ncaptions of each training image into a single caption by exploiting sequence to\nsequence neural networks and eliminates the redundancy present in the training\nset. The third step automatically defines the adaptive weights associated to\neach RS image to combine the standard captions with the summarized captions\nbased on the semantic content of the image. This is achieved by a novel\nadaptive weighting strategy defined in the context of LSTM networks.\nExperimental results obtained on the RSCID, UCM-Captions and Sydney-Captions\ndatasets show the effectiveness of the proposed approach compared to the\nstate-of-the-art RS image captioning approaches. The code of the proposed\napproach is publicly available at\nhttps://gitlab.tubit.tu-berlin.de/rsim/SD-RSIC.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 14:29:12 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 10:09:15 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Sumbul", "Gencer", ""], ["Nayak", "Sonali", ""], ["Demir", "Beg\u00fcm", ""]]}, {"id": "2006.08466", "submitter": "Joakim Bruslund Haurum", "authors": "Malte Pedersen, Joakim Bruslund Haurum, Stefan Hein Bengtson, Thomas\n  B. Moeslund", "title": "3D-ZeF: A 3D Zebrafish Tracking Benchmark Dataset", "comments": "CVPR 2020. Project webpage: https://vap.aau.dk/3d-zef/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a novel publicly available stereo based 3D RGB\ndataset for multi-object zebrafish tracking, called 3D-ZeF. Zebrafish is an\nincreasingly popular model organism used for studying neurological disorders,\ndrug addiction, and more. Behavioral analysis is often a critical part of such\nresearch. However, visual similarity, occlusion, and erratic movement of the\nzebrafish makes robust 3D tracking a challenging and unsolved problem. The\nproposed dataset consists of eight sequences with a duration between 15-120\nseconds and 1-10 free moving zebrafish. The videos have been annotated with a\ntotal of 86,400 points and bounding boxes. Furthermore, we present a complexity\nscore and a novel open-source modular baseline system for 3D tracking of\nzebrafish. The performance of the system is measured with respect to two\ndetectors: a naive approach and a Faster R-CNN based fish head detector. The\nsystem reaches a MOTA of up to 77.6%. Links to the code and dataset is\navailable at the project page https://vap.aau.dk/3d-zef\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 15:15:20 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Pedersen", "Malte", ""], ["Haurum", "Joakim Bruslund", ""], ["Bengtson", "Stefan Hein", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "2006.08470", "submitter": "Florian Wirthm\\\"uller", "authors": "Florian Wirthm\\\"uller, Julian Schlechtriemen, Jochen Hipp, Manfred\n  Reichert", "title": "Towards Incorporating Contextual Knowledge into the Prediction of\n  Driving Behavior", "comments": "the article has been accepted for publication during the 23rd IEEE\n  Intelligent Transportation Systems Conference (ITSC), 7 pages, 6 figures, 1\n  table", "journal-ref": null, "doi": "10.1109/ITSC45102.2020.9294665", "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the behavior of surrounding traffic participants is crucial for\nadvanced driver assistance systems and autonomous driving. Most researchers\nhowever do not consider contextual knowledge when predicting vehicle motion.\nExtending former studies, we investigate how predictions are affected by\nexternal conditions. To do so, we categorize different kinds of contextual\ninformation and provide a carefully chosen definition as well as examples for\nexternal conditions. More precisely, we investigate how a state-of-the-art\napproach for lateral motion prediction is influenced by one selected external\ncondition, namely the traffic density. Our investigations demonstrate that this\nkind of information is highly relevant in order to improve the performance of\nprediction algorithms. Therefore, this study constitutes the first step towards\nthe integration of such information into automated vehicles. Moreover, our\nmotion prediction approach is evaluated based on the public highD data set\nshowing a maneuver prediction performance with areas under the ROC curve above\n97% and a median lateral prediction error of only 0.18m on a prediction horizon\nof 5s.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 15:21:02 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 14:41:43 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Wirthm\u00fcller", "Florian", ""], ["Schlechtriemen", "Julian", ""], ["Hipp", "Jochen", ""], ["Reichert", "Manfred", ""]]}, {"id": "2006.08509", "submitter": "Tianzhe Wang", "authors": "Tianzhe Wang, Kuan Wang, Han Cai, Ji Lin, Zhijian Liu, Song Han", "title": "APQ: Joint Search for Network Architecture, Pruning and Quantization\n  Policy", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present APQ for efficient deep learning inference on resource-constrained\nhardware. Unlike previous methods that separately search the neural\narchitecture, pruning policy, and quantization policy, we optimize them in a\njoint manner. To deal with the larger design space it brings, a promising\napproach is to train a quantization-aware accuracy predictor to quickly get the\naccuracy of the quantized model and feed it to the search engine to select the\nbest fit. However, training this quantization-aware accuracy predictor requires\ncollecting a large number of quantized <model, accuracy> pairs, which involves\nquantization-aware finetuning and thus is highly time-consuming. To tackle this\nchallenge, we propose to transfer the knowledge from a full-precision (i.e.,\nfp32) accuracy predictor to the quantization-aware (i.e., int8) accuracy\npredictor, which greatly improves the sample efficiency. Besides, collecting\nthe dataset for the fp32 accuracy predictor only requires to evaluate neural\nnetworks without any training cost by sampling from a pretrained once-for-all\nnetwork, which is highly efficient. Extensive experiments on ImageNet\ndemonstrate the benefits of our joint optimization approach. With the same\naccuracy, APQ reduces the latency/energy by 2x/1.3x over MobileNetV2+HAQ.\nCompared to the separate optimization approach (ProxylessNAS+AMC+HAQ), APQ\nachieves 2.3% higher ImageNet accuracy while reducing orders of magnitude GPU\nhours and CO2 emission, pushing the frontier for green AI that is\nenvironmental-friendly. The code and video are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 16:09:17 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Wang", "Tianzhe", ""], ["Wang", "Kuan", ""], ["Cai", "Han", ""], ["Lin", "Ji", ""], ["Liu", "Zhijian", ""], ["Han", "Song", ""]]}, {"id": "2006.08517", "submitter": "Yang You", "authors": "Yang You and Yuhui Wang and Huan Zhang and Zhao Zhang and James Demmel\n  and Cho-Jui Hsieh", "title": "The Limit of the Batch Size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-batch training is an efficient approach for current distributed deep\nlearning systems. It has enabled researchers to reduce the ImageNet/ResNet-50\ntraining from 29 hours to around 1 minute. In this paper, we focus on studying\nthe limit of the batch size. We think it may provide a guidance to AI\nsupercomputer and algorithm designers. We provide detailed numerical\noptimization instructions for step-by-step comparison. Moreover, it is\nimportant to understand the generalization and optimization performance of huge\nbatch training. Hoffer et al. introduced \"ultra-slow diffusion\" theory to\nlarge-batch training. However, our experiments show contradictory results with\nthe conclusion of Hoffer et al. We provide comprehensive experimental results\nand detailed analysis to study the limitations of batch size scaling and\n\"ultra-slow diffusion\" theory. For the first time we scale the batch size on\nImageNet to at least a magnitude larger than all previous work, and provide\ndetailed studies on the performance of many state-of-the-art optimization\nschemes under this setting. We propose an optimization recipe that is able to\nimprove the top-1 test accuracy by 18% compared to the baseline.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 16:18:05 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["You", "Yang", ""], ["Wang", "Yuhui", ""], ["Zhang", "Huan", ""], ["Zhang", "Zhao", ""], ["Demmel", "James", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2006.08521", "submitter": "Lukas Stappen", "authors": "Lukas Stappen, Xinchen Du, Vincent Karas, Stefan M\\\"uller, Bj\\\"orn W.\n  Schuller", "title": "Domain Adaptation with Joint Learning for Generic, Optical Car Part\n  Recognition and Detection Systems (Go-CaRD)", "comments": "Demonstration and instructions to obtain data and models:\n  https://github.com/lstappen/GoCarD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems for the automatic recognition and detection of automotive parts are\ncrucial in several emerging research areas in the development of intelligent\nvehicles. They enable, for example, the detection and modelling of interactions\nbetween human and the vehicle. In this paper, we quantitatively and\nqualitatively explore the efficacy of deep learning architectures for the\nclassification and localisation of 29 interior and exterior vehicle regions on\nthree novel datasets. Furthermore, we experiment with joint and transfer\nlearning approaches across datasets and point out potential applications of our\nsystems. Our best network architecture achieves an F1 score of 93.67 % for\nrecognition, while our best localisation approach utilising state-of-the-art\nbackbone networks achieve a mAP of 63.01 % for detection. The MuSe-CAR-Part\ndataset, which is based on a large variety of human-car interactions in videos,\nthe weights of the best models, and the code is publicly available to academic\nparties for benchmarking and future research.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 16:28:53 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 21:23:49 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Stappen", "Lukas", ""], ["Du", "Xinchen", ""], ["Karas", "Vincent", ""], ["M\u00fcller", "Stefan", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "2006.08532", "submitter": "Karren Yang", "authors": "Karren Yang, Samuel Goldman, Wengong Jin, Alex Lu, Regina Barzilay,\n  Tommi Jaakkola, Caroline Uhler", "title": "Improved Conditional Flow Models for Molecule to Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.CV cs.LG eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to synthesize cell microscopy images under different\nmolecular interventions, motivated by practical applications to drug\ndevelopment. Building on the recent success of graph neural networks for\nlearning molecular embeddings and flow-based models for image generation, we\npropose Mol2Image: a flow-based generative model for molecule to cell image\nsynthesis. To generate cell features at different resolutions and scale to\nhigh-resolution images, we develop a novel multi-scale flow architecture based\non a Haar wavelet image pyramid. To maximize the mutual information between the\ngenerated images and the molecular interventions, we devise a training strategy\nbased on contrastive learning. To evaluate our model, we propose a new set of\nmetrics for biological image generation that are robust, interpretable, and\nrelevant to practitioners. We show quantitatively that our method learns a\nmeaningful embedding of the molecular intervention, which is translated into an\nimage representation reflecting the biological effects of the intervention.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 16:39:50 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Yang", "Karren", ""], ["Goldman", "Samuel", ""], ["Jin", "Wengong", ""], ["Lu", "Alex", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""], ["Uhler", "Caroline", ""]]}, {"id": "2006.08538", "submitter": "Yan Feng", "authors": "Yan Feng, Baoyuan Wu, Yanbo Fan, Li Liu, Zhifeng Li, Shutao Xia", "title": "Boosting Black-Box Attack with Partially Transferred Conditional\n  Adversarial Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies black-box adversarial attacks against deep neural networks\n(DNNs), where the attacker can only access the query feedback returned by the\nattacked DNN model, while other information such as model parameters or the\ntraining datasets are unknown. One promising approach to improve attack\nperformance is utilizing the adversarial transferability between some white-box\nsurrogate models and the target model (i.e., the attacked model). However, due\nto the possible differences on model architectures and training datasets\nbetween surrogate and target models, dubbed \"surrogate biases\", the\ncontribution of adversarial transferability to improving the attack performance\nmay be weakened. To tackle this issue, we innovatively propose a black-box\nattack method by developing a novel mechanism of adversarial transferability,\nwhich is robust to the surrogate biases. The general idea is transferring\npartial parameters of the conditional adversarial distribution (CAD) of\nsurrogate models, while learning the untransferred parameters based on queries\nto the target model, to keep the flexibility to adjust the CAD of the target\nmodel on any new benign sample. Extensive experiments on benchmark datasets and\nattacking against real-world API demonstrate the superior attack performance of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 16:45:27 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 02:26:36 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 06:28:19 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 08:56:09 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Feng", "Yan", ""], ["Wu", "Baoyuan", ""], ["Fan", "Yanbo", ""], ["Liu", "Li", ""], ["Li", "Zhifeng", ""], ["Xia", "Shutao", ""]]}, {"id": "2006.08547", "submitter": "Nils G\\\"ahlert", "authors": "Nils G\\\"ahlert, Niklas Hanselmann, Uwe Franke, Joachim Denzler", "title": "Visibility Guided NMS: Efficient Boosting of Amodal Object Detection in\n  Crowded Traffic Scenes", "comments": "Machine Learning for Autonomous Driving Workshop at the 33rd\n  Conference on Neural Information Processing Systems (NeurIPS 2019),\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is an important task in environment perception for\nautonomous driving. Modern 2D object detection frameworks such as Yolo, SSD or\nFaster R-CNN predict multiple bounding boxes per object that are refined using\nNon-Maximum-Suppression (NMS) to suppress all but one bounding box. While\nobject detection itself is fully end-to-end learnable and does not require any\nmanual parameter selection, standard NMS is parametrized by an overlap\nthreshold that has to be chosen by hand. In practice, this often leads to an\ninability of standard NMS strategies to distinguish different objects in\ncrowded scenes in the presence of high mutual occlusion, e.g. for parked cars\nor crowds of pedestrians. Our novel Visibility Guided NMS (vg-NMS) leverages\nboth pixel-based as well as amodal object detection paradigms and improves the\ndetection performance especially for highly occluded objects with little\ncomputational overhead. We evaluate vg-NMS using KITTI, VIPER as well as the\nSynscapes dataset and show that it outperforms current state-of-the-art NMS.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 17:03:23 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["G\u00e4hlert", "Nils", ""], ["Hanselmann", "Niklas", ""], ["Franke", "Uwe", ""], ["Denzler", "Joachim", ""]]}, {"id": "2006.08554", "submitter": "Aditya Rajagopal", "authors": "Aditya Rajagopal, Christos-Savvas Bouganis", "title": "Now that I can see, I can improve: Enabling data-driven finetuning of\n  CNNs on the edge", "comments": "Accepted for publication at CVPR2020 workshop - Efficient Deep\n  Learning for Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's world, a vast amount of data is being generated by edge devices\nthat can be used as valuable training data to improve the performance of\nmachine learning algorithms in terms of the achieved accuracy or to reduce the\ncompute requirements of the model. However, due to user data privacy concerns\nas well as storage and communication bandwidth limitations, this data cannot be\nmoved from the device to the data centre for further improvement of the model\nand subsequent deployment. As such there is a need for increased edge\nintelligence, where the deployed models can be fine-tuned on the edge, leading\nto improved accuracy and/or reducing the model's workload as well as its memory\nand power footprint. In the case of Convolutional Neural Networks (CNNs), both\nthe weights of the network as well as its topology can be tuned to adapt to the\ndata that it processes. This paper provides a first step towards enabling CNN\nfinetuning on an edge device based on structured pruning. It explores the\nperformance gains and costs of doing so and presents an extensible open-source\nframework that allows the deployment of such approaches on a wide range of\nnetwork architectures and devices. The results show that on average, data-aware\npruning with retraining can provide 10.2pp increased accuracy over a wide range\nof subsets, networks and pruning levels with a maximum improvement of 42.0pp\nover pruning and retraining in a manner agnostic to the data being processed by\nthe network.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 17:16:45 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Rajagopal", "Aditya", ""], ["Bouganis", "Christos-Savvas", ""]]}, {"id": "2006.08558", "submitter": "Yaodong Yu", "authors": "Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, Yi Ma", "title": "Learning Diverse and Discriminative Representations via the Principle of\n  Maximal Coding Rate Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To learn intrinsic low-dimensional structures from high-dimensional data that\nmost discriminate between classes, we propose the principle of Maximal Coding\nRate Reduction ($\\text{MCR}^2$), an information-theoretic measure that\nmaximizes the coding rate difference between the whole dataset and the sum of\neach individual class. We clarify its relationships with most existing\nframeworks such as cross-entropy, information bottleneck, information gain,\ncontractive and contrastive learning, and provide theoretical guarantees for\nlearning diverse and discriminative features. The coding rate can be accurately\ncomputed from finite samples of degenerate subspace-like distributions and can\nlearn intrinsic representations in supervised, self-supervised, and\nunsupervised settings in a unified manner. Empirically, the representations\nlearned using this principle alone are significantly more robust to label\ncorruptions in classification than those using cross-entropy, and can lead to\nstate-of-the-art results in clustering mixed data from self-learned invariant\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 17:23:55 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Yu", "Yaodong", ""], ["Chan", "Kwan Ho Ryan", ""], ["You", "Chong", ""], ["Song", "Chaobing", ""], ["Ma", "Yi", ""]]}, {"id": "2006.08565", "submitter": "Kristina Monakhova", "authors": "Kristina Monakhova, Kyrollos Yanny, Neerja Aggarwal, Laura Waller", "title": "Spectral DiffuserCam: lensless snapshot hyperspectral imaging with a\n  spectral filter array", "comments": "10 pages, 10 figures, Optica", "journal-ref": "Optica 7, 1298-1307 (2020)", "doi": "10.1364/OPTICA.397214", "report-no": null, "categories": "eess.IV cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging is useful for applications ranging from medical\ndiagnostics to agricultural crop monitoring; however, traditional scanning\nhyperspectral imagers are prohibitively slow and expensive for widespread\nadoption. Snapshot techniques exist but are often confined to bulky benchtop\nsetups or have low spatio-spectral resolution. In this paper, we propose a\nnovel, compact, and inexpensive computational camera for snapshot hyperspectral\nimaging. Our system consists of a tiled spectral filter array placed directly\non the image sensor and a diffuser placed close to the sensor. Each point in\nthe world maps to a unique pseudorandom pattern on the spectral filter array,\nwhich encodes multiplexed spatio-spectral information. By solving a\nsparsity-constrained inverse problem, we recover the hyperspectral volume with\nsub-super-pixel resolution. Our hyperspectral imaging framework is flexible and\ncan be designed with contiguous or non-contiguous spectral filters that can be\nchosen for a given application. We provide theory for system design,\ndemonstrate a prototype device, and present experimental results with high\nspatio-spectral resolution.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 17:31:17 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 03:21:47 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Monakhova", "Kristina", ""], ["Yanny", "Kyrollos", ""], ["Aggarwal", "Neerja", ""], ["Waller", "Laura", ""]]}, {"id": "2006.08586", "submitter": "Wen Jiang", "authors": "Wen Jiang, Nikos Kolotouros, Georgios Pavlakos, Xiaowei Zhou, Kostas\n  Daniilidis", "title": "Coherent Reconstruction of Multiple Humans from a Single Image", "comments": "CVPR 2020. Project Page: https://jiangwenpl.github.io/multiperson/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of multi-person 3D pose estimation from\na single image. A typical regression approach in the top-down setting of this\nproblem would first detect all humans and then reconstruct each one of them\nindependently. However, this type of prediction suffers from incoherent\nresults, e.g., interpenetration and inconsistent depth ordering between the\npeople in the scene. Our goal is to train a single network that learns to avoid\nthese problems and generate a coherent 3D reconstruction of all the humans in\nthe scene. To this end, a key design choice is the incorporation of the SMPL\nparametric body model in our top-down framework, which enables the use of two\nnovel losses. First, a distance field-based collision loss penalizes\ninterpenetration among the reconstructed people. Second, a depth ordering-aware\nloss reasons about occlusions and promotes a depth ordering of people that\nleads to a rendering which is consistent with the annotated instance\nsegmentation. This provides depth supervision signals to the network, even if\nthe image has no explicit 3D annotations. The experiments show that our\napproach outperforms previous methods on standard 3D pose benchmarks, while our\nproposed losses enable more coherent reconstruction in natural images. The\nproject website with videos, results, and code can be found at:\nhttps://jiangwenpl.github.io/multiperson\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 17:51:45 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Jiang", "Wen", ""], ["Kolotouros", "Nikos", ""], ["Pavlakos", "Georgios", ""], ["Zhou", "Xiaowei", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "2006.08602", "submitter": "Alex Wong", "authors": "Alex Wong, Safa Cicek, Stefano Soatto", "title": "Targeted Adversarial Perturbations for Monocular Depth Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of adversarial perturbations on the task of monocular\ndepth prediction. Specifically, we explore the ability of small, imperceptible\nadditive perturbations to selectively alter the perceived geometry of the\nscene. We show that such perturbations can not only globally re-scale the\npredicted distances from the camera, but also alter the prediction to match a\ndifferent target scene. We also show that, when given semantic or instance\ninformation, perturbations can fool the network to alter the depth of specific\ncategories or instances in the scene, and even remove them while preserving the\nrest of the scene. To understand the effect of targeted perturbations, we\nconduct experiments on state-of-the-art monocular depth prediction methods. Our\nexperiments reveal vulnerabilities in monocular depth prediction networks, and\nshed light on the biases and context learned by them.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 19:29:43 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 22:28:46 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Wong", "Alex", ""], ["Cicek", "Safa", ""], ["Soatto", "Stefano", ""]]}, {"id": "2006.08613", "submitter": "Jonas L\\\"ohdefink", "authors": "Jonas L\\\"ohdefink, Justin Fehrling, Marvin Klingner, Fabian H\\\"uger,\n  Peter Schlicht, Nico M. Schmidt, Tim Fingscheidt", "title": "Self-Supervised Domain Mismatch Estimation for Autonomous Perception", "comments": "Proc. of CVPR - Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving requires self awareness of its perception functions.\nTechnically spoken, this can be realized by observers, which monitor the\nperformance indicators of various perception modules. In this work we choose,\nexemplarily, a semantic segmentation to be monitored, and propose an\nautoencoder, trained in a self-supervised fashion on the very same training\ndata as the semantic segmentation to be monitored. While the autoencoder's\nimage reconstruction performance (PSNR) during online inference shows already a\ngood predictive power w.r.t. semantic segmentation performance, we propose a\nnovel domain mismatch metric DM as the earth mover's distance between a\npre-stored PSNR distribution on training (source) data, and an online-acquired\nPSNR distribution on any inference (target) data. We are able to show by\nexperiments that the DM metric has a strong rank order correlation with the\nsemantic segmentation within its functional scope. We also propose a training\ndomain-dependent threshold for the DM metric to define this functional scope.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 09:22:03 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["L\u00f6hdefink", "Jonas", ""], ["Fehrling", "Justin", ""], ["Klingner", "Marvin", ""], ["H\u00fcger", "Fabian", ""], ["Schlicht", "Peter", ""], ["Schmidt", "Nico M.", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "2006.08656", "submitter": "Shaojie Bai", "authors": "Shaojie Bai and Vladlen Koltun and J. Zico Kolter", "title": "Multiscale Deep Equilibrium Models", "comments": "NeurIPS 2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of implicit networks, the multiscale deep equilibrium\nmodel (MDEQ), suited to large-scale and highly hierarchical pattern recognition\ndomains. An MDEQ directly solves for and backpropagates through the equilibrium\npoints of multiple feature resolutions simultaneously, using implicit\ndifferentiation to avoid storing intermediate states (and thus requiring only\n$O(1)$ memory consumption). These simultaneously-learned multi-resolution\nfeatures allow us to train a single model on a diverse set of tasks and loss\nfunctions, such as using a single MDEQ to perform both image classification and\nsemantic segmentation. We illustrate the effectiveness of this approach on two\nlarge-scale vision tasks: ImageNet classification and semantic segmentation on\nhigh-resolution images from the Cityscapes dataset. In both settings, MDEQs are\nable to match or exceed the performance of recent competitive computer vision\nmodels: the first time such performance and scale have been achieved by an\nimplicit deep learning approach. The code and pre-trained models are at\nhttps://github.com/locuslab/mdeq .\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 18:07:44 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 06:59:38 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Bai", "Shaojie", ""], ["Koltun", "Vladlen", ""], ["Kolter", "J. Zico", ""]]}, {"id": "2006.08658", "submitter": "Antoine Saporta", "authors": "Antoine Saporta, Tuan-Hung Vu, Matthieu Cord, Patrick P\\'erez", "title": "ESL: Entropy-guided Self-supervised Learning for Domain Adaptation in\n  Semantic Segmentation", "comments": "Accepted at the CVPR 2020 Workshop on Scalability in Autonomous\n  Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While fully-supervised deep learning yields good models for urban scene\nsemantic segmentation, these models struggle to generalize to new environments\nwith different lighting or weather conditions for instance. In addition,\nproducing the extensive pixel-level annotations that the task requires comes at\na great cost. Unsupervised domain adaptation (UDA) is one approach that tries\nto address these issues in order to make such systems more scalable. In\nparticular, self-supervised learning (SSL) has recently become an effective\nstrategy for UDA in semantic segmentation. At the core of such methods lies\n`pseudo-labeling', that is, the practice of assigning high-confident class\npredictions as pseudo-labels, subsequently used as true labels, for target\ndata. To collect pseudo-labels, previous works often rely on the highest\nsoftmax score, which we here argue as an unfavorable confidence measurement.\n  In this work, we propose Entropy-guided Self-supervised Learning (ESL),\nleveraging entropy as the confidence indicator for producing more accurate\npseudo-labels. On different UDA benchmarks, ESL consistently outperforms strong\nSSL baselines and achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 18:10:09 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Saporta", "Antoine", ""], ["Vu", "Tuan-Hung", ""], ["Cord", "Matthieu", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "2006.08661", "submitter": "Jihyeon Lee", "authors": "Jihyeon Lee, Dylan Grosz, Burak Uzkent, Sicheng Zeng, Marshall Burke,\n  David Lobell, Stefano Ermon", "title": "Predicting Livelihood Indicators from Community-Generated Street-Level\n  Imagery", "comments": "Accepted to AAAI 2021. Code:\n  https://github.com/sustainlab-group/mapillarygcn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major decisions from governments and other large organizations rely on\nmeasurements of the populace's well-being, but making such measurements at a\nbroad scale is expensive and thus infrequent in much of the developing world.\nWe propose an inexpensive, scalable, and interpretable approach to predict key\nlivelihood indicators from public crowd-sourced street-level imagery. Such\nimagery can be cheaply collected and more frequently updated compared to\ntraditional surveying methods, while containing plausibly relevant information\nfor a range of livelihood indicators. We propose two approaches to learn from\nthe street-level imagery: (1) a method that creates multi-household cluster\nrepresentations by detecting informative objects and (2) a graph-based approach\nthat captures the relationships between images. By visualizing what features\nare important to a model and how they are used, we can help end-user\norganizations understand the models and offer an alternate approach for index\nestimation that uses cheaply obtained roadway features. By comparing our\nresults against ground data collected in nationally-representative household\nsurveys, we demonstrate the performance of our approach in accurately\npredicting indicators of poverty, population, and health and its scalability by\ntesting in two different countries, India and Kenya. Our code is available at\nhttps://github.com/sustainlab-group/mapillarygcn.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 18:12:12 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 19:03:55 GMT"}, {"version": "v3", "created": "Sat, 5 Sep 2020 22:52:08 GMT"}, {"version": "v4", "created": "Thu, 3 Dec 2020 02:27:31 GMT"}, {"version": "v5", "created": "Sat, 5 Dec 2020 08:29:27 GMT"}, {"version": "v6", "created": "Fri, 26 Feb 2021 19:45:49 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Lee", "Jihyeon", ""], ["Grosz", "Dylan", ""], ["Uzkent", "Burak", ""], ["Zeng", "Sicheng", ""], ["Burke", "Marshall", ""], ["Lobell", "David", ""], ["Ermon", "Stefano", ""]]}, {"id": "2006.08679", "submitter": "Justin Shenk", "authors": "Justin Shenk and Mats L. Richter and Wolf Byttner and Anders Arpteg\n  and Mikael Huss", "title": "Feature Space Saturation during Training", "comments": "23 pages, 26 figures, fix citation formatting, add link highlighting,\n  fix table formatting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose layer saturation - a simple, online-computable method for\nanalyzing the information processing in neural networks. First, we show that a\nlayer's output can be restricted to the eigenspace of its variance matrix\nwithout performance loss. We propose a computationally lightweight method for\napproximating the variance matrix during training. From the dimension of its\nlossless eigenspace we derive layer saturation - the ratio between the\neigenspace dimension and layer width. We show that saturation seems to indicate\nwhich layers contribute to network performance. We demonstrate how to alter\nlayer saturation in a neural network by changing network depth, filter sizes\nand input resolution. Furthermore, we show that well-chosen input resolution\nincreases network performance by distributing the inference process more evenly\nacross the network.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 18:28:21 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 15:07:19 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 09:24:39 GMT"}, {"version": "v4", "created": "Fri, 13 Nov 2020 19:17:39 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Shenk", "Justin", ""], ["Richter", "Mats L.", ""], ["Byttner", "Wolf", ""], ["Arpteg", "Anders", ""], ["Huss", "Mikael", ""]]}, {"id": "2006.08686", "submitter": "Nicholas Trieu", "authors": "Nicholas Trieu, Sebastian Goodman, Pradyumna Narayana, Kazoo Sone,\n  Radu Soricut", "title": "Multi-Image Summarization: Textual Summary from a Set of Cohesive Images", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-sentence summarization is a well studied problem in NLP, while\ngenerating image descriptions for a single image is a well studied problem in\nComputer Vision. However, for applications such as image cluster labeling or\nweb page summarization, summarizing a set of images is also a useful and\nchallenging task. This paper proposes the new task of multi-image\nsummarization, which aims to generate a concise and descriptive textual summary\ngiven a coherent set of input images. We propose a model that extends the\nimage-captioning Transformer-based architecture for single image to\nmulti-image. A dense average image feature aggregation network allows the model\nto focus on a coherent subset of attributes across the input images. We explore\nvarious input representations to the Transformer network and empirically show\nthat aggregated image features are superior to individual image embeddings. We\nadditionally show that the performance of the model is further improved by\npretraining the model parameters on a single-image captioning task, which\nappears to be particularly effective in eliminating hallucinations in the\noutput.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 18:45:35 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Trieu", "Nicholas", ""], ["Goodman", "Sebastian", ""], ["Narayana", "Pradyumna", ""], ["Sone", "Kazoo", ""], ["Soricut", "Radu", ""]]}, {"id": "2006.08694", "submitter": "Gulcin Baykal", "authors": "Gulcin Baykal, Gozde Unal", "title": "DeshuffleGAN: A Self-Supervised GAN to Improve Structure Learning", "comments": "Accepted at ICIP 2020", "journal-ref": null, "doi": "10.1109/ICIP40778.2020.9190774", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) triggered an increased interest in\nproblem of image generation due to their improved output image quality and\nversatility for expansion towards new methods. Numerous GAN-based works attempt\nto improve generation by architectural and loss-based extensions. We argue that\none of the crucial points to improve the GAN performance in terms of realism\nand similarity to the original data distribution is to be able to provide the\nmodel with a capability to learn the spatial structure in data. To that end, we\npropose the DeshuffleGAN to enhance the learning of the discriminator and the\ngenerator, via a self-supervision approach. Specifically, we introduce a\ndeshuffling task that solves a puzzle of randomly shuffled image tiles, which\nin turn helps the DeshuffleGAN learn to increase its expressive capacity for\nspatial structure and realistic appearance. We provide experimental evidence\nfor the performance improvement in generated images, compared to the baseline\nmethods, which is consistently observed over two different datasets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 19:06:07 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Baykal", "Gulcin", ""], ["Unal", "Gozde", ""]]}, {"id": "2006.08696", "submitter": "Prashant Pandey", "authors": "Prashant Pandey, Aayush Kumar Tyagi, Sameer Ambekar, and Prathosh AP", "title": "Unsupervised Domain Adaptation for Semantic Segmentation of NIR Images\n  through Generative Latent Search", "comments": "ECCV 2020 [Spotlight]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of the pixels corresponding to human skin is an essential first\nstep in multiple applications ranging from surveillance to heart-rate\nestimation from remote-photoplethysmography. However, the existing literature\nconsiders the problem only in the visible-range of the EM-spectrum which limits\ntheir utility in low or no light settings where the criticality of the\napplication is higher. To alleviate this problem, we consider the problem of\nskin segmentation from the Near-infrared images. However, Deep learning based\nstate-of-the-art segmentation techniques demands large amounts of labelled data\nthat is unavailable for the current problem. Therefore we cast the skin\nsegmentation problem as that of target-independent Unsupervised Domain\nAdaptation (UDA) where we use the data from the Red-channel of the\nvisible-range to develop skin segmentation algorithm on NIR images. We propose\na method for target-independent segmentation where the 'nearest-clone' of a\ntarget image in the source domain is searched and used as a proxy in the\nsegmentation network trained only on the source domain. We prove the existence\nof 'nearest-clone' and propose a method to find it through an optimization\nalgorithm over the latent space of a Deep generative model based on variational\ninference. We demonstrate the efficacy of the proposed method for NIR skin\nsegmentation over the state-of-the-art UDA segmentation methods on the two\nnewly created skin segmentation datasets in NIR domain despite not having\naccess to the target NIR data. Additionally, we report state-of-the-art results\nfor adaption from Synthia to Cityscapes which is a popular setting in\nUnsupervised Domain Adaptation for semantic segmentation. The code and datasets\nare available at https://github.com/ambekarsameer96/GLSS.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 19:07:55 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 12:07:42 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Pandey", "Prashant", ""], ["Tyagi", "Aayush Kumar", ""], ["Ambekar", "Sameer", ""], ["AP", "Prathosh", ""]]}, {"id": "2006.08710", "submitter": "Przemys{\\l}aw Spurek", "authors": "Przemys{\\l}aw Spurek, Maciej Zi\\k{e}ba, Jacek Tabor, Tomasz\n  Trzci\\'nski", "title": "HyperFlow: Representing 3D Objects as Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present HyperFlow - a novel generative model that leverages\nhypernetworks to create continuous 3D object representations in a form of\nlightweight surfaces (meshes), directly out of point clouds. Efficient object\nrepresentations are essential for many computer vision applications, including\nrobotic manipulation and autonomous driving. However, creating those\nrepresentations is often cumbersome, because it requires processing unordered\nsets of point clouds. Therefore, it is either computationally expensive, due to\nadditional optimization constraints such as permutation invariance, or leads to\nquantization losses introduced by binning point clouds into discrete voxels.\nInspired by mesh-based representations of objects used in computer graphics, we\npostulate a fundamentally different approach and represent 3D objects as a\nfamily of surfaces. To that end, we devise a generative model that uses a\nhypernetwork to return the weights of a Continuous Normalizing Flows (CNF)\ntarget network. The goal of this target network is to map points from a\nprobability distribution into a 3D mesh. To avoid numerical instability of the\nCNF on compact support distributions, we propose a new Spherical Log-Normal\nfunction which models density of 3D points around object surfaces mimicking\nnoise introduced by 3D capturing devices. As a result, we obtain continuous\nmesh-based object representations that yield better qualitative results than\ncompeting approaches, while reducing training time by over an order of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 19:18:02 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Spurek", "Przemys\u0142aw", ""], ["Zi\u0119ba", "Maciej", ""], ["Tabor", "Jacek", ""], ["Trzci\u0144ski", "Tomasz", ""]]}, {"id": "2006.08747", "submitter": "Salvador El\\'ias Venegas-Andraca", "authors": "Fei Yan, Salvador E. Venegas-Andraca, Kaoru Hirota", "title": "A Critical and Moving-Forward View on Quantum Image Processing", "comments": "16 pages, 4 figures. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physics and computer science have a long tradition of cross-fertilization.\nOne of the latest outcomes of this mutually beneficial relationship is quantum\ninformation science, which comprises the study of information processing tasks\nthat can be accomplished using quantum mechanical systems. Quantum Image\nProcessing (QIMP) is an emergent field of quantum information science whose\nmain goal is to strengthen our capacity for storing, processing, and retrieving\nvisual information from images and video either by transitioning from digital\nto quantum paradigms or by complementing digital imaging with quantum\ntechniques. The expectation is that harnessing the properties of quantum\nmechanical systems in QIMP will result in the realization of advanced\ntechnologies that will outperform, enhance or complement existing and upcoming\ndigital technologies for image and video processing tasks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 20:38:25 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Yan", "Fei", ""], ["Venegas-Andraca", "Salvador E.", ""], ["Hirota", "Kaoru", ""]]}, {"id": "2006.08761", "submitter": "Sayeed Shafayet Chowdhury", "authors": "Sayeed Shafayet Chowdhury, Chankyu Lee and Kaushik Roy", "title": "Towards Understanding the Effect of Leak in Spiking Neural Networks", "comments": "Sayeed Shafayet Chowdhury and Chankyu Lee contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) are being explored to emulate the astounding\ncapabilities of human brain that can learn and compute functions robustly and\nefficiently with noisy spiking activities. A variety of spiking neuron models\nhave been proposed to resemble biological neuronal functionalities. With\nvarying levels of bio-fidelity, these models often contain a leak path in their\ninternal states, called membrane potentials. While the leaky models have been\nargued as more bioplausible, a comparative analysis between models with and\nwithout leak from a purely computational point of view demands attention. In\nthis paper, we investigate the questions regarding the justification of leak\nand the pros and cons of using leaky behavior. Our experimental results reveal\nthat leaky neuron model provides improved robustness and better generalization\ncompared to models with no leak. However, leak decreases the sparsity of\ncomputation contrary to the common notion. Through a frequency domain analysis,\nwe demonstrate the effect of leak in eliminating the high-frequency components\nfrom the input, thus enabling SNNs to be more robust against noisy\nspike-inputs.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 20:56:31 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Chowdhury", "Sayeed Shafayet", ""], ["Lee", "Chankyu", ""], ["Roy", "Kaushik", ""]]}, {"id": "2006.08789", "submitter": "Thomas Pock", "authors": "Erich Kobler, Alexander Effland, Karl Kunisch, Thomas Pock", "title": "Total Deep Variation: A Stable Regularizer for Inverse Problems", "comments": "30 pages, 12 figures. arXiv admin note: text overlap with\n  arXiv:2001.05005", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various problems in computer vision and medical imaging can be cast as\ninverse problems. A frequent method for solving inverse problems is the\nvariational approach, which amounts to minimizing an energy composed of a data\nfidelity term and a regularizer. Classically, handcrafted regularizers are\nused, which are commonly outperformed by state-of-the-art deep learning\napproaches. In this work, we combine the variational formulation of inverse\nproblems with deep learning by introducing the data-driven general-purpose\ntotal deep variation regularizer. In its core, a convolutional neural network\nextracts local features on multiple scales and in successive blocks. This\ncombination allows for a rigorous mathematical analysis including an optimal\ncontrol formulation of the training problem in a mean-field setting and a\nstability analysis with respect to the initial values and the parameters of the\nregularizer. In addition, we experimentally verify the robustness against\nadversarial attacks and numerically derive upper bounds for the generalization\nerror. Finally, we achieve state-of-the-art results for numerous imaging tasks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 21:54:15 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Kobler", "Erich", ""], ["Effland", "Alexander", ""], ["Kunisch", "Karl", ""], ["Pock", "Thomas", ""]]}, {"id": "2006.08792", "submitter": "Emiel Van Miltenburg", "authors": "Emiel van Miltenburg", "title": "On the use of human reference data for evaluating automatic image\n  descriptions", "comments": "Originally presented as a (non-archival) poster at the VizWiz 2020\n  workshop, collocated with CVPR 2020. See:\n  https://vizwiz.org/workshops/2020-workshop/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic image description systems are commonly trained and evaluated using\ncrowdsourced, human-generated image descriptions. The best-performing system is\nthen determined using some measure of similarity to the reference data (BLEU,\nMeteor, CIDER, etc). Thus, both the quality of the systems as well as the\nquality of the evaluation depends on the quality of the descriptions. As\nSection 2 will show, the quality of current image description datasets is\ninsufficient. I argue that there is a need for more detailed guidelines that\ntake into account the needs of visually impaired users, but also the\nfeasibility of generating suitable descriptions. With high-quality data,\nevaluation of image description systems could use reference descriptions, but\nwe should also look for alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 21:57:27 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["van Miltenburg", "Emiel", ""]]}, {"id": "2006.08819", "submitter": "Ioannis Ivrissimtzis", "authors": "Xin Zhang and Ning Jia and Ioannis Ivrissimtzis", "title": "A study of the effect of the illumination model on the generation of\n  synthetic training datasets", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of computer generated images to train Deep Neural Networks is a\nviable alternative to real images when the latter are scarce or expensive. In\nthis paper, we study how the illumination model used by the rendering software\naffects the quality of the generated images. We created eight training sets,\neach one with a different illumination model, and tested them on three\ndifferent network architectures, ResNet, U-Net and a combined architecture\ndeveloped by us. The test set consisted of photos of 3D printed objects\nproduced from the same CAD models used to generate the training set. The effect\nof the other parameters of the rendering process, such as textures and camera\nposition, was randomized.\n  Our results show that the effect of the illumination model is important,\ncomparable in significance to the network architecture. We also show that both\nlight probes capturing natural environmental light, and modelled lighting\nenvironments, can give good results. In the case of light probes, we identified\nas two significant factors affecting performance the similarity between the\nlight probe and the test environment, as well as the light probe's resolution.\nRegarding modelled lighting environment, similarity with the test environment\nwas again identified as a significant factor.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 23:22:24 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Zhang", "Xin", ""], ["Jia", "Ning", ""], ["Ivrissimtzis", "Ioannis", ""]]}, {"id": "2006.08825", "submitter": "Nathan Painchaud", "authors": "Nathan Painchaud, Youssef Skandarani, Thierry Judge, Olivier Bernard,\n  Alain Lalande, Pierre-Marc Jodoin", "title": "Cardiac Segmentation with Strong Anatomical Guarantees", "comments": "11 pages, accepted for publication in IEEE TMI", "journal-ref": null, "doi": "10.1109/TMI.2020.3003240", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have had unprecedented success in medical\nimaging and, in particular, in medical image segmentation. However, despite the\nfact that segmentation results are closer than ever to the inter-expert\nvariability, CNNs are not immune to producing anatomically inaccurate\nsegmentations, even when built upon a shape prior. In this paper, we present a\nframework for producing cardiac image segmentation maps that are guaranteed to\nrespect pre-defined anatomical criteria, while remaining within the\ninter-expert variability. The idea behind our method is to use a well-trained\nCNN, have it process cardiac images, identify the anatomically implausible\nresults and warp these results toward the closest anatomically valid cardiac\nshape. This warping procedure is carried out with a constrained variational\nautoencoder (cVAE) trained to learn a representation of valid cardiac shapes\nthrough a smooth, yet constrained, latent space. With this cVAE, we can project\nany implausible shape into the cardiac latent space and steer it toward the\nclosest correct shape. We tested our framework on short-axis MRI as well as\napical two and four-chamber view ultrasound images, two modalities for which\ncardiac shapes are drastically different. With our method, CNNs can now produce\nresults that are both within the inter-expert variability and always\nanatomically plausible without having to rely on a shape prior.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 23:38:31 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Painchaud", "Nathan", ""], ["Skandarani", "Youssef", ""], ["Judge", "Thierry", ""], ["Bernard", "Olivier", ""], ["Lalande", "Alain", ""], ["Jodoin", "Pierre-Marc", ""]]}, {"id": "2006.08844", "submitter": "Xinghui Li Mr.", "authors": "Xinghui Li, Kai Han, Shuda Li, Victor Adrian Prisacariu", "title": "Dual-Resolution Correspondence Networks", "comments": "NeurIPS 2020, code at https://dualrcnet.active.vision/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of establishing dense pixel-wise correspondences\nbetween a pair of images. In this work, we introduce Dual-Resolution\nCorrespondence Networks (DualRC-Net), to obtain pixel-wise correspondences in a\ncoarse-to-fine manner. DualRC-Net extracts both coarse- and fine- resolution\nfeature maps. The coarse maps are used to produce a full but coarse 4D\ncorrelation tensor, which is then refined by a learnable neighbourhood\nconsensus module. The fine-resolution feature maps are used to obtain the final\ndense correspondences guided by the refined coarse 4D correlation tensor. The\nselected coarse-resolution matching scores allow the fine-resolution features\nto focus only on a limited number of possible matches with high confidence. In\nthis way, DualRC-Net dramatically increases matching reliability and\nlocalisation accuracy, while avoiding to apply the expensive 4D convolution\nkernels on fine-resolution feature maps. We comprehensively evaluate our method\non large-scale public benchmarks including HPatches, InLoc, and Aachen\nDay-Night. It achieves the state-of-the-art results on all of them.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 00:42:43 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 17:16:58 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Li", "Xinghui", ""], ["Han", "Kai", ""], ["Li", "Shuda", ""], ["Prisacariu", "Victor Adrian", ""]]}, {"id": "2006.08857", "submitter": "Chong You", "authors": "Chong You, Zhihui Zhu, Qing Qu, Yi Ma", "title": "Robust Recovery via Implicit Bias of Discrepant Learning Rates for\n  Double Over-parameterization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances have shown that implicit bias of gradient descent on\nover-parameterized models enables the recovery of low-rank matrices from linear\nmeasurements, even with no prior knowledge on the intrinsic rank. In contrast,\nfor robust low-rank matrix recovery from grossly corrupted measurements,\nover-parameterization leads to overfitting without prior knowledge on both the\nintrinsic rank and sparsity of corruption. This paper shows that with a double\nover-parameterization for both the low-rank matrix and sparse corruption,\ngradient descent with discrepant learning rates provably recovers the\nunderlying matrix even without prior knowledge on neither rank of the matrix\nnor sparsity of the corruption. We further extend our approach for the robust\nrecovery of natural images by over-parameterizing images with deep\nconvolutional networks. Experiments show that our method handles different test\nimages and varying corruption levels with a single learning pipeline where the\nnetwork width and termination conditions do not need to be adjusted on a\ncase-by-case basis. Underlying the success is again the implicit bias with\ndiscrepant learning rates on different over-parameterized parameters, which may\nbear on broader applications.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 01:21:22 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["You", "Chong", ""], ["Zhu", "Zhihui", ""], ["Qu", "Qing", ""], ["Ma", "Yi", ""]]}, {"id": "2006.08861", "submitter": "Feng Hu", "authors": "Feng Hu", "title": "GPU-accelerated Hierarchical Panoramic Image Feature Retrieval for\n  Indoor Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor localization has many applications, such as commercial Location Based\nServices (LBS), robotic navigation, and assistive navigation for the blind.\nThis paper formulates the indoor localization problem into a multimedia\nretrieving problem by modeling visual landmarks with a panoramic image feature,\nand calculating a user's location via GPU- accelerated parallel retrieving\nalgorithm. To solve the scene similarity problem, we apply a multi-images based\nretrieval strategy and a 2D aggregation method to estimate the final retrieval\nlocation. Experiments on a campus building real data demonstrate real-time\nresponses (14fps) and robust localization.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 01:42:20 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Hu", "Feng", ""]]}, {"id": "2006.08878", "submitter": "Nikolay Kozyrskiy", "authors": "Nikolay Kozyrskiy, Anh-Huy Phan", "title": "CNN Acceleration by Low-rank Approximation with Quantized Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern convolutional neural networks although achieve great results in\nsolving complex computer vision tasks still cannot be effectively used in\nmobile and embedded devices due to the strict requirements for computational\ncomplexity, memory and power consumption. The CNNs have to be compressed and\naccelerated before deployment. In order to solve this problem the novel\napproach combining two known methods, low-rank tensor approximation in Tucker\nformat and quantization of weights and feature maps (activations), is proposed.\nThe greedy one-step and multi-step algorithms for the task of multilinear rank\nselection are proposed. The approach for quality restoration after applying\nTucker decomposition and quantization is developed. The efficiency of our\nmethod is demonstrated for ResNet18 and ResNet34 on CIFAR-10, CIFAR-100 and\nImagenet classification tasks. As a result of comparative analysis performed\nfor other methods for compression and acceleration our approach showed its\npromising features.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 02:28:05 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Kozyrskiy", "Nikolay", ""], ["Phan", "Anh-Huy", ""]]}, {"id": "2006.08885", "submitter": "Sharif Abuadbba Dr", "authors": "Bedeuro Kim, Sharif Abuadbba, Hyoungshick Kim", "title": "DeepCapture: Image Spam Detection Using Deep Learning and Data\n  Augmentation", "comments": "15 pages, single column. ACISP 2020: Australasian Conference on\n  Information Security and Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image spam emails are often used to evade text-based spam filters that detect\nspam emails with their frequently used keywords. In this paper, we propose a\nnew image spam email detection tool called DeepCapture using a convolutional\nneural network (CNN) model. There have been many efforts to detect image spam\nemails, but there is a significant performance degrade against entirely new and\nunseen image spam emails due to overfitting during the training phase. To\naddress this challenging issue, we mainly focus on developing a more robust\nmodel to address the overfitting problem. Our key idea is to build a\nCNN-XGBoost framework consisting of eight layers only with a large number of\ntraining samples using data augmentation techniques tailored towards the image\nspam detection task. To show the feasibility of DeepCapture, we evaluate its\nperformance with publicly available datasets consisting of 6,000 spam and 2,313\nnon-spam image samples. The experimental results show that DeepCapture is\ncapable of achieving an F1-score of 88%, which has a 6% improvement over the\nbest existing spam detection model CNN-SVM with an F1-score of 82%. Moreover,\nDeepCapture outperformed existing image spam detection solutions against new\nand unseen image datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 02:50:04 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Kim", "Bedeuro", ""], ["Abuadbba", "Sharif", ""], ["Kim", "Hyoungshick", ""]]}, {"id": "2006.08889", "submitter": "Zerun Feng", "authors": "Zerun Feng, Zhimin Zeng, Caili Guo, Zheng Li", "title": "Exploiting Visual Semantic Reasoning for Video-Text Retrieval", "comments": "Accepted by IJCAI 2020. SOLE copyright holder is IJCAI (International\n  Joint Conferences on Artificial Intelligence), all rights reserved.\n  http://static.ijcai.org/2020-accepted_papers.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video retrieval is a challenging research topic bridging the vision and\nlanguage areas and has attracted broad attention in recent years. Previous\nworks have been devoted to representing videos by directly encoding from\nframe-level features. In fact, videos consist of various and abundant semantic\nrelations to which existing methods pay less attention. To address this issue,\nwe propose a Visual Semantic Enhanced Reasoning Network (ViSERN) to exploit\nreasoning between frame regions. Specifically, we consider frame regions as\nvertices and construct a fully-connected semantic correlation graph. Then, we\nperform reasoning by novel random walk rule-based graph convolutional networks\nto generate region features involved with semantic relations. With the benefit\nof reasoning, semantic interactions between regions are considered, while the\nimpact of redundancy is suppressed. Finally, the region features are aggregated\nto form frame-level features for further encoding to measure video-text\nsimilarity. Extensive experiments on two public benchmark datasets validate the\neffectiveness of our method by achieving state-of-the-art performance due to\nthe powerful semantic reasoning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 02:56:46 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Feng", "Zerun", ""], ["Zeng", "Zhimin", ""], ["Guo", "Caili", ""], ["Li", "Zheng", ""]]}, {"id": "2006.08903", "submitter": "Alex Kuefler", "authors": "Ben Goodrich, Alex Kuefler, William D. Richards", "title": "Depth by Poking: Learning to Estimate Depth from Self-Supervised\n  Grasping", "comments": "IEEE International Conference on Robotics and Automation (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate depth estimation remains an open problem for robotic manipulation;\neven state of the art techniques including structured light and LiDAR sensors\nfail on reflective or transparent surfaces. We address this problem by training\na neural network model to estimate depth from RGB-D images, using labels from\nphysical interactions between a robot and its environment. Our network\npredicts, for each pixel in an input image, the z position that a robot's end\neffector would reach if it attempted to grasp or poke at the corresponding\nposition. Given an autonomous grasping policy, our approach is self-supervised\nas end effector position labels can be recovered through forward kinematics,\nwithout human annotation. Although gathering such physical interaction data is\nexpensive, it is necessary for training and routine operation of state of the\nart manipulation systems. Therefore, this depth estimator comes ``for free''\nwhile collecting data for other tasks (e.g., grasping, pushing, placing). We\nshow our approach achieves significantly lower root mean squared error than\ntraditional structured light sensors and unsupervised deep learning methods on\ndifficult, industry-scale jumbled bin datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 03:34:26 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Goodrich", "Ben", ""], ["Kuefler", "Alex", ""], ["Richards", "William D.", ""]]}, {"id": "2006.08933", "submitter": "Muhammad Umar Karim Khan", "authors": "Muhammad Umar Karim Khan, Mishal Fatima, Chong-Min Kyung", "title": "Plug-and-Play Anomaly Detection with Expectation Maximization Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in crowds enables early rescue response. A plug-and-play\nsmart camera for crowd surveillance has numerous constraints different from\ntypical anomaly detection: the training data cannot be used iteratively; there\nare no training labels; and training and classification needs to be performed\nsimultaneously. We tackle all these constraints with our approach in this\npaper. We propose a Core Anomaly-Detection (CAD) neural network which learns\nthe motion behavior of objects in the scene with an unsupervised method. On\naverage over standard datasets, CAD with a single epoch of training shows a\npercentage increase in Area Under the Curve (AUC) of 4.66% and 4.9% compared to\nthe best results with convolutional autoencoders and convolutional LSTM-based\nmethods, respectively. With a single epoch of training, our method improves the\nAUC by 8.03% compared to the convolutional LSTM-based approach. We also propose\nan Expectation Maximization filter which chooses samples for training the core\nanomaly-detection network. The overall framework improves the AUC compared to\nfuture frame prediction-based approach by 24.87% when crowd anomaly detection\nis performed on a video stream. We believe our work is the first step towards\nusing deep learning methods with autonomous plug-and-play smart cameras for\ncrowd anomaly detection.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 05:28:40 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Khan", "Muhammad Umar Karim", ""], ["Fatima", "Mishal", ""], ["Kyung", "Chong-Min", ""]]}, {"id": "2006.08937", "submitter": "Yuan Minglei", "authors": "Minglei Yuan and Cunhao Cai and Tong Lu", "title": "Channel Relationship Prediction with Forget-Update Module for Few-shot\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a pipeline for inferring the relationship of each\nclass in support set and a query sample using forget-update module. We first\npropose a novel architectural module called \"channel vector sequence\nconstruction module\", which boosts the performance of\nsequence-prediction-model-based few-shot classification methods by collecting\nthe overall information of all support samples and a query sample. The channel\nvector sequence generated by this module is organized in a way that each time\nstep of the sequence contains the information from the corresponding channel of\nall support samples and the query sample to be inferred. Channel vector\nsequence is obtained by a convolutional neural network and a fully connected\nnetwork, and the spliced channel vector sequence is spliced of the\ncorresponding channel vectors of support samples and a query sample in the\noriginal channel order. Also, we propose a forget-update module consisting of\nstacked forget-update blocks. The forget block modify the original information\nwith the learned weights and the update block establishes a dense connection\nfor the model. The proposed pipeline, which consists of channel vector sequence\nconstruction module and forget-update module, can infer the relationship\nbetween the query sample and support samples in few-shot classification\nscenario. Experimental results show that the pipeline can achieve\nstate-of-the-art results on miniImagenet, CUB dataset, and cross-domain\nscenario.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 05:48:08 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Yuan", "Minglei", ""], ["Cai", "Cunhao", ""], ["Lu", "Tong", ""]]}, {"id": "2006.08939", "submitter": "Zongyan Han", "authors": "Zongyan Han, Zhenyong Fu and Jian Yang", "title": "Learning the Redundancy-free Features for Generalized Zero-Shot Object\n  Recognition", "comments": "Some researchers and we have found KNN results in 1st version are\n  incorrect, due to a careless mistake in the code. Concretely, the parameters\n  for accuracy function of KNN were organized in the wrong order by mistake.\n  The softmax results are correct. We have removed all KNN results and remove\n  the SOTA claims. According to the Program Chairs' suggestion, we have made\n  errata request to CVF and IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot object recognition or zero-shot learning aims to transfer the\nobject recognition ability among the semantically related categories, such as\nfine-grained animal or bird species. However, the images of different\nfine-grained objects tend to merely exhibit subtle differences in appearance,\nwhich will severely deteriorate zero-shot object recognition. To reduce the\nsuperfluous information in the fine-grained objects, in this paper, we propose\nto learn the redundancy-free features for generalized zero-shot learning. We\nachieve our motivation by projecting the original visual features into a new\n(redundancy-free) feature space and then restricting the statistical dependence\nbetween these two feature spaces. Furthermore, we require the projected\nfeatures to keep and even strengthen the category relationship in the\nredundancy-free feature space. In this way, we can remove the redundant\ninformation from the visual features without losing the discriminative\ninformation. We extensively evaluate the performance on four benchmark\ndatasets. The results show that our redundancy-free feature based generalized\nzero-shot learning (RFF-GZSL) approach can achieve competitive results compared\nwith the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 05:53:25 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 06:09:22 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Han", "Zongyan", ""], ["Fu", "Zhenyong", ""], ["Yang", "Jian", ""]]}, {"id": "2006.08942", "submitter": "Mishal Fatima", "authors": "Mishal Fatima, Muhammad Umar Karim Khan, and Chong Min Kyung", "title": "Global Feature Aggregation for Accident Anticipation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anticipation of accidents ahead of time in autonomous and non-autonomous\nvehicles aids in accident avoidance. In order to recognize abnormal events such\nas traffic accidents in a video sequence, it is important that the network\ntakes into account interactions of objects in a given frame. We propose a novel\nFeature Aggregation (FA) block that refines each object's features by computing\na weighted sum of the features of all objects in a frame. We use FA block along\nwith Long Short Term Memory (LSTM) network to anticipate accidents in the video\nsequences. We report mean Average Precision (mAP) and Average Time-to-Accident\n(ATTA) on Street Accident (SA) dataset. Our proposed method achieves the\nhighest score for risk anticipation by predicting accidents 0.32 sec and 0.75\nsec earlier compared to the best results with Adaptive Loss and dynamic\nparameter prediction based methods respectively.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 06:17:15 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Fatima", "Mishal", ""], ["Khan", "Muhammad Umar Karim", ""], ["Kyung", "Chong Min", ""]]}, {"id": "2006.09000", "submitter": "Kirill Bykov", "authors": "Kirill Bykov, Marina M.-C. H\\\"ohne, Klaus-Robert M\\\"uller, Shinichi\n  Nakajima, Marius Kloft", "title": "How Much Can I Trust You? -- Quantifying Uncertainties in Explaining\n  Neural Networks", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable AI (XAI) aims to provide interpretations for predictions made by\nlearning machines, such as deep neural networks, in order to make the machines\nmore transparent for the user and furthermore trustworthy also for applications\nin e.g. safety-critical areas. So far, however, no methods for quantifying\nuncertainties of explanations have been conceived, which is problematic in\ndomains where a high confidence in explanations is a prerequisite. We therefore\ncontribute by proposing a new framework that allows to convert any arbitrary\nexplanation method for neural networks into an explanation method for Bayesian\nneural networks, with an in-built modeling of uncertainties. Within the\nBayesian framework a network's weights follow a distribution that extends\nstandard single explanation scores and heatmaps to distributions thereof, in\nthis manner translating the intrinsic network model uncertainties into a\nquantification of explanation uncertainties. This allows us for the first time\nto carve out uncertainties associated with a model explanation and subsequently\ngauge the appropriate level of explanation confidence for a user (using\npercentiles). We demonstrate the effectiveness and usefulness of our approach\nextensively in various experiments, both qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 08:54:42 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Bykov", "Kirill", ""], ["H\u00f6hne", "Marina M. -C.", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Nakajima", "Shinichi", ""], ["Kloft", "Marius", ""]]}, {"id": "2006.09001", "submitter": "Kanishka Rao", "authors": "Kanishka Rao, Chris Harris, Alex Irpan, Sergey Levine, Julian Ibarz,\n  Mohi Khansari", "title": "RL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real", "comments": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network based reinforcement learning (RL) can learn appropriate\nvisual representations for complex tasks like vision-based robotic grasping\nwithout the need for manually engineering or prior learning a perception\nsystem. However, data for RL is collected via running an agent in the desired\nenvironment, and for applications like robotics, running a robot in the real\nworld may be extremely costly and time consuming. Simulated training offers an\nappealing alternative, but ensuring that policies trained in simulation can\ntransfer effectively into the real world requires additional machinery.\nSimulations may not match reality, and typically bridging the\nsimulation-to-reality gap requires domain knowledge and task-specific\nengineering. We can automate this process by employing generative models to\ntranslate simulated images into realistic ones. However, this sort of\ntranslation is typically task-agnostic, in that the translated images may not\npreserve all features that are relevant to the task. In this paper, we\nintroduce the RL-scene consistency loss for image translation, which ensures\nthat the translation operation is invariant with respect to the Q-values\nassociated with the image. This allows us to learn a task-aware translation.\nIncorporating this loss into unsupervised domain translation, we obtain\nRL-CycleGAN, a new approach for simulation-to-real-world transfer for\nreinforcement learning. In evaluations of RL-CycleGAN on two vision-based\nrobotics grasping tasks, we show that RL-CycleGAN offers a substantial\nimprovement over a number of prior methods for sim-to-real transfer, attaining\nexcellent real-world performance with only a modest number of real-world\nobservations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 08:58:07 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Rao", "Kanishka", ""], ["Harris", "Chris", ""], ["Irpan", "Alex", ""], ["Levine", "Sergey", ""], ["Ibarz", "Julian", ""], ["Khansari", "Mohi", ""]]}, {"id": "2006.09011", "submitter": "Yang Song", "authors": "Yang Song and Stefano Ermon", "title": "Improved Techniques for Training Score-Based Generative Models", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Score-based generative models can produce high quality image samples\ncomparable to GANs, without requiring adversarial optimization. However,\nexisting training procedures are limited to images of low resolution (typically\nbelow 32x32), and can be unstable under some settings. We provide a new\ntheoretical analysis of learning and sampling from score models in high\ndimensional spaces, explaining existing failure modes and motivating new\nsolutions that generalize across datasets. To enhance stability, we also\npropose to maintain an exponential moving average of model weights. With these\nimprovements, we can effortlessly scale score-based generative models to images\nwith unprecedented resolutions ranging from 64x64 to 256x256. Our score-based\nmodels can generate high-fidelity samples that rival best-in-class GANs on\nvarious image datasets, including CelebA, FFHQ, and multiple LSUN categories.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 09:17:17 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 19:37:51 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Song", "Yang", ""], ["Ermon", "Stefano", ""]]}, {"id": "2006.09029", "submitter": "Jie An", "authors": "Jie An, Tao Li, Haozhi Huang, Li Shen, Xuan Wang, Yongyi Tang, Jinwen\n  Ma, Wei Liu, and Jiebo Luo", "title": "Real-time Universal Style Transfer on High-resolution Images via\n  Zero-channel Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting effective deep features to represent content and style information\nis the key to universal style transfer. Most existing algorithms use VGG19 as\nthe feature extractor, which incurs a high computational cost and impedes\nreal-time style transfer on high-resolution images. In this work, we propose a\nlightweight alternative architecture - ArtNet, which is based on GoogLeNet, and\nlater pruned by a novel channel pruning method named Zero-channel Pruning\nspecially designed for style transfer approaches. Besides, we propose a\ntheoretically sound sandwich swap transform (S2) module to transfer deep\nfeatures, which can create a pleasing holistic appearance and good local\ntextures with an improved content preservation ability. By using ArtNet and S2,\nour method is 2.3 to 107.4 times faster than state-of-the-art approaches. The\ncomprehensive experiments demonstrate that ArtNet can achieve universal,\nreal-time, and high-quality style transfer on high-resolution images\nsimultaneously, (68.03 FPS on 512 times 512 images).\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 09:50:14 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 03:37:40 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["An", "Jie", ""], ["Li", "Tao", ""], ["Huang", "Haozhi", ""], ["Shen", "Li", ""], ["Wang", "Xuan", ""], ["Tang", "Yongyi", ""], ["Ma", "Jinwen", ""], ["Liu", "Wei", ""], ["Luo", "Jiebo", ""]]}, {"id": "2006.09034", "submitter": "Jesper Christensen", "authors": "Jesper Haahr Christensen, Lars Valdemar Mogensen, Ole Ravn", "title": "Deep Learning based Segmentation of Fish in Noisy Forward Looking MBES\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we investigate a Deep Learning (DL) approach to fish\nsegmentation in a small dataset of noisy low-resolution images generated by a\nforward-looking multibeam echosounder (MBES). We build on recent advances in DL\nand Convolutional Neural Networks (CNNs) for semantic segmentation and\ndemonstrate an end-to-end approach for a fish/non-fish probability prediction\nfor all range-azimuth positions projected by an imaging sonar. We use\nself-collected datasets from the Danish Sound and the Faroe Islands to train\nand test our model and present techniques to obtain satisfying performance and\ngeneralization even with a low-volume dataset. We show that our model proves\nthe desired performance and has learned to harness the importance of semantic\ncontext and take this into account to separate noise and non-targets from real\ntargets. Furthermore, we present techniques to deploy models on low-cost\nembedded platforms to obtain higher performance fit for edge environments -\nwhere compute and power are restricted by size/cost - for testing and\nprototyping.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 09:57:38 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Christensen", "Jesper Haahr", ""], ["Mogensen", "Lars Valdemar", ""], ["Ravn", "Ole", ""]]}, {"id": "2006.09042", "submitter": "Muhammad Suhaib Tanveer", "authors": "Muhammad Suhaib Tanveer, Muhammad Umar Karim Khan, Chong-Min Kyung", "title": "Fine-Tuning DARTS for Image Classification", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has gained attraction due to superior\nclassification performance. Differential Architecture Search (DARTS) is a\ncomputationally light method. To limit computational resources DARTS makes\nnumerous approximations. These approximations result in inferior performance.\nWe propose to fine-tune DARTS using fixed operations as they are independent of\nthese approximations. Our method offers a good trade-off between the number of\nparameters and classification accuracy. Our approach improves the top-1\naccuracy on Fashion-MNIST, CompCars, and MIO-TCD datasets by 0.56%, 0.50%, and\n0.39%, respectively compared to the state-of-the-art approaches. Our approach\nperforms better than DARTS, improving the accuracy by 0.28%, 1.64%, 0.34%,\n4.5%, and 3.27% compared to DARTS, on CIFAR-10, CIFAR-100, Fashion-MNIST,\nCompCars, and MIO-TCD datasets, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 10:00:45 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Tanveer", "Muhammad Suhaib", ""], ["Khan", "Muhammad Umar Karim", ""], ["Kyung", "Chong-Min", ""]]}, {"id": "2006.09043", "submitter": "Maurice Quach", "authors": "Maurice Quach, Giuseppe Valenzise, Frederic Dufaux", "title": "Improved Deep Point Cloud Geometry Compression", "comments": "Code is available at https://github.com/mauriceqch/pcc_geo_cnn_v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds have been recognized as a crucial data structure for 3D content\nand are essential in a number of applications such as virtual and mixed\nreality, autonomous driving, cultural heritage, etc. In this paper, we propose\na set of contributions to improve deep point cloud compression, i.e.: using a\nscale hyperprior model for entropy coding; employing deeper transforms; a\ndifferent balancing weight in the focal loss; optimal thresholding for\ndecoding; and sequential model training. In addition, we present an extensive\nablation study on the impact of each of these factors, in order to provide a\nbetter understanding about why they improve RD performance. An optimal\ncombination of the proposed improvements achieves BD-PSNR gains over G-PCC\ntrisoup and octree of 5.50 (6.48) dB and 6.84 (5.95) dB, respectively, when\nusing the point-to-point (point-to-plane) metric. Code is available at\nhttps://github.com/mauriceqch/pcc_geo_cnn_v2 .\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 10:03:14 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 12:56:23 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Quach", "Maurice", ""], ["Valenzise", "Giuseppe", ""], ["Dufaux", "Frederic", ""]]}, {"id": "2006.09049", "submitter": "Aditya Rajagopal", "authors": "Aditya Rajagopal, Diederik Adriaan Vink, Stylianos I. Venieris,\n  Christos-Savvas Bouganis", "title": "Multi-Precision Policy Enforced Training (MuPPET): A precision-switching\n  strategy for quantised fixed-point training of CNNs", "comments": "Accepted at the 37th International Conference on Machine Learning\n  (ICML), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale convolutional neural networks (CNNs) suffer from very long\ntraining times, spanning from hours to weeks, limiting the productivity and\nexperimentation of deep learning practitioners. As networks grow in size and\ncomplexity, training time can be reduced through low-precision data\nrepresentations and computations. However, in doing so the final accuracy\nsuffers due to the problem of vanishing gradients. Existing state-of-the-art\nmethods combat this issue by means of a mixed-precision approach utilising two\ndifferent precision levels, FP32 (32-bit floating-point) and FP16/FP8\n(16-/8-bit floating-point), leveraging the hardware support of recent GPU\narchitectures for FP16 operations to obtain performance gains. This work pushes\nthe boundary of quantised training by employing a multilevel optimisation\napproach that utilises multiple precisions including low-precision fixed-point\nrepresentations. The novel training strategy, MuPPET, combines the use of\nmultiple number representation regimes together with a precision-switching\nmechanism that decides at run time the transition point between precision\nregimes. Overall, the proposed strategy tailors the training process to the\nhardware-level capabilities of the target hardware architecture and yields\nimprovements in training time and energy efficiency compared to\nstate-of-the-art approaches. Applying MuPPET on the training of AlexNet,\nResNet18 and GoogLeNet on ImageNet (ILSVRC12) and targeting an NVIDIA Turing\nGPU, MuPPET achieves the same accuracy as standard full-precision training with\ntraining-time speedup of up to 1.84$\\times$ and an average speedup of\n1.58$\\times$ across the networks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 10:14:36 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Rajagopal", "Aditya", ""], ["Vink", "Diederik Adriaan", ""], ["Venieris", "Stylianos I.", ""], ["Bouganis", "Christos-Savvas", ""]]}, {"id": "2006.09050", "submitter": "Sergio Vitale", "authors": "Sergio Vitale, Giampaolo Ferraioli and Vito Pascazio", "title": "Multi-Objective CNN Based Algorithm for SAR Despeckling", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, (2020) 1-14", "doi": "10.1109/TGRS.2020.3034852", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) in remote sensing has nowadays become an effective\noperative tool: it is largely used in applications such as change detection,\nimage restoration, segmentation, detection and classification. With reference\nto synthetic aperture radar (SAR) domain the application of DL techniques is\nnot straightforward due to non trivial interpretation of SAR images, specially\ncaused by the presence of speckle. Several deep learning solutions for SAR\ndespeckling have been proposed in the last few years. Most of these solutions\nfocus on the definition of different network architectures with similar cost\nfunctions not involving SAR image properties. In this paper, a convolutional\nneural network (CNN) with a multi-objective cost function taking care of\nspatial and statistical properties of the SAR image is proposed. This is\nachieved by the definition of a peculiar loss function obtained by the weighted\ncombination of three different terms. Each of this term is dedicated mainly to\none of the following SAR image characteristics: spatial details, speckle\nstatistical properties and strong scatterers identification. Their combination\nallows to balance these effects. Moreover, a specifically designed architecture\nis proposed for effectively extract distinctive features within the considered\nframework. Experiments on simulated and real SAR images show the accuracy of\nthe proposed method compared to the State-of-Art despeckling algorithms, both\nfrom quantitative and qualitative point of view. The importance of considering\nsuch SAR properties in the cost function is crucial for a correct noise\nrejection and details preservation in different underlined scenarios, such as\nhomogeneous, heterogeneous and extremely heterogeneous.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 10:15:42 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 09:43:02 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 16:24:45 GMT"}, {"version": "v4", "created": "Fri, 30 Oct 2020 13:55:51 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Vitale", "Sergio", ""], ["Ferraioli", "Giampaolo", ""], ["Pascazio", "Vito", ""]]}, {"id": "2006.09073", "submitter": "Zihao Zhu", "authors": "Zihao Zhu, Jing Yu, Yujing Wang, Yajing Sun, Yue Hu, Qi Wu", "title": "Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based Visual\n  Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fact-based Visual Question Answering (FVQA) requires external knowledge\nbeyond visible content to answer questions about an image, which is challenging\nbut indispensable to achieve general VQA. One limitation of existing FVQA\nsolutions is that they jointly embed all kinds of information without\nfine-grained selection, which introduces unexpected noises for reasoning the\nfinal answer. How to capture the question-oriented and\ninformation-complementary evidence remains a key challenge to solve the\nproblem. In this paper, we depict an image by a multi-modal heterogeneous\ngraph, which contains multiple layers of information corresponding to the\nvisual, semantic and factual features. On top of the multi-layer graph\nrepresentations, we propose a modality-aware heterogeneous graph convolutional\nnetwork to capture evidence from different layers that is most relevant to the\ngiven question. Specifically, the intra-modal graph convolution selects\nevidence from each modality and cross-modal graph convolution aggregates\nrelevant information across different modalities. By stacking this process\nmultiple times, our model performs iterative reasoning and predicts the optimal\nanswer by analyzing all question-oriented evidence. We achieve a new\nstate-of-the-art performance on the FVQA task and demonstrate the effectiveness\nand interpretability of our model with extensive experiments.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 11:03:37 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 00:49:02 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 01:36:36 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Zhu", "Zihao", ""], ["Yu", "Jing", ""], ["Wang", "Yujing", ""], ["Sun", "Yajing", ""], ["Hu", "Yue", ""], ["Wu", "Qi", ""]]}, {"id": "2006.09081", "submitter": "Pau De Jorge Aranda", "authors": "Pau de Jorge, Amartya Sanyal, Harkirat S. Behl, Philip H.S. Torr,\n  Gregory Rogez, Puneet K. Dokania", "title": "Progressive Skeletonization: Trimming more fat from a network at\n  initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that skeletonization (pruning parameters) of\nnetworks \\textit{at initialization} provides all the practical benefits of\nsparsity both at inference and training time, while only marginally degrading\ntheir performance. However, we observe that beyond a certain level of sparsity\n(approx $95\\%$), these approaches fail to preserve the network performance, and\nto our surprise, in many cases perform even worse than trivial random pruning.\nTo this end, we propose an objective to find a skeletonized network with\nmaximum {\\em foresight connection sensitivity} (FORCE) whereby the\ntrainability, in terms of connection sensitivity, of a pruned network is taken\ninto consideration. We then propose two approximate procedures to maximize our\nobjective (1) Iterative SNIP: allows parameters that were unimportant at\nearlier stages of skeletonization to become important at later stages; and (2)\nFORCE: iterative process that allows exploration by allowing already pruned\nparameters to resurrect at later stages of skeletonization. Empirical analyses\non a large suite of experiments show that our approach, while providing at\nleast as good a performance as other recent approaches on moderate pruning\nlevels, provides remarkably improved performance on higher pruning levels\n(could remove up to $99.5\\%$ parameters while keeping the networks trainable).\nCode can be found in https://github.com/naver/force.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 11:32:47 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 14:41:08 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 12:02:15 GMT"}, {"version": "v4", "created": "Wed, 21 Oct 2020 13:54:26 GMT"}, {"version": "v5", "created": "Fri, 19 Mar 2021 13:06:16 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["de Jorge", "Pau", ""], ["Sanyal", "Amartya", ""], ["Behl", "Harkirat S.", ""], ["Torr", "Philip H. S.", ""], ["Rogez", "Gregory", ""], ["Dokania", "Puneet K.", ""]]}, {"id": "2006.09102", "submitter": "Kacper Kania", "authors": "Kacper Kania, Maciej Zi\\k{e}ba, Tomasz Kajdanowicz", "title": "UCSG-Net -- Unsupervised Discovering of Constructive Solid Geometry Tree", "comments": "Accepted to Thirty-fourth Conference on Neural Information Processing\n  Systems (NeurIPS 2020). Project page: https://kacperkan.github.io/ucsgnet.\n  Project video: https://www.youtube.com/watch?v=s1p4UHtUG3g&feature=emb_title.\n  Comments: 13 pages, 7 figures; apply reviewers' remarks, fix the reference to\n  the CSG-Net work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Signed distance field (SDF) is a prominent implicit representation of 3D\nmeshes. Methods that are based on such representation achieved state-of-the-art\n3D shape reconstruction quality. However, these methods struggle to reconstruct\nnon-convex shapes. One remedy is to incorporate a constructive solid geometry\nframework (CSG) that represents a shape as a decomposition into primitives. It\nallows to embody a 3D shape of high complexity and non-convexity with a simple\ntree representation of Boolean operations. Nevertheless, existing approaches\nare supervised and require the entire CSG parse tree that is given upfront\nduring the training process. On the contrary, we propose a model that extracts\na CSG parse tree without any supervision - UCSG-Net. Our model predicts\nparameters of primitives and binarizes their SDF representation through\ndifferentiable indicator function. It is achieved jointly with discovering the\nstructure of a Boolean operators tree. The model selects dynamically which\noperator combination over primitives leads to the reconstruction of high\nfidelity. We evaluate our method on 2D and 3D autoencoding tasks. We show that\nthe predicted parse tree representation is interpretable and can be used in CAD\nsoftware.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 12:13:37 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 07:24:14 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 17:32:15 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Kania", "Kacper", ""], ["Zi\u0119ba", "Maciej", ""], ["Kajdanowicz", "Tomasz", ""]]}, {"id": "2006.09107", "submitter": "Yordan Hristov", "authors": "Yordan Hristov, Subramanian Ramamoorthy", "title": "Learning from Demonstration with Weakly Supervised Disentanglement", "comments": "18 pages, 16 figures, accepted at the International Conference on\n  Learning Representations (ICLR) 2021, supplementary website at\n  https://sites.google.com/view/weak-label-lfd", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic manipulation tasks, such as wiping with a soft sponge, require\ncontrol from multiple rich sensory modalities. Human-robot interaction, aimed\nat teaching robots, is difficult in this setting as there is potential for\nmismatch between human and machine comprehension of the rich data streams. We\ntreat the task of interpretable learning from demonstration as an optimisation\nproblem over a probabilistic generative model. To account for the\nhigh-dimensionality of the data, a high-capacity neural network is chosen to\nrepresent the model. The latent variables in this model are explicitly aligned\nwith high-level notions and concepts that are manifested in a set of\ndemonstrations. We show that such alignment is best achieved through the use of\nlabels from the end user, in an appropriately restricted vocabulary, in\ncontrast to the conventional approach of the designer picking a prior over the\nlatent variables. Our approach is evaluated in the context of two table-top\nrobot manipulation tasks performed by a PR2 robot -- that of dabbing liquids\nwith a sponge (forcefully pressing a sponge and moving it along a surface) and\npouring between different containers. The robot provides visual information,\narm joint positions and arm joint efforts. We have made videos of the tasks and\ndata available - see supplementary materials at:\nhttps://sites.google.com/view/weak-label-lfd.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 12:29:51 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 12:15:52 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Hristov", "Yordan", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "2006.09116", "submitter": "Junting Pan", "authors": "Siyu Chen, Junting Pan, Guanglu Song, Manyuan Zhang, Hao Shao, Ziyi\n  Lin, Jing Shao, Hongsheng Li, Yu Liu", "title": "1st place solution for AVA-Kinetics Crossover in AcitivityNet Challenge\n  2020", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.07976", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report introduces our winning solution to the spatio-temporal\naction localization track, AVA-Kinetics Crossover, in ActivityNet Challenge\n2020. Our entry is mainly based on Actor-Context-Actor Relation Network. We\ndescribe technical details for the new AVA-Kinetics dataset, together with some\nexperimental results. Without any bells and whistles, we achieved 39.62 mAP on\nthe test set of AVA-Kinetics, which outperforms other entries by a large\nmargin. Code will be available at: https://github.com/Siyu-C/ACAR-Net.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 12:52:59 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Chen", "Siyu", ""], ["Pan", "Junting", ""], ["Song", "Guanglu", ""], ["Zhang", "Manyuan", ""], ["Shao", "Hao", ""], ["Lin", "Ziyi", ""], ["Shao", "Jing", ""], ["Li", "Hongsheng", ""], ["Liu", "Yu", ""]]}, {"id": "2006.09117", "submitter": "Anh Nguyen", "authors": "Anh Nguyen, Dennis Kundrat, Giulio Dagnino, Wenqiang Chi, Mohamed E.\n  M. K. Abdelaziz, Yao Guo, YingLiang Ma, Trevor M. Y. Kwok, Celia Riga, and\n  Guang-Zhong Yang", "title": "End-to-End Real-time Catheter Segmentation with Optical Flow-Guided\n  Warping during Endovascular Intervention", "comments": "ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate real-time catheter segmentation is an important pre-requisite for\nrobot-assisted endovascular intervention. Most of the existing learning-based\nmethods for catheter segmentation and tracking are only trained on small-scale\ndatasets or synthetic data due to the difficulties of ground-truth annotation.\nFurthermore, the temporal continuity in intraoperative imaging sequences is not\nfully utilised. In this paper, we present FW-Net, an end-to-end and real-time\ndeep learning framework for endovascular intervention. The proposed FW-Net has\nthree modules: a segmentation network with encoder-decoder architecture, a flow\nnetwork to extract optical flow information, and a novel flow-guided warping\nfunction to learn the frame-to-frame temporal continuity. We show that by\neffectively learning temporal continuity, the network can successfully segment\nand track the catheters in real-time sequences using only raw ground-truth for\ntraining. Detailed validation results confirm that our FW-Net outperforms\nstate-of-the-art techniques while achieving real-time performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 12:53:27 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Nguyen", "Anh", ""], ["Kundrat", "Dennis", ""], ["Dagnino", "Giulio", ""], ["Chi", "Wenqiang", ""], ["Abdelaziz", "Mohamed E. M. K.", ""], ["Guo", "Yao", ""], ["Ma", "YingLiang", ""], ["Kwok", "Trevor M. Y.", ""], ["Riga", "Celia", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "2006.09128", "submitter": "Suraj Srinivas", "authors": "Suraj Srinivas, Francois Fleuret", "title": "Rethinking the Role of Gradient-Based Attribution Methods for Model\n  Interpretability", "comments": "Oral Presentation at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current methods for the interpretability of discriminative deep neural\nnetworks commonly rely on the model's input-gradients, i.e., the gradients of\nthe output logits w.r.t. the inputs. The common assumption is that these\ninput-gradients contain information regarding $p_{\\theta} ( y \\mid x)$, the\nmodel's discriminative capabilities, thus justifying their use for\ninterpretability. However, in this work we show that these input-gradients can\nbe arbitrarily manipulated as a consequence of the shift-invariance of softmax\nwithout changing the discriminative function. This leaves an open question: if\ninput-gradients can be arbitrary, why are they highly structured and\nexplanatory in standard models?\n  We investigate this by re-interpreting the logits of standard softmax-based\nclassifiers as unnormalized log-densities of the data distribution and show\nthat input-gradients can be viewed as gradients of a class-conditional density\nmodel $p_{\\theta}(x \\mid y)$ implicit within the discriminative model. This\nleads us to hypothesize that the highly structured and explanatory nature of\ninput-gradients may be due to the alignment of this class-conditional model\n$p_{\\theta}(x \\mid y)$ with that of the ground truth data distribution\n$p_{\\text{data}} (x \\mid y)$. We test this hypothesis by studying the effect of\ndensity alignment on gradient explanations. To achieve this alignment we use\nscore-matching, and propose novel approximations to this algorithm to enable\ntraining large-scale models.\n  Our experiments show that improving the alignment of the implicit density\nmodel with the data distribution enhances gradient structure and explanatory\npower while reducing this alignment has the opposite effect. Overall, our\nfinding that input-gradients capture information regarding an implicit\ngenerative model implies that we need to re-think their use for interpreting\ndiscriminative models.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 13:17:32 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 09:42:58 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Srinivas", "Suraj", ""], ["Fleuret", "Francois", ""]]}, {"id": "2006.09134", "submitter": "Yuesong Tian", "authors": "Yuesong Tian, Li Shen, Li Shen, Guinan Su, Zhifeng Li, Wei Liu", "title": "AlphaGAN: Fully Differentiable Architecture Search for Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are formulated as minimax game\nproblems, whereby generators attempt to approach real data distributions by\nvirtue of adversarial learning against discriminators. The intrinsic problem\ncomplexity poses the challenge to enhance the performance of generative\nnetworks. In this work, we aim to boost model learning from the perspective of\nnetwork architectures, by incorporating recent progress on automated\narchitecture search into GANs. To this end, we propose a fully differentiable\nsearch framework for generative adversarial networks, dubbed alphaGAN. The\nsearching process is formalized as solving a bi-level minimax optimization\nproblem, in which the outer-level objective aims for seeking a suitable network\narchitecture towards pure Nash Equilibrium conditioned on the generator and the\ndiscriminator network parameters optimized with a traditional GAN loss in the\ninner level. The entire optimization performs a first-order method by\nalternately minimizing the two-level objective in a fully differentiable\nmanner, enabling architecture search to be completed in an enormous search\nspace. Extensive experiments on CIFAR-10 and STL-10 datasets show that our\nalgorithm can obtain high-performing architectures only with 3-GPU hours on a\nsingle GPU in the search space comprised of approximate 2 ? 1011 possible\nconfigurations. We also provide a comprehensive analysis on the behavior of the\nsearching process and the properties of searched architectures, which would\nbenefit further research on architectures for generative models. Pretrained\nmodels and codes are available at https://github.com/yuesongtian/AlphaGAN.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 13:27:30 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 08:08:18 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Tian", "Yuesong", ""], ["Shen", "Li", ""], ["Shen", "Li", ""], ["Su", "Guinan", ""], ["Li", "Zhifeng", ""], ["Liu", "Wei", ""]]}, {"id": "2006.09141", "submitter": "Javier Ferrando", "authors": "Javier Ferrando and Juan Luis Dominguez and Jordi Torres and Raul\n  Garcia and David Garcia and Daniel Garrido and Jordi Cortada and Mateo Valero", "title": "Improving accuracy and speeding up Document Image Classification through\n  parallel systems", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-50417-5_29", "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study showing the benefits of the EfficientNet models\ncompared with heavier Convolutional Neural Networks (CNNs) in the Document\nClassification task, essential problem in the digitalization process of\ninstitutions. We show in the RVL-CDIP dataset that we can improve previous\nresults with a much lighter model and present its transfer learning\ncapabilities on a smaller in-domain dataset such as Tobacco3482. Moreover, we\npresent an ensemble pipeline which is able to boost solely image input by\ncombining image model predictions with the ones generated by BERT model on\nextracted text by OCR. We also show that the batch size can be effectively\nincreased without hindering its accuracy so that the training process can be\nsped up by parallelizing throughout multiple GPUs, decreasing the computational\ntime needed. Lastly, we expose the training performance differences between\nPyTorch and Tensorflow Deep Learning frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 13:36:07 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Ferrando", "Javier", ""], ["Dominguez", "Juan Luis", ""], ["Torres", "Jordi", ""], ["Garcia", "Raul", ""], ["Garcia", "David", ""], ["Garrido", "Daniel", ""], ["Cortada", "Jordi", ""], ["Valero", "Mateo", ""]]}, {"id": "2006.09142", "submitter": "Li'an Zhuo", "authors": "Li'an Zhuo, Baochang Zhang, Linlin Yang, Hanlin Chen, Qixiang Ye,\n  David Doermann, Guodong Guo, Rongrong Ji", "title": "Cogradient Descent for Bilinear Optimization", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional learning methods simplify the bilinear model by regarding two\nintrinsically coupled factors independently, which degrades the optimization\nprocedure. One reason lies in the insufficient training due to the asynchronous\ngradient descent, which results in vanishing gradients for the coupled\nvariables. In this paper, we introduce a Cogradient Descent algorithm (CoGD) to\naddress the bilinear problem, based on a theoretical framework to coordinate\nthe gradient of hidden variables via a projection function. We solve one\nvariable by considering its coupling relationship with the other, leading to a\nsynchronous gradient descent to facilitate the optimization procedure. Our\nalgorithm is applied to solve problems with one variable under the sparsity\nconstraint, which is widely used in the learning paradigm. We validate our CoGD\nconsidering an extensive set of applications including image reconstruction,\ninpainting, and network pruning. Experiments show that it improves the\nstate-of-the-art by a significant margin.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 13:41:54 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Zhuo", "Li'an", ""], ["Zhang", "Baochang", ""], ["Yang", "Linlin", ""], ["Chen", "Hanlin", ""], ["Ye", "Qixiang", ""], ["Doermann", "David", ""], ["Guo", "Guodong", ""], ["Ji", "Rongrong", ""]]}, {"id": "2006.09158", "submitter": "Muhammad Naseer Bajwa", "authors": "Muhammad Naseer Bajwa, Gur Amrit Pal Singh, Wolfgang Neumeier,\n  Muhammad Imran Malik, Andreas Dengel, Sheraz Ahmed", "title": "G1020: A Benchmark Retinal Fundus Image Dataset for Computer-Aided\n  Glaucoma Detection", "comments": "Accepted in IJCNN-2020, 7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scarcity of large publicly available retinal fundus image datasets for\nautomated glaucoma detection has been the bottleneck for successful application\nof artificial intelligence towards practical Computer-Aided Diagnosis (CAD). A\nfew small datasets that are available for research community usually suffer\nfrom impractical image capturing conditions and stringent inclusion criteria.\nThese shortcomings in already limited choice of existing datasets make it\nchallenging to mature a CAD system so that it can perform in real-world\nenvironment. In this paper we present a large publicly available retinal fundus\nimage dataset for glaucoma classification called G1020. The dataset is curated\nby conforming to standard practices in routine ophthalmology and it is expected\nto serve as standard benchmark dataset for glaucoma detection. This database\nconsists of 1020 high resolution colour fundus images and provides ground truth\nannotations for glaucoma diagnosis, optic disc and optic cup segmentation,\nvertical cup-to-disc ratio, size of neuroretinal rim in inferior, superior,\nnasal and temporal quadrants, and bounding box location for optic disc. We also\nreport baseline results by conducting extensive experiments for automated\nglaucoma diagnosis and segmentation of optic disc and optic cup.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 14:29:03 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Bajwa", "Muhammad Naseer", ""], ["Singh", "Gur Amrit Pal", ""], ["Neumeier", "Wolfgang", ""], ["Malik", "Muhammad Imran", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "2006.09179", "submitter": "Michele Buzzicotti", "authors": "M. Buzzicotti, F. Bonaccorso, P. Clark Di Leoni, L. Biferale", "title": "Reconstruction of turbulent data with deep generative models for\n  semantic inpainting from TURB-Rot database", "comments": null, "journal-ref": "Phys. Rev. Fluids 6, 050503 (2021)", "doi": "10.1103/PhysRevFluids.6.050503", "report-no": null, "categories": "physics.flu-dyn cond-mat.stat-mech cs.CV cs.LG nlin.CD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the applicability of tools developed by the computer vision\ncommunity for features learning and semantic image inpainting to perform data\nreconstruction of fluid turbulence configurations. The aim is twofold. First,\nwe explore on a quantitative basis, the capability of Convolutional Neural\nNetworks embedded in a Deep Generative Adversarial Model (Deep-GAN) to generate\nmissing data in turbulence, a paradigmatic high dimensional chaotic system. In\nparticular, we investigate their use in reconstructing two-dimensional damaged\nsnapshots extracted from a large database of numerical configurations of 3d\nturbulence in the presence of rotation, a case with multi-scale random features\nwhere both large-scale organised structures and small-scale highly intermittent\nand non-Gaussian fluctuations are present. Second, following a reverse\nengineering approach, we aim to rank the input flow properties (features) in\nterms of their qualitative and quantitative importance to obtain a better set\nof reconstructed fields. We present two approaches both based on Context\nEncoders. The first one infers the missing data via a minimization of the L2\npixel-wise reconstruction loss, plus a small adversarial penalisation. The\nsecond searches for the closest encoding of the corrupted flow configuration\nfrom a previously trained generator. Finally, we present a comparison with a\ndifferent data assimilation tool, based on Nudging, an equation-informed\nunbiased protocol, well known in the numerical weather prediction community.\nThe TURB-Rot database, http://smart-turb.roma2.infn.it, of roughly 300K 2d\nturbulent images is released and details on how to download it are given.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 14:26:07 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 09:26:53 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Buzzicotti", "M.", ""], ["Bonaccorso", "F.", ""], ["Di Leoni", "P. Clark", ""], ["Biferale", "L.", ""]]}, {"id": "2006.09197", "submitter": "Dr. Suryansh Kumar", "authors": "Suryansh Kumar, Luc Van Gool, Carlos E. P. de Oliveira, Anoop Cherian,\n  Yuchao Dai, Hongdong Li", "title": "Dense Non-Rigid Structure from Motion: A Manifold Viewpoint", "comments": "A comprehensive version that combines our cvpr 2018 and cvpr 2019\n  work (Still under development and refinement, Initial Version). 13 Figures, 1\n  Table. arXiv admin note: text overlap with arXiv:1902.01077", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Rigid Structure-from-Motion (NRSfM) problem aims to recover 3D geometry\nof a deforming object from its 2D feature correspondences across multiple\nframes. Classical approaches to this problem assume a small number of feature\npoints and, ignore the local non-linearities of the shape deformation, and\ntherefore, struggles to reliably model non-linear deformations. Furthermore,\navailable dense NRSfM algorithms are often hurdled by scalability,\ncomputations, noisy measurements and, restricted to model just global\ndeformation. In this paper, we propose algorithms that can overcome these\nlimitations with the previous methods and, at the same time, can recover a\nreliable dense 3D structure of a non-rigid object with higher accuracy.\nAssuming that a deforming shape is composed of a union of local linear subspace\nand, span a global low-rank space over multiple frames enables us to\nefficiently model complex non-rigid deformations. To that end, each local\nlinear subspace is represented using Grassmannians and, the global 3D shape\nacross multiple frames is represented using a low-rank representation. We show\nthat our approach significantly improves accuracy, scalability, and robustness\nagainst noise. Also, our representation naturally allows for simultaneous\nreconstruction and clustering framework which in general is observed to be more\nsuitable for NRSfM problems. Our method currently achieves leading performance\non the standard benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 09:15:54 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Kumar", "Suryansh", ""], ["Van Gool", "Luc", ""], ["de Oliveira", "Carlos E. P.", ""], ["Cherian", "Anoop", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""]]}, {"id": "2006.09199", "submitter": "Andrew Rouditchenko", "authors": "Andrew Rouditchenko, Angie Boggust, David Harwath, Brian Chen, Dhiraj\n  Joshi, Samuel Thomas, Kartik Audhkhasi, Hilde Kuehne, Rameswar Panda, Rogerio\n  Feris, Brian Kingsbury, Michael Picheny, Antonio Torralba, James Glass", "title": "AVLnet: Learning Audio-Visual Language Representations from\n  Instructional Videos", "comments": "A version of this work has been accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for learning visually grounded language from videos often\nrely on text annotation, such as human generated captions or machine generated\nautomatic speech recognition (ASR) transcripts. In this work, we introduce the\nAudio-Video Language Network (AVLnet), a self-supervised network that learns a\nshared audio-visual embedding space directly from raw video inputs. To\ncircumvent the need for text annotation, we learn audio-visual representations\nfrom randomly segmented video clips and their raw audio waveforms. We train\nAVLnet on HowTo100M, a large corpus of publicly available instructional videos,\nand evaluate on image retrieval and video retrieval tasks, achieving\nstate-of-the-art performance. We perform analysis of AVLnet's learned\nrepresentations, showing our model utilizes speech and natural sounds to learn\naudio-visual concepts. Further, we propose a tri-modal model that jointly\nprocesses raw audio, video, and text captions from videos to learn a\nmulti-modal semantic embedding space useful for text-video retrieval. Our code,\ndata, and trained models will be released at avlnet.csail.mit.edu\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 14:38:03 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 18:44:50 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Rouditchenko", "Andrew", ""], ["Boggust", "Angie", ""], ["Harwath", "David", ""], ["Chen", "Brian", ""], ["Joshi", "Dhiraj", ""], ["Thomas", "Samuel", ""], ["Audhkhasi", "Kartik", ""], ["Kuehne", "Hilde", ""], ["Panda", "Rameswar", ""], ["Feris", "Rogerio", ""], ["Kingsbury", "Brian", ""], ["Picheny", "Michael", ""], ["Torralba", "Antonio", ""], ["Glass", "James", ""]]}, {"id": "2006.09205", "submitter": "Andrew Dowsey", "authors": "William Andrew, Jing Gao, Siobhan Mullan, Neill Campbell, Andrew W\n  Dowsey, Tilo Burghardt", "title": "Visual Identification of Individual Holstein-Friesian Cattle via Deep\n  Metric Learning", "comments": "41 pages, 18 figures, 2 tables; Submitted to Computers and\n  Electronics in Agriculture ; Source code and network weights available at\n  https://github.com/CWOA/MetricLearningIdentification ; OpenCows2020 dataset\n  available at https://doi.org/10.5523/bris.10m32xl88x2b61zlkkgz3fml17", "journal-ref": "Computers and Electronics in Agriculture 185, 106133 (2021)", "doi": "10.1016/j.compag.2021.106133", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holstein-Friesian cattle exhibit individually-characteristic black and white\ncoat patterns visually akin to those arising from Turing's reaction-diffusion\nsystems. This work takes advantage of these natural markings in order to\nautomate visual detection and biometric identification of individual\nHolstein-Friesians via convolutional neural networks and deep metric learning\ntechniques. Existing approaches rely on markings, tags or wearables with a\nvariety of maintenance requirements, whereas we present a totally hands-off\nmethod for the automated detection, localisation, and identification of\nindividual animals from overhead imaging in an open herd setting, i.e. where\nnew additions to the herd are identified without re-training. We propose the\nuse of SoftMax-based reciprocal triplet loss to address the identification\nproblem and evaluate the techniques in detail against fixed herd paradigms. We\nfind that deep metric learning systems show strong performance even when many\ncattle unseen during system training are to be identified and re-identified --\nachieving 93.8% accuracy when trained on just half of the population. This work\npaves the way for facilitating the non-intrusive monitoring of cattle\napplicable to precision farming and surveillance for automated productivity,\nhealth and welfare monitoring, and to veterinary research such as behavioural\nanalysis, disease outbreak tracing, and more. Key parts of the source code,\nnetwork weights and datasets are available publicly.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 14:41:55 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 11:38:09 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 10:58:30 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Andrew", "William", ""], ["Gao", "Jing", ""], ["Mullan", "Siobhan", ""], ["Campbell", "Neill", ""], ["Dowsey", "Andrew W", ""], ["Burghardt", "Tilo", ""]]}, {"id": "2006.09208", "submitter": "Hana Alghamdi", "authors": "Hana Alghamdi and Rozenn Dahyot", "title": "Iterative Nadaraya-Watson Distribution Transfer for Colour Grading", "comments": "6 pages, 6 figures, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:2005.09015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method with Nadaraya-Watson that maps one N-dimensional\ndistribution to another taking into account available information about\ncorrespondences. We extend the 2D/3D problem to higher dimensions by encoding\noverlapping neighborhoods of data points and solve the high dimensional problem\nin 1D space using an iterative projection approach. To show potentials of this\nmapping, we apply it to colour transfer between two images that exhibit\noverlapped scene. Experiments show quantitative and qualitative improvements\nover previous state of the art colour transfer methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 00:14:03 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Alghamdi", "Hana", ""], ["Dahyot", "Rozenn", ""]]}, {"id": "2006.09214", "submitter": "Chunhua Shen", "authors": "Zhi Tian, Chunhua Shen, Hao Chen, Tong He", "title": "FCOS: A simple and strong anchor-free object detector", "comments": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence. Extended version of arXiv:1904.01355; Code is at:\n  https://git.io/AdelaiDet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In computer vision, object detection is one of most important tasks, which\nunderpins a few instance-level recognition tasks and many downstream\napplications. Recently one-stage methods have gained much attention over\ntwo-stage approaches due to their simpler design and competitive performance.\nHere we propose a fully convolutional one-stage object detector (FCOS) to solve\nobject detection in a per-pixel prediction fashion, analogue to other dense\nprediction problems such as semantic segmentation. Almost all state-of-the-art\nobject detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on\npre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box\nfree, as well as proposal free. By eliminating the pre-defined set of anchor\nboxes, FCOS completely avoids the complicated computation related to anchor\nboxes such as calculating the intersection over union (IoU) scores during\ntraining. More importantly, we also avoid all hyper-parameters related to\nanchor boxes, which are often sensitive to the final detection performance.\nWith the only post-processing non-maximum suppression (NMS), we demonstrate a\nmuch simpler and flexible detection framework achieving improved detection\naccuracy. We hope that the proposed FCOS framework can serve as a simple and\nstrong alternative for many other instance-level tasks. Code and pre-trained\nmodels are available at: https://git.io/AdelaiDet\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 01:03:39 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 08:31:26 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 14:04:33 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Tian", "Zhi", ""], ["Shen", "Chunhua", ""], ["Chen", "Hao", ""], ["He", "Tong", ""]]}, {"id": "2006.09220", "submitter": "Yazan Abu Farha", "authors": "Shijie Li, Yazan Abu Farha, Yun Liu, Ming-Ming Cheng, Juergen Gall", "title": "MS-TCN++: Multi-Stage Temporal Convolutional Network for Action\n  Segmentation", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence. arXiv\n  admin note: substantial text overlap with arXiv:1903.01945", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the success of deep learning in classifying short trimmed videos, more\nattention has been focused on temporally segmenting and classifying activities\nin long untrimmed videos. State-of-the-art approaches for action segmentation\nutilize several layers of temporal convolution and temporal pooling. Despite\nthe capabilities of these approaches in capturing temporal dependencies, their\npredictions suffer from over-segmentation errors. In this paper, we propose a\nmulti-stage architecture for the temporal action segmentation task that\novercomes the limitations of the previous approaches. The first stage generates\nan initial prediction that is refined by the next ones. In each stage we stack\nseveral layers of dilated temporal convolutions covering a large receptive\nfield with few parameters. While this architecture already performs well, lower\nlayers still suffer from a small receptive field. To address this limitation,\nwe propose a dual dilated layer that combines both large and small receptive\nfields. We further decouple the design of the first stage from the refining\nstages to address the different requirements of these stages. Extensive\nevaluation shows the effectiveness of the proposed model in capturing\nlong-range dependencies and recognizing action segments. Our models achieve\nstate-of-the-art results on three datasets: 50Salads, Georgia Tech Egocentric\nActivities (GTEA), and the Breakfast dataset.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 14:50:47 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 10:18:47 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Li", "Shijie", ""], ["Farha", "Yazan Abu", ""], ["Liu", "Yun", ""], ["Cheng", "Ming-Ming", ""], ["Gall", "Juergen", ""]]}, {"id": "2006.09225", "submitter": "Hongruixuan Chen", "authors": "Hongruixuan Chen and Chen Wu and Bo Du and Liangpei Zhang", "title": "DSDANet: Deep Siamese Domain Adaptation Convolutional Neural Network for\n  Cross-domain Change Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection (CD) is one of the most vital applications in remote\nsensing. Recently, deep learning has achieved promising performance in the CD\ntask. However, the deep models are task-specific and CD data set bias often\nexists, hence it is inevitable that deep CD models would suffer degraded\nperformance after transferring it from original CD data set to new ones, making\nmanually label numerous samples in the new data set unavoidable, which costs a\nlarge amount of time and human labor. How to learn a transferable CD model in\nthe data set with enough labeled data (original domain) but can well detect\nchanges in another data set without labeled data (target domain)? This is\ndefined as the cross-domain change detection problem. In this paper, we propose\na novel deep siamese domain adaptation convolutional neural network (DSDANet)\narchitecture for cross-domain CD. In DSDANet, a siamese convolutional neural\nnetwork first extracts spatial-spectral features from multi-temporal images.\nThen, through multi-kernel maximum mean discrepancy (MK-MMD), the learned\nfeature representation is embedded into a reproducing kernel Hilbert space\n(RKHS), in which the distribution of two domains can be explicitly matched. By\noptimizing the network parameters and kernel coefficients with the source\nlabeled data and target unlabeled data, DSDANet can learn transferrable feature\nrepresentation that can bridge the discrepancy between two domains. To the best\nof our knowledge, it is the first time that such a domain adaptation-based deep\nnetwork is proposed for CD. The theoretical analysis and experimental results\ndemonstrate the effectiveness and potential of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 15:00:54 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Chen", "Hongruixuan", ""], ["Wu", "Chen", ""], ["Du", "Bo", ""], ["Zhang", "Liangpei", ""]]}, {"id": "2006.09238", "submitter": "Chen Joya", "authors": "Joya Chen, Qi Wu, Dong Liu, Tong Xu", "title": "Foreground-Background Imbalance Problem in Deep Object Detectors: A\n  Review", "comments": "Accepted by IEEE MIPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the remarkable developments made by deep learning\ntechniques for object detection, a fundamentally challenging problem of\ncomputer vision. Nevertheless, there are still difficulties in training\naccurate deep object detectors, one of which is owing to the\nforeground-background imbalance problem. In this paper, we survey the recent\nadvances about the solutions to the imbalance problem. First, we analyze the\ncharacteristics of the imbalance problem in different kinds of deep detectors,\nincluding one-stage and two-stage ones. Second, we divide the existing\nsolutions into two categories: sampling heuristics and non-sampling schemes,\nand review them in detail. Third, we experimentally compare the performance of\nsome state-of-the-art solutions on the COCO benchmark. Promising directions for\nfuture work are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 15:15:53 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Chen", "Joya", ""], ["Wu", "Qi", ""], ["Liu", "Dong", ""], ["Xu", "Tong", ""]]}, {"id": "2006.09241", "submitter": "Sheila Seidel", "authors": "Sheila W. Seidel, John Murray-Bruce, Yanting Ma, Christopher Yu,\n  William T. Freeman, and Vivek K Goyal", "title": "Two-Dimensional Non-Line-of-Sight Scene Estimation from a Single Edge\n  Occluder", "comments": "14 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Passive non-line-of-sight imaging methods are often faster and stealthier\nthan their active counterparts, requiring less complex and costly equipment.\nHowever, many of these methods exploit motion of an occluder or the hidden\nscene, or require knowledge or calibration of complicated occluders. The edge\nof a wall is a known and ubiquitous occluding structure that may be used as an\naperture to image the region hidden behind it. Light from around the corner is\ncast onto the floor forming a fan-like penumbra rather than a sharp shadow.\nSubtle variations in the penumbra contain a remarkable amount of information\nabout the hidden scene. Previous work has leveraged the vertical nature of the\nedge to demonstrate 1D (in angle measured around the corner) reconstructions of\nmoving and stationary hidden scenery from as little as a single photograph of\nthe penumbra. In this work, we introduce a second reconstruction dimension:\nrange measured from the edge. We derive a new forward model, accounting for\nradial falloff, and propose two inversion algorithms to form 2D reconstructions\nfrom a single photograph of the penumbra. Performances of both algorithms are\ndemonstrated on experimental data corresponding to several different hidden\nscene configurations. A Cramer-Rao bound analysis further demonstrates the\nfeasibility (and utility) of the 2D corner camera.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 15:19:20 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Seidel", "Sheila W.", ""], ["Murray-Bruce", "John", ""], ["Ma", "Yanting", ""], ["Yu", "Christopher", ""], ["Freeman", "William T.", ""], ["Goyal", "Vivek K", ""]]}, {"id": "2006.09243", "submitter": "Kunal Swami", "authors": "Kunal Swami, Prasanna Vishnu Bondada, Pankaj Kumar Bajpai", "title": "AcED: Accurate and Edge-consistent Monocular Depth Estimation", "comments": "Accepted in IEEE ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image depth estimation is a challenging problem. The current\nstate-of-the-art method formulates the problem as that of ordinal regression.\nHowever, the formulation is not fully differentiable and depth maps are not\ngenerated in an end-to-end fashion. The method uses a na\\\"ive threshold\nstrategy to determine per-pixel depth labels, which results in significant\ndiscretization errors. For the first time, we formulate a fully differentiable\nordinal regression and train the network in end-to-end fashion. This enables us\nto include boundary and smoothness constraints in the optimization function,\nleading to smooth and edge-consistent depth maps. A novel per-pixel confidence\nmap computation for depth refinement is also proposed. Extensive evaluation of\nthe proposed model on challenging benchmarks reveals its superiority over\nrecent state-of-the-art methods, both quantitatively and qualitatively.\nAdditionally, we demonstrate practical utility of the proposed method for\nsingle camera bokeh solution using in-house dataset of challenging real-life\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 15:21:00 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Swami", "Kunal", ""], ["Bondada", "Prasanna Vishnu", ""], ["Bajpai", "Pankaj Kumar", ""]]}, {"id": "2006.09261", "submitter": "Alex Nowak-Vila", "authors": "Thomas Eboli, Alex Nowak-Vila, Jian Sun, Francis Bach, Jean Ponce,\n  Alessandro Rudi", "title": "Structured and Localized Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to image restoration that leverages ideas from\nlocalized structured prediction and non-linear multi-task learning. We optimize\na penalized energy function regularized by a sum of terms measuring the\ndistance between patches to be restored and clean patches from an external\ndatabase gathered beforehand. The resulting estimator comes with strong\nstatistical guarantees leveraging local dependency properties of overlapping\npatches. We derive the corresponding algorithms for energies based on the\nmean-squared and Euclidean norm errors. Finally, we demonstrate the practical\neffectiveness of our model on different image restoration problems using\nstandard benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 15:43:12 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Eboli", "Thomas", ""], ["Nowak-Vila", "Alex", ""], ["Sun", "Jian", ""], ["Bach", "Francis", ""], ["Ponce", "Jean", ""], ["Rudi", "Alessandro", ""]]}, {"id": "2006.09275", "submitter": "Stephan Eismann", "authors": "Stephan Eismann, Raphael J.L. Townshend, Nathaniel Thomas, Milind\n  Jagota, Bowen Jing, Ron O. Dror", "title": "Hierarchical, rotation-equivariant neural networks to select structural\n  models of protein complexes", "comments": "11 pages, 5 figures + SI: Updated based on the published version in\n  PROTEINS. Presented at NeurIPS 2019 workshop Learning Meaningful\n  Representations of Life", "journal-ref": null, "doi": "10.1002/prot.26033", "report-no": null, "categories": "q-bio.BM cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the structure of multi-protein complexes is a grand challenge in\nbiochemistry, with major implications for basic science and drug discovery.\nComputational structure prediction methods generally leverage pre-defined\nstructural features to distinguish accurate structural models from less\naccurate ones. This raises the question of whether it is possible to learn\ncharacteristics of accurate models directly from atomic coordinates of protein\ncomplexes, with no prior assumptions. Here we introduce a machine learning\nmethod that learns directly from the 3D positions of all atoms to identify\naccurate models of protein complexes, without using any pre-computed\nphysics-inspired or statistical terms. Our neural network architecture combines\nmultiple ingredients that together enable end-to-end learning from molecular\nstructures containing tens of thousands of atoms: a point-based representation\nof atoms, equivariance with respect to rotation and translation, local\nconvolutions, and hierarchical subsampling operations. When used in combination\nwith previously developed scoring functions, our network substantially improves\nthe identification of accurate structural models among a large set of possible\nmodels. Our network can also be used to predict the accuracy of a given\nstructural model in absolute terms. The architecture we present is readily\napplicable to other tasks involving learning on 3D structures of large atomic\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 20:17:12 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 00:47:10 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Eismann", "Stephan", ""], ["Townshend", "Raphael J. L.", ""], ["Thomas", "Nathaniel", ""], ["Jagota", "Milind", ""], ["Jing", "Bowen", ""], ["Dror", "Ron O.", ""]]}, {"id": "2006.09276", "submitter": "Hawzhin Mohammed", "authors": "Hawzhin Mohammed, Tolulope A. Odetola, Syed Rafay Hasan", "title": "How Secure is Distributed Convolutional Neural Network on IoT Edge\n  Devices?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) has found successful adoption in many\napplications. The deployment of CNN on resource-constrained edge devices have\nproved challenging. CNN distributed deployment across different edge devices\nhas been adopted. In this paper, we propose Trojan attacks on CNN deployed\nacross a distributed edge network across different nodes. We propose five\nstealthy attack scenarios for distributed CNN inference. These attacks are\ndivided into trigger and payload circuitry. These attacks are tested on deep\nlearning models (LeNet, AlexNet). The results show how the degree of\nvulnerability of individual layers and how critical they are to the final\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 16:10:09 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Mohammed", "Hawzhin", ""], ["Odetola", "Tolulope A.", ""], ["Hasan", "Syed Rafay", ""]]}, {"id": "2006.09299", "submitter": "Florian Lemaitre", "authors": "Florian Lemaitre and Lionel Lacassagne", "title": "A New Run-based Connected Component Labeling for Efficiently Analyzing\n  and Processing Holes", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a new connected component labeling and analysis\nalgorithm for foreground and background labeling that computes the adjacency\ntree. The computation of features (bounding boxes, first statistical moments,\nEuler number) is done on-the-fly. The transitive closure enables an efficient\nhole processing that can be filled while their features are merged with the\nsurrounding connected component without the need to rescan the image. A\ncomparison with existing algorithms shows that this new algorithm can do all\nthese computations faster than algorithms processing black and white\ncomponents.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 16:40:00 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Lemaitre", "Florian", ""], ["Lacassagne", "Lionel", ""]]}, {"id": "2006.09303", "submitter": "Ying Qu", "authors": "Ying Qu, Razieh Kaviani Baghbaderani, Hairong Qi, Chiman Kwan", "title": "Unsupervised Pansharpening Based on Self-Attention Mechanism", "comments": "Accepted by TGRS", "journal-ref": null, "doi": "10.1109/TGRS.2020.3009207", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pansharpening is to fuse a multispectral image (MSI) of\nlow-spatial-resolution (LR) but rich spectral characteristics with a\npanchromatic image (PAN) of high-spatial-resolution (HR) but poor spectral\ncharacteristics. Traditional methods usually inject the extracted\nhigh-frequency details from PAN into the up-sampled MSI. Recent deep learning\nendeavors are mostly supervised assuming the HR MSI is available, which is\nunrealistic especially for satellite images. Nonetheless, these methods could\nnot fully exploit the rich spectral characteristics in the MSI. Due to the wide\nexistence of mixed pixels in satellite images where each pixel tends to cover\nmore than one constituent material, pansharpening at the subpixel level becomes\nessential. In this paper, we propose an unsupervised pansharpening (UP) method\nin a deep-learning framework to address the above challenges based on the\nself-attention mechanism (SAM), referred to as UP-SAM. The contribution of this\npaper is three-fold. First, the self-attention mechanism is proposed where the\nspatial varying detail extraction and injection functions are estimated\naccording to the attention representations indicating spectral characteristics\nof the MSI with sub-pixel accuracy. Second, such attention representations are\nderived from mixed pixels with the proposed stacked attention network powered\nwith a stick-breaking structure to meet the physical constraints of mixed pixel\nformulations. Third, the detail extraction and injection functions are spatial\nvarying based on the attention representations, which largely improves the\nreconstruction accuracy. Extensive experimental results demonstrate that the\nproposed approach is able to reconstruct sharper MSI of different types, with\nmore details and less spectral distortion as compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 16:46:30 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 11:30:07 GMT"}, {"version": "v3", "created": "Sun, 30 Aug 2020 11:48:18 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Qu", "Ying", ""], ["Baghbaderani", "Razieh Kaviani", ""], ["Qi", "Hairong", ""], ["Kwan", "Chiman", ""]]}, {"id": "2006.09306", "submitter": "Roozbeh Mottaghi", "authors": "Martin Lohmann, Jordi Salvador, Aniruddha Kembhavi, Roozbeh Mottaghi", "title": "Learning About Objects by Learning to Interact with Them", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the remarkable progress in computer vision has been focused around\nfully supervised learning mechanisms relying on highly curated datasets for a\nvariety of tasks. In contrast, humans often learn about their world with little\nto no external supervision. Taking inspiration from infants learning from their\nenvironment through play and interaction, we present a computational framework\nto discover objects and learn their physical properties along this paradigm of\nLearning from Interaction. Our agent, when placed within the near\nphoto-realistic and physics-enabled AI2-THOR environment, interacts with its\nworld and learns about objects, their geometric extents and relative masses,\nwithout any external guidance. Our experiments reveal that this agent learns\nefficiently and effectively; not just for objects it has interacted with\nbefore, but also for novel instances from seen categories as well as novel\nobject categories.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 16:47:50 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 23:46:17 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Lohmann", "Martin", ""], ["Salvador", "Jordi", ""], ["Kembhavi", "Aniruddha", ""], ["Mottaghi", "Roozbeh", ""]]}, {"id": "2006.09308", "submitter": "Rakshith Sathish", "authors": "Rakshith Sathish, Rachana Sathish, Ramanathan Sethuraman and Debdoot\n  Sheet", "title": "Lung Segmentation and Nodule Detection in Computed Tomography Scan using\n  a Convolutional Neural Network Trained Adversarially using Turing Test Loss", "comments": "Accepted at 42nd Annual International Conferences of the IEEE\n  Engineering in Medicine and Biology Society (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Lung cancer is the most common form of cancer found worldwide with a high\nmortality rate. Early detection of pulmonary nodules by screening with a\nlow-dose computed tomography (CT) scan is crucial for its effective clinical\nmanagement. Nodules which are symptomatic of malignancy occupy about 0.0125 -\n0.025\\% of volume in a CT scan of a patient. Manual screening of all slices is\na tedious task and presents a high risk of human errors. To tackle this problem\nwe propose a computationally efficient two stage framework. In the first stage,\na convolutional neural network (CNN) trained adversarially using Turing test\nloss segments the lung region. In the second stage, patches sampled from the\nsegmented region are then classified to detect the presence of nodules. The\nproposed method is experimentally validated on the LUNA16 challenge dataset\nwith a dice coefficient of $0.984\\pm0.0007$ for 10-fold cross-validation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 16:51:53 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Sathish", "Rakshith", ""], ["Sathish", "Rachana", ""], ["Sethuraman", "Ramanathan", ""], ["Sheet", "Debdoot", ""]]}, {"id": "2006.09310", "submitter": "Levi McClenny", "authors": "Levi McClenny, Mulugeta Haile, Vahid Attari, Brian Sadler, Ulisses\n  Braga-Neto, Raymundo Arroyave", "title": "Deep Multimodal Transfer-Learned Regression in Data-Poor Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world applications of deep learning, estimation of a target may\nrely on various types of input data modes, such as audio-video, image-text,\netc. This task can be further complicated by a lack of sufficient data. Here we\npropose a Deep Multimodal Transfer-Learned Regressor (DMTL-R) for multimodal\nlearning of image and feature data in a deep regression architecture effective\nat predicting target parameters in data-poor domains. Our model is capable of\nfine-tuning a given set of pre-trained CNN weights on a small amount of\ntraining image data, while simultaneously conditioning on feature information\nfrom a complimentary data mode during network training, yielding more accurate\nsingle-target or multi-target regression than can be achieved using the images\nor the features alone. We present results using phase-field simulation\nmicrostructure images with an accompanying set of physical features, using\npre-trained weights from various well-known CNN architectures, which\ndemonstrate the efficacy of the proposed multimodal approach.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 16:52:44 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["McClenny", "Levi", ""], ["Haile", "Mulugeta", ""], ["Attari", "Vahid", ""], ["Sadler", "Brian", ""], ["Braga-Neto", "Ulisses", ""], ["Arroyave", "Raymundo", ""]]}, {"id": "2006.09322", "submitter": "Dominik L. Michels Ph.D.", "authors": "Jonathan Klein, S\\\"oren Pirk, Dominik L. Michels", "title": "Domain Adaptation with Morphologic Segmentation", "comments": "This work has been supported by KAUST under individual baseline\n  funding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel domain adaptation framework that uses morphologic\nsegmentation to translate images from arbitrary input domains (real and\nsynthetic) into a uniform output domain. Our framework is based on an\nestablished image-to-image translation pipeline that allows us to first\ntransform the input image into a generalized representation that encodes\nmorphology and semantics - the edge-plus-segmentation map (EPS) - which is then\ntransformed into an output domain. Images transformed into the output domain\nare photo-realistic and free of artifacts that are commonly present across\ndifferent real (e.g. lens flare, motion blur, etc.) and synthetic (e.g.\nunrealistic textures, simplified geometry, etc.) data sets. Our goal is to\nestablish a preprocessing step that unifies data from multiple sources into a\ncommon representation that facilitates training downstream tasks in computer\nvision. This way, neural networks for existing tasks can be trained on a larger\nvariety of training data, while they are also less affected by overfitting to\nspecific data sets. We showcase the effectiveness of our approach by\nqualitatively and quantitatively evaluating our method on four data sets of\nsimulated and real data of urban scenes. Additional results can be found on the\nproject website available at http://jonathank.de/research/eps/ .\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 17:06:02 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Klein", "Jonathan", ""], ["Pirk", "S\u00f6ren", ""], ["Michels", "Dominik L.", ""]]}, {"id": "2006.09332", "submitter": "Yuval Bahat", "authors": "Yuval Bahat and Tomer Michaeli", "title": "What's in the Image? Explorable Decoding of Compressed Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-growing amounts of visual contents captured on a daily basis\nnecessitate the use of lossy compression methods in order to save storage space\nand transmission bandwidth. While extensive research efforts are devoted to\nimproving compression techniques, every method inevitably discards information.\nEspecially at low bit rates, this information often corresponds to semantically\nmeaningful visual cues, so that decompression involves significant ambiguity.\nIn spite of this fact, existing decompression algorithms typically produce only\na single output, and do not allow the viewer to explore the set of images that\nmap to the given compressed code. In this work we propose the first image\ndecompression method to facilitate user-exploration of the diverse set of\nnatural images that could have given rise to the compressed input code, thus\ngranting users the ability to determine what could and what could not have been\nthere in the original scene. Specifically, we develop a novel deep-network\nbased decoder architecture for the ubiquitous JPEG standard, which allows\ntraversing the set of decompressed images that are consistent with the\ncompressed JPEG file. To allow for simple user interaction, we develop a\ngraphical user interface comprising several intuitive exploration tools,\nincluding an automatic tool for examining specific solutions of interest. We\nexemplify our framework on graphical, medical and forensic use cases,\ndemonstrating its wide range of potential applications.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 17:15:44 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 09:01:59 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bahat", "Yuval", ""], ["Michaeli", "Tomer", ""]]}, {"id": "2006.09348", "submitter": "Sivabalan Manivasagam", "authors": "Sivabalan Manivasagam, Shenlong Wang, Kelvin Wong, Wenyuan Zeng,\n  Mikita Sazanovich, Shuhan Tan, Bin Yang, Wei-Chiu Ma, Raquel Urtasun", "title": "LiDARsim: Realistic LiDAR Simulation by Leveraging the Real World", "comments": "CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of producing realistic simulations of LiDAR point\nclouds, the sensor of preference for most self-driving vehicles. We argue that,\nby leveraging real data, we can simulate the complex world more realistically\ncompared to employing virtual worlds built from CAD/procedural models. Towards\nthis goal, we first build a large catalog of 3D static maps and 3D dynamic\nobjects by driving around several cities with our self-driving fleet. We can\nthen generate scenarios by selecting a scene from our catalog and \"virtually\"\nplacing the self-driving vehicle (SDV) and a set of dynamic objects from the\ncatalog in plausible locations in the scene. To produce realistic simulations,\nwe develop a novel simulator that captures both the power of physics-based and\nlearning-based simulation. We first utilize ray casting over the 3D scene and\nthen use a deep neural network to produce deviations from the physics-based\nsimulation, producing realistic LiDAR point clouds. We showcase LiDARsim's\nusefulness for perception algorithms-testing on long-tail events and end-to-end\nclosed-loop evaluation on safety-critical scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 17:44:35 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Manivasagam", "Sivabalan", ""], ["Wang", "Shenlong", ""], ["Wong", "Kelvin", ""], ["Zeng", "Wenyuan", ""], ["Sazanovich", "Mikita", ""], ["Tan", "Shuhan", ""], ["Yang", "Bin", ""], ["Ma", "Wei-Chiu", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2006.09363", "submitter": "Leslie Smith", "authors": "Leslie N. Smith, Adam Conovaloff", "title": "Building One-Shot Semi-supervised (BOSS) Learning up to Fully Supervised\n  Performance", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reaching the performance of fully supervised learning with unlabeled data and\nonly labeling one sample per class might be ideal for deep learning\napplications. We demonstrate for the first time the potential for building\none-shot semi-supervised (BOSS) learning on Cifar-10 and SVHN up to attain test\naccuracies that are comparable to fully supervised learning. Our method\ncombines class prototype refining, class balancing, and self-training. A good\nprototype choice is essential and we propose a technique for obtaining iconic\nexamples. In addition, we demonstrate that class balancing methods\nsubstantially improve accuracy results in semi-supervised learning to levels\nthat allow self-training to reach the level of fully supervised learning\nperformance. Rigorous empirical evaluations provide evidence that labeling\nlarge datasets is not necessary for training deep neural networks. We made our\ncode available at https://github.com/lnsmith54/BOSS to facilitate replication\nand for use with future real-world applications.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 17:56:00 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 18:31:29 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Smith", "Leslie N.", ""], ["Conovaloff", "Adam", ""]]}, {"id": "2006.09367", "submitter": "Devendra Singh Chaplot", "authors": "Devendra Singh Chaplot, Helen Jiang, Saurabh Gupta, Abhinav Gupta", "title": "Semantic Curiosity for Active Visual Learning", "comments": "See project webpage at\n  https://devendrachaplot.github.io/projects/SemanticCuriosity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the task of embodied interactive learning for object\ndetection. Given a set of environments (and some labeling budget), our goal is\nto learn an object detector by having an agent select what data to obtain\nlabels for. How should an exploration policy decide which trajectory should be\nlabeled? One possibility is to use a trained object detector's failure cases as\nan external reward. However, this will require labeling millions of frames\nrequired for training RL policies, which is infeasible. Instead, we explore a\nself-supervised approach for training our exploration policy by introducing a\nnotion of semantic curiosity. Our semantic curiosity policy is based on a\nsimple observation -- the detection outputs should be consistent. Therefore,\nour semantic curiosity rewards trajectories with inconsistent labeling behavior\nand encourages the exploration policy to explore such areas. The exploration\npolicy trained via semantic curiosity generalizes to novel scenes and helps\ntrain an object detector that outperforms baselines trained with other possible\nalternatives such as random exploration, prediction-error curiosity, and\ncoverage-maximizing exploration.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 17:59:24 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Chaplot", "Devendra Singh", ""], ["Jiang", "Helen", ""], ["Gupta", "Saurabh", ""], ["Gupta", "Abhinav", ""]]}, {"id": "2006.09373", "submitter": "Peijie Chen", "authors": "Peijie Chen, Chirag Agarwal, Anh Nguyen", "title": "The shape and simplicity biases of adversarially robust ImageNet-trained\n  CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training has been the topic of dozens of studies and a leading\nmethod for defending against adversarial attacks. Yet, it remains largely\nunknown (a) how adversarially-robust ImageNet classifiers (R classifiers)\ngeneralize to out-of-distribution examples; and (b) how their generalization\ncapability relates to their hidden representations. In this paper, we perform a\nthorough, systematic study to answer these two questions across AlexNet,\nGoogLeNet, and ResNet-50 architectures. We found that while standard ImageNet\nclassifiers have a strong texture bias, their R counterparts rely heavily on\nshapes. Remarkably, adversarial training induces three simplicity biases into\nhidden neurons in the process of 'robustifying' the network. That is, each\nconvolutional neuron in R networks often changes to detecting (1) pixel-wise\nsmoother patterns i.e. a mechanism that blocks high-frequency noise from\npassing through the network; (2) more lower-level features i.e. textures and\ncolors (instead of objects); and (3) fewer types of inputs. Our findings reveal\nthe interesting mechanisms that made networks more adversarially robust and\nalso explain some recent findings. Our findings reveal the interesting\nmechanisms that made networks more adversarially robust and also explain some\nrecent findings e.g. why R networks benefit from much larger capacity (Xie and\nYuille, 2020) and can act as a strong image prior in image synthesis (Santurkar\net al., 2019).\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 16:38:16 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 19:11:02 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 13:57:59 GMT"}, {"version": "v4", "created": "Wed, 16 Jun 2021 00:09:37 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Chen", "Peijie", ""], ["Agarwal", "Chirag", ""], ["Nguyen", "Anh", ""]]}, {"id": "2006.09450", "submitter": "Burhaneddin Yaman", "authors": "Burhaneddin Yaman, Seyed Amir Hossein Hosseini, Mehmet Ak\\c{c}akaya", "title": "Noise2Inpaint: Learning Referenceless Denoising by Inpainting Unrolling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based image denoising methods have been recently popular due to\ntheir improved performance. Traditionally, these methods are trained in a\nsupervised manner, requiring a set of noisy input and clean target image pairs.\nMore recently, self-supervised approaches have been proposed to learn denoising\nfrom only noisy images. These methods assume that noise across pixels is\nstatistically independent, and the underlying image pixels show spatial\ncorrelations across neighborhoods. These methods rely on a masking approach\nthat divides the image pixels into two disjoint sets, where one is used as\ninput to the network while the other is used to define the loss. However, these\nprevious self-supervised approaches rely on a purely data-driven regularization\nneural network without explicitly taking the masking model into account. In\nthis work, building on these self-supervised approaches, we introduce\nNoise2Inpaint (N2I), a training approach that recasts the denoising problem\ninto a regularized image inpainting framework. This allows us to use an\nobjective function, which can incorporate different statistical properties of\nthe noise as needed. We use algorithm unrolling to unroll an iterative\noptimization for solving this objective function and train the unrolled network\nend-to-end. The training paradigm follows the masking approach from previous\nworks, splitting the pixels into two disjoint sets. Importantly, one of these\nis now used to impose data fidelity in the unrolled network, while the other\nstill defines the loss. We demonstrate that N2I performs successful denoising\non real-world datasets, while better preserving details compared to its purely\ndata-driven counterpart Noise2Self.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 18:46:42 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 16:56:54 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Yaman", "Burhaneddin", ""], ["Hosseini", "Seyed Amir Hossein", ""], ["Ak\u00e7akaya", "Mehmet", ""]]}, {"id": "2006.09454", "submitter": "Wenxing Hu", "authors": "Wenxing Hu, Xianghe Meng, Yuntong Bai, Aiying Zhang, Biao Cai, Gemeng\n  Zhang, Tony W. Wilson, Julia M. Stephen, Vince D. Calhoun, Yu-Ping Wang", "title": "Interpretable multimodal fusion networks reveal mechanisms of brain\n  cognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal fusion benefits disease diagnosis by providing a more\ncomprehensive perspective. Developing algorithms is challenging due to data\nheterogeneity and the complex within- and between-modality associations.\nDeep-network-based data-fusion models have been developed to capture the\ncomplex associations and the performance in diagnosis has been improved\naccordingly. Moving beyond diagnosis prediction, evaluation of disease\nmechanisms is critically important for biomedical research. Deep-network-based\ndata-fusion models, however, are difficult to interpret, bringing about\ndifficulties for studying biological mechanisms. In this work, we develop an\ninterpretable multimodal fusion model, namely gCAM-CCL, which can perform\nautomated diagnosis and result interpretation simultaneously. The gCAM-CCL\nmodel can generate interpretable activation maps, which quantify pixel-level\ncontributions of the input features. This is achieved by combining intermediate\nfeature maps using gradient-based weights. Moreover, the estimated activation\nmaps are class-specific, and the captured cross-data associations are\ninterest/label related, which further facilitates class-specific analysis and\nbiological mechanism analysis. We validate the gCAM-CCL model on a brain\nimaging-genetic study, and show gCAM-CCL's performed well for both\nclassification and mechanism analysis. Mechanism analysis suggests that during\ntask-fMRI scans, several object recognition related regions of interests (ROIs)\nare first activated and then several downstream encoding ROIs get involved.\nResults also suggest that the higher cognition performing group may have\nstronger neurotransmission signaling while the lower cognition performing group\nmay have problem in brain/neuron development, resulting from genetic\nvariations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 18:52:50 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Hu", "Wenxing", ""], ["Meng", "Xianghe", ""], ["Bai", "Yuntong", ""], ["Zhang", "Aiying", ""], ["Cai", "Biao", ""], ["Zhang", "Gemeng", ""], ["Wilson", "Tony W.", ""], ["Stephen", "Julia M.", ""], ["Calhoun", "Vince D.", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "2006.09501", "submitter": "Rajesh Kumar", "authors": "Vishaal Udandarao and Mohit Agrawal and Rajesh Kumar and Rajiv Ratn\n  Shah", "title": "On the Inference of Soft Biometrics from Typing Patterns Collected in a\n  Multi-device Environment", "comments": "The first two authors contributed equally. The code is available upon\n  request. Please contact the last author", "journal-ref": "The Sixth IEEE International Conference on Multimedia Big Data,\n  August 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the inference of gender, major/minor (computer\nscience, non-computer science), typing style, age, and height from the typing\npatterns collected from 117 individuals in a multi-device environment. The\ninference of the first three identifiers was considered as classification\ntasks, while the rest as regression tasks. For classification tasks, we\nbenchmark the performance of six classical machine learning (ML) and four deep\nlearning (DL) classifiers. On the other hand, for regression tasks, we\nevaluated three ML and four DL-based regressors. The overall experiment\nconsisted of two text-entry (free and fixed) and four device (Desktop, Tablet,\nPhone, and Combined) configurations. The best arrangements achieved accuracies\nof 96.15%, 93.02%, and 87.80% for typing style, gender, and major/minor,\nrespectively, and mean absolute errors of 1.77 years and 2.65 inches for age\nand height, respectively. The results are promising considering the variety of\napplication scenarios that we have listed in this work.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 20:25:58 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Udandarao", "Vishaal", ""], ["Agrawal", "Mohit", ""], ["Kumar", "Rajesh", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2006.09504", "submitter": "Shailja Thakur", "authors": "Shailja Thakur, Sebastian Fischmeister", "title": "A generalizable saliency map-based interpretation of model outcome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the significant challenges of deep neural networks is that the complex\nnature of the network prevents human comprehension of the outcome of the\nnetwork. Consequently, the applicability of complex machine learning models is\nlimited in the safety-critical domains, which incurs risk to life and property.\nTo fully exploit the capabilities of complex neural networks, we propose a\nnon-intrusive interpretability technique that uses the input and output of the\nmodel to generate a saliency map. The method works by empirically optimizing a\nrandomly initialized input mask by localizing and weighing individual pixels\naccording to their sensitivity towards the target class. Our experiments show\nthat the proposed model interpretability approach performs better than the\nexisting saliency map-based approaches methods at localizing the relevant input\npixels.\n  Furthermore, to obtain a global perspective on the target-specific\nexplanation, we propose a saliency map reconstruction approach to generate\nacceptable variations of the salient inputs from the space of input data\ndistribution for which the model outcome remains unaltered. Experiments show\nthat our interpretability method can reconstruct the salient part of the input\nwith a classification accuracy of 89%.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 20:34:42 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 04:51:53 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Thakur", "Shailja", ""], ["Fischmeister", "Sebastian", ""]]}, {"id": "2006.09510", "submitter": "Sergey Bochkanov", "authors": "Sergey Bochkanov", "title": "On sparse connectivity, adversarial robustness, and a novel model of the\n  artificial neuron", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved human-level accuracy on almost all\nperceptual benchmarks. It is interesting that these advances were made using\ntwo ideas that are decades old: (a) an artificial neuron based on a linear\nsummator and (b) SGD training.\n  However, there are important metrics beyond accuracy: computational\nefficiency and stability against adversarial perturbations. In this paper, we\npropose two closely connected methods to improve these metrics on contour\nrecognition tasks: (a) a novel model of an artificial neuron, a \"strong\nneuron,\" with low hardware requirements and inherent robustness against\nadversarial perturbations and (b) a novel constructive training algorithm that\ngenerates sparse networks with $O(1)$ connections per neuron.\n  We demonstrate the feasibility of our approach through experiments on SVHN\nand GTSRB benchmarks. We achieved an impressive 10x-100x reduction in\noperations count (10x when compared with other sparsification approaches, 100x\nwhen compared with dense networks) and a substantial reduction in hardware\nrequirements (8-bit fixed-point math was used) with no reduction in model\naccuracy. Superior stability against adversarial perturbations (exceeding that\nof adversarial training) was achieved without any counteradversarial measures,\nrelying on the robustness of strong neurons alone. We also proved that\nconstituent blocks of our strong neuron are the only activation functions with\nperfect stability against adversarial attacks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 20:45:08 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Bochkanov", "Sergey", ""]]}, {"id": "2006.09512", "submitter": "Zhiqiu Lin", "authors": "Zhiqiu Lin, Jin Sun, Abe Davis, Noah Snavely", "title": "Visual Chirality", "comments": "Published at CVPR 2020, Best Paper Nomination, Oral Presentation.\n  Project Page: https://linzhiqiu.github.io/papers/chirality/", "journal-ref": "CVPR (2020), 12292-12300", "doi": "10.1109/CVPR42600.2020.01231", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we tell whether an image has been mirrored? While we understand the\ngeometry of mirror reflections very well, less has been said about how it\naffects distributions of imagery at scale, despite widespread use for data\naugmentation in computer vision. In this paper, we investigate how the\nstatistics of visual data are changed by reflection. We refer to these changes\nas \"visual chirality\", after the concept of geometric chirality - the notion of\nobjects that are distinct from their mirror image. Our analysis of visual\nchirality reveals surprising results, including low-level chiral signals\npervading imagery stemming from image processing in cameras, to the ability to\ndiscover visual chirality in images of people and faces. Our work has\nimplications for data augmentation, self-supervised learning, and image\nforensics.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 20:48:23 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Lin", "Zhiqiu", ""], ["Sun", "Jin", ""], ["Davis", "Abe", ""], ["Snavely", "Noah", ""]]}, {"id": "2006.09562", "submitter": "Federico Baldassarre", "authors": "Federico Baldassarre, Kevin Smith, Josephine Sullivan, Hossein\n  Azizpour", "title": "Explanation-based Weakly-supervised Learning of Visual Relations with\n  Graph Networks", "comments": "Published at the European Conference on Computer Vision, ECCV 2020\n  (Poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual relationship detection is fundamental for holistic image\nunderstanding. However, the localization and classification of (subject,\npredicate, object) triplets remain challenging tasks, due to the combinatorial\nexplosion of possible relationships, their long-tailed distribution in natural\nimages, and an expensive annotation process. This paper introduces a novel\nweakly-supervised method for visual relationship detection that relies on\nminimal image-level predicate labels. A graph neural network is trained to\nclassify predicates in images from a graph representation of detected objects,\nimplicitly encoding an inductive bias for pairwise relations. We then frame\nrelationship detection as the explanation of such a predicate classifier, i.e.\nwe obtain a complete relation by recovering the subject and object of a\npredicted predicate. We present results comparable to recent fully- and\nweakly-supervised methods on three diverse and challenging datasets: HICO-DET\nfor human-object interaction, Visual Relationship Detection for generic\nobject-to-object relations, and UnRel for unusual triplets; demonstrating\nrobustness to non-comprehensive annotations and good few-shot generalization.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 23:14:52 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 21:01:44 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Baldassarre", "Federico", ""], ["Smith", "Kevin", ""], ["Sullivan", "Josephine", ""], ["Azizpour", "Hossein", ""]]}, {"id": "2006.09565", "submitter": "Peizhao Li", "authors": "Peizhao Li, Zhengming Ding, Hongfu Liu", "title": "Mining Label Distribution Drift in Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation targets to transfer task knowledge from\nlabeled source domain to related yet unlabeled target domain, and is catching\nextensive interests from academic and industrial areas. Although tremendous\nefforts along this direction have been made to minimize the domain divergence,\nunfortunately, most of existing methods only manage part of the picture by\naligning feature representations from different domains. Beyond the discrepancy\nin feature space, the gap between known source label and unknown target label\ndistribution, recognized as label distribution drift, is another crucial factor\nraising domain divergence, and has not been paid enough attention and well\nexplored. From this point, in this paper, we first experimentally reveal how\nlabel distribution drift brings negative effects on current domain adaptation\nmethods. Next, we propose Label distribution Matching Domain Adversarial\nNetwork (LMDAN) to handle data distribution shift and label distribution drift\njointly. In LMDAN, label distribution drift problem is addressed by the\nproposed source samples weighting strategy, which select samples to contribute\nto positive adaptation and avoid negative effects brought by the mismatched in\nlabel distribution. Finally, different from general domain adaptation\nexperiments, we modify domain adaptation datasets to create the considerable\nlabel distribution drift between source and target domain. Numerical results\nand empirical model analysis show that LMDAN delivers superior performance\ncompared to other state-of-the-art domain adaptation methods under such\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 23:41:42 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Li", "Peizhao", ""], ["Ding", "Zhengming", ""], ["Liu", "Hongfu", ""]]}, {"id": "2006.09597", "submitter": "Jieming Zhou", "authors": "Jieming Zhou, Soumava Kumar Roy, Pengfei Fang, Mehrtash Harandi, Lars\n  Petersson", "title": "Cross-Correlated Attention Networks for Person Re-Identification", "comments": "Accepted by Image and Vision Computing", "journal-ref": "Image and Vision Computing, Vol. 100, 2020, p. 103931", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks need to make robust inference in the presence of\nocclusion, background clutter, pose and viewpoint variations -- to name a few\n-- when the task of person re-identification is considered. Attention\nmechanisms have recently proven to be successful in handling the aforementioned\nchallenges to some degree. However previous designs fail to capture inherent\ninter-dependencies between the attended features; leading to restricted\ninteractions between the attention blocks. In this paper, we propose a new\nattention module called Cross-Correlated Attention (CCA); which aims to\novercome such limitations by maximizing the information gain between different\nattended regions. Moreover, we also propose a novel deep network that makes use\nof different attention mechanisms to learn robust and discriminative\nrepresentations of person images. The resulting model is called the\nCross-Correlated Attention Network (CCAN). Extensive experiments demonstrate\nthat the CCAN comfortably outperforms current state-of-the-art algorithms by a\ntangible margin.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 01:47:23 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Zhou", "Jieming", ""], ["Roy", "Soumava Kumar", ""], ["Fang", "Pengfei", ""], ["Harandi", "Mehrtash", ""], ["Petersson", "Lars", ""]]}, {"id": "2006.09603", "submitter": "Longguang Wang", "authors": "Longguang Wang, Xiaoyu Dong, Yingqian Wang, Xinyi Ying, Zaiping Lin,\n  Wei An, and Yulan Guo", "title": "Exploring Sparsity in Image Super-Resolution for Efficient Inference", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current CNN-based super-resolution (SR) methods process all locations equally\nwith computational resources being uniformly assigned in space. However, since\nmissing details in low-resolution (LR) images mainly exist in regions of edges\nand textures, less computational resources are required for those flat regions.\nTherefore, existing CNN-based methods involve redundant computation in flat\nregions, which increases their computational cost and limits their applications\non mobile devices. In this paper, we explore the sparsity in image SR to\nimprove inference efficiency of SR networks. Specifically, we develop a Sparse\nMask SR (SMSR) network to learn sparse masks to prune redundant computation.\nWithin our SMSR, spatial masks learn to identify \"important\" regions while\nchannel masks learn to mark redundant channels in those \"unimportant\" regions.\nConsequently, redundant computation can be accurately localized and skipped\nwhile maintaining comparable performance. It is demonstrated that our SMSR\nachieves state-of-the-art performance with 41%/33%/27% FLOPs being reduced for\nx2/3/4 SR. Code is available at: https://github.com/LongguangWang/SMSR.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 02:08:26 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 11:23:48 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wang", "Longguang", ""], ["Dong", "Xiaoyu", ""], ["Wang", "Yingqian", ""], ["Ying", "Xinyi", ""], ["Lin", "Zaiping", ""], ["An", "Wei", ""], ["Guo", "Yulan", ""]]}, {"id": "2006.09618", "submitter": "Chieh-Ning Fang", "authors": "Chieh-Ning Fang, Chin-Teng Lin", "title": "Multi-Subspace Neural Network for Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image classification task, feature extraction is always a big issue.\nIntra-class variability increases the difficulty in designing the extractors.\nFurthermore, hand-crafted feature extractor cannot simply adapt new situation.\nRecently, deep learning has drawn lots of attention on automatically learning\nfeatures from data. In this study, we proposed multi-subspace neural network\n(MSNN) which integrates key components of the convolutional neural network\n(CNN), receptive field, with subspace concept. Associating subspace with the\ndeep network is a novel designing, providing various viewpoints of data. Basis\nvectors, trained by adaptive subspace self-organization map (ASSOM) span the\nsubspace, serve as a transfer function to access axial components and define\nthe receptive field to extract basic patterns of data without distorting the\ntopology in the visual task. Moreover, the multiple-subspace strategy is\nimplemented as parallel blocks to adapt real-world data and contribute various\ninterpretations of data hoping to be more robust dealing with intra-class\nvariability issues. To this end, handwritten digit and object image datasets\n(i.e., MNIST and COIL-20) for classification are employed to validate the\nproposed MSNN architecture. Experimental results show MSNN is competitive to\nother state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 02:55:34 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Fang", "Chieh-Ning", ""], ["Lin", "Chin-Teng", ""]]}, {"id": "2006.09623", "submitter": "Alireza Zareian", "authors": "Alireza Zareian and Zhecan Wang and Haoxuan You and Shih-Fu Chang", "title": "Learning Visual Commonsense for Robust Scene Graph Generation", "comments": "To be presented at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graph generation models understand the scene through object and\npredicate recognition, but are prone to mistakes due to the challenges of\nperception in the wild. Perception errors often lead to nonsensical\ncompositions in the output scene graph, which do not follow real-world rules\nand patterns, and can be corrected using commonsense knowledge. We propose the\nfirst method to acquire visual commonsense such as affordance and intuitive\nphysics automatically from data, and use that to improve the robustness of\nscene understanding. To this end, we extend Transformer models to incorporate\nthe structure of scene graphs, and train our Global-Local Attention Transformer\non a scene graph corpus. Once trained, our model can be applied on any scene\ngraph generation model and correct its obvious mistakes, resulting in more\nsemantically plausible scene graphs. Through extensive experiments, we show our\nmodel learns commonsense better than any alternative, and improves the accuracy\nof state-of-the-art scene graph generation methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 03:07:53 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 11:10:45 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zareian", "Alireza", ""], ["Wang", "Zhecan", ""], ["You", "Haoxuan", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2006.09628", "submitter": "Rishabh Poddar", "authors": "Rishabh Poddar and Ganesh Ananthanarayanan and Srinath Setty and\n  Stavros Volos and Raluca Ada Popa", "title": "Visor: Privacy-Preserving Video Analytics as a Cloud Service", "comments": "USENIX Security 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-analytics-as-a-service is becoming an important offering for cloud\nproviders. A key concern in such services is privacy of the videos being\nanalyzed. While trusted execution environments (TEEs) are promising options for\npreventing the direct leakage of private video content, they remain vulnerable\nto side-channel attacks.\n  We present Visor, a system that provides confidentiality for the user's video\nstream as well as the ML models in the presence of a compromised cloud platform\nand untrusted co-tenants. Visor executes video pipelines in a hybrid TEE that\nspans both the CPU and GPU. It protects the pipeline against side-channel\nattacks induced by data-dependent access patterns of video modules, and also\naddresses leakage in the CPU-GPU communication channel. Visor is up to\n$1000\\times$ faster than na\\\"ive oblivious solutions, and its overheads\nrelative to a non-oblivious baseline are limited to $2\\times$--$6\\times$.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 03:25:11 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 04:37:24 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Poddar", "Rishabh", ""], ["Ananthanarayanan", "Ganesh", ""], ["Setty", "Srinath", ""], ["Volos", "Stavros", ""], ["Popa", "Raluca Ada", ""]]}, {"id": "2006.09654", "submitter": "Rabia Ali", "authors": "Rabia Ali, Muhammad Umar Karim Khan, Chong Min Kyung", "title": "Self-Supervised Representation Learning for Visual Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning allows for better utilization of unlabelled data.\nThe feature representation obtained by self-supervision can be used in\ndownstream tasks such as classification, object detection, segmentation, and\nanomaly detection. While classification, object detection, and segmentation\nhave been investigated with self-supervised learning, anomaly detection needs\nmore attention. We consider the problem of anomaly detection in images and\nvideos, and present a new visual anomaly detection technique for videos.\nNumerous seminal and state-of-the-art self-supervised methods are evaluated for\nanomaly detection on a variety of image datasets. The best performing\nimage-based self-supervised representation learning method is then used for\nvideo anomaly detection to see the importance of spatial features in visual\nanomaly detection in videos. We also propose a simple self-supervision approach\nfor learning temporal coherence across video frames without the use of any\noptical flow information. At its core, our method identifies the frame indices\nof a jumbled video sequence allowing it to learn the spatiotemporal features of\nthe video. This intuitive approach shows superior performance of visual anomaly\ndetection compared to numerous methods for images and videos on UCF101 and\nILSVRC2015 video datasets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 04:37:29 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Ali", "Rabia", ""], ["Khan", "Muhammad Umar Karim", ""], ["Kyung", "Chong Min", ""]]}, {"id": "2006.09661", "submitter": "Vincent Sitzmann", "authors": "Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B.\n  Lindell, Gordon Wetzstein", "title": "Implicit Neural Representations with Periodic Activation Functions", "comments": "Project website: https://vsitzmann.github.io/siren/ Project video:\n  https://youtu.be/Q2fLWGBeaiI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicitly defined, continuous, differentiable signal representations\nparameterized by neural networks have emerged as a powerful paradigm, offering\nmany possible benefits over conventional representations. However, current\nnetwork architectures for such implicit neural representations are incapable of\nmodeling signals with fine detail, and fail to represent a signal's spatial and\ntemporal derivatives, despite the fact that these are essential to many\nphysical signals defined implicitly as the solution to partial differential\nequations. We propose to leverage periodic activation functions for implicit\nneural representations and demonstrate that these networks, dubbed sinusoidal\nrepresentation networks or Sirens, are ideally suited for representing complex\nnatural signals and their derivatives. We analyze Siren activation statistics\nto propose a principled initialization scheme and demonstrate the\nrepresentation of images, wavefields, video, sound, and their derivatives.\nFurther, we show how Sirens can be leveraged to solve challenging boundary\nvalue problems, such as particular Eikonal equations (yielding signed distance\nfunctions), the Poisson equation, and the Helmholtz and wave equations. Lastly,\nwe combine Sirens with hypernetworks to learn priors over the space of Siren\nfunctions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 05:13:33 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Sitzmann", "Vincent", ""], ["Martel", "Julien N. P.", ""], ["Bergman", "Alexander W.", ""], ["Lindell", "David B.", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2006.09662", "submitter": "Vincent Sitzmann", "authors": "Vincent Sitzmann, Eric R. Chan, Richard Tucker, Noah Snavely, Gordon\n  Wetzstein", "title": "MetaSDF: Meta-learning Signed Distance Functions", "comments": "Project website: https://vsitzmann.github.io/metasdf/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural implicit shape representations are an emerging paradigm that offers\nmany potential benefits over conventional discrete representations, including\nmemory efficiency at a high spatial resolution. Generalizing across shapes with\nsuch neural implicit representations amounts to learning priors over the\nrespective function space and enables geometry reconstruction from partial or\nnoisy observations. Existing generalization methods rely on conditioning a\nneural network on a low-dimensional latent code that is either regressed by an\nencoder or jointly optimized in the auto-decoder framework. Here, we formalize\nlearning of a shape space as a meta-learning problem and leverage\ngradient-based meta-learning algorithms to solve this task. We demonstrate that\nthis approach performs on par with auto-decoder based approaches while being an\norder of magnitude faster at test-time inference. We further demonstrate that\nthe proposed gradient-based method outperforms encoder-decoder based methods\nthat leverage pooling-based set encoders.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 05:14:53 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Sitzmann", "Vincent", ""], ["Chan", "Eric R.", ""], ["Tucker", "Richard", ""], ["Snavely", "Noah", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2006.09674", "submitter": "Zhaoqiang Xia", "authors": "Zhaoqiang Xia, Wei Peng, Huai-Qian Khor, Xiaoyi Feng, Guoying Zhao", "title": "Revealing the Invisible with Model and Data Shrinking for\n  Composite-database Micro-expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composite-database micro-expression recognition is attracting increasing\nattention as it is more practical to real-world applications. Though the\ncomposite database provides more sample diversity for learning good\nrepresentation models, the important subtle dynamics are prone to disappearing\nin the domain shift such that the models greatly degrade their performance,\nespecially for deep models. In this paper, we analyze the influence of learning\ncomplexity, including the input complexity and model complexity, and discover\nthat the lower-resolution input data and shallower-architecture model are\nhelpful to ease the degradation of deep models in composite-database task.\nBased on this, we propose a recurrent convolutional network (RCN) to explore\nthe shallower-architecture and lower-resolution input data, shrinking model and\ninput complexities simultaneously. Furthermore, we develop three parameter-free\nmodules (i.e., wide expansion, shortcut connection and attention unit) to\nintegrate with RCN without increasing any learnable parameters. These three\nmodules can enhance the representation ability in various perspectives while\npreserving not-very-deep architecture for lower-resolution data. Besides, three\nmodules can further be combined by an automatic strategy (a neural architecture\nsearch strategy) and the searched architecture becomes more robust. Extensive\nexperiments on MEGC2019 dataset (composited of existing SMIC, CASME II and SAMM\ndatasets) have verified the influence of learning complexity and shown that\nRCNs with three modules and the searched combination outperform the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 06:19:24 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Xia", "Zhaoqiang", ""], ["Peng", "Wei", ""], ["Khor", "Huai-Qian", ""], ["Feng", "Xiaoyi", ""], ["Zhao", "Guoying", ""]]}, {"id": "2006.09675", "submitter": "Kun Liu", "authors": "Kun Liu, Wu Liu, Huadong Ma, Mingkui Tan, Chuang Gan", "title": "A Real-time Action Representation with Temporal Encoding and Deep\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved remarkable success for video-based action\nrecognition. However, most of existing approaches cannot be deployed in\npractice due to the high computational cost. To address this challenge, we\npropose a new real-time convolutional architecture, called Temporal\nConvolutional 3D Network (T-C3D), for action representation. T-C3D learns video\naction representations in a hierarchical multi-granularity manner while\nobtaining a high process speed. Specifically, we propose a residual 3D\nConvolutional Neural Network (CNN) to capture complementary information on the\nappearance of a single frame and the motion between consecutive frames. Based\non this CNN, we develop a new temporal encoding method to explore the temporal\ndynamics of the whole video. Furthermore, we integrate deep compression\ntechniques with T-C3D to further accelerate the deployment of models via\nreducing the size of the model. By these means, heavy calculations can be\navoided when doing the inference, which enables the method to deal with videos\nbeyond real-time speed while keeping promising performance. Our method achieves\nclear improvements on UCF101 action recognition benchmark against\nstate-of-the-art real-time methods by 5.4% in terms of accuracy and 2 times\nfaster in terms of inference speed with a less than 5MB storage model. We\nvalidate our approach by studying its action representation performance on four\ndifferent benchmarks over three different tasks. Extensive experiments\ndemonstrate comparable recognition performance to the state-of-the-art methods.\nThe source code and the pre-trained models are publicly available at\nhttps://github.com/tc3d.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 06:30:43 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Liu", "Kun", ""], ["Liu", "Wu", ""], ["Ma", "Huadong", ""], ["Tan", "Mingkui", ""], ["Gan", "Chuang", ""]]}, {"id": "2006.09679", "submitter": "Taehoon Kim", "authors": "Taehoon Kim, YoungJoon Yoo, Jihoon Yang", "title": "FrostNet: Towards Quantization-Aware Network Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  INT8 quantization has become one of the standard techniques for deploying\nconvolutional neural networks (CNNs) on edge devices to reduce the memory and\ncomputational resource usages. By analyzing quantized performances of existing\nmobile-target network architectures, we can raise an issue regarding the\nimportance of network architecture for optimal INT8 quantization. In this\npaper, we present a new network architecture search (NAS) procedure to find a\nnetwork that guarantees both full-precision (FLOAT32) and quantized (INT8)\nperformances. We first propose critical but straightforward optimization method\nwhich enables quantization-aware training (QAT) : floating-point statistic\nassisting (StatAssist) and stochastic gradient boosting (GradBoost). By\nintegrating the gradient-based NAS with StatAssist and GradBoost, we discovered\na quantization-efficient network building block, Frost bottleneck. Furthermore,\nwe used Frost bottleneck as the building block for hardware-aware NAS to obtain\nquantization-efficient networks, FrostNets, which show improved quantization\nperformances compared to other mobile-target networks while maintaining\ncompetitive FLOAT32 performance. Our FrostNets achieve higher recognition\naccuracy than existing CNNs with comparable latency when quantized, due to\nhigher latency reduction rate (average 65%).\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 06:40:43 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 07:59:37 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2020 08:01:00 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 10:09:33 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Kim", "Taehoon", ""], ["Yoo", "YoungJoon", ""], ["Yang", "Jihoon", ""]]}, {"id": "2006.09694", "submitter": "Jiayun Wang", "authors": "Jiayun Wang, Jierui Lin, Qian Yu, Runtao Liu, Yubei Chen, Stella X. Yu", "title": "3D Shape Reconstruction from Free-Hand Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketches are the most abstract 2D representations of real-world objects.\nAlthough a sketch usually has geometrical distortion and lacks visual cues,\nhumans can effortlessly envision a 3D object from it. This indicates that\nsketches encode the appropriate information to recover 3D shapes. Although\ngreat progress has been achieved in 3D reconstruction from distortion-free line\ndrawings, such as CAD and edge maps, little effort has been made to reconstruct\n3D shapes from free-hand sketches. We pioneer to study this task and aim to\nenhance the power of sketches in 3D-related applications such as interactive\ndesign and VR/AR games. Further, we propose an end-to-end sketch-based 3D\nreconstruction framework. Instead of well-used edge maps, synthesized sketches\nare adopted as training data. Additionally, we propose a sketch standardization\nmodule to handle different sketch styles and distortions. With extensive\nexperiments, we demonstrate the effectiveness of our model and its strong\ngeneralizability to various free-hand sketches.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 07:43:10 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Wang", "Jiayun", ""], ["Lin", "Jierui", ""], ["Yu", "Qian", ""], ["Liu", "Runtao", ""], ["Chen", "Yubei", ""], ["Yu", "Stella X.", ""]]}, {"id": "2006.09701", "submitter": "Shuo Wang", "authors": "Shuo Wang, Surya Nepal, Marthie Grobler, Carsten Rudolph, Tianle Chen,\n  Shangyu Chen", "title": "Adversarial Defense by Latent Style Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models have demonstrated vulnerability to adversarial\nattacks, more specifically misclassification of adversarial examples.\n  In this paper, we investigate an attack-agnostic defense against adversarial\nattacks on high-resolution images by detecting suspicious inputs.\n  The intuition behind our approach is that the essential characteristics of a\nnormal image are generally consistent with non-essential style transformations,\ne.g., slightly changing the facial expression of human portraits.\n  In contrast, adversarial examples are generally sensitive to such\ntransformations.\n  In our approach to detect adversarial instances, we propose an\nin\\underline{V}ertible \\underline{A}utoencoder based on the\n\\underline{S}tyleGAN2 generator via \\underline{A}dversarial training (VASA) to\ninverse images to disentangled latent codes that reveal hierarchical styles.\n  We then build a set of edited copies with non-essential style transformations\nby performing latent shifting and reconstruction, based on the correspondences\nbetween latent codes and style transformations.\n  The classification-based consistency of these edited copies is used to\ndistinguish adversarial instances.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 07:56:36 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Wang", "Shuo", ""], ["Nepal", "Surya", ""], ["Grobler", "Marthie", ""], ["Rudolph", "Carsten", ""], ["Chen", "Tianle", ""], ["Chen", "Shangyu", ""]]}, {"id": "2006.09717", "submitter": "Guillermo Ortiz-Jimenez", "authors": "Guillermo Ortiz-Jimenez, Apostolos Modas, Seyed-Mohsen\n  Moosavi-Dezfooli, Pascal Frossard", "title": "Neural Anisotropy Directions", "comments": "Accepted to the 34th Conference on Neural Information Processing\n  Systems (NeurIPS 2020) (39 pages, 22 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we analyze the role of the network architecture in shaping the\ninductive bias of deep classifiers. To that end, we start by focusing on a very\nsimple problem, i.e., classifying a class of linearly separable distributions,\nand show that, depending on the direction of the discriminative feature of the\ndistribution, many state-of-the-art deep convolutional neural networks (CNNs)\nhave a surprisingly hard time solving this simple task. We then define as\nneural anisotropy directions (NADs) the vectors that encapsulate the\ndirectional inductive bias of an architecture. These vectors, which are\nspecific for each architecture and hence act as a signature, encode the\npreference of a network to separate the input data based on some particular\nfeatures. We provide an efficient method to identify NADs for several CNN\narchitectures and thus reveal their directional inductive biases. Furthermore,\nwe show that, for the CIFAR-10 dataset, NADs characterize the features used by\nCNNs to discriminate between different classes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 08:36:28 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 10:21:58 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Ortiz-Jimenez", "Guillermo", ""], ["Modas", "Apostolos", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""]]}, {"id": "2006.09738", "submitter": "Michael F\\\"urst", "authors": "Michael F\\\"urst and Oliver Wasenm\\\"uller and Didier Stricker", "title": "LRPD: Long Range 3D Pedestrian Detection Leveraging Specific Strengths\n  of LiDAR and RGB", "comments": "7 Pages, 5 Figures, Autonomous Vehicles, 3D Object Detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While short range 3D pedestrian detection is sufficient for emergency\nbreaking, long range detections are required for smooth breaking and gaining\ntrust in autonomous vehicles. The current state-of-the-art on the KITTI\nbenchmark performs suboptimal in detecting the position of pedestrians at long\nrange. Thus, we propose an approach specifically targeting long range 3D\npedestrian detection (LRPD), leveraging the density of RGB and the precision of\nLiDAR. Therefore, for proposals, RGB instance segmentation and LiDAR point\nbased proposal generation are combined, followed by a second stage using both\nsensor modalities symmetrically. This leads to a significant improvement in mAP\non long range compared to the current state-of-the art. The evaluation of our\nLRPD approach was done on the pedestrians from the KITTI benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 09:27:38 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["F\u00fcrst", "Michael", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Stricker", "Didier", ""]]}, {"id": "2006.09740", "submitter": "David Mohlin", "authors": "D. Mohlin, G. Bianchi, J. Sullivan", "title": "Probabilistic orientation estimation with matrix Fisher distributions", "comments": "20 pages, 11 figures, submitted to NeurIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on estimating probability distributions over the set of 3D\nrotations ($SO(3)$) using deep neural networks. Learning to regress models to\nthe set of rotations is inherently difficult due to differences in topology\nbetween $\\mathbb{R}^N$ and $SO(3)$. We overcome this issue by using a neural\nnetwork to output the parameters for a matrix Fisher distribution since these\nparameters are homeomorphic to $\\mathbb{R}^9$. By using a negative log\nlikelihood loss for this distribution we get a loss which is convex with\nrespect to the network outputs. By optimizing this loss we improve\nstate-of-the-art on several challenging applicable datasets, namely Pascal3D+,\nModelNet10-$SO(3)$ and UPNA head pose.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 09:28:19 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Mohlin", "D.", ""], ["Bianchi", "G.", ""], ["Sullivan", "J.", ""]]}, {"id": "2006.09761", "submitter": "Sebastiano Chiodini", "authors": "Sebastiano Chiodini, Luca Torresin, Marco Pertile, Stefano Debei", "title": "Evaluation of 3D CNN Semantic Mapping for Rover Navigation", "comments": "To be presented at the 7th IEEE International Workshop on Metrology\n  for Aerospace (MetroAerospace)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Terrain assessment is a key aspect for autonomous exploration rovers,\nsurrounding environment recognition is required for multiple purposes, such as\noptimal trajectory planning and autonomous target identification. In this work\nwe present a technique to generate accurate three-dimensional semantic maps for\nMartian environment. The algorithm uses as input a stereo image acquired by a\ncamera mounted on a rover. Firstly, images are labeled with DeepLabv3+, which\nis an encoder-decoder Convolutional Neural Networl (CNN). Then, the labels\nobtained by the semantic segmentation are combined to stereo depth-maps in a\nVoxel representation. We evaluate our approach on the ESA Katwijk Beach\nPlanetary Rover Dataset.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 10:24:29 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Chiodini", "Sebastiano", ""], ["Torresin", "Luca", ""], ["Pertile", "Marco", ""], ["Debei", "Stefano", ""]]}, {"id": "2006.09762", "submitter": "Lucas Pascal", "authors": "Lucas Pascal and Pietro Michiardi and Xavier Bost and Benoit Huet and\n  Maria A. Zuluaga", "title": "Maximum Roaming Multi-Task Learning", "comments": "Accepted at the 35th AAAI Conference on Artificial Intelligence (AAAI\n  2021)", "journal-ref": "Proceedings of the AAAI Conference on Artificial Intelligence:\n  35(10), 9331-9341 (2021)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning has gained popularity due to the advantages it provides\nwith respect to resource usage and performance. Nonetheless, the joint\noptimization of parameters with respect to multiple tasks remains an active\nresearch topic. Sub-partitioning the parameters between different tasks has\nproven to be an efficient way to relax the optimization constraints over the\nshared weights, may the partitions be disjoint or overlapping. However, one\ndrawback of this approach is that it can weaken the inductive bias generally\nset up by the joint task optimization. In this work, we present a novel way to\npartition the parameter space without weakening the inductive bias.\nSpecifically, we propose Maximum Roaming, a method inspired by dropout that\nrandomly varies the parameter partitioning, while forcing them to visit as many\ntasks as possible at a regulated frequency, so that the network fully adapts to\neach update. We study the properties of our method through experiments on a\nvariety of visual multi-task data sets. Experimental results suggest that the\nregularization brought by roaming has more impact on performance than usual\npartitioning optimization strategies. The overall method is flexible, easily\napplicable, provides superior regularization and consistently achieves improved\nperformances compared to recent multi-task learning formulations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 10:25:41 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 10:55:28 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 12:35:19 GMT"}, {"version": "v4", "created": "Wed, 19 May 2021 09:20:37 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Pascal", "Lucas", ""], ["Michiardi", "Pietro", ""], ["Bost", "Xavier", ""], ["Huet", "Benoit", ""], ["Zuluaga", "Maria A.", ""]]}, {"id": "2006.09772", "submitter": "Pushpak Pati", "authors": "Pushpak Pati, Antonio Foncubierta-Rodriguez, Orcun Goksel, Maria\n  Gabrani", "title": "Mitosis Detection Under Limited Annotation: A Joint Learning Approach", "comments": "2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)", "journal-ref": null, "doi": "10.1109/ISBI45749.2020.9098431", "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mitotic counting is a vital prognostic marker of tumor proliferation in\nbreast cancer. Deep learning-based mitotic detection is on par with\npathologists, but it requires large labeled data for training. We propose a\ndeep classification framework for enhancing mitosis detection by leveraging\nclass label information, via softmax loss, and spatial distribution information\namong samples, via distance metric learning. We also investigate strategies\ntowards steadily providing informative samples to boost the learning. The\nefficacy of the proposed framework is established through evaluation on ICPR\n2012 and AMIDA 2013 mitotic data. Our framework significantly improves the\ndetection with small training data and achieves on par or superior performance\ncompared to state-of-the-art methods for using the entire training data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 10:46:29 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 08:37:08 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Pati", "Pushpak", ""], ["Foncubierta-Rodriguez", "Antonio", ""], ["Goksel", "Orcun", ""], ["Gabrani", "Maria", ""]]}, {"id": "2006.09785", "submitter": "Jathushan Rajasegaran", "authors": "Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan,\n  Mubarak Shah", "title": "Self-supervised Knowledge Distillation for Few-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world contains an overwhelmingly large number of object classes,\nlearning all of which at once is infeasible. Few shot learning is a promising\nlearning paradigm due to its ability to learn out of order distributions\nquickly with only a few samples. Recent works [7, 41] show that simply learning\na good feature embedding can outperform more sophisticated meta-learning and\nmetric learning algorithms for few-shot learning. In this paper, we propose a\nsimple approach to improve the representation capacity of deep neural networks\nfor few-shot learning tasks. We follow a two-stage learning process: First, we\ntrain a neural network to maximize the entropy of the feature embedding, thus\ncreating an optimal output manifold using a self-supervised auxiliary loss. In\nthe second stage, we minimize the entropy on feature embedding by bringing\nself-supervised twins together, while constraining the manifold with\nstudent-teacher distillation. Our experiments show that, even in the first\nstage, self-supervision can outperform current state-of-the-art methods, with\nfurther gains achieved by our second stage distillation process. Our codes are\navailable at: https://github.com/brjathu/SKD.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 11:27:00 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 05:22:39 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Rajasegaran", "Jathushan", ""], ["Khan", "Salman", ""], ["Hayat", "Munawar", ""], ["Khan", "Fahad Shahbaz", ""], ["Shah", "Mubarak", ""]]}, {"id": "2006.09788", "submitter": "Yaxiong Wang", "authors": "Yaxiong Wang, Yunchao Wei, Xueming Qian, Li Zhu, Yi Yang", "title": "Sketch-Guided Scenery Image Outpainting", "comments": "Accepted by TIP", "journal-ref": null, "doi": "10.1109/TIP.2021.3054477", "report-no": null, "categories": "cs.CV cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outpainting results produced by existing approaches are often too random\nto meet users' requirement. In this work, we take the image outpainting one\nstep forward by allowing users to harvest personal custom outpainting results\nusing sketches as the guidance. To this end, we propose an encoder-decoder\nbased network to conduct sketch-guided outpainting, where two alignment modules\nare adopted to impose the generated content to be realistic and consistent with\nthe provided sketches. First, we apply a holistic alignment module to make the\nsynthesized part be similar to the real one from the global view. Second, we\nreversely produce the sketches from the synthesized part and encourage them be\nconsistent with the ground-truth ones using a sketch alignment module. In this\nway, the learned generator will be imposed to pay more attention to fine\ndetails and be sensitive to the guiding sketches. To our knowledge, this work\nis the first attempt to explore the challenging yet meaningful conditional\nscenery image outpainting. We conduct extensive experiments on two collected\nbenchmarks to qualitatively and quantitatively validate the effectiveness of\nour approach compared with the other state-of-the-art generative models.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 11:34:36 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 12:19:13 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Wang", "Yaxiong", ""], ["Wei", "Yunchao", ""], ["Qian", "Xueming", ""], ["Zhu", "Li", ""], ["Yang", "Yi", ""]]}, {"id": "2006.09791", "submitter": "Jos\\'e Cano", "authors": "Perry Gibson, Jos\\'e Cano, Jack Turner, Elliot J. Crowley, Michael\n  O'Boyle, Amos Storkey", "title": "Optimizing Grouped Convolutions on Edge Devices", "comments": "Camera ready version to be published at ASAP 2020 - The 31st IEEE\n  International Conference on Application-specific Systems, Architectures and\n  Processors. 8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When deploying a deep neural network on constrained hardware, it is possible\nto replace the network's standard convolutions with grouped convolutions. This\nallows for substantial memory savings with minimal loss of accuracy. However,\ncurrent implementations of grouped convolutions in modern deep learning\nframeworks are far from performing optimally in terms of speed. In this paper\nwe propose Grouped Spatial Pack Convolutions (GSPC), a new implementation of\ngrouped convolutions that outperforms existing solutions. We implement GSPC in\nTVM, which provides state-of-the-art performance on edge devices. We analyze a\nset of networks utilizing different types of grouped convolutions and evaluate\ntheir performance in terms of inference time on several edge devices. We\nobserve that our new implementation scales well with the number of groups and\nprovides the best inference times in all settings, improving the existing\nimplementations of grouped convolutions in TVM, PyTorch and TensorFlow Lite by\n3.4x, 8x and 4x on average respectively. Code is available at\nhttps://github.com/gecLAB/tvm-GSPC/\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 11:48:37 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Gibson", "Perry", ""], ["Cano", "Jos\u00e9", ""], ["Turner", "Jack", ""], ["Crowley", "Elliot J.", ""], ["O'Boyle", "Michael", ""], ["Storkey", "Amos", ""]]}, {"id": "2006.09845", "submitter": "Aykut Erdem", "authors": "Ahmet Serdar Karadeniz and Erkut Erdem and Aykut Erdem", "title": "Burst Photography for Learning to Enhance Extremely Dark Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing images under extremely low-light conditions poses significant\nchallenges for the standard camera pipeline. Images become too dark and too\nnoisy, which makes traditional enhancement techniques almost impossible to\napply. Recently, learning-based approaches have shown very promising results\nfor this task since they have substantially more expressive capabilities to\nallow for improved quality. Motivated by these studies, in this paper, we aim\nto leverage burst photography to boost the performance and obtain much sharper\nand more accurate RGB images from extremely dark raw images. The backbone of\nour proposed framework is a novel coarse-to-fine network architecture that\ngenerates high-quality outputs progressively. The coarse network predicts a\nlow-resolution, denoised raw image, which is then fed to the fine network to\nrecover fine-scale details and realistic textures. To further reduce the noise\nlevel and improve the color accuracy, we extend this network to a permutation\ninvariant structure so that it takes a burst of low-light images as input and\nmerges information from multiple images at the feature-level. Our experiments\ndemonstrate that our approach leads to perceptually more pleasing results than\nthe state-of-the-art methods by producing more detailed and considerably higher\nquality images.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 13:19:07 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Karadeniz", "Ahmet Serdar", ""], ["Erdem", "Erkut", ""], ["Erdem", "Aykut", ""]]}, {"id": "2006.09853", "submitter": "Yunqi Miao", "authors": "Yunqi Miao, Zijia Lin, Guiguang Ding, Jungong Han", "title": "Shallow Feature Based Dense Attention Network for Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the performance of crowd counting via deep learning has been improved\ndramatically in the recent years, it remains an ingrained problem due to\ncluttered backgrounds and varying scales of people within an image. In this\npaper, we propose a Shallow feature based Dense Attention Network (SDANet) for\ncrowd counting from still images, which diminishes the impact of backgrounds\nvia involving a shallow feature based attention model, and meanwhile, captures\nmulti-scale information via densely connecting hierarchical image features.\nSpecifically, inspired by the observation that backgrounds and human crowds\ngenerally have noticeably different responses in shallow features, we decide to\nbuild our attention model upon shallow-feature maps, which results in accurate\nbackground-pixel detection. Moreover, considering that the most representative\nfeatures of people across different scales can appear in different layers of a\nfeature extraction network, to better keep them all, we propose to densely\nconnect hierarchical image features of different layers and subsequently encode\nthem for estimating crowd density. Experimental results on three benchmark\ndatasets clearly demonstrate the superiority of SDANet when dealing with\ndifferent scenarios. Particularly, on the challenging UCF CC 50 dataset, our\nmethod outperforms other existing methods by a large margin, as is evident from\na remarkable 11.9% Mean Absolute Error (MAE) drop of our SDANet.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 13:34:42 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Miao", "Yunqi", ""], ["Lin", "Zijia", ""], ["Ding", "Guiguang", ""], ["Han", "Jungong", ""]]}, {"id": "2006.09865", "submitter": "Pallav Bera", "authors": "Pallav Kumar Bera, Can Isik", "title": "Intelligent Protection & Classification of Transients in Two-Core\n  Symmetric Phase Angle Regulating Transformers", "comments": null, "journal-ref": "IEEE Access, vol. 9, pp. 72937-72948, 2021", "doi": "10.1109/ACCESS.2021.3081015", "report-no": null, "categories": "eess.SP cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the applicability of time and time-frequency features\nbased classifiers to distinguish internal faults and other transients -\nmagnetizing inrush, sympathetic inrush, external faults with current\ntransformer saturation, and overexcitation - for Indirect Symmetrical Phase\nAngle Regulating Transformers (ISPAR). Then the faulty transformer unit\n(series/exciting) of the ISPAR is located, or else the transient disturbance is\nidentified. An event detector detects variation in differential currents and\nregisters one-cycle of 3-phase post transient samples which are used to extract\nthe time and time-frequency features for training seven classifiers. Three\ndifferent sets of features - wavelet coefficients, time-domain features, and\ncombination of time and wavelet energy - obtained from exhaustive search using\nDecision Tree, random forest feature selection, and maximum Relevance Minimum\nRedundancy are used. The internal fault is detected with a balanced accuracy of\n99.9%, the faulty unit is localized with balanced accuracy of 98.7% and the\nno-fault transients are classified with balanced accuracy of 99.5%. The results\nshow potential for accurate internal fault detection and localization, and\ntransient identification. The proposed scheme can supervise the operation of\nexisting microprocessor-based differential relays resulting in higher stability\nand dependability. The ISPAR is modeled and the transients are simulated in\nPSCAD/EMTDC by varying several parameters.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 13:42:58 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bera", "Pallav Kumar", ""], ["Isik", "Can", ""]]}, {"id": "2006.09876", "submitter": "Ge Zhang", "authors": "Jianrong Wang and Ge Zhang and Zhenyu Wu and XueWei Li and Li Liu", "title": "Self-Supervised Joint Learning Framework of Depth Estimation via\n  Implicit Cues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In self-supervised monocular depth estimation, the depth discontinuity and\nmotion objects' artifacts are still challenging problems. Existing\nself-supervised methods usually utilize a single view to train the depth\nestimation network. Compared with static views, abundant dynamic properties\nbetween video frames are beneficial to refined depth estimation, especially for\ndynamic objects. In this work, we propose a novel self-supervised joint\nlearning framework for depth estimation using consecutive frames from monocular\nand stereo videos. The main idea is using an implicit depth cue extractor which\nleverages dynamic and static cues to generate useful depth proposals. These\ncues can predict distinguishable motion contours and geometric scene\nstructures. Furthermore, a new high-dimensional attention module is introduced\nto extract clear global transformation, which effectively suppresses\nuncertainty of local descriptors in high-dimensional space, resulting in a more\nreliable optimization in learning framework. Experiments demonstrate that the\nproposed framework outperforms the state-of-the-art(SOTA) on KITTI and Make3D\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 13:56:59 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 08:19:50 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 07:11:18 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Wang", "Jianrong", ""], ["Zhang", "Ge", ""], ["Wu", "Zhenyu", ""], ["Li", "XueWei", ""], ["Liu", "Li", ""]]}, {"id": "2006.09879", "submitter": "Roozbeh Yousefzadeh", "authors": "Roozbeh Yousefzadeh and Furong Huang", "title": "Using Wavelets and Spectral Methods to Study Patterns in\n  Image-Classification Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NA eess.IV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models extract, before a final classification layer, features\nor patterns which are key for their unprecedented advantageous performance.\nHowever, the process of complex nonlinear feature extraction is not well\nunderstood, a major reason why interpretation, adversarial robustness, and\ngeneralization of deep neural nets are all open research problems. In this\npaper, we use wavelet transformation and spectral methods to analyze the\ncontents of image classification datasets, extract specific patterns from the\ndatasets and find the associations between patterns and classes. We show that\neach image can be written as the summation of a finite number of rank-1\npatterns in the wavelet space, providing a low rank approximation that captures\nthe structures and patterns essential for learning. Regarding the studies on\nmemorization vs learning, our results clearly reveal disassociation of patterns\nfrom classes, when images are randomly labeled. Our method can be used as a\npattern recognition approach to understand and interpret learnability of these\ndatasets. It may also be used for gaining insights about the features and\npatterns that deep classifiers learn from the datasets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 13:58:24 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Yousefzadeh", "Roozbeh", ""], ["Huang", "Furong", ""]]}, {"id": "2006.09882", "submitter": "Mathilde Caron", "authors": "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr\n  Bojanowski, Armand Joulin", "title": "Unsupervised Learning of Visual Features by Contrasting Cluster\n  Assignments", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image representations have significantly reduced the gap with\nsupervised pretraining, notably with the recent achievements of contrastive\nlearning methods. These contrastive methods typically work online and rely on a\nlarge number of explicit pairwise feature comparisons, which is computationally\nchallenging. In this paper, we propose an online algorithm, SwAV, that takes\nadvantage of contrastive methods without requiring to compute pairwise\ncomparisons. Specifically, our method simultaneously clusters the data while\nenforcing consistency between cluster assignments produced for different\naugmentations (or views) of the same image, instead of comparing features\ndirectly as in contrastive learning. Simply put, we use a swapped prediction\nmechanism where we predict the cluster assignment of a view from the\nrepresentation of another view. Our method can be trained with large and small\nbatches and can scale to unlimited amounts of data. Compared to previous\ncontrastive methods, our method is more memory efficient since it does not\nrequire a large memory bank or a special momentum network. In addition, we also\npropose a new data augmentation strategy, multi-crop, that uses a mix of views\nwith different resolutions in place of two full-resolution views, without\nincreasing the memory or compute requirements much. We validate our findings by\nachieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as\nsurpassing supervised pretraining on all the considered transfer tasks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 14:00:42 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 14:10:03 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 09:07:59 GMT"}, {"version": "v4", "created": "Thu, 15 Oct 2020 14:30:40 GMT"}, {"version": "v5", "created": "Fri, 8 Jan 2021 17:01:05 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Caron", "Mathilde", ""], ["Misra", "Ishan", ""], ["Mairal", "Julien", ""], ["Goyal", "Priya", ""], ["Bojanowski", "Piotr", ""], ["Joulin", "Armand", ""]]}, {"id": "2006.09887", "submitter": "Ashraf Rayed", "authors": "Asharf, Balasubramanian E, Sankarasrinivasan S", "title": "Quantification of groundnut leaf defects using image processing\n  algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification, classification, and quantification of crop defects are of\nparamount of interest to the farmers for preventive measures and decrease the\nyield loss through necessary remedial actions. Due to the vast agricultural\nfield, manual inspection of crops is tedious and time-consuming. UAV based data\ncollection, observation, identification, and quantification of defected leaves\narea are considered to be an effective solution. The present work attempts to\nestimate the percentage of affected groundnut leaves area across four regions\nof Andharapradesh using image processing techniques. The proposed method\ninvolves colour space transformation combined with thresholding technique to\nperform the segmentation. The calibration measures are performed during\nacquisition with respect to UAV capturing distance, angle and other relevant\ncamera parameters. Finally, our method can estimate the consolidated leaves and\ndefected area. The image analysis results across these four regions reveal that\naround 14 - 28% of leaves area is affected across the groundnut field and\nthereby yield will be diminished correspondingly. Hence, it is recommended to\nspray the pesticides on the affected regions alone across the field to improve\nthe plant growth and thereby yield will be increased.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 15:07:12 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Asharf", "", ""], ["E", "Balasubramanian", ""], ["S", "Sankarasrinivasan", ""]]}, {"id": "2006.09888", "submitter": "Patrik Jonell", "authors": "Patrik Jonell, Taras Kucherenko, Gustav Eje Henter, Jonas Beskow", "title": "Let's Face It: Probabilistic Multi-modal Interlocutor-aware Generation\n  of Facial Gestures in Dyadic Settings", "comments": "Best Paper Award. 8 pages, 4 figures, IVA '20: Proceedings of the\n  20th ACM International Conference on Intelligent Virtual Agent", "journal-ref": null, "doi": "10.1145/3383652.3423911", "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.SD eess.AS eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enable more natural face-to-face interactions, conversational agents need\nto adapt their behavior to their interlocutors. One key aspect of this is\ngeneration of appropriate non-verbal behavior for the agent, for example facial\ngestures, here defined as facial expressions and head movements. Most existing\ngesture-generating systems do not utilize multi-modal cues from the\ninterlocutor when synthesizing non-verbal behavior. Those that do, typically\nuse deterministic methods that risk producing repetitive and non-vivid motions.\nIn this paper, we introduce a probabilistic method to synthesize\ninterlocutor-aware facial gestures - represented by highly expressive FLAME\nparameters - in dyadic conversations. Our contributions are: a) a method for\nfeature extraction from multi-party video and speech recordings, resulting in a\nrepresentation that allows for independent control and manipulation of\nexpression and speech articulation in a 3D avatar; b) an extension to MoGlow, a\nrecent motion-synthesis method based on normalizing flows, to also take\nmulti-modal signals from the interlocutor as input and subsequently output\ninterlocutor-aware facial gestures; and c) a subjective evaluation assessing\nthe use and relative importance of the input modalities. The results show that\nthe model successfully leverages the input from the interlocutor to generate\nmore appropriate behavior. Videos, data, and code available at:\nhttps://jonepatr.github.io/lets_face_it.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 14:11:51 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 21:22:20 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Jonell", "Patrik", ""], ["Kucherenko", "Taras", ""], ["Henter", "Gustav Eje", ""], ["Beskow", "Jonas", ""]]}, {"id": "2006.09902", "submitter": "Ahmed Alkhateeb", "authors": "Gouranga Charan, Muhammad Alrabeiah, and Ahmed Alkhateeb", "title": "Vision-Aided Dynamic Blockage Prediction for 6G Wireless Communication\n  Networks", "comments": "The dataset and code files will be available soon on the ViWi\n  website: https://www.viwi-dataset.net/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlocking the full potential of millimeter-wave and sub-terahertz wireless\ncommunication networks hinges on realizing unprecedented low-latency and\nhigh-reliability requirements. The challenge in meeting those requirements lies\npartly in the sensitivity of signals in the millimeter-wave and sub-terahertz\nfrequency ranges to blockages. One promising way to tackle that challenge is to\nhelp a wireless network develop a sense of its surrounding using machine\nlearning. This paper attempts to do that by utilizing deep learning and\ncomputer vision. It proposes a novel solution that proactively predicts\n\\textit{dynamic} link blockages. More specifically, it develops a deep neural\nnetwork architecture that learns from observed sequences of RGB images and\nbeamforming vectors how to predict possible future link blockages. The proposed\narchitecture is evaluated on a publicly available dataset that represents a\nsynthetic dynamic communication scenario with multiple moving users and\nblockages. It scores a link-blockage prediction accuracy in the neighborhood of\n86\\%, a performance that is unlikely to be matched without utilizing visual\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 14:37:38 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 03:09:15 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Charan", "Gouranga", ""], ["Alrabeiah", "Muhammad", ""], ["Alkhateeb", "Ahmed", ""]]}, {"id": "2006.09904", "submitter": "Paridhi Maheshwari", "authors": "Paridhi Maheshwari, Manoj Ghuhan, Vishwa Vinay", "title": "Learning Colour Representations of Search Queries", "comments": "Accepted as a full paper at SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401095", "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image search engines rely on appropriately designed ranking features that\ncapture various aspects of the content semantics as well as the historic\npopularity. In this work, we consider the role of colour in this relevance\nmatching process. Our work is motivated by the observation that a significant\nfraction of user queries have an inherent colour associated with them. While\nsome queries contain explicit colour mentions (such as 'black car' and 'yellow\ndaisies'), other queries have implicit notions of colour (such as 'sky' and\n'grass'). Furthermore, grounding queries in colour is not a mapping to a single\ncolour, but a distribution in colour space. For instance, a search for 'trees'\ntends to have a bimodal distribution around the colours green and brown. We\nleverage historical clickthrough data to produce a colour representation for\nsearch queries and propose a recurrent neural network architecture to encode\nunseen queries into colour space. We also show how this embedding can be learnt\nalongside a cross-modal relevance ranker from impression logs where a subset of\nthe result images were clicked. We demonstrate that the use of a query-image\ncolour distance feature leads to an improvement in the ranker performance as\nmeasured by users' preferences of clicked versus skipped images.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 14:38:44 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Maheshwari", "Paridhi", ""], ["Ghuhan", "Manoj", ""], ["Vinay", "Vishwa", ""]]}, {"id": "2006.09917", "submitter": "Noureldin Hendy", "authors": "Noureldin Hendy, Cooper Sloan, Feng Tian, Pengfei Duan, Nick Charchut,\n  Yuesong Xie, Chuang Wang, James Philbin", "title": "FISHING Net: Future Inference of Semantic Heatmaps In Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For autonomous robots to navigate a complex environment, it is crucial to\nunderstand the surrounding scene both geometrically and semantically. Modern\nautonomous robots employ multiple sets of sensors, including lidars, radars,\nand cameras. Managing the different reference frames and characteristics of the\nsensors, and merging their observations into a single representation\ncomplicates perception. Choosing a single unified representation for all\nsensors simplifies the task of perception and fusion. In this work, we present\nan end-to-end pipeline that performs semantic segmentation and short term\nprediction using a top-down representation. Our approach consists of an\nensemble of neural networks which take in sensor data from different sensor\nmodalities and transform them into a single common top-down semantic grid\nrepresentation. We find this representation favorable as it is agnostic to\nsensor-specific reference frames and captures both the semantic and geometric\ninformation for the surrounding scene. Because the modalities share a single\noutput representation, they can be easily aggregated to produce a fused output.\nIn this work we predict short-term semantic grids but the framework can be\nextended to other tasks. This approach offers a simple, extensible, end-to-end\napproach for multi-modal perception and prediction.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 14:56:08 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Hendy", "Noureldin", ""], ["Sloan", "Cooper", ""], ["Tian", "Feng", ""], ["Duan", "Pengfei", ""], ["Charchut", "Nick", ""], ["Xie", "Yuesong", ""], ["Wang", "Chuang", ""], ["Philbin", "James", ""]]}, {"id": "2006.09920", "submitter": "Arash Vahdat", "authors": "Tanmay Gupta, Arash Vahdat, Gal Chechik, Xiaodong Yang, Jan Kautz, and\n  Derek Hoiem", "title": "Contrastive Learning for Weakly Supervised Phrase Grounding", "comments": "ECCV 2020 (spotlight paper), Project page:\n  http://tanmaygupta.info/info-ground", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phrase grounding, the problem of associating image regions to caption words,\nis a crucial component of vision-language tasks. We show that phrase grounding\ncan be learned by optimizing word-region attention to maximize a lower bound on\nmutual information between images and caption words. Given pairs of images and\ncaptions, we maximize compatibility of the attention-weighted regions and the\nwords in the corresponding caption, compared to non-corresponding pairs of\nimages and captions. A key idea is to construct effective negative captions for\nlearning through language model guided word substitutions. Training with our\nnegatives yields a $\\sim10\\%$ absolute gain in accuracy over randomly-sampled\nnegatives from the training data. Our weakly supervised phrase grounding model\ntrained on COCO-Captions shows a healthy gain of $5.7\\%$ to achieve $76.7\\%$\naccuracy on Flickr30K Entities benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 15:00:53 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 04:11:42 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 21:53:38 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Gupta", "Tanmay", ""], ["Vahdat", "Arash", ""], ["Chechik", "Gal", ""], ["Yang", "Xiaodong", ""], ["Kautz", "Jan", ""], ["Hoiem", "Derek", ""]]}, {"id": "2006.09930", "submitter": "Emre Aksan", "authors": "Emre Aksan, Thomas Deselaers, Andrea Tagliasacchi, Otmar Hilliges", "title": "CoSE: Compositional Stroke Embeddings", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a generative model for complex free-form structures such as\nstroke-based drawing tasks. While previous approaches rely on sequence-based\nmodels for drawings of basic objects or handwritten text, we propose a model\nthat treats drawings as a collection of strokes that can be composed into\ncomplex structures such as diagrams (e.g., flow-charts). At the core of the\napproach lies a novel autoencoder that projects variable-length strokes into a\nlatent space of fixed dimension. This representation space allows a relational\nmodel, operating in latent space, to better capture the relationship between\nstrokes and to predict subsequent strokes. We demonstrate qualitatively and\nquantitatively that our proposed approach is able to model the appearance of\nindividual strokes, as well as the compositional structure of larger diagram\ndrawings. Our approach is suitable for interactive use cases such as\nauto-completing diagrams. We make code and models publicly available at\nhttps://eth-ait.github.io/cose.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 15:22:54 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 18:50:51 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Aksan", "Emre", ""], ["Deselaers", "Thomas", ""], ["Tagliasacchi", "Andrea", ""], ["Hilliges", "Otmar", ""]]}, {"id": "2006.09952", "submitter": "Lucas Theis", "authors": "Eirikur Agustsson and Lucas Theis", "title": "Universally Quantized Neural Compression", "comments": "Authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach to learning encoders for lossy compression is to use\nadditive uniform noise during training as a differentiable approximation to\ntest-time quantization. We demonstrate that a uniform noise channel can also be\nimplemented at test time using universal quantization (Ziv, 1985). This allows\nus to eliminate the mismatch between training and test phases while maintaining\na completely differentiable loss function. Implementing the uniform noise\nchannel is a special case of the more general problem of communicating a\nsample, which we prove is computationally hard if we do not make assumptions\nabout its distribution. However, the uniform special case is efficient as well\nas easy to implement and thus of great interest from a practical point of view.\nFinally, we show that quantization can be obtained as a limiting case of a soft\nquantizer applied to the uniform noise channel, bridging compression with and\nwithout quantization.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 15:59:24 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 17:10:20 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Agustsson", "Eirikur", ""], ["Theis", "Lucas", ""]]}, {"id": "2006.09959", "submitter": "Xinshuo Weng", "authors": "Xi Sun, Xinshuo Weng and Kris Kitani", "title": "When We First Met: Visual-Inertial Person Localization for Co-Robot\n  Rendezvous", "comments": "Published in IROS 2020. Project website is\n  http://www.xinshuoweng.com/projects/VIPL/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to enable robots to visually localize a target person through the aid\nof an additional sensing modality -- the target person's 3D inertial\nmeasurements. The need for such technology may arise when a robot is to meet\nperson in a crowd for the first time or when an autonomous vehicle must\nrendezvous with a rider amongst a crowd without knowing the appearance of the\nperson in advance. A person's inertial information can be measured with a\nwearable device such as a smart-phone and can be shared selectively with an\nautonomous system during the rendezvous. We propose a method to learn a\nvisual-inertial feature space in which the motion of a person in video can be\neasily matched to the motion measured by a wearable inertial measurement unit\n(IMU). The transformation of the two modalities into the joint feature space is\nlearned through the use of a contrastive loss which forces inertial motion\nfeatures and video motion features generated by the same person to lie close in\nthe joint feature space. To validate our approach, we compose a dataset of over\n60,000 video segments of moving people along with wearable IMU data. Our\nexperiments show that our proposed method is able to accurately localize a\ntarget person with 80.7% accuracy using only 5 seconds of IMU data and video.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 16:15:01 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 13:57:23 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Sun", "Xi", ""], ["Weng", "Xinshuo", ""], ["Kitani", "Kris", ""]]}, {"id": "2006.09962", "submitter": "Rita Pucci", "authors": "Rita Pucci, Jitendra Shankaraiah, Devcharan Jathanna, Ullas Karanth,\n  and Kartic Subr", "title": "WhoAmI: An Automatic Tool for Visual Recognition of Tiger and Leopard\n  Individuals in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photographs of wild animals in their natural habitats can be recorded\nunobtrusively via cameras that are triggered by motion nearby. The installation\nof such camera traps is becoming increasingly common across the world. Although\nthis is a convenient source of invaluable data for biologists, ecologists and\nconservationists, the arduous task of poring through potentially millions of\npictures each season introduces prohibitive costs and frustrating delays. We\ndevelop automatic algorithms that are able to detect animals, identify the\nspecies of animals and to recognize individual animals for two species. we\npropose the first fully-automatic tool that can recognize specific individuals\nof leopard and tiger due to their characteristic body markings. We adopt a\nclass of supervised learning approach of machine learning where a Deep\nConvolutional Neural Network (DCNN) is trained using several instances of\nmanually-labelled images for each of the three classification tasks. We\ndemonstrate the effectiveness of our approach on a data set of camera-trap\nimages recorded in the jungles of Southern India.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 16:17:46 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Pucci", "Rita", ""], ["Shankaraiah", "Jitendra", ""], ["Jathanna", "Devcharan", ""], ["Karanth", "Ullas", ""], ["Subr", "Kartic", ""]]}, {"id": "2006.09965", "submitter": "Fabian Mentzer", "authors": "Fabian Mentzer, George Toderici, Michael Tschannen, Eirikur Agustsson", "title": "High-Fidelity Generative Image Compression", "comments": "This is the Camera Ready version for NeurIPS 2020. Project page:\n  https://hific.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extensively study how to combine Generative Adversarial Networks and\nlearned compression to obtain a state-of-the-art generative lossy compression\nsystem. In particular, we investigate normalization layers, generator and\ndiscriminator architectures, training strategies, as well as perceptual losses.\nIn contrast to previous work, i) we obtain visually pleasing reconstructions\nthat are perceptually similar to the input, ii) we operate in a broad range of\nbitrates, and iii) our approach can be applied to high-resolution images. We\nbridge the gap between rate-distortion-perception theory and practice by\nevaluating our approach both quantitatively with various perceptual metrics,\nand with a user study. The study shows that our method is preferred to previous\napproaches even if they use more than 2x the bitrate.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 16:21:10 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 06:28:33 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 08:55:23 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Mentzer", "Fabian", ""], ["Toderici", "George", ""], ["Tschannen", "Michael", ""], ["Agustsson", "Eirikur", ""]]}, {"id": "2006.09987", "submitter": "Liang Shen", "authors": "Xiaotao Huang, Liang Shen, Chongyi Fan, Jiahua zhu and Sixian Chen", "title": "Multilevel Image Thresholding Using a Fully Informed Cuckoo Search\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though effective in the segmentation, conventional multilevel thresholding\nmethods are computationally expensive as exhaustive search are used for optimal\nthresholds to optimize the objective functions. To overcome this problem,\npopulation-based metaheuristic algorithms are widely used to improve the\nsearching capacity. In this paper, we improve a popular metaheuristic called\ncuckoo search using a ring topology based fully informed strategy. In this\nstrategy, each individual in the population learns from its neighborhoods to\nimprove the cooperation of the population and the learning efficiency. Best\nsolution or best fitness value can be obtained from the initial random\nthreshold values, whose quality is evaluated by the correlation function.\nExperimental results have been examined on various numbers of thresholds. The\nresults demonstrate that the proposed algorithm is more accurate and efficient\nthan other four popular methods.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 13:22:27 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Huang", "Xiaotao", ""], ["Shen", "Liang", ""], ["Fan", "Chongyi", ""], ["zhu", "Jiahua", ""], ["Chen", "Sixian", ""]]}, {"id": "2006.09989", "submitter": "Elvis Dohmatob", "authors": "Elvis Dohmatob", "title": "Classifier-independent Lower-Bounds for Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We theoretically analyse the limits of robustness to test-time adversarial\nand noisy examples in classification. Our work focuses on deriving bounds which\nuniformly apply to all classifiers (i.e all measurable functions from features\nto labels) for a given problem. Our contributions are two-fold. (1) We use\noptimal transport theory to derive variational formulae for the Bayes-optimal\nerror a classifier can make on a given classification problem, subject to\nadversarial attacks. The optimal adversarial attack is then an optimal\ntransport plan for a certain binary cost-function induced by the specific\nattack model, and can be computed via a simple algorithm based on maximal\nmatching on bipartite graphs. (2) We derive explicit lower-bounds on the\nBayes-optimal error in the case of the popular distance-based attacks. These\nbounds are universal in the sense that they depend on the geometry of the\nclass-conditional distributions of the data, but not on a particular\nclassifier. Our results are in sharp contrast with the existing literature,\nwherein adversarial vulnerability of classifiers is derived as a consequence of\nnonzero ordinary test error.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 16:46:39 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 13:33:31 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 17:57:17 GMT"}, {"version": "v4", "created": "Tue, 23 Jun 2020 17:56:29 GMT"}, {"version": "v5", "created": "Tue, 7 Jul 2020 17:39:22 GMT"}, {"version": "v6", "created": "Tue, 10 Nov 2020 00:32:30 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Dohmatob", "Elvis", ""]]}, {"id": "2006.09994", "submitter": "Kai Xiao", "authors": "Kai Xiao and Logan Engstrom and Andrew Ilyas and Aleksander Madry", "title": "Noise or Signal: The Role of Image Backgrounds in Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assess the tendency of state-of-the-art object recognition models to\ndepend on signals from image backgrounds. We create a toolkit for disentangling\nforeground and background signal on ImageNet images, and find that (a) models\ncan achieve non-trivial accuracy by relying on the background alone, (b) models\noften misclassify images even in the presence of correctly classified\nforegrounds--up to 87.5% of the time with adversarially chosen backgrounds, and\n(c) more accurate models tend to depend on backgrounds less. Our analysis of\nbackgrounds brings us closer to understanding which correlations machine\nlearning models use, and how they determine models' out of distribution\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 16:54:43 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Xiao", "Kai", ""], ["Engstrom", "Logan", ""], ["Ilyas", "Andrew", ""], ["Madry", "Aleksander", ""]]}, {"id": "2006.10004", "submitter": "Tamara Grossmann", "authors": "Tamara G. Grossmann, Yury Korolev, Guy Gilboa, Carola-Bibiane\n  Sch\\\"onlieb", "title": "Deeply Learned Spectral Total Variation Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear spectral decompositions of images based on one-homogeneous\nfunctionals such as total variation have gained considerable attention in the\nlast few years. Due to their ability to extract spectral components\ncorresponding to objects of different size and contrast, such decompositions\nenable filtering, feature transfer, image fusion and other applications.\nHowever, obtaining this decomposition involves solving multiple non-smooth\noptimisation problems and is therefore computationally highly intensive. In\nthis paper, we present a neural network approximation of a non-linear spectral\ndecomposition. We report up to four orders of magnitude ($\\times 10,000$)\nspeedup in processing of mega-pixel size images, compared to classical GPU\nimplementations. Our proposed network, TVSpecNET, is able to implicitly learn\nthe underlying PDE and, despite being entirely data driven, inherits\ninvariances of the model based transform. To the best of our knowledge, this is\nthe first approach towards learning a non-linear spectral decomposition of\nimages. Not only do we gain a staggering computational advantage, but this\napproach can also be seen as a step towards studying neural networks that can\ndecompose an image into spectral components defined by a user rather than a\nhandcrafted functional.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 17:10:43 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 17:03:54 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Grossmann", "Tamara G.", ""], ["Korolev", "Yury", ""], ["Gilboa", "Guy", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2006.10011", "submitter": "Lukas Hahn", "authors": "Lukas Hahn and Frederik Hasecke and Anton Kummert", "title": "Fast Object Classification and Meaningful Data Representation of\n  Segmented Lidar Instances", "comments": "6 pages, 5 figures, 4 tables; accepted to appear in IEEE ITSC 2020", "journal-ref": "2020 IEEE 23rd International Conference on Intelligent\n  Transportation Systems (ITSC), Rhodes, Greece, 2020, pp. 1-6", "doi": "10.1109/ITSC45102.2020.9294217", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection algorithms for Lidar data have seen numerous publications in\nrecent years, reporting good results on dataset benchmarks oriented towards\nautomotive requirements. Nevertheless, many of these are not deployable to\nembedded vehicle systems, as they require immense computational power to be\nexecuted close to real time. In this work, we propose a way to facilitate\nreal-time Lidar object classification on CPU. We show how our approach uses\nsegmented object instances to extract important features, enabling a\ncomputationally efficient batch-wise classification. For this, we introduce a\ndata representation which translates three-dimensional information into small\nimage patches, using decomposed normal vector images. We couple this with\ndedicated object statistics to handle edge cases. We apply our method on the\ntasks of object detection and semantic segmentation, as well as the relatively\nnew challenge of panoptic segmentation. Through evaluation, we show, that our\nalgorithm is capable of producing good results on public data, while running in\nreal time on CPU without using specific optimisation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 17:16:38 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Hahn", "Lukas", ""], ["Hasecke", "Frederik", ""], ["Kummert", "Anton", ""]]}, {"id": "2006.10029", "submitter": "Ting Chen", "authors": "Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey\n  Hinton", "title": "Big Self-Supervised Models are Strong Semi-Supervised Learners", "comments": "NeurIPS'2020. Code and pretrained models at\n  https://github.com/google-research/simclr", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One paradigm for learning from few labeled examples while making best use of\na large amount of unlabeled data is unsupervised pretraining followed by\nsupervised fine-tuning. Although this paradigm uses unlabeled data in a\ntask-agnostic way, in contrast to common approaches to semi-supervised learning\nfor computer vision, we show that it is surprisingly effective for\nsemi-supervised learning on ImageNet. A key ingredient of our approach is the\nuse of big (deep and wide) networks during pretraining and fine-tuning. We find\nthat, the fewer the labels, the more this approach (task-agnostic use of\nunlabeled data) benefits from a bigger network. After fine-tuning, the big\nnetwork can be further improved and distilled into a much smaller one with\nlittle loss in classification accuracy by using the unlabeled examples for a\nsecond time, but in a task-specific way. The proposed semi-supervised learning\nalgorithm can be summarized in three steps: unsupervised pretraining of a big\nResNet model using SimCLRv2, supervised fine-tuning on a few labeled examples,\nand distillation with unlabeled examples for refining and transferring the\ntask-specific knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy\nwith just 1% of the labels ($\\le$13 labeled images per class) using ResNet-50,\na $10\\times$ improvement in label efficiency over the previous\nstate-of-the-art. With 10% of labels, ResNet-50 trained with our method\nachieves 77.5% top-1 accuracy, outperforming standard supervised training with\nall of the labels.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 17:48:22 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 03:09:28 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Chen", "Ting", ""], ["Kornblith", "Simon", ""], ["Swersky", "Kevin", ""], ["Norouzi", "Mohammad", ""], ["Hinton", "Geoffrey", ""]]}, {"id": "2006.10034", "submitter": "Matthew Chang", "authors": "Matthew Chang, Arjun Gupta, Saurabh Gupta", "title": "Semantic Visual Navigation by Watching YouTube Videos", "comments": "NeurIPS 2020. Project website with code, models, and videos:\n  https://matthewchang.github.io/value-learning-from-video", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic cues and statistical regularities in real-world environment layouts\ncan improve efficiency for navigation in novel environments. This paper learns\nand leverages such semantic cues for navigating to objects of interest in novel\nenvironments, by simply watching YouTube videos. This is challenging because\nYouTube videos don't come with labels for actions or goals, and may not even\nshowcase optimal behavior. Our method tackles these challenges through the use\nof Q-learning on pseudo-labeled transition quadruples (image, action, next\nimage, reward). We show that such off-policy Q-learning from passive data is\nable to learn meaningful semantic cues for navigation. These cues, when used in\na hierarchical navigation policy, lead to improved efficiency at the ObjectGoal\ntask in visually realistic simulations. We observe a relative improvement of\n15-83% over end-to-end RL, behavior cloning, and classical methods, while using\nminimal direct interaction.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 17:56:00 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 05:46:46 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Chang", "Matthew", ""], ["Gupta", "Arjun", ""], ["Gupta", "Saurabh", ""]]}, {"id": "2006.10039", "submitter": "Sylvestre-Alvise Rebuffi", "authors": "Sylvestre-Alvise Rebuffi, Sebastien Ehrhardt, Kai Han, Andrea Vedaldi,\n  Andrew Zisserman", "title": "LSD-C: Linearly Separable Deep Clusters", "comments": "Code available at https://github.com/srebuffi/lsd-clusters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LSD-C, a novel method to identify clusters in an unlabeled\ndataset. Our algorithm first establishes pairwise connections in the feature\nspace between the samples of the minibatch based on a similarity metric. Then\nit regroups in clusters the connected samples and enforces a linear separation\nbetween clusters. This is achieved by using the pairwise connections as targets\ntogether with a binary cross-entropy loss on the predictions that the\nassociated pairs of samples belong to the same cluster. This way, the feature\nrepresentation of the network will evolve such that similar samples in this\nfeature space will belong to the same linearly separated cluster. Our method\ndraws inspiration from recent semi-supervised learning practice and proposes to\ncombine our clustering algorithm with self-supervised pretraining and strong\ndata augmentation. We show that our approach significantly outperforms\ncompetitors on popular public image benchmarks including CIFAR 10/100, STL 10\nand MNIST, as well as the document classification dataset Reuters 10K.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 17:58:10 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Rebuffi", "Sylvestre-Alvise", ""], ["Ehrhardt", "Sebastien", ""], ["Han", "Kai", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2006.10042", "submitter": "Yichao Zhou", "authors": "Yichao Zhou, Shichen Liu, Yi Ma", "title": "Learning to Detect 3D Reflection Symmetry for Single-View Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction from a single RGB image is a challenging problem in\ncomputer vision. Previous methods are usually solely data-driven, which lead to\ninaccurate 3D shape recovery and limited generalization capability. In this\nwork, we focus on object-level 3D reconstruction and present a geometry-based\nend-to-end deep learning framework that first detects the mirror plane of\nreflection symmetry that commonly exists in man-made objects and then predicts\ndepth maps by finding the intra-image pixel-wise correspondence of the\nsymmetry. Our method fully utilizes the geometric cues from symmetry during the\ntest time by building plane-sweep cost volumes, a powerful tool that has been\nused in multi-view stereopsis. To our knowledge, this is the first work that\nuses the concept of cost volumes in the setting of single-image 3D\nreconstruction. We conduct extensive experiments on the ShapeNet dataset and\nfind that our reconstruction method significantly outperforms the previous\nstate-of-the-art single-view 3D reconstruction networks in term of the accuracy\nof camera poses and depth maps, without requiring objects being completely\nsymmetric. Code is available at https://github.com/zhou13/symmetrynet.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 17:58:59 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Zhou", "Yichao", ""], ["Liu", "Shichen", ""], ["Ma", "Yi", ""]]}, {"id": "2006.10079", "submitter": "R\\'emi Cad\\`ene", "authors": "Corentin Dancette and Remi Cadene and Xinlei Chen and Matthieu Cord", "title": "Overcoming Statistical Shortcuts for Open-ended Visual Counting", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models tend to over-rely on statistical shortcuts. These\nspurious correlations between parts of the input and the output labels does not\nhold in real-world settings. We target this issue on the recent open-ended\nvisual counting task which is well suited to study statistical shortcuts. We\naim to develop models that learn a proper mechanism of counting regardless of\nthe output label. First, we propose the Modifying Count Distribution (MCD)\nprotocol, which penalizes models that over-rely on statistical shortcuts. It is\nbased on pairs of training and testing sets that do not follow the same count\nlabel distribution such as the odd-even sets. Intuitively, models that have\nlearned a proper mechanism of counting on odd numbers should perform well on\neven numbers. Secondly, we introduce the Spatial Counting Network (SCN), which\nis dedicated to visual analysis and counting based on natural language\nquestions. Our model selects relevant image regions, scores them with fusion\nand self-attention mechanisms, and provides a final counting score. We apply\nour protocol on the recent dataset, TallyQA, and show superior performances\ncompared to state-of-the-art models. We also demonstrate the ability of our\nmodel to select the correct instances to count in the image. Code and datasets\nare available: https://github.com/cdancette/spatial-counting-network\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 18:02:01 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 11:04:02 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Dancette", "Corentin", ""], ["Cadene", "Remi", ""], ["Chen", "Xinlei", ""], ["Cord", "Matthieu", ""]]}, {"id": "2006.10125", "submitter": "Nathan Wang", "authors": "Petteri Haverinen, Krithik Ramesh, Nathan Wang", "title": "Sustainable Recreational Fishing Using a Novel Electrical Muscle\n  Stimulation (EMS) Lure and Ensemble Network Algorithm to Maximize Catch and\n  Release Survivability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With 200-700 million anglers in the world, sportfishing is nearly five times\nmore common than commercial trawling. Worldwide, hundreds of thousands of jobs\nare linked to the sportfishing industry, which generates billions of dollars\nfor water-side communities and fisheries conservatories alike. However, the\nsheer popularity of recreational fishing poses threats to aquatic biodiversity\nthat are hard to regulate. For example, as much as 25% of overfished\npopulations can be traced to anglers. This alarming statistic is explained by\nthe average catch and release mortality rate of 43%, which primarily results\nfrom hook-related injuries and careless out-of-water handling. The\nprovisional-patented design proposed in this paper addresses both these\nproblems separately First, a novel, electrical muscle stimulation based fishing\nlure is proposed as a harmless and low cost alternative to sharp hooks. Early\nprototypes show a constant electrical current of 90 mA applied through a 200g\nEuropean perch's jaw can support a reeling tension of 2N - safely within the\nnecessary ranges. Second, a fish-eye camera bob is designed to wirelessly relay\nunderwater footage to a smartphone app, where an ensemble convolutional neural\nnetwork automatically classifies the fish's species, estimates its length, and\ncross references with local and state fishing regulations (ie. minimum size,\nmaximum bag limit, and catch season). This capability reduces overfishing by\nhelping anglers avoid accidentally violating guidelines and eliminates the need\nto reel the fish in and expose it to negligent handling. IN conjunction, this\ncheap, lightweight, yet high-tech invention is a paradigm shift in preserving a\nworld favorite pastime; while at the same time making recreational fishing more\nsustainable.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 20:35:33 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Haverinen", "Petteri", ""], ["Ramesh", "Krithik", ""], ["Wang", "Nathan", ""]]}, {"id": "2006.10132", "submitter": "Ziqiang Li", "authors": "Ziqiang Li, Rentuo Tao, Hongjing Niu, Bin Li", "title": "Interpreting the Latent Space of GANs via Correlation Analysis for\n  Controllable Concept Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial nets (GANs) have been successfully applied in many\nfields like image generation, inpainting, super-resolution and drug discovery,\netc., by now, the inner process of GANs is far from been understood. To get\ndeeper insight of the intrinsic mechanism of GANs, in this paper, a method for\ninterpreting the latent space of GANs by analyzing the correlation between\nlatent variables and the corresponding semantic contents in generated images is\nproposed. Unlike previous methods that focus on dissecting models via feature\nvisualization, the emphasis of this work is put on the variables in latent\nspace, i.e. how the latent variables affect the quantitative analysis of\ngenerated results. Given a pretrained GAN model with weights fixed, the latent\nvariables are intervened to analyze their effect on the semantic content in\ngenerated images. A set of controlling latent variables can be derived for\nspecific content generation, and the controllable semantic content manipulation\nbe achieved. The proposed method is testified on the datasets Fashion-MNIST and\nUT Zappos50K, experiment results show its effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 03:50:27 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 15:07:58 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Li", "Ziqiang", ""], ["Tao", "Rentuo", ""], ["Niu", "Hongjing", ""], ["Li", "Bin", ""]]}, {"id": "2006.10135", "submitter": "Tao Zhou", "authors": "Tao Zhou, Huazhu Fu, Yu Zhang, Changqing Zhang, Xiankai Lu, Jianbing\n  Shen, and Ling Shao", "title": "M2Net: Multi-modal Multi-channel Network for Overall Survival Time\n  Prediction of Brain Tumor Patients", "comments": "Accepted by MICCAI'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early and accurate prediction of overall survival (OS) time can help to\nobtain better treatment planning for brain tumor patients. Although many OS\ntime prediction methods have been developed and obtain promising results, there\nare still several issues. First, conventional prediction methods rely on\nradiomic features at the local lesion area of a magnetic resonance (MR) volume,\nwhich may not represent the full image or model complex tumor patterns. Second,\ndifferent types of scanners (i.e., multi-modal data) are sensitive to different\nbrain regions, which makes it challenging to effectively exploit the\ncomplementary information across multiple modalities and also preserve the\nmodality-specific properties. Third, existing methods focus on prediction\nmodels, ignoring complex data-to-label relationships. To address the above\nissues, we propose an end-to-end OS time prediction model; namely, Multi-modal\nMulti-channel Network (M2Net). Specifically, we first project the 3D MR volume\nonto 2D images in different directions, which reduces computational costs,\nwhile preserving important information and enabling pre-trained models to be\ntransferred from other tasks. Then, we use a modality-specific network to\nextract implicit and high-level features from different MR scans. A multi-modal\nshared network is built to fuse these features using a bilinear pooling model,\nexploiting their correlations to provide complementary information. Finally, we\nintegrate the outputs from each modality-specific network and the multi-modal\nshared network to generate the final prediction result. Experimental results\ndemonstrate the superiority of our M2Net model over other methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 05:21:37 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 18:47:11 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Zhou", "Tao", ""], ["Fu", "Huazhu", ""], ["Zhang", "Yu", ""], ["Zhang", "Changqing", ""], ["Lu", "Xiankai", ""], ["Shen", "Jianbing", ""], ["Shao", "Ling", ""]]}, {"id": "2006.10138", "submitter": "Qi Qi", "authors": "Qi Qi, Zhishuai Guo, Yi Xu, Rong Jin, Tianbao Yang", "title": "An Online Method for Distributionally Deep Robust Optimization", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a practical online method for solving a\ndistributionally robust optimization (DRO) for deep learning, which has\nimportant applications in machine learning for improving the robustness of\nneural networks. In the literature, most methods for solving DRO are based on\nstochastic primal-dual methods. However, primal-dual methods for deep DRO\nsuffer from several drawbacks: (1) manipulating a high-dimensional dual\nvariable corresponding to the size of data is time expensive; (2) they are not\nfriendly to online learning where data is coming sequentially. To address these\nissues, we transform the min-max formulation into a minimization formulation\nand propose a practical duality-free online stochastic method for solving deep\nDRO with KL divergence regularization. The proposed online stochastic method\nresembles the practical stochastic Nesterovs method in several perspectives\nthat are widely used for learning deep neural networks. Under a\nPolyak-Lojasiewicz (PL) condition, we prove that the proposed method can enjoy\nan optimal sample complexity without any requirements on large batch size. Of\nindependent interest, the proposed method can be also used for solving a family\nof stochastic compositional problems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 20:19:25 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 07:15:56 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 20:39:47 GMT"}, {"version": "v4", "created": "Tue, 24 Nov 2020 22:24:01 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Qi", "Qi", ""], ["Guo", "Zhishuai", ""], ["Xu", "Yi", ""], ["Jin", "Rong", ""], ["Yang", "Tianbao", ""]]}, {"id": "2006.10147", "submitter": "Radu Tudor Ionescu", "authors": "Nicolae-C\\u{a}t\\u{a}lin Ristea, Radu Tudor Ionescu", "title": "Are you wearing a mask? Improving mask detection from speech using\n  augmentation by cycle-consistent GANs", "comments": "Accepted at INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of detecting whether a person wears a face mask from speech is\nuseful in modelling speech in forensic investigations, communication between\nsurgeons or people protecting themselves against infectious diseases such as\nCOVID-19. In this paper, we propose a novel data augmentation approach for mask\ndetection from speech. Our approach is based on (i) training Generative\nAdversarial Networks (GANs) with cycle-consistency loss to translate unpaired\nutterances between two classes (with mask and without mask), and on (ii)\ngenerating new training utterances using the cycle-consistent GANs, assigning\nopposite labels to each translated utterance. Original and translated\nutterances are converted into spectrograms which are provided as input to a set\nof ResNet neural networks with various depths. The networks are combined into\nan ensemble through a Support Vector Machines (SVM) classifier. With this\nsystem, we participated in the Mask Sub-Challenge (MSC) of the INTERSPEECH 2020\nComputational Paralinguistics Challenge, surpassing the baseline proposed by\nthe organizers by 2.8%. Our data augmentation technique provided a performance\nboost of 0.9% on the private test set. Furthermore, we show that our data\naugmentation approach yields better results than other baseline and\nstate-of-the-art augmentation methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 20:46:50 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 21:52:21 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ristea", "Nicolae-C\u0103t\u0103lin", ""], ["Ionescu", "Radu Tudor", ""]]}, {"id": "2006.10166", "submitter": "Lin Zhang", "authors": "Lin Zhang, Valery Vishnevskiy, Orcun Goksel", "title": "Deep Network for Scatterer Distribution Estimation for Ultrasound Image\n  Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation-based ultrasound training can be an essential educational tool.\nRealistic ultrasound image appearance with typical speckle texture can be\nmodeled as convolution of a point spread function with point scatterers\nrepresenting tissue microstructure. Such scatterer distribution, however, is in\ngeneral not known and its estimation for a given tissue type is fundamentally\nan ill-posed inverse problem. In this paper, we demonstrate a convolutional\nneural network approach for probabilistic scatterer estimation from observed\nultrasound data. We herein propose to impose a known statistical distribution\non scatterers and learn the mapping between ultrasound image and distribution\nparameter map by training a convolutional neural network on synthetic images.\nIn comparison with several existing approaches, we demonstrate in numerical\nsimulations and with in-vivo images that the synthesized images from scatterer\nrepresentations estimated with our approach closely match the observations with\nvarying acquisition parameters such as compression and rotation of the imaged\ndomain.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 21:25:13 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Zhang", "Lin", ""], ["Vishnevskiy", "Valery", ""], ["Goksel", "Orcun", ""]]}, {"id": "2006.10172", "submitter": "Orly Liba", "authors": "Orly Liba, Longqi Cai, Yun-Ta Tsai, Elad Eban, Yair Movshovitz-Attias,\n  Yael Pritch, Huizhong Chen, Jonathan T. Barron", "title": "Sky Optimization: Semantically aware image processing of skies in\n  low-light photography", "comments": "Published in Proceedings of the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition Workshops. 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The sky is a major component of the appearance of a photograph, and its color\nand tone can strongly influence the mood of a picture. In nighttime\nphotography, the sky can also suffer from noise and color artifacts. For this\nreason, there is a strong desire to process the sky in isolation from the rest\nof the scene to achieve an optimal look. In this work, we propose an automated\nmethod, which can run as a part of a camera pipeline, for creating accurate sky\nalpha-masks and using them to improve the appearance of the sky. Our method\nperforms end-to-end sky optimization in less than half a second per image on a\nmobile device. We introduce a method for creating an accurate sky-mask dataset\nthat is based on partially annotated images that are inpainted and refined by\nour modified weighted guided filter. We use this dataset to train a neural\nnetwork for semantic sky segmentation. Due to the compute and power constraints\nof mobile devices, sky segmentation is performed at a low image resolution. Our\nmodified weighted guided filter is used for edge-aware upsampling to resize the\nalpha-mask to a higher resolution. With this detailed mask we automatically\napply post-processing steps to the sky in isolation, such as automatic\nspatially varying white-balance, brightness adjustments, contrast enhancement,\nand noise reduction.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 20:19:12 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Liba", "Orly", ""], ["Cai", "Longqi", ""], ["Tsai", "Yun-Ta", ""], ["Eban", "Elad", ""], ["Movshovitz-Attias", "Yair", ""], ["Pritch", "Yael", ""], ["Chen", "Huizhong", ""], ["Barron", "Jonathan T.", ""]]}, {"id": "2006.10178", "submitter": "Atanas Mirchev", "authors": "Atanas Mirchev, Baris Kayalibay, Patrick van der Smagt and Justin\n  Bayer", "title": "Variational State-Space Models for Localisation and Dense 3D Mapping in\n  6 DoF", "comments": "Update for ICLR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve the problem of 6-DoF localisation and 3D dense reconstruction in\nspatial environments as approximate Bayesian inference in a deep state-space\nmodel. Our approach leverages both learning and domain knowledge from\nmultiple-view geometry and rigid-body dynamics. This results in an expressive\npredictive model of the world, often missing in current state-of-the-art visual\nSLAM solutions. The combination of variational inference, neural networks and a\ndifferentiable raycaster ensures that our model is amenable to end-to-end\ngradient-based optimisation. We evaluate our approach on realistic unmanned\naerial vehicle flight data, nearing the performance of state-of-the-art\nvisual-inertial odometry systems. We demonstrate the applicability of the model\nto generative prediction and planning.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 22:06:35 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 07:53:09 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 17:11:08 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Mirchev", "Atanas", ""], ["Kayalibay", "Baris", ""], ["van der Smagt", "Patrick", ""], ["Bayer", "Justin", ""]]}, {"id": "2006.10187", "submitter": "Jiahao Pang", "authors": "Jiahao Pang, Duanshun Li, Dong Tian", "title": "TearingNet: Point Cloud Autoencoder to Learn Topology-Friendly\n  Representations", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topology matters. Despite the recent success of point cloud processing with\ngeometric deep learning, it remains arduous to capture the complex topologies\nof point cloud data with a learning model. Given a point cloud dataset\ncontaining objects with various genera, or scenes with multiple objects, we\npropose an autoencoder, TearingNet, which tackles the challenging task of\nrepresenting the point clouds using a fixed-length descriptor. Unlike existing\nworks directly deforming predefined primitives of genus zero (e.g., a 2D square\npatch) to an object-level point cloud, our TearingNet is characterized by a\nproposed Tearing network module and a Folding network module interacting with\neach other iteratively. Particularly, the Tearing network module learns the\npoint cloud topology explicitly. By breaking the edges of a primitive graph, it\ntears the graph into patches or with holes to emulate the topology of a target\npoint cloud, leading to faithful reconstructions. Experimentation shows the\nsuperiority of our proposal in terms of reconstructing point clouds as well as\ngenerating more topology-friendly representations than benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 22:42:43 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 21:24:30 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 23:05:59 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Pang", "Jiahao", ""], ["Li", "Duanshun", ""], ["Tian", "Dong", ""]]}, {"id": "2006.10199", "submitter": "Mohammad Rami Koujan", "authors": "Michail Christos Doukas, Mohammad Rami Koujan, Viktoriia Sharmanska,\n  Anastasios Roussos", "title": "Head2Head++: Deep Facial Attributes Re-Targeting", "comments": "Submitted to the IEEE Transactions on Biometrics, Behavior, and\n  Identity Science (TBIOM) journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial video re-targeting is a challenging problem aiming to modify the\nfacial attributes of a target subject in a seamless manner by a driving\nmonocular sequence. We leverage the 3D geometry of faces and Generative\nAdversarial Networks (GANs) to design a novel deep learning architecture for\nthe task of facial and head reenactment. Our method is different to purely 3D\nmodel-based approaches, or recent image-based methods that use Deep\nConvolutional Neural Networks (DCNNs) to generate individual frames. We manage\nto capture the complex non-rigid facial motion from the driving monocular\nperformances and synthesise temporally consistent videos, with the aid of a\nsequential Generator and an ad-hoc Dynamics Discriminator network. We conduct a\ncomprehensive set of quantitative and qualitative tests and demonstrate\nexperimentally that our proposed method can successfully transfer facial\nexpressions, head pose and eye gaze from a source video to a target subject, in\na photo-realistic and faithful fashion, better than other state-of-the-art\nmethods. Most importantly, our system performs end-to-end reenactment in nearly\nreal-time speed (18 fps).\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 23:38:37 GMT"}], "update_date": "2020-06-20", "authors_parsed": [["Doukas", "Michail Christos", ""], ["Koujan", "Mohammad Rami", ""], ["Sharmanska", "Viktoriia", ""], ["Roussos", "Anastasios", ""]]}, {"id": "2006.10202", "submitter": "Yurun Tian", "authors": "Yurun Tian, Axel Barroso-Laguna, Tony Ng, Vassileios Balntas, Krystian\n  Mikolajczyk", "title": "HyNet: Learning Local Descriptor with Hybrid Similarity Measure and\n  Triplet Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works show that local descriptor learning benefits from the use of L2\nnormalisation, however, an in-depth analysis of this effect lacks in the\nliterature. In this paper, we investigate how L2 normalisation affects the\nback-propagated descriptor gradients during training. Based on our\nobservations, we propose HyNet, a new local descriptor that leads to\nstate-of-the-art results in matching. HyNet introduces a hybrid similarity\nmeasure for triplet margin loss, a regularisation term constraining the\ndescriptor norm, and a new network architecture that performs L2 normalisation\nof all intermediate feature maps and the output descriptors. HyNet surpasses\nprevious methods by a significant margin on standard benchmarks that include\npatch matching, verification, and retrieval, as well as outperforming full\nend-to-end methods on 3D reconstruction tasks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 23:49:45 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 11:02:37 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 16:25:48 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Tian", "Yurun", ""], ["Barroso-Laguna", "Axel", ""], ["Ng", "Tony", ""], ["Balntas", "Vassileios", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "2006.10204", "submitter": "Valentin Bazarevsky", "authors": "Valentin Bazarevsky, Ivan Grishchenko, Karthik Raveendran, Tyler Zhu,\n  Fan Zhang, Matthias Grundmann", "title": "BlazePose: On-device Real-time Body Pose tracking", "comments": "4 pages, 6 figures; CVPR Workshop on Computer Vision for Augmented\n  and Virtual Reality, Seattle, WA, USA, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BlazePose, a lightweight convolutional neural network architecture\nfor human pose estimation that is tailored for real-time inference on mobile\ndevices. During inference, the network produces 33 body keypoints for a single\nperson and runs at over 30 frames per second on a Pixel 2 phone. This makes it\nparticularly suited to real-time use cases like fitness tracking and sign\nlanguage recognition. Our main contributions include a novel body pose tracking\nsolution and a lightweight body pose estimation neural network that uses both\nheatmaps and regression to keypoint coordinates.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 23:52:46 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Bazarevsky", "Valentin", ""], ["Grishchenko", "Ivan", ""], ["Raveendran", "Karthik", ""], ["Zhu", "Tyler", ""], ["Zhang", "Fan", ""], ["Grundmann", "Matthias", ""]]}, {"id": "2006.10211", "submitter": "Pradeep Kumar Jayaraman", "authors": "Pradeep Kumar Jayaraman, Aditya Sanghi, Joseph G. Lambourne, Karl D.D.\n  Willis, Thomas Davies, Hooman Shayani, Nigel Morris", "title": "UV-Net: Learning from Boundary Representations", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce UV-Net, a novel neural network architecture and representation\ndesigned to operate directly on Boundary representation (B-rep) data from 3D\nCAD models. The B-rep format is widely used in the design, simulation and\nmanufacturing industries to enable sophisticated and precise CAD modeling\noperations. However, B-rep data presents some unique challenges when used with\nmodern machine learning due to the complexity of the data structure and its\nsupport for both continuous non-Euclidean geometric entities and discrete\ntopological entities. In this paper, we propose a unified representation for\nB-rep data that exploits the U and V parameter domain of curves and surfaces to\nmodel geometry, and an adjacency graph to explicitly model topology. This leads\nto a unique and efficient network architecture, UV-Net, that couples image and\ngraph convolutional neural networks in a compute and memory-efficient manner.\nTo aid in future research we present a synthetic labelled B-rep dataset,\nSolidLetters, derived from human designed fonts with variations in both\ngeometry and topology. Finally we demonstrate that UV-Net can generalize to\nsupervised and unsupervised tasks on five datasets, while outperforming\nalternate 3D shape representations such as point clouds, voxels, and meshes.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 00:12:52 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 03:27:44 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Jayaraman", "Pradeep Kumar", ""], ["Sanghi", "Aditya", ""], ["Lambourne", "Joseph G.", ""], ["Willis", "Karl D. D.", ""], ["Davies", "Thomas", ""], ["Shayani", "Hooman", ""], ["Morris", "Nigel", ""]]}, {"id": "2006.10214", "submitter": "Valentin Bazarevsky", "authors": "Fan Zhang, Valentin Bazarevsky, Andrey Vakunov, Andrei Tkachenka,\n  George Sung, Chuo-Ling Chang, Matthias Grundmann", "title": "MediaPipe Hands: On-device Real-time Hand Tracking", "comments": "5 pages, 7 figures; CVPR Workshop on Computer Vision for Augmented\n  and Virtual Reality, Seattle, WA, USA, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a real-time on-device hand tracking pipeline that predicts hand\nskeleton from single RGB camera for AR/VR applications. The pipeline consists\nof two models: 1) a palm detector, 2) a hand landmark model. It's implemented\nvia MediaPipe, a framework for building cross-platform ML solutions. The\nproposed model and pipeline architecture demonstrates real-time inference speed\non mobile GPUs and high prediction quality. MediaPipe Hands is open sourced at\nhttps://mediapipe.dev.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 00:19:13 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Zhang", "Fan", ""], ["Bazarevsky", "Valentin", ""], ["Vakunov", "Andrey", ""], ["Tkachenka", "Andrei", ""], ["Sung", "George", ""], ["Chang", "Chuo-Ling", ""], ["Grundmann", "Matthias", ""]]}, {"id": "2006.10216", "submitter": "Wanyue Li", "authors": "Wanyue Li, Wen Kong, Yiwei Chen, Jing Wang, Yi He, Guohua Shi, Guohua\n  Deng", "title": "Generating Fundus Fluorescence Angiography Images from Structure Fundus\n  Images Using Generative Adversarial Networks", "comments": "16 pages, 6 figures, accepted by Medical Imaging on Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluorescein angiography can provide a map of retinal vascular structure and\nfunction, which is commonly used in ophthalmology diagnosis, however, this\nimaging modality may pose risks of harm to the patients. To help physicians\nreduce the potential risks of diagnosis, an image translation method is\nadopted. In this work, we proposed a conditional generative adversarial\nnetwork(GAN) - based method to directly learn the mapping relationship between\nstructure fundus images and fundus fluorescence angiography images. Moreover,\nlocal saliency maps, which define each pixel's importance, are used to define a\nnovel saliency loss in the GAN cost function. This facilitates more accurate\nlearning of small-vessel and fluorescein leakage features.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 00:27:20 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Li", "Wanyue", ""], ["Kong", "Wen", ""], ["Chen", "Yiwei", ""], ["Wang", "Jing", ""], ["He", "Yi", ""], ["Shi", "Guohua", ""], ["Deng", "Guohua", ""]]}, {"id": "2006.10219", "submitter": "Razvan Caramalau", "authors": "Razvan Caramalau, Binod Bhattarai, Tae-Kyun Kim", "title": "Sequential Graph Convolutional Network for Active Learning", "comments": "Accepted as Poster at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel pool-based Active Learning framework constructed on a\nsequential Graph Convolution Network (GCN). Each image's feature from a pool of\ndata represents a node in the graph and the edges encode their similarities.\nWith a small number of randomly sampled images as seed labelled examples, we\nlearn the parameters of the graph to distinguish labelled vs unlabelled nodes\nby minimising the binary cross-entropy loss. GCN performs message-passing\noperations between the nodes, and hence, induces similar representations of the\nstrongly associated nodes. We exploit these characteristics of GCN to select\nthe unlabelled examples which are sufficiently different from labelled ones. To\nthis end, we utilise the graph node embeddings and their confidence scores and\nadapt sampling techniques such as CoreSet and uncertainty-based methods to\nquery the nodes. We flip the label of newly queried nodes from unlabelled to\nlabelled, re-train the learner to optimise the downstream task and the graph to\nminimise its modified objective. We continue this process within a fixed\nbudget. We evaluate our method on 6 different benchmarks:4 real image\nclassification, 1 depth-based hand pose estimation and 1 synthetic RGB image\nclassification datasets. Our method outperforms several competitive baselines\nsuch as VAAL, Learning Loss, CoreSet and attains the new state-of-the-art\nperformance on multiple applications The implementations can be found here:\nhttps://github.com/razvancaramalau/Sequential-GCN-for-Active-Learning\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 00:55:10 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 14:07:05 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 16:18:44 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Caramalau", "Razvan", ""], ["Bhattarai", "Binod", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "2006.10250", "submitter": "Guoqiang Zhong", "authors": "Jinxuan Sun, Yang Chen, Junyu Dong and Guoqiang Zhong", "title": "Progressively Unfreezing Perceptual GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are widely used in image generation\ntasks, yet the generated images are usually lack of texture details. In this\npaper, we propose a general framework, called Progressively Unfreezing\nPerceptual GAN (PUPGAN), which can generate images with fine texture details.\nParticularly, we propose an adaptive perceptual discriminator with a\npre-trained perceptual feature extractor, which can efficiently measure the\ndiscrepancy between multi-level features of the generated and real images. In\naddition, we propose a progressively unfreezing scheme for the adaptive\nperceptual discriminator, which ensures a smooth transfer process from a large\nscale classification task to a specified image generation task. The qualitative\nand quantitative experiments with comparison to the classical baselines on\nthree image generation tasks, i.e. single image super-resolution, paired\nimage-to-image translation and unpaired image-to-image translation demonstrate\nthe superiority of PUPGAN over the compared approaches.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 03:12:41 GMT"}], "update_date": "2020-06-20", "authors_parsed": [["Sun", "Jinxuan", ""], ["Chen", "Yang", ""], ["Dong", "Junyu", ""], ["Zhong", "Guoqiang", ""]]}, {"id": "2006.10260", "submitter": "Madhawa Vidanapathirana", "authors": "Madhawa Vidanapathirana, Supriya Pandhre, Sonia Raychaudhuri, Anjali\n  Khurana", "title": "Video Moment Localization using Object Evidence and Reverse Captioning", "comments": "7 pages. 6 figures. For source code, refer\n  https://github.com/madhawav/MML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of language-based temporal localization of moments in\nuntrimmed videos. Compared to temporal localization with fixed categories, this\nproblem is more challenging as the language-based queries have no predefined\nactivity classes and may also contain complex descriptions. Current\nstate-of-the-art model MAC addresses it by mining activity concepts from both\nvideo and language modalities. This method encodes the semantic activity\nconcepts from the verb/object pair in a language query and leverages visual\nactivity concepts from video activity classification prediction scores. We\npropose \"Multi-faceted VideoMoment Localizer\" (MML), an extension of MAC model\nby the introduction of visual object evidence via object segmentation masks and\nvideo understanding features via video captioning. Furthermore, we improve\nlanguage modelling in sentence embedding. We experimented on Charades-STA\ndataset and identified that MML outperforms MAC baseline by 4.93% and 1.70% on\nR@1 and R@5metrics respectively. Our code and pre-trained model are publicly\navailable at https://github.com/madhawav/MML.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 03:45:49 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Vidanapathirana", "Madhawa", ""], ["Pandhre", "Supriya", ""], ["Raychaudhuri", "Sonia", ""], ["Khurana", "Anjali", ""]]}, {"id": "2006.10297", "submitter": "Changhwa Park", "authors": "Changhwa Park, Jonghyun Lee, Jaeyoon Yoo, Minhoe Hur, Sungroh Yoon", "title": "Joint Contrastive Learning for Unsupervised Domain Adaptation", "comments": "16 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enhancing feature transferability by matching marginal distributions has led\nto improvements in domain adaptation, although this is at the expense of\nfeature discrimination. In particular, the ideal joint hypothesis error in the\ntarget error upper bound, which was previously considered to be minute, has\nbeen found to be significant, impairing its theoretical guarantee. In this\npaper, we propose an alternative upper bound on the target error that\nexplicitly considers the joint error to render it more manageable. With the\ntheoretical analysis, we suggest a joint optimization framework that combines\nthe source and target domains. Further, we introduce Joint Contrastive Learning\n(JCL) to find class-level discriminative features, which is essential for\nminimizing the joint error. With a solid theoretical framework, JCL employs\ncontrastive loss to maximize the mutual information between a feature and its\nlabel, which is equivalent to maximizing the Jensen-Shannon divergence between\nconditional distributions. Experiments on two real-world datasets demonstrate\nthat JCL outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 06:25:34 GMT"}], "update_date": "2020-06-20", "authors_parsed": [["Park", "Changhwa", ""], ["Lee", "Jonghyun", ""], ["Yoo", "Jaeyoon", ""], ["Hur", "Minhoe", ""], ["Yoon", "Sungroh", ""]]}, {"id": "2006.10304", "submitter": "Juan Pablo Zuluaga-Gomez", "authors": "Juan Zuluaga-Gomez and Petr Motlicek and Qingran Zhan and Karel Vesely\n  and Rudolf Braun", "title": "Automatic Speech Recognition Benchmark for Air-Traffic Communications", "comments": "Accepted to: 21st INTERSPEECH conference (Shanghai, October 25-29)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in Automatic Speech Recognition (ASR) over the last decade opened\nnew areas of speech-based automation such as in Air-Traffic Control (ATC)\nenvironment. Currently, voice communication and data links communications are\nthe only way of contact between pilots and Air-Traffic Controllers (ATCo),\nwhere the former is the most widely used and the latter is a non-spoken method\nmandatory for oceanic messages and limited for some domestic issues. ASR\nsystems on ATCo environments inherit increasing complexity due to accents from\nnon-English speakers, cockpit noise, speaker-dependent biases, and small\nin-domain ATC databases for training. Hereby, we introduce CleanSky EC-H2020\nATCO2, a project that aims to develop an ASR-based platform to collect,\norganize and automatically pre-process ATCo speech-data from air space. This\npaper conveys an exploratory benchmark of several state-of-the-art ASR models\ntrained on more than 170 hours of ATCo speech-data. We demonstrate that the\ncross-accent flaws due to speakers' accents are minimized due to the amount of\ndata, making the system feasible for ATC environments. The developed ASR system\nachieves an averaged word error rate (WER) of 7.75% across four databases. An\nadditional 35% relative improvement in WER is achieved on one test set when\ntraining a TDNNF system with byte-pair encoding.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 06:49:22 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 06:46:34 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Zuluaga-Gomez", "Juan", ""], ["Motlicek", "Petr", ""], ["Zhan", "Qingran", ""], ["Vesely", "Karel", ""], ["Braun", "Rudolf", ""]]}, {"id": "2006.10336", "submitter": "Ning Wang", "authors": "Ning Wang, Wengang Zhou, Qi Tian, Houqiang Li", "title": "Cascaded Regression Tracking: Towards Online Hard Distractor\n  Discrimination", "comments": "Accepted by IEEE TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking can be easily disturbed by similar surrounding objects. Such\nobjects as hard distractors, even though being the minority among negative\nsamples, increase the risk of target drift and model corruption, which deserve\nadditional attention in online tracking and model update. To enhance the\ntracking robustness, in this paper, we propose a cascaded regression tracker\nwith two sequential stages. In the first stage, we filter out abundant\neasily-identified negative candidates via an efficient convolutional\nregression. In the second stage, a discrete sampling based ridge regression is\ndesigned to double-check the remaining ambiguous hard samples, which serves as\nan alternative of fully-connected layers and benefits from the closed-form\nsolver for efficient learning. Extensive experiments are conducted on 11\nchallenging tracking benchmarks including OTB-2013, OTB-2015, VOT2018, VOT2019,\nUAV123, Temple-Color, NfS, TrackingNet, LaSOT, UAV20L, and OxUvA. The proposed\nmethod achieves state-of-the-art performance on prevalent benchmarks, while\nrunning in a real-time speed.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 07:48:01 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Wang", "Ning", ""], ["Zhou", "Wengang", ""], ["Tian", "Qi", ""], ["Li", "Houqiang", ""]]}, {"id": "2006.10347", "submitter": "Zhang Jian", "authors": "Shuai Zhang, Xiaoyan Xin, Yang Wang, Yachong Guo, Qiuqiao Hao,\n  Xianfeng Yang, Jun Wang, Jian Zhang, Bing Zhang, Wei Wang", "title": "Automated Radiological Report Generation For Chest X-Rays With\n  Weakly-Supervised End-to-End Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chest X-Ray (CXR) is the one of the most common clinical exam used to\ndiagnose thoracic diseases and abnormalities. The volume of CXR scans generated\ndaily in hospitals is huge. Therefore, an automated diagnosis system able to\nsave the effort of doctors is of great value. At present, the applications of\nartificial intelligence in CXR diagnosis usually use pattern recognition to\nclassify the scans. However, such methods rely on labeled databases, which are\ncostly and usually have large error rates. In this work, we built a database\ncontaining more than 12,000 CXR scans and radiological reports, and developed a\nmodel based on deep convolutional neural network and recurrent network with\nattention mechanism. The model learns features from the CXR scans and the\nassociated raw radiological reports directly; no additional labeling of the\nscans are needed. The model provides automated recognition of given scans and\ngeneration of reports. The quality of the generated reports was evaluated with\nboth the CIDEr scores and by radiologists as well. The CIDEr scores are found\nto be around 5.8 on average for the testing dataset. Further blind evaluation\nsuggested a comparable performance against human radiologist.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 08:12:54 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Zhang", "Shuai", ""], ["Xin", "Xiaoyan", ""], ["Wang", "Yang", ""], ["Guo", "Yachong", ""], ["Hao", "Qiuqiao", ""], ["Yang", "Xianfeng", ""], ["Wang", "Jun", ""], ["Zhang", "Jian", ""], ["Zhang", "Bing", ""], ["Wang", "Wei", ""]]}, {"id": "2006.10355", "submitter": "Xiangning Chen", "authors": "Xiangning Chen, Ruochen Wang, Minhao Cheng, Xiaocheng Tang, Cho-Jui\n  Hsieh", "title": "DrNAS: Dirichlet Neural Architecture Search", "comments": "ICLR 2021, code is available at\n  https://github.com/xiangning-chen/DrNAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel differentiable architecture search method by\nformulating it into a distribution learning problem. We treat the continuously\nrelaxed architecture mixing weight as random variables, modeled by Dirichlet\ndistribution. With recently developed pathwise derivatives, the Dirichlet\nparameters can be easily optimized with gradient-based optimizer in an\nend-to-end manner. This formulation improves the generalization ability and\ninduces stochasticity that naturally encourages exploration in the search\nspace. Furthermore, to alleviate the large memory consumption of differentiable\nNAS, we propose a simple yet effective progressive learning scheme that enables\nsearching directly on large-scale tasks, eliminating the gap between search and\nevaluation phases. Extensive experiments demonstrate the effectiveness of our\nmethod. Specifically, we obtain a test error of 2.46% for CIFAR-10, 23.7% for\nImageNet under the mobile setting. On NAS-Bench-201, we also achieve\nstate-of-the-art results on all three datasets and provide insights for the\neffective design of neural architecture search algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 08:23:02 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 20:47:17 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 08:29:59 GMT"}, {"version": "v4", "created": "Tue, 16 Mar 2021 02:32:55 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Chen", "Xiangning", ""], ["Wang", "Ruochen", ""], ["Cheng", "Minhao", ""], ["Tang", "Xiaocheng", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2006.10370", "submitter": "Lukas Hahn", "authors": "Lukas Hahn and Lutz Roese-Koerner and Peet Cremer and Urs Zimmermann\n  and Ori Maoz and Anton Kummert", "title": "On the Robustness of Active Learning", "comments": "11 pages, 6 figures, 1 table; as published in the proceedings of the\n  5th Global Conference on Artificial Intelligence (GCAI), EPiC Series in\n  Computing, Volume 65, pages 152-162, https://doi.org/10.29007/thws, 2019", "journal-ref": "Proceedings of the 5th Global Conference on Artificial\n  Intelligence (GCAI), EPiC Series in Computing, Volume 65, pages 152-162, 2019", "doi": "10.29007/thws", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Learning is concerned with the question of how to identify the most\nuseful samples for a Machine Learning algorithm to be trained with. When\napplied correctly, it can be a very powerful tool to counteract the immense\ndata requirements of Artificial Neural Networks. However, we find that it is\noften applied with not enough care and domain knowledge. As a consequence,\nunrealistic hopes are raised and transfer of the experimental results from one\ndataset to another becomes unnecessarily hard.\n  In this work we analyse the robustness of different Active Learning methods\nwith respect to classifier capacity, exchangeability and type, as well as\nhyperparameters and falsely labelled data. Experiments reveal possible biases\ntowards the architecture used for sample selection, resulting in suboptimal\nperformance for other classifiers. We further propose the new \"Sum of Squared\nLogits\" method based on the Simpson diversity index and investigate the effect\nof using the confusion matrix for balancing in sample selection.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 09:07:23 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Hahn", "Lukas", ""], ["Roese-Koerner", "Lutz", ""], ["Cremer", "Peet", ""], ["Zimmermann", "Urs", ""], ["Maoz", "Ori", ""], ["Kummert", "Anton", ""]]}, {"id": "2006.10380", "submitter": "Jiafan Zhuang", "authors": "Jiafan Zhuang, Zilei Wang, Bingke Wang", "title": "Video Semantic Segmentation with Distortion-Aware Feature Correction", "comments": "12 pages, 16 figures. The paper has been accepted by IEEE\n  Transactions on Circuits and Systems for Video Technology(TCSVT) 2020", "journal-ref": null, "doi": "10.1109/TCSVT.2020.3037234", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video semantic segmentation is active in recent years benefited from the\ngreat progress of image semantic segmentation. For such a task, the per-frame\nimage segmentation is generally unacceptable in practice due to high\ncomputation cost. To tackle this issue, many works use the flow-based feature\npropagation to reuse the features of previous frames. However, the optical flow\nestimation inevitably suffers inaccuracy and then causes the propagated\nfeatures distorted. In this paper, we propose distortion-aware feature\ncorrection to alleviate the issue, which improves video segmentation\nperformance by correcting distorted propagated features. To be specific, we\nfirstly propose to transfer distortion patterns from feature into image space\nand conduct effective distortion map prediction. Benefited from the guidance of\ndistortion maps, we proposed Feature Correction Module (FCM) to rectify\npropagated features in the distorted areas. Our proposed method can\nsignificantly boost the accuracy of video semantic segmentation at a low price.\nThe extensive experimental results on Cityscapes and CamVid show that our\nmethod outperforms the recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 09:30:00 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 09:12:50 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Zhuang", "Jiafan", ""], ["Wang", "Zilei", ""], ["Wang", "Bingke", ""]]}, {"id": "2006.10383", "submitter": "Hajime Taira", "authors": "Sho kagami, Hajime Taira, Naoyuki Miyashita, Akihiko Torii, Masatoshi\n  Okutomi", "title": "3D Pipe Network Reconstruction Based on Structure from Motion with\n  Incremental Conic Shape Detection and Cylindrical Constraint", "comments": "This manuscript was accepted and presented in the 29th IEEE\n  International Symposium on Industrial Electronics (ISIE2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pipe inspection is a critical task for many industries and infrastructure of\na city. The 3D information of a pipe can be used for revealing the deformation\nof the pipe surface and position of the camera during the inspection. In this\npaper, we propose a 3D pipe reconstruction system using sequential images\ncaptured by a monocular endoscopic camera. Our work extends a state-of-the-art\nincremental Structure-from-Motion (SfM) method to incorporate prior constraints\ngiven by the target shape into bundle adjustment (BA). Using this constraint,\nwe can minimize the scale-drift that is the general problem in SfM. Moreover,\nour method can reconstruct a pipe network composed of multiple parts including\nstraight pipes, elbows, and tees. In the experiments, we show that the proposed\nsystem enables more accurate and robust pipe mapping from a monocular camera in\ncomparison with existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 09:37:00 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 04:52:21 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["kagami", "Sho", ""], ["Taira", "Hajime", ""], ["Miyashita", "Naoyuki", ""], ["Torii", "Akihiko", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "2006.10386", "submitter": "Daniele Di Mauro", "authors": "Daniele Di Mauro, Antonino Furnari, Giuseppe Patan\\`e, Sebastiano\n  Battiato, Giovanni Maria Farinella", "title": "SceneAdapt: Scene-based domain adaptation for semantic segmentation\n  using adversarial learning", "comments": null, "journal-ref": "Pattern Recognition Letters, Volume 136, August 2020, Pages\n  175-182", "doi": "10.1016/j.patrec.2020.06.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation methods have achieved outstanding performance thanks to\ndeep learning. Nevertheless, when such algorithms are deployed to new contexts\nnot seen during training, it is necessary to collect and label scene-specific\ndata in order to adapt them to the new domain using fine-tuning. This process\nis required whenever an already installed camera is moved or a new camera is\nintroduced in a camera network due to the different scene layouts induced by\nthe different viewpoints. To limit the amount of additional training data to be\ncollected, it would be ideal to train a semantic segmentation method using\nlabeled data already available and only unlabeled data coming from the new\ncamera. We formalize this problem as a domain adaptation task and introduce a\nnovel dataset of urban scenes with the related semantic labels. As a first\napproach to address this challenging task, we propose SceneAdapt, a method for\nscene adaptation of semantic segmentation algorithms based on adversarial\nlearning. Experiments and comparisons with state-of-the-art approaches to\ndomain adaptation highlight that promising performance can be achieved using\nadversarial learning both when the two scenes have different but points of\nview, and when they comprise images of completely different scenes. To\nencourage research on this topic, we made our code available at our web page:\nhttps://iplab.dmi.unict.it/ParkSmartSceneAdaptation/.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 09:43:31 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Di Mauro", "Daniele", ""], ["Furnari", "Antonino", ""], ["Patan\u00e8", "Giuseppe", ""], ["Battiato", "Sebastiano", ""], ["Farinella", "Giovanni Maria", ""]]}, {"id": "2006.10406", "submitter": "Thomas Schultz", "authors": "Ikram Jumakulyyev and Thomas Schultz", "title": "Fourth-Order Anisotropic Diffusion for Inpainting and Image Compression", "comments": "Accepted for publication in Springer book \"Anisotropy Across Fields\n  and Scales\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Edge-enhancing diffusion (EED) can reconstruct a close approximation of an\noriginal image from a small subset of its pixels. This makes it an attractive\nfoundation for PDE based image compression. In this work, we generalize\nsecond-order EED to a fourth-order counterpart. It involves a fourth-order\ndiffusion tensor that is constructed from the regularized image gradient in a\nsimilar way as in traditional second-order EED, permitting diffusion along\nedges, while applying a non-linear diffusivity function across them. We show\nthat our fourth-order diffusion tensor formalism provides a unifying framework\nfor all previous anisotropic fourth-order diffusion based methods, and that it\nprovides additional flexibility. We achieve an efficient implementation using a\nfast semi-iterative scheme. Experimental results on natural and medical images\nsuggest that our novel fourth-order method produces more accurate\nreconstructions compared to the existing second-order EED.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 10:13:31 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Jumakulyyev", "Ikram", ""], ["Schultz", "Thomas", ""]]}, {"id": "2006.10408", "submitter": "Yu Li", "authors": "Yu Li, Tao Wang, Bingyi Kang, Sheng Tang, Chunfeng Wang, Jintao Li,\n  Jiashi Feng", "title": "Overcoming Classifier Imbalance for Long-tail Object Detection with\n  Balanced Group Softmax", "comments": "CVPR 2020 (Oral). Code is available at\n  https://github.com/FishYuLi/BalancedGroupSoftmax", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving long-tail large vocabulary object detection with deep learning based\nmodels is a challenging and demanding task, which is however under-explored.In\nthis work, we provide the first systematic analysis on the underperformance of\nstate-of-the-art models in front of long-tail distribution. We find existing\ndetection methods are unable to model few-shot classes when the dataset is\nextremely skewed, which can result in classifier imbalance in terms of\nparameter magnitude. Directly adapting long-tail classification models to\ndetection frameworks can not solve this problem due to the intrinsic difference\nbetween detection and classification.In this work, we propose a novel balanced\ngroup softmax (BAGS) module for balancing the classifiers within the detection\nframeworks through group-wise training. It implicitly modulates the training\nprocess for the head and tail classes and ensures they are both sufficiently\ntrained, without requiring any extra sampling for the instances from the tail\nclasses.Extensive experiments on the very recent long-tail large vocabulary\nobject recognition benchmark LVIS show that our proposed BAGS significantly\nimproves the performance of detectors with various backbones and frameworks on\nboth object detection and instance segmentation. It beats all state-of-the-art\nmethods transferred from long-tail image classification and establishes new\nstate-of-the-art.Code is available at\nhttps://github.com/FishYuLi/BalancedGroupSoftmax.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 10:24:26 GMT"}], "update_date": "2020-06-20", "authors_parsed": [["Li", "Yu", ""], ["Wang", "Tao", ""], ["Kang", "Bingyi", ""], ["Tang", "Sheng", ""], ["Wang", "Chunfeng", ""], ["Li", "Jintao", ""], ["Feng", "Jiashi", ""]]}, {"id": "2006.10451", "submitter": "Danil Galeev", "authors": "Danil Galeev, Konstantin Sofiiuk, Danila Rukhovich, Mikhail Romanov,\n  Olga Barinova, Anton Konushin", "title": "Learning High-Resolution Domain-Specific Representations with a GAN\n  Generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years generative models of visual data have made a great progress,\nand now they are able to produce images of high quality and diversity. In this\nwork we study representations learnt by a GAN generator. First, we show that\nthese representations can be easily projected onto semantic segmentation map\nusing a lightweight decoder. We find that such semantic projection can be\nlearnt from just a few annotated images. Based on this finding, we propose\nLayerMatch scheme for approximating the representation of a GAN generator that\ncan be used for unsupervised domain-specific pretraining. We consider the\nsemi-supervised learning scenario when a small amount of labeled data is\navailable along with a large unlabeled dataset from the same domain. We find\nthat the use of LayerMatch-pretrained backbone leads to superior accuracy\ncompared to standard supervised pretraining on ImageNet. Moreover, this simple\napproach also outperforms recent semi-supervised semantic segmentation methods\nthat use both labeled and unlabeled data during training. Source code for\nreproducing our experiments will be available at the time of publication.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 11:57:18 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Galeev", "Danil", ""], ["Sofiiuk", "Konstantin", ""], ["Rukhovich", "Danila", ""], ["Romanov", "Mikhail", ""], ["Barinova", "Olga", ""], ["Konushin", "Anton", ""]]}, {"id": "2006.10457", "submitter": "Kun Liu", "authors": "Kun Liu, Huadong Ma, and Chuang Gan", "title": "Language Guided Networks for Cross-modal Moment Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the challenging task of cross-modal moment retrieval, which aims\nto localize a temporal segment from an untrimmed video described by a natural\nlanguage query. It poses great challenges over the proper semantic alignment\nbetween vision and linguistic domains. Existing methods independently extract\nthe features of videos and sentences and purely utilize the sentence embedding\nin the multi-modal fusion stage, which do not make full use of the potential of\nlanguage. In this paper, we present Language Guided Networks (LGN), a new\nframework that leverages the sentence embedding to guide the whole process of\nmoment retrieval. In the first feature extraction stage, we propose to jointly\nlearn visual and language features to capture the powerful visual information\nwhich can cover the complex semantics in the sentence query. Specifically, the\nearly modulation unit is designed to modulate the visual feature extractor's\nfeature maps by a linguistic embedding. Then we adopt a multi-modal fusion\nmodule in the second fusion stage. Finally, to get a precise localizer, the\nsentence information is utilized to guide the process of predicting temporal\npositions. Specifically, the late guidance module is developed to linearly\ntransform the output of localization networks via the channel attention\nmechanism. The experimental results on two popular datasets demonstrate the\nsuperior performance of our proposed method on moment retrieval (improving by\n5.8\\% in terms of Rank1@IoU0.5 on Charades-STA and 5.2\\% on TACoS). The source\ncode for the complete system will be publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 12:08:40 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 05:19:24 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Liu", "Kun", ""], ["Ma", "Huadong", ""], ["Gan", "Chuang", ""]]}, {"id": "2006.10461", "submitter": "Konstantin Klemmer", "authors": "Konstantin Klemmer, Daniel B. Neill", "title": "SXL: Spatially explicit learning of geographic processes with auxiliary\n  tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From earth system sciences to climate modeling and ecology, many of the\ngreatest empirical modeling challenges are geographic in nature. As these\nprocesses are characterized by spatial dynamics, we can exploit their\nautoregressive nature to inform learning algorithms. We introduce SXL, a method\nfor learning with geospatial data using explicitly spatial auxiliary tasks. We\nembed the local Moran's I, a well-established measure of local spatial\nautocorrelation, into the training process, \"nudging\" the model to learn the\ndirection and magnitude of local autoregressive effects in parallel with the\nprimary task. Further, we propose an expansion of Moran's I to multiple\nresolutions to capture effects at different spatial granularities and over\nvarying distance scales. We show the superiority of this method for training\nneural networks using experiments with real-world geospatial data in both\ngenerative and predictive modeling tasks. The Moran's I embedding can be\nconstructed easily for any spatial, numerical input and our approach can be\nused with arbitrary network architectures, consistently improving their\nperformance as shown by our experiments. We also outperform appropriate,\ndomain-specific spatial interpolation benchmarks. Our work highlights how\nintegrating the geographic information sciences and spatial statistics into\nneural network models can address the specific challenges of spatial data. The\ncode for our experiments is available on Github:\nhttps://github.com/konstantinklemmer/sxl.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 12:16:08 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 10:29:32 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Klemmer", "Konstantin", ""], ["Neill", "Daniel B.", ""]]}, {"id": "2006.10499", "submitter": "Mohammad Rami Koujan", "authors": "Mohammad Rami Koujan, Nikolai Dochev, Anastasios Roussos", "title": "Real-Time Monocular 4D Face Reconstruction using the LSFM models", "comments": "Published in Proceedings of the 15th ACM SIGGRAPH European Conference\n  on Visual Media Production", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  4D face reconstruction from a single camera is a challenging task, especially\nwhen it is required to be performed in real time. We demonstrate a system of\nour own implementation that solves this task accurately and runs in real time\non a commodity laptop, using a webcam as the only input. Our system is\ninteractive, allowing the user to freely move their head and show various\nexpressions while standing in front of the camera. As a result, the put forward\nsystem both reconstructs and visualises the identity of the subject in the\ncorrect pose along with the acted facial expressions in real-time. The 4D\nreconstruction in our framework is based on the recently-released Large-Scale\nFacial Models (LSFM) \\cite{LSFM1, LSFM2}, which are the largest-scale 3D\nMorphable Models of facial shapes ever constructed, based on a dataset of more\nthan 10,000 facial identities from a wide range of gender, age and ethnicity\ncombinations. This is the first real-time demo that gives users the opportunity\nto test in practice the capabilities of the recently-released Large-Scale\nFacial Models (LSFM)\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 02:14:45 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Koujan", "Mohammad Rami", ""], ["Dochev", "Nikolai", ""], ["Roussos", "Anastasios", ""]]}, {"id": "2006.10500", "submitter": "Mohammad Rami Koujan", "authors": "Mohammad Rami Koujan, Michail Christos Doukas, Anastasios Roussos,\n  Stefanos Zafeiriou", "title": "ReenactNet: Real-time Full Head Reenactment", "comments": "to be published in 15th IEEE International Conference on Automatic\n  Face and Gesture Recognition (FG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video-to-video synthesis is a challenging problem aiming at learning a\ntranslation function between a sequence of semantic maps and a photo-realistic\nvideo depicting the characteristics of a driving video. We propose a\nhead-to-head system of our own implementation capable of fully transferring the\nhuman head 3D pose, facial expressions and eye gaze from a source to a target\nactor, while preserving the identity of the target actor. Our system produces\nhigh-fidelity, temporally-smooth and photo-realistic synthetic videos\nfaithfully transferring the human time-varying head attributes from the source\nto the target actor. Our proposed implementation: 1) works in real time ($\\sim\n20$ fps), 2) runs on a commodity laptop with a webcam as the only input, 3) is\ninteractive, allowing the participant to drive a target person, e.g. a\ncelebrity, politician, etc, instantly by varying their expressions, head pose,\nand eye gaze, and visualising the synthesised video concurrently.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 00:51:38 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Koujan", "Mohammad Rami", ""], ["Doukas", "Michail Christos", ""], ["Roussos", "Anastasios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2006.10502", "submitter": "Artem Yashenko Mr.", "authors": "A.V. Yashchenko, A.V. Belikov, M.V. Peterson, A.S. Potapov", "title": "Distillation of neural network models for detection and description of\n  key points of images", "comments": "in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image matching and classification methods, as well as synchronous location\nand mapping, are widely used on embedded and mobile devices. Their most\nresource-intensive part is the detection and description of the key points of\nthe images. And if the classical methods of detecting and describing key points\ncan be executed in real time on mobile devices, then for modern neural network\nmethods with the best quality, such use is difficult. Thus, it is important to\nincrease the speed of neural network models for the detection and description\nof key points. The subject of research is distillation as one of the methods\nfor reducing neural network models. The aim of thestudy is to obtain a more\ncompact model of detection and description of key points, as well as a\ndescription of the procedure for obtaining this model. A method for the\ndistillation of neural networks for the task of detecting and describing key\npoints was tested. The objective function and training parameters that provide\nthe best results in the framework of the study are proposed. A new data set has\nbeen introduced for testing key point detection methods and a new quality\nindicator of the allocated key points and their corresponding local features.\nAs a result of training in the described way, the new model, with the same\nnumber of parameters, showed greater accuracy in comparing key points than the\noriginal model. A new model with a significantly smaller number of parameters\nshows the accuracy of point matching close to the accuracy of the original\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 18:59:35 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Yashchenko", "A. V.", ""], ["Belikov", "A. V.", ""], ["Peterson", "M. V.", ""], ["Potapov", "A. S.", ""]]}, {"id": "2006.10509", "submitter": "Peter Christopher", "authors": "Peter J. Christopher and Timothy D. Wilkinson", "title": "Structure and Design of HoloGen", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing popularity of augmented and mixed reality systems has seen a\nsimilar increase of interest in 2D and 3D computer generated holography (CGH).\nUnlike stereoscopic approaches, CGH can fully represent a light field including\ndepth of focus, accommodation and vergence. Along with existing\ntelecommunications, imaging, projection, lithography, beam shaping and optical\ntweezing applications, CGH is an exciting technique applicable to a wide array\nof photonic problems including full 3D representation. Traditionally, the\nprimary roadblock to acceptance has been the significant numerical processing\nrequired to generate holograms requiring both significant expertise and\nsignificant computational power. This article discusses the structure and\ndesign of HoloGen. HoloGen is an MIT licensed application that may be used to\ngenerate holograms using a wide array of algorithms without expert guidance.\nHoloGen uses a Cuda C and C++ backend with a C# and Windows Presentation\nFramework graphical user interface. The article begins by introducing HoloGen\nbefore providing an in-depth discussion of its design and structure. Particular\nfocus is given to the communication, data transfer and algorithmic aspects.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 13:29:46 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Christopher", "Peter J.", ""], ["Wilkinson", "Timothy D.", ""]]}, {"id": "2006.10511", "submitter": "Krishna Chaitanya", "authors": "Krishna Chaitanya, Ertunc Erdil, Neerav Karani, Ender Konukoglu", "title": "Contrastive learning of global and local features for medical image\n  segmentation with limited annotations", "comments": "18 pages, 2 figures, 10 tables. This article has been accepted as\n  Oral Presentation at NeurIPS 2020 (34th Conference on Neural Information\n  Processing Systems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key requirement for the success of supervised deep learning is a large\nlabeled dataset - a condition that is difficult to meet in medical image\nanalysis. Self-supervised learning (SSL) can help in this regard by providing a\nstrategy to pre-train a neural network with unlabeled data, followed by\nfine-tuning for a downstream task with limited annotations. Contrastive\nlearning, a particular variant of SSL, is a powerful technique for learning\nimage-level representations. In this work, we propose strategies for extending\nthe contrastive learning framework for segmentation of volumetric medical\nimages in the semi-supervised setting with limited annotations, by leveraging\ndomain-specific and problem-specific cues. Specifically, we propose (1) novel\ncontrasting strategies that leverage structural similarity across volumetric\nmedical images (domain-specific cue) and (2) a local version of the contrastive\nloss to learn distinctive representations of local regions that are useful for\nper-pixel segmentation (problem-specific cue). We carry out an extensive\nevaluation on three Magnetic Resonance Imaging (MRI) datasets. In the limited\nannotation setting, the proposed method yields substantial improvements\ncompared to other self-supervision and semi-supervised learning techniques.\nWhen combined with a simple data augmentation technique, the proposed method\nreaches within 8% of benchmark performance using only two labeled MRI volumes\nfor training, corresponding to only 4% (for ACDC) of the training data used to\ntrain the benchmark. The code is made public at\nhttps://github.com/krishnabits001/domain_specific_cl.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 13:31:26 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 14:45:25 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Chaitanya", "Krishna", ""], ["Erdil", "Ertunc", ""], ["Karani", "Neerav", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2006.10520", "submitter": "Xiangzhu Meng", "authors": "Xiangzhu Meng, Lin Feng, Huibing Wang", "title": "Multi-view Low-rank Preserving Embedding: A Novel Method for Multi-view\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, we have witnessed a surge of interest in multi-view\nrepresentation learning, which is concerned with the problem of learning\nrepresentations of multi-view data. When facing multiple views that are highly\nrelated but sightly different from each other, most of existing multi-view\nmethods might fail to fully integrate multi-view information. Besides,\ncorrelations between features from multiple views always vary seriously, which\nmakes multi-view representation challenging. Therefore, how to learn\nappropriate embedding from multi-view information is still an open problem but\nchallenging. To handle this issue, this paper proposes a novel multi-view\nlearning method, named Multi-view Low-rank Preserving Embedding (MvLPE). It\nintegrates different views into one centroid view by minimizing the\ndisagreement term, based on distance or similarity matrix among instances,\nbetween the centroid view and each view meanwhile maintaining low-rank\nreconstruction relations among samples for each view, which could make more\nfull use of compatible and complementary information from multi-view features.\nUnlike existing methods with additive parameters, the proposed method could\nautomatically allocate a suitable weight for each view in multi-view\ninformation fusion. However, MvLPE couldn't be directly solved, which makes the\nproposed MvLPE difficult to obtain an analytic solution. To this end, we\napproximate this solution based on stationary hypothesis and normalization\npost-processing to efficiently obtain the optimal solution. Furthermore, an\niterative alternating strategy is provided to solve this multi-view\nrepresentation problem. The experiments on six benchmark datasets demonstrate\nthat the proposed method outperforms its counterparts while achieving very\ncompetitive performance.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 12:47:25 GMT"}], "update_date": "2020-06-20", "authors_parsed": [["Meng", "Xiangzhu", ""], ["Feng", "Lin", ""], ["Wang", "Huibing", ""]]}, {"id": "2006.10547", "submitter": "Aayush Kumar", "authors": "Aayush Kumar, Sanat B Singh, Suresh Chandra Satapathy, Minakhi Rout", "title": "MOSQUITO-NET: A deep learning based CADx system for malaria diagnosis\n  along with model interpretation using GradCam and class activation maps", "comments": "arXiv admin note: text overlap with arXiv:2003.09871 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Malaria is considered one of the deadliest diseases in today world which\ncauses thousands of deaths per year. The parasites responsible for malaria are\nscientifically known as Plasmodium which infects the red blood cells in human\nbeings. The parasites are transmitted by a female class of mosquitos known as\nAnopheles. The diagnosis of malaria requires identification and manual counting\nof parasitized cells by medical practitioners in microscopic blood smears. Due\nto the unavailability of resources, its diagnostic accuracy is largely affected\nby large scale screening. State of the art Computer-aided diagnostic techniques\nbased on deep learning algorithms such as CNNs, with end to end feature\nextraction and classification, have widely contributed to various image\nrecognition tasks. In this paper, we evaluate the performance of custom made\nconvnet Mosquito-Net, to classify the infected and uninfected cells for malaria\ndiagnosis which could be deployed on the edge and mobile devices owing to its\nfewer parameters and less computation power. Therefore, it can be wildly\npreferred for diagnosis in remote and countryside areas where there is a lack\nof medical facilities.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 13:00:30 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 05:57:59 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Kumar", "Aayush", ""], ["Singh", "Sanat B", ""], ["Satapathy", "Suresh Chandra", ""], ["Rout", "Minakhi", ""]]}, {"id": "2006.10550", "submitter": "Jingyu Liu", "authors": "Jingyu Liu, Jie Lian, Yizhou Yu", "title": "ChestX-Det10: Chest X-ray Dataset on Detection of Thoracic Abnormalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance level detection of thoracic diseases or abnormalities are crucial\nfor automatic diagnosis in chest X-ray images. Most existing works on chest\nX-rays focus on disease classification and weakly supervised localization. In\norder to push forward the research on disease classification and localization\non chest X-rays. We provide a new benchmark called ChestX-Det10, including\nbox-level annotations of 10 categories of disease/abnormality of $\\sim$ 3,500\nimages. The annotations are located at\nhttps://github.com/Deepwise-AILab/ChestX-Det10-Dataset.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 10:15:50 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 02:48:21 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 06:25:25 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Liu", "Jingyu", ""], ["Lian", "Jie", ""], ["Yu", "Yizhou", ""]]}, {"id": "2006.10552", "submitter": "Xingyi Yang", "authors": "Xingyi Yang, Nandiraju Gireesh, Eric Xing, Pengtao Xie", "title": "XRayGAN: Consistency-preserving Generation of X-ray Images from\n  Radiology Reports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To effectively train medical students to become qualified radiologists, a\nlarge number of X-ray images collected from patients with diverse medical\nconditions are needed. However, due to data privacy concerns, such images are\ntypically difficult to obtain. To address this problem, we develop methods to\ngenerate view-consistent, high-fidelity, and high-resolution X-ray images from\nradiology reports to facilitate radiology training of medical students. This\ntask is presented with several challenges. First, from a single report, images\nwith different views (e.g., frontal, lateral) need to be generated. How to\nensure consistency of these images (i.e., make sure they are about the same\npatient)? Second, X-ray images are required to have high resolution. Otherwise,\nmany details of diseases would be lost. How to generate high-resolutions\nimages? Third, radiology reports are long and have complicated structure. How\nto effectively understand their semantics to generate high-fidelity images that\naccurately reflect the contents of the reports? To address these three\nchallenges, we propose an XRayGAN composed of three modules: (1) a view\nconsistency network that maximizes the consistency between generated\nfrontal-view and lateral-view images; (2) a multi-scale conditional GAN that\nprogressively generates a cascade of images with increasing resolution; (3) a\nhierarchical attentional encoder that learns the latent semantics of a\nradiology report by capturing its hierarchical linguistic structure and various\nlevels of clinical importance of words and sentences. Experiments on two\nradiology datasets demonstrate the effectiveness of our methods. To our best\nknowledge, this work represents the first one generating consistent and\nhigh-resolution X-ray images from radiology reports. The code is available at\nhttps://github.com/UCSD-AI4H/XRayGAN.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 05:32:14 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Yang", "Xingyi", ""], ["Gireesh", "Nandiraju", ""], ["Xing", "Eric", ""], ["Xie", "Pengtao", ""]]}, {"id": "2006.10560", "submitter": "Sunitha Basodi", "authors": "Sunitha Basodi, Chunyan Ji, Haiping Zhang, and Yi Pan", "title": "Gradient Amplification: An efficient way to train deep neural networks", "comments": "9 page document with 7 figures and one results table", "journal-ref": "Big Data Mining and Analytics 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving performance of deep learning models and reducing their training\ntimes are ongoing challenges in deep neural networks. There are several\napproaches proposed to address these challenges one of which is to increase the\ndepth of the neural networks. Such deeper networks not only increase training\ntimes, but also suffer from vanishing gradients problem while training. In this\nwork, we propose gradient amplification approach for training deep learning\nmodels to prevent vanishing gradients and also develop a training strategy to\nenable or disable gradient amplification method across several epochs with\ndifferent learning rates. We perform experiments on VGG-19 and resnet\n(Resnet-18 and Resnet-34) models, and study the impact of amplification\nparameters on these models in detail. Our proposed approach improves\nperformance of these deep learning models even at higher learning rates,\nthereby allowing these models to achieve higher performance with reduced\ntraining time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 20:30:55 GMT"}], "update_date": "2020-06-20", "authors_parsed": [["Basodi", "Sunitha", ""], ["Ji", "Chunyan", ""], ["Zhang", "Haiping", ""], ["Pan", "Yi", ""]]}, {"id": "2006.10569", "submitter": "Xuelin Chen", "authors": "Xuelin Chen, Daniel Cohen-Or, Baoquan Chen and Niloy J. Mitra", "title": "Towards a Neural Graphics Pipeline for Controllable Image Generation", "comments": "Eurographics 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we leverage advances in neural networks towards forming a\nneural rendering for controllable image generation, and thereby bypassing the\nneed for detailed modeling in conventional graphics pipeline. To this end, we\npresent Neural Graphics Pipeline (NGP), a hybrid generative model that brings\ntogether neural and traditional image formation models. NGP decomposes the\nimage into a set of interpretable appearance feature maps, uncovering direct\ncontrol handles for controllable image generation. To form an image, NGP\ngenerates coarse 3D models that are fed into neural rendering modules to\nproduce view-specific interpretable 2D maps, which are then composited into the\nfinal output image using a traditional image formation model. Our approach\noffers control over image generation by providing direct handles controlling\nillumination and camera parameters, in addition to control over shape and\nappearance variations. The key challenge is to learn these controls through\nunsupervised training that links generated coarse 3D models with unpaired real\nimages via neural and traditional (e.g., Blinn- Phong) rendering functions,\nwithout establishing an explicit correspondence between them. We demonstrate\nthe effectiveness of our approach on controllable image generation of\nsingle-object scenes. We evaluate our hybrid modeling framework, compare with\nneural-only generation methods (namely, DCGAN, LSGAN, WGAN-GP, VON, and SRNs),\nreport improvement in FID scores against real images, and demonstrate that NGP\nsupports direct controls common in traditional forward rendering. Code is\navailable at http://geometry.cs.ucl.ac.uk/projects/2021/ngp.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 14:22:54 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 09:18:55 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Chen", "Xuelin", ""], ["Cohen-Or", "Daniel", ""], ["Chen", "Baoquan", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "2006.10575", "submitter": "Mohammad Hamed Mozaffari", "authors": "M. Hamed Mozaffari and Li-Lin Tay", "title": "A Review of 1D Convolutional Neural Networks toward Unknown Substance\n  Identification in Portable Raman Spectrometer", "comments": "19 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raman spectroscopy is a powerful analytical tool with applications ranging\nfrom quality control to cutting edge biomedical research. One particular area\nwhich has seen tremendous advances in the past decade is the development of\npowerful handheld Raman spectrometers. They have been adopted widely by first\nresponders and law enforcement agencies for the field analysis of unknown\nsubstances. Field detection and identification of unknown substances with Raman\nspectroscopy rely heavily on the spectral matching capability of the devices on\nhand. Conventional spectral matching algorithms (such as correlation, dot\nproduct, etc.) have been used in identifying unknown Raman spectrum by\ncomparing the unknown to a large reference database. This is typically achieved\nthrough brute-force summation of pixel-by-pixel differences between the\nreference and the unknown spectrum. Conventional algorithms have noticeable\ndrawbacks. For example, they tend to work well with identifying pure compounds\nbut less so for mixture compounds. For instance, limited reference spectra\ninaccessible databases with a large number of classes relative to the number of\nsamples have been a setback for the widespread usage of Raman spectroscopy for\nfield analysis applications. State-of-the-art deep learning methods\n(specifically convolutional neural networks CNNs), as an alternative approach,\npresents a number of advantages over conventional spectral comparison algorism.\nWith optimization, they are ideal to be deployed in handheld spectrometers for\nfield detection of unknown substances. In this study, we present a\ncomprehensive survey in the use of one-dimensional CNNs for Raman spectrum\nidentification. Specifically, we highlight the use of this powerful deep\nlearning technique for handheld Raman spectrometers taking into consideration\nthe potential limit in power consumption and computation ability of handheld\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 14:28:00 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Mozaffari", "M. Hamed", ""], ["Tay", "Li-Lin", ""]]}, {"id": "2006.10598", "submitter": "Bryan Plummer", "authors": "Bryan A. Plummer, Nikoli Dryden, Julius Frost, Torsten Hoefler, Kate\n  Saenko", "title": "Neural Parameter Allocation Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting a model into GPU memory during training is an increasing concern as\nmodels continue to grow. Parameter sharing can reduce memory requirements, but\nexisting methods only share parameters between identical layers, limiting their\nimpact. This paper removes these restrictions with a novel task called Neural\nParameter Allocation Search (NPAS), where the goal is to generate weights for a\nnetwork using a given parameter budget. NPAS requires new techniques to morph\navailable parameters to fit any architecture. To address this new task we\nintroduce Shapeshifter Networks (SSNs), which automatically learns where and\nhow to share parameters between all layers in a network, even between layers of\nvarying sizes and operations. SSNs do not require any loss function or\narchitecture modifications, making them easy to use. We evaluate SSNs in key\nNPAS settings using seven network architectures across diverse tasks including\nimage classification, bidirectional image-sentence retrieval, and phrase\ngrounding, creating high performing models even when using as little as 1% of\nthe parameters.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 15:01:00 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 18:43:30 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 14:08:20 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Plummer", "Bryan A.", ""], ["Dryden", "Nikoli", ""], ["Frost", "Julius", ""], ["Hoefler", "Torsten", ""], ["Saenko", "Kate", ""]]}, {"id": "2006.10621", "submitter": "Jonathan Rosenfeld", "authors": "Jonathan S. Rosenfeld, Jonathan Frankle, Michael Carbin, Nir Shavit", "title": "On the Predictability of Pruning Across Scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the error of iteratively magnitude-pruned networks empirically\nfollows a scaling law with interpretable coefficients that depend on the\narchitecture and task. We functionally approximate the error of the pruned\nnetworks, showing it is predictable in terms of an invariant tying width,\ndepth, and pruning level, such that networks of vastly different pruned\ndensities are interchangeable. We demonstrate the accuracy of this\napproximation over orders of magnitude in depth, width, dataset size, and\ndensity. We show that the functional form holds (generalizes) for large scale\ndata (e.g., ImageNet) and architectures (e.g., ResNets). As neural networks\nbecome ever larger and costlier to train, our findings suggest a framework for\nreasoning conceptually and analytically about a standard method for\nunstructured pruning.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 15:41:46 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 14:01:40 GMT"}, {"version": "v3", "created": "Sun, 4 Jul 2021 02:51:24 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Rosenfeld", "Jonathan S.", ""], ["Frankle", "Jonathan", ""], ["Carbin", "Michael", ""], ["Shavit", "Nir", ""]]}, {"id": "2006.10623", "submitter": "Vasileios Syrris", "authors": "Vasileios Syrris, Ondrej Pesek, Pierre Soille", "title": "SatImNet: Structured and Harmonised Training Data for Enhanced Satellite\n  Imagery Classification", "comments": "22 pages, 10 figures", "journal-ref": "Remote Sensing 12(20) (2020) 3358", "doi": "10.3390/rs12203358", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic supervised classification with complex modelling such as deep\nneural networks requires the availability of representative training data sets.\nWhile there exists a plethora of data sets that can be used for this purpose,\nthey are usually very heterogeneous and not interoperable. In this context, the\npresent work has a twofold objective: i) to describe procedures of open-source\ntraining data management, integration, and data retrieval, and ii) to\ndemonstrate the practical use of varying source training data for remote\nsensing image classification. For the former, we propose SatImNet, a collection\nof open training data, structured and harmonized according to specific rules.\nFor the latter, two modelling approaches based on convolutional neural networks\nhave been designed and configured to deal with satellite image classification\nand segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 15:46:24 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 22:44:00 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Syrris", "Vasileios", ""], ["Pesek", "Ondrej", ""], ["Soille", "Pierre", ""]]}, {"id": "2006.10626", "submitter": "Ioannis Ivrissimtzis", "authors": "Latifah Abduh and Ioannis Ivrissimtzis", "title": "Use of in-the-wild images for anomaly detection in face anti-spoofing", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional approach to face anti-spoofing sees it as a binary\nclassification problem, and binary classifiers are trained and validated on\nspecialized anti-spoofing databases. One of the drawbacks of this approach is\nthat, due to the variability of face spoofing attacks, environmental factors,\nand the typically small sample size, such classifiers do not generalize well to\npreviously unseen databases. Anomaly detection, which approaches face\nanti-spoofing as a one-class classification problem, is emerging as an\nincreasingly popular alternative approach. Nevertheless, in all existing work\non anomaly detection for face anti-spoofing, the proposed training protocols\nutilize images from specialized anti-spoofing databases only, even though only\ncommon images of real faces are needed. Here, we explore the use of in-the-wild\nimages, and images from non-specialized face databases, to train one-class\nclassifiers for face anti-spoofing. Employing a well-established technique, we\ntrain a convolutional autoencoder on real faces and compare the reconstruction\nerror of the input against a threshold to classify a face image accordingly as\neither client or imposter.\n  Our results show that the inclusion in the training set of in-the-wild images\nincreases the discriminating power of the classifier significantly on an unseen\ndatabase, as evidenced by a large increase in the value of the Area Under the\nCurve. In a limitation of our approach, we note that the problem of finding a\nsuitable operating point on the unseen database remains a challenge, as\nevidenced by the values of the Half Total Error Rate.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 15:49:36 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Abduh", "Latifah", ""], ["Ivrissimtzis", "Ioannis", ""]]}, {"id": "2006.10645", "submitter": "Xiaohang Zhan", "authors": "Xiaohang Zhan, Jiahao Xie, Ziwei Liu, Yew Soon Ong, Chen Change Loy", "title": "Online Deep Clustering for Unsupervised Representation Learning", "comments": "Accepted by CVPR 2020. Code:\n  https://github.com/open-mmlab/OpenSelfSup", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint clustering and feature learning methods have shown remarkable\nperformance in unsupervised representation learning. However, the training\nschedule alternating between feature clustering and network parameters update\nleads to unstable learning of visual representations. To overcome this\nchallenge, we propose Online Deep Clustering (ODC) that performs clustering and\nnetwork update simultaneously rather than alternatingly. Our key insight is\nthat the cluster centroids should evolve steadily in keeping the classifier\nstably updated. Specifically, we design and maintain two dynamic memory\nmodules, i.e., samples memory to store samples labels and features, and\ncentroids memory for centroids evolution. We break down the abrupt global\nclustering into steady memory update and batch-wise label re-assignment. The\nprocess is integrated into network update iterations. In this way, labels and\nthe network evolve shoulder-to-shoulder rather than alternatingly. Extensive\nexperiments demonstrate that ODC stabilizes the training process and boosts the\nperformance effectively. Code: https://github.com/open-mmlab/OpenSelfSup.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 16:15:46 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Zhan", "Xiaohang", ""], ["Xie", "Jiahao", ""], ["Liu", "Ziwei", ""], ["Ong", "Yew Soon", ""], ["Loy", "Chen Change", ""]]}, {"id": "2006.10649", "submitter": "Jialu Huang", "authors": "Jialu Huang, Jing Liao, Zhifeng Tan, Sam Kwong", "title": "Multi-Density Sketch-to-Image Translation Network", "comments": "2020 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch-to-image (S2I) translation plays an important role in image synthesis\nand manipulation tasks, such as photo editing and colorization. Some specific\nS2I translation including sketch-to-photo and sketch-to-painting can be used as\npowerful tools in the art design industry. However, previous methods only\nsupport S2I translation with a single level of density, which gives less\nflexibility to users for controlling the input sketches. In this work, we\npropose the first multi-level density sketch-to-image translation framework,\nwhich allows the input sketch to cover a wide range from rough object outlines\nto micro structures. Moreover, to tackle the problem of noncontinuous\nrepresentation of multi-level density input sketches, we project the density\nlevel into a continuous latent space, which can then be linearly controlled by\na parameter. This allows users to conveniently control the densities of input\nsketches and generation of images. Moreover, our method has been successfully\nverified on various datasets for different applications including face editing,\nmulti-modal sketch-to-photo translation, and anime colorization, providing\ncoarse-to-fine levels of controls to these applications.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 16:21:04 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Huang", "Jialu", ""], ["Liao", "Jing", ""], ["Tan", "Zhifeng", ""], ["Kwong", "Sam", ""]]}, {"id": "2006.10679", "submitter": "Lokender Tiwari", "authors": "Lokender Tiwari, Anish Madan, Saket Anand, Subhashis Banerjee", "title": "Dissecting Deep Networks into an Ensemble of Generative Classifiers for\n  Robust Predictions", "comments": "Demo code available at https://github.com/lokender/REGroup", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are often criticized for being susceptible to\nadversarial attacks. Most successful defense strategies adopt adversarial\ntraining or random input transformations that typically require retraining or\nfine-tuning the model to achieve reasonable performance. In this work, our\ninvestigations of intermediate representations of a pre-trained DNN lead to an\ninteresting discovery pointing to intrinsic robustness to adversarial attacks.\nWe find that we can learn a generative classifier by statistically\ncharacterizing the neural response of an intermediate layer to clean training\nsamples. The predictions of multiple such intermediate-layer based classifiers,\nwhen aggregated, show unexpected robustness to adversarial attacks.\nSpecifically, we devise an ensemble of these generative classifiers that\nrank-aggregates their predictions via a Borda count-based consensus. Our\nproposed approach uses a subset of the clean training data and a pre-trained\nmodel, and yet is agnostic to network architectures or the adversarial attack\ngeneration method. We show extensive experiments to establish that our defense\nstrategy achieves state-of-the-art performance on the ImageNet validation set.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:07:19 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Tiwari", "Lokender", ""], ["Madan", "Anish", ""], ["Anand", "Saket", ""], ["Banerjee", "Subhashis", ""]]}, {"id": "2006.10702", "submitter": "Cheng Cui", "authors": "Cheng Cui, Zhi Ye, Yangxi Li, Xinjian Li, Min Yang, Kai Wei, Bing Dai,\n  Yanmei Zhao, Zhongji Liu, and Rong Pang", "title": "Semi-Supervised Recognition under a Noisy and Fine-grained Dataset", "comments": "5 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simi-Supervised Recognition Challenge-FGVC7 is a challenging fine-grained\nrecognition competition. One of the difficulties of this competition is how to\nuse unlabeled data. We adopted pseudo-tag data mining to increase the amount of\ntraining data. The other one is how to identify similar birds with a very small\ndifference, especially those have a relatively tiny main-body in examples. We\ncombined generic image recognition and fine-grained image recognition method to\nsolve the problem. All generic image recognition models were training using\nPaddleClas . Using the combination of two different ways of deep recognition\nmodels, we finally won the third place in the competition.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:37:13 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Cui", "Cheng", ""], ["Ye", "Zhi", ""], ["Li", "Yangxi", ""], ["Li", "Xinjian", ""], ["Yang", "Min", ""], ["Wei", "Kai", ""], ["Dai", "Bing", ""], ["Zhao", "Yanmei", ""], ["Liu", "Zhongji", ""], ["Pang", "Rong", ""]]}, {"id": "2006.10704", "submitter": "Denis Volkhonskiy", "authors": "Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin,\n  Evgeny Burnaev", "title": "Latent Video Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The video generation task can be formulated as a prediction of future video\nframes given some past frames. Recent generative models for videos face the\nproblem of high computational requirements. Some models require up to 512\nTensor Processing Units for parallel training. In this work, we address this\nproblem via modeling the dynamics in a latent space. After the transformation\nof frames into the latent space, our model predicts latent representation for\nthe next frames in an autoregressive manner. We demonstrate the performance of\nour approach on BAIR Robot Pushing and Kinetics-600 datasets. The approach\ntends to reduce requirements to 8 Graphical Processing Units for training the\nmodels while maintaining comparable generation quality.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:38:38 GMT"}], "update_date": "2020-06-20", "authors_parsed": [["Rakhimov", "Ruslan", ""], ["Volkhonskiy", "Denis", ""], ["Artemov", "Alexey", ""], ["Zorin", "Denis", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2006.10705", "submitter": "Shuangfei Zhai", "authors": "Shuangfei Zhai, Walter Talbott, Miguel Angel Bautista, Carlos\n  Guestrin, Josh M. Susskind", "title": "Set Distribution Networks: a Generative Model for Sets of Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images with shared characteristics naturally form sets. For example, in a\nface verification benchmark, images of the same identity form sets. For\ngenerative models, the standard way of dealing with sets is to represent each\nas a one hot vector, and learn a conditional generative model\n$p(\\mathbf{x}|\\mathbf{y})$. This representation assumes that the number of sets\nis limited and known, such that the distribution over sets reduces to a simple\nmultinomial distribution. In contrast, we study a more generic problem where\nthe number of sets is large and unknown. We introduce Set Distribution Networks\n(SDNs), a novel framework that learns to autoencode and freely generate sets.\nWe achieve this by jointly learning a set encoder, set discriminator, set\ngenerator, and set prior. We show that SDNs are able to reconstruct image sets\nthat preserve salient attributes of the inputs in our benchmark datasets, and\nare also able to generate novel objects/identities. We examine the sets\ngenerated by SDN with a pre-trained 3D reconstruction network and a face\nverification network, respectively, as a novel way to evaluate the quality of\ngenerated sets of images.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:38:56 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Zhai", "Shuangfei", ""], ["Talbott", "Walter", ""], ["Bautista", "Miguel Angel", ""], ["Guestrin", "Carlos", ""], ["Susskind", "Josh M.", ""]]}, {"id": "2006.10712", "submitter": "Ertunc Erdil", "authors": "Ertunc Erdil, Krishna Chaitanya, Neerav Karani, Ender Konukoglu", "title": "Task-agnostic Out-of-Distribution Detection Using Kernel Density\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, researchers proposed a number of successful methods to\nperform out-of-distribution (OOD) detection in deep neural networks (DNNs). So\nfar the scope of the highly accurate methods has been limited to image level\nclassification tasks. However, attempts for generally applicable methods beyond\nclassification did not attain similar performance. In this paper, we address\nthis limitation by proposing a simple yet effective task-agnostic OOD detection\nmethod. We estimate the probability density functions (pdfs) of intermediate\nfeatures of a pre-trained DNN by performing kernel density estimation (KDE) on\nthe training dataset. As direct application of KDE to feature maps is hindered\nby their high dimensionality, we use a set of lower-dimensional marginalized\nKDE models instead of a single high-dimensional one. At test time, we evaluate\nthe pdfs on a test sample and produce a confidence score that indicates the\nsample is OOD. The use of KDE eliminates the need for making simplifying\nassumptions about the underlying feature pdfs and makes the proposed method\ntask-agnostic. We perform extensive experiments on classification tasks using\nbenchmark datasets for OOD detection. Additionally, we perform experiments on\nmedical image segmentation tasks using brain MRI datasets. The results\ndemonstrate that the proposed method consistently achieves high OOD detection\nperformance in both classification and segmentation tasks and improves\nstate-of-the-art in almost all cases. Code is available at\n\\url{https://github.com/eerdil/task_agnostic_ood}\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:46:06 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 20:39:03 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 11:29:14 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 21:55:47 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Erdil", "Ertunc", ""], ["Chaitanya", "Krishna", ""], ["Karani", "Neerav", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2006.10713", "submitter": "Nihal V. Nayak", "authors": "Nihal V. Nayak, Stephen H. Bach", "title": "Zero-Shot Learning with Common Sense Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning relies on semantic class representations such as\nhand-engineered attributes or learned embeddings to predict classes without any\nlabeled examples. We propose to learn class representations from common sense\nknowledge graphs. Common sense knowledge graphs are an untapped source of\nexplicit high-level knowledge that requires little human effort to apply to a\nrange of tasks. To capture the knowledge in the graph, we introduce ZSL-KG, a\ngeneral-purpose framework with a novel transformer graph convolutional network\n(TrGCN) for generating class representations. Our proposed TrGCN architecture\ncomputes non-linear combinations of the node neighbourhood and shows\nimprovements on zero-shot learning tasks in language and vision. Our results\nshow ZSL-KG outperforms the best performing graph-based zero-shot learning\nframework by an average of 2.1 accuracy points with improvements as high as 3.4\naccuracy points. Our ablation study on ZSL-KG with alternate graph neural\nnetworks shows that our TrGCN adds up to 1.2 accuracy points improvement on\nthese tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:46:17 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 20:06:18 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Nayak", "Nihal V.", ""], ["Bach", "Stephen H.", ""]]}, {"id": "2006.10721", "submitter": "Zhipeng Zhang", "authors": "Zhipeng Zhang, Houwen Peng, Jianlong Fu, Bing Li, Weiming Hu", "title": "Ocean: Object-aware Anchor-free Tracking", "comments": "Accepted by ECCV2020", "journal-ref": "ECCV2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anchor-based Siamese trackers have achieved remarkable advancements in\naccuracy, yet the further improvement is restricted by the lagged tracking\nrobustness. We find the underlying reason is that the regression network in\nanchor-based methods is only trained on the positive anchor boxes (i.e., $IoU\n\\geq0.6$). This mechanism makes it difficult to refine the anchors whose\noverlap with the target objects are small. In this paper, we propose a novel\nobject-aware anchor-free network to address this issue. First, instead of\nrefining the reference anchor boxes, we directly predict the position and scale\nof target objects in an anchor-free fashion. Since each pixel in groundtruth\nboxes is well trained, the tracker is capable of rectifying inexact predictions\nof target objects during inference. Second, we introduce a feature alignment\nmodule to learn an object-aware feature from predicted bounding boxes. The\nobject-aware feature can further contribute to the classification of target\nobjects and background. Moreover, we present a novel tracking framework based\non the anchor-free model. The experiments show that our anchor-free tracker\nachieves state-of-the-art performance on five benchmarks, including VOT-2018,\nVOT-2019, OTB-100, GOT-10k and LaSOT. The source code is available at\nhttps://github.com/researchmm/TracKit.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:51:39 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 17:00:21 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Zhang", "Zhipeng", ""], ["Peng", "Houwen", ""], ["Fu", "Jianlong", ""], ["Li", "Bing", ""], ["Hu", "Weiming", ""]]}, {"id": "2006.10724", "submitter": "Hongyuan Yu", "authors": "Hongyuan Yu, Houwen Peng", "title": "Cyclic Differentiable Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, differentiable architecture search has draw great attention due to\nits high efficiency and competitive performance. It searches the optimal\narchitecture in a shallow network, and then measures its performance in a deep\nevaluation network. This leads to the optimization of architecture search is\nindependent of the target evaluation network, and the discovered architecture\nis sub-optimal. To address this issue, we propose a novel cyclic differentiable\narchitecture search framework (CDARTS). Considering the structure difference,\nCDARTS builds a cyclic feedback mechanism between the search and evaluation\nnetworks. First, the search network generates an initial topology for\nevaluation, so that the weights of the evaluation network can be optimized.\nSecond, the architecture topology in the search network is further optimized by\nthe label supervision in classification, as well as the regularization from the\nevaluation network through feature distillation. Repeating the above cycle\nresults in a joint optimization of the search and evaluation networks, and thus\nenables the evolution of the topology to fit the final evaluation network. The\nexperiments and analysis on CIFAR, ImageNet and NAS-Bench- 201 demonstrate the\nefficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:55:19 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Yu", "Hongyuan", ""], ["Peng", "Houwen", ""]]}, {"id": "2006.10726", "submitter": "Evan Shelhamer", "authors": "Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, Trevor\n  Darrell", "title": "Tent: Fully Test-time Adaptation by Entropy Minimization", "comments": "ICLR 2021 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model must adapt itself to generalize to new and different data during\ntesting. In this setting of fully test-time adaptation the model has only the\ntest data and its own parameters. We propose to adapt by test entropy\nminimization (tent): we optimize the model for confidence as measured by the\nentropy of its predictions. Our method estimates normalization statistics and\noptimizes channel-wise affine transformations to update online on each batch.\nTent reduces generalization error for image classification on corrupted\nImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on\nImageNet-C. Tent handles source-free domain adaptation on digit recognition\nfrom SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to\nCityscapes, and on the VisDA-C benchmark. These results are achieved in one\nepoch of test-time optimization without altering training.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:55:28 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 18:57:39 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 17:58:01 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Wang", "Dequan", ""], ["Shelhamer", "Evan", ""], ["Liu", "Shaoteng", ""], ["Olshausen", "Bruno", ""], ["Darrell", "Trevor", ""]]}, {"id": "2006.10728", "submitter": "Steven Liu", "authors": "Steven Liu, Tongzhou Wang, David Bau, Jun-Yan Zhu, Antonio Torralba", "title": "Diverse Image Generation via Self-Conditioned GANs", "comments": "CVPR 2020. Code: https://github.com/stevliu/self-conditioned-gan.\n  Webpage: http://selfcondgan.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a simple but effective unsupervised method for generating\nrealistic and diverse images. We train a class-conditional GAN model without\nusing manually annotated class labels. Instead, our model is conditional on\nlabels automatically derived from clustering in the discriminator's feature\nspace. Our clustering step automatically discovers diverse modes, and\nexplicitly requires the generator to cover them. Experiments on standard mode\ncollapse benchmarks show that our method outperforms several competing methods\nwhen addressing mode collapse. Our method also performs well on large-scale\ndatasets such as ImageNet and Places365, improving both image diversity and\nstandard quality metrics, compared to previous methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:56:03 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Liu", "Steven", ""], ["Wang", "Tongzhou", ""], ["Bau", "David", ""], ["Zhu", "Jun-Yan", ""], ["Torralba", "Antonio", ""]]}, {"id": "2006.10731", "submitter": "Carlos Esteves", "authors": "Carlos Esteves, Ameesh Makadia, Kostas Daniilidis", "title": "Spin-Weighted Spherical CNNs", "comments": "Accepted to NeurIPS'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning equivariant representations is a promising way to reduce sample and\nmodel complexity and improve the generalization performance of deep neural\nnetworks. The spherical CNNs are successful examples, producing\nSO(3)-equivariant representations of spherical inputs. There are two main types\nof spherical CNNs. The first type lifts the inputs to functions on the rotation\ngroup SO(3) and applies convolutions on the group, which are computationally\nexpensive since SO(3) has one extra dimension. The second type applies\nconvolutions directly on the sphere, which are limited to zonal (isotropic)\nfilters, and thus have limited expressivity. In this paper, we present a new\ntype of spherical CNN that allows anisotropic filters in an efficient way,\nwithout ever leaving the spherical domain. The key idea is to consider\nspin-weighted spherical functions, which were introduced in physics in the\nstudy of gravitational waves. These are complex-valued functions on the sphere\nwhose phases change upon rotation. We define a convolution between\nspin-weighted functions and build a CNN based on it. The spin-weighted\nfunctions can also be interpreted as spherical vector fields, allowing\napplications to tasks where the inputs or outputs are vector fields.\nExperiments show that our method outperforms previous methods on tasks like\nclassification of spherical images, classification of 3D shapes and semantic\nsegmentation of spherical panoramas.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:57:21 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 17:38:17 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Esteves", "Carlos", ""], ["Makadia", "Ameesh", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "2006.10734", "submitter": "Rohit Girdhar", "authors": "Rohit Girdhar, Laura Gustafson, Aaron Adcock and Laurens van der\n  Maaten", "title": "Forward Prediction for Physical Reasoning", "comments": "Webpage/code/models: https://facebookresearch.github.io/phyre-fwd/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical reasoning requires forward prediction: the ability to forecast what\nwill happen next given some initial world state. We study the performance of\nstate-of-the-art forward-prediction models in the complex physical-reasoning\ntasks of the PHYRE benchmark. We do so by incorporating models that operate on\nobject or pixel-based representations of the world into simple\nphysical-reasoning agents. We find that forward-prediction models can improve\nphysical-reasoning performance, particularly on complex tasks that involve many\nobjects. However, we also find that these improvements are contingent on the\ntest tasks being small variations of train tasks, and that generalization to\ncompletely new task templates is challenging. Surprisingly, we observe that\nforward predictors with better pixel accuracy do not necessarily lead to better\nphysical-reasoning performance.Nevertheless, our best models set a new\nstate-of-the-art on the PHYRE benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:57:42 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 19:41:24 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Girdhar", "Rohit", ""], ["Gustafson", "Laura", ""], ["Adcock", "Aaron", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "2006.10738", "submitter": "Shengyu Zhao", "authors": "Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, Song Han", "title": "Differentiable Augmentation for Data-Efficient GAN Training", "comments": "NeurIPS 2020. Project: https://data-efficient-gans.mit.edu/ Code:\n  https://github.com/mit-han-lab/data-efficient-gans", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of generative adversarial networks (GANs) heavily\ndeteriorates given a limited amount of training data. This is mainly because\nthe discriminator is memorizing the exact training set. To combat it, we\npropose Differentiable Augmentation (DiffAugment), a simple method that\nimproves the data efficiency of GANs by imposing various types of\ndifferentiable augmentations on both real and fake samples. Previous attempts\nto directly augment the training data manipulate the distribution of real\nimages, yielding little benefit; DiffAugment enables us to adopt the\ndifferentiable augmentation for the generated samples, effectively stabilizes\ntraining, and leads to better convergence. Experiments demonstrate consistent\ngains of our method over a variety of GAN architectures and loss functions for\nboth unconditional and class-conditional generation. With DiffAugment, we\nachieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128x128\nand 2-4x reductions of FID given 1,000 images on FFHQ and LSUN. Furthermore,\nwith only 20% training data, we can match the top performance on CIFAR-10 and\nCIFAR-100. Finally, our method can generate high-fidelity images using only 100\nimages without pre-training, while being on par with existing transfer learning\nalgorithms. Code is available at\nhttps://github.com/mit-han-lab/data-efficient-gans.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:59:01 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 17:57:20 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2020 07:28:46 GMT"}, {"version": "v4", "created": "Mon, 7 Dec 2020 05:51:53 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhao", "Shengyu", ""], ["Liu", "Zhijian", ""], ["Lin", "Ji", ""], ["Zhu", "Jun-Yan", ""], ["Han", "Song", ""]]}, {"id": "2006.10739", "submitter": "Ben Mildenhall", "authors": "Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara\n  Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan\n  T. Barron, Ren Ng", "title": "Fourier Features Let Networks Learn High Frequency Functions in Low\n  Dimensional Domains", "comments": "Project page: https://people.eecs.berkeley.edu/~bmild/fourfeat/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that passing input points through a simple Fourier feature mapping\nenables a multilayer perceptron (MLP) to learn high-frequency functions in\nlow-dimensional problem domains. These results shed light on recent advances in\ncomputer vision and graphics that achieve state-of-the-art results by using\nMLPs to represent complex 3D objects and scenes. Using tools from the neural\ntangent kernel (NTK) literature, we show that a standard MLP fails to learn\nhigh frequencies both in theory and in practice. To overcome this spectral\nbias, we use a Fourier feature mapping to transform the effective NTK into a\nstationary kernel with a tunable bandwidth. We suggest an approach for\nselecting problem-specific Fourier features that greatly improves the\nperformance of MLPs for low-dimensional regression tasks relevant to the\ncomputer vision and graphics communities.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:59:11 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Tancik", "Matthew", ""], ["Srinivasan", "Pratul P.", ""], ["Mildenhall", "Ben", ""], ["Fridovich-Keil", "Sara", ""], ["Raghavan", "Nithin", ""], ["Singhal", "Utkarsh", ""], ["Ramamoorthi", "Ravi", ""], ["Barron", "Jonathan T.", ""], ["Ng", "Ren", ""]]}, {"id": "2006.10792", "submitter": "Eileen Li", "authors": "Eileen Li, Eric Kim, Andrew Zhai, Josh Beal, Kunlong Gu", "title": "Bootstrapping Complete The Look at Pinterest", "comments": "9 pages, 12 figures, To be published in KDD '20", "journal-ref": null, "doi": "10.1145/3394486.3403382", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Putting together an ideal outfit is a process that involves creativity and\nstyle intuition. This makes it a particularly difficult task to automate.\nExisting styling products generally involve human specialists and a highly\ncurated set of fashion items. In this paper, we will describe how we\nbootstrapped the Complete The Look (CTL) system at Pinterest. This is a\ntechnology that aims to learn the subjective task of \"style compatibility\" in\norder to recommend complementary items that complete an outfit. In particular,\nwe want to show recommendations from other categories that are compatible with\nan item of interest. For example, what are some heels that go well with this\ncocktail dress? We will introduce our outfit dataset of over 1 million outfits\nand 4 million objects, a subset of which we will make available to the research\ncommunity, and describe the pipeline used to obtain and refresh this dataset.\nFurthermore, we will describe how we evaluate this subjective task and compare\nmodel performance across multiple training methods. Lastly, we will share our\nlessons going from experimentation to working prototype, and how to mitigate\nfailure modes in the production environment. Our work represents one of the\nfirst examples of an industrial-scale solution for compatibility-based fashion\nrecommendation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 18:20:19 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 18:24:47 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Li", "Eileen", ""], ["Kim", "Eric", ""], ["Zhai", "Andrew", ""], ["Beal", "Josh", ""], ["Gu", "Kunlong", ""]]}, {"id": "2006.10802", "submitter": "Soumick Chatterjee", "authors": "Soumick Chatterjee, Kartik Prabhu, Mahantesh Pattadkal, Gerda\n  Bortsova, Chompunuch Sarasaen, Florian Dubost, Hendrik Mattern, Marleen de\n  Bruijne, Oliver Speck and Andreas N\\\"urnberger", "title": "DS6, Deformation-aware Semi-supervised Learning: Application to Small\n  Vessel Segmentation with Noisy Training Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blood vessels of the brain are providing the human brain with the required\nnutrients and oxygen. As a vulnerable part of the cerebral blood supply,\npathology of small vessels can cause serious problems such as Cerebral Small\nVessel Diseases (CSVD). It has also been shown that CSVD is related to\nneurodegeneration, such as in Alzheimer's disease. With the advancement of 7\nTesla MRI systems, higher spatial image resolution can be achieved, enabling\nthe depiction of very small vessels in the brain. Non-Deep Learning based\napproaches for vessel segmentation, e.g. Frangi's vessel enhancement with\nsubsequent thresholding are capable of segmenting medium to large vessels but\noften fail to segment small vessels. The sensitivity of these methods to small\nvessels can be increased by extensive parameter tuning or by manual\ncorrections, albeit making them time-consuming, laborious, and not feasible for\nlarger datasets. This paper proposes a deep learning architecture to\nautomatically segment small vessels in 7 Tesla 3D Time-of-Flight (ToF) Magnetic\nResonance Angiography (MRA) data. The algorithm was trained and evaluated on a\nsmall imperfect semi-automatically segmented dataset of only 11 subjects; using\nsix for training, two for validation and three for testing. Deep learning model\nbased on U-Net Multi-Scale Supervision was trained using the training subset\nand were made equivariant to elastic deformations in a self-supervised manner\nusing deformation-aware learning to improve the generalisation performance. The\nproposed technique was evaluated quantitatively and qualitatively against the\ntest set and achieved a dice score of 80.44$\\pm$0.83. Furthermore, the result\nof the proposed method was compared against a selected manually segmented\nregion (62.07 resultant dice) and has shown a considerable improvement (18.98%)\nwith deformation-aware learning.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 18:42:57 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 02:50:21 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Chatterjee", "Soumick", ""], ["Prabhu", "Kartik", ""], ["Pattadkal", "Mahantesh", ""], ["Bortsova", "Gerda", ""], ["Sarasaen", "Chompunuch", ""], ["Dubost", "Florian", ""], ["Mattern", "Hendrik", ""], ["de Bruijne", "Marleen", ""], ["Speck", "Oliver", ""], ["N\u00fcrnberger", "Andreas", ""]]}, {"id": "2006.10803", "submitter": "Mahmoud Assran", "authors": "Mahmoud Assran, Nicolas Ballas, Lluis Castrejon, Michael Rabbat", "title": "Supervision Accelerates Pre-training in Contrastive Semi-Supervised\n  Learning of Visual Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a strategy for improving the efficiency of contrastive\nlearning of visual representations by leveraging a small amount of supervised\ninformation during pre-training. We propose a semi-supervised loss, SuNCEt,\nbased on noise-contrastive estimation and neighbourhood component analysis,\nthat aims to distinguish examples of different classes in addition to the\nself-supervised instance-wise pretext tasks. On ImageNet, we find that SuNCEt\ncan be used to match the semi-supervised learning accuracy of previous\ncontrastive approaches while using less than half the amount of pre-training\nand compute. Our main insight is that leveraging even a small amount of labeled\ndata during pre-training, and not only during fine-tuning, provides an\nimportant signal that can significantly accelerate contrastive learning of\nvisual representations. Our code is available online at\ngithub.com/facebookresearch/suncet.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 18:44:13 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 19:39:24 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Assran", "Mahmoud", ""], ["Ballas", "Nicolas", ""], ["Castrejon", "Lluis", ""], ["Rabbat", "Michael", ""]]}, {"id": "2006.10841", "submitter": "Matteo Pedone", "authors": "Matteo Pedone, Abdelrahman Mostafa, Janne heikkil\\\"a", "title": "Learning non-rigid surface reconstruction from spatio-temporal image\n  patches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to reconstruct a dense spatio-temporal depth map of a\nnon-rigidly deformable object directly from a video sequence. The estimation of\ndepth is performed locally on spatio-temporal patches of the video, and then\nthe full depth video of the entire shape is recovered by combining them\ntogether. Since the geometric complexity of a local spatio-temporal patch of a\ndeforming non-rigid object is often simple enough to be faithfully represented\nwith a parametric model, we artificially generate a database of small deforming\nrectangular meshes rendered with different material properties and light\nconditions, along with their corresponding depth videos, and use such data to\ntrain a convolutional neural network. We tested our method on both synthetic\nand Kinect data and experimentally observed that the reconstruction error is\nsignificantly lower than the one obtained using other approaches like\nconventional non-rigid structure from motion.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 20:25:15 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Pedone", "Matteo", ""], ["Mostafa", "Abdelrahman", ""], ["heikkil\u00e4", "Janne", ""]]}, {"id": "2006.10848", "submitter": "Robin Tibor Schirrmeister", "authors": "Robin Tibor Schirrmeister, Yuxuan Zhou, Tonio Ball and Dan Zhang", "title": "Understanding Anomaly Detection with Deep Invertible Networks through\n  Hierarchies of Distributions and Features", "comments": "Published at NeurIPS 2020. Code can be found at\n  https://github.com/boschresearch/hierarchical_anomaly_detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative networks trained via maximum likelihood on a natural image\ndataset like CIFAR10 often assign high likelihoods to images from datasets with\ndifferent objects (e.g., SVHN). We refine previous investigations of this\nfailure at anomaly detection for invertible generative networks and provide a\nclear explanation of it as a combination of model bias and domain prior:\nConvolutional networks learn similar low-level feature distributions when\ntrained on any natural image dataset and these low-level features dominate the\nlikelihood. Hence, when the discriminative features between inliers and\noutliers are on a high-level, e.g., object shapes, anomaly detection becomes\nparticularly challenging. To remove the negative impact of model bias and\ndomain prior on detecting high-level differences, we propose two methods,\nfirst, using the log likelihood ratios of two identical models, one trained on\nthe in-distribution data (e.g., CIFAR10) and the other one on a more general\ndistribution of images (e.g., 80 Million Tiny Images). We also derive a novel\noutlier loss for the in-distribution network on samples from the more general\ndistribution to further improve the performance. Secondly, using a multi-scale\nmodel like Glow, we show that low-level features are mainly captured at early\nscales. Therefore, using only the likelihood contribution of the final scale\nperforms remarkably well for detecting high-level feature differences of the\nout-of-distribution and the in-distribution. This method is especially useful\nif one does not have access to a suitable general distribution. Overall, our\nmethods achieve strong anomaly detection performance in the unsupervised\nsetting, and only slightly underperform state-of-the-art classifier-based\nmethods in the supervised setting. Code can be found at\nhttps://github.com/boschresearch/hierarchical_anomaly_detection.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 20:56:14 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 17:09:58 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 17:27:25 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Schirrmeister", "Robin Tibor", ""], ["Zhou", "Yuxuan", ""], ["Ball", "Tonio", ""], ["Zhang", "Dan", ""]]}, {"id": "2006.10850", "submitter": "Lin Zhang", "authors": "Lin Zhang, Tiziano Portenier, Christoph Paulus, Orcun Goksel", "title": "Deep Image Translation for Enhancing Simulated Ultrasound Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound simulation based on ray tracing enables the synthesis of highly\nrealistic images. It can provide an interactive environment for training\nsonographers as an educational tool. However, due to high computational demand,\nthere is a trade-off between image quality and interactivity, potentially\nleading to sub-optimal results at interactive rates. In this work we introduce\na deep learning approach based on adversarial training that mitigates this\ntrade-off by improving the quality of simulated images with constant\ncomputation time. An image-to-image translation framework is utilized to\ntranslate low quality images into high quality versions. To incorporate\nanatomical information potentially lost in low quality images, we additionally\nprovide segmentation maps to image translation. Furthermore, we propose to\nleverage information from acoustic attenuation maps to better preserve acoustic\nshadows and directional artifacts, an invaluable feature for ultrasound image\ninterpretation. The proposed method yields an improvement of 7.2% in\nFr\\'{e}chet Inception Distance and 8.9% in patch-based Kullback-Leibler\ndivergence.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 21:05:27 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Zhang", "Lin", ""], ["Portenier", "Tiziano", ""], ["Paulus", "Christoph", ""], ["Goksel", "Orcun", ""]]}, {"id": "2006.10853", "submitter": "Thomio Watanabe", "authors": "Thomio Watanabe and Denis F. Wolf", "title": "Image classification in frequency domain with 2SReLU: a second harmonics\n  superposition activation function", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks are able to identify complex patterns and\nperform tasks with super-human capabilities. However, besides the exceptional\nresults, they are not completely understood and it is still impractical to\nhand-engineer similar solutions. In this work, an image classification\nConvolutional Neural Network and its building blocks are described from a\nfrequency domain perspective. Some network layers have established counterparts\nin the frequency domain like the convolutional and pooling layers. We propose\nthe 2SReLU layer, a novel non-linear activation function that preserves high\nfrequency components in deep networks. It is demonstrated that in the frequency\ndomain it is possible to achieve competitive results without using the\ncomputationally costly convolution operation. A source code implementation in\nPyTorch is provided at: https://gitlab.com/thomio/2srelu\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 21:11:43 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Watanabe", "Thomio", ""], ["Wolf", "Denis F.", ""]]}, {"id": "2006.10866", "submitter": "Raymond Shiau", "authors": "Raymond Shiau, Hao-Yu Wu, Eric Kim, Yue Li Du, Anqi Guo, Zhiyuan\n  Zhang, Eileen Li, Kunlong Gu, Charles Rosenberg, Andrew Zhai", "title": "Shop The Look: Building a Large Scale Visual Shopping System at\n  Pinterest", "comments": "10 pages, 7 figures, Accepted to KDD'20", "journal-ref": null, "doi": "10.1145/3394486.3403372", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As online content becomes ever more visual, the demand for searching by\nvisual queries grows correspondingly stronger. Shop The Look is an online\nshopping discovery service at Pinterest, leveraging visual search to enable\nusers to find and buy products within an image. In this work, we provide a\nholistic view of how we built Shop The Look, a shopping oriented visual search\nsystem, along with lessons learned from addressing shopping needs. We discuss\ntopics including core technology across object detection and visual embeddings,\nserving infrastructure for realtime inference, and data labeling methodology\nfor training/evaluation data collection and human evaluation. The user-facing\nimpacts of our system design choices are measured through offline evaluations,\nhuman relevance judgements, and online A/B experiments. The collective\nimprovements amount to cumulative relative gains of over 160% in end-to-end\nhuman relevance judgements and over 80% in engagement. Shop The Look is\ndeployed in production at Pinterest.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 21:38:07 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Shiau", "Raymond", ""], ["Wu", "Hao-Yu", ""], ["Kim", "Eric", ""], ["Du", "Yue Li", ""], ["Guo", "Anqi", ""], ["Zhang", "Zhiyuan", ""], ["Li", "Eileen", ""], ["Gu", "Kunlong", ""], ["Rosenberg", "Charles", ""], ["Zhai", "Andrew", ""]]}, {"id": "2006.10869", "submitter": "Jaweria Amjad", "authors": "Jaweria Amjad, Zhaoyan Lyu, Miguel R. D. Rodrigues", "title": "Model-Aware Regularization For Learning Approaches To Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are various inverse problems -- including reconstruction problems\narising in medical imaging -- where one is often aware of the forward operator\nthat maps variables of interest to the observations. It is therefore natural to\nask whether such knowledge of the forward operator can be exploited in deep\nlearning approaches increasingly used to solve inverse problems.\n  In this paper, we provide one such way via an analysis of the generalisation\nerror of deep learning methods applicable to inverse problems. In particular,\nby building on the algorithmic robustness framework, we offer a generalisation\nerror bound that encapsulates key ingredients associated with the learning\nproblem such as the complexity of the data space, the size of the training set,\nthe Jacobian of the deep neural network and the Jacobian of the composition of\nthe forward operator with the neural network. We then propose a 'plug-and-play'\nregulariser that leverages the knowledge of the forward map to improve the\ngeneralization of the network. We likewise also propose a new method allowing\nus to tightly upper bound the Lipschitz constants of the relevant functions\nthat is much more computational efficient than existing ones. We demonstrate\nthe efficacy of our model-aware regularised deep learning algorithms against\nother state-of-the-art approaches on inverse problems involving various\nsub-sampling operators such as those used in classical compressed sensing setup\nand accelerated Magnetic Resonance Imaging (MRI).\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 21:59:03 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Amjad", "Jaweria", ""], ["Lyu", "Zhaoyan", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "2006.10873", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Suhas Lohit, Pavan Turaga", "title": "Generative Patch Priors for Practical Compressive Image Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the generative patch prior (GPP) that defines a\ngenerative prior for compressive image recovery, based on patch-manifold\nmodels. Unlike learned, image-level priors that are restricted to the range\nspace of a pre-trained generator, GPP can recover a wide variety of natural\nimages using a pre-trained patch generator. Additionally, GPP retains the\nbenefits of generative priors like high reconstruction quality at extremely low\nsensing rates, while also being much more generally applicable. We show that\nGPP outperforms several unsupervised and supervised techniques on three\ndifferent sensing models -- linear compressive sensing with known, and unknown\ncalibration settings, and the non-linear phase retrieval problem. Finally, we\npropose an alternating optimization strategy using GPP for joint\ncalibration-and-reconstruction which performs favorably against several\nbaselines on a real world, un-calibrated compressive sensing dataset.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 22:24:46 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 16:51:14 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Anirudh", "Rushil", ""], ["Lohit", "Suhas", ""], ["Turaga", "Pavan", ""]]}, {"id": "2006.10923", "submitter": "Amish Patel", "authors": "Amish Patel and Aravind Varier", "title": "Hyperparameter Analysis for Image Captioning", "comments": "10 pages, 9 figures, and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we perform a thorough sensitivity analysis on state-of-the-art\nimage captioning approaches using two different architectures: CNN+LSTM and\nCNN+Transformer. Experiments were carried out using the Flickr8k dataset. The\nbiggest takeaway from the experiments is that fine-tuning the CNN encoder\noutperforms the baseline and all other experiments carried out for both\narchitectures.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 01:49:37 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Patel", "Amish", ""], ["Varier", "Aravind", ""]]}, {"id": "2006.10950", "submitter": "Zhen Yu", "authors": "Zhen Yu, Jennifer Nguyen, Xiaojun Chang, John Kelly, Catriona Mclean,\n  Lei Zhang, Victoria Mar, Zongyuan Ge", "title": "Melanoma Diagnosis with Spatio-Temporal Feature Learning on Sequential\n  Dermoscopic Images", "comments": "submission of miccai 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing studies for automated melanoma diagnosis are based on single-time\npoint images of lesions. However, melanocytic lesions de facto are\nprogressively evolving and, moreover, benign lesions can progress into\nmalignant melanoma. Ignoring cross-time morphological changes of lesions thus\nmay lead to misdiagnosis in borderline cases. Based on the fact that\ndermatologists diagnose ambiguous skin lesions by evaluating the dermoscopic\nchanges over time via follow-up examination, in this study, we propose an\nautomated framework for melanoma diagnosis using sequential dermoscopic images.\nTo capture the spatio-temporal characterization of dermoscopic evolution, we\nconstruct our model in a two-stream network architecture which capable of\nsimultaneously learning appearance representations of individual lesions while\nperforming temporal reasoning on both raw pixels difference and abstract\nfeatures difference. We collect 184 cases of serial dermoscopic image data,\nwhich consists of histologically confirmed 92 benign lesions and 92 melanoma\nlesions, to evaluate the effectiveness of the proposed method. Our model\nachieved AUC of 74.34%, which is ~8% higher than that of only using single\nimages and ~6% higher than the widely used sequence learning model based on\nLSTM.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 04:08:22 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Yu", "Zhen", ""], ["Nguyen", "Jennifer", ""], ["Chang", "Xiaojun", ""], ["Kelly", "John", ""], ["Mclean", "Catriona", ""], ["Zhang", "Lei", ""], ["Mar", "Victoria", ""], ["Ge", "Zongyuan", ""]]}, {"id": "2006.10955", "submitter": "Jasmine Bayrooti", "authors": "Nikka Mofid, Jasmine Bayrooti, Shreya Ravi", "title": "Keep Your AI-es on the Road: Tackling Distracted Driver Detection with\n  Convolutional Neural Networks and Targeted Data Augmentation", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the World Health Organization, distracted driving is one of the\nleading cause of motor accidents and deaths in the world. In our study, we\ntackle the problem of distracted driving by aiming to build a robust\nmulti-class classifier to detect and identify different forms of driver\ninattention using the State Farm Distracted Driving Dataset. We utilize\ncombinations of pretrained image classification models, classical data\naugmentation, OpenCV based image preprocessing and skin segmentation\naugmentation approaches. Our best performing model combines several\naugmentation techniques, including skin segmentation, facial blurring, and\nclassical augmentation techniques. This model achieves an approximately 15%\nincrease in F1 score over the baseline, thus showing the promise in these\ntechniques in enhancing the power of neural networks for the task of distracted\ndriver detection.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 04:56:08 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 01:05:55 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Mofid", "Nikka", ""], ["Bayrooti", "Jasmine", ""], ["Ravi", "Shreya", ""]]}, {"id": "2006.10962", "submitter": "Ivan Grishchenko", "authors": "Ivan Grishchenko, Artsiom Ablavatski, Yury Kartynnik, Karthik\n  Raveendran, Matthias Grundmann", "title": "Attention Mesh: High-fidelity Face Mesh Prediction in Real-time", "comments": "4 pages, 5 figures; CVPR Workshop on Computer Vision for Augmented\n  and Virtual Reality, Seattle, WA, USA, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Attention Mesh, a lightweight architecture for 3D face mesh\nprediction that uses attention to semantically meaningful regions. Our neural\nnetwork is designed for real-time on-device inference and runs at over 50 FPS\non a Pixel 2 phone. Our solution enables applications like AR makeup, eye\ntracking and AR puppeteering that rely on highly accurate landmarks for eye and\nlips regions. Our main contribution is a unified network architecture that\nachieves the same accuracy on facial landmarks as a multi-stage cascaded\napproach, while being 30 percent faster.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 05:07:38 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Grishchenko", "Ivan", ""], ["Ablavatski", "Artsiom", ""], ["Kartynnik", "Yury", ""], ["Raveendran", "Karthik", ""], ["Grundmann", "Matthias", ""]]}, {"id": "2006.10990", "submitter": "Luyan Liu", "authors": "Qinming Zhang, Luyan Liu, Kai Ma, Cheng Zhuo, Yefeng Zheng", "title": "Cross-denoising Network against Corrupted Labels in Medical Image\n  Segmentation with Domain Shift", "comments": "Accepted by IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNNs) have contributed many\nbreakthroughs in segmentation tasks, especially in the field of medical\nimaging. However, \\textit{domain shift} and \\textit{corrupted annotations},\nwhich are two common problems in medical imaging, dramatically degrade the\nperformance of DCNNs in practice. In this paper, we propose a novel robust\ncross-denoising framework using two peer networks to address domain shift and\ncorrupted label problems with a peer-review strategy. Specifically, each\nnetwork performs as a mentor, mutually supervised to learn from reliable\nsamples selected by the peer network to combat with corrupted labels. In\naddition, a noise-tolerant loss is proposed to encourage the network to capture\nthe key location and filter the discrepancy under various noise-contaminant\nlabels. To further reduce the accumulated error, we introduce a\nclass-imbalanced cross learning using most confident predictions at the\nclass-level. Experimental results on REFUGE and Drishti-GS datasets for optic\ndisc (OD) and optic cup (OC) segmentation demonstrate the superior performance\nof our proposed approach to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 07:35:25 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Zhang", "Qinming", ""], ["Liu", "Luyan", ""], ["Ma", "Kai", ""], ["Zhuo", "Cheng", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2006.11035", "submitter": "Stefano Melacci", "authors": "Lapo Faggi, Alessandro Betti, Dario Zanca, Stefano Melacci, Marco Gori", "title": "Wave Propagation of Visual Stimuli in Focus of Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast reactions to changes in the surrounding visual environment require\nefficient attention mechanisms to reallocate computational resources to most\nrelevant locations in the visual field. While current computational models keep\nimproving their predictive ability thanks to the increasing availability of\ndata, they still struggle approximating the effectiveness and efficiency\nexhibited by foveated animals. In this paper, we present a\nbiologically-plausible computational model of focus of attention that exhibits\nspatiotemporal locality and that is very well-suited for parallel and\ndistributed implementations. Attention emerges as a wave propagation process\noriginated by visual stimuli corresponding to details and motion information.\nThe resulting field obeys the principle of \"inhibition of return\" so as not to\nget stuck in potential holes. An accurate experimentation of the model shows\nthat it achieves top level performance in scanpath prediction tasks. This can\neasily be understood at the light of a theoretical result that we establish in\nthe paper, where we prove that as the velocity of wave propagation goes to\ninfinity, the proposed model reduces to recently proposed state of the art\ngravitational models of focus of attention.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 09:33:21 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Faggi", "Lapo", ""], ["Betti", "Alessandro", ""], ["Zanca", "Dario", ""], ["Melacci", "Stefano", ""], ["Gori", "Marco", ""]]}, {"id": "2006.11091", "submitter": "Christian Rathgeb", "authors": "Torsten Schlett, Christian Rathgeb, Christoph Busch", "title": "Deep Learning-based Single Image Face Depth Data Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition can benefit from the utilization of depth data captured\nusing low-cost cameras, in particular for presentation attack detection\npurposes. Depth video output from these capture devices can however contain\ndefects such as holes or general depth inaccuracies. This work proposes a deep\nlearning face depth enhancement method in this context of facial biometrics,\nwhich adds a security aspect to the topic. U-Net-like architectures are\nutilized, and the networks are compared against hand-crafted enhancer types, as\nwell as a similar depth enhancer network from related work trained for an\nadjacent application scenario. All tested enhancer types exclusively use depth\ndata as input, which differs from methods that enhance depth based on\nadditional input data such as visible light color images. Synthetic face depth\nground truth images and degraded forms thereof are created with help of PRNet,\nto train multiple deep learning enhancer models with different network sizes\nand training configurations. Evaluations are carried out on the synthetic data,\non Kinect v1 images from the KinectFaceDB, and on in-house RealSense D435\nimages. These evaluations include an assessment of the falsification for\noccluded face depth input, which is relevant to biometric security. The\nproposed deep learning enhancers yield noticeably better results than the\ntested preexisting enhancers, without overly falsifying depth data when\nnon-face input is provided, and are shown to reduce the error of a simple\nlandmark-based PAD method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 11:52:38 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 10:17:33 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 09:09:50 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Schlett", "Torsten", ""], ["Rathgeb", "Christian", ""], ["Busch", "Christoph", ""]]}, {"id": "2006.11117", "submitter": "Davood Karimi", "authors": "Davood Karimi, Lana Vasung, Camilo Jaimes, Fedel Machado-Rivas, Shadab\n  Khan, Simon K. Warfield, Ali Gholipour", "title": "A machine learning-based method for estimating the number and\n  orientations of major fascicles in diffusion-weighted magnetic resonance\n  imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-compartment modeling of diffusion-weighted magnetic resonance imaging\nmeasurements is necessary for accurate brain connectivity analysis. Existing\nmethods for estimating the number and orientations of fascicles in an imaging\nvoxel either depend on non-convex optimization techniques that are sensitive to\ninitialization and measurement noise, or are prone to predicting spurious\nfascicles. In this paper, we propose a machine learning-based technique that\ncan accurately estimate the number and orientations of fascicles in a voxel.\nOur method can be trained with either simulated or real diffusion-weighted\nimaging data. Our method estimates the angle to the closest fascicle for each\ndirection in a set of discrete directions uniformly spread on the unit sphere.\nThis information is then processed to extract the number and orientations of\nfascicles in a voxel. On realistic simulated phantom data with known ground\ntruth, our method predicts the number and orientations of crossing fascicles\nmore accurately than several existing methods. It also leads to more accurate\ntractography. On real data, our method is better than or compares favorably\nwith standard methods in terms of robustness to measurement down-sampling and\nalso in terms of expert quality assessment of tractography results.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 13:07:45 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Karimi", "Davood", ""], ["Vasung", "Lana", ""], ["Jaimes", "Camilo", ""], ["Machado-Rivas", "Fedel", ""], ["Khan", "Shadab", ""], ["Warfield", "Simon K.", ""], ["Gholipour", "Ali", ""]]}, {"id": "2006.11120", "submitter": "Assaf Shocher", "authors": "Assaf Shocher and Ben Feinstein and Niv Haim and Michal Irani", "title": "From Discrete to Continuous Convolution Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic operation in Convolutional Neural Networks (CNNs) is spatial resizing\nof feature maps. This is done either by strided convolution (donwscaling) or\ntransposed convolution (upscaling). Such operations are limited to a fixed\nfilter moving at predetermined integer steps (strides). Spatial sizes of\nconsecutive layers are related by integer scale factors, predetermined at\narchitectural design, and remain fixed throughout training and inference time.\nWe propose a generalization of the common Conv-layer, from a discrete layer to\na Continuous Convolution (CC) Layer. CC Layers naturally extend Conv-layers by\nrepresenting the filter as a learned continuous function over sub-pixel\ncoordinates. This allows learnable and principled resizing of feature maps, to\nany size, dynamically and consistently across scales. Once trained, the CC\nlayer can be used to output any scale/size chosen at inference time. The scale\ncan be non-integer and differ between the axes. CC gives rise to new freedoms\nfor architectural design, such as dynamic layer shapes at inference time, or\ngradual architectures where the size changes by a small factor at each layer.\nThis gives rise to many desired CNN properties, new architectural design\ncapabilities, and useful applications. We further show that current Conv-layers\nsuffer from inherent misalignments, which are ameliorated by CC layers.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 13:16:06 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Shocher", "Assaf", ""], ["Feinstein", "Ben", ""], ["Haim", "Niv", ""], ["Irani", "Michal", ""]]}, {"id": "2006.11132", "submitter": "Tom Monnier", "authors": "Tom Monnier, Thibault Groueix, Mathieu Aubry", "title": "Deep Transformation-Invariant Clustering", "comments": "Accepted at NeurIPS 2020 (oral). Project webpage:\n  http://imagine.enpc.fr/~monniert/DTIClustering/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in image clustering typically focus on learning better deep\nrepresentations. In contrast, we present an orthogonal approach that does not\nrely on abstract features but instead learns to predict image transformations\nand performs clustering directly in image space. This learning process\nnaturally fits in the gradient-based training of K-means and Gaussian mixture\nmodel, without requiring any additional loss or hyper-parameters. It leads us\nto two new deep transformation-invariant clustering frameworks, which jointly\nlearn prototypes and transformations. More specifically, we use deep learning\nmodules that enable us to resolve invariance to spatial, color and\nmorphological transformations. Our approach is conceptually simple and comes\nwith several advantages, including the possibility to easily adapt the desired\ninvariance to the task and a strong interpretability of both cluster centers\nand assignments to clusters. We demonstrate that our novel approach yields\ncompetitive and highly promising results on standard image clustering\nbenchmarks. Finally, we showcase its robustness and the advantages of its\nimproved interpretability by visualizing clustering results over real\nphotograph collections.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 13:43:08 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 18:08:13 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Monnier", "Tom", ""], ["Groueix", "Thibault", ""], ["Aubry", "Mathieu", ""]]}, {"id": "2006.11147", "submitter": "Tal\\'ia V\\'azquez Romaguera", "authors": "Tal\\'ia V\\'azquez Romaguera, Liset V\\'azquez Romaguera, David Castro\n  Pi\\~nol, Carlos Rom\\'an V\\'azquez Seisdedos", "title": "Pupil Center Detection Approaches: A comparative analysis", "comments": "15 pages, 9 figures, submitted to the journal \"Computaci\\'on y\n  Sistemas\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, the development of technologies and tools for eye\ntracking has been a constantly growing area. Detecting the center of the pupil,\nusing image processing techniques, has been an essential step in this process.\nA large number of techniques have been proposed for pupil center detection\nusing both traditional image processing and machine learning-based methods.\nDespite the large number of methods proposed, no comparative work on their\nperformance was found, using the same images and performance metrics. In this\nwork, we aim at comparing four of the most frequently cited traditional methods\nfor pupil center detection in terms of accuracy, robustness, and computational\ncost. These methods are based on the circular Hough transform, ellipse fitting,\nDaugman's integro-differential operator and radial symmetry transform. The\ncomparative analysis was performed with 800 infrared images from the\nCASIA-IrisV3 and CASIA-IrisV4 databases containing various types of\ndisturbances. The best performance was obtained by the method based on the\nradial symmetry transform with an accuracy and average robustness higher than\n94%. The shortest processing time, obtained with the ellipse fitting method,\nwas 0.06 s.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 14:19:07 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Romaguera", "Tal\u00eda V\u00e1zquez", ""], ["Romaguera", "Liset V\u00e1zquez", ""], ["Pi\u00f1ol", "David Castro", ""], ["Seisdedos", "Carlos Rom\u00e1n V\u00e1zquez", ""]]}, {"id": "2006.11149", "submitter": "Muhammad Umer Anwaar", "authors": "Muhammad Umer Anwaar, Egor Labintcev, Martin Kleinsteuber", "title": "Compositional Learning of Image-Text Query for Image Retrieval", "comments": "Published at IEEE WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of retrieving images from a\ndatabase based on a multi-modal (image-text) query. Specifically, the query\ntext prompts some modification in the query image and the task is to retrieve\nimages with the desired modifications. For instance, a user of an E-Commerce\nplatform is interested in buying a dress, which should look similar to her\nfriend's dress, but the dress should be of white color with a ribbon sash. In\nthis case, we would like the algorithm to retrieve some dresses with desired\nmodifications in the query dress. We propose an autoencoder based model,\nComposeAE, to learn the composition of image and text query for retrieving\nimages. We adopt a deep metric learning approach and learn a metric that pushes\ncomposition of source image and text query closer to the target images. We also\npropose a rotational symmetry constraint on the optimization problem. Our\napproach is able to outperform the state-of-the-art method TIRG \\cite{TIRG} on\nthree benchmark datasets, namely: MIT-States, Fashion200k and Fashion IQ. In\norder to ensure fair comparison, we introduce strong baselines by enhancing\nTIRG method. To ensure reproducibility of the results, we publish our code\nhere: \\url{https://github.com/ecom-research/ComposeAE}.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 14:21:41 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 06:06:12 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 21:35:55 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Anwaar", "Muhammad Umer", ""], ["Labintcev", "Egor", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "2006.11161", "submitter": "Aman Chadha Mr.", "authors": "Aman Chadha, John Britto, and M. Mani Roja", "title": "iSeeBetter: Spatio-temporal video super-resolution using recurrent\n  generative back-projection networks", "comments": "11 pages, 6 figures, 4 tables, Project Page:\n  https://iseebetter.amanchadha.com/", "journal-ref": "Springer Journal of Computational Visual Media, Tsinghua\n  University Press, 6(3):307-317, 2020", "doi": "10.1007/s41095-020-0175-7", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, learning-based models have enhanced the performance of single-image\nsuper-resolution (SISR). However, applying SISR successively to each video\nframe leads to a lack of temporal coherency. Convolutional neural networks\n(CNNs) outperform traditional approaches in terms of image quality metrics such\nas peak signal to noise ratio (PSNR) and structural similarity (SSIM). However,\ngenerative adversarial networks (GANs) offer a competitive advantage by being\nable to mitigate the issue of a lack of finer texture details, usually seen\nwith CNNs when super-resolving at large upscaling factors. We present\niSeeBetter, a novel GAN-based spatio-temporal approach to video\nsuper-resolution (VSR) that renders temporally consistent super-resolution\nvideos. iSeeBetter extracts spatial and temporal information from the current\nand neighboring frames using the concept of recurrent back-projection networks\nas its generator. Furthermore, to improve the \"naturality\" of the\nsuper-resolved image while eliminating artifacts seen with traditional\nalgorithms, we utilize the discriminator from super-resolution generative\nadversarial network (SRGAN). Although mean squared error (MSE) as a primary\nloss-minimization objective improves PSNR/SSIM, these metrics may not capture\nfine details in the image resulting in misrepresentation of perceptual quality.\nTo address this, we use a four-fold (MSE, perceptual, adversarial, and\ntotal-variation (TV)) loss function. Our results demonstrate that iSeeBetter\noffers superior VSR fidelity and surpasses state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 01:36:30 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 20:34:00 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2020 21:38:05 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2020 00:45:38 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Chadha", "Aman", ""], ["Britto", "John", ""], ["Roja", "M. Mani", ""]]}, {"id": "2006.11162", "submitter": "Wang Yiqi", "authors": "Tian YingJie, Wang YiQi, Yang LinRui, Qi ZhiQuan", "title": "Concatenated Attention Neural Network for Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a general framework for low-level vision tasks\nincluding image compression artifacts reduction and image denoising. Under this\nframework, a novel concatenated attention neural network (CANet) is\nspecifically designed for image restoration. The main contributions of this\npaper are as follows: First, by applying concise but effective concatenation\nand feature selection mechanism, we establish a novel connection mechanism\nwhich connect different modules in the modules stacking network. Second, both\npixel-wise and channel-wise attention mechanisms are used in each module\nconvolution layer, which promotes further extraction of more essential\ninformation in images. Lastly, we demonstrate that CANet achieves better\nresults than previous state-of-the-art approaches with sufficient experiments\nin compression artifacts removing and image denoising.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 14:49:25 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["YingJie", "Tian", ""], ["YiQi", "Wang", ""], ["LinRui", "Yang", ""], ["ZhiQuan", "Qi", ""]]}, {"id": "2006.11168", "submitter": "Denis Rangulov", "authors": "Denis Rangulov, Muhammad Fahim", "title": "Emotion Recognition on large video dataset based on Convolutional\n  Feature Extractor and Recurrent Neural Network", "comments": "6 pages, 7 figures, Face and Gesture 2020 Workshop Paper (ABAW2020\n  competition)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many years, the emotion recognition task has remained one of the most\ninteresting and important problems in the field of human-computer interaction.\nIn this study, we consider the emotion recognition task as a classification as\nwell as a regression task by processing encoded emotions in different datasets\nusing deep learning models. Our model combines convolutional neural network\n(CNN) with recurrent neural network (RNN) to predict dimensional emotions on\nvideo data. At the first step, CNN extracts feature vectors from video frames.\nIn the second step, we fed these feature vectors to train RNN for exploiting\nthe temporal dynamics of video. Furthermore, we analyzed how each neural\nnetwork contributes to the system's overall performance. The experiments are\nperformed on publicly available datasets including the largest modern Aff-Wild2\ndatabase. It contains over sixty hours of video data. We discovered the problem\nof overfitting of the model on an unbalanced dataset with an illustrative\nexample using confusion matrices. The problem is solved by downsampling\ntechnique to balance the dataset. By significantly decreasing training data, we\nbalance the dataset, thereby, the overall performance of the model is improved.\nHence, the study qualitatively describes the abilities of deep learning models\nexploring enough amount of data to predict facial emotions. Our proposed method\nis implemented using Tensorflow Keras.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 14:54:13 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Rangulov", "Denis", ""], ["Fahim", "Muhammad", ""]]}, {"id": "2006.11183", "submitter": "Hacer Yalim Keles", "authors": "Anil Osman Tur, Hacer Yalim Keles", "title": "Evaluation Of Hidden Markov Models Using Deep CNN Features In Isolated\n  Sign Recognition", "comments": "This paper is the preprint of the accepted manuscript at Multimedia\n  Tools and Applications Journal. It contains 16 pages, 5 figure, 8 tables", "journal-ref": null, "doi": "10.1007/s11042-021-10593-w", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isolated sign recognition from video streams is a challenging problem due to\nthe multi-modal nature of the signs, where both local and global hand features\nand face gestures needs to be attended simultaneously. This problem has\nrecently been studied widely using deep Convolutional Neural Network (CNN)\nbased features and Long Short-Term Memory (LSTM) based deep sequence models.\nHowever, the current literature is lack of providing empirical analysis using\nHidden Markov Models (HMMs) with deep features. In this study, we provide a\nframework that is composed of three modules to solve isolated sign recognition\nproblem using different sequence models. The dimensions of deep features are\nusually too large to work with HMM models. To solve this problem, we propose\ntwo alternative CNN based architectures as the second module in our framework,\nto reduce deep feature dimensions effectively. After extensive experiments, we\nshow that using pretrained Resnet50 features and one of our CNN based dimension\nreduction models, HMMs can classify isolated signs with 90.15% accuracy in\nMontalbano dataset using RGB and Skeletal data. This performance is comparable\nwith the current LSTM based models. HMMs have fewer parameters and can be\ntrained and run on commodity computers fast, without requiring GPUs. Therefore,\nour analysis with deep features show that HMMs could also be utilized as well\nas deep sequence models in challenging isolated sign recognition problem.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 15:18:03 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 13:24:33 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Tur", "Anil Osman", ""], ["Keles", "Hacer Yalim", ""]]}, {"id": "2006.11184", "submitter": "Jeff Calder", "authors": "Jeff Calder, Brendan Cook, Matthew Thorpe, Dejan Slepcev", "title": "Poisson Learning: Graph Based Semi-Supervised Learning At Very Low Label\n  Rates", "comments": "Proceedings of the 37th International Conference on Machine Learning,\n  Online, PMLR 119, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA math.AP math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework, called Poisson learning, for graph based\nsemi-supervised learning at very low label rates. Poisson learning is motivated\nby the need to address the degeneracy of Laplacian semi-supervised learning in\nthis regime. The method replaces the assignment of label values at training\npoints with the placement of sources and sinks, and solves the resulting\nPoisson equation on the graph. The outcomes are provably more stable and\ninformative than those of Laplacian learning. Poisson learning is efficient and\nsimple to implement, and we present numerical experiments showing the method is\nsuperior to other recent approaches to semi-supervised learning at low label\nrates on MNIST, FashionMNIST, and Cifar-10. We also propose a graph-cut\nenhancement of Poisson learning, called Poisson MBO, that gives higher accuracy\nand can incorporate prior knowledge of relative class sizes.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 15:21:04 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 14:46:56 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Calder", "Jeff", ""], ["Cook", "Brendan", ""], ["Thorpe", "Matthew", ""], ["Slepcev", "Dejan", ""]]}, {"id": "2006.11193", "submitter": "S\\'ergio Pereira", "authors": "Sergio Pereira, Adriano Pinto, Joana Amorim, Alexandrine Ribeiro,\n  Victor Alves, Carlos A. Silva", "title": "Adaptive feature recombination and recalibration for semantic\n  segmentation with Fully Convolutional Networks", "comments": "Published in IEEE Transactions on Medical Imaging (TMI)", "journal-ref": "IEEE Transactions on Medical Imaging, vol. 38, no. 12, pp.\n  2914-2925, Dec. 2019", "doi": "10.1109/TMI.2019.2918096", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully Convolutional Networks have been achieving remarkable results in image\nsemantic segmentation, while being efficient. Such efficiency results from the\ncapability of segmenting several voxels in a single forward pass. So, there is\na direct spatial correspondence between a unit in a feature map and the voxel\nin the same location. In a convolutional layer, the kernel spans over all\nchannels and extracts information from them. We observe that linear\nrecombination of feature maps by increasing the number of channels followed by\ncompression may enhance their discriminative power. Moreover, not all feature\nmaps have the same relevance for the classes being predicted. In order to learn\nthe inter-channel relationships and recalibrate the channels to suppress the\nless relevant ones, Squeeze and Excitation blocks were proposed in the context\nof image classification with Convolutional Neural Networks. However, this is\nnot well adapted for segmentation with Fully Convolutional Networks since they\nsegment several objects simultaneously, hence a feature map may contain\nrelevant information only in some locations. In this paper, we propose\nrecombination of features and a spatially adaptive recalibration block that is\nadapted for semantic segmentation with Fully Convolutional Networks - the SegSE\nblock. Feature maps are recalibrated by considering the cross-channel\ninformation together with spatial relevance. Experimental results indicate that\nRecombination and Recalibration improve the results of a competitive baseline,\nand generalize across three different problems: brain tumor segmentation,\nstroke penumbra estimation, and ischemic stroke lesion outcome prediction. The\nobtained results are competitive or outperform the state of the art in the\nthree applications.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 15:45:03 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Pereira", "Sergio", ""], ["Pinto", "Adriano", ""], ["Amorim", "Joana", ""], ["Ribeiro", "Alexandrine", ""], ["Alves", "Victor", ""], ["Silva", "Carlos A.", ""]]}, {"id": "2006.11207", "submitter": "Nathan Somavarapu", "authors": "Nathan Somavarapu and Chih-Yao Ma and Zsolt Kira", "title": "Frustratingly Simple Domain Generalization via Image Stylization", "comments": "Code: https://github.com/GT-RIPL/DomainGeneralization-Stylization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) show impressive performance in the\nstandard classification setting where training and testing data are drawn\ni.i.d. from a given domain. However, CNNs do not readily generalize to new\ndomains with different statistics, a setting that is simple for humans. In this\nwork, we address the Domain Generalization problem, where the classifier must\ngeneralize to an unknown target domain. Inspired by recent works that have\nshown a difference in biases between CNNs and humans, we demonstrate an\nextremely simple yet effective method, namely correcting this bias by\naugmenting the dataset with stylized images. In contrast with existing\nstylization works, which use external data sources such as art, we further\nintroduce a method that is entirely in-domain using no such extra sources of\ndata. We provide a detailed analysis as to the mechanism by which the method\nworks, verifying our claim that it changes the shape/texture bias, and\ndemonstrate results surpassing or comparable to the state of the arts that\nutilize much more complex methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 16:20:40 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 15:13:11 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Somavarapu", "Nathan", ""], ["Ma", "Chih-Yao", ""], ["Kira", "Zsolt", ""]]}, {"id": "2006.11223", "submitter": "Ghada Zamzmi", "authors": "Ghada Zamzmi, Sivaramakrishnan Rajaraman, Sameer Antani", "title": "Unified Representation Learning for Efficient Medical Image Analysis", "comments": null, "journal-ref": "Under Review 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image analysis typically includes several tasks such as enhancement,\nsegmentation, and classification. Traditionally, these tasks are implemented\nusing separate deep learning models for separate tasks, which is not efficient\nbecause it involves unnecessary training repetitions, demands greater\ncomputational resources, and requires a relatively large amount of labeled\ndata. In this paper, we propose a multi-task training approach for medical\nimage analysis, where individual tasks are fine-tuned simultaneously through\nrelevant knowledge transfer using a unified modality-specific feature\nrepresentation (UMS-Rep). We explore different fine-tuning strategies to\ndemonstrate the impact of the strategy on the performance of target medical\nimage tasks. We experiment with different visual tasks (e.g., image denoising,\nsegmentation, and classification) to highlight the advantages offered with our\napproach for two imaging modalities, chest X-ray and Doppler echocardiography.\nOur results demonstrate that the proposed approach reduces the overall demand\nfor computational resources and improves target task generalization and\nperformance. Further, our results prove that the performance of target tasks in\nmedical images is highly influenced by the utilized fine-tuning strategy.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 16:52:16 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 00:04:34 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zamzmi", "Ghada", ""], ["Rajaraman", "Sivaramakrishnan", ""], ["Antani", "Sameer", ""]]}, {"id": "2006.11227", "submitter": "Hadi Jamali-Rad", "authors": "Hadi Jamali-Rad, Attila Szabo", "title": "Lookahead Adversarial Learning for Near Real-Time Semantic Segmentation", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is one of the most fundamental problems in computer\nvision with significant impact on a wide variety of applications. Adversarial\nlearning is shown to be an effective approach for improving semantic\nsegmentation quality by enforcing higher-level pixel correlations and\nstructural information. However, state-of-the-art semantic segmentation models\ncannot be easily plugged into an adversarial setting because they are not\ndesigned to accommodate convergence and stability issues in adversarial\nnetworks. We bridge this gap by building a conditional adversarial network with\na state-of-the-art segmentation model (DeepLabv3+) at its core. To battle the\nstability issues, we introduce a novel lookahead adversarial learning (LoAd)\napproach with an embedded label map aggregation module. We focus on semantic\nsegmentation models that run fast at inference for near real-time field\napplications. Through extensive experimentation, we demonstrate that the\nproposed solution can alleviate divergence issues in an adversarial semantic\nsegmentation setting and results in considerable performance improvements (+5%\nin some classes) on the baseline for three standard datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 17:04:38 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 08:46:07 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 15:00:09 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Jamali-Rad", "Hadi", ""], ["Szabo", "Attila", ""]]}, {"id": "2006.11242", "submitter": "Yuhua Chen", "authors": "Yuhua Chen, Luc Van Gool, Cordelia Schmid, Cristian Sminchisescu", "title": "Consistency Guided Scene Flow Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistency Guided Scene Flow Estimation (CGSF) is a self-supervised\nframework for the joint reconstruction of 3D scene structure and motion from\nstereo video. The model takes two temporal stereo pairs as input, and predicts\ndisparity and scene flow. The model self-adapts at test time by iteratively\nrefining its predictions. The refinement process is guided by a consistency\nloss, which combines stereo and temporal photo-consistency with a geometric\nterm that couples disparity and 3D motion. To handle inherent modeling error in\nthe consistency loss (e.g. Lambertian assumptions) and for better\ngeneralization, we further introduce a learned, output refinement network,\nwhich takes the initial predictions, the loss, and the gradient as input, and\nefficiently predicts a correlated output update. In multiple experiments,\nincluding ablation studies, we show that the proposed model can reliably\npredict disparity and scene flow in challenging imagery, achieves better\ngeneralization than the state-of-the-art, and adapts quickly and robustly to\nunseen domains.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 17:28:07 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 09:58:47 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Chen", "Yuhua", ""], ["Van Gool", "Luc", ""], ["Schmid", "Cordelia", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "2006.11275", "submitter": "Tianwei Yin", "authors": "Tianwei Yin, Xingyi Zhou, Philipp Kr\\\"ahenb\\\"uhl", "title": "Center-based 3D Object Detection and Tracking", "comments": "update nuScenes and Waymo results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional objects are commonly represented as 3D boxes in a\npoint-cloud. This representation mimics the well-studied image-based 2D\nbounding-box detection but comes with additional challenges. Objects in a 3D\nworld do not follow any particular orientation, and box-based detectors have\ndifficulties enumerating all orientations or fitting an axis-aligned bounding\nbox to rotated objects. In this paper, we instead propose to represent, detect,\nand track 3D objects as points. Our framework, CenterPoint, first detects\ncenters of objects using a keypoint detector and regresses to other attributes,\nincluding 3D size, 3D orientation, and velocity. In a second stage, it refines\nthese estimates using additional point features on the object. In CenterPoint,\n3D object tracking simplifies to greedy closest-point matching. The resulting\ndetection and tracking algorithm is simple, efficient, and effective.\nCenterPoint achieved state-of-the-art performance on the nuScenes benchmark for\nboth 3D detection and tracking, with 65.5 NDS and 63.8 AMOTA for a single\nmodel. On the Waymo Open Dataset, CenterPoint outperforms all previous single\nmodel method by a large margin and ranks first among all Lidar-only\nsubmissions. The code and pretrained models are available at\nhttps://github.com/tianweiy/CenterPoint.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 17:59:39 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 18:56:03 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Yin", "Tianwei", ""], ["Zhou", "Xingyi", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "2006.11316", "submitter": "Forrest Iandola", "authors": "Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer", "title": "SqueezeBERT: What can computer vision teach NLP about efficient neural\n  networks?", "comments": "9 pages + appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans read and write hundreds of billions of messages every day. Further,\ndue to the availability of large datasets, large computing systems, and better\nneural network models, natural language processing (NLP) technology has made\nsignificant strides in understanding, proofreading, and organizing these\nmessages. Thus, there is a significant opportunity to deploy NLP in myriad\napplications to help web users, social networks, and businesses. In particular,\nwe consider smartphones and other mobile devices as crucial platforms for\ndeploying NLP models at scale. However, today's highly-accurate NLP neural\nnetwork models such as BERT and RoBERTa are extremely computationally\nexpensive, with BERT-base taking 1.7 seconds to classify a text snippet on a\nPixel 3 smartphone. In this work, we observe that methods such as grouped\nconvolutions have yielded significant speedups for computer vision networks,\nbut many of these techniques have not been adopted by NLP neural network\ndesigners. We demonstrate how to replace several operations in self-attention\nlayers with grouped convolutions, and we use this technique in a novel network\narchitecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the\nPixel 3 while achieving competitive accuracy on the GLUE test set. The\nSqueezeBERT code will be released.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 18:40:29 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Iandola", "Forrest N.", ""], ["Shaw", "Albert E.", ""], ["Krishna", "Ravi", ""], ["Keutzer", "Kurt W.", ""]]}, {"id": "2006.11325", "submitter": "Carlos Roberto Medina Temme", "authors": "Carlos Medina, Arnout Devos, Matthias Grossglauser", "title": "Self-Supervised Prototypical Transfer Learning for Few-Shot\n  Classification", "comments": "Extended version of work presented at the 7th ICML Workshop on\n  Automated Machine Learning (2020). Code available at\n  https://github.com/indy-lab/ProtoTransfer ; 17 pages, 3 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most approaches in few-shot learning rely on costly annotated data related to\nthe goal task domain during (pre-)training. Recently, unsupervised\nmeta-learning methods have exchanged the annotation requirement for a reduction\nin few-shot classification performance. Simultaneously, in settings with\nrealistic domain shift, common transfer learning has been shown to outperform\nsupervised meta-learning. Building on these insights and on advances in\nself-supervised learning, we propose a transfer learning approach which\nconstructs a metric embedding that clusters unlabeled prototypical samples and\ntheir augmentations closely together. This pre-trained embedding is a starting\npoint for few-shot classification by summarizing class clusters and\nfine-tuning. We demonstrate that our self-supervised prototypical transfer\nlearning approach ProtoTransfer outperforms state-of-the-art unsupervised\nmeta-learning methods on few-shot tasks from the mini-ImageNet dataset. In\nfew-shot experiments with domain shift, our approach even has comparable\nperformance to supervised methods, but requires orders of magnitude fewer\nlabels.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 19:00:11 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Medina", "Carlos", ""], ["Devos", "Arnout", ""], ["Grossglauser", "Matthias", ""]]}, {"id": "2006.11328", "submitter": "Ivan Skorokhodov", "authors": "Ivan Skorokhodov, Mohamed Elhoseiny", "title": "Class Normalization for (Continual)? Generalized Zero-Shot Learning", "comments": "22 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization techniques have proved to be a crucial ingredient of successful\ntraining in a traditional supervised learning regime. However, in the zero-shot\nlearning (ZSL) world, these ideas have received only marginal attention. This\nwork studies normalization in ZSL scenario from both theoretical and practical\nperspectives. First, we give a theoretical explanation to two popular tricks\nused in zero-shot learning: normalize+scale and attributes normalization and\nshow that they help training by preserving variance during a forward pass.\nNext, we demonstrate that they are insufficient to normalize a deep ZSL model\nand propose Class Normalization (CN): a normalization scheme, which alleviates\nthis issue both provably and in practice. Third, we show that ZSL models\ntypically have more irregular loss surface compared to traditional classifiers\nand that the proposed method partially remedies this problem. Then, we test our\napproach on 4 standard ZSL datasets and outperform sophisticated modern SotA\nwith a simple MLP optimized without any bells and whistles and having ~50 times\nfaster training speed. Finally, we generalize ZSL to a broader problem --\ncontinual ZSL, and introduce some principled metrics and rigorous baselines for\nthis new setup. The project page is located at\nhttps://universome.github.io/class-norm.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 19:05:24 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 16:12:34 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Skorokhodov", "Ivan", ""], ["Elhoseiny", "Mohamed", ""]]}, {"id": "2006.11337", "submitter": "Tianlang Chen", "authors": "Tianlang Chen, Wei Xiong, Haitian Zheng, Jiebo Luo", "title": "Image Sentiment Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce an important but still unexplored research task --\nimage sentiment transfer. Compared with other related tasks that have been\nwell-studied, such as image-to-image translation and image style transfer,\ntransferring the sentiment of an image is more challenging. Given an input\nimage, the rule to transfer the sentiment of each contained object can be\ncompletely different, making existing approaches that perform global image\ntransfer by a single reference image inadequate to achieve satisfactory\nperformance. In this paper, we propose an effective and flexible framework that\nperforms image sentiment transfer at the object level. It first detects the\nobjects and extracts their pixel-level masks, and then performs object-level\nsentiment transfer guided by multiple reference images for the corresponding\nobjects. For the core object-level sentiment transfer, we propose a novel\nSentiment-aware GAN (SentiGAN). Both global image-level and local object-level\nsupervisions are imposed to train SentiGAN. More importantly, an effective\ncontent disentanglement loss cooperating with a content alignment step is\napplied to better disentangle the residual sentiment-related information of the\ninput image. Extensive quantitative and qualitative experiments are performed\non the object-oriented VSO dataset we create, demonstrating the effectiveness\nof the proposed framework.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 19:28:08 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Chen", "Tianlang", ""], ["Xiong", "Wei", ""], ["Zheng", "Haitian", ""], ["Luo", "Jiebo", ""]]}, {"id": "2006.11339", "submitter": "Dahun Kim", "authors": "Dahun Kim, Sanghyun Woo, Joon-Young Lee, In So Kweon", "title": "Video Panoptic Segmentation", "comments": "CVPR 2020 Oral. Code: see https://github.com/mcahny/vps", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation has become a new standard of visual recognition task by\nunifying previous semantic segmentation and instance segmentation tasks in\nconcert. In this paper, we propose and explore a new video extension of this\ntask, called video panoptic segmentation. The task requires generating\nconsistent panoptic segmentation as well as an association of instance ids\nacross video frames. To invigorate research on this new task, we present two\ntypes of video panoptic datasets. The first is a re-organization of the\nsynthetic VIPER dataset into the video panoptic format to exploit its\nlarge-scale pixel annotations. The second is a temporal extension on the\nCityscapes val. set, by providing new video panoptic annotations\n(Cityscapes-VPS). Moreover, we propose a novel video panoptic segmentation\nnetwork (VPSNet) which jointly predicts object classes, bounding boxes, masks,\ninstance id tracking, and semantic segmentation in video frames. To provide\nappropriate metrics for this task, we propose a video panoptic quality (VPQ)\nmetric and evaluate our method and several other baselines. Experimental\nresults demonstrate the effectiveness of the presented two datasets. We achieve\nstate-of-the-art results in image PQ on Cityscapes and also in VPQ on\nCityscapes-VPS and VIPER datasets. The datasets and code are made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 19:35:47 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kim", "Dahun", ""], ["Woo", "Sanghyun", ""], ["Lee", "Joon-Young", ""], ["Kweon", "In So", ""]]}, {"id": "2006.11341", "submitter": "Artsiom Ablavatski", "authors": "Artsiom Ablavatski, Andrey Vakunov, Ivan Grishchenko, Karthik\n  Raveendran, Matsvei Zhdanovich", "title": "Real-time Pupil Tracking from Monocular Video for Digital Puppetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, real-time approach for pupil tracking from live video on\nmobile devices. Our method extends a state-of-the-art face mesh detector with\ntwo new components: a tiny neural network that predicts positions of the pupils\nin 2D, and a displacement-based estimation of the pupil blend shape\ncoefficients. Our technique can be used to accurately control the pupil\nmovements of a virtual puppet, and lends liveliness and energy to it. The\nproposed approach runs at over 50 FPS on modern phones, and enables its usage\nin any real-time puppeteering pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 19:39:32 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Ablavatski", "Artsiom", ""], ["Vakunov", "Andrey", ""], ["Grishchenko", "Ivan", ""], ["Raveendran", "Karthik", ""], ["Zhdanovich", "Matsvei", ""]]}, {"id": "2006.11364", "submitter": "Alexander Lavin", "authors": "Louise Naud and Alexander Lavin", "title": "Manifolds for Unsupervised Visual Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies are by definition rare, thus labeled examples are very limited or\nnonexistent, and likely do not cover unforeseen scenarios. Unsupervised\nlearning methods that don't necessarily encounter anomalies in training would\nbe immensely useful. Generative vision models can be useful in this regard but\ndo not sufficiently represent normal and abnormal data distributions. To this\nend, we propose constant curvature manifolds for embedding data distributions\nin unsupervised visual anomaly detection. Through theoretical and empirical\nexplorations of manifold shapes, we develop a novel hyperspherical Variational\nAuto-Encoder (VAE) via stereographic projections with a gyroplane layer - a\ncomplete equivalent to the Poincar\\'e VAE. This approach with manifold\nprojections is beneficial in terms of model generalization and can yield more\ninterpretable representations. We present state-of-the-art results on visual\nanomaly benchmarks in precision manufacturing and inspection, demonstrating\nreal-world utility in industrial AI scenarios. We further demonstrate the\napproach on the challenging problem of histopathology: our unsupervised\napproach effectively detects cancerous brain tissue from noisy whole-slide\nimages, learning a smooth, latent organization of tissue types that provides an\ninterpretable decisions tool for medical professionals.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 20:41:58 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Naud", "Louise", ""], ["Lavin", "Alexander", ""]]}, {"id": "2006.11366", "submitter": "Dan Zeng", "authors": "Dan Zeng, Raymond Veldhuis and Luuk Spreeuwers", "title": "A survey of face recognition techniques under occlusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The limited capacity to recognize faces under occlusions is a long-standing\nproblem that presents a unique challenge for face recognition systems and even\nfor humans. The problem regarding occlusion is less covered by research when\ncompared to other challenges such as pose variation, different expressions,\netc. Nevertheless, occluded face recognition is imperative to exploit the full\npotential of face recognition for real-world applications. In this paper, we\nrestrict the scope to occluded face recognition. First, we explore what the\nocclusion problem is and what inherent difficulties can arise. As a part of\nthis review, we introduce face detection under occlusion, a preliminary step in\nface recognition. Second, we present how existing face recognition methods cope\nwith the occlusion problem and classify them into three categories, which are\n1) occlusion robust feature extraction approaches, 2) occlusion aware face\nrecognition approaches, and 3) occlusion recovery based face recognition\napproaches. Furthermore, we analyze the motivations, innovations, pros and\ncons, and the performance of representative approaches for comparison. Finally,\nfuture challenges and method trends of occluded face recognition are thoroughly\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 20:44:02 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zeng", "Dan", ""], ["Veldhuis", "Raymond", ""], ["Spreeuwers", "Luuk", ""]]}, {"id": "2006.11371", "submitter": "Arun Das", "authors": "Arun Das and Paul Rad", "title": "Opportunities and Challenges in Explainable Artificial Intelligence\n  (XAI): A Survey", "comments": "24 pages, 20 figures, survey paper, submitting to IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, deep neural networks are widely used in mission critical systems\nsuch as healthcare, self-driving vehicles, and military which have direct\nimpact on human lives. However, the black-box nature of deep neural networks\nchallenges its use in mission critical applications, raising ethical and\njudicial concerns inducing lack of trust. Explainable Artificial Intelligence\n(XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools,\ntechniques, and algorithms that can generate high-quality interpretable,\nintuitive, human-understandable explanations of AI decisions. In addition to\nproviding a holistic view of the current XAI landscape in deep learning, this\npaper provides mathematical summaries of seminal work. We start by proposing a\ntaxonomy and categorizing the XAI techniques based on their scope of\nexplanations, methodology behind the algorithms, and explanation level or usage\nwhich helps build trustworthy, interpretable, and self-explanatory deep\nlearning models. We then describe the main principles used in XAI research and\npresent the historical timeline for landmark studies in XAI from 2007 to 2020.\nAfter explaining each category of algorithms and approaches in detail, we then\nevaluate the explanation maps generated by eight XAI algorithms on image data,\ndiscuss the limitations of this approach, and provide potential future\ndirections to improve XAI evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 02:58:10 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 01:48:56 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Das", "Arun", ""], ["Rad", "Paul", ""]]}, {"id": "2006.11372", "submitter": "Eyke H\\\"ullermeier", "authors": "Javad Rahnama and Eyke H\\\"ullermeier", "title": "Learning Tversky Similarity", "comments": null, "journal-ref": "Proc. IPMU, International Conference on Information Processing and\n  Management of Uncertainty in Knowledge-Based Systems, Springer, CCIS 1238,\n  2020", "doi": "10.1007/978-3-030-50143-3_21", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we advocate Tversky's ratio model as an appropriate basis for\ncomputational approaches to semantic similarity, that is, the comparison of\nobjects such as images in a semantically meaningful way. We consider the\nproblem of learning Tversky similarity measures from suitable training data\nindicating whether two objects tend to be similar or dissimilar.\nExperimentally, we evaluate our approach to similarity learning on two image\ndatasets, showing that is performs very well compared to existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 07:58:35 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Rahnama", "Javad", ""], ["H\u00fcllermeier", "Eyke", ""]]}, {"id": "2006.11373", "submitter": "Jimut Bahan Pal Mr.", "authors": "Jimut Bahan Pal", "title": "Deceiving computers in Reverse Turing Test through Deep Learning", "comments": "Masters thesis. All Text CAPTCHAs are broken with over 99% accuracy,\n  hence they are proved to be unreliable", "journal-ref": null, "doi": null, "report-no": "B1930050", "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is increasingly becoming difficult for human beings to work on their day\nto day life without going through the process of reverse Turing test, where the\nComputers tests the users to be humans or not. Almost every website and service\nproviders today have the process of checking whether their website is being\ncrawled or not by automated bots which could extract valuable information from\ntheir site. In the process the bots are getting more intelligent by the use of\nDeep Learning techniques to decipher those tests and gain unwanted automated\naccess to data while create nuisance by posting spam. Humans spend a\nconsiderable amount of time almost every day when trying to decipher CAPTCHAs.\nThe aim of this investigation is to check whether the use of a subset of\ncommonly used CAPTCHAs, known as the text CAPTCHA is a reliable process for\nverifying their human customers. We mainly focused on the preprocessing step\nfor every CAPTCHA which converts them in binary intensity and removes the\nconfusion as much as possible and developed various models to correctly label\nas many CAPTCHAs as possible. We also suggested some ways to improve the\nprocess of verifying the humans which makes it easy for humans to solve the\nexisting CAPTCHAs and difficult for bots to do the same.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 10:11:42 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Pal", "Jimut Bahan", ""]]}, {"id": "2006.11374", "submitter": "Venkat Margapuri", "authors": "Venkat Margapuri, George Lavezzi, Robert Stewart, Dan Wagner", "title": "Bombus Species Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entomologists, ecologists and others struggle to rapidly and accurately\nidentify the species of bumble bees they encounter in their field work and\nresearch. The current process requires the bees to be mounted, then physically\nshipped to a taxonomic expert for proper categorization. We investigated\nwhether an image classification system derived from transfer learning can do\nthis task. We used Google Inception, Oxford VGG16 and VGG19 and Microsoft\nResNet 50. We found Inception and VGG classifiers were able to make some\nprogress at identifying bumble bee species from the available data, whereas\nResNet was not. Individual classifiers achieved accuracies of up to 23% for\nsingle species identification and 44% top-3 labels, where a composite model\nperformed better, 27% and 50%. We feel the performance was most hampered by our\nlimited data set of 5,000-plus labeled images of 29 species, with individual\nspecies represented by 59 -315 images.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 21:28:32 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Margapuri", "Venkat", ""], ["Lavezzi", "George", ""], ["Stewart", "Robert", ""], ["Wagner", "Dan", ""]]}, {"id": "2006.11375", "submitter": "Hassler Castro A.B", "authors": "Hassler Castro and Mariana Ramirez", "title": "Segmentation task for fashion and apparel", "comments": "8 pages, 7 figures. To appear as one of the projects associated with\n  the advance machine learning class at Universidad EAFIT, June 2019. Medellin,\n  Colombia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Fashion Industry is a strong and important industry in the global\neconomy. Globalization has brought fast fashion, quick shifting consumer\nshopping preferences, more competition, and abundance in fashion shops and\nretailers, making it more difficult for professionals in the fashion industry\nto keep track of what fashion items people wear and how they combine them. This\npaper solves this problem by implementing several Deep Learning Architectures\nusing the iMaterialist dataset consisting of 45,000 images with 46 different\nclothing and apparel categories.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 03:44:58 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Castro", "Hassler", ""], ["Ramirez", "Mariana", ""]]}, {"id": "2006.11376", "submitter": "Haoliang Jiang", "authors": "Haoliang Jiang, Zhenguo Nie, Roselyn Yeo, Amir Barati Farimani, Levent\n  Burak Kara", "title": "StressGAN: A Generative Deep Learning Model for 2D Stress Distribution\n  Prediction", "comments": null, "journal-ref": null, "doi": "10.1115/1.4049805", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using deep learning to analyze mechanical stress distributions has been\ngaining interest with the demand for fast stress analysis methods. Deep\nlearning approaches have achieved excellent outcomes when utilized to speed up\nstress computation and learn the physics without prior knowledge of underlying\nequations. However, most studies restrict the variation of geometry or boundary\nconditions, making these methods difficult to be generalized to unseen\nconfigurations. We propose a conditional generative adversarial network (cGAN)\nmodel for predicting 2D von Mises stress distributions in solid structures. The\ncGAN learns to generate stress distributions conditioned by geometries, load,\nand boundary conditions through a two-player minimax game between two neural\nnetworks with no prior knowledge. By evaluating the generative network on two\nstress distribution datasets under multiple metrics, we demonstrate that our\nmodel can predict more accurate high-resolution stress distributions than a\nbaseline convolutional neural network model, given various and complex cases of\ngeometry, load and boundary conditions.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 00:28:21 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Jiang", "Haoliang", ""], ["Nie", "Zhenguo", ""], ["Yeo", "Roselyn", ""], ["Farimani", "Amir Barati", ""], ["Kara", "Levent Burak", ""]]}, {"id": "2006.11379", "submitter": "Anuraag Kaashyap", "authors": "Kirthi Kumar and Anuraag Kaashyap", "title": "Improving Train Track Safety using Drones, Computer Vision and Machine\n  Learning", "comments": "27 pages and 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of human casualties resulting from train accidents globally are\ncaused by the inefficient, manual track inspections. Government agencies are\nseriously concerned about the safe operations of the rail industry after series\nof accidents reported across e USA and around the globe, mainly attributed to\ntrack defects. Casualties resulting from track defects result in billions of\ndollars loss in public and private investments and loss of revenue due to\ndowntime, ultimately resulting in loss of the public's confidence. The manual,\nmundane, and expensive monitoring of rail track safety can be transform through\nthe use of drones, computer vision, and machine learning. The primary goal of\nthis study is to develop multiple algorithms that implement supervised and\nsemi-supervised learning that accurately analyze whether a track is safe or\nunsafe based on simulated training data of train tracks. This includes being\nable to develop a Convolutional Neural Network that can identify track defects\nusing supervised learning without having to specify a particular algorithm for\ndetecting those defects, and that the new model would both speed up and improve\nthe quality of the track defect detection process, accompanied with a computer\nvision image-processing algorithm. Our other goals included designing and\nbuilding a prototype representation of train tracks to simulate track defects,\nto precisely and consistently conduct the visual inspection using drones.\nUltimately, the goal demonstrates that the state of good repairs in railway\ntracks can be attained through the use of drones, computer vision and machine\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 23:17:23 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kumar", "Kirthi", ""], ["Kaashyap", "Anuraag", ""]]}, {"id": "2006.11384", "submitter": "Yuhong Guo", "authors": "Jianan Jiang, Zhenpeng Li, Yuhong Guo, Jieping Ye", "title": "A Transductive Multi-Head Model for Cross-Domain Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new method, Transductive Multi-Head Few-Shot\nlearning (TMHFS), to address the Cross-Domain Few-Shot Learning (CD-FSL)\nchallenge. The TMHFS method extends the Meta-Confidence Transduction (MCT) and\nDense Feature-Matching Networks (DFMN) method [2] by introducing a new\nprediction head, i.e, an instance-wise global classification network based on\nsemantic information, after the common feature embedding network. We train the\nembedding network with the multiple heads, i.e,, the MCT loss, the DFMN loss\nand the semantic classifier loss, simultaneously in the source domain. For the\nfew-shot learning in the target domain, we first perform fine-tuning on the\nembedding network with only the semantic global classifier and the support\ninstances, and then use the MCT part to predict labels of the query set with\nthe fine-tuned embedding network. Moreover, we further exploit data\naugmentation techniques during the fine-tuning and test stages to improve the\nprediction performance. The experimental results demonstrate that the proposed\nmethods greatly outperform the strong baseline, fine-tuning, on four different\ntarget domains.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 02:39:59 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Jiang", "Jianan", ""], ["Li", "Zhenpeng", ""], ["Guo", "Yuhong", ""], ["Ye", "Jieping", ""]]}, {"id": "2006.11385", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Fakhri Karray, Mark Crowley", "title": "Quantile-Quantile Embedding for Distribution Transformation and Manifold\n  Embedding with Ability to Choose the Embedding Distribution", "comments": "Published in Machine Learning with Applications, Elsevier, Volume 6,\n  Pages 100088, 2021", "journal-ref": "Machine Learning with Applications, Elsevier, Volume 6, Pages\n  100088, 2021", "doi": "10.1016/j.mlwa.2021.100088", "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new embedding method, named Quantile-Quantile Embedding (QQE),\nfor distribution transformation and manifold embedding with the ability to\nchoose the embedding distribution. QQE, which uses the concept of\nquantile-quantile plot from visual statistical tests, can transform the\ndistribution of data to any theoretical desired distribution or empirical\nreference sample. Moreover, QQE gives the user a choice of embedding\ndistribution in embedding the manifold of data into the low dimensional\nembedding space. It can also be used for modifying the embedding distribution\nof other dimensionality reduction methods, such as PCA, t-SNE, and deep metric\nlearning, for better representation or visualization of data. We propose QQE in\nboth unsupervised and supervised forms. QQE can also transform a distribution\nto either an exact reference distribution or its shape. We show that QQE allows\nfor better discrimination of classes in some cases. Our experiments on\ndifferent synthetic and image datasets show the effectiveness of the proposed\nembedding method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 21:09:09 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 03:06:05 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2006.11389", "submitter": "Sergey Tarasenko", "authors": "Sergey Tarasenko", "title": "Sparsifying and Down-scaling Networks to Increase Robustness to\n  Distortions", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that perfectly trained networks exhibit drastic reduction\nin performance when presented with distorted images. Streaming Network (STNet)\nis a novel architecture capable of robust classification of the distorted\nimages while been trained on undistorted images. The distortion robustness is\nenabled by means of sparse input and isolated parallel streams with decoupled\nweights. Recent results prove STNet is robust to 20 types of noise and\ndistortions. STNet exhibits state-of-the-art performance for classification of\nlow light images, while being of much smaller size when other networks. In this\npaper, we construct STNets by using scaled versions (number of filters in each\nlayer is reduced by factor of n) of popular networks like VGG16, ResNet50 and\nMobileNetV2 as parallel streams. These new STNets are tested on several\ndatasets. Our results indicate that more efficient (less FLOPS), new STNets\nexhibit higher or equal accuracy in comparison with original networks.\nConsidering a diversity of datasets and networks used for tests, we conclude\nthat a new type of STNets is an efficient tool for robust classification of\ndistorted images.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 03:58:27 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Tarasenko", "Sergey", ""]]}, {"id": "2006.11391", "submitter": "Akshay L Chandra", "authors": "Akshay L Chandra, Sai Vikas Desai, Wei Guo, Vineeth N Balasubramanian", "title": "Computer Vision with Deep Learning for Plant Phenotyping in Agriculture:\n  A Survey", "comments": "Featured as an article at Journal of Advanced Computing and\n  Communications, April 2020. arXiv admin note: text overlap with\n  arXiv:1805.00881 by other authors", "journal-ref": null, "doi": "10.34048/ACC.2020.1.F1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In light of growing challenges in agriculture with ever growing food demand\nacross the world, efficient crop management techniques are necessary to\nincrease crop yield. Precision agriculture techniques allow the stakeholders to\nmake effective and customized crop management decisions based on data gathered\nfrom monitoring crop environments. Plant phenotyping techniques play a major\nrole in accurate crop monitoring. Advancements in deep learning have made\npreviously difficult phenotyping tasks possible. This survey aims to introduce\nthe reader to the state of the art research in deep plant phenotyping.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 14:21:19 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Chandra", "Akshay L", ""], ["Desai", "Sai Vikas", ""], ["Guo", "Wei", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2006.11392", "submitter": "Huazhu Fu", "authors": "Deng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng Chen, Huazhu Fu, Jianbing\n  Shen, Ling Shao", "title": "PraNet: Parallel Reverse Attention Network for Polyp Segmentation", "comments": "Accepted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colonoscopy is an effective technique for detecting colorectal polyps, which\nare highly related to colorectal cancer. In clinical practice, segmenting\npolyps from colonoscopy images is of great importance since it provides\nvaluable information for diagnosis and surgery. However, accurate polyp\nsegmentation is a challenging task, for two major reasons: (i) the same type of\npolyps has a diversity of size, color and texture; and (ii) the boundary\nbetween a polyp and its surrounding mucosa is not sharp. To address these\nchallenges, we propose a parallel reverse attention network (PraNet) for\naccurate polyp segmentation in colonoscopy images. Specifically, we first\naggregate the features in high-level layers using a parallel partial decoder\n(PPD). Based on the combined feature, we then generate a global map as the\ninitial guidance area for the following components. In addition, we mine the\nboundary cues using a reverse attention (RA) module, which is able to establish\nthe relationship between areas and boundary cues. Thanks to the recurrent\ncooperation mechanism between areas and boundaries, our PraNet is capable of\ncalibrating any misaligned predictions, improving the segmentation accuracy.\nQuantitative and qualitative evaluations on five challenging datasets across\nsix metrics show that our PraNet improves the segmentation accuracy\nsignificantly, and presents a number of advantages in terms of\ngeneralizability, and real-time segmentation efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 08:13:43 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 16:39:51 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 10:30:19 GMT"}, {"version": "v4", "created": "Fri, 3 Jul 2020 13:14:44 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Fan", "Deng-Ping", ""], ["Ji", "Ge-Peng", ""], ["Zhou", "Tao", ""], ["Chen", "Geng", ""], ["Fu", "Huazhu", ""], ["Shen", "Jianbing", ""], ["Shao", "Ling", ""]]}, {"id": "2006.11393", "submitter": "Tyler Scott", "authors": "Tyler R. Scott, Michael Shvartsman and Karl Ridgeway", "title": "Unifying Few- and Zero-Shot Egocentric Action Recognition", "comments": "Accepted for presentation at the EPIC@CVPR2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there has been significant research in egocentric action\nrecognition, most methods and tasks, including EPIC-KITCHENS, suppose a fixed\nset of action classes. Fixed-set classification is useful for benchmarking\nmethods, but is often unrealistic in practical settings due to the\ncompositionality of actions, resulting in a functionally infinite-cardinality\nlabel set. In this work, we explore generalization with an open set of classes\nby unifying two popular approaches: few- and zero-shot generalization (the\nlatter which we reframe as cross-modal few-shot generalization). We propose a\nnew set of splits derived from the EPIC-KITCHENS dataset that allow evaluation\nof open-set classification, and use these splits to show that adding a\nmetric-learning loss to the conventional direct-alignment baseline can improve\nzero-shot classification by as much as 10%, while not sacrificing few-shot\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 02:23:38 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Scott", "Tyler R.", ""], ["Shvartsman", "Michael", ""], ["Ridgeway", "Karl", ""]]}, {"id": "2006.11395", "submitter": "Chen Delong", "authors": "Delong Chen, Fan Liu, Zewen Li", "title": "Deep Learning Based Single Sample Per Person Face Recognition: A Survey", "comments": "under review of ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition has been an active research area in the field of pattern\nrecognition, especially since the rise of deep learning in recent years.\nHowever, in some practical situations, each identity in the training set has\nonly a single sample. This type of situation is called Single Sample Per Person\n(SSPP), which brings a great challenge to the effective training of deep\nmodels. To resolve this issue, and to unleash the full potential of deep\nlearning, many deep learning based SSPP face recognition methods have been\nproposed in recent years. There have been several comprehensive surveys for\ntraditional methods based SSPP face recognition approaches, but emerging deep\nlearning based methods are rarely involved. In this paper, we focus on those\ndeep methods, classifying them as virtual sample methods and generic learning\nmethods. In virtual sample methods, virtual face images or virtual face\nfeatures are generated to benefit the training of the deep model. In generic\nlearning methods, additional multi-sample generic set are used. Efforts of\ntraditional methods and deep feature combining, loss function improving and\nnetwork structure improving are involved in our analysis in the generic\nlearning methods section. Finally, we discuss existing problems of identity\ninformation retention in virtual sample methods and domain adaption in generic\nlearning methods. Further, we regard the semantic gap as an important future\nissue that needs to be considered in deep SSPP methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 13:24:27 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Chen", "Delong", ""], ["Liu", "Fan", ""], ["Li", "Zewen", ""]]}, {"id": "2006.11397", "submitter": "Anjan Dutta", "authors": "Anjan Dutta and Zeynep Akata", "title": "Semantically Tied Paired Cycle Consistency for Any-Shot Sketch-based\n  Image Retrieval", "comments": "In International Journal of Computer Vision (IJCV) 2020 (17 pages, 12\n  figures, 5 tables). arXiv admin note: substantial text overlap with\n  arXiv:1903.03372", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-shot sketch-based image retrieval is an emerging task in computer vision,\nallowing to retrieve natural images relevant to hand-drawn sketch queries that\nare rarely seen during the training phase. Related prior works either require\naligned sketch-image pairs that are costly to obtain or inefficient memory\nfusion layer for mapping the visual information to a semantic space. In this\npaper, we address any-shot, i.e. zero-shot and few-shot, sketch-based image\nretrieval (SBIR) tasks, where we introduce the few-shot setting for SBIR. For\nsolving these tasks, we propose a semantically aligned paired cycle-consistent\ngenerative adversarial network (SEM-PCYC) for any-shot SBIR, where each branch\nof the generative adversarial network maps the visual information from sketch\nand image to a common semantic space via adversarial training. Each of these\nbranches maintains cycle consistency that only requires supervision at the\ncategory level, and avoids the need of aligned sketch-image pairs. A\nclassification criteria on the generators' outputs ensures the visual to\nsemantic space mapping to be class-specific. Furthermore, we propose to combine\ntextual and hierarchical side information via an auto-encoder that selects\ndiscriminating side information within a same end-to-end model. Our results\ndemonstrate a significant boost in any-shot SBIR performance over the\nstate-of-the-art on the extended version of the challenging Sketchy, TU-Berlin\nand QuickDraw datasets.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 22:43:53 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Dutta", "Anjan", ""], ["Akata", "Zeynep", ""]]}, {"id": "2006.11403", "submitter": "Soroush Vosoughi Dr", "authors": "Lili Wang, Ruibo Liu, and Soroush Vosoughi", "title": "Salienteye: Maximizing Engagement While Maintaining Artistic Style on\n  Instagram Using Deep Neural Networks", "comments": "Proceedings of the 2020 International Conference on Multimedia\n  Retrieval. 2020", "journal-ref": null, "doi": "10.1145/3372278.3390736", "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instagram has become a great venue for amateur and professional photographers\nalike to showcase their work. It has, in other words, democratized photography.\nGenerally, photographers take thousands of photos in a session, from which they\npick a few to showcase their work on Instagram. Photographers trying to build a\nreputation on Instagram have to strike a balance between maximizing their\nfollowers' engagement with their photos, while also maintaining their artistic\nstyle. We used transfer learning to adapt Xception, which is a model for object\nrecognition trained on the ImageNet dataset, to the task of engagement\nprediction and utilized Gram matrices generated from VGG19, another object\nrecognition model trained on ImageNet, for the task of style similarity\nmeasurement on photos posted on Instagram. Our models can be trained on\nindividual Instagram accounts to create personalized engagement prediction and\nstyle similarity models. Once trained on their accounts, users can have new\nphotos sorted based on predicted engagement and style similarity to their\nprevious work, thus enabling them to upload photos that not only have the\npotential to maximize engagement from their followers but also maintain their\nstyle of photography. We trained and validated our models on several Instagram\naccounts, showing it to be adept at both tasks, also outperforming several\nbaseline models and human annotators.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 01:58:02 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Wang", "Lili", ""], ["Liu", "Ruibo", ""], ["Vosoughi", "Soroush", ""]]}, {"id": "2006.11404", "submitter": "Safalya Pal", "authors": "Safalya Pal", "title": "Auto-Encoding for Shared Cross Domain Feature Representation and\n  Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation is a subset of computer vision and pattern\nrecognition problems where our goal is to learn a mapping between input images\nof domain $\\mathbf{X}_1$ and output images of domain $\\mathbf{X}_2$. Current\nmethods use neural networks with an encoder-decoder structure to learn a\nmapping $G:\\mathbf{X}_1 \\to\\mathbf{X}_2$ such that the distribution of images\nfrom $\\mathbf{X}_2$ and $G(\\mathbf{X}_1)$ are identical, where $G(\\mathbf{X}_1)\n= d_G (f_G (\\mathbf{X}_1))$ and $f_G (\\cdot)$ is referred as the encoder and\n$d_G(\\cdot)$ is referred to as the decoder. Currently, such methods which also\ncompute an inverse mapping $F:\\mathbf{X}_2 \\to \\mathbf{X}_1$ use a separate\nencoder-decoder pair $d_F (f_F (\\mathbf{X}_2))$ or at least a separate decoder\n$d_F (\\cdot)$ to do so. Here we introduce a method to perform cross domain\nimage-to-image translation across multiple domains using a single\nencoder-decoder architecture. We use an auto-encoder network which given an\ninput image $\\mathbf{X}_1$, first computes a latent domain encoding $Z_d = f_d\n(\\mathbf{X}_1)$ and a latent content encoding $Z_c = f_c (\\mathbf{X}_1)$, where\nthe domain encoding $Z_d$ and content encoding $Z_c$ are independent. And then\na decoder network $g(Z_d,Z_c)$ creates a reconstruction of the original image\n$\\mathbf{\\widehat{X}}_1=g(Z_d,Z_c )\\approx \\mathbf{X}_1$. Ideally, the domain\nencoding $Z_d$ contains no information regarding the content of the image and\nthe content encoding $Z_c$ contains no information regarding the domain of the\nimage. We use this property of the encodings to find the mapping across domains\n$G: X\\to Y$ by simply changing the domain encoding $Z_d$ of the decoder's\ninput. $G(\\mathbf{X}_1 )=d(f_d (\\mathbf{x}_2^i ),f_c (\\mathbf{X}_1))$ where\n$\\mathbf{x}_2^i$ is the $i^{th}$ observation of $\\mathbf{X}_2$.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 21:38:23 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Pal", "Safalya", ""]]}, {"id": "2006.11405", "submitter": "Chongyang Bai", "authors": "Chongyang Bai, Haipeng Chen, Srijan Kumar, Jure Leskovec, V.S.\n  Subrahmanian", "title": "M2P2: Multimodal Persuasion Prediction using Adaptive Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying persuasive speakers in an adversarial environment is a critical\ntask. In a national election, politicians would like to have persuasive\nspeakers campaign on their behalf. When a company faces adverse publicity, they\nwould like to engage persuasive advocates for their position in the presence of\nadversaries who are critical of them. Debates represent a common platform for\nthese forms of adversarial persuasion. This paper solves two problems: the\nDebate Outcome Prediction (DOP) problem predicts who wins a debate while the\nIntensity of Persuasion Prediction (IPP) problem predicts the change in the\nnumber of votes before and after a speaker speaks. Though DOP has been\npreviously studied, we are the first to study IPP. Past studies on DOP fail to\nleverage two important aspects of multimodal data: 1) multiple modalities are\noften semantically aligned, and 2) different modalities may provide diverse\ninformation for prediction. Our M2P2 (Multimodal Persuasion Prediction)\nframework is the first to use multimodal (acoustic, visual, language) data to\nsolve the IPP problem. To leverage the alignment of different modalities while\nmaintaining the diversity of the cues they provide, M2P2 devises a novel\nadaptive fusion learning framework which fuses embeddings obtained from two\nmodules -- an alignment module that extracts shared information between\nmodalities and a heterogeneity module that learns the weights of different\nmodalities with guidance from three separately trained unimodal reference\nmodels. We test M2P2 on the popular IQ2US dataset designed for DOP. We also\nintroduce a new dataset called QPS (from Qipashuo, a popular Chinese debate TV\nshow ) for IPP. M2P2 significantly outperforms 3 recent baselines on both\ndatasets. Our code and QPS dataset can be found at\nhttp://snap.stanford.edu/m2p2/.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 18:47:24 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Bai", "Chongyang", ""], ["Chen", "Haipeng", ""], ["Kumar", "Srijan", ""], ["Leskovec", "Jure", ""], ["Subrahmanian", "V. S.", ""]]}, {"id": "2006.11406", "submitter": "Jan-Peter Kucklick", "authors": "Jan-Peter Kucklick and Oliver M\\\"uller", "title": "Location, location, location: Satellite image-based real-estate\n  appraisal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Buying a home is one of the most important buying decisions people have to\nmake in their life. The latest research on real-estate appraisal focuses on\nincorporating image data in addition to structured data into the modeling\nprocess. This research measures the prediction performance of satellite images\nand structured data by using convolutional neural networks. The resulting CNN\nmodel trained performs 7% better in MAE than the advanced baseline of a neural\nnetwork trained on structured data. Moreover, sliding-window heatmap provides\nvisual interpretability of satellite images, revealing that neighborhood\nstructures are essential in the price estimation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 07:25:02 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kucklick", "Jan-Peter", ""], ["M\u00fcller", "Oliver", ""]]}, {"id": "2006.11407", "submitter": "Mahdi Elhousni", "authors": "Mahdi Elhousni and Xinming Huang", "title": "Pedestrian Tracking with Gated Recurrent Units and Attention Mechanisms", "comments": "Accepted by ISCAS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian tracking has long been considered an important problem, especially\nin security applications. Previously,many approaches have been proposed with\nvarious types of sensors. One popular method is Pedestrian Dead Reckoning(PDR)\n[1] which is based on the inertial measurement unit(IMU) sensor. However PDR is\nan integration and threshold based method, which suffers from accumulation\nerrors and low accuracy. In this paper, we propose a novel method in which the\nsensor data is fed into a deep learning model to predict the displacements and\norientations of the pedestrian. We also devise a new apparatus to collect and\nconstruct databases containing synchronized IMU sensor data and precise\nlocations measured by a LIDAR. The preliminary results are promising, and we\nplan to push this forward by collecting more data and adapting the deep\nlearning model for all general pedestrian motions.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 23:58:43 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Elhousni", "Mahdi", ""], ["Huang", "Xinming", ""]]}, {"id": "2006.11408", "submitter": "Hei Long Chan", "authors": "Hei-Long Chan, Hoi-Man Yuen, Chun-Ting Au, Kate Ching-Ching Chan,\n  Albert Martin Li, Lok-Ming Lui", "title": "Quasi-conformal Geometry based Local Deformation Analysis of Lateral\n  Cephalogram for Childhood OSA Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Craniofacial profile is one of the anatomical causes of obstructive sleep\napnea(OSA). By medical research, cephalometry provides information on patients'\nskeletal structures and soft tissues. In this work, a novel approach to\ncephalometric analysis using quasi-conformal geometry based local deformation\ninformation was proposed for OSA classification. Our study was a retrospective\nanalysis based on 60 case-control pairs with accessible lateral cephalometry\nand polysomnography (PSG) data. By using the quasi-conformal geometry to study\nthe local deformation around 15 landmark points, and combining the results with\nthree linear distances between landmark points, a total of 1218 information\nfeatures were obtained per subject. A L2 norm based classification model was\nbuilt. Under experiments, our proposed model achieves 92.5% testing accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 04:14:38 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Chan", "Hei-Long", ""], ["Yuen", "Hoi-Man", ""], ["Au", "Chun-Ting", ""], ["Chan", "Kate Ching-Ching", ""], ["Li", "Albert Martin", ""], ["Lui", "Lok-Ming", ""]]}, {"id": "2006.11412", "submitter": "Benjamin R. Cowley", "authors": "Benjamin R. Cowley, Jonathan W. Pillow", "title": "High-contrast \"gaudy\" images improve the training of deep neural network\n  models of visual cortex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in understanding the sensory transformations of the visual\nsystem is to obtain a highly predictive model of responses from visual cortical\nneurons. Deep neural networks (DNNs) provide a promising candidate for such a\nmodel. However, DNNs require orders of magnitude more training data than\nneuroscientists can collect from real neurons because experimental recording\ntime is severely limited. This motivates us to find images that train\nhighly-predictive DNNs with as little training data as possible. We propose\ngaudy images---high-contrast binarized versions of natural images---to\nefficiently train DNNs. In extensive simulation experiments, we find that\ntraining DNNs with gaudy images substantially reduces the number of training\nimages needed to accurately predict the simulated responses of visual cortical\nneurons. We also find that gaudy images, chosen before training, outperform\nimages chosen during training by active learning algorithms. Thus, gaudy images\noveremphasize features of natural images, especially edges, that are the most\nimportant for efficiently training DNNs. We believe gaudy images will aid in\nthe modeling of visual cortical neurons, potentially opening new scientific\nquestions about visual processing, as well as aid general practitioners that\nseek ways to improve the training of DNNs.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 20:05:16 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Cowley", "Benjamin R.", ""], ["Pillow", "Jonathan W.", ""]]}, {"id": "2006.11413", "submitter": "Feng Qi", "authors": "Feng Qi, Guanjun Jiang", "title": "Visualizing and Understanding Vision System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How the human vision system addresses the object identity-preserving\nrecognition problem is largely unknown. Here, we use a vision\nrecognition-reconstruction network (RRN) to investigate the development,\nrecognition, learning and forgetting mechanisms, and achieve similar\ncharacteristics to electrophysiological measurements in monkeys. First, in\nnetwork development study, the RRN also experiences critical developmental\nstages characterized by specificities in neuron types, synapse and activation\npatterns, and visual task performance from the early stage of coarse salience\nmap recognition to mature stage of fine structure recognition. In digit\nrecognition study, we witness that the RRN could maintain object invariance\nrepresentation under various viewing conditions by coordinated adjustment of\nresponses of population neurons. And such concerted population responses\ncontained untangled object identity and properties information that could be\naccurately extracted via high-level cortices or even a simple weighted\nsummation decoder. In the learning and forgetting study, novel structure\nrecognition is implemented by adjusting entire synapses in low magnitude while\npattern specificities of original synaptic connectivity are preserved, which\nguaranteed a learning process without disrupting the existing functionalities.\nThis work benefits the understanding of the human visual processing mechanism\nand the development of human-like machine intelligence.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 07:08:49 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Qi", "Feng", ""], ["Jiang", "Guanjun", ""]]}, {"id": "2006.11416", "submitter": "S V Aruna Kumar", "authors": "S V Aruna Kumar, Ehsan Yaghoubi and Hugo Proen\\c{c}a", "title": "A Symbolic Temporal Pooling method for Video-based Person\n  Re-Identification", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video-based person re-identification, both the spatial and temporal\nfeatures are known to provide orthogonal cues to effective representations.\nSuch representations are currently typically obtained by aggregating the\nframe-level features using max/avg pooling, at different points of the models.\nHowever, such operations also decrease the amount of discriminating information\navailable, which is particularly hazardous in case of poor separability between\nthe different classes. To alleviate this problem, this paper introduces a\nsymbolic temporal pooling method, where frame-level features are represented in\nthe distribution valued symbolic form, yielding from fitting an Empirical\nCumulative Distribution Function (ECDF) to each feature. Also, considering that\nthe original triplet loss formulation cannot be applied directly to this kind\nof representations, we introduce a symbolic triplet loss function that infers\nthe similarity between two symbolic objects. Having carried out an extensive\nempirical evaluation of the proposed solution against the state-of-the-art, in\nfour well known data sets (MARS, iLIDS-VID, PRID2011 and P-DESTRE), the\nobserved results point for consistent improvements in performance over the\nprevious best performing techniques.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 21:52:33 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kumar", "S V Aruna", ""], ["Yaghoubi", "Ehsan", ""], ["Proen\u00e7a", "Hugo", ""]]}, {"id": "2006.11418", "submitter": "Renato J Cintra", "authors": "D. R. Canterle, T. L. T. da Silveira, F. M. Bayer, R. J. Cintra", "title": "A Multiparametric Class of Low-complexity Transforms for Image and Video\n  Coding", "comments": "Fixed Figure 1 and typos in the reference list", "journal-ref": "Signal Processing, Volume 176, November 2020", "doi": "10.1016/j.sigpro.2020.107685", "report-no": null, "categories": "eess.SP cs.CV cs.MM eess.IV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete transforms play an important role in many signal processing\napplications, and low-complexity alternatives for classical transforms became\npopular in recent years. Particularly, the discrete cosine transform (DCT) has\nproven to be convenient for data compression, being employed in well-known\nimage and video coding standards such as JPEG, H.264, and the recent high\nefficiency video coding (HEVC). In this paper, we introduce a new class of\nlow-complexity 8-point DCT approximations based on a series of works published\nby Bouguezel, Ahmed and Swamy. Also, a multiparametric fast algorithm that\nencompasses both known and novel transforms is derived. We select the\nbest-performing DCT approximations after solving a multicriteria optimization\nproblem, and submit them to a scaling method for obtaining larger size\ntransforms. We assess these DCT approximations in both JPEG-like image\ncompression and video coding experiments. We show that the optimal DCT\napproximations present compelling results in terms of coding efficiency and\nimage quality metrics, and require only few addition or bit-shifting\noperations, being suitable for low-complexity and low-power systems.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 21:56:58 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Canterle", "D. R.", ""], ["da Silveira", "T. L. T.", ""], ["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""]]}, {"id": "2006.11424", "submitter": "Pavan Madhusudana", "authors": "Pavan C. Madhusudana, Neil Birkbeck, Yilin Wang, Balu Adsumilli, Alan\n  C. Bovik", "title": "Capturing Video Frame Rate Variations via Entropic Differencing", "comments": null, "journal-ref": "IEEE Signal Processing Letters. 27 (2020) 1809-1813", "doi": "10.1109/LSP.2020.3028687", "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High frame rate videos are increasingly getting popular in recent years,\ndriven by the strong requirements of the entertainment and streaming industries\nto provide high quality of experiences to consumers. To achieve the best\ntrade-offs between the bandwidth requirements and video quality in terms of\nframe rate adaptation, it is imperative to understand the effects of frame rate\non video quality. In this direction, we devise a novel statistical entropic\ndifferencing method based on a Generalized Gaussian Distribution model\nexpressed in the spatial and temporal band-pass domains, which measures the\ndifference in quality between reference and distorted videos. The proposed\ndesign is highly generalizable and can be employed when the reference and\ndistorted sequences have different frame rates. Our proposed model correlates\nvery well with subjective scores in the recently proposed LIVE-YT-HFR database\nand achieves state of the art performance when compared with existing\nmethodologies.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 22:16:52 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 01:02:00 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Madhusudana", "Pavan C.", ""], ["Birkbeck", "Neil", ""], ["Wang", "Yilin", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2006.11436", "submitter": "Mong Ng", "authors": "Mong H. Ng, Kaahan Radia, Jianfei Chen, Dequan Wang, Ionel Gog, and\n  Joseph E. Gonzalez", "title": "BEV-Seg: Bird's Eye View Semantic Segmentation Using Geometry and\n  Semantic Point Cloud", "comments": "Accepted into CVPR 2020 Workshop Scalability in Autonomous Driving by\n  Waymo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bird's-eye-view (BEV) is a powerful and widely adopted representation for\nroad scenes that captures surrounding objects and their spatial locations,\nalong with overall context in the scene. In this work, we focus on bird's eye\nsemantic segmentation, a task that predicts pixel-wise semantic segmentation in\nBEV from side RGB images. This task is made possible by simulators such as\nCarla, which allow for cheap data collection, arbitrary camera placements, and\nsupervision in ways otherwise not possible in the real world. There are two\nmain challenges to this task: the view transformation from side view to bird's\neye view, as well as transfer learning to unseen domains. Existing work\ntransforms between views through fully connected layers and transfer learns via\nGANs. This suffers from a lack of depth reasoning and performance degradation\nacross domains. Our novel 2-staged perception pipeline explicitly predicts\npixel depths and combines them with pixel semantics in an efficient manner,\nallowing the model to leverage depth information to infer objects' spatial\nlocations in the BEV. In addition, we transfer learning by abstracting\nhigh-level geometric features and predicting an intermediate representation\nthat is common across different domains. We publish a new dataset called\nBEVSEG-Carla and show that our approach improves state-of-the-art by 24% mIoU\nand performs well when transferred to a new domain.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 23:30:11 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 16:45:07 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Ng", "Mong H.", ""], ["Radia", "Kaahan", ""], ["Chen", "Jianfei", ""], ["Wang", "Dequan", ""], ["Gog", "Ionel", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "2006.11453", "submitter": "Jackson Shields", "authors": "Jackson Shields, Oscar Pizarro, Stefan B. Williams", "title": "Towards Adaptive Benthic Habitat Mapping", "comments": "To be published in ICRA2020 conference proceedings. 6 pages, 7\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous Underwater Vehicles (AUVs) are increasingly being used to support\nscientific research and monitoring studies. One such application is in benthic\nhabitat mapping where these vehicles collect seafloor imagery that complements\nbroadscale bathymetric data collected using sonar. Using these two data\nsources, the relationship between remotely-sensed acoustic data and the sampled\nimagery can be learned, creating a habitat model. As the areas to be mapped are\noften very large and AUV systems collecting seafloor imagery can only sample\nfrom a small portion of the survey area, the information gathered should be\nmaximised for each deployment. This paper illustrates how the habitat models\nthemselves can be used to plan more efficient AUV surveys by identifying where\nto collect further samples in order to most improve the habitat model. A\nBayesian neural network is used to predict visually-derived habitat classes\nwhen given broad-scale bathymetric data. This network can also estimate the\nuncertainty associated with a prediction, which can be deconstructed into its\naleatoric (data) and epistemic (model) components. We demonstrate how these\nstructured uncertainty estimates can be utilised to improve the model with\nfewer samples. Such adaptive approaches to benthic surveys have the potential\nto reduce costs by prioritizing further sampling efforts. We illustrate the\neffectiveness of the proposed approach using data collected by an AUV on\noffshore reefs in Tasmania, Australia.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 01:03:41 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Shields", "Jackson", ""], ["Pizarro", "Oscar", ""], ["Williams", "Stefan B.", ""]]}, {"id": "2006.11476", "submitter": "Yuan Yao", "authors": "Yuan Yao, Chang Liu, Dezhao Luo, Yu Zhou, Qixiang Ye", "title": "Video Playback Rate Perception for Self-supervisedSpatio-Temporal\n  Representation Learning", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In self-supervised spatio-temporal representation learning, the temporal\nresolution and long-short term characteristics are not yet fully explored,\nwhich limits representation capabilities of learned models. In this paper, we\npropose a novel self-supervised method, referred to as video Playback Rate\nPerception (PRP), to learn spatio-temporal representation in a\nsimple-yet-effective way. PRP roots in a dilated sampling strategy, which\nproduces self-supervision signals about video playback rates for representation\nmodel learning. PRP is implemented with a feature encoder, a classification\nmodule, and a reconstructing decoder, to achieve spatio-temporal semantic\nretention in a collaborative discrimination-generation manner. The\ndiscriminative perception model follows a feature encoder to prefer perceiving\nlow temporal resolution and long-term representation by classifying\nfast-forward rates. The generative perception model acts as a feature decoder\nto focus on comprehending high temporal resolution and short-term\nrepresentation by introducing a motion-attention mechanism. PRP is applied on\ntypical video target tasks including action recognition and video retrieval.\nExperiments show that PRP outperforms state-of-the-art self-supervised models\nwith significant margins. Code is available at github.com/yuanyao366/PRP\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 02:26:07 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Yao", "Yuan", ""], ["Liu", "Chang", ""], ["Luo", "Dezhao", ""], ["Zhou", "Yu", ""], ["Ye", "Qixiang", ""]]}, {"id": "2006.11480", "submitter": "Di Xie", "authors": "Weijie Chen and Shiliang Pu and Di Xie and Shicai Yang and Yilu Guo\n  and Luojun Lin", "title": "Unsupervised Image Classification for Deep Representation Learning", "comments": "Accepted by ECCV2020 Workshop VIPriors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep clustering against self-supervised learning is a very important and\npromising direction for unsupervised visual representation learning since it\nrequires little domain knowledge to design pretext tasks. However, the key\ncomponent, embedding clustering, limits its extension to the extremely\nlarge-scale dataset due to its prerequisite to save the global latent embedding\nof the entire dataset. In this work, we aim to make this framework more simple\nand elegant without performance decline. We propose an unsupervised image\nclassification framework without using embedding clustering, which is very\nsimilar to standard supervised training manner. For detailed interpretation, we\nfurther analyze its relation with deep clustering and contrastive learning.\nExtensive experiments on ImageNet dataset have been conducted to prove the\neffectiveness of our method. Furthermore, the experiments on transfer learning\nbenchmarks have verified its generalization to other downstream tasks,\nincluding multi-label image classification, object detection, semantic\nsegmentation and few-shot image classification.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 02:57:06 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 06:42:41 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Chen", "Weijie", ""], ["Pu", "Shiliang", ""], ["Xie", "Di", ""], ["Yang", "Shicai", ""], ["Guo", "Yilu", ""], ["Lin", "Luojun", ""]]}, {"id": "2006.11481", "submitter": "Haojie Liu", "authors": "Haojie Liu, Kang Liao, Chunyu Lin, Yao Zhao and Yulan Guo", "title": "Pseudo-LiDAR Point Cloud Interpolation Based on 3D Motion Representation\n  and Spatial Supervision", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-LiDAR point cloud interpolation is a novel and challenging task in the\nfield of autonomous driving, which aims to address the frequency mismatching\nproblem between camera and LiDAR. Previous works represent the 3D spatial\nmotion relationship induced by a coarse 2D optical flow, and the quality of\ninterpolated point clouds only depends on the supervision of depth maps. As a\nresult, the generated point clouds suffer from inferior global distributions\nand local appearances. To solve the above problems, we propose a Pseudo-LiDAR\npoint cloud interpolation network to generates temporally and spatially\nhigh-quality point cloud sequences. By exploiting the scene flow between point\nclouds, the proposed network is able to learn a more accurate representation of\nthe 3D spatial motion relationship. For the more comprehensive perception of\nthe distribution of point cloud, we design a novel reconstruction loss function\nthat implements the chamfer distance to supervise the generation of\nPseudo-LiDAR point clouds in 3D space. In addition, we introduce a multi-modal\ndeep aggregation module to facilitate the efficient fusion of texture and depth\nfeatures. As the benefits of the improved motion representation, training loss\nfunction, and model structure, our approach gains significant improvements on\nthe Pseudo-LiDAR point cloud interpolation task. The experimental results\nevaluated on KITTI dataset demonstrate the state-of-the-art performance of the\nproposed network, quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 03:11:04 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Liu", "Haojie", ""], ["Liao", "Kang", ""], ["Lin", "Chunyu", ""], ["Zhao", "Yao", ""], ["Guo", "Yulan", ""]]}, {"id": "2006.11486", "submitter": "Yang Wang", "authors": "Jinjia Peng, Yang Wang, Huibing Wang, Zhao Zhang, Xianping Fu, Meng\n  Wang", "title": "Unsupervised Vehicle Re-identification with Progressive Adaptation", "comments": "Appearing at IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification (reID) aims at identifying vehicles across\ndifferent non-overlapping cameras views. The existing methods heavily relied on\nwell-labeled datasets for ideal performance, which inevitably causes fateful\ndrop due to the severe domain bias between the training domain and the\nreal-world scenes; worse still, these approaches required full annotations,\nwhich is labor-consuming. To tackle these challenges, we propose a novel\nprogressive adaptation learning method for vehicle reID, named PAL, which\ninfers from the abundant data without annotations. For PAL, a data adaptation\nmodule is employed for source domain, which generates the images with similar\ndata distribution to unlabeled target domain as ``pseudo target samples''.\nThese pseudo samples are combined with the unlabeled samples that are selected\nby a dynamic sampling strategy to make training faster. We further proposed a\nweighted label smoothing (WLS) loss, which considers the similarity between\nsamples with different clusters to balance the confidence of pseudo labels.\nComprehensive experimental results validate the advantages of PAL on both\nVehicleID and VeRi-776 dataset.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 03:59:41 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Peng", "Jinjia", ""], ["Wang", "Yang", ""], ["Wang", "Huibing", ""], ["Zhang", "Zhao", ""], ["Fu", "Xianping", ""], ["Wang", "Meng", ""]]}, {"id": "2006.11487", "submitter": "Duong Le Hoang", "authors": "Duong H. Le, Trung-Nhan Vo, Nam Thoai", "title": "Paying more attention to snapshots of Iterative Pruning: Improving Model\n  Compression via Ensemble Distillation", "comments": "BMVC 2020 - Camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network pruning is one of the most dominant methods for reducing the heavy\ninference cost of deep neural networks. Existing methods often iteratively\nprune networks to attain high compression ratio without incurring significant\nloss in performance. However, we argue that conventional methods for retraining\npruned networks (i.e., using small, fixed learning rate) are inadequate as they\ncompletely ignore the benefits from snapshots of iterative pruning. In this\nwork, we show that strong ensembles can be constructed from snapshots of\niterative pruning, which achieve competitive performance and vary in network\nstructure. Furthermore, we present simple, general and effective pipeline that\ngenerates strong ensembles of networks during pruning with large learning rate\nrestarting, and utilizes knowledge distillation with those ensembles to improve\nthe predictive power of compact models. In standard image classification\nbenchmarks such as CIFAR and Tiny-Imagenet, we advance state-of-the-art pruning\nratio of structured pruning by integrating simple l1-norm filters pruning into\nour pipeline. Specifically, we reduce 75-80% of total parameters and 65-70%\nMACs of numerous variants of ResNet architectures while having comparable or\nbetter performance than that of original networks. Code associate with this\npaper is made publicly available at https://github.com/lehduong/kesi.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 03:59:46 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 19:15:43 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 05:41:26 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Le", "Duong H.", ""], ["Vo", "Trung-Nhan", ""], ["Thoai", "Nam", ""]]}, {"id": "2006.11510", "submitter": "Cong Wang", "authors": "Cong Wang and Witold Pedrycz and ZhiWu Li and MengChu Zhou and Shuzhi\n  Sam Ge", "title": "G-image Segmentation: Similarity-preserving Fuzzy C-Means with Spatial\n  Information Constraint in Wavelet Space", "comments": "This paper has been withdrawn by the author since some statements are\n  not right as raised by other researchers", "journal-ref": "IEEE Transactions on Fuzzy Systems, 2020", "doi": "10.1109/TFUZZ.2020.3029285", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  G-images refer to image data defined on irregular graph domains. This work\nelaborates a similarity-preserving Fuzzy C-Means (FCM) algorithm for G-image\nsegmentation and aims to develop techniques and tools for segmenting G-images.\nTo preserve the membership similarity between an arbitrary image pixel and its\nneighbors, a Kullback-Leibler divergence term on membership partition is\nintroduced as a part of FCM. As a result, similarity-preserving FCM is\ndeveloped by considering spatial information of image pixels for its robustness\nenhancement. Due to superior characteristics of a wavelet space, the proposed\nFCM is performed in this space rather than Euclidean one used in conventional\nFCM to secure its high robustness. Experiments on synthetic and real-world\nG-images demonstrate that it indeed achieves higher robustness and performance\nthan the state-of-the-art FCM algorithms. Moreover, it requires less\ncomputation than most of them.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 07:26:33 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 01:43:13 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Wang", "Cong", ""], ["Pedrycz", "Witold", ""], ["Li", "ZhiWu", ""], ["Zhou", "MengChu", ""], ["Ge", "Shuzhi Sam", ""]]}, {"id": "2006.11524", "submitter": "Saeed Amizadeh", "authors": "Saeed Amizadeh, Hamid Palangi, Oleksandr Polozov, Yichen Huang,\n  Kazuhito Koishida", "title": "Neuro-Symbolic Visual Reasoning: Disentangling \"Visual\" from \"Reasoning\"", "comments": "Published in Proceedings of the 37th International Conference on\n  Machine Learning (ICML), Online, PMLR 119, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.SC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual reasoning tasks such as visual question answering (VQA) require an\ninterplay of visual perception with reasoning about the question semantics\ngrounded in perception. However, recent advances in this area are still\nprimarily driven by perception improvements (e.g. scene graph generation)\nrather than reasoning. Neuro-symbolic models such as Neural Module Networks\nbring the benefits of compositional reasoning to VQA, but they are still\nentangled with visual representation learning, and thus neural reasoning is\nhard to improve and assess on its own. To address this, we propose (1) a\nframework to isolate and evaluate the reasoning aspect of VQA separately from\nits perception, and (2) a novel top-down calibration technique that allows the\nmodel to answer reasoning questions even with imperfect perception. To this\nend, we introduce a differentiable first-order logic formalism for VQA that\nexplicitly decouples question answering from visual perception. On the\nchallenging GQA dataset, this framework is used to perform in-depth,\ndisentangled comparisons between well-known VQA models leading to informative\ninsights regarding the participating models as well as the task.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 08:48:29 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 07:34:34 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 23:30:57 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Amizadeh", "Saeed", ""], ["Palangi", "Hamid", ""], ["Polozov", "Oleksandr", ""], ["Huang", "Yichen", ""], ["Koishida", "Kazuhito", ""]]}, {"id": "2006.11529", "submitter": "Gencer Sumbul", "authors": "Akshara Preethy Byju, Gencer Sumbul, Beg\\\"um Demir, Lorenzo Bruzzone", "title": "Remote Sensing Image Scene Classification with Deep Neural Networks in\n  JPEG 2000 Compressed Domain", "comments": "Accepted to IEEE Transactions on Geoscience and Remote Sensing", "journal-ref": null, "doi": "10.1109/TGRS.2020.3007523", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce the storage requirements, remote sensing (RS) images are usually\nstored in compressed format. Existing scene classification approaches using\ndeep neural networks (DNNs) require to fully decompress the images, which is a\ncomputationally demanding task in operational applications. To address this\nissue, in this paper we propose a novel approach to achieve scene\nclassification in JPEG 2000 compressed RS images. The proposed approach\nconsists of two main steps: i) approximation of the finer resolution sub-bands\nof reversible biorthogonal wavelet filters used in JPEG 2000; and ii)\ncharacterization of the high-level semantic content of approximated wavelet\nsub-bands and scene classification based on the learnt descriptors. This is\nachieved by taking codestreams associated with the coarsest resolution wavelet\nsub-band as input to approximate finer resolution sub-bands using a number of\ntransposed convolutional layers. Then, a series of convolutional layers models\nthe high-level semantic content of the approximated wavelet sub-band. Thus, the\nproposed approach models the multiresolution paradigm given in the JPEG 2000\ncompression algorithm in an end-to-end trainable unified neural network. In the\nclassification stage, the proposed approach takes only the coarsest resolution\nwavelet sub-bands as input, thereby reducing the time required to apply\ndecoding. Experimental results performed on two benchmark aerial image archives\ndemonstrate that the proposed approach significantly reduces the computational\ntime with similar classification accuracies when compared to traditional RS\nscene classification approaches (which requires full image decompression).\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 09:13:38 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 17:23:15 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Byju", "Akshara Preethy", ""], ["Sumbul", "Gencer", ""], ["Demir", "Beg\u00fcm", ""], ["Bruzzone", "Lorenzo", ""]]}, {"id": "2006.11538", "submitter": "Ionut Cosmin Duta", "authors": "Ionut Cosmin Duta, Li Liu, Fan Zhu, Ling Shao", "title": "Pyramidal Convolution: Rethinking Convolutional Neural Networks for\n  Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces pyramidal convolution (PyConv), which is capable of\nprocessing the input at multiple filter scales. PyConv contains a pyramid of\nkernels, where each level involves different types of filters with varying size\nand depth, which are able to capture different levels of details in the scene.\nOn top of these improved recognition capabilities, PyConv is also efficient\nand, with our formulation, it does not increase the computational cost and\nparameters compared to standard convolution. Moreover, it is very flexible and\nextensible, providing a large space of potential network architectures for\ndifferent applications. PyConv has the potential to impact nearly every\ncomputer vision task and, in this work, we present different architectures\nbased on PyConv for four main tasks on visual recognition: image\nclassification, video action classification/recognition, object detection and\nsemantic image segmentation/parsing. Our approach shows significant\nimprovements over all these core tasks in comparison with the baselines. For\ninstance, on image recognition, our 50-layers network outperforms in terms of\nrecognition performance on ImageNet dataset its counterpart baseline ResNet\nwith 152 layers, while having 2.39 times less parameters, 2.52 times lower\ncomputational complexity and more than 3 times less layers. On image\nsegmentation, our novel framework sets a new state-of-the-art on the\nchallenging ADE20K benchmark for scene parsing. Code is available at:\nhttps://github.com/iduta/pyconv\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 10:19:29 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Duta", "Ionut Cosmin", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""], ["Shao", "Ling", ""]]}, {"id": "2006.11539", "submitter": "Yijun Quan", "authors": "Yijun Quan and Chang-Tsun Li", "title": "On Addressing the Impact of ISO Speed upon PRNU and Forgery Detection", "comments": "The paper is accepted to IEEE Transactions on Information Forensics\n  and Security with the supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo Response Non-Uniformity (PRNU) has been used as a powerful device\nfingerprint for image forgery detection because image forgeries can be revealed\nby finding the absence of the PRNU in the manipulated areas. The correlation\nbetween an image's noise residual with the device's reference PRNU is often\ncompared with a decision threshold to check the existence of the PRNU. A PRNU\ncorrelation predictor is usually used to determine this decision threshold\nassuming the correlation is content-dependent. However, we found that not only\nthe correlation is content-dependent, but it also depends on the camera\nsensitivity setting. \\textit{Camera sensitivity}, commonly known by the name of\n\\textit{ISO speed}, is an important attribute in digital photography. In this\nwork, we will show the PRNU correlation's dependency on ISO speed. Due to such\ndependency, we postulate that a correlation predictor is ISO speed-specific,\ni.e. \\textit{reliable correlation predictions can only be made when a\ncorrelation predictor is trained with images of similar ISO speeds to the image\nin question}. We report the experiments we conducted to validate the postulate.\nIt is realized that in the real-world, information about the ISO speed may not\nbe available in the metadata to facilitate the implementation of our postulate\nin the correlation prediction process. We hence propose a method called\nContent-based Inference of ISO Speeds (CINFISOS) to infer the ISO speed from\nthe image content.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 10:23:54 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Quan", "Yijun", ""], ["Li", "Chang-Tsun", ""]]}, {"id": "2006.11547", "submitter": "Pascal Kerschke", "authors": "Lennart Sch\\\"apermeier and Christian Grimme and Pascal Kerschke", "title": "One PLOT to Show Them All: Visualization of Efficient Sets in\n  Multi-Objective Landscapes", "comments": "This version has been accepted for publication at the 16th\n  International Conference on Parallel Problem Solving from Nature (PPSN XVI)", "journal-ref": "Proceedings of the 16th International Conference on Parallel\n  Problem Solving from Nature (PPSN XVI), pp. 154 - 167, Springer (2020)", "doi": "10.1007/978-3-030-58115-2_11", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization techniques for the decision space of continuous multi-objective\noptimization problems (MOPs) are rather scarce in research. For long, all\ntechniques focused on global optimality and even for the few available\nlandscape visualizations, e.g., cost landscapes, globality is the main\ncriterion. In contrast, the recently proposed gradient field heatmaps (GFHs)\nemphasize the location and attraction basins of local efficient sets, but\nignore the relation of sets in terms of solution quality.\n  In this paper, we propose a new and hybrid visualization technique, which\ncombines the advantages of both approaches in order to represent local and\nglobal optimality together within a single visualization. Therefore, we build\non the GFH approach but apply a new technique for approximating the location of\nlocally efficient points and using the divergence of the multi-objective\ngradient vector field as a robust second-order condition. Then, the relative\ndominance relationship of the determined locally efficient points is used to\nvisualize the complete landscape of the MOP. Augmented by information on the\nbasins of attraction, this Plot of Landscapes with Optimal Trade-offs (PLOT)\nbecomes one of the most informative multi-objective landscape visualization\ntechniques available.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 11:03:11 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Sch\u00e4permeier", "Lennart", ""], ["Grimme", "Christian", ""], ["Kerschke", "Pascal", ""]]}, {"id": "2006.11557", "submitter": "Yao Rong", "authors": "Yao Rong, Zeynep Akata, Enkelejda Kasneci", "title": "Driver Intention Anticipation Based on In-Cabin and Driving Scene\n  Monitoring", "comments": "8 pages, 9 figures", "journal-ref": "IEEE Conference on Intelligent Transportation Systems (ITSC), 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous car accidents are caused by improper driving maneuvers. Serious\ninjuries are however avoidable if such driving maneuvers are detected\nbeforehand and the driver is assisted accordingly. In fact, various recent\nresearch has focused on the automated prediction of driving maneuver based on\nhand-crafted features extracted mainly from in-cabin driver videos. Since the\noutside view from the traffic scene may also contain informative features for\ndriving maneuver prediction, we present a framework for the detection of the\ndrivers' intention based on both in-cabin and traffic scene videos. More\nspecifically, we (1) propose a Convolutional-LSTM (ConvLSTM)-based auto-encoder\nto extract motion features from the out-cabin traffic, (2) train a classifier\nwhich considers motions from both in- and outside of the cabin jointly for\nmaneuver intention anticipation, (3) experimentally prove that the in- and\noutside image features have complementary information. Our evaluation based on\nthe publicly available dataset Brain4cars shows that our framework achieves a\nprediction with the accuracy of 83.98% and F1-score of 84.3%.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 11:56:32 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Rong", "Yao", ""], ["Akata", "Zeynep", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2006.11593", "submitter": "Yongming Li", "authors": "Yongming Li, Lang Zhou, Lingyun Qin, Yuwei Zeng, Yuchuan Liu, Yan Lei,\n  Pin Wang, Fan Li", "title": "Deep Double-Side Learning Ensemble Model for Few-Shot Parkinson Speech\n  Recognition", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosis and therapeutic effect assessment of Parkinson disease based on\nvoice data are very important,but its few-shot learning problem is\nchallenging.Although deep learning is good at automatic feature extraction, it\nsuffers from few-shot learning problem. Therefore, the general effective method\nis first conduct feature extraction based on prior knowledge, and then carry\nout feature reduction for subsequent classification. However, there are two\nmajor problems: 1) Structural information among speech features has not been\nmined and new features of higher quality have not been reconstructed. 2)\nStructural information between data samples has not been mined and new samples\nwith higher quality have not been reconstructed. To solve these two problems,\nbased on the existing Parkinson speech feature data set, a deep double-side\nlearning ensemble model is designed in this paper that can reconstruct speech\nfeatures and samples deeply and simultaneously. As to feature reconstruction,\nan embedded deep stacked group sparse auto-encoder is designed in this paper to\nconduct nonlinear feature transformation, so as to acquire new high-level deep\nfeatures, and then the deep features are fused with original speech features by\nL1 regularization feature selection method. As to speech sample reconstruction,\na deep sample learning algorithm is designed in this paper based on iterative\nmean clustering to conduct samples transformation, so as to obtain new\nhigh-level deep samples. Finally, the bagging ensemble learning mode is adopted\nto fuse the deep feature learning algorithm and the deep samples learning\nalgorithm together, thereby constructing a deep double-side learning ensemble\nmodel. At the end of this paper, two representative speech datasets of\nParkinson's disease were used for verification. The experimental results show\nthat the proposed algorithm are effective.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 15:14:41 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Li", "Yongming", ""], ["Zhou", "Lang", ""], ["Qin", "Lingyun", ""], ["Zeng", "Yuwei", ""], ["Liu", "Yuchuan", ""], ["Lei", "Yan", ""], ["Wang", "Pin", ""], ["Li", "Fan", ""]]}, {"id": "2006.11604", "submitter": "Sandesh Kamath K", "authors": "Sandesh Kamath, Amit Deshpande, K V Subrahmanyam", "title": "How do SGD hyperparameters in natural training affect adversarial\n  robustness?", "comments": "Preliminary version presented in ICML 2019 Workshop on \"Understanding\n  and Improving Generalization in Deep Learning\" as \"On Adversarial Robustness\n  of Small vs Large Batch Training\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning rate, batch size and momentum are three important hyperparameters in\nthe SGD algorithm. It is known from the work of Jastrzebski et al.\narXiv:1711.04623 that large batch size training of neural networks yields\nmodels which do not generalize well. Yao et al. arXiv:1802.08241 observe that\nlarge batch training yields models that have poor adversarial robustness. In\nthe same paper, the authors train models with different batch sizes and compute\nthe eigenvalues of the Hessian of loss function. They observe that as the batch\nsize increases, the dominant eigenvalues of the Hessian become larger. They\nalso show that both adversarial training and small-batch training leads to a\ndrop in the dominant eigenvalues of the Hessian or lowering its spectrum. They\ncombine adversarial training and second order information to come up with a new\nlarge-batch training algorithm and obtain robust models with good\ngeneralization. In this paper, we empirically observe the effect of the SGD\nhyperparameters on the accuracy and adversarial robustness of networks trained\nwith unperturbed samples. Jastrzebski et al. considered training models with a\nfixed learning rate to batch size ratio. They observed that higher the ratio,\nbetter is the generalization. We observe that networks trained with constant\nlearning rate to batch size ratio, as proposed in Jastrzebski et al., yield\nmodels which generalize well and also have almost constant adversarial\nrobustness, independent of the batch size. We observe that momentum is more\neffective with varying batch sizes and a fixed learning rate than with constant\nlearning rate to batch size ratio based SGD training.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 16:04:44 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kamath", "Sandesh", ""], ["Deshpande", "Amit", ""], ["Subrahmanyam", "K V", ""]]}, {"id": "2006.11623", "submitter": "Esha Sarkar", "authors": "Esha Sarkar, Hadjer Benkraouda, Michail Maniatakos", "title": "FaceHack: Triggering backdoored facial recognition systems using facial\n  characteristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Machine Learning (ML) have opened up new avenues for its\nextensive use in real-world applications. Facial recognition, specifically, is\nused from simple friend suggestions in social-media platforms to critical\nsecurity applications for biometric validation in automated immigration at\nairports. Considering these scenarios, security vulnerabilities to such ML\nalgorithms pose serious threats with severe outcomes. Recent work demonstrated\nthat Deep Neural Networks (DNNs), typically used in facial recognition systems,\nare susceptible to backdoor attacks; in other words,the DNNs turn malicious in\nthe presence of a unique trigger. Adhering to common characteristics for being\nunnoticeable, an ideal trigger is small, localized, and typically not a part of\nthe main im-age. Therefore, detection mechanisms have focused on detecting\nthese distinct trigger-based outliers statistically or through their\nreconstruction. In this work, we demonstrate that specific changes to facial\ncharacteristics may also be used to trigger malicious behavior in an ML model.\nThe changes in the facial attributes maybe embedded artificially using\nsocial-media filters or introduced naturally using movements in facial muscles.\nBy construction, our triggers are large, adaptive to the input, and spread over\nthe entire image. We evaluate the success of the attack and validate that it\ndoes not interfere with the performance criteria of the model. We also\nsubstantiate the undetectability of our triggers by exhaustively testing them\nwith state-of-the-art defenses.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 17:39:23 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Sarkar", "Esha", ""], ["Benkraouda", "Hadjer", ""], ["Maniatakos", "Michail", ""]]}, {"id": "2006.11629", "submitter": "Bahram Mohammadi", "authors": "Masoud Pourreza, Bahram Mohammadi, Mostafa Khaki, Samir Bouindour,\n  Hichem Snoussi, Mohammad Sabokrou", "title": "G2D: Generate to Detect Anomaly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method for irregularity detection. Previous\nresearches solve this problem as a One-Class Classification (OCC) task where\nthey train a reference model on all of the available samples. Then, they\nconsider a test sample as an anomaly if it has a diversion from the reference\nmodel. Generative Adversarial Networks (GANs) have achieved the most promising\nresults for OCC while implementing and training such networks, especially for\nthe OCC task, is a cumbersome and computationally expensive procedure. To cope\nwith the mentioned challenges, we present a simple but effective method to\nsolve the irregularity detection as a binary classification task in order to\nmake the implementation easier along with improving the detection performance.\nWe learn two deep neural networks (generator and discriminator) in a GAN-style\nsetting on merely the normal samples. During training, the generator gradually\nbecomes an expert to generate samples which are similar to the normal ones. In\nthe training phase, when the generator fails to produce normal data (in the\nearly stages of learning and also prior to the complete convergence), it can be\nconsidered as an irregularity generator. In this way, we simultaneously\ngenerate the irregular samples. Afterward, we train a binary classifier on the\ngenerated anomalous samples along with the normal instances in order to be\ncapable of detecting irregularities. The proposed framework applies to\ndifferent related applications of outlier and anomaly detection in images and\nvideos, respectively. The results confirm that our proposed method is superior\nto the baseline and state-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 18:02:50 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 18:18:52 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Pourreza", "Masoud", ""], ["Mohammadi", "Bahram", ""], ["Khaki", "Mostafa", ""], ["Bouindour", "Samir", ""], ["Snoussi", "Hichem", ""], ["Sabokrou", "Mohammad", ""]]}, {"id": "2006.11630", "submitter": "Junqi Tang", "authors": "Junqi Tang, Mike Davies", "title": "A Fast Stochastic Plug-and-Play ADMM for Imaging Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose an efficient stochastic plug-and-play (PnP) algorithm\nfor imaging inverse problems. The PnP stochastic gradient descent methods have\nbeen recently proposed and shown improved performance in some imaging\napplications over standard deterministic PnP methods. However, current\nstochastic PnP methods need to frequently compute the image denoisers which can\nbe computationally expensive. To overcome this limitation, we propose a new\nstochastic PnP-ADMM method which is based on introducing stochastic gradient\ndescent inner-loops within an inexact ADMM framework. We provide the\ntheoretical guarantee on the fixed-point convergence for our algorithm under\nstandard assumptions. Our numerical results demonstrate the effectiveness of\nour approach compared with state-of-the-art PnP methods.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 18:03:52 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 08:47:00 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Tang", "Junqi", ""], ["Davies", "Mike", ""]]}, {"id": "2006.11631", "submitter": "Jongseok Lee", "authors": "Jongseok Lee, Matthias Humt, Jianxiang Feng, Rudolph Triebel", "title": "Estimating Model Uncertainty of Neural Networks in Sparse Information\n  Form", "comments": "Accepted to the Thirty-seventh International Conference on Machine\n  Learning (ICML) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sparse representation of model uncertainty for Deep Neural\nNetworks (DNNs) where the parameter posterior is approximated with an inverse\nformulation of the Multivariate Normal Distribution (MND), also known as the\ninformation form. The key insight of our work is that the information matrix,\ni.e. the inverse of the covariance matrix tends to be sparse in its spectrum.\nTherefore, dimensionality reduction techniques such as low rank approximations\n(LRA) can be effectively exploited. To achieve this, we develop a novel\nsparsification algorithm and derive a cost-effective analytical sampler. As a\nresult, we show that the information form can be scalably applied to represent\nmodel uncertainty in DNNs. Our exhaustive theoretical analysis and empirical\nevaluations on various benchmarks show the competitiveness of our approach over\nthe current methods.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 18:09:59 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Lee", "Jongseok", ""], ["Humt", "Matthias", ""], ["Feng", "Jianxiang", ""], ["Triebel", "Rudolph", ""]]}, {"id": "2006.11643", "submitter": "Dmitry V. Dylov", "authors": "Iaroslav Bespalov, Nazar Buzun, Dmitry V. Dylov", "title": "BRUL\\`E: Barycenter-Regularized Unsupervised Landmark Extraction", "comments": "10 main pages with 6 figures and 1 Table, 14 pages total with 6\n  supplementary figures. I.B. and N.B. contributed equally. D.V.D. is\n  corresponding author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised retrieval of image features is vital for many computer vision\ntasks where the annotation is missing or scarce. In this work, we propose a new\nunsupervised approach to detect the landmarks in images, validating it on the\npopular task of human face key-points extraction. The method is based on the\nidea of auto-encoding the wanted landmarks in the latent space while discarding\nthe non-essential information (and effectively preserving the\ninterpretability). The interpretable latent space representation (the\nbottleneck containing nothing but the wanted key-points) is achieved by a new\ntwo-step regularization approach. The first regularization step evaluates\ntransport distance from a given set of landmarks to some average value (the\nbarycenter by Wasserstein distance). The second regularization step controls\ndeviations from the barycenter by applying random geometric deformations\nsynchronously to the initial image and to the encoded landmarks. We demonstrate\nthe effectiveness of the approach both in unsupervised and semi-supervised\ntraining scenarios using 300-W, CelebA, and MAFL datasets. The proposed\nregularization paradigm is shown to prevent overfitting, and the detection\nquality is shown to improve beyond the state-of-the-art face models.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 20:04:00 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 11:14:13 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 09:18:05 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Bespalov", "Iaroslav", ""], ["Buzun", "Nazar", ""], ["Dylov", "Dmitry V.", ""]]}, {"id": "2006.11652", "submitter": "Philipp Harms", "authors": "Martin Bauer, Nicolas Charon, Philipp Harms, and Hsi-Wei Hsieh", "title": "A numerical framework for elastic surface matching, comparison, and\n  interpolation", "comments": "21 pages, 11 figures, 1 table, 3 algorithms. Forthcoming in the\n  International Journal of Computer Vision", "journal-ref": null, "doi": "10.1007/s11263-021-01476-6", "report-no": null, "categories": "cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface comparison and matching is a challenging problem in computer vision.\nWhile reparametrization-invariant Sobolev metrics provide meaningful elastic\ndistances and point correspondences via the geodesic boundary value problem,\nsolving this problem numerically tends to be difficult. Square root normal\nfields (SRNF) considerably simplify the computation of certain elastic\ndistances between parametrized surfaces. Yet they leave open the issue of\nfinding optimal reparametrizations, which induce elastic distances between\nunparametrized surfaces. This issue has concentrated much effort in recent\nyears and led to the development of several numerical frameworks. In this\npaper, we take an alternative approach which bypasses the direct estimation of\nreparametrizations: we relax the geodesic boundary constraint using an\nauxiliary parametrization-blind varifold fidelity metric. This reformulation\nhas several notable benefits. By avoiding altogether the need for\nreparametrizations, it provides the flexibility to deal with simplicial meshes\nof arbitrary topologies and sampling patterns. Moreover, the problem lends\nitself to a coarse-to-fine multi-resolution implementation, which makes the\nalgorithm scalable to large meshes. Furthermore, this approach extends readily\nto higher-order feature maps such as square root curvature fields and is also\nable to include surface textures in the matching problem. We demonstrate these\nadvantages on several examples, synthetic and real.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 20:35:59 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 15:14:23 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Bauer", "Martin", ""], ["Charon", "Nicolas", ""], ["Harms", "Philipp", ""], ["Hsieh", "Hsi-Wei", ""]]}, {"id": "2006.11653", "submitter": "Yi Xu", "authors": "Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Rong Jin", "title": "Towards Understanding Label Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label smoothing regularization (LSR) has a great success in training deep\nneural networks by stochastic algorithms such as stochastic gradient descent\nand its variants. However, the theoretical understanding of its power from the\nview of optimization is still rare. This study opens the door to a deep\nunderstanding of LSR by initiating the analysis. In this paper, we analyze the\nconvergence behaviors of stochastic gradient descent with label smoothing\nregularization for solving non-convex problems and show that an appropriate LSR\ncan help to speed up the convergence by reducing the variance. More\ninterestingly, we proposed a simple yet effective strategy, namely Two-Stage\nLAbel smoothing algorithm (TSLA), that uses LSR in the early training epochs\nand drops it off in the later training epochs. We observe from the improved\nconvergence result of TSLA that it benefits from LSR in the first stage and\nessentially converges faster in the second stage. To the best of our knowledge,\nthis is the first work for understanding the power of LSR via establishing\nconvergence complexity of stochastic methods with LSR in non-convex\noptimization. We empirically demonstrate the effectiveness of the proposed\nmethod in comparison with baselines on training ResNet models over benchmark\ndata sets.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 20:36:17 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 03:05:47 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Xu", "Yi", ""], ["Xu", "Yuanhong", ""], ["Qian", "Qi", ""], ["Li", "Hao", ""], ["Jin", "Rong", ""]]}, {"id": "2006.11658", "submitter": "Boris Chidlovskii", "authors": "Boris Chidlovskii, Assem Sadek", "title": "Adversarial Transfer of Pose Estimation Regression", "comments": "Published in ECCV'20 TASK-CV Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of camera pose estimation in visual localization.\nCurrent regression-based methods for pose estimation are trained and evaluated\nscene-wise. They depend on the coordinate frame of the training dataset and\nshow a low generalization across scenes and datasets. We identify the dataset\nshift an important barrier to generalization and consider transfer learning as\nan alternative way towards a better reuse of pose estimation models. We revise\ndomain adaptation techniques for classification and extend them to camera pose\nestimation, which is a multi-regression task. We develop a deep adaptation\nnetwork for learning scene-invariant image representations and use adversarial\nlearning to generate such representations for model transfer. We enrich the\nnetwork with self-supervised learning and use the adaptability theory to\nvalidate the existence of scene-invariant representation of images in two given\nscenes. We evaluate our network on two public datasets, Cambridge Landmarks and\n7Scene, demonstrate its superiority over several baselines and compare to the\nstate of the art methods.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 21:16:37 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 13:45:18 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Chidlovskii", "Boris", ""], ["Sadek", "Assem", ""]]}, {"id": "2006.11692", "submitter": "Min-Kook Choi", "authors": "Jihun Yoon, Seungbum Hong, Sanha Jeong, Min-Kook Choi", "title": "Semi-Supervised Object Detection with Sparsely Annotated Dataset", "comments": "Challenge Winner in Epic-Kitchens 2020 Object Detection Challenge\n  (EPIC@CVPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In training object detector based on convolutional neural networks, selection\nof effective positive examples for training is an important factor. However,\nwhen training an anchor-based detectors with sparse annotations on an image,\neffort to find effective positive examples can hinder training performance.\nWhen using the anchor-based training for the ground truth bounding box to\ncollect positive examples under given IoU, it is often possible to include\nobjects from other classes in the current training class, or objects that are\nneeded to be trained can only be sampled as negative examples. We used two\napproaches to solve this problem: 1) the use of an anchorless object detector\nand 2) a semi-supervised learning-based object detection using a single object\ntracker. The proposed technique performs single object tracking by using the\nsparsely annotated bounding box as an anchor in the temporal domain for\nsuccessive frames. From the tracking results, dense annotations for training\nimages were generated in an automated manner and used for training the object\ndetector. We applied the proposed single object tracking-based semi-supervised\nlearning to the Epic-Kitchens dataset. As a result, we were able to achieve\n\\textbf{runner-up} performance in the Unseen section while achieving the first\nplace in the Seen section of the Epic-Kitchens 2020 object detection challenge\nunder IoU > 0.5 evaluation\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 02:26:48 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Yoon", "Jihun", ""], ["Hong", "Seungbum", ""], ["Jeong", "Sanha", ""], ["Choi", "Min-Kook", ""]]}, {"id": "2006.11693", "submitter": "Teng Wang", "authors": "Teng Wang, Huicheng Zheng, Mingjing Yu", "title": "Dense-Captioning Events in Videos: SYSU Submission to ActivityNet\n  Challenge 2020", "comments": "Second-place solution to TASK 2 (Dense video captioning) in\n  ActivityNet Challenge 2020. Code is available at\n  https://github.com/ttengwang/dense-video-captioning-pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report presents a brief description of our submission to the\ndense video captioning task of ActivityNet Challenge 2020. Our approach follows\na two-stage pipeline: first, we extract a set of temporal event proposals; then\nwe propose a multi-event captioning model to capture the event-level temporal\nrelationships and effectively fuse the multi-modal information. Our approach\nachieves a 9.28 METEOR score on the test set.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 02:38:59 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 03:44:21 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Wang", "Teng", ""], ["Zheng", "Huicheng", ""], ["Yu", "Mingjing", ""]]}, {"id": "2006.11697", "submitter": "Beier Zhu", "authors": "Beier Zhu, Chunze Lin, Quan Wang, Renjie Liao, Chen Qian", "title": "Fast and Accurate: Structure Coherence Component for Face Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a fast and accurate coordinate regression method\nfor face alignment. Unlike most existing facial landmark regression methods\nwhich usually employ fully connected layers to convert feature maps into\nlandmark coordinate, we present a structure coherence component to explicitly\ntake the relation among facial landmarks into account. Due to the geometric\nstructure of human face, structure coherence between different facial parts\nprovides important cues for effectively localizing facial landmarks. However,\nthe dense connection in the fully connected layers overuses such coherence,\nmaking the important cues unable to be distinguished from all connections.\nInstead, our structure coherence component leverages a dynamic sparse graph\nstructure to passing features among the most related landmarks. Furthermore, we\npropose a novel objective function, named Soft Wing loss, to improve the\naccuracy. Extensive experiments on three popular benchmarks, including WFLW,\nCOFW and 300W, demonstrate the effectiveness of the proposed method, achieving\nstate-of-the-art performance with fast speed. Our approach is especially robust\nto challenging cases resulting in impressively low failure rate (0% and 2.88%)\nin COFW and WFLW datasets.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 02:52:29 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zhu", "Beier", ""], ["Lin", "Chunze", ""], ["Wang", "Quan", ""], ["Liao", "Renjie", ""], ["Qian", "Chen", ""]]}, {"id": "2006.11702", "submitter": "Lu Liu", "authors": "Lu Liu, William Hamilton, Guodong Long, Jing Jiang, Hugo Larochelle", "title": "A Universal Representation Transformer Layer for Few-Shot Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot classification aims to recognize unseen classes when presented with\nonly a small number of samples. We consider the problem of multi-domain\nfew-shot image classification, where unseen classes and examples come from\ndiverse data sources. This problem has seen growing interest and has inspired\nthe development of benchmarks such as Meta-Dataset. A key challenge in this\nmulti-domain setting is to effectively integrate the feature representations\nfrom the diverse set of training domains. Here, we propose a Universal\nRepresentation Transformer (URT) layer, that meta-learns to leverage universal\nfeatures for few-shot classification by dynamically re-weighting and composing\nthe most appropriate domain-specific representations. In experiments, we show\nthat URT sets a new state-of-the-art result on Meta-Dataset. Specifically, it\nachieves top-performance on the highest number of data sources compared to\ncompeting methods. We analyze variants of URT and present a visualization of\nthe attention score heatmaps that sheds light on how the model performs\ncross-domain generalization. Our code is available at\nhttps://github.com/liulu112601/URT.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 03:08:00 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 21:34:58 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 06:30:08 GMT"}, {"version": "v4", "created": "Wed, 2 Sep 2020 22:35:10 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Liu", "Lu", ""], ["Hamilton", "William", ""], ["Long", "Guodong", ""], ["Jiang", "Jing", ""], ["Larochelle", "Hugo", ""]]}, {"id": "2006.11706", "submitter": "Ismail Elezi", "authors": "Ismail Elezi", "title": "Exploiting Contextual Information with Deep Neural Networks", "comments": "Ph.D. thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context matters! Nevertheless, there has not been much research in exploiting\ncontextual information in deep neural networks. For most part, the entire usage\nof contextual information has been limited to recurrent neural networks.\nAttention models and capsule networks are two recent ways of introducing\ncontextual information in non-recurrent models, however both of these\nalgorithms have been developed after this work has started.\n  In this thesis, we show that contextual information can be exploited in 2\nfundamentally different ways: implicitly and explicitly. In the DeepScore\nproject, where the usage of context is very important for the recognition of\nmany tiny objects, we show that by carefully crafting convolutional\narchitectures, we can achieve state-of-the-art results, while also being able\nto implicitly correctly distinguish between objects which are virtually\nidentical, but have different meanings based on their surrounding. In parallel,\nwe show that by explicitly designing algorithms (motivated from graph theory\nand game theory) that take into considerations the entire structure of the\ndataset, we can achieve state-of-the-art results in different topics like\nsemi-supervised learning and similarity learning.\n  To the best of our knowledge, we are the first to integrate graph-theoretical\nmodules, carefully crafted for the problem of similarity learning and that are\ndesigned to consider contextual information, not only outperforming the other\nmodels, but also gaining a speed improvement while using a smaller number of\nparameters.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 03:40:30 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 18:00:07 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Elezi", "Ismail", ""]]}, {"id": "2006.11708", "submitter": "Vasileios Lioutas", "authors": "Vasileios Lioutas", "title": "Mapping Low-Resolution Images To Multiple High-Resolution Images Using\n  Non-Adversarial Mapping", "comments": "Paper completed in April 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods have recently been proposed for the Single Image\nSuper-Resolution (SISR) problem. The current methods assume that a single\nlow-resolution image can only yield a single high-resolution image. In\naddition, all of these methods use low-resolution images that were artificially\ngenerated through simple bilinear down-sampling. We argue that, first and\nforemost, the problem of SISR is an one-to-many mapping problem between the\nlow-resolution and all possible candidate high-resolution images and we address\nthe challenging task of learning how to realistically degrade and down-sample\nhigh-resolution images. To circumvent this problem, we propose SR-NAM which\nutilizes the Non-Adversarial Mapping (NAM) technique. Furthermore, we propose a\ndegradation model that learns how to transform high-resolution images to\nlow-resolution images that resemble realistically taken low-resolution photos.\nFinally, some qualitative results for the proposed method along with the\nweaknesses of SR-NAM are included.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 04:14:24 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 18:14:13 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Lioutas", "Vasileios", ""]]}, {"id": "2006.11714", "submitter": "Shiyang Yan", "authors": "Shiyang Yan, Yang Hua, Neil M. Robertson", "title": "Off-Policy Self-Critical Training for Transformer in Visual Paragraph\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several approaches have been proposed to solve language generation\nproblems. Transformer is currently state-of-the-art seq-to-seq model in\nlanguage generation. Reinforcement Learning (RL) is useful in solving exposure\nbias and the optimisation on non-differentiable metrics in seq-to-seq language\nlearning. However, Transformer is hard to combine with RL as the costly\ncomputing resource is required for sampling. We tackle this problem by\nproposing an off-policy RL learning algorithm where a behaviour policy\nrepresented by GRUs performs the sampling. We reduce the high variance of\nimportance sampling (IS) by applying the truncated relative importance sampling\n(TRIS) technique and Kullback-Leibler (KL)-control concept. TRIS is a simple\nyet effective technique, and there is a theoretical proof that KL-control helps\nto reduce the variance of IS. We formulate this off-policy RL based on\nself-critical sequence training. Specifically, we use a Transformer-based\ncaptioning model as the target policy and use an image-guided language\nauto-encoder as the behaviour policy to explore the environment. The proposed\nalgorithm achieves state-of-the-art performance on the visual paragraph\ngeneration and improved results on image captioning.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 05:10:17 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Yan", "Shiyang", ""], ["Hua", "Yang", ""], ["Robertson", "Neil M.", ""]]}, {"id": "2006.11716", "submitter": "Vijay Veerabadran", "authors": "Vijay Veerabadran, Virginia R. de Sa", "title": "Learning compact generalizable neural representations supporting\n  perceptual grouping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work at the intersection of vision science and deep learning is starting to\nexplore the efficacy of deep convolutional networks (DCNs) and recurrent\nnetworks in solving perceptual grouping problems that underlie primate visual\nrecognition and segmentation. Here, we extend this line of work to investigate\nthe compactness and generalizability of DCN solutions to learning low-level\nperceptual grouping routines involving contour integration. We introduce V1Net,\na bio-inspired recurrent unit that incorporates lateral connections ubiquitous\nin cortical circuitry. Feedforward convolutional layers in DCNs can be\nsubstituted with V1Net modules to enhance their contextual visual processing\nsupport for perceptual grouping. We compare the learning efficiency and\naccuracy of V1Net-DCNs to that of 14 carefully selected feedforward and\nrecurrent neural architectures (including state-of-the-art DCNs) on MarkedLong\n-- a synthetic forced-choice contour integration dataset of 800,000 images we\nintroduce here -- and the previously published Pathfinder contour integration\nbenchmarks. We gauged solution generalizability by measuring the transfer\nlearning performance of our candidate models trained on MarkedLong that were\nfine-tuned to learn PathFinder. Our results demonstrate that a compact 3-layer\nV1Net-DCN matches or outperforms the test accuracy and sample efficiency of all\ntested comparison models which contain between 5x and 1000x more trainable\nparameters; we also note that V1Net-DCN learns the most compact generalizable\nsolution to MarkedLong. A visualization of the temporal dynamics of a V1Net-DCN\nelucidates its usage of interpretable grouping computations to solve\nMarkedLong. The compact and rich representations of V1Net-DCN also make it a\npromising candidate to build on-device machine vision algorithms as well as\nhelp better understand biological cortical circuitry.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 05:46:01 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Veerabadran", "Vijay", ""], ["de Sa", "Virginia R.", ""]]}, {"id": "2006.11718", "submitter": "Richard Yang", "authors": "Steven Chen and Richard R. Yang", "title": "Pose Trainer: Correcting Exercise Posture using Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitness exercises are very beneficial to personal health and fitness;\nhowever, they can also be ineffective and potentially dangerous if performed\nincorrectly by the user. Exercise mistakes are made when the user does not use\nthe proper form, or pose. In our work, we introduce Pose Trainer, an\napplication that detects the user's exercise pose and provides personalized,\ndetailed recommendations on how the user can improve their form. Pose Trainer\nuses the state of the art in pose estimation to detect a user's pose, then\nevaluates the vector geometry of the pose through an exercise to provide useful\nfeedback. We record a dataset of over 100 exercise videos of correct and\nincorrect form, based on personal training guidelines, and build\ngeometric-heuristic and machine learning algorithms for evaluation. Pose\nTrainer works on four common exercises and supports any Windows or Linux\ncomputer with a GPU.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 05:51:37 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Chen", "Steven", ""], ["Yang", "Richard R.", ""]]}, {"id": "2006.11729", "submitter": "Mahla Nejati", "authors": "Mahla Nejati, Nicky Penhall, Henry Williams, Jamie Bell, JongYoon Lim,\n  Ho Seok Ahn, Bruce MacDonald", "title": "Kiwifruit detection in challenging conditions", "comments": "Accepted to the Australasian conference on robotics and automation\n  (ACRA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reliable kiwifruit detection is one of the biggest challenges in\ndeveloping a selective fruit harvesting robot. The vision system of an orchard\nrobot faces difficulties such as dynamic lighting conditions and fruit\nocclusions. This paper presents a semantic segmentation approach with two novel\nimage prepossessing techniques designed to detect kiwifruit under the harsh\nlighting conditions found in the canopy. The performance of the presented\nsystem is evaluated on a 3D real-world image set of kiwifruit under different\nlighting conditions (typical, glare, and overexposed). Alone the semantic\nsegmentation approach achieves an F1_score of 0.82 on the typical lighting\nimage set, but struggles with harsh lighting with an F1_score of 0.13.\nUtilising the prepossessing techniques the vision system under harsh lighting\nimproves to an F1_score 0.42. To address the fruit occlusion challenge, the\noverall approach was found to be capable of detecting 87.0% of non-occluded and\n30.0% of occluded kiwifruit across all lighting conditions.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 07:35:57 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Nejati", "Mahla", ""], ["Penhall", "Nicky", ""], ["Williams", "Henry", ""], ["Bell", "Jamie", ""], ["Lim", "JongYoon", ""], ["Ahn", "Ho Seok", ""], ["MacDonald", "Bruce", ""]]}, {"id": "2006.11735", "submitter": "Hengrui Zhao", "authors": "Hengrui Zhao and Dong Liu and Houqiang Li", "title": "Efficient Integer-Arithmetic-Only Convolutional Neural Networks", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integer-arithmetic-only networks have been demonstrated effective to reduce\ncomputational cost and to ensure cross-platform consistency. However, previous\nworks usually report a decline in the inference accuracy when converting\nwell-trained floating-point-number (FPN) networks into integer networks. We\nanalyze this phonomenon and find that the decline is due to activation\nquantization. Specifically, when we replace conventional ReLU with Bounded\nReLU, how to set the bound for each neuron is a key problem. Considering the\ntradeoff between activation quantization error and network learning ability, we\nset an empirical rule to tune the bound of each Bounded ReLU. We also design a\nmechanism to handle the cases of feature map addition and feature map\nconcatenation. Based on the proposed method, our trained 8-bit integer ResNet\noutperforms the 8-bit networks of Google's TensorFlow and NVIDIA's TensorRT for\nimage recognition. We also experiment on VDSR for image super-resolution and on\nVRCNN for compression artifact reduction, both of which serve for regression\ntasks that natively require high inference accuracy. Our integer networks\nachieve equivalent performance as the corresponding FPN networks, but have only\n1/4 memory cost and run 2x faster on modern GPUs. Our code and models can be\nfound at github.com/HengRuiZ/brelu.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 08:23:03 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zhao", "Hengrui", ""], ["Liu", "Dong", ""], ["Li", "Houqiang", ""]]}, {"id": "2006.11739", "submitter": "Andrei Shadrikov", "authors": "Andrei Shadrikov", "title": "Achieving Better Kinship Recognition Through Better Baseline", "comments": "Accepted for the 4th Recognizing Families In the Wild Workshop", "journal-ref": null, "doi": "10.1109/FG47880.2020.00137", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing blood relations using face images can be seen as an application\nof face recognition systems with additional restrictions. These restrictions\nproved to be difficult to deal with, however, recent advancements in face\nverification show that there is still much to gain using more data and novel\nideas. As a result face recognition is a great source domain from which we can\ntransfer the knowledge to get better performance in kinship recognition as a\nsource domain. We present a new baseline for an automatic kinship recognition\ntask and relatives search based on RetinaFace[1] for face registration and\nArcFace[2] face verification model. With the approach described above as the\nfoundation, we constructed a pipeline that achieved state-of-the-art\nperformance on two tracks in the recent Recognizing Families In the Wild Data\nChallenge.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 08:40:53 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Shadrikov", "Andrei", ""]]}, {"id": "2006.11747", "submitter": "Zhiyuan Fang", "authors": "Zhiyuan Fang, Shu Kong, Zhe Wang, Charless Fowlkes, Yezhou Yang", "title": "Weak Supervision and Referring Attention for Temporal-Textual\n  Association Learning", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system capturing the association between video frames and textual queries\noffer great potential for better video analysis. However, training such a\nsystem in a fully supervised way inevitably demands a meticulously curated\nvideo dataset with temporal-textual annotations. Therefore we provide a\nWeak-Supervised alternative with our proposed Referring Attention mechanism to\nlearn temporal-textual association (dubbed WSRA). The weak supervision is\nsimply a textual expression (e.g., short phrases or sentences) at video level,\nindicating this video contains relevant frames. The referring attention is our\ndesigned mechanism acting as a scoring function for grounding the given queries\nover frames temporally. It consists of multiple novel losses and sampling\nstrategies for better training. The principle in our designed mechanism is to\nfully exploit 1) the weak supervision by considering informative and\ndiscriminative cues from intra-video segments anchored with the textual query,\n2) multiple queries compared to the single video, and 3) cross-video visual\nsimilarities. We validate our WSRA through extensive experiments for temporally\ngrounding by languages, demonstrating that it outperforms the state-of-the-art\nweakly-supervised methods notably.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 09:25:28 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 08:19:35 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Fang", "Zhiyuan", ""], ["Kong", "Shu", ""], ["Wang", "Zhe", ""], ["Fowlkes", "Charless", ""], ["Yang", "Yezhou", ""]]}, {"id": "2006.11757", "submitter": "Shubhajit Basak", "authors": "Shubhajit Basak, Hossein Javidnia, Faisal Khan, Rachel McDonnell,\n  Michael Schukat", "title": "Methodology for Building Synthetic Datasets with Virtual Humans", "comments": "Conference - ISSC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in deep learning methods have increased the performance of\nface detection and recognition systems. The accuracy of these models relies on\nthe range of variation provided in the training data. Creating a dataset that\nrepresents all variations of real-world faces is not feasible as the control\nover the quality of the data decreases with the size of the dataset.\nRepeatability of data is another challenge as it is not possible to exactly\nrecreate 'real-world' acquisition conditions outside of the laboratory. In this\nwork, we explore a framework to synthetically generate facial data to be used\nas part of a toolchain to generate very large facial datasets with a high\ndegree of control over facial and environmental variations. Such large datasets\ncan be used for improved, targeted training of deep neural networks. In\nparticular, we make use of a 3D morphable face model for the rendering of\nmultiple 2D images across a dataset of 100 synthetic identities, providing full\ncontrol over image variations such as pose, illumination, and background.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 10:29:36 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Basak", "Shubhajit", ""], ["Javidnia", "Hossein", ""], ["Khan", "Faisal", ""], ["McDonnell", "Rachel", ""], ["Schukat", "Michael", ""]]}, {"id": "2006.11767", "submitter": "Mahesh Pal Dr.", "authors": "Mahesh Pal, Akshay, Himanshu Rohilla and B. Charan Teja", "title": "Patch Based Classification of Remote Sensing Data: A Comparison of\n  2D-CNN, SVM and NN Classifiers", "comments": "8 pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel based algorithms including back propagation neural networks (NN) and\nsupport vector machines (SVM) have been widely used for remotely sensed image\nclassifications. Within last few years, deep learning based image classifier\nlike convolution neural networks (2D-CNN) are becoming popular alternatives to\nthese classifiers. In this paper, we compare performance of patch based SVM and\nNN with that of a deep learning algorithms comprising of 2D-CNN and fully\nconnected layers. Similar to CNN which utilise image patches to derive features\nfor further classification, we propose to use patches as an input in place of\nindividual pixel with both SVM and NN classifiers. Two datasets, one\nmultispectral and other hyperspectral data was used to compare the performance\nof different classifiers. Results with both datasets suggest the effectiveness\nof patch based SVM and NN classifiers in comparison to state of art 2D-CNN\nclassifier.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 11:07:37 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Pal", "Mahesh", ""], ["Akshay", "", ""], ["Rohilla", "Himanshu", ""], ["Teja", "B. Charan", ""]]}, {"id": "2006.11802", "submitter": "Yu Feng", "authors": "Yu Feng, Claus Brenner, Monika Sester", "title": "Flood severity mapping from Volunteered Geographic Information by\n  interpreting water level from images containing people: a case study of\n  Hurricane Harvey", "comments": null, "journal-ref": null, "doi": "10.1016/j.isprsjprs.2020.09.011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing urbanization, in recent years there has been a growing\ninterest and need in monitoring and analyzing urban flood events. Social media,\nas a new data source, can provide real-time information for flood monitoring.\nThe social media posts with locations are often referred to as Volunteered\nGeographic Information (VGI), which can reveal the spatial pattern of such\nevents. Since more images are shared on social media than ever before, recent\nresearch focused on the extraction of flood-related posts by analyzing images\nin addition to texts. Apart from merely classifying posts as flood relevant or\nnot, more detailed information, e.g. the flood severity, can also be extracted\nbased on image interpretation. However, it has been less tackled and has not\nyet been applied for flood severity mapping.\n  In this paper, we propose a novel three-step process to extract and map flood\nseverity information. First, flood relevant images are retrieved with the help\nof pre-trained convolutional neural networks as feature extractors. Second, the\nimages containing people are further classified into four severity levels by\nobserving the relationship between body parts and their partial inundation,\ni.e. images are classified according to the water level with respect to\ndifferent body parts, namely ankle, knee, hip, and chest. Lastly, locations of\nthe Tweets are used for generating a map of estimated flood extent and\nseverity. This process was applied to an image dataset collected during\nHurricane Harvey in 2017, as a proof of concept. The results show that VGI can\nbe used as a supplement to remote sensing observations for flood extent mapping\nand is beneficial, especially for urban areas, where the infrastructure is\noften occluding water. Based on the extracted water level information, an\nintegrated overview of flood severity can be provided for the early stages of\nemergency response.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 13:57:42 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 16:11:42 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Feng", "Yu", ""], ["Brenner", "Claus", ""], ["Sester", "Monika", ""]]}, {"id": "2006.11807", "submitter": "Zhan Shi", "authors": "Zhan Shi, Xu Zhou, Xipeng Qiu, Xiaodan Zhu", "title": "Improving Image Captioning with Better Use of Captions", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning is a multimodal problem that has drawn extensive attention\nin both the natural language processing and computer vision community. In this\npaper, we present a novel image captioning architecture to better explore\nsemantics available in captions and leverage that to enhance both image\nrepresentation and caption generation. Our models first construct\ncaption-guided visual relationship graphs that introduce beneficial inductive\nbias using weakly supervised multi-instance learning. The representation is\nthen enhanced with neighbouring and contextual nodes with their textual and\nvisual features. During generation, the model further incorporates visual\nrelationships using multi-task learning for jointly predicting word and\nobject/predicate tag sequences. We perform extensive experiments on the MSCOCO\ndataset, showing that the proposed framework significantly outperforms the\nbaselines, resulting in the state-of-the-art performance under a wide range of\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 14:10:47 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Shi", "Zhan", ""], ["Zhou", "Xu", ""], ["Qiu", "Xipeng", ""], ["Zhu", "Xiaodan", ""]]}, {"id": "2006.11808", "submitter": "MinSeok Seo", "authors": "Minseok Seo, Jaemin Lee, Jongchan Park, Dong-Geol Choi", "title": "Sequential Feature Filtering Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Sequential Feature Filtering Classifier (FFC), a simple but\neffective classifier for convolutional neural networks (CNNs). With sequential\nLayerNorm and ReLU, FFC zeroes out low-activation units and preserves\nhigh-activation units. The sequential feature filtering process generates\nmultiple features, which are fed into a shared classifier for multiple outputs.\nFFC can be applied to any CNNs with a classifier, and significantly improves\nperformances with negligible overhead. We extensively validate the efficacy of\nFFC on various tasks: ImageNet-1K classification, MS COCO detection, Cityscapes\nsegmentation, and HMDB51 action recognition. Moreover, we empirically show that\nFFC can further improve performances upon other techniques, including attention\nmodules and augmentation techniques. The code and models will be publicly\navailable.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 14:31:45 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Seo", "Minseok", ""], ["Lee", "Jaemin", ""], ["Park", "Jongchan", ""], ["Choi", "Dong-Geol", ""]]}, {"id": "2006.11812", "submitter": "Giancarlo Paoletti", "authors": "Giancarlo Paoletti, Jacopo Cavazza, Cigdem Beyan and Alessio Del Bue", "title": "Subspace Clustering for Action Recognition with Covariance\n  Representations and Temporal Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of human action recognition, defined as\nclassifying which action is displayed in a trimmed sequence, from skeletal\ndata. Albeit state-of-the-art approaches designed for this application are all\nsupervised, in this paper we pursue a more challenging direction: Solving the\nproblem with unsupervised learning. To this end, we propose a novel subspace\nclustering method, which exploits covariance matrix to enhance the action's\ndiscriminability and a timestamp pruning approach that allow us to better\nhandle the temporal dimension of the data. Through a broad experimental\nvalidation, we show that our computational pipeline surpasses existing\nunsupervised approaches but also can result in favorable performances as\ncompared to supervised methods.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 14:44:03 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Paoletti", "Giancarlo", ""], ["Cavazza", "Jacopo", ""], ["Beyan", "Cigdem", ""], ["Del Bue", "Alessio", ""]]}, {"id": "2006.11825", "submitter": "Yecheng Lyu", "authors": "Yecheng Lyu, Ming Li, Xinming Huang, Ulkuhan Guler, Patrick Schaumont,\n  Ziming Zhang", "title": "TreeRNN: Topology-Preserving Deep GraphEmbedding and Learning", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General graphs are difficult for learning due to their irregular structures.\nExisting works employ message passing along graph edges to extract local\npatterns using customized graph kernels, but few of them are effective for the\nintegration of such local patterns into global features. In contrast, in this\npaper we study the methods to transfer the graphs into trees so that explicit\norders are learned to direct the feature integration from local to global. To\nthis end, we apply the breadth first search (BFS) to construct trees from the\ngraphs, which adds direction to the graph edges from the center node to the\nperipheral nodes. In addition, we proposed a novel projection scheme that\ntransfer the trees to image representations, which is suitable for conventional\nconvolution neural networks (CNNs) and recurrent neural networks (RNNs). To\nbest learn the patterns from the graph-tree-images, we propose TreeRNN, a 2D\nRNN architecture that recurrently integrates the image pixels by rows and\ncolumns to help classify the graph categories. We evaluate the proposed method\non several graph classification datasets, and manage to demonstrate comparable\naccuracy with the state-of-the-art on MUTAG, PTC-MR and NCI1 datasets.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 15:22:24 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 13:45:36 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Lyu", "Yecheng", ""], ["Li", "Ming", ""], ["Huang", "Xinming", ""], ["Guler", "Ulkuhan", ""], ["Schaumont", "Patrick", ""], ["Zhang", "Ziming", ""]]}, {"id": "2006.11840", "submitter": "Sizhuo Ma", "authors": "Sizhuo Ma, Shantanu Gupta, Arin C. Ulku, Claudio Bruschini, Edoardo\n  Charbon, Mohit Gupta", "title": "Quanta Burst Photography", "comments": "A version with better-quality images can be found on the project\n  webpage: http://wisionlab.cs.wisc.edu/project/quanta-burst-photography/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-photon avalanche diodes (SPADs) are an emerging sensor technology\ncapable of detecting individual incident photons, and capturing their\ntime-of-arrival with high timing precision. While these sensors were limited to\nsingle-pixel or low-resolution devices in the past, recently, large (up to 1\nMPixel) SPAD arrays have been developed. These single-photon cameras (SPCs) are\ncapable of capturing high-speed sequences of binary single-photon images with\nno read noise. We present quanta burst photography, a computational photography\ntechnique that leverages SPCs as passive imaging devices for photography in\nchallenging conditions, including ultra low-light and fast motion. Inspired by\nrecent success of conventional burst photography, we design algorithms that\nalign and merge binary sequences captured by SPCs into intensity images with\nminimal motion blur and artifacts, high signal-to-noise ratio (SNR), and high\ndynamic range. We theoretically analyze the SNR and dynamic range of quanta\nburst photography, and identify the imaging regimes where it provides\nsignificant benefits. We demonstrate, via a recently developed SPAD array, that\nthe proposed method is able to generate high-quality images for scenes with\nchallenging lighting, complex geometries, high dynamic range and moving\nobjects. With the ongoing development of SPAD arrays, we envision quanta burst\nphotography finding applications in both consumer and scientific photography.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 16:20:29 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Ma", "Sizhuo", ""], ["Gupta", "Shantanu", ""], ["Ulku", "Arin C.", ""], ["Bruschini", "Claudio", ""], ["Charbon", "Edoardo", ""], ["Gupta", "Mohit", ""]]}, {"id": "2006.11843", "submitter": "Sanghoon Lee", "authors": "Sanghoon Lee, Colton Farley, Simon Shim, Yanjun Zhao, Wookjin Choi,\n  Wook-Sung Yoo", "title": "Unsupervised Learning of Deep-Learned Features from Breast Cancer Images", "comments": "7 pages for IEEE BIBE", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting cancer manually in whole slide images requires significant time and\neffort on the laborious process. Recent advances in whole slide image analysis\nhave stimulated the growth and development of machine learning-based approaches\nthat improve the efficiency and effectiveness in the diagnosis of cancer\ndiseases. In this paper, we propose an unsupervised learning approach for\ndetecting cancer in breast invasive carcinoma (BRCA) whole slide images. The\nproposed method is fully automated and does not require human involvement\nduring the unsupervised learning procedure. We demonstrate the effectiveness of\nthe proposed approach for cancer detection in BRCA and show how the machine can\nchoose the most appropriate clusters during the unsupervised learning\nprocedure. Moreover, we present a prototype application that enables users to\nselect relevant groups mapping all regions related to the groups in whole slide\nimages.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 16:38:36 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Lee", "Sanghoon", ""], ["Farley", "Colton", ""], ["Shim", "Simon", ""], ["Zhao", "Yanjun", ""], ["Choi", "Wookjin", ""], ["Yoo", "Wook-Sung", ""]]}, {"id": "2006.11851", "submitter": "Syed Muhammad Arsalan Bashir Mr.", "authors": "Syed Muhammad Arsalan Bashir and Farhan Ali Khan Ghouri", "title": "Perspective Texture Synthesis Based on Improved Energy Optimization", "comments": "Published in PLOS One", "journal-ref": "PLoS ONE 9 (10), e110622 (2014)", "doi": "10.1371/journal.pone.0110622", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Perspective texture synthesis has great significance in many fields like\nvideo editing, scene capturing etc., due to its ability to read and control\nglobal feature information. In this paper, we present a novel example-based,\nspecifically energy optimization-based algorithm, to synthesize perspective\ntextures. Energy optimization technique is a pixel-based approach, so it is\ntime-consuming. We improve it from two aspects with the purpose of achieving\nfaster synthesis and high quality. Firstly, we change this pixel-based\ntechnique by replacing the pixel computation with a little patch. Secondly, we\npresent a novel technique to accelerate searching nearest neighborhoods in\nenergy optimization. Using k- means clustering technique to build a search tree\nto accelerate the search. Hence, we make use of principal component analysis\n(PCA) technique to reduce dimensions of input vectors. The high quality results\nprove that our approach is feasible. Besides, our proposed algorithm needs\nshorter time relative to other similar methods.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 17:12:58 GMT"}], "update_date": "2020-06-28", "authors_parsed": [["Bashir", "Syed Muhammad Arsalan", ""], ["Ghouri", "Farhan Ali Khan", ""]]}, {"id": "2006.11863", "submitter": "Shivangi Aneja Ms", "authors": "Shivangi Aneja and Matthias Nie{\\ss}ner", "title": "Generalized Zero and Few-Shot Transfer for Facial Forgery Detection", "comments": "Project page: https://shivangi-aneja.github.io/ddt/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Deep Distribution Transfer(DDT), a new transfer learning approach\nto address the problem of zero and few-shot transfer in the context of facial\nforgery detection. We examine how well a model (pre-)trained with one forgery\ncreation method generalizes towards a previously unseen manipulation technique\nor different dataset. To facilitate this transfer, we introduce a new mixture\nmodel-based loss formulation that learns a multi-modal distribution, with modes\ncorresponding to class categories of the underlying data of the source forgery\nmethod. Our core idea is to first pre-train an encoder neural network, which\nmaps each mode of this distribution to the respective class labels, i.e., real\nor fake images in the source domain by minimizing wasserstein distance between\nthem. In order to transfer this model to a new domain, we associate a few\ntarget samples with one of the previously trained modes. In addition, we\npropose a spatial mixup augmentation strategy that further helps generalization\nacross domains. We find this learning strategy to be surprisingly effective at\ndomain transfer compared to a traditional classification or even\nstate-of-the-art domain adaptation/few-shot learning methods. For instance,\ncompared to the best baseline, our method improves the classification accuracy\nby 4.88% for zero-shot and by 8.38% for the few-shot case transferred from the\nFaceForensics++ to Dessa dataset.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 18:10:52 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Aneja", "Shivangi", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2006.11921", "submitter": "Pramuditha Perera", "authors": "Pramuditha Perera, Julian Fierrez, Vishal M. Patel", "title": "Quickest Intruder Detection for Multiple User Active Authentication", "comments": "Accepted for publication in ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate how to detect intruders with low latency for\nActive Authentication (AA) systems with multiple-users. We extend the Quickest\nChange Detection (QCD) framework to the multiple-user case and formulate the\nMultiple-user Quickest Intruder Detection (MQID) algorithm. Furthermore, we\nextend the algorithm to the data-efficient scenario where intruder detection is\ncarried out with fewer observation samples. We evaluate the effectiveness of\nthe proposed method on two publicly available AA datasets on the face modality.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 21:59:01 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Perera", "Pramuditha", ""], ["Fierrez", "Julian", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2006.11930", "submitter": "Zhihui Guo", "authors": "Zhihui Guo, Honghai Zhang, Zhi Chen, Ellen van der Plas, Laurie\n  Gutmann, Daniel Thedens, Peggy Nopoulos, Milan Sonka", "title": "Fully Automated 3D Segmentation of MR-Imaged Calf Muscle Compartments:\n  Neighborhood Relationship Enhanced Fully Convolutional Network", "comments": "To be appeared in journal Computerized Medical Imaging and Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated segmentation of individual calf muscle compartments from 3D\nmagnetic resonance (MR) images is essential for developing quantitative\nbiomarkers for muscular disease progression and its prediction. Achieving\nclinically acceptable results is a challenging task due to large variations in\nmuscle shape and MR appearance. Although deep convolutional neural networks\n(DCNNs) achieved improved accuracy in various image segmentation tasks, certain\nproblems such as utilizing long-range information and incorporating high-level\nconstraints remain unsolved. We present a novel fully convolutional network\n(FCN), called FilterNet, that utilizes contextual information in a large\nneighborhood and embeds edge-aware constraints for individual calf muscle\ncompartment segmentations. An encoder-decoder architecture with flexible\nbackbone blocks is used to systematically enlarge convolution receptive field\nand preserve information at all resolutions. Edge positions derived from the\nFCN output muscle probability maps are explicitly regularized using\nkernel-based edge detection in an end-to-end optimization framework. Our\nFilterNet was evaluated on 40 T1-weighted MR images of 10 healthy and 30\ndiseased subjects by 4-fold cross-validation. Mean DICE coefficients of\n88.00%--91.29% and mean absolute surface positioning errors of 1.04--1.66 mm\nwere achieved for the five 3D muscle compartments.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 22:53:58 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 22:15:25 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Guo", "Zhihui", ""], ["Zhang", "Honghai", ""], ["Chen", "Zhi", ""], ["van der Plas", "Ellen", ""], ["Gutmann", "Laurie", ""], ["Thedens", "Daniel", ""], ["Nopoulos", "Peggy", ""], ["Sonka", "Milan", ""]]}, {"id": "2006.11933", "submitter": "Shota Sakaguchi", "authors": "Shota Sakaguchi, Jun Kato, Masataka Goto, and Seiichi Uchida", "title": "Lyric Video Analysis Using Text Detection and Tracking", "comments": "15 pages, 8 figures, DAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We attempt to recognize and track lyric words in lyric videos. Lyric video is\na music video showing the lyric words of a song. The main characteristic of\nlyric videos is that the lyric words are shown at frames synchronously with the\nmusic. The difficulty of recognizing and tracking the lyric words is that (1)\nthe words are often decorated and geometrically distorted and (2) the words\nmove arbitrarily and drastically in the video frame. The purpose of this paper\nis to analyze the motion of the lyric words in lyric videos, as the first step\nof automatic lyric video generation. In order to analyze the motion of lyric\nwords, we first apply a state-of-the-art scene text detector and recognizer to\neach video frame. Then, lyric-frame matching is performed to establish the\noptimal correspondence between lyric words and the frames. After fixing the\nmotion trajectories of individual lyric words from correspondence, we analyze\nthe trajectories of the lyric words by k-medoids clustering and dynamic time\nwarping (DTW).\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 22:57:09 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Sakaguchi", "Shota", ""], ["Kato", "Jun", ""], ["Goto", "Masataka", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2006.11971", "submitter": "Svetlana Yanushkevich", "authors": "Svetlana Yanushkevich, Shawn Eastwood, Kenneth Lai, Vlad Shmerko", "title": "Emerging Biometrics: Deep Inference and Other Computational Intelligence", "comments": "Survey paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at identifying emerging computational intelligence trends for\nthe design and modeling of complex biometric-enabled infrastructure and\nsystems. Biometric-enabled systems are evolving towards deep learning and deep\ninference using the principles of adaptive computing, - the front tides of the\nmodern computational intelligence domain. Therefore, we focus on intelligent\ninference engines widely deployed in biometrics. Computational intelligence\napplications that cover a wide spectrum of biometric tasks using physiological\nand behavioral traits are chosen for illustration. We highlight the technology\ngaps that must be addressed in future generations of biometric systems. The\nreported approaches and results primarily address the researchers who work\ntowards developing the next generation of intelligent biometric-enabled\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 02:35:00 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Yanushkevich", "Svetlana", ""], ["Eastwood", "Shawn", ""], ["Lai", "Kenneth", ""], ["Shmerko", "Vlad", ""]]}, {"id": "2006.11979", "submitter": "Rahul Duggal", "authors": "Rahul Duggal, Scott Freitas, Sunny Dhamnani, Duen Horng Chau, Jimeng\n  Sun", "title": "ELF: An Early-Exiting Framework for Long-Tailed Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The natural world often follows a long-tailed data distribution where only a\nfew classes account for most of the examples. This long-tail causes classifiers\nto overfit to the majority class. To mitigate this, prior solutions commonly\nadopt class rebalancing strategies such as data resampling and loss reshaping.\nHowever, by treating each example within a class equally, these methods fail to\naccount for the important notion of example hardness, i.e., within each class\nsome examples are easier to classify than others. To incorporate this notion of\nhardness into the learning process, we propose the EarLy-exiting\nFramework(ELF). During training, ELF learns to early-exit easy examples through\nauxiliary branches attached to a backbone network. This offers a dual\nbenefit-(1) the neural network increasingly focuses on hard examples, since\nthey contribute more to the overall network loss; and (2) it frees up\nadditional model capacity to distinguish difficult examples. Experimental\nresults on two large-scale datasets, ImageNet LT and iNaturalist'18,\ndemonstrate that ELF can improve state-of-the-art accuracy by more than 3\npercent. This comes with the additional benefit of reducing up to 20 percent of\ninference time FLOPS. ELF is complementary to prior work and can naturally\nintegrate with a variety of existing methods to tackle the challenge of\nlong-tailed distributions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 02:54:26 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2020 17:12:03 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Duggal", "Rahul", ""], ["Freitas", "Scott", ""], ["Dhamnani", "Sunny", ""], ["Chau", "Duen Horng", ""], ["Sun", "Jimeng", ""]]}, {"id": "2006.11988", "submitter": "Joseph Paul Cohen", "authors": "Joseph Paul Cohen and Paul Morrison and Lan Dao and Karsten Roth and\n  Tim Q Duong and Marzyeh Ghassemi", "title": "COVID-19 Image Data Collection: Prospective Predictions Are the Future", "comments": "Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org. Code for baseline\n  experiments can be found here: https://github.com/mlmed/covid-baselines", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Across the world's coronavirus disease 2019 (COVID-19) hot spots, the need to\nstreamline patient diagnosis and management has become more pressing than ever.\nAs one of the main imaging tools, chest X-rays (CXRs) are common, fast,\nnon-invasive, relatively cheap, and potentially bedside to monitor the\nprogression of the disease. This paper describes the first public COVID-19\nimage data collection as well as a preliminary exploration of possible use\ncases for the data. This dataset currently contains hundreds of frontal view\nX-rays and is the largest public resource for COVID-19 image and prognostic\ndata, making it a necessary resource to develop and evaluate tools to aid in\nthe treatment of COVID-19. It was manually aggregated from publication figures\nas well as various web based repositories into a machine learning (ML) friendly\nformat with accompanying dataloader code. We collected frontal and lateral view\nimagery and metadata such as the time since first symptoms, intensive care unit\n(ICU) status, survival status, intubation status, or hospital location. We\npresent multiple possible use cases for the data such as predicting the need\nfor the ICU, predicting patient survival, and understanding a patient's\ntrajectory during treatment. Data can be accessed here:\nhttps://github.com/ieee8023/covid-chestxray-dataset\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 03:20:36 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 20:31:04 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 18:52:43 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Cohen", "Joseph Paul", ""], ["Morrison", "Paul", ""], ["Dao", "Lan", ""], ["Roth", "Karsten", ""], ["Duong", "Tim Q", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "2006.11989", "submitter": "Jie An", "authors": "Jie An, Tianlang Chen, Songyang Zhang, and Jiebo Luo", "title": "Global Image Sentiment Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring the sentiment of an image is an unexplored research topic in the\narea of computer vision. This work proposes a novel framework consisting of a\nreference image retrieval step and a global sentiment transfer step to transfer\nsentiments of images according to a given sentiment tag. The proposed image\nretrieval algorithm is based on the SSIM index. The retrieved reference images\nby the proposed algorithm are more content-related against the algorithm based\non the perceptual loss. Therefore can lead to a better image sentiment transfer\nresult. In addition, we propose a global sentiment transfer step, which employs\nan optimization algorithm to iteratively transfer sentiment of images based on\nfeature maps produced by the Densenet121 architecture. The proposed sentiment\ntransfer algorithm can transfer the sentiment of images while ensuring the\ncontent structure of the input image intact. The qualitative and quantitative\nexperiments demonstrate that the proposed sentiment transfer framework\noutperforms existing artistic and photorealistic style transfer algorithms in\nmaking reliable sentiment transfer results with rich and exact details.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 03:22:25 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["An", "Jie", ""], ["Chen", "Tianlang", ""], ["Zhang", "Songyang", ""], ["Luo", "Jiebo", ""]]}, {"id": "2006.11993", "submitter": "Andrew Berlin", "authors": "Andrew A. Berlin, Mon Young, Ahmed El Kaffas, Sam Gambhir, Amelie\n  Lutz, Maria Luigia Storto, and Juergen Willmann", "title": "Computational Enhancement of Molecularly Targeted Contrast-Enhanced\n  Ultrasound: Application to Human Breast Tumor Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecularly targeted contrast enhanced ultrasound (mCEUS) is a clinically\npromising approach for early cancer detection through targeted imaging of\nVEGFR2 (KDR) receptors. We have developed computational enhancement techniques\nfor mCEUS tailored to address the unique challenges of imaging contrast\naccumulation in humans. These techniques utilize dynamic analysis to\ndistinguish molecularly bound contrast agent from other contrast-mode signal\nsources, enabling analysis of contrast agent accumulation to be performed\nduring contrast bolus arrival when the signal due to molecular binding is\nstrongest.\n  Applied to the 18 human patient examinations of the first-in-human molecular\nultrasound breast lesion study, computational enhancement improved the ability\nto differentiate between pathology-proven lesion and pathology-proven normal\ntissue in real-world human examination conditions that involved both patient\nand probe motion, with improvements in contrast ratio between lesion and normal\ntissue that in most cases exceed an order of magnitude (10x). Notably,\ncomputational enhancement eliminated a false positive result in which tissue\nleakage signal was misinterpreted by radiologists to be contrast agent\naccumulation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 03:45:52 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Berlin", "Andrew A.", ""], ["Young", "Mon", ""], ["Kaffas", "Ahmed El", ""], ["Gambhir", "Sam", ""], ["Lutz", "Amelie", ""], ["Storto", "Maria Luigia", ""], ["Willmann", "Juergen", ""]]}, {"id": "2006.11999", "submitter": "Yaolong Wang", "authors": "Yaolong Wang, Mingqing Xiao, Chang Liu, Shuxin Zheng, Tie-Yan Liu", "title": "Modeling Lost Information in Lossy Image Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy image compression is one of the most commonly used operators for\ndigital images. Most recently proposed deep-learning-based image compression\nmethods leverage the auto-encoder structure, and reach a series of promising\nresults in this field. The images are encoded into low dimensional latent\nfeatures first, and entropy coded subsequently by exploiting the statistical\nredundancy. However, the information lost during encoding is unfortunately\ninevitable, which poses a significant challenge to the decoder to reconstruct\nthe original images. In this work, we propose a novel invertible framework\ncalled Invertible Lossy Compression (ILC) to largely mitigate the information\nloss problem. Specifically, ILC introduces an invertible encoding module to\nreplace the encoder-decoder structure to produce the low dimensional\ninformative latent representation, meanwhile, transform the lost information\ninto an auxiliary latent variable that won't be further coded or stored. The\nlatent representation is quantized and encoded into bit-stream, and the latent\nvariable is forced to follow a specified distribution, i.e. isotropic Gaussian\ndistribution. In this way, recovering the original image is made tractable by\neasily drawing a surrogate latent variable and applying the inverse pass of the\nmodule with the sampled variable and decoded latent features. Experimental\nresults demonstrate that with a new component replacing the auto-encoder in\nimage compression methods, ILC can significantly outperform the baseline method\non extensive benchmark datasets by combining with the existing compression\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 04:04:56 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 08:49:06 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 01:55:56 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Wang", "Yaolong", ""], ["Xiao", "Mingqing", ""], ["Liu", "Chang", ""], ["Zheng", "Shuxin", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2006.12004", "submitter": "Ankit Kariryaa", "authors": "Ankit Kariryaa", "title": "MaskIt: Masking for efficient utilization of incomplete public datasets\n  for training deep learning models", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in training deep learning models is the lack of high\nquality and complete datasets. In the paper, we present a masking approach for\ntraining deep learning models from a publicly available but incomplete dataset.\nFor example, city of Hamburg, Germany maintains a list of trees along the\nroads, but this dataset does not contain any information about trees in private\nhomes and parks. To train a deep learning model on such a dataset, we mask the\nstreet trees and aerial images with the road network. Road network used for\ncreating the mask is downloaded from OpenStreetMap, and it marks the area where\nthe training data is available. The mask is passed to the model as one of the\ninputs and it also coats the output. Our model learns to successfully predict\ntrees only in the masked region with 78.4% accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 04:49:41 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kariryaa", "Ankit", ""]]}, {"id": "2006.12009", "submitter": "Xin Jin", "authors": "Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen", "title": "Feature Alignment and Restoration for Domain Generalization and\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For domain generalization (DG) and unsupervised domain adaptation (UDA),\ncross domain feature alignment has been widely explored to pull the feature\ndistributions of different domains in order to learn domain-invariant\nrepresentations. However, the feature alignment is in general task-ignorant and\ncould result in degradation of the discrimination power of the feature\nrepresentation and thus hinders the high performance. In this paper, we propose\na unified framework termed Feature Alignment and Restoration (FAR) to\nsimultaneously ensure high generalization and discrimination power of the\nnetworks for effective DG and UDA. Specifically, we perform feature alignment\n(FA) across domains by aligning the moments of the distributions of attentively\nselected features to reduce their discrepancy. To ensure high discrimination,\nwe propose a Feature Restoration (FR) operation to distill task-relevant\nfeatures from the residual information and use them to compensate for the\naligned features. For better disentanglement, we enforce a dual ranking entropy\nloss constraint in the FR step to encourage the separation of task-relevant and\ntask-irrelevant features. Extensive experiments on multiple classification\nbenchmarks demonstrate the high performance and strong generalization of our\nFAR framework for both domain generalization and unsupervised domain\nadaptation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 05:08:13 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Jin", "Xin", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Chen", "Zhibo", ""]]}, {"id": "2006.12015", "submitter": "Hujie Pan", "authors": "Hujie Pan, Zining Wang, Wei Zhan, Masayoshi Tomizuka", "title": "Towards Better Performance and More Explainable Uncertainty for 3D\n  Object Detection of Autonomous Vehicles", "comments": "ITSC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel form of the loss function to increase the\nperformance of LiDAR-based 3d object detection and obtain more explainable and\nconvincing uncertainty for the prediction. The loss function was designed using\ncorner transformation and uncertainty modeling. With the new loss function, the\nperformance of our method on the val split of KITTI dataset shows up to a 15%\nincrease in terms of Average Precision (AP) comparing with the baseline using\nsimple L1 Loss. In the study of the characteristics of predicted uncertainties,\nwe find that generally more accurate prediction of the bounding box is usually\naccompanied by lower uncertainty. The distribution of corner uncertainties\nagrees on the distribution of the point cloud in the bounding box, which means\nthe corner with denser observed points has lower uncertainty. Moreover, our\nmethod also learns the constraint from the cuboid geometry of the bounding box\nin uncertainty prediction. Finally, we propose an efficient Bayesian updating\nmethod to recover the uncertainty for the original parameters of the bounding\nboxes which can help to provide probabilistic results for the planning module.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 05:49:58 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 21:37:14 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Pan", "Hujie", ""], ["Wang", "Zining", ""], ["Zhan", "Wei", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "2006.12030", "submitter": "Yangyan Li", "authors": "Jinming Cao, Yangyan Li, Mingchao Sun, Ying Chen, Dani Lischinski,\n  Daniel Cohen-Or, Baoquan Chen, Changhe Tu", "title": "DO-Conv: Depthwise Over-parameterized Convolutional Layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional layers are the core building blocks of Convolutional Neural\nNetworks (CNNs). In this paper, we propose to augment a convolutional layer\nwith an additional depthwise convolution, where each input channel is convolved\nwith a different 2D kernel. The composition of the two convolutions constitutes\nan over-parameterization, since it adds learnable parameters, while the\nresulting linear operation can be expressed by a single convolution layer. We\nrefer to this depthwise over-parameterized convolutional layer as DO-Conv. We\nshow with extensive experiments that the mere replacement of conventional\nconvolutional layers with DO-Conv layers boosts the performance of CNNs on many\nclassical vision tasks, such as image classification, detection, and\nsegmentation. Moreover, in the inference phase, the depthwise convolution is\nfolded into the conventional convolution, reducing the computation to be\nexactly equivalent to that of a convolutional layer without\nover-parameterization. As DO-Conv introduces performance gains without\nincurring any computational complexity increase for inference, we advocate it\nas an alternative to the conventional convolutional layer. We open-source a\nreference implementation of DO-Conv in Tensorflow, PyTorch and GluonCV at\nhttps://github.com/yangyanli/DO-Conv.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 06:57:10 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Cao", "Jinming", ""], ["Li", "Yangyan", ""], ["Sun", "Mingchao", ""], ["Chen", "Ying", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Daniel", ""], ["Chen", "Baoquan", ""], ["Tu", "Changhe", ""]]}, {"id": "2006.12041", "submitter": "Ramanathan Subramanian", "authors": "Harshit Malik, Hersh Dhillon, Roland Goecke, Ramanathan Subramanian", "title": "Characterizing Hirability via Personality and Behavior", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While personality traits have been extensively modeled as behavioral\nconstructs, we model \\textbf{\\textit{job hirability}} as a \\emph{personality\nconstruct}. On the {\\emph{First Impressions Candidate Screening}} (FICS)\ndataset, we examine relationships among personality and hirability measures.\nModeling hirability as a discrete/continuous variable with the \\emph{big-five}\npersonality traits as predictors, we utilize (a) apparent personality\nannotations, and (b) personality estimates obtained via audio, visual and\ntextual cues for hirability prediction (HP). We also examine the efficacy of a\ntwo-step HP process involving (1) personality estimation from multimodal\nbehavioral cues, followed by (2) HP from personality estimates.\n  Interesting results from experiments performed on $\\approx$~5000 FICS videos\nare as follows. (1) For each of the \\emph{text}, \\emph{audio} and \\emph{visual}\nmodalities, HP via the above two-step process is more effective than directly\npredicting from behavioral cues. Superior results are achieved when hirability\nis modeled as a continuous vis-\\'a-vis categorical variable. (2) Among visual\ncues, eye and bodily information achieve performance comparable to face cues\nfor predicting personality and hirability. (3) Explanatory analyses reveal the\nimpact of multimodal behavior on personality impressions; \\eg,\nConscientiousness impressions are impacted by the use of \\emph{cuss words}\n(verbal behavior), and \\emph{eye movements} (non-verbal behavior), confirming\nprior observations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 07:24:22 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Malik", "Harshit", ""], ["Dhillon", "Hersh", ""], ["Goecke", "Roland", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "2006.12052", "submitter": "Na Zhao", "authors": "Na Zhao, Tat-Seng Chua, Gim Hee Lee", "title": "Few-shot 3D Point Cloud Semantic Segmentation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing approaches for 3D point cloud semantic segmentation are fully\nsupervised. These fully supervised approaches heavily rely on large amounts of\nlabeled training data that are difficult to obtain and cannot segment new\nclasses after training. To mitigate these limitations, we propose a novel\nattention-aware multi-prototype transductive few-shot point cloud semantic\nsegmentation method to segment new classes given a few labeled examples.\nSpecifically, each class is represented by multiple prototypes to model the\ncomplex data distribution of labeled points. Subsequently, we employ a\ntransductive label propagation method to exploit the affinities between labeled\nmulti-prototypes and unlabeled points, and among the unlabeled points.\nFurthermore, we design an attention-aware multi-level feature learning network\nto learn the discriminative features that capture the geometric dependencies\nand semantic correlations between points. Our proposed method shows significant\nand consistent improvements compared to baselines in different few-shot point\ncloud semantic segmentation settings (i.e., 2/3-way 1/5-shot) on two benchmark\ndatasets. Our code is available at https://github.com/Na-Z/attMPTI.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 08:05:25 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 05:55:06 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhao", "Na", ""], ["Chua", "Tat-Seng", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2006.12057", "submitter": "Hiroharu Kato", "authors": "Hiroharu Kato, Deniz Beker, Mihai Morariu, Takahiro Ando, Toru\n  Matsuoka, Wadim Kehl, Adrien Gaidon", "title": "Differentiable Rendering: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have shown remarkable performance improvements on\nvision-related tasks such as object detection or image segmentation. Despite\ntheir success, they generally lack the understanding of 3D objects which form\nthe image, as it is not always possible to collect 3D information about the\nscene or to easily annotate it. Differentiable rendering is a novel field which\nallows the gradients of 3D objects to be calculated and propagated through\nimages. It also reduces the requirement of 3D data collection and annotation,\nwhile enabling higher success rate in various applications. This paper reviews\nexisting literature and discusses the current state of differentiable\nrendering, its applications and open research problems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 08:14:52 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 00:01:27 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Kato", "Hiroharu", ""], ["Beker", "Deniz", ""], ["Morariu", "Mihai", ""], ["Ando", "Takahiro", ""], ["Matsuoka", "Toru", ""], ["Kehl", "Wadim", ""], ["Gaidon", "Adrien", ""]]}, {"id": "2006.12061", "submitter": "Lia Morra", "authors": "Fabio Garcea and Alessandro Cucco and Lia Morra and Fabrizio Lamberti", "title": "Object Tracking through Residual and Dense LSTMs", "comments": null, "journal-ref": "Proceedings of 17th International Conference On Image Analysis and\n  Recognition (ICIAR 2020)", "doi": "10.1007/978-3-030-50516-5_9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object tracking task is constantly gaining importance in several\nfields of application as traffic monitoring, robotics, and surveillance, to\nname a few. Dealing with changes in the appearance of the tracked object is\nparamount to achieve high tracking accuracy, and is usually achieved by\ncontinually learning features. Recently, deep learning-based trackers based on\nLSTMs (Long Short-Term Memory) recurrent neural networks have emerged as a\npowerful alternative, bypassing the need to retrain the feature extraction in\nan online fashion. Inspired by the success of residual and dense networks in\nimage recognition, we propose here to enhance the capabilities of hybrid\ntrackers using residual and/or dense LSTMs. By introducing skip connections, it\nis possible to increase the depth of the architecture while ensuring a fast\nconvergence. Experimental results on the Re3 tracker show that DenseLSTMs\noutperform Residual and regular LSTM, and offer a higher resilience to\nnuisances such as occlusions and out-of-view objects. Our case study supports\nthe adoption of residual-based RNNs for enhancing the robustness of other\ntrackers.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 08:20:17 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Garcea", "Fabio", ""], ["Cucco", "Alessandro", ""], ["Morra", "Lia", ""], ["Lamberti", "Fabrizio", ""]]}, {"id": "2006.12075", "submitter": "Mingyi Shi", "authors": "Mingyi Shi, Kfir Aberman, Andreas Aristidou, Taku Komura, Dani\n  Lischinski, Daniel Cohen-Or, Baoquan Chen", "title": "MotioNet: 3D Human Motion Reconstruction from Monocular Video with\n  Skeleton Consistency", "comments": "Accepted to Transactions on Graphics (ToG) 2020. Project page:\n  {https://rubbly.cn/publications/motioNet} Video:\n  {https://youtu.be/8YubchlzvFA}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce MotioNet, a deep neural network that directly reconstructs the\nmotion of a 3D human skeleton from monocular video.While previous methods rely\non either rigging or inverse kinematics (IK) to associate a consistent skeleton\nwith temporally coherent joint rotations, our method is the first data-driven\napproach that directly outputs a kinematic skeleton, which is a complete,\ncommonly used, motion representation. At the crux of our approach lies a deep\nneural network with embedded kinematic priors, which decomposes sequences of 2D\njoint positions into two separate attributes: a single, symmetric, skeleton,\nencoded by bone lengths, and a sequence of 3D joint rotations associated with\nglobal root positions and foot contact labels. These attributes are fed into an\nintegrated forward kinematics (FK) layer that outputs 3D positions, which are\ncompared to a ground truth. In addition, an adversarial loss is applied to the\nvelocities of the recovered rotations, to ensure that they lie on the manifold\nof natural joint rotations. The key advantage of our approach is that it learns\nto infer natural joint rotations directly from the training data, rather than\nassuming an underlying model, or inferring them from joint positions using a\ndata-agnostic IK solver. We show that enforcing a single consistent skeleton\nalong with temporally coherent joint rotations constrains the solution space,\nleading to a more robust handling of self-occlusions and depth ambiguities.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 08:50:09 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Shi", "Mingyi", ""], ["Aberman", "Kfir", ""], ["Aristidou", "Andreas", ""], ["Komura", "Taku", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Daniel", ""], ["Chen", "Baoquan", ""]]}, {"id": "2006.12085", "submitter": "Qiulin Zhang", "authors": "Qiulin Zhang, Zhuqing Jiang, Qishuo Lu, Jia'nan Han, Zhengxin Zeng,\n  Shang-hua Gao, Aidong Men", "title": "Split to Be Slim: An Overlooked Redundancy in Vanilla Convolution", "comments": "Preprint version. The final version has been accepted to appear at\n  IJCAI20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many effective solutions have been proposed to reduce the redundancy of\nmodels for inference acceleration. Nevertheless, common approaches mostly focus\non eliminating less important filters or constructing efficient operations,\nwhile ignoring the pattern redundancy in feature maps. We reveal that many\nfeature maps within a layer share similar but not identical patterns. However,\nit is difficult to identify if features with similar patterns are redundant or\ncontain essential details. Therefore, instead of directly removing uncertain\nredundant features, we propose a \\textbf{sp}lit based \\textbf{conv}olutional\noperation, namely SPConv, to tolerate features with similar patterns but\nrequire less computation. Specifically, we split input feature maps into the\nrepresentative part and the uncertain redundant part, where intrinsic\ninformation is extracted from the representative part through relatively heavy\ncomputation while tiny hidden details in the uncertain redundant part are\nprocessed with some light-weight operation. To recalibrate and fuse these two\ngroups of processed features, we propose a parameters-free feature fusion\nmodule. Moreover, our SPConv is formulated to replace the vanilla convolution\nin a plug-and-play way. Without any bells and whistles, experimental results on\nbenchmarks demonstrate SPConv-equipped networks consistently outperform\nstate-of-the-art baselines in both accuracy and inference time on GPU, with\nFLOPs and parameters dropped sharply.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 09:08:51 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zhang", "Qiulin", ""], ["Jiang", "Zhuqing", ""], ["Lu", "Qishuo", ""], ["Han", "Jia'nan", ""], ["Zeng", "Zhengxin", ""], ["Gao", "Shang-hua", ""], ["Men", "Aidong", ""]]}, {"id": "2006.12087", "submitter": "Yadan Luo", "authors": "Yadan Luo, Zijian Wang, Zi Huang, Mahsa Baktashmotlagh", "title": "Progressive Graph Learning for Open-Set Domain Adaptation", "comments": null, "journal-ref": "International Conference on Machine Learning (ICML 2020)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Domain shift is a fundamental problem in visual recognition which typically\narises when the source and target data follow different distributions. The\nexisting domain adaptation approaches which tackle this problem work in the\nclosed-set setting with the assumption that the source and the target data\nshare exactly the same classes of objects. In this paper, we tackle a more\nrealistic problem of open-set domain shift where the target data contains\nadditional classes that are not present in the source data. More specifically,\nwe introduce an end-to-end Progressive Graph Learning (PGL) framework where a\ngraph neural network with episodic training is integrated to suppress\nunderlying conditional shift and adversarial learning is adopted to close the\ngap between the source and target distributions. Compared to the existing\nopen-set adaptation approaches, our approach guarantees to achieve a tighter\nupper bound of the target error. Extensive experiments on three standard\nopen-set benchmarks evidence that our approach significantly outperforms the\nstate-of-the-arts in open-set domain adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 09:10:34 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 00:44:21 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Luo", "Yadan", ""], ["Wang", "Zijian", ""], ["Huang", "Zi", ""], ["Baktashmotlagh", "Mahsa", ""]]}, {"id": "2006.12090", "submitter": "Ziwen Ke", "authors": "Ziwen Ke, Wenqi Huang, Jing Cheng, Zhuoxu Cui, Sen Jia, Haifeng Wang,\n  Xin Liu, Hairong Zheng, Leslie Ying, Yanjie Zhu and Dong Liang", "title": "Deep Low-rank Prior in Dynamic MR Imaging", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep learning methods have achieved attractive performance in dynamic MR\ncine imaging. However, all of these methods are only driven by the sparse prior\nof MR images, while the important low-rank (LR) prior of dynamic MR cine images\nis not explored, which limits the further improvements on dynamic MR\nreconstruction. In this paper, a learned singular value thresholding\n(Learned-SVT) operation is proposed to explore deep low-rank prior in dynamic\nMR imaging for obtaining improved reconstruction results. In particular, we\ncome up with two novel and distinct schemes to introduce the learnable low-rank\nprior into deep network architectures in an unrolling manner and a\nplug-and-play manner respectively. In the unrolling manner, we put forward a\nmodel-based unrolling sparse and low-rank network for dynamic MR imaging,\ndubbed SLR-Net. The SLR-Net is defined over a deep network flow graph, which is\nunrolled from the iterative procedures in the Iterative Shrinkage-Thresholding\nAlgorithm (ISTA) for optimizing a sparse and low-rank based dynamic MRI model.\nIn the plug-and-play manner, we present a plug-and-play LR network module that\ncan be easily embedded into any other dynamic MR neural networks without\nchanging the network paradigm. Experimental results show that both schemes can\nfurther improve the state-of-the-art CS methods, such as k-t SLR, and\nsparsity-driven deep learning-based methods, such as DC-CNN and CRNN, both\nqualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 09:26:10 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 09:55:50 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 03:26:08 GMT"}, {"version": "v4", "created": "Tue, 28 Jul 2020 09:26:49 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Ke", "Ziwen", ""], ["Huang", "Wenqi", ""], ["Cheng", "Jing", ""], ["Cui", "Zhuoxu", ""], ["Jia", "Sen", ""], ["Wang", "Haifeng", ""], ["Liu", "Xin", ""], ["Zheng", "Hairong", ""], ["Ying", "Leslie", ""], ["Zhu", "Yanjie", ""], ["Liang", "Dong", ""]]}, {"id": "2006.12119", "submitter": "Stefano Vincenzi", "authors": "Stefano Vincenzi, Angelo Porrello, Pietro Buzzega, Marco Cipriano,\n  Pietro Fronte, Roberto Cuccu, Carla Ippoliti, Annamaria Conte, Simone\n  Calderara", "title": "The color out of space: learning self-supervised representations for\n  Earth Observation imagery", "comments": "8 pages, 2 figures. Accepted in the 25th International Conference on\n  PATTERN RECOGNITION (ICPR 2020), Milan, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent growth in the number of satellite images fosters the development\nof effective deep-learning techniques for Remote Sensing (RS). However, their\nfull potential is untapped due to the lack of large annotated datasets. Such a\nproblem is usually countered by fine-tuning a feature extractor that is\npreviously trained on the ImageNet dataset. Unfortunately, the domain of\nnatural images differs from the RS one, which hinders the final performance. In\nthis work, we propose to learn meaningful representations from satellite\nimagery, leveraging its high-dimensionality spectral bands to reconstruct the\nvisible colors. We conduct experiments on land cover classification\n(BigEarthNet) and West Nile Virus detection, showing that colorization is a\nsolid pretext task for training a feature extractor. Furthermore, we\nqualitatively observe that guesses based on natural images and colorization\nrely on different parts of the input. This paves the way to an ensemble model\nthat eventually outperforms both the above-mentioned techniques.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 10:21:36 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Vincenzi", "Stefano", ""], ["Porrello", "Angelo", ""], ["Buzzega", "Pietro", ""], ["Cipriano", "Marco", ""], ["Fronte", "Pietro", ""], ["Cuccu", "Roberto", ""], ["Ippoliti", "Carla", ""], ["Conte", "Annamaria", ""], ["Calderara", "Simone", ""]]}, {"id": "2006.12121", "submitter": "Nabeel Seedat", "authors": "Nabeel Seedat, Vered Aharonson, Ilana Schlesinger", "title": "Automated machine vision enabled detection of movement disorders from\n  hand drawn spirals", "comments": "Accepted IEEE International Conference on Healthcare Informatics 2020\n  (ICHI 2020), Upcoming Dec 2020, Copyright IEEE - 978-1-5386-5541-2/18/$31.00\n  Copyright, 2020 IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A widely used test for the diagnosis of Parkinson's disease (PD) and\nEssential tremor (ET) is hand-drawn shapes,where the analysis is\nobservationally performed by the examining neurologist. This method is\nsubjective and is prone to bias amongst different physicians. Due to the\nsimilarities in the symptoms of the two diseases, they are often\nmisdiagnosed.Studies which attempt to automate the process typically use\ndigitized input, where the tablet or specialized equipment are not affordable\nin many clinical settings. This study uses a dataset of scanned pen and paper\ndrawings and a convolutional neural network (CNN) to perform classification\nbetween PD, ET and control subjects. The discrimination accuracy of PD from\ncontrols was 98.2%. The discrimination accuracy of PD from ET and from controls\nwas 92%. An ablation study was conducted and indicated that correct hyper\nparameter optimization can increases the accuracy up to 4.33%. Finally, the\nstudy indicates the viability of using a CNN-enabled machine vision system to\nprovide robust and accurate detection of movement disorders from hand drawn\nspirals.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 10:21:51 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Seedat", "Nabeel", ""], ["Aharonson", "Vered", ""], ["Schlesinger", "Ilana", ""]]}, {"id": "2006.12127", "submitter": "Francisco Heras", "authors": "Francisco J. H. Heras, Gonzalo G. de Polavieja", "title": "Supervised dimensionality reduction by a Linear Discriminant Analysis on\n  pre-trained CNN features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We explore the application of linear discriminant analysis (LDA) to the\nfeatures obtained in different layers of pretrained deep convolutional neural\nnetworks (CNNs). The advantage of LDA compared to other techniques in\ndimensionality reduction is that it reduces dimensions while preserving the\nglobal structure of data, so distances in the low-dimensional structure found\nare meaningful. The LDA applied to the CNN features finds that the centroids of\nclasses corresponding to the similar data lay closer than classes corresponding\nto different data. We applied the method to a modification of the MNIST dataset\nwith ten additional classes, each new class with half of the images from one of\nthe standard ten classes. The method finds the new classes close to the\ncorresponding standard classes we took the data form. We also applied the\nmethod to a dataset of images of butterflies to find that related subspecies\nare found to be close. For both datasets, we find a performance similar to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 10:31:04 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Heras", "Francisco J. H.", ""], ["de Polavieja", "Gonzalo G.", ""]]}, {"id": "2006.12135", "submitter": "Divyam Madaan", "authors": "Divyam Madaan, Jinwoo Shin, Sung Ju Hwang", "title": "Learning to Generate Noise for Multi-Attack Robustness", "comments": "Accepted to ICML 2021. Code available at\n  https://github.com/divyam3897/MNG_AC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial learning has emerged as one of the successful techniques to\ncircumvent the susceptibility of existing methods against adversarial\nperturbations. However, the majority of existing defense methods are tailored\nto defend against a single category of adversarial perturbation (e.g.\n$\\ell_\\infty$-attack). In safety-critical applications, this makes these\nmethods extraneous as the attacker can adopt diverse adversaries to deceive the\nsystem. Moreover, training on multiple perturbations simultaneously\nsignificantly increases the computational overhead during training. To address\nthese challenges, we propose a novel meta-learning framework that explicitly\nlearns to generate noise to improve the model's robustness against multiple\ntypes of attacks. Its key component is Meta Noise Generator (MNG) that outputs\noptimal noise to stochastically perturb a given sample, such that it helps\nlower the error on diverse adversarial perturbations. By utilizing samples\ngenerated by MNG, we train a model by enforcing the label consistency across\nmultiple perturbations. We validate the robustness of models trained by our\nscheme on various datasets and against a wide variety of perturbations,\ndemonstrating that it significantly outperforms the baselines across multiple\nperturbations with a marginal computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 10:44:05 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 17:36:33 GMT"}, {"version": "v3", "created": "Sun, 20 Jun 2021 06:47:26 GMT"}, {"version": "v4", "created": "Thu, 24 Jun 2021 18:41:57 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Madaan", "Divyam", ""], ["Shin", "Jinwoo", ""], ["Hwang", "Sung Ju", ""]]}, {"id": "2006.12140", "submitter": "Laurent Kloeker", "authors": "Laurent Kloeker, Christian Geller, Amarin Kloeker, Lutz Eckstein", "title": "High-Precision Digital Traffic Recording with Multi-LiDAR Infrastructure\n  Sensor Setups", "comments": "Accepted to be published as part of the 23rd IEEE International\n  Conference on Intelligent Transportation Systems (ITSC), Rhodes, Greece,\n  September 20-23, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large driving datasets are a key component in the current development and\nsafeguarding of automated driving functions. Various methods can be used to\ncollect such driving data records. In addition to the use of sensor equipped\nresearch vehicles or unmanned aerial vehicles (UAVs), the use of infrastructure\nsensor technology offers another alternative. To minimize object occlusion\nduring data collection, it is crucial to record the traffic situation from\nseveral perspectives in parallel. A fusion of all raw sensor data might create\nbetter conditions for multi-object detection and tracking (MODT) compared to\nthe use of individual raw sensor data. So far, no sufficient studies have been\nconducted to sufficiently confirm this approach. In our work we investigate the\nimpact of fused LiDAR point clouds compared to single LiDAR point clouds. We\nmodel different urban traffic scenarios with up to eight 64-layer LiDARs in\nsimulation and in reality. We then analyze the properties of the resulting\npoint clouds and perform MODT for all emerging traffic participants. The\nevaluation of the extracted trajectories shows that a fused infrastructure\napproach significantly increases the tracking results and reaches accuracies\nwithin a few centimeters.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 10:57:52 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kloeker", "Laurent", ""], ["Geller", "Christian", ""], ["Kloeker", "Amarin", ""], ["Eckstein", "Lutz", ""]]}, {"id": "2006.12150", "submitter": "Bryan Guevara Cardenas", "authors": "Bryan G. Cardenas, Devanshu Arya, Deepak K. Gupta", "title": "Generating Annotated High-Fidelity Images Containing Multiple Coherent\n  Objects", "comments": "21 pages, 5 tables, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments related to generative models have made it possible to\ngenerate diverse high-fidelity images. In particular, layout-to-image\ngeneration models have gained significant attention due to their capability to\ngenerate realistic complex images containing distinct objects. These models are\ngenerally conditioned on either semantic layouts or textual descriptions.\nHowever, unlike natural images, providing auxiliary information can be\nextremely hard in domains such as biomedical imaging and remote sensing. In\nthis work, we propose a multi-object generation framework that can synthesize\nimages with multiple objects without explicitly requiring their contextual\ninformation during the generation process. Based on a vector-quantized\nvariational autoencoder (VQ-VAE) backbone, our model learns to preserve spatial\ncoherency within an image as well as semantic coherency between the objects and\nthe background through two powerful autoregressive priors: PixelSNAIL and\nLayoutPixelSNAIL. While the PixelSNAIL learns the distribution of the latent\nencodings of the VQ-VAE, the LayoutPixelSNAIL is used to specifically learn the\nsemantic distribution of the objects. An implicit advantage of our approach is\nthat the generated samples are accompanied by object-level annotations. We\ndemonstrate how coherency and fidelity are preserved with our method through\nexperiments on the Multi-MNIST and CLEVR datasets; thereby outperforming\nstate-of-the-art multi-object generative methods. The efficacy of our approach\nis demonstrated through application on medical imaging datasets, where we show\nthat augmenting the training set with generated samples using our approach\nimproves the performance of existing models.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 11:33:55 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 10:20:24 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 21:42:29 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Cardenas", "Bryan G.", ""], ["Arya", "Devanshu", ""], ["Gupta", "Deepak K.", ""]]}, {"id": "2006.12155", "submitter": "Alejandro Hernandez Ruiz", "authors": "Alejandro Hernandez Ruiz, Armand Vilalta, Francesc Moreno-Noguer", "title": "Neural Cellular Automata Manifold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very recently, the Neural Cellular Automata (NCA) has been proposed to\nsimulate the morphogenesis process with deep networks. NCA learns to grow an\nimage starting from a fixed single pixel. In this work, we show that the neural\nnetwork (NN) architecture of the NCA can be encapsulated in a larger NN. This\nallows us to propose a new model that encodes a manifold of NCA, each of them\ncapable of generating a distinct image. Therefore, we are effectively learning\nan embedding space of CA, which shows generalization capabilities. We\naccomplish this by introducing dynamic convolutions inside an Auto-Encoder\narchitecture, for the first time used to join two different sources of\ninformation, the encoding and cells environment information. In biological\nterms, our approach would play the role of the transcription factors,\nmodulating the mapping of genes into specific proteins that drive cellular\ndifferentiation, which occurs right before the morphogenesis. We thoroughly\nevaluate our approach in a dataset of synthetic emojis and also in real images\nof CIFAR10. Our model introduces a general-purpose network, which can be used\nin a broad range of problems beyond image generation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 11:41:57 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 20:33:28 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 10:38:43 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Ruiz", "Alejandro Hernandez", ""], ["Vilalta", "Armand", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "2006.12195", "submitter": "Aleksandra Nowak", "authors": "Romuald A. Janik, Aleksandra Nowak", "title": "Neural networks adapting to datasets: learning network size and topology", "comments": "Fixed blank page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a flexible setup allowing for a neural network to learn both its\nsize and topology during the course of a standard gradient-based training. The\nresulting network has the structure of a graph tailored to the particular\nlearning task and dataset. The obtained networks can also be trained from\nscratch and achieve virtually identical performance. We explore the properties\nof the network architectures for a number of datasets of varying difficulty\nobserving systematic regularities. The obtained graphs can be therefore\nunderstood as encoding nontrivial characteristics of the particular\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 12:46:44 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 10:00:07 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Janik", "Romuald A.", ""], ["Nowak", "Aleksandra", ""]]}, {"id": "2006.12201", "submitter": "Davide Moroni", "authors": "Bilal Hussain, Bushra Jalil, Maria Antonietta Pascali, Muhammad Imran,\n  Giovanni Serafino, Davide Moroni and Paolo Ghelfi", "title": "Thermal vulnerability detection in integrated electronic and photonic\n  circuits using IR thermography", "comments": "2020 Optical Society of America. One print or electronic copy may be\n  made for personal use only. Systematic reproduction and distribution,\n  duplication of any material in this paper for a fee or for commercial\n  purposes, or modifications of the content of this paper are prohibited", "journal-ref": null, "doi": "10.1364/AO.389960", "report-no": null, "categories": "physics.app-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Failure prediction of any electrical/optical component is crucial for\nestimating its operating life. Using high temperature operating life (HTOL)\ntests, it is possible to model the failure mechanisms for integrated circuits.\nConventional HTOL standards are not suitable for operating life prediction of\nphotonic components owing to their functional dependence on thermo-optic\neffect. This work presents an IR-assisted thermal vulnerability detection\ntechnique suitable for photonic as well as electronic components. By accurately\nmapping the thermal profile of an integrated circuit under a stress condition,\nit is possible to precisely locate the heat center for predicting the long-term\noperational failures within the device under test. For the first time, the\nreliability testing is extended to a fully functional microwave photonic system\nusing conventional IR thermography. By applying image fusion using affine\ntransformation on multimodal acquisition, it was demonstrated that by comparing\nthe IR profile and GDSII layout, it is possible to accurately locate the heat\ncenters along with spatial information on the type of component. Multiple IR\nprofiles of optical as well as electrical components/circuits were acquired and\nmapped onto the layout files. In order to ascertain the degree of effectiveness\nof the proposed technique, IR profiles of CMOS RF and digital circuits were\nalso analyzed. The presented technique offers a reliable automated\nidentification of heat spots within a circuit/system.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 09:25:55 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Hussain", "Bilal", ""], ["Jalil", "Bushra", ""], ["Pascali", "Maria Antonietta", ""], ["Imran", "Muhammad", ""], ["Serafino", "Giovanni", ""], ["Moroni", "Davide", ""], ["Ghelfi", "Paolo", ""]]}, {"id": "2006.12209", "submitter": "Jinghuang Lin", "authors": "Jinghuang Lin, Zhanzhan Cheng, Fan Bai, Yi Niu, Shiliang Pu, Shuigeng\n  Zhou", "title": "Text Recognition in Real Scenarios with a Few Labeled Samples", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition (STR) is still a hot research topic in computer vision\nfield due to its various applications. Existing works mainly focus on learning\na general model with a huge number of synthetic text images to recognize\nunconstrained scene texts, and have achieved substantial progress. However,\nthese methods are not quite applicable in many real-world scenarios where 1)\nhigh recognition accuracy is required, while 2) labeled samples are lacked. To\ntackle this challenging problem, this paper proposes a few-shot adversarial\nsequence domain adaptation (FASDA) approach to build sequence adaptation\nbetween the synthetic source domain (with many synthetic labeled samples) and a\nspecific target domain (with only some or a few real labeled samples). This is\ndone by simultaneously learning each character's feature representation with an\nattention mechanism and establishing the corresponding character-level latent\nsubspace with adversarial learning. Our approach can maximize the\ncharacter-level confusion between the source domain and the target domain, thus\nachieves the sequence-level adaptation with even a small number of labeled\nsamples in the target domain. Extensive experiments on various datasets show\nthat our method significantly outperforms the finetuning scheme, and obtains\ncomparable performance to the state-of-the-art STR methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 13:03:01 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Lin", "Jinghuang", ""], ["Cheng", "Zhanzhan", ""], ["Bai", "Fan", ""], ["Niu", "Yi", ""], ["Pu", "Shiliang", ""], ["Zhou", "Shuigeng", ""]]}, {"id": "2006.12210", "submitter": "Alexandra Lindt", "authors": "Alexandra Lindt, Pablo Barros, Henrique Siqueira and Stefan Wermter", "title": "Facial Expression Editing with Continuous Emotion Labels", "comments": "8 pages, 5 figures. 14th IEEE International Conference on Automatic\n  Face and Gesture Recognition (FG 2019), May 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep generative models have achieved impressive results in the field\nof automated facial expression editing. However, the approaches presented so\nfar presume a discrete representation of human emotions and are therefore\nlimited in the modelling of non-discrete emotional expressions. To overcome\nthis limitation, we explore how continuous emotion representations can be used\nto control automated expression editing. We propose a deep generative model\nthat can be used to manipulate facial expressions in facial images according to\ncontinuous two-dimensional emotion labels. One dimension represents an\nemotion's valence, the other represents its degree of arousal. We demonstrate\nthe functionality of our model with a quantitative analysis using classifier\nnetworks as well as with a qualitative analysis.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 13:03:02 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Lindt", "Alexandra", ""], ["Barros", "Pablo", ""], ["Siqueira", "Henrique", ""], ["Wermter", "Stefan", ""]]}, {"id": "2006.12220", "submitter": "Pengyi Zhang", "authors": "Pengyi Zhang, Yunxin Zhong, Xiaoying Tang, Yunlin Deng, Xiaoqiong Li", "title": "Learning Diagnosis of COVID-19 from a Single Radiological Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Radiological image is currently adopted as the visual evidence for COVID-19\ndiagnosis in clinical. Using deep models to realize automated infection\nmeasurement and COVID-19 diagnosis is important for faster examination based on\nradiological imaging. Unfortunately, collecting large training data\nsystematically in the early stage is difficult. To address this problem, we\nexplore the feasibility of learning deep models for COVID-19 diagnosis from a\nsingle radiological image by resorting to synthesizing diverse radiological\nimages. Specifically, we propose a novel conditional generative model, called\nCoSinGAN, which can be learned from a single radiological image with a given\ncondition, i.e., the annotations of the lung and COVID-19 infection. Our\nCoSinGAN is able to capture the conditional distribution of visual finds of\nCOVID-19 infection, and further synthesize diverse and high-resolution\nradiological images that match the input conditions precisely. Both deep\nclassification and segmentation networks trained on synthesized samples from\nCoSinGAN achieve notable detection accuracy of COVID-19 infection. Such results\nare significantly better than the counterparts trained on the same extremely\nsmall number of real samples (1 or 2 real samples) by using strong data\naugmentation, and approximate to the counterparts trained on large dataset\n(2846 real images). It confirms our method can significantly reduce the\nperformance gap between deep models trained on extremely small dataset and on\nlarge dataset, and thus has the potential to realize learning COVID-19\ndiagnosis from few radiological images in the early stage of COVID-19 pandemic.\nOur codes are made publicly available at\nhttps://github.com/PengyiZhang/CoSinGAN.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 07:41:28 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zhang", "Pengyi", ""], ["Zhong", "Yunxin", ""], ["Tang", "Xiaoying", ""], ["Deng", "Yunlin", ""], ["Li", "Xiaoqiong", ""]]}, {"id": "2006.12226", "submitter": "Shir Gur", "authors": "Shir Gur, Sagie Benaim, Lior Wolf", "title": "Hierarchical Patch VAE-GAN: Generating Diverse Videos from a Single\n  Sample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of generating diverse and novel videos from a single\nvideo sample. Recently, new hierarchical patch-GAN based approaches were\nproposed for generating diverse images, given only a single sample at training\ntime. Moving to videos, these approaches fail to generate diverse samples, and\noften collapse into generating samples similar to the training video. We\nintroduce a novel patch-based variational autoencoder (VAE) which allows for a\nmuch greater diversity in generation. Using this tool, a new hierarchical video\ngeneration scheme is constructed: at coarse scales, our patch-VAE is employed,\nensuring samples are of high diversity. Subsequently, at finer scales, a\npatch-GAN renders the fine details, resulting in high quality videos. Our\nexperiments show that the proposed method produces diverse samples in both the\nimage domain, and the more challenging video domain.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 13:24:25 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 12:30:31 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 11:38:19 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Gur", "Shir", ""], ["Benaim", "Sagie", ""], ["Wolf", "Lior", ""]]}, {"id": "2006.12230", "submitter": "Mart Van Rijthoven", "authors": "Mart van Rijthoven, Maschenka Balkenhol, Karina Sili\\c{n}a, Jeroen van\n  der Laak, Francesco Ciompi", "title": "HookNet: multi-resolution convolutional neural networks for semantic\n  segmentation in histopathology whole-slide images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose HookNet, a semantic segmentation model for histopathology\nwhole-slide images, which combines context and details via multiple branches of\nencoder-decoder convolutional neural networks. Concentricpatches at multiple\nresolutions with different fields of view are used to feed different branches\nof HookNet, and intermediate representations are combined via a hooking\nmechanism. We describe a framework to design and train HookNet for achieving\nhigh-resolution semantic segmentation and introduce constraints to guarantee\npixel-wise alignment in feature maps during hooking. We show the advantages of\nusing HookNet in two histopathology image segmentation tasks where tissue type\nprediction accuracy strongly depends on contextual information, namely (1)\nmulti-class tissue segmentation in breast cancer and, (2) segmentation of\ntertiary lymphoid structures and germinal centers in lung cancer. Weshow the\nsuperiority of HookNet when compared with single-resolution U-Net models\nworking at different resolutions as well as with a recently published\nmulti-resolution model for histopathology image segmentation\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 13:26:37 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["van Rijthoven", "Mart", ""], ["Balkenhol", "Maschenka", ""], ["Sili\u0146a", "Karina", ""], ["van der Laak", "Jeroen", ""], ["Ciompi", "Francesco", ""]]}, {"id": "2006.12235", "submitter": "Ren\\'e Schuster", "authors": "Rishav, Ren\\'e Schuster, Ramy Battrawy, Oliver Wasenm\\\"uller, Didier\n  Stricker", "title": "ResFPN: Residual Skip Connections in Multi-Resolution Feature Pyramid\n  Networks for Accurate Dense Pixel Matching", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense pixel matching is required for many computer vision algorithms such as\ndisparity, optical flow or scene flow estimation. Feature Pyramid Networks\n(FPN) have proven to be a suitable feature extractor for CNN-based dense\nmatching tasks. FPN generates well localized and semantically strong features\nat multiple scales. However, the generic FPN is not utilizing its full\npotential, due to its reasonable but limited localization accuracy. Thus, we\npresent ResFPN -- a multi-resolution feature pyramid network with multiple\nresidual skip connections, where at any scale, we leverage the information from\nhigher resolution maps for stronger and better localized features. In our\nablation study, we demonstrate the effectiveness of our novel architecture with\nclearly higher accuracy than FPN. In addition, we verify the superior accuracy\nof ResFPN in many different pixel matching applications on established datasets\nlike KITTI, Sintel, and FlyingThings3D.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 13:31:31 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Rishav", "", ""], ["Schuster", "Ren\u00e9", ""], ["Battrawy", "Ramy", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Stricker", "Didier", ""]]}, {"id": "2006.12245", "submitter": "Peyman Bateni", "authors": "Peyman Bateni, Jarred Barber, Jan-Willem van de Meent, Frank Wood", "title": "Enhancing Few-Shot Image Classification with Unlabelled Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a transductive meta-learning method that uses unlabelled instances\nto improve few-shot image classification performance. Our approach combines a\nregularized Mahalanobis-distance-based soft k-means clustering procedure with a\nmodified state of the art neural adaptive feature extractor to achieve improved\ntest-time classification accuracy using unlabelled data. We evaluate our method\non transductive few-shot learning tasks, in which the goal is to jointly\npredict labels for query (test) examples given a set of support (training)\nexamples. We achieve state-of-the-art performance on the Meta-Dataset,\nmini-ImageNet and tiered-ImageNet benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 05:42:47 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 19:16:37 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 17:55:50 GMT"}, {"version": "v4", "created": "Thu, 15 Apr 2021 17:58:42 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Bateni", "Peyman", ""], ["Barber", "Jarred", ""], ["van de Meent", "Jan-Willem", ""], ["Wood", "Frank", ""]]}, {"id": "2006.12246", "submitter": "Matthew Lee", "authors": "Matthew Lee, Lyndon Kennedy, Andreas Girgensohn, Lynn Wilcox, John\n  Song En Lee, Chin Wen Tan, Ban Leong Sng", "title": "Pain Intensity Estimation from Mobile Video Using 2D and 3D Facial\n  Keypoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing post-surgical pain is critical for successful surgical outcomes. One\nof the challenges of pain management is accurately assessing the pain level of\npatients. Self-reported numeric pain ratings are limited because they are\nsubjective, can be affected by mood, and can influence the patient's perception\nof pain when making comparisons. In this paper, we introduce an approach that\nanalyzes 2D and 3D facial keypoints of post-surgical patients to estimate their\npain intensity level. Our approach leverages the previously unexplored\ncapabilities of a smartphone to capture a dense 3D representation of a person's\nface as input for pain intensity level estimation. Our contributions are adata\ncollection study with post-surgical patients to collect ground-truth labeled\nsequences of 2D and 3D facial keypoints for developing a pain estimation\nalgorithm, a pain estimation model that uses multiple instance learning to\novercome inherent limitations in facial keypoint sequences, and the preliminary\nresults of the pain estimation model using 2D and 3D features with comparisons\nof alternate approaches.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 00:18:29 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Lee", "Matthew", ""], ["Kennedy", "Lyndon", ""], ["Girgensohn", "Andreas", ""], ["Wilcox", "Lynn", ""], ["Lee", "John Song En", ""], ["Tan", "Chin Wen", ""], ["Sng", "Ban Leong", ""]]}, {"id": "2006.12247", "submitter": "Eran Segalis", "authors": "Eran Segalis, Eran Galili", "title": "OGAN: Disrupting Deepfakes with an Adversarial Attack that Survives\n  Training", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in autoencoders and generative models have given rise to\neffective video forgery methods, used for generating so-called \"deepfakes\".\nMitigation research is mostly focused on post-factum deepfake detection and not\non prevention. We complement these efforts by introducing a novel class of\nadversarial attacks---training-resistant attacks---which can disrupt\nface-swapping autoencoders whether or not its adversarial images have been\nincluded in the training set of said autoencoders. We propose the Oscillating\nGAN (OGAN) attack, a novel attack optimized to be training-resistant, which\nintroduces spatial-temporal distortions to the output of face-swapping\nautoencoders. To implement OGAN, we construct a bilevel optimization problem,\nwhere we train a generator and a face-swapping model instance against each\nother. Specifically, we pair each input image with a target distortion, and\nfeed them into a generator that produces an adversarial image. This image will\nexhibit the distortion when a face-swapping autoencoder is applied to it. We\nsolve the optimization problem by training the generator and the face-swapping\nmodel simultaneously using an iterative process of alternating optimization.\nNext, we analyze the previously published Distorting Attack and show it is\ntraining-resistant, though it is outperformed by our suggested OGAN. Finally,\nwe validate both attacks using a popular implementation of FaceSwap, and show\nthat they transfer across different target models and target faces, including\nfaces the adversarial attacks were not trained on. More broadly, these results\ndemonstrate the existence of training-resistant adversarial attacks,\npotentially applicable to a wide range of domains.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 17:18:29 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 11:47:30 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Segalis", "Eran", ""], ["Galili", "Eran", ""]]}, {"id": "2006.12250", "submitter": "Haozhe Xie", "authors": "Haozhe Xie, Hongxun Yao, Shengping Zhang, Shangchen Zhou, Wenxiu Sun", "title": "Pix2Vox++: Multi-scale Context-aware 3D Object Reconstruction from\n  Single and Multiple Images", "comments": "International Journal of Computer Vision (IJCV). arXiv admin note:\n  text overlap with arXiv:1901.11153", "journal-ref": "International Journal of Computer Vision, 128(12): 2919-2935, 2020", "doi": "10.1007/s11263-020-01347-6", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recovering the 3D shape of an object from single or multiple images with deep\nneural networks has been attracting increasing attention in the past few years.\nMainstream works (e.g. 3D-R2N2) use recurrent neural networks (RNNs) to\nsequentially fuse feature maps of input images. However, RNN-based approaches\nare unable to produce consistent reconstruction results when given the same\ninput images with different orders. Moreover, RNNs may forget important\nfeatures from early input images due to long-term memory loss. To address these\nissues, we propose a novel framework for single-view and multi-view 3D object\nreconstruction, named Pix2Vox++. By using a well-designed encoder-decoder, it\ngenerates a coarse 3D volume from each input image. A multi-scale context-aware\nfusion module is then introduced to adaptively select high-quality\nreconstructions for different parts from all coarse 3D volumes to obtain a\nfused 3D volume. To further correct the wrongly recovered parts in the fused 3D\nvolume, a refiner is adopted to generate the final output. Experimental results\non the ShapeNet, Pix3D, and Things3D benchmarks show that Pix2Vox++ performs\nfavorably against state-of-the-art methods in terms of both accuracy and\nefficiency.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 13:48:09 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 09:30:13 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Xie", "Haozhe", ""], ["Yao", "Hongxun", ""], ["Zhang", "Shengping", ""], ["Zhou", "Shangchen", ""], ["Sun", "Wenxiu", ""]]}, {"id": "2006.12263", "submitter": "Lingtong Kong", "authors": "Lingtong Kong, Jie Yang", "title": "FDFlowNet: Fast Optical Flow Estimation using a Deep Lightweight Network", "comments": "Accepted by ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant progress has been made for estimating optical flow using deep\nneural networks. Advanced deep models achieve accurate flow estimation often\nwith a considerable computation complexity and time-consuming training\nprocesses. In this work, we present a lightweight yet effective model for\nreal-time optical flow estimation, termed FDFlowNet (fast deep flownet). We\nachieve better or similar accuracy on the challenging KITTI and Sintel\nbenchmarks while being about 2 times faster than PWC-Net. This is achieved by a\ncarefully-designed structure and newly proposed components. We first introduce\nan U-shape network for constructing multi-scale feature which benefits upper\nlevels with global receptive field compared with pyramid network. In each\nscale, a partial fully connected structure with dilated convolution is proposed\nfor flow estimation that obtains a good balance among speed, accuracy and\nnumber of parameters compared with sequential connected and dense connected\nstructures. Experiments demonstrate that our model achieves state-of-the-art\nperformance while being fast and lightweight.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 14:01:01 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kong", "Lingtong", ""], ["Yang", "Jie", ""]]}, {"id": "2006.12285", "submitter": "Diyuan Lu", "authors": "Diyuan Lu, Nenad Polomac, Iskra Gacheva, Elke Hattingen, Jochen\n  Triesch", "title": "Human-Expert-Level Brain Tumor Detection Using Deep Learning with Data\n  Distillation and Augmentation", "comments": "Submitted to IEEE Transactions on Neural Networks and Learning\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of Deep Learning (DL) for medical diagnosis is often hampered\nby two problems. First, the amount of training data may be scarce, as it is\nlimited by the number of patients who have acquired the condition to be\ndiagnosed. Second, the training data may be corrupted by various types of\nnoise. Here, we study the problem of brain tumor detection from magnetic\nresonance spectroscopy (MRS) data, where both types of problems are prominent.\nTo overcome these challenges, we propose a new method for training a deep\nneural network that distills particularly representative training examples and\naugments the training data by mixing these samples from one class with those\nfrom the same and other classes to create additional training samples. We\ndemonstrate that this technique substantially improves performance, allowing\nour method to reach human-expert-level accuracy with just a few thousand\ntraining examples. Interestingly, the network learns to rely on features of the\ndata that are usually ignored by human experts, suggesting new directions for\nfuture research.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 15:52:28 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 15:37:44 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 12:32:26 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Lu", "Diyuan", ""], ["Polomac", "Nenad", ""], ["Gacheva", "Iskra", ""], ["Hattingen", "Elke", ""], ["Triesch", "Jochen", ""]]}, {"id": "2006.12316", "submitter": "Brian Kenji Iwana", "authors": "Kohei Baba, Seiichi Uchida, Brian Kenji Iwana", "title": "On the Ability of a CNN to Realize Image-to-Image Language Conversion", "comments": "Published at ICDAR 2019", "journal-ref": null, "doi": "10.1109/ICDAR.2019.00078", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to reveal the ability that Convolutional Neural\nNetworks (CNN) have on the novel task of image-to-image language conversion. We\npropose a new network to tackle this task by converting images of Korean Hangul\ncharacters directly into images of the phonetic Latin character equivalent. The\nconversion rules between Hangul and the phonetic symbols are not explicitly\nprovided. The results of the proposed network show that it is possible to\nperform image-to-image language conversion. Moreover, it shows that it can\ngrasp the structural features of Hangul even from limited learning data. In\naddition, it introduces a new network to use when the input and output have\nsignificantly different features.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 14:54:42 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Baba", "Kohei", ""], ["Uchida", "Seiichi", ""], ["Iwana", "Brian Kenji", ""]]}, {"id": "2006.12323", "submitter": "Xu Ji", "authors": "Xu Ji, Joao Henriques, Tinne Tuytelaars, Andrea Vedaldi", "title": "Automatic Recall Machines: Internal Replay, Continual Learning and the\n  Brain", "comments": "NeurIPS 2020 Workshop on BabyMind", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replay in neural networks involves training on sequential data with memorized\nsamples, which counteracts forgetting of previous behavior caused by\nnon-stationarity. We present a method where these auxiliary samples are\ngenerated on the fly, given only the model that is being trained for the\nassessed objective, without extraneous buffers or generator networks. Instead\nthe implicit memory of learned samples within the assessed model itself is\nexploited. Furthermore, whereas existing work focuses on reinforcing the full\nseen data distribution, we show that optimizing for not forgetting calls for\nthe generation of samples that are specialized to each real training batch,\nwhich is more efficient and scalable. We consider high-level parallels with the\nbrain, notably the use of a single model for inference and recall, the\ndependency of recalled samples on the current environment batch, top-down\nmodulation of activations and learning, abstract recall, and the dependency\nbetween the degree to which a task is learned and the degree to which it is\nrecalled. These characteristics emerge naturally from the method without being\ncontrolled for.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 15:07:06 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 15:17:33 GMT"}, {"version": "v3", "created": "Sat, 12 Dec 2020 17:58:30 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ji", "Xu", ""], ["Henriques", "Joao", ""], ["Tuytelaars", "Tinne", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2006.12356", "submitter": "JunYoung Gwak", "authors": "JunYoung Gwak, Christopher Choy, Silvio Savarese", "title": "Generative Sparse Detection Networks for 3D Single-shot Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection has been widely studied due to its potential\napplicability to many promising areas such as robotics and augmented reality.\nYet, the sparse nature of the 3D data poses unique challenges to this task.\nMost notably, the observable surface of the 3D point clouds is disjoint from\nthe center of the instance to ground the bounding box prediction on. To this\nend, we propose Generative Sparse Detection Network (GSDN), a\nfully-convolutional single-shot sparse detection network that efficiently\ngenerates the support for object proposals. The key component of our model is a\ngenerative sparse tensor decoder, which uses a series of transposed\nconvolutions and pruning layers to expand the support of sparse tensors while\ndiscarding unlikely object centers to maintain minimal runtime and memory\nfootprint. GSDN can process unprecedentedly large-scale inputs with a single\nfully-convolutional feed-forward pass, thus does not require the heuristic\npost-processing stage that stitches results from sliding windows as other\nprevious methods have. We validate our approach on three 3D indoor datasets\nincluding the large-scale 3D indoor reconstruction dataset where our method\noutperforms the state-of-the-art methods by a relative improvement of 7.14%\nwhile being 3.78 times faster than the best prior work.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 15:54:24 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Gwak", "JunYoung", ""], ["Choy", "Christopher", ""], ["Savarese", "Silvio", ""]]}, {"id": "2006.12373", "submitter": "Daniel Bear", "authors": "Daniel M. Bear, Chaofei Fan, Damian Mrowca, Yunzhu Li, Seth Alter,\n  Aran Nayebi, Jeremy Schwartz, Li Fei-Fei, Jiajun Wu, Joshua B. Tenenbaum,\n  Daniel L.K. Yamins", "title": "Learning Physical Graph Representations from Visual Scenes", "comments": "23 pages; corrected affiliations and acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have proved exceptional at learning\nrepresentations for visual object categorization. However, CNNs do not\nexplicitly encode objects, parts, and their physical properties, which has\nlimited CNNs' success on tasks that require structured understanding of visual\nscenes. To overcome these limitations, we introduce the idea of Physical Scene\nGraphs (PSGs), which represent scenes as hierarchical graphs, with nodes in the\nhierarchy corresponding intuitively to object parts at different scales, and\nedges to physical connections between parts. Bound to each node is a vector of\nlatent attributes that intuitively represent object properties such as surface\nshape and texture. We also describe PSGNet, a network architecture that learns\nto extract PSGs by reconstructing scenes through a PSG-structured bottleneck.\nPSGNet augments standard CNNs by including: recurrent feedback connections to\ncombine low and high-level image information; graph pooling and vectorization\noperations that convert spatially-uniform feature maps into object-centric\ngraph structures; and perceptual grouping principles to encourage the\nidentification of meaningful scene elements. We show that PSGNet outperforms\nalternative self-supervised scene representation algorithms at scene\nsegmentation tasks, especially on complex real-world images, and generalizes\nwell to unseen object types and scene arrangements. PSGNet is also able learn\nfrom physical motion, enhancing scene estimates even for static images. We\npresent a series of ablation studies illustrating the importance of each\ncomponent of the PSGNet architecture, analyses showing that learned latent\nattributes capture intuitive scene properties, and illustrate the use of PSGs\nfor compositional scene inference.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 16:10:26 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 17:33:35 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Bear", "Daniel M.", ""], ["Fan", "Chaofei", ""], ["Mrowca", "Damian", ""], ["Li", "Yunzhu", ""], ["Alter", "Seth", ""], ["Nayebi", "Aran", ""], ["Schwartz", "Jeremy", ""], ["Fei-Fei", "Li", ""], ["Wu", "Jiajun", ""], ["Tenenbaum", "Joshua B.", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "2006.12378", "submitter": "Yi Fang", "authors": "Lingjing Wang, Yi Shi, Xiang Li, Yi Fang", "title": "Unsupervised Learning of Global Registration of Temporal Sequence of\n  Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global registration of point clouds aims to find an optimal alignment of a\nsequence of 2D or 3D point sets. In this paper, we present a novel method that\ntakes advantage of current deep learning techniques for unsupervised learning\nof global registration from a temporal sequence of point clouds. Our key\nnovelty is that we introduce a deep Spatio-Temporal REPresentation (STREP)\nfeature, which describes the geometric essence of both temporal and spatial\nrelationship of the sequence of point clouds acquired with sensors in an\nunknown environment. In contrast to the previous practice that treats each time\nstep (pair-wise registration) individually, our unsupervised model starts with\noptimizing a sequence of latent STREP feature, which is then decoded to a\ntemporally and spatially continuous sequence of geometric transformations to\nglobally align multiple point clouds. We have evaluated our proposed approach\nover both simulated 2D and real 3D datasets and the experimental results\ndemonstrate that our method can beat other techniques by taking into account\nthe temporal information in deep feature learning.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 06:00:36 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Wang", "Lingjing", ""], ["Shi", "Yi", ""], ["Li", "Xiang", ""], ["Fang", "Yi", ""]]}, {"id": "2006.12379", "submitter": "\\'Angel Gonz\\'alez-Prieto", "authors": "Jes\\'us Bobadilla, \\'Angel Gonz\\'alez-Prieto, Fernando Ortega, Ra\\'ul\n  Lara-Cabrera", "title": "Deep Learning feature selection to unhide demographic recommender\n  systems factors", "comments": "20 pages, 14 figures, 1 table", "journal-ref": "Neural Computing and Applications, 1-18, 2020", "doi": "10.1007/s00521-020-05494-2", "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting demographic features from hidden factors is an innovative concept\nthat provides multiple and relevant applications. The matrix factorization\nmodel generates factors which do not incorporate semantic knowledge. This paper\nprovides a deep learning-based method: DeepUnHide, able to extract demographic\ninformation from the users and items factors in collaborative filtering\nrecommender systems. The core of the proposed method is the gradient-based\nlocalization used in the image processing literature to highlight the\nrepresentative areas of each classification class. Validation experiments make\nuse of two public datasets and current baselines. Results show the superiority\nof DeepUnHide to make feature selection and demographic classification,\ncompared to the state of art of feature selection methods. Relevant and direct\napplications include recommendations explanation, fairness in collaborative\nfiltering and recommendation to groups of users.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 17:36:48 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Bobadilla", "Jes\u00fas", ""], ["Gonz\u00e1lez-Prieto", "\u00c1ngel", ""], ["Ortega", "Fernando", ""], ["Lara-Cabrera", "Ra\u00fal", ""]]}, {"id": "2006.12430", "submitter": "Dmitry V. Dylov", "authors": "Kristina Belikova, Oleg Rogov, Aleksandr Rybakov, Maxim V. Maslov,\n  Dmitry V. Dylov", "title": "Deep Negative Volume Segmentation", "comments": "20 pages, 5 main figures, 3 tables, 11 supplemental figures,\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical examination of three-dimensional image data of compound anatomical\nobjects, such as complex joints, remains a tedious process, demanding the time\nand the expertise of physicians. For instance, automation of the segmentation\ntask of the TMJ (temporomandibular joint) has been hindered by its compound\nthree-dimensional shape, multiple overlaid textures, an abundance of\nsurrounding irregularities in the skull, and a virtually omnidirectional range\nof the jaw's motion - all of which extend the manual annotation process to more\nthan an hour per patient. To address the challenge, we invent a new angle to\nthe 3D segmentation task: namely, we propose to segment empty spaces between\nall the tissues surrounding the object - the so-called negative volume\nsegmentation. Our approach is an end-to-end pipeline that comprises a V-Net for\nbone segmentation, a 3D volume construction by inflation of the reconstructed\nbone head in all directions along the normal vector to its mesh faces.\nEventually confined within the skull bones, the inflated surface occupies the\nentire \"negative\" space in the joint, effectively providing a\ngeometrical/topological metric of the joint's health. We validate the idea on\nthe CT scans in a 50-patient dataset, annotated by experts in maxillofacial\nmedicine, quantitatively compare the asymmetry given the left and the right\nnegative volumes, and automate the entire framework for clinical adoption.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 16:55:23 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Belikova", "Kristina", ""], ["Rogov", "Oleg", ""], ["Rybakov", "Aleksandr", ""], ["Maslov", "Maxim V.", ""], ["Dylov", "Dmitry V.", ""]]}, {"id": "2006.12434", "submitter": "Lei Li", "authors": "Xiahai Zhuang, Jiahang Xu, Xinzhe Luo, Chen Chen, Cheng Ouyang, Daniel\n  Rueckert, Victor M. Campello, Karim Lekadir, Sulaiman Vesal, Nishant\n  RaviKumar, Yashu Liu, Gongning Luo, Jingkun Chen, Hongwei Li, Buntheng Ly,\n  Maxime Sermesant, Holger Roth, Wentao Zhu, Jiexiang Wang, Xinghao Ding,\n  Xinyue Wang, Sen Yang, Lei Li", "title": "Cardiac Segmentation on Late Gadolinium Enhancement MRI: A Benchmark\n  Study from Multi-Sequence Cardiac MR Segmentation Challenge", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurate computing, analysis and modeling of the ventricles and myocardium\nfrom medical images are important, especially in the diagnosis and treatment\nmanagement for patients suffering from myocardial infarction (MI). Late\ngadolinium enhancement (LGE) cardiac magnetic resonance (CMR) provides an\nimportant protocol to visualize MI. However, automated segmentation of LGE CMR\nis still challenging, due to the indistinguishable boundaries, heterogeneous\nintensity distribution and complex enhancement patterns of pathological\nmyocardium from LGE CMR. Furthermore, compared with the other sequences LGE CMR\nimages with gold standard labels are particularly limited, which represents\nanother obstacle for developing novel algorithms for automatic segmentation of\nLGE CMR. This paper presents the selective results from the Multi-Sequence\nCardiac MR (MS-CMR) Segmentation challenge, in conjunction with MICCAI 2019.\nThe challenge offered a data set of paired MS-CMR images, including auxiliary\nCMR sequences as well as LGE CMR, from 45 patients who underwent\ncardiomyopathy. It was aimed to develop new algorithms, as well as benchmark\nexisting ones for LGE CMR segmentation and compare them objectively. In\naddition, the paired MS-CMR images could enable algorithms to combine the\ncomplementary information from the other sequences for the segmentation of LGE\nCMR. Nine representative works were selected for evaluation and comparisons,\namong which three methods are unsupervised methods and the other six are\nsupervised. The results showed that the average performance of the nine methods\nwas comparable to the inter-observer variations. The success of these methods\nwas mainly attributed to the inclusion of the auxiliary sequences from the\nMS-CMR images, which provide important label information for the training of\ndeep neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:04:38 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 13:55:30 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhuang", "Xiahai", ""], ["Xu", "Jiahang", ""], ["Luo", "Xinzhe", ""], ["Chen", "Chen", ""], ["Ouyang", "Cheng", ""], ["Rueckert", "Daniel", ""], ["Campello", "Victor M.", ""], ["Lekadir", "Karim", ""], ["Vesal", "Sulaiman", ""], ["RaviKumar", "Nishant", ""], ["Liu", "Yashu", ""], ["Luo", "Gongning", ""], ["Chen", "Jingkun", ""], ["Li", "Hongwei", ""], ["Ly", "Buntheng", ""], ["Sermesant", "Maxime", ""], ["Roth", "Holger", ""], ["Zhu", "Wentao", ""], ["Wang", "Jiexiang", ""], ["Ding", "Xinghao", ""], ["Wang", "Xinyue", ""], ["Yang", "Sen", ""], ["Li", "Lei", ""]]}, {"id": "2006.12449", "submitter": "Jan Egger", "authors": "Jianning Li, Antonio Pepe, Christina Gsaxner, Gord von Campe, Jan\n  Egger", "title": "A Baseline Approach for AutoImplant: the MICCAI 2020 Cranial Implant\n  Design Challenge", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present a baseline approach for AutoImplant\n(https://autoimplant.grand-challenge.org/) - the cranial implant design\nchallenge, which, as suggested by the organizers, can be formulated as a\nvolumetric shape learning task. In this task, the defective skull, the complete\nskull and the cranial implant are represented as binary voxel grids. To\naccomplish this task, the implant can be either reconstructed directly from the\ndefective skull or obtained by taking the difference between a defective skull\nand a complete skull. In the latter case, a complete skull has to be\nreconstructed given a defective skull, which defines a volumetric shape\ncompletion problem. Our baseline approach for this task is based on the former\nformulation, i.e., a deep neural network is trained to predict the implants\ndirectly from the defective skulls. The approach generates high-quality\nimplants in two steps: First, an encoder-decoder network learns a coarse\nrepresentation of the implant from down-sampled, defective skulls; The coarse\nimplant is only used to generate the bounding box of the defected region in the\noriginal high-resolution skull. Second, another encoder-decoder network is\ntrained to generate a fine implant from the bounded area. On the test set, the\nproposed approach achieves an average dice similarity score (DSC) of 0.8555 and\nHausdorff distance (HD) of 5.1825 mm. The code is publicly available at\nhttps://github.com/Jianningli/autoimplant.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:27:56 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 11:22:39 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Li", "Jianning", ""], ["Pepe", "Antonio", ""], ["Gsaxner", "Christina", ""], ["von Campe", "Gord", ""], ["Egger", "Jan", ""]]}, {"id": "2006.12456", "submitter": "Jiayu Liu", "authors": "Jiayu Liu, Ioannis Chiotellis, Rudolph Triebel, Daniel Cremers", "title": "Effective Version Space Reduction for Convolutional Neural Networks", "comments": "22 pages, 8 figures, to be published in the Proceedings of the\n  European Conference on Machine Learning and Principles and Practice of\n  Knowledge Discovery in Databases (ECML PKDD) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In active learning, sampling bias could pose a serious inconsistency problem\nand hinder the algorithm from finding the optimal hypothesis. However, many\nmethods for neural networks are hypothesis space agnostic and do not address\nthis problem. We examine active learning with convolutional neural networks\nthrough the principled lens of version space reduction. We identify the\nconnection between two approaches---prior mass reduction and diameter\nreduction---and propose a new diameter-based querying method---the minimum\nGibbs-vote disagreement. By estimating version space diameter and bias, we\nillustrate how version space of neural networks evolves and examine the\nrealizability assumption. With experiments on MNIST, Fashion-MNIST, SVHN and\nSTL-10 datasets, we demonstrate that diameter reduction methods reduce the\nversion space more effectively and perform better than prior mass reduction and\nother baselines, and that the Gibbs vote disagreement is on par with the best\nquery method.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:40:03 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Liu", "Jiayu", ""], ["Chiotellis", "Ioannis", ""], ["Triebel", "Rudolph", ""], ["Cremers", "Daniel", ""]]}, {"id": "2006.12463", "submitter": "Madan Ravi Ganesh", "authors": "Madan Ravi Ganesh, Dawsin Blanchard, Jason J. Corso and Salimeh Yasaei\n  Sekeh", "title": "Slimming Neural Networks using Adaptive Connectivity Scores", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two broad approaches to deep neural network (DNN) pruning: 1)\napplying a deterministic constraint on the weight matrices, which takes\nadvantage of their ease of implementation and the learned structures of the\nweight matrix, and 2) using a probabilistic framework aimed at maintaining the\nflow of information between layers, which leverages the connections between\nfilters and their downstream impact. Each approach's advantage supplements the\nmissing portions of the alternate approach yet no one has combined and fully\ncapitalized on both of them. Further,there are some common practical issues\nthat affect both, e.g., intense manual effort to analyze sensitivity and set\nthe upper pruning limits of layers. In this work,we propose Slimming Neural\nnetworks using Adaptive Connectivity Measures(SNACS), as an algorithm that uses\na probabilistic framework for compression while incorporating weight-based\nconstraints at multiple levels to capitalize on both their strengths and\novercome previous issues. We propose a hash-based estimator of Adaptive\nConditional Mutual Information(ACMI) to evaluate the connectivity between\nfilters of different layers, which includes a magnitude-based scaling criteria\nthat leverages weight matrices. To reduce the amount of unnecessary manual\neffort required to set the upper pruning limit of different layers in a DNN we\npropose a set of operating constraints to help automatically set them. Further,\nwe take extended advantage of weight matrices by defining a sensitivity\ncriteria for filters that measures the strength of their contributions to the\nfollowing layer and highlights critical filters that need to be protected from\npruning. We show that our proposed approach is faster by over 17x the nearest\ncomparable method and outperforms all existing pruning approaches on three\nstandard Dataset-DNN benchmarks: CIFAR10-VGG16, CIFAR10-ResNet56 and\nILSVRC2012-ResNet50.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:45:16 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 14:20:09 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Ganesh", "Madan Ravi", ""], ["Blanchard", "Dawsin", ""], ["Corso", "Jason J.", ""], ["Sekeh", "Salimeh Yasaei", ""]]}, {"id": "2006.12480", "submitter": "Weidi Xie", "authors": "Fangrui Zhu, Li Zhang, Yanwei Fu, Guodong Guo, Weidi Xie", "title": "Self-supervised Video Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this paper is self-supervised representation learning, with\nthe goal of solving semi-supervised video object segmentation (a.k.a. dense\ntracking). We make the following contributions: (i) we propose to improve the\nexisting self-supervised approach, with a simple, yet more effective memory\nmechanism for long-term correspondence matching, which resolves the challenge\ncaused by the dis-appearance and reappearance of objects; (ii) by augmenting\nthe self-supervised approach with an online adaptation module, our method\nsuccessfully alleviates tracker drifts caused by spatial-temporal\ndiscontinuity, e.g. occlusions or dis-occlusions, fast motions; (iii) we\nexplore the efficiency of self-supervised representation learning for dense\ntracking, surprisingly, we show that a powerful tracking model can be trained\nwith as few as 100 raw video clips (equivalent to a duration of 11mins),\nindicating that low-level statistics have already been effective for tracking\ntasks; (iv) we demonstrate state-of-the-art results among the self-supervised\napproaches on DAVIS-2017 and YouTube-VOS, as well as surpassing most of methods\ntrained with millions of manual segmentation annotations, further bridging the\ngap between self-supervised and supervised learning. Codes are released to\nfoster any further research (https://github.com/fangruizhu/self_sup_semiVOS).\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:55:59 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zhu", "Fangrui", ""], ["Zhang", "Li", ""], ["Fu", "Yanwei", ""], ["Guo", "Guodong", ""], ["Xie", "Weidi", ""]]}, {"id": "2006.12485", "submitter": "Yang Long", "authors": "Yang Long, Gui-Song Xia, Shengyang Li, Wen Yang, Michael Ying Yang,\n  Xiao Xiang Zhu, Liangpei Zhang, Deren Li", "title": "On Creating Benchmark Dataset for Aerial Image Interpretation: Reviews,\n  Guidances and Million-AID", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The past years have witnessed great progress on remote sensing (RS) image\ninterpretation and its wide applications. With RS images becoming more\naccessible than ever before, there is an increasing demand for the automatic\ninterpretation of these images. In this context, the benchmark datasets serve\nas essential prerequisites for developing and testing intelligent\ninterpretation algorithms. After reviewing existing benchmark datasets in the\nresearch community of RS image interpretation, this article discusses the\nproblem of how to efficiently prepare a suitable benchmark dataset for RS image\ninterpretation. Specifically, we first analyze the current challenges of\ndeveloping intelligent algorithms for RS image interpretation with bibliometric\ninvestigations. We then present the general guidances on creating benchmark\ndatasets in efficient manners. Following the presented guidances, we also\nprovide an example on building RS image dataset, i.e., Million-AID, a new\nlarge-scale benchmark dataset containing a million instances for RS image scene\nclassification. Several challenges and perspectives in RS image annotation are\nfinally discussed to facilitate the research in benchmark dataset construction.\nWe do hope this paper will provide the RS community an overall perspective on\nconstructing large-scale and practical image datasets for further research,\nespecially data-driven ones.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:59:00 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 10:53:03 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Long", "Yang", ""], ["Xia", "Gui-Song", ""], ["Li", "Shengyang", ""], ["Yang", "Wen", ""], ["Yang", "Michael Ying", ""], ["Zhu", "Xiao Xiang", ""], ["Zhang", "Liangpei", ""], ["Li", "Deren", ""]]}, {"id": "2006.12486", "submitter": "Ajay Jain", "authors": "Ajay Jain and Pieter Abbeel and Deepak Pathak", "title": "Locally Masked Convolution for Autoregressive Models", "comments": "Published at Conference on Uncertainty in AI (UAI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  High-dimensional generative models have many applications including image\ncompression, multimedia generation, anomaly detection and data completion.\nState-of-the-art estimators for natural images are autoregressive, decomposing\nthe joint distribution over pixels into a product of conditionals parameterized\nby a deep neural network, e.g. a convolutional neural network such as the\nPixelCNN. However, PixelCNNs only model a single decomposition of the joint,\nand only a single generation order is efficient. For tasks such as image\ncompletion, these models are unable to use much of the observed context. To\ngenerate data in arbitrary orders, we introduce LMConv: a simple modification\nto the standard 2D convolution that allows arbitrary masks to be applied to the\nweights at each location in the image. Using LMConv, we learn an ensemble of\ndistribution estimators that share parameters but differ in generation order,\nachieving improved performance on whole-image density estimation (2.89 bpd on\nunconditional CIFAR10), as well as globally coherent image completions. Our\ncode is available at https://ajayjain.github.io/lmconv.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:59:07 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 09:02:52 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 04:53:14 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Jain", "Ajay", ""], ["Abbeel", "Pieter", ""], ["Pathak", "Deepak", ""]]}, {"id": "2006.12556", "submitter": "Kalaiarasi G", "authors": "G.Kalaiarasi, S.Maheswari", "title": "Frost filtered scale-invariant feature extraction and multilayer\n  perceptron for hyperspectral image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) classification plays a significant in the field of\nremote sensing due to its ability to provide spatial and spectral information.\nDue to the rapid development and increasing of hyperspectral remote sensing\ntechnology, many methods have been developed for HSI classification but still a\nlack of achieving the better performance. A Frost Filtered Scale-Invariant\nFeature Transformation based MultiLayer Perceptron Classification (FFSIFT-MLPC)\ntechnique is introduced for classifying the hyperspectral image with higher\naccuracy and minimum time consumption. The FFSIFT-MLPC technique performs three\nmajor processes, namely preprocessing, feature extraction and classification\nusing multiple layers. Initially, the hyperspectral image is divided into\nnumber of spectral bands. These bands are given as input in the input layer of\nperceptron. Then the Frost filter is used in FFSIFT-MLPC technique for\npreprocessing the input bands which helps to remove the noise from\nhyper-spectral image at the first hidden layer. After preprocessing task,\ntexture, color and object features of hyper-spectral image are extracted at\nsecond hidden layer using Gaussian distributive scale-invariant feature\ntransform. At the third hidden layer, Euclidean distance is measured between\nthe extracted features and testing features. Finally, feature matching is\ncarried out at the output layer for hyper-spectral image classification. The\nclassified outputs are resulted in terms of spectral bands (i.e., different\ncolors). Experimental analysis is performed with PSNR, classification accuracy,\nfalse positive rate and classification time with number of spectral bands. The\nresults evident that presented FFSIFT-MLPC technique improves the hyperspectral\nimage classification accuracy, PSNR and minimizes false positive rate as well\nas classification time than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 10:51:04 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Kalaiarasi", "G.", ""], ["Maheswari", "S.", ""]]}, {"id": "2006.12557", "submitter": "Avi Schwarzschild", "authors": "Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, Tom\n  Goldstein", "title": "Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and\n  Data Poisoning Attacks", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data poisoning and backdoor attacks manipulate training data in order to\ncause models to fail during inference. A recent survey of industry\npractitioners found that data poisoning is the number one concern among threats\nranging from model stealing to adversarial attacks. However, it remains unclear\nexactly how dangerous poisoning methods are and which ones are more effective\nconsidering that these methods, even ones with identical objectives, have not\nbeen tested in consistent or realistic settings. We observe that data poisoning\nand backdoor attacks are highly sensitive to variations in the testing setup.\nMoreover, we find that existing methods may not generalize to realistic\nsettings. While these existing works serve as valuable prototypes for data\npoisoning, we apply rigorous tests to determine the extent to which we should\nfear them. In order to promote fair comparison in future work, we develop\nstandardized benchmarks for data poisoning and backdoor attacks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 18:34:08 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 21:59:59 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 14:10:57 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Schwarzschild", "Avi", ""], ["Goldblum", "Micah", ""], ["Gupta", "Arjun", ""], ["Dickerson", "John P", ""], ["Goldstein", "Tom", ""]]}, {"id": "2006.12567", "submitter": "Changhao Chen", "authors": "Changhao Chen, Bing Wang, Chris Xiaoxuan Lu, Niki Trigoni, Andrew\n  Markham", "title": "A Survey on Deep Learning for Localization and Mapping: Towards the Age\n  of Spatial Machine Intelligence", "comments": "26 pages, 10 figures. Project website:\n  https://github.com/changhao-chen/deep-learning-localization-mapping", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning based localization and mapping has recently attracted\nsignificant attention. Instead of creating hand-designed algorithms through\nexploitation of physical models or geometric theories, deep learning based\nsolutions provide an alternative to solve the problem in a data-driven way.\nBenefiting from ever-increasing volumes of data and computational power, these\nmethods are fast evolving into a new area that offers accurate and robust\nsystems to track motion and estimate scenes and their structure for real-world\napplications. In this work, we provide a comprehensive survey, and propose a\nnew taxonomy for localization and mapping using deep learning. We also discuss\nthe limitations of current models, and indicate possible future directions. A\nwide range of topics are covered, from learning odometry estimation, mapping,\nto global localization and simultaneous localization and mapping (SLAM). We\nrevisit the problem of perceiving self-motion and scene understanding with\non-board sensors, and show how to solve it by integrating these modules into a\nprospective spatial machine intelligence system (SMIS). It is our hope that\nthis work can connect emerging works from robotics, computer vision and machine\nlearning communities, and serve as a guide for future researchers to apply deep\nlearning to tackle localization and mapping problems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 19:01:21 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 18:58:07 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Chen", "Changhao", ""], ["Wang", "Bing", ""], ["Lu", "Chris Xiaoxuan", ""], ["Trigoni", "Niki", ""], ["Markham", "Andrew", ""]]}, {"id": "2006.12575", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Can Zhao, Wenqi Li, Holger Roth, Ziyue Xu, Daguang Xu", "title": "LAMP: Large Deep Nets with Automated Model Parallelism for Image\n  Segmentation", "comments": "MICCAI 2020 Early Accepted paper. Code is\n  available\\footnote{https://monai.io/research/lamp-automated-model-parallelism}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL) models are becoming larger, because the increase in model\nsize might offer significant accuracy gain. To enable the training of large\ndeep networks, data parallelism and model parallelism are two well-known\napproaches for parallel training. However, data parallelism does not help\nreduce memory footprint per device. In this work, we introduce Large deep 3D\nConvNets with Automated Model Parallelism (LAMP) and investigate the impact of\nboth input's and deep 3D ConvNets' size on segmentation accuracy. Through\nautomated model parallelism, it is feasible to train large deep 3D ConvNets\nwith a large input patch, even the whole image. Extensive experiments\ndemonstrate that, facilitated by the automated model parallelism, the\nsegmentation accuracy can be improved through increasing model size and input\ncontext size, and large input yields significant inference speedup compared\nwith sliding window of small patches in the inference. Code is\navailable\\footnote{https://monai.io/research/lamp-automated-model-parallelism}.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 19:20:35 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 19:41:26 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 17:51:41 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Zhu", "Wentao", ""], ["Zhao", "Can", ""], ["Li", "Wenqi", ""], ["Roth", "Holger", ""], ["Xu", "Ziyue", ""], ["Xu", "Daguang", ""]]}, {"id": "2006.12582", "submitter": "Mohammad Majdi", "authors": "Mohammad Sadegh Majdi, Emad Fatemizadeh", "title": "Laplacian Mixture Model Point Based Registration", "comments": null, "journal-ref": "2015 9th Iranian Conference on Machine Vision and Image Processing\n  (MVIP), Tehran, 2015, pp. 57-60", "doi": "10.1109/IranianMVIP.2015.7397504", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point base registration is an important part in many machine VISIOn\napplications, medical diagnostics, agricultural studies etc. The goal of point\nset registration is to find correspondences between different data sets and\nestimate the appropriate transformation that can map one set to another. Here\nwe introduce a novel method for matching of different data sets based on\nLaplacian distribution. We consider the alignment of two point sets as\nprobability density estimation problem. By using maximum likelihood methods we\ntry to fit the Laplacian mixture model (LMM) centroids (source point set) to\nthe data point set.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 19:46:41 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Majdi", "Mohammad Sadegh", ""], ["Fatemizadeh", "Emad", ""]]}, {"id": "2006.12585", "submitter": "Preethi Srinivasan Ms", "authors": "Preethi Srinivasan, Prabhjot Kaur, Aditya Nigam, Arnav Bhavsar", "title": "Semantic Features Aided Multi-Scale Reconstruction of Inter-Modality\n  Magnetic Resonance Images", "comments": "Accepted in IEEE CMBS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long acquisition time (AQT) due to series acquisition of multi-modality MR\nimages (especially T2 weighted images (T2WI) with longer AQT), though\nbeneficial for disease diagnosis, is practically undesirable. We propose a\nnovel deep network based solution to reconstruct T2W images from T1W images\n(T1WI) using an encoder-decoder architecture. The proposed learning is aided\nwith semantic features by using multi-channel input with intensity values and\ngradient of image in two orthogonal directions. A reconstruction module (RM)\naugmenting the network along with a domain adaptation module (DAM) which is an\nencoder-decoder model built-in with sharp bottleneck module (SBM) is trained\nvia modular training. The proposed network significantly reduces the total AQT\nwith negligible qualitative artifacts and quantitative loss (reconstructs one\nvolume in approximately 1 second). The testing is done on publicly available\ndataset with real MR images, and the proposed network shows (approximately 1dB)\nincrease in PSNR over SOTA.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 19:53:50 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Srinivasan", "Preethi", ""], ["Kaur", "Prabhjot", ""], ["Nigam", "Aditya", ""], ["Bhavsar", "Arnav", ""]]}, {"id": "2006.12586", "submitter": "Mohammad Majdi", "authors": "Mohammed S. Majdi, Sundaresh Ram, Jonathan T. Gill, Jeffery J.\n  Rodriguez", "title": "Drive-Net: Convolutional Network for Driver Distraction Detection", "comments": null, "journal-ref": "2018 IEEE Southwest Symposium on Image Analysis and Interpretation\n  (SSIAI), Las Vegas, NV, 2018, pp. 1-4,", "doi": "10.1109/SSIAI.2018.8470309", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To help prevent motor vehicle accidents, there has been significant interest\nin finding an automated method to recognize signs of driver distraction, such\nas talking to passengers, fixing hair and makeup, eating and drinking, and\nusing a mobile phone. In this paper, we present an automated supervised\nlearning method called Drive-Net for driver distraction detection. Drive-Net\nuses a combination of a convolutional neural network (CNN) and a random\ndecision forest for classifying images of a driver. We compare the performance\nof our proposed Drive-Net to two other popular machine-learning approaches: a\nrecurrent neural network (RNN), and a multi-layer perceptron (MLP). We test the\nmethods on a publicly available database of images acquired under a controlled\nenvironment containing about 22425 images manually annotated by an expert.\nResults show that Drive-Net achieves a detection accuracy of 95%, which is 2%\nmore than the best results obtained on the same database using other methods\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 19:54:53 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Majdi", "Mohammed S.", ""], ["Ram", "Sundaresh", ""], ["Gill", "Jonathan T.", ""], ["Rodriguez", "Jeffery J.", ""]]}, {"id": "2006.12634", "submitter": "Chang Xiao", "authors": "Jingtian Peng, Chang Xiao, Yifan Li", "title": "RP2K: A Large-Scale Retail Product Dataset for Fine-Grained Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce RP2K, a new large-scale retail product dataset for fine-grained\nimage classification. Unlike previous datasets focusing on relatively few\nproducts, we collect more than 500,000 images of retail products on shelves\nbelonging to 2000 different products. Our dataset aims to advance the research\nin retail object recognition, which has massive applications such as automatic\nshelf auditing and image-based product information retrieval. Our dataset\nenjoys following properties: (1) It is by far the largest scale dataset in\nterms of product categories. (2) All images are captured manually in physical\nretail stores with natural lightings, matching the scenario of real\napplications. (3) We provide rich annotations to each object, including the\nsizes, shapes and flavors/scents. We believe our dataset could benefit both\ncomputer vision research and retail industry. Our dataset is publicly available\nat https://www.pinlandata.com/rp2k_dataset.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 21:39:56 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 02:50:10 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 20:36:39 GMT"}, {"version": "v4", "created": "Wed, 8 Jul 2020 00:42:47 GMT"}, {"version": "v5", "created": "Mon, 18 Jan 2021 16:09:55 GMT"}, {"version": "v6", "created": "Wed, 3 Mar 2021 18:56:12 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Peng", "Jingtian", ""], ["Xiao", "Chang", ""], ["Li", "Yifan", ""]]}, {"id": "2006.12655", "submitter": "Cassidy Laidlaw", "authors": "Cassidy Laidlaw and Sahil Singla and Soheil Feizi", "title": "Perceptual Adversarial Robustness: Defense Against Unseen Threat Models", "comments": "Published in ICLR 2021. Code and data are available at\n  https://github.com/cassidylaidlaw/perceptual-advex", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in adversarial robustness is the lack of a precise\nmathematical characterization of human perception, used in the very definition\nof adversarial attacks that are imperceptible to human eyes. Most current\nattacks and defenses try to avoid this issue by considering restrictive\nadversarial threat models such as those bounded by $L_2$ or $L_\\infty$\ndistance, spatial perturbations, etc. However, models that are robust against\nany of these restrictive threat models are still fragile against other threat\nmodels. To resolve this issue, we propose adversarial training against the set\nof all imperceptible adversarial examples, approximated using deep neural\nnetworks. We call this threat model the neural perceptual threat model (NPTM);\nit includes adversarial examples with a bounded neural perceptual distance (a\nneural network-based approximation of the true perceptual distance) to natural\nimages. Through an extensive perceptual study, we show that the neural\nperceptual distance correlates well with human judgements of perceptibility of\nadversarial examples, validating our threat model.\n  Under the NPTM, we develop novel perceptual adversarial attacks and defenses.\nBecause the NPTM is very broad, we find that Perceptual Adversarial Training\n(PAT) against a perceptual attack gives robustness against many other types of\nadversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five\ndiverse adversarial attacks. We find that PAT achieves state-of-the-art\nrobustness against the union of these five attacks, more than doubling the\naccuracy over the next best model, without training against any of them. That\nis, PAT generalizes well to unforeseen perturbation types. This is vital in\nsensitive applications where a particular threat model cannot be assumed, and\nto the best of our knowledge, PAT is the first adversarial training defense\nwith this property.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 22:40:46 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 22:07:41 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 17:28:08 GMT"}, {"version": "v4", "created": "Sun, 4 Jul 2021 19:34:05 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Laidlaw", "Cassidy", ""], ["Singla", "Sahil", ""], ["Feizi", "Soheil", ""]]}, {"id": "2006.12671", "submitter": "Runzhou Ge", "authors": "Runzhou Ge, Zhuangzhuang Ding, Yihan Hu, Yu Wang, Sijia Chen, Li\n  Huang, Yuan Li", "title": "AFDet: Anchor Free One Stage 3D Object Detection", "comments": "Accepted on May 6th, 2020 by CVPRW 2020, published on June 7th, 2020;\n  Baseline detector for the 1st place solutions of Waymo Open Dataset\n  Challenges 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-efficiency point cloud 3D object detection operated on embedded systems\nis important for many robotics applications including autonomous driving. Most\nprevious works try to solve it using anchor-based detection methods which come\nwith two drawbacks: post-processing is relatively complex and computationally\nexpensive; tuning anchor parameters is tricky. We are the first to address\nthese drawbacks with an anchor free and Non-Maximum Suppression free one stage\ndetector called AFDet. The entire AFDet can be processed efficiently on a CNN\naccelerator or a GPU with the simplified post-processing. Without bells and\nwhistles, our proposed AFDet performs competitively with other one stage\nanchor-based methods on KITTI validation set and Waymo Open Dataset validation\nset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 00:15:07 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 07:03:40 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Ge", "Runzhou", ""], ["Ding", "Zhuangzhuang", ""], ["Hu", "Yihan", ""], ["Wang", "Yu", ""], ["Chen", "Sijia", ""], ["Huang", "Li", ""], ["Li", "Yuan", ""]]}, {"id": "2006.12674", "submitter": "Lindon Roberts", "authors": "Matthias J. Ehrhardt, Lindon Roberts", "title": "Inexact Derivative-Free Optimization for Bilevel Learning", "comments": "Accepted to Journal of Mathematical Imaging and Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational regularization techniques are dominant in the field of\nmathematical imaging. A drawback of these techniques is that they are dependent\non a number of parameters which have to be set by the user. A by now common\nstrategy to resolve this issue is to learn these parameters from data. While\nmathematically appealing this strategy leads to a nested optimization problem\n(known as bilevel optimization) which is computationally very difficult to\nhandle. It is common when solving the upper-level problem to assume access to\nexact solutions of the lower-level problem, which is practically infeasible. In\nthis work we propose to solve these problems using inexact derivative-free\noptimization algorithms which never require exact lower-level problem\nsolutions, but instead assume access to approximate solutions with controllable\naccuracy, which is achievable in practice. We prove global convergence and a\nworstcase complexity bound for our approach. We test our proposed framework on\nROFdenoising and learning MRI sampling patterns. Dynamically adjusting the\nlower-level accuracy yields learned parameters with similar reconstruction\nquality as highaccuracy evaluations but with dramatic reductions in\ncomputational work (up to 100 times faster in some cases).\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 00:17:32 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 23:10:15 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Ehrhardt", "Matthias J.", ""], ["Roberts", "Lindon", ""]]}, {"id": "2006.12676", "submitter": "Michael Hegedus", "authors": "Michael Hegedus, Kamal Gupta, Mehran Mehrandezh", "title": "Generalized Grasping for Mechanical Grippers for Unknown Objects with\n  Partial Point Cloud Representations", "comments": "8 pages, 12 figures, submitted to 2020 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2018) on 2/24/2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generalized grasping algorithm that uses point clouds (i.e. a\ngroup of points and their respective surface normals) to discover grasp pose\nsolutions for multiple grasp types, executed by a mechanical gripper, in near\nreal-time. The algorithm introduces two ideas: 1) a histogram of finger contact\nnormals is used to represent a grasp 'shape' to guide a gripper orientation\nsearch in a histogram of object(s) surface normals, and 2) voxel grid\nrepresentations of gripper and object(s) are cross-correlated to match finger\ncontact points, i.e. grasp 'size', to discover a grasp pose. Constraints, such\nas collisions with neighbouring objects, are optionally incorporated in the\ncross-correlation computation. We show via simulations and experiments that 1)\ngrasp poses for three grasp types can be found in near real-time, 2) grasp pose\nsolutions are consistent with respect to voxel resolution changes for both\npartial and complete point cloud scans, and 3) a planned grasp is executed with\na mechanical gripper.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 00:34:05 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Hegedus", "Michael", ""], ["Gupta", "Kamal", ""], ["Mehrandezh", "Mehran", ""]]}, {"id": "2006.12681", "submitter": "Minguk Kang", "authors": "Minguk Kang and Jaesik Park", "title": "ContraGAN: Contrastive Learning for Conditional Image Generation", "comments": "34th Conference on Neural Information Processing Systems (NeurIPS\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional image generation is the task of generating diverse images using\nclass label information. Although many conditional Generative Adversarial\nNetworks (GAN) have shown realistic results, such methods consider pairwise\nrelations between the embedding of an image and the embedding of the\ncorresponding label (data-to-class relations) as the conditioning losses. In\nthis paper, we propose ContraGAN that considers relations between multiple\nimage embeddings in the same batch (data-to-data relations) as well as the\ndata-to-class relations by using a conditional contrastive loss. The\ndiscriminator of ContraGAN discriminates the authenticity of given samples and\nminimizes a contrastive objective to learn the relations between training\nimages. Simultaneously, the generator tries to generate realistic images that\ndeceive the authenticity and have a low contrastive loss. The experimental\nresults show that ContraGAN outperforms state-of-the-art-models by 7.3% and\n7.7% on Tiny ImageNet and ImageNet datasets, respectively. Besides, we\nexperimentally demonstrate that contrastive learning helps to relieve the\noverfitting of the discriminator. For a fair comparison, we re-implement twelve\nstate-of-the-art GANs using the PyTorch library. The software package is\navailable at https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 00:49:05 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 13:52:46 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 05:34:11 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Kang", "Minguk", ""], ["Park", "Jaesik", ""]]}, {"id": "2006.12704", "submitter": "Junshen Xu", "authors": "Junshen Xu, Sayeri Lala, Borjan Gagoski, Esra Abaci Turk, P. Ellen\n  Grant, Polina Golland, Elfar Adalsteinsson", "title": "Semi-Supervised Learning for Fetal Brain MRI Quality Assessment with ROI\n  consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fetal brain MRI is useful for diagnosing brain abnormalities but is\nchallenged by fetal motion. The current protocol for T2-weighted fetal brain\nMRI is not robust to motion so image volumes are degraded by inter- and intra-\nslice motion artifacts. Besides, manual annotation for fetal MR image quality\nassessment are usually time-consuming. Therefore, in this work, a\nsemi-supervised deep learning method that detects slices with artifacts during\nthe brain volume scan is proposed. Our method is based on the mean teacher\nmodel, where we not only enforce consistency between student and teacher models\non the whole image, but also adopt an ROI consistency loss to guide the network\nto focus on the brain region. The proposed method is evaluated on a fetal brain\nMR dataset with 11,223 labeled images and more than 200,000 unlabeled images.\nResults show that compared with supervised learning, the proposed method can\nimprove model accuracy by about 6\\% and outperform other state-of-the-art\nsemi-supervised learning methods. The proposed method is also implemented and\nevaluated on an MR scanner, which demonstrates the feasibility of online image\nquality assessment and image reacquisition during fetal MR scans.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 02:40:45 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Xu", "Junshen", ""], ["Lala", "Sayeri", ""], ["Gagoski", "Borjan", ""], ["Turk", "Esra Abaci", ""], ["Grant", "P. Ellen", ""], ["Golland", "Polina", ""], ["Adalsteinsson", "Elfar", ""]]}, {"id": "2006.12706", "submitter": "Ali Hatamizadeh", "authors": "Ali Hatamizadeh", "title": "Deep Learning of Unified Region, Edge, and Contour Models for Automated\n  Image Segmentation", "comments": "PhD dissertation, UCLA, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is a fundamental and challenging problem in computer\nvision with applications spanning multiple areas, such as medical imaging,\nremote sensing, and autonomous vehicles. Recently, convolutional neural\nnetworks (CNNs) have gained traction in the design of automated segmentation\npipelines. Although CNN-based models are adept at learning abstract features\nfrom raw image data, their performance is dependent on the availability and\nsize of suitable training datasets. Additionally, these models are often unable\nto capture the details of object boundaries and generalize poorly to unseen\nclasses. In this thesis, we devise novel methodologies that address these\nissues and establish robust representation learning frameworks for\nfully-automatic semantic segmentation in medical imaging and mainstream\ncomputer vision. In particular, our contributions include (1) state-of-the-art\n2D and 3D image segmentation networks for computer vision and medical image\nanalysis, (2) an end-to-end trainable image segmentation framework that unifies\nCNNs and active contour models with learnable parameters for fast and robust\nobject delineation, (3) a novel approach for disentangling edge and texture\nprocessing in segmentation networks, and (4) a novel few-shot learning model in\nboth supervised settings and semi-supervised settings where synergies between\nlatent and image spaces are leveraged to learn to segment images given limited\ntraining data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 02:54:55 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Hatamizadeh", "Ali", ""]]}, {"id": "2006.12708", "submitter": "Mingyuan Mao", "authors": "Mingyuan Mao, Yuxin Tian, Baochang Zhang, Qixiang Ye, Wanquan Liu,\n  Guodong Guo, David Doermann", "title": "iffDetector: Inference-aware Feature Filtering for Object Detection", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern CNN-based object detectors focus on feature configuration during\ntraining but often ignore feature optimization during inference. In this paper,\nwe propose a new feature optimization approach to enhance features and suppress\nbackground noise in both the training and inference stages. We introduce a\ngeneric Inference-aware Feature Filtering (IFF) module that can easily be\ncombined with modern detectors, resulting in our iffDetector. Unlike\nconventional open-loop feature calculation approaches without feedback, the IFF\nmodule performs closed-loop optimization by leveraging high-level semantics to\nenhance the convolutional features. By applying Fourier transform analysis, we\ndemonstrate that the IFF module acts as a negative feedback that theoretically\nguarantees the stability of feature learning. IFF can be fused with CNN-based\nobject detectors in a plug-and-play manner with negligible computational cost\noverhead. Experiments on the PASCAL VOC and MS COCO datasets demonstrate that\nour iffDetector consistently outperforms state-of-the-art methods by\nsignificant margins\\footnote{The test code and model are anonymously available\nin https://github.com/anonymous2020new/iffDetector }.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 02:57:29 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Mao", "Mingyuan", ""], ["Tian", "Yuxin", ""], ["Zhang", "Baochang", ""], ["Ye", "Qixiang", ""], ["Liu", "Wanquan", ""], ["Guo", "Guodong", ""], ["Doermann", "David", ""]]}, {"id": "2006.12709", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi, Abdelrahman Abdelhamed, Abdullah Abuolaim, Abhijith\n  Punnappurath, and Michael S. Brown", "title": "CIE XYZ Net: Unprocessing Images for Low-Level Computer Vision Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cameras currently allow access to two image states: (i) a minimally processed\nlinear raw-RGB image state (i.e., raw sensor data) or (ii) a highly-processed\nnonlinear image state (e.g., sRGB). There are many computer vision tasks that\nwork best with a linear image state, such as image deblurring and image\ndehazing. Unfortunately, the vast majority of images are saved in the nonlinear\nimage state. Because of this, a number of methods have been proposed to\n\"unprocess\" nonlinear images back to a raw-RGB state. However, existing\nunprocessing methods have a drawback because raw-RGB images are\nsensor-specific. As a result, it is necessary to know which camera produced the\nsRGB output and use a method or network tailored for that sensor to properly\nunprocess it. This paper addresses this limitation by exploiting another camera\nimage state that is not available as an output, but it is available inside the\ncamera pipeline. In particular, cameras apply a colorimetric conversion step to\nconvert the raw-RGB image to a device-independent space based on the CIE XYZ\ncolor space before they apply the nonlinear photo-finishing. Leveraging this\ncanonical image state, we propose a deep learning framework, CIE XYZ Net, that\ncan unprocess a nonlinear image back to the canonical CIE XYZ image. This image\ncan then be processed by any low-level computer vision operator and re-rendered\nback to the nonlinear image. We demonstrate the usefulness of the CIE XYZ Net\non several low-level vision tasks and show significant gains that can be\nobtained by this processing framework. Code and dataset are publicly available\nat https://github.com/mahmoudnafifi/CIE_XYZ_NET.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 02:59:11 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Afifi", "Mahmoud", ""], ["Abdelhamed", "Abdelrahman", ""], ["Abuolaim", "Abdullah", ""], ["Punnappurath", "Abhijith", ""], ["Brown", "Michael S.", ""]]}, {"id": "2006.12712", "submitter": "Kanglin Liu", "authors": "Kanglin Liu and Qing Li and Guoping Qiu", "title": "PoseGAN: A Pose-to-Image Translation Framework for Camera Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera localization is a fundamental requirement in robotics and computer\nvision. This paper introduces a pose-to-image translation framework to tackle\nthe camera localization problem. We present PoseGANs, a conditional generative\nadversarial networks (cGANs) based framework for the implementation of\npose-to-image translation. PoseGANs feature a number of innovations including a\ndistance metric based conditional discriminator to conduct camera localization\nand a pose estimation technique for generated camera images as a stronger\nconstraint to improve camera localization performance. Compared with\nlearning-based regression methods such as PoseNet, PoseGANs can achieve better\nperformance with model sizes that are 70% smaller. In addition, PoseGANs\nintroduce the view synthesis technique to establish the correspondence between\nthe 2D images and the scene, \\textit{i.e.}, given a pose, PoseGANs are able to\nsynthesize its corresponding camera images. Furthermore, we demonstrate that\nPoseGANs differ in principle from structure-based localization and\nlearning-based regressions for camera localization, and show that PoseGANs\nexploit the geometric structures to accomplish the camera localization task,\nand is therefore more stable than and superior to learning-based regressions\nwhich rely on local texture features instead. In addition to camera\nlocalization and view synthesis, we also demonstrate that PoseGANs can be\nsuccessfully used for other interesting applications such as moving object\nelimination and frame interpolation in video sequences.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 03:15:04 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Liu", "Kanglin", ""], ["Li", "Qing", ""], ["Qiu", "Guoping", ""]]}, {"id": "2006.12745", "submitter": "Shuqi Yang", "authors": "Shuqi Yang, Xingzhe He, Bo Zhu", "title": "Learning Physical Constraints with Neural Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new family of neural networks to predict the behaviors of\nphysical systems by learning their underpinning constraints. A neural\nprojection operator lies at the heart of our approach, composed of a\nlightweight network with an embedded recursive architecture that interactively\nenforces learned underpinning constraints and predicts the various governed\nbehaviors of different physical systems. Our neural projection operator is\nmotivated by the position-based dynamics model that has been used widely in\ngame and visual effects industries to unify the various fast physics\nsimulators. Our method can automatically and effectively uncover a broad range\nof constraints from observation point data, such as length, angle, bending,\ncollision, boundary effects, and their arbitrary combinations, without any\nconnectivity priors. We provide a multi-group point representation in\nconjunction with a configurable network connection mechanism to incorporate\nprior inputs for processing complex physical systems. We demonstrated the\nefficacy of our approach by learning a set of challenging physical systems all\nin a unified and simple fashion including: rigid bodies with complex\ngeometries, ropes with varying length and bending, articulated soft and rigid\nbodies, and multi-object collisions with complex boundaries.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 04:19:04 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 23:08:33 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Yang", "Shuqi", ""], ["He", "Xingzhe", ""], ["Zhu", "Bo", ""]]}, {"id": "2006.12761", "submitter": "Mingxi Lei", "authors": "Mingxi Lei, Bino Varghese, Darryl Hwang, Steven Cen, Xiaomeng Lei,\n  Afshin Azadikhah, Bhushan Desai, Assad Oberai, Vinay Duddalwar", "title": "Benchmarking features from different radiomics toolkits / toolboxes\n  using Image Biomarkers Standardization Initiative", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is no consensus regarding the radiomic feature terminology, the\nunderlying mathematics, or their implementation. This creates a scenario where\nfeatures extracted using different toolboxes could not be used to build or\nvalidate the same model leading to a non-generalization of radiomic results. In\nthis study, the image biomarker standardization initiative (IBSI) established\nphantom and benchmark values were used to compare the variation of the radiomic\nfeatures while using 6 publicly available software programs and 1 in-house\nradiomics pipeline. All IBSI-standardized features (11 classes, 173 in total)\nwere extracted. The relative differences between the extracted feature values\nfrom the different software and the IBSI benchmark values were calculated to\nmeasure the inter-software agreement. To better understand the variations,\nfeatures are further grouped into 3 categories according to their properties:\n1) morphology, 2) statistic/histogram and 3)texture features. While a good\nagreement was observed for a majority of radiomics features across the various\nprograms, relatively poor agreement was observed for morphology features.\nSignificant differences were also found in programs that use different gray\nlevel discretization approaches. Since these programs do not include all IBSI\nfeatures, the level of quantitative assessment for each category was analyzed\nusing Venn and the UpSet diagrams and also quantified using two ad hoc metrics.\nMorphology features earns lowest scores for both metrics, indicating that\nmorphological features are not consistently evaluated among software programs.\nWe conclude that radiomic features calculated using different software programs\nmay not be identical and reliable. Further studies are needed to standardize\nthe workflow of radiomic feature extraction.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 05:02:11 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Lei", "Mingxi", ""], ["Varghese", "Bino", ""], ["Hwang", "Darryl", ""], ["Cen", "Steven", ""], ["Lei", "Xiaomeng", ""], ["Azadikhah", "Afshin", ""], ["Desai", "Bhushan", ""], ["Oberai", "Assad", ""], ["Duddalwar", "Vinay", ""]]}, {"id": "2006.12770", "submitter": "Jing Wang", "authors": "Jing Wang, Jiahong Chen, Jianzhe Lin, Leonid Sigal, and Clarence W. de\n  Silva", "title": "Discriminative Feature Alignment: Improving Transferability of\n  Unsupervised Domain Adaptation by Gaussian-guided Latent Alignment", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we focus on the unsupervised domain adaptation problem where\nan approximate inference model is to be learned from a labeled data domain and\nexpected to generalize well to an unlabeled data domain. The success of\nunsupervised domain adaptation largely relies on the cross-domain feature\nalignment. Previous work has attempted to directly align latent features by the\nclassifier-induced discrepancies. Nevertheless, a common feature space cannot\nalways be learned via this direct feature alignment especially when a large\ndomain gap exists. To solve this problem, we introduce a Gaussian-guided latent\nalignment approach to align the latent feature distributions of the two domains\nunder the guidance of the prior distribution. In such an indirect way, the\ndistributions over the samples from the two domains will be constructed on a\ncommon feature space, i.e., the space of the prior, which promotes better\nfeature alignment. To effectively align the target latent distribution with\nthis prior distribution, we also propose a novel unpaired L1-distance by taking\nadvantage of the formulation of the encoder-decoder. The extensive evaluations\non nine benchmark datasets validate the superior knowledge transferability\nthrough outperforming state-of-the-art methods and the versatility of the\nproposed method by improving the existing work significantly.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 05:33:54 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 21:34:41 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 18:28:05 GMT"}, {"version": "v4", "created": "Wed, 22 Jul 2020 23:44:29 GMT"}, {"version": "v5", "created": "Sun, 9 Aug 2020 18:10:15 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wang", "Jing", ""], ["Chen", "Jiahong", ""], ["Lin", "Jianzhe", ""], ["Sigal", "Leonid", ""], ["de Silva", "Clarence W.", ""]]}, {"id": "2006.12774", "submitter": "Yanan Wang", "authors": "Yanan Wang, Shengcai Liao, Ling Shao", "title": "Surpassing Real-World Source Training Data: Random 3D Characters for\n  Generalizable Person Re-Identification", "comments": "This is the ACMMM 2020 version, including the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification has seen significant advancement in recent years.\nHowever, the ability of learned models to generalize to unknown target domains\nstill remains limited. One possible reason for this is the lack of large-scale\nand diverse source training data, since manually labeling such a dataset is\nvery expensive and privacy sensitive. To address this, we propose to\nautomatically synthesize a large-scale person re-identification dataset\nfollowing a set-up similar to real surveillance but with virtual environments,\nand then use the synthesized person images to train a generalizable person\nre-identification model. Specifically, we design a method to generate a large\nnumber of random UV texture maps and use them to create different 3D clothing\nmodels. Then, an automatic code is developed to randomly generate various\ndifferent 3D characters with diverse clothes, races and attributes. Next, we\nsimulate a number of different virtual environments using Unity3D, with\ncustomized camera networks similar to real surveillance systems, and import\nmultiple 3D characters at the same time, with various movements and\ninteractions along different paths through the camera networks. As a result, we\nobtain a virtual dataset, called RandPerson, with 1,801,816 person images of\n8,000 identities. By training person re-identification models on these\nsynthesized person images, we demonstrate, for the first time, that models\ntrained on virtual data can generalize well to unseen target images, surpassing\nthe models trained on various real-world datasets, including CUHK03,\nMarket-1501, DukeMTMC-reID, and almost MSMT17. The RandPerson dataset is\navailable at https://github.com/VideoObjectSearch/RandPerson.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 05:38:47 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 14:22:24 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Wang", "Yanan", ""], ["Liao", "Shengcai", ""], ["Shao", "Ling", ""]]}, {"id": "2006.12791", "submitter": "Saad Imran", "authors": "Saad Imran, Muhammad Umar Karim Khan, Sikander Bin Mukarram, Chong-Min\n  Kyung", "title": "Increased-Range Unsupervised Monocular Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised deep learning methods have shown promising performance for\nsingle-image depth estimation. Since most of these methods use binocular stereo\npairs for self-supervision, the depth range is generally limited.\nSmall-baseline stereo pairs provide small depth range but handle occlusions\nwell. On the other hand, stereo images acquired with a wide-baseline rig cause\nocclusions-related errors in the near range but estimate depth well in the far\nrange. In this work, we propose to integrate the advantages of the small and\nwide baselines. By training the network using three horizontally aligned views,\nwe obtain accurate depth predictions for both close and far ranges. Our\nstrategy allows to infer multi-baseline depth from a single image. This is\nunlike previous multi-baseline systems which employ more than two cameras. The\nqualitative and quantitative results show the superior performance of\nmulti-baseline approach over previous stereo-based monocular methods. For 0.1\nto 80 meters depth range, our approach decreases the absolute relative error of\ndepth by 24% compared to Monodepth2. Our approach provides 21 frames per second\non a single Nvidia1080 GPU, making it useful for practical applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 07:01:32 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Imran", "Saad", ""], ["Khan", "Muhammad Umar Karim", ""], ["Mukarram", "Sikander Bin", ""], ["Kyung", "Chong-Min", ""]]}, {"id": "2006.12797", "submitter": "Zhelun Shen", "authors": "Zhelun Shen, Yuchao Dai, Zhibo Rao", "title": "MSMD-Net: Deep Stereo Matching with Multi-scale and Multi-dimension Cost\n  Volume", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep end-to-end learning based stereo matching methods have achieved great\nsuccess as witnessed by the leaderboards across different benchmarking datasets\n(KITTI, Middlebury, ETH3D, etc). However, real scenarios not only require\napproaches to have state-of-the-art performance but also real-time speed and\ndomain-across generalization, which cannot be satisfied by existing methods. In\nthis paper, we propose MSMD-Net (Multi-Scale and Multi-Dimension) to construct\nmulti-scale and multi-dimension cost volume. At the multi-scale level, we\ngenerate four 4D combination volumes at different scales and integrate them\nwith an encoder-decoder process to predict an initial disparity estimation. At\nthe multi-dimension level, we additionally construct a 3D warped correlation\nvolume and use it to refine the initial disparity map with residual learning.\nThese two dimensional cost volumes are complementary to each other and can\nboost the performance of disparity estimation. Additionally, we propose a\nswitch training strategy to alleviate the overfitting issue appeared in the\npre-training process and further improve the generalization ability and\naccuracy of final disparity estimation. Our proposed method was evaluated on\nseveral benchmark datasets and ranked first on KITTI 2012 leaderboard and\nsecond on KITTI 2015 leaderboard as of September 9. In addition, our method\nshows strong domain-across generalization and outperforms best prior work by a\nnoteworthy margin with three or even five times faster speed. The code of\nMSMD-Net is available at https://github.com/gallenszl/MSMD-Net.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 07:12:00 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 11:21:08 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Shen", "Zhelun", ""], ["Dai", "Yuchao", ""], ["Rao", "Zhibo", ""]]}, {"id": "2006.12800", "submitter": "Thalaiyasingam Ajanthan", "authors": "Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink,\n  Cristian Sminchisescu, Richard Hartley", "title": "Calibration of Neural Networks using Splines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibrating neural networks is of utmost importance when employing them in\nsafety-critical applications where the downstream decision making depends on\nthe predicted probabilities. Measuring calibration error amounts to comparing\ntwo empirical distributions. In this work, we introduce a binning-free\ncalibration measure inspired by the classical Kolmogorov-Smirnov (KS)\nstatistical test in which the main idea is to compare the respective cumulative\nprobability distributions. From this, by approximating the empirical cumulative\ndistribution using a differentiable function via splines, we obtain a\nrecalibration function, which maps the network outputs to actual (calibrated)\nclass assignment probabilities. The spine-fitting is performed using a held-out\ncalibration set and the obtained recalibration function is evaluated on an\nunseen test set. We tested our method against existing calibration approaches\non various image classification datasets and our spline-based recalibration\napproach consistently outperforms existing methods on KS error as well as other\ncommonly used calibration measures.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 07:18:05 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Gupta", "Kartik", ""], ["Rahimi", "Amir", ""], ["Ajanthan", "Thalaiyasingam", ""], ["Mensink", "Thomas", ""], ["Sminchisescu", "Cristian", ""], ["Hartley", "Richard", ""]]}, {"id": "2006.12807", "submitter": "Thalaiyasingam Ajanthan", "authors": "Amir Rahimi, Kartik Gupta, Thalaiyasingam Ajanthan, Thomas Mensink,\n  Cristian Sminchisescu, Richard Hartley", "title": "Post-hoc Calibration of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration of neural networks is a critical aspect to consider when\nincorporating machine learning models in real-world decision-making systems\nwhere the confidence of decisions are equally important as the decisions\nthemselves. In recent years, there is a surge of research on neural network\ncalibration and the majority of the works can be categorized into post-hoc\ncalibration methods, defined as methods that learn an additional function to\ncalibrate an already trained base network. In this work, we intend to\nunderstand the post-hoc calibration methods from a theoretical point of view.\nEspecially, it is known that minimizing Negative Log-Likelihood (NLL) will lead\nto a calibrated network on the training set if the global optimum is attained\n(Bishop, 1994). Nevertheless, it is not clear learning an additional function\nin a post-hoc manner would lead to calibration in the theoretical sense. To\nthis end, we prove that even though the base network ($f$) does not lead to the\nglobal optimum of NLL, by adding additional layers ($g$) and minimizing NLL by\noptimizing the parameters of $g$ one can obtain a calibrated network $g \\circ\nf$. This not only provides a less stringent condition to obtain a calibrated\nnetwork but also provides a theoretical justification of post-hoc calibration\nmethods. Our experiments on various image classification benchmarks confirm the\ntheory.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 07:55:10 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Rahimi", "Amir", ""], ["Gupta", "Kartik", ""], ["Ajanthan", "Thalaiyasingam", ""], ["Mensink", "Thomas", ""], ["Sminchisescu", "Cristian", ""], ["Hartley", "Richard", ""]]}, {"id": "2006.12809", "submitter": "Athanasios Vlontzos", "authors": "Athanasios Vlontzos, Samuel Budd, Benjamin Hou, Daniel Rueckert,\n  Bernhard Kainz", "title": "3D Probabilistic Segmentation and Volumetry from 2D projection images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-Ray imaging is quick, cheap and useful for front-line care assessment and\nintra-operative real-time imaging (e.g., C-Arm Fluoroscopy). However, it\nsuffers from projective information loss and lacks vital volumetric information\non which many essential diagnostic biomarkers are based on. In this paper we\nexplore probabilistic methods to reconstruct 3D volumetric images from 2D\nimaging modalities and measure the models' performance and confidence. We show\nour models' performance on large connected structures and we test for\nlimitations regarding fine structures and image domain sensitivity. We utilize\nfast end-to-end training of a 2D-3D convolutional networks, evaluate our method\non 117 CT scans segmenting 3D structures from digitally reconstructed\nradiographs (DRRs) with a Dice score of $0.91 \\pm 0.0013$. Source code will be\nmade available by the time of the conference.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 08:00:51 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Vlontzos", "Athanasios", ""], ["Budd", "Samuel", ""], ["Hou", "Benjamin", ""], ["Rueckert", "Daniel", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2006.12813", "submitter": "Eugene Lee", "authors": "Eugene Lee and Chen-Yi Lee", "title": "NeuralScale: Efficient Scaling of Neurons for Resource-Constrained Deep\n  Neural Networks", "comments": "17 pages, 11 figures, accepted by CVPR as oral paper", "journal-ref": "In Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (pp. 1478-1487) 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deciding the amount of neurons during the design of a deep neural network to\nmaximize performance is not intuitive. In this work, we attempt to search for\nthe neuron (filter) configuration of a fixed network architecture that\nmaximizes accuracy. Using iterative pruning methods as a proxy, we parameterize\nthe change of the neuron (filter) number of each layer with respect to the\nchange in parameters, allowing us to efficiently scale an architecture across\narbitrary sizes. We also introduce architecture descent which iteratively\nrefines the parameterized function used for model scaling. The combination of\nboth proposed methods is coined as NeuralScale. To prove the efficiency of\nNeuralScale in terms of parameters, we show empirical simulations on VGG11,\nMobileNetV2 and ResNet18 using CIFAR10, CIFAR100 and TinyImageNet as benchmark\ndatasets. Our results show an increase in accuracy of 3.04%, 8.56% and 3.41%\nfor VGG11, MobileNetV2 and ResNet18 on CIFAR10, CIFAR100 and TinyImageNet\nrespectively under a parameter-constrained setting (output neurons (filters) of\ndefault configuration with scaling factor of 0.25).\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 08:14:02 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Lee", "Eugene", ""], ["Lee", "Chen-Yi", ""]]}, {"id": "2006.12834", "submitter": "Francesco Croce", "authors": "Francesco Croce, Maksym Andriushchenko, Naman D. Singh, Nicolas\n  Flammarion, Matthias Hein", "title": "Sparse-RS: a versatile framework for query-efficient sparse black-box\n  adversarial attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse adversarial perturbations received much less attention in the\nliterature compared to $l_2$- and $l_\\infty$-attacks. However, it is equally\nimportant to accurately assess the robustness of a model against sparse\nperturbations. Motivated by this goal, we propose a versatile framework based\non random search, Sparse-RS, for score-based sparse targeted and untargeted\nattacks in the black-box setting. Sparse-RS does not rely on substitute models\nand achieves state-of-the-art success rate and query efficiency for multiple\nsparse attack models: $l_0$-bounded perturbations, adversarial patches, and\nadversarial frames. Unlike existing methods, the $l_0$-version of untargeted\nSparse-RS achieves almost 100% success rate on ImageNet by perturbing only 0.1%\nof the total number of pixels, outperforming all existing white-box attacks\nincluding $l_0$-PGD. Moreover, our untargeted Sparse-RS achieves very high\nsuccess rates even for the challenging settings of $20\\times20$ adversarial\npatches and $2$-pixel wide adversarial frames for $224\\times224$ images.\nFinally, we show that Sparse-RS can be applied for universal adversarial\npatches where it significantly outperforms transfer-based approaches. The code\nof our framework is available at https://github.com/fra31/sparse-rs.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 08:50:37 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 11:03:16 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Croce", "Francesco", ""], ["Andriushchenko", "Maksym", ""], ["Singh", "Naman D.", ""], ["Flammarion", "Nicolas", ""], ["Hein", "Matthias", ""]]}, {"id": "2006.12852", "submitter": "Christoph Baur", "authors": "Christoph Baur, Benedikt Wiestler, Shadi Albarqouni, Nassir Navab", "title": "Scale-Space Autoencoders for Unsupervised Anomaly Segmentation in Brain\n  MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain pathologies can vary greatly in size and shape, ranging from few pixels\n(i.e. MS lesions) to large, space-occupying tumors. Recently proposed\nAutoencoder-based methods for unsupervised anomaly segmentation in brain MRI\nhave shown promising performance, but face difficulties in modeling\ndistributions with high fidelity, which is crucial for accurate delineation of\nparticularly small lesions. Here, similar to these previous works, we model the\ndistribution of healthy brain MRI to localize pathologies from erroneous\nreconstructions. However, to achieve improved reconstruction fidelity at higher\nresolutions, we learn to compress and reconstruct different frequency bands of\nhealthy brain MRI using the laplacian pyramid. In a range of experiments\ncomparing our method to different State-of-the-Art approaches on three\ndifferent brain MR datasets with MS lesions and tumors, we show improved\nanomaly segmentation performance and the general capability to obtain much more\ncrisp reconstructions of input data at native resolution. The modeling of the\nlaplacian pyramid further enables the delineation and aggregation of lesions at\nmultiple scales, which allows to effectively cope with different pathologies\nand lesion sizes using a single model.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 09:20:42 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Baur", "Christoph", ""], ["Wiestler", "Benedikt", ""], ["Albarqouni", "Shadi", ""], ["Navab", "Nassir", ""]]}, {"id": "2006.12864", "submitter": "Anitta D", "authors": "Anitta D, Annis Fathima A", "title": "Object recognition through pose and shape estimation", "comments": null, "journal-ref": "Journal of advanced research in dynamic and control systems 2018", "doi": null, "report-no": "20182959", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision helps machines or computer to see like humans. Computer Takes\ninformation from the images and then understands of useful information from\nimages. Gesture recognition and movement recognition are the current area of\nresearch in computer vision. For both gesture and movement recognition finding\npose of an object is of great importance. The purpose of this paper is to\nreview many state of art which is already available for finding the pose of\nobject based on shape, based on appearance, based on feature and comparison for\nits accuracy, complexity and performance\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 09:55:40 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["D", "Anitta", ""], ["A", "Annis Fathima", ""]]}, {"id": "2006.12874", "submitter": "Ligang Zhang", "authors": "Ligang Zhang", "title": "Non-parametric spatially constrained local prior for scene parsing on\n  real-world data", "comments": "10 pages, journal", "journal-ref": "Engineering Applications of Artificial Intelligence,Volume\n  93,2020,103708", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene parsing aims to recognize the object category of every pixel in scene\nimages, and it plays a central role in image content understanding and computer\nvision applications. However, accurate scene parsing from unconstrained\nreal-world data is still a challenging task. In this paper, we present the\nnon-parametric Spatially Constrained Local Prior (SCLP) for scene parsing on\nrealistic data. For a given query image, the non-parametric SCLP is learnt by\nfirst retrieving a subset of most similar training images to the query image\nand then collecting prior information about object co-occurrence statistics\nbetween spatial image blocks and between adjacent superpixels from the\nretrieved subset. The SCLP is powerful in capturing both long- and short-range\ncontext about inter-object correlations in the query image and can be\neffectively integrated with traditional visual features to refine the\nclassification results. Our experiments on the SIFT Flow and PASCAL-Context\nbenchmark datasets show that the non-parametric SCLP used in conjunction with\nsuperpixel-level visual features achieves one of the top performance compared\nwith state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 10:12:08 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Zhang", "Ligang", ""]]}, {"id": "2006.12884", "submitter": "Ze Chen", "authors": "Ze Chen, Zhihang Fu, Rongxin Jiang, Yaowu Chen, Xian-sheng Hua", "title": "SLV: Spatial Likelihood Voting for Weakly Supervised Object Detection", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the framework of multiple instance learning (MIL), tremendous works\nhave promoted the advances of weakly supervised object detection (WSOD).\nHowever, most MIL-based methods tend to localize instances to their\ndiscriminative parts instead of the whole content. In this paper, we propose a\nspatial likelihood voting (SLV) module to converge the proposal localizing\nprocess without any bounding box annotations. Specifically, all region\nproposals in a given image play the role of voters every iteration during\ntraining, voting for the likelihood of each category in spatial dimensions.\nAfter dilating alignment on the area with large likelihood values, the voting\nresults are regularized as bounding boxes, being used for the final\nclassification and localization. Based on SLV, we further propose an end-to-end\ntraining framework for multi-task learning. The classification and localization\ntasks promote each other, which further improves the detection performance.\nExtensive experiments on the PASCAL VOC 2007 and 2012 datasets demonstrate the\nsuperior performance of SLV.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 10:24:13 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Chen", "Ze", ""], ["Fu", "Zhihang", ""], ["Jiang", "Rongxin", ""], ["Chen", "Yaowu", ""], ["Hua", "Xian-sheng", ""]]}, {"id": "2006.12890", "submitter": "Hyeonsoo Lee", "authors": "Hyeonsoo Lee, Won-Ki Jeong", "title": "Scribble2Label: Scribble-Supervised Cell Segmentation via\n  Self-Generating Pseudo-Labels with Consistency", "comments": "MICCAI 2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation is a fundamental process in microscopic cell image analysis.\nWith the advent of recent advances in deep learning, more accurate and\nhigh-throughput cell segmentation has become feasible. However, most existing\ndeep learning-based cell segmentation algorithms require fully annotated\nground-truth cell labels, which are time-consuming and labor-intensive to\ngenerate. In this paper, we introduce Scribble2Label, a novel weakly-supervised\ncell segmentation framework that exploits only a handful of scribble\nannotations without full segmentation labels. The core idea is to combine\npseudo-labeling and label filtering to generate reliable labels from weak\nsupervision. For this, we leverage the consistency of predictions by\niteratively averaging the predictions to improve pseudo labels. We demonstrate\nthe performance of Scribble2Label by comparing it to several state-of-the-art\ncell segmentation methods with various cell image modalities, including\nbright-field, fluorescence, and electron microscopy. We also show that our\nmethod performs robustly across different levels of scribble details, which\nconfirms that only a few scribble annotations are required in real-use cases.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 10:51:26 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 12:45:08 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Lee", "Hyeonsoo", ""], ["Jeong", "Won-Ki", ""]]}, {"id": "2006.12906", "submitter": "Stuart Eiffert", "authors": "Stuart Eiffert, Kunming Li, Mao Shan, Stewart Worrall, Salah Sukkarieh\n  and Eduardo Nebot", "title": "Probabilistic Crowd GAN: Multimodal Pedestrian Trajectory Prediction\n  using a Graph Vehicle-Pedestrian Attention Network", "comments": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L) Copyright may be transferred without notice, after which this version\n  may no longer be accessible", "journal-ref": null, "doi": "10.1109/LRA.2020.3004324", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and predicting the intention of pedestrians is essential to\nenable autonomous vehicles and mobile robots to navigate crowds. This problem\nbecomes increasingly complex when we consider the uncertainty and multimodality\nof pedestrian motion, as well as the implicit interactions between members of a\ncrowd, including any response to a vehicle. Our approach, Probabilistic Crowd\nGAN, extends recent work in trajectory prediction, combining Recurrent Neural\nNetworks (RNNs) with Mixture Density Networks (MDNs) to output probabilistic\nmultimodal predictions, from which likely modal paths are found and used for\nadversarial training. We also propose the use of Graph Vehicle-Pedestrian\nAttention Network (GVAT), which models social interactions and allows input of\na shared vehicle feature, showing that inclusion of this module leads to\nimproved trajectory prediction both with and without the presence of a vehicle.\nThrough evaluation on various datasets, we demonstrate improvements on the\nexisting state of the art methods for trajectory prediction and illustrate how\nthe true multimodal and uncertain nature of crowd interactions can be directly\nmodelled.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 11:25:16 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 23:33:28 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Eiffert", "Stuart", ""], ["Li", "Kunming", ""], ["Shan", "Mao", ""], ["Worrall", "Stewart", ""], ["Sukkarieh", "Salah", ""], ["Nebot", "Eduardo", ""]]}, {"id": "2006.12915", "submitter": "Heye Zhang", "authors": "Yifeng Guo, Chengjia Wang, Heye Zhang and Guang Yang", "title": "Deep Attentive Wasserstein Generative Adversarial Networks for MRI\n  Reconstruction with Recurrent Context-Awareness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of traditional compressive sensing-based MRI (CS-MRI)\nreconstruction is affected by its slow iterative procedure and noise-induced\nartefacts. Although many deep learning-based CS-MRI methods have been proposed\nto mitigate the problems of traditional methods, they have not been able to\nachieve more robust results at higher acceleration factors. Most of the deep\nlearning-based CS-MRI methods still can not fully mine the information from the\nk-space, which leads to unsatisfactory results in the MRI reconstruction. In\nthis study, we propose a new deep learning-based CS-MRI reconstruction method\nto fully utilise the relationship among sequential MRI slices by coupling\nWasserstein Generative Adversarial Networks (WGAN) with Recurrent Neural\nNetworks. Further development of an attentive unit enables our model to\nreconstruct more accurate anatomical structures for the MRI data. By\nexperimenting on different MRI datasets, we have demonstrated that our method\ncan not only achieve better results compared to the state-of-the-arts but can\nalso effectively reduce residual noise generated during the reconstruction\nprocess.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 11:50:21 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Guo", "Yifeng", ""], ["Wang", "Chengjia", ""], ["Zhang", "Heye", ""], ["Yang", "Guang", ""]]}, {"id": "2006.12963", "submitter": "Jianrong Xu", "authors": "Jianrong Xu, Chao Li, Bifeng Cui, Kang Yang, Yongjun Xu", "title": "PFGDF: Pruning Filter via Gaussian Distribution Feature for Deep Neural\n  Networks Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existence of a lot of redundant information in convolutional neural\nnetworks leads to the slow deployment of its equipment on the edge. To solve\nthis issue, we proposed a novel deep learning model compression acceleration\nmethod based on data distribution characteristics, namely Pruning Filter via\nGaussian Distribution Feature(PFGDF) which was to found the smaller interval of\nthe convolution layer of a certain layer to describe the original on the\ngrounds of distribution characteristics . Compared with revious advanced\nmethods, PFGDF compressed the model by filters with insignificance in\ndistribution regardless of the contribution and sensitivity information of the\nconvolution filter. The pruning process of the model was automated, and always\nensured that the compressed model could restore the performance of original\nmodel. Notably, on CIFAR-10, PFGDF compressed the convolution filter on VGG-16\nby 66:62%, the parameter reducing more than 90%, and FLOPs achieved 70:27%. On\nResNet-32, PFGDF reduced the convolution filter by 21:92%. The parameter was\nreduced to 54:64%, and the FLOPs exceeded 42%\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 13:03:21 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Xu", "Jianrong", ""], ["Li", "Chao", ""], ["Cui", "Bifeng", ""], ["Yang", "Kang", ""], ["Xu", "Yongjun", ""]]}, {"id": "2006.12986", "submitter": "Jiemin Fang", "authors": "Jiemin Fang, Yuzhu Sun, Qian Zhang, Kangjian Peng, Yuan Li, Wenyu Liu,\n  Xinggang Wang", "title": "FNA++: Fast Network Adaptation via Parameter Remapping and Architecture\n  Search", "comments": "Accepted by TPAMI (extension of arXiv:2001.02525)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks achieve remarkable performance in many computer vision\ntasks. Most state-of-the-art (SOTA) semantic segmentation and object detection\napproaches reuse neural network architectures designed for image classification\nas the backbone, commonly pre-trained on ImageNet. However, performance gains\ncan be achieved by designing network architectures specifically for detection\nand segmentation, as shown by recent neural architecture search (NAS) research\nfor detection and segmentation. One major challenge though is that ImageNet\npre-training of the search space representation (a.k.a. super network) or the\nsearched networks incurs huge computational cost. In this paper, we propose a\nFast Network Adaptation (FNA++) method, which can adapt both the architecture\nand parameters of a seed network (e.g. an ImageNet pre-trained network) to\nbecome a network with different depths, widths, or kernel sizes via a parameter\nremapping technique, making it possible to use NAS for segmentation and\ndetection tasks a lot more efficiently. In our experiments, we apply FNA++ on\nMobileNetV2 to obtain new networks for semantic segmentation, object detection,\nand human pose estimation that clearly outperform existing networks designed\nboth manually and by NAS. We also implement FNA++ on ResNets and NAS networks,\nwhich demonstrates a great generalization ability. The total computation cost\nof FNA++ is significantly less than SOTA segmentation and detection NAS\napproaches: 1737x less than DPC, 6.8x less than Auto-DeepLab, and 8.0x less\nthan DetNAS. A series of ablation studies are performed to demonstrate the\neffectiveness, and detailed analysis is provided for more insights into the\nworking mechanism. Codes are available at https://github.com/JaminFong/FNA.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 10:03:34 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 03:57:51 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Fang", "Jiemin", ""], ["Sun", "Yuzhu", ""], ["Zhang", "Qian", ""], ["Peng", "Kangjian", ""], ["Li", "Yuan", ""], ["Liu", "Wenyu", ""], ["Wang", "Xinggang", ""]]}, {"id": "2006.12987", "submitter": "Shuo Chang", "authors": "Shuo Chang, YiFan Zhang, Sai Huang, Yuanyuan Yao and Zhiyong Feng", "title": "Exemplar Loss for Siamese Network in Visual Tracking", "comments": "The experiment results have some error. And the pdf format is not\n  proper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking plays an important role in perception system, which is a\ncrucial part of intelligent transportation. Recently, Siamese network is a hot\ntopic for visual tracking to estimate moving targets' trajectory, due to its\nsuperior accuracy and simple framework. In general, Siamese tracking\nalgorithms, supervised by logistic loss and triplet loss, increase the value of\ninner product between exemplar template and positive sample while reduce the\nvalue of inner product with background sample. However, the distractors from\ndifferent exemplars are not considered by mentioned loss functions, which limit\nthe feature models' discrimination. In this paper, a new exemplar loss\nintegrated with logistic loss is proposed to enhance the feature model's\ndiscrimination by reducing inner products among exemplars. Without the bells\nand whistles, the proposed algorithm outperforms the methods supervised by\nlogistic loss or triplet loss. Numerical results suggest that the newly\ndeveloped algorithm achieves comparable performance in public benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 01:47:53 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 02:29:33 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chang", "Shuo", ""], ["Zhang", "YiFan", ""], ["Huang", "Sai", ""], ["Yao", "Yuanyuan", ""], ["Feng", "Zhiyong", ""]]}, {"id": "2006.13011", "submitter": "Lei Li", "authors": "Lei Li, Xin Weng, Julia A. Schnabel, Xiahai Zhuang", "title": "Joint Left Atrial Segmentation and Scar Quantification Based on a DNN\n  with Spatial Encoding and Shape Attention", "comments": "10 pages", "journal-ref": "MICCAI 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an end-to-end deep neural network (DNN) which can simultaneously\nsegment the left atrial (LA) cavity and quantify LA scars. The framework\nincorporates the continuous spatial information of the target by introducing a\nspatially encoded (SE) loss based on the distance transform map. Compared to\nconventional binary label based loss, the proposed SE loss can reduce noisy\npatches in the resulting segmentation, which is commonly seen for deep\nlearning-based methods. To fully utilize the inherent spatial relationship\nbetween LA and LA scars, we further propose a shape attention (SA) mechanism\nthrough an explicit surface projection to build an end-to-end-trainable model.\nSpecifically, the SA scheme is embedded into a two-task network to perform the\njoint LA segmentation and scar quantification. Moreover, the proposed method\ncan alleviate the severe class-imbalance problem when detecting small and\ndiscrete targets like scars. We evaluated the proposed framework on 60 LGE MRI\ndata from the MICCAI2018 LA challenge. For LA segmentation, the proposed method\nreduced the mean Hausdorff distance from 36.4 mm to 20.0 mm compared to the 3D\nbasic U-Net using the binary cross-entropy loss. For scar quantification, the\nmethod was compared with the results or algorithms reported in the literature\nand demonstrated better performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 13:55:29 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Li", "Lei", ""], ["Weng", "Xin", ""], ["Schnabel", "Julia A.", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "2006.13016", "submitter": "Quanshi Zhang", "authors": "Hao Zhang, Yiting Chen, Haotian Ma, Xu Cheng, Qihan Ren, Liyao Xiang,\n  Jie Shi, Quanshi Zhang", "title": "Rotation-Equivariant Neural Networks for Privacy Protection", "comments": "arXiv admin note: text overlap with arXiv:2003.08365", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to prevent leaking input information from intermediate-layer\nfeatures, this paper proposes a method to revise the traditional neural network\ninto the rotation-equivariant neural network (RENN). Compared to the\ntraditional neural network, the RENN uses d-ary vectors/tensors as features, in\nwhich each element is a d-ary number. These d-ary features can be rotated\n(analogous to the rotation of a d-dimensional vector) with a random angle as\nthe encryption process. Input information is hidden in this target phase of\nd-ary features for attribute obfuscation. Even if attackers have obtained\nnetwork parameters and intermediate-layer features, they cannot extract input\ninformation without knowing the target phase. Hence, the input privacy can be\neffectively protected by the RENN. Besides, the output accuracy of RENNs only\ndegrades mildly compared to traditional neural networks, and the computational\ncost is significantly less than the homomorphic encryption.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 08:00:14 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Zhang", "Hao", ""], ["Chen", "Yiting", ""], ["Ma", "Haotian", ""], ["Cheng", "Xu", ""], ["Ren", "Qihan", ""], ["Xiang", "Liyao", ""], ["Shi", "Jie", ""], ["Zhang", "Quanshi", ""]]}, {"id": "2006.13017", "submitter": "Li Tao", "authors": "Li Tao, Xueting Wang, Toshihiko Yamasaki", "title": "Motion Representation Using Residual Frames with 3D CNN", "comments": "Accepted in IEEE ICIP 2020. arXiv admin note: substantial text\n  overlap with arXiv:2001.05661", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, 3D convolutional networks (3D ConvNets) yield good performance in\naction recognition. However, optical flow stream is still needed to ensure\nbetter performance, the cost of which is very high. In this paper, we propose a\nfast but effective way to extract motion features from videos utilizing\nresidual frames as the input data in 3D ConvNets. By replacing traditional\nstacked RGB frames with residual ones, 35.6% and 26.6% points improvements over\ntop-1 accuracy can be obtained on the UCF101 and HMDB51 datasets when ResNet-18\nmodels are trained from scratch. And we achieved the state-of-the-art results\nin this training mode. Analysis shows that better motion features can be\nextracted using residual frames compared to RGB counterpart. By combining with\na simple appearance path, our proposal can be even better than some methods\nusing optical flow streams.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 07:35:41 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Tao", "Li", ""], ["Wang", "Xueting", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2006.13022", "submitter": "Feng Liu", "authors": "Li Zhong, Zhen Fang, Feng Liu, Bo Yuan, Guangquan Zhang, Jie Lu", "title": "Bridging the Theoretical Bound and Deep Algorithms for Open Set Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the unsupervised open set domain adaptation (UOSDA), the target domain\ncontains unknown classes that are not observed in the source domain.\nResearchers in this area aim to train a classifier to accurately: 1) recognize\nunknown target data (data with unknown classes) and, 2) classify other target\ndata. To achieve this aim, a previous study has proven an upper bound of the\ntarget-domain risk, and the open set difference, as an important term in the\nupper bound, is used to measure the risk on unknown target data. By minimizing\nthe upper bound, a shallow classifier can be trained to achieve the aim.\nHowever, if the classifier is very flexible (e.g., deep neural networks\n(DNNs)), the open set difference will converge to a negative value when\nminimizing the upper bound, which causes an issue where most target data are\nrecognized as unknown data. To address this issue, we propose a new upper bound\nof target-domain risk for UOSDA, which includes four terms: source-domain risk,\n$\\epsilon$-open set difference ($\\Delta_\\epsilon$), a distributional\ndiscrepancy between domains, and a constant. Compared to the open set\ndifference, $\\Delta_\\epsilon$ is more robust against the issue when it is being\nminimized, and thus we are able to use very flexible classifiers (i.e., DNNs).\nThen, we propose a new principle-guided deep UOSDA method that trains DNNs via\nminimizing the new upper bound. Specifically, source-domain risk and\n$\\Delta_\\epsilon$ are minimized by gradient descent, and the distributional\ndiscrepancy is minimized via a novel open-set conditional adversarial training\nstrategy. Finally, compared to existing shallow and deep UOSDA methods, our\nmethod shows the state-of-the-art performance on several benchmark datasets,\nincluding digit recognition (MNIST, SVHN, USPS), object recognition (Office-31,\nOffice-Home), and face recognition (PIE).\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 14:01:06 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Zhong", "Li", ""], ["Fang", "Zhen", ""], ["Liu", "Feng", ""], ["Yuan", "Bo", ""], ["Zhang", "Guangquan", ""], ["Lu", "Jie", ""]]}, {"id": "2006.13026", "submitter": "Grigorios Chrysos", "authors": "Grigorios Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Jiankang\n  Deng, Yannis Panagakis, Stefanos Zafeiriou", "title": "Deep Polynomial Neural Networks", "comments": "Published in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI). Code: https://github.com/grigorisg9gr/polynomial_nets.\n  arXiv admin note: substantial text overlap with arXiv:2003.03828", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3058891", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) are currently the method of choice\nboth for generative, as well as for discriminative learning in computer vision\nand machine learning. The success of DCNNs can be attributed to the careful\nselection of their building blocks (e.g., residual blocks, rectifiers,\nsophisticated normalization schemes, to mention but a few). In this paper, we\npropose $\\Pi$-Nets, a new class of function approximators based on polynomial\nexpansions. $\\Pi$-Nets are polynomial neural networks, i.e., the output is a\nhigh-order polynomial of the input. The unknown parameters, which are naturally\nrepresented by high-order tensors, are estimated through a collective tensor\nfactorization with factors sharing. We introduce three tensor decompositions\nthat significantly reduce the number of parameters and show how they can be\nefficiently implemented by hierarchical neural networks. We empirically\ndemonstrate that $\\Pi$-Nets are very expressive and they even produce good\nresults without the use of non-linear activation functions in a large battery\nof tasks and signals, i.e., images, graphs, and audio. When used in conjunction\nwith activation functions, $\\Pi$-Nets produce state-of-the-art results in three\nchallenging tasks, i.e. image generation, face verification and 3D mesh\nrepresentation learning. The source code is available at\n\\url{https://github.com/grigorisg9gr/polynomial_nets}.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 16:23:32 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 13:32:18 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chrysos", "Grigorios", ""], ["Moschoglou", "Stylianos", ""], ["Bouritsas", "Giorgos", ""], ["Deng", "Jiankang", ""], ["Panagakis", "Yannis", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2006.13046", "submitter": "Subhadip Maji", "authors": "Subhadip Maji and Smarajit Bose", "title": "Rotation Invariant Deep CBIR", "comments": "arXiv admin note: text overlap with arXiv:2002.07877", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction of Convolutional Neural Networks has improved results on almost\nevery image-based problem and Content-Based Image Retrieval is not an\nexception. But the CNN features, being rotation invariant, creates problems to\nbuild a rotation-invariant CBIR system. Though rotation-invariant features can\nbe hand-engineered, the retrieval accuracy is very low because by hand\nengineering only low-level features can be created, unlike deep learning models\nthat create high-level features along with low-level features. This paper shows\na novel method to build a rotational invariant CBIR system by introducing a\ndeep learning orientation angle detection model along with the CBIR feature\nextraction model. This paper also highlights that this rotation invariant deep\nCBIR can retrieve images from a large dataset in real-time.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 21:09:31 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Maji", "Subhadip", ""], ["Bose", "Smarajit", ""]]}, {"id": "2006.13065", "submitter": "Ashley Williamson", "authors": "A. Williamson (1), P. Dickinson (2), T. Lambrou (2), J. C. Murray (1)\n  ((1) University of Hull, (2) University of Lincoln)", "title": "DCNNs: A Transfer Learning comparison of Full Weapon Family threat\n  detection for Dual-Energy X-Ray Baggage Imagery", "comments": "Submitted to BMVC 2019 Workshop on \"Object Detection and Recognition\n  for Security Screening\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in Convolutional Neural Networks have yielded super-human\nlevels of performance in image recognition tasks [13, 25]; however, with\nincreasing volumes of parcels crossing UK borders each year, classification of\nthreats becomes integral to the smooth operation of UK borders. In this work we\npropose the first pipeline to effectively process Dual-Energy X-Ray scanner\noutput, and perform classification capable of distinguishing between firearm\nfamilies (Assault Rifle, Revolver, Self-Loading Pistol,Shotgun, and Sub-Machine\nGun) from this output. With this pipeline we compare re-cent Convolutional\nNeural Network architectures against the X-Ray baggage domain via Transfer\nLearning and show ResNet50 to be most suitable to classification - outlining a\nnumber of considerations for operational success within the domain.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 14:38:34 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 05:57:44 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Williamson", "A.", "", "University of Hull"], ["Dickinson", "P.", "", "University of Lincoln"], ["Lambrou", "T.", "", "University of Lincoln"], ["Murray", "J. C.", "", "University of Hull"]]}, {"id": "2006.13084", "submitter": "Nils G\\\"ahlert", "authors": "Nils G\\\"ahlert and Jun-Jun Wan and Nicolas Jourdan and Jan Finkbeiner\n  and Uwe Franke and Joachim Denzler", "title": "Single-Shot 3D Detection of Vehicles from Monocular RGB Images via\n  Geometry Constrained Keypoints in Real-Time", "comments": "2020 IEEE IV Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel 3D single-shot object detection method for\ndetecting vehicles in monocular RGB images. Our approach lifts 2D detections to\n3D space by predicting additional regression and classification parameters and\nhence keeping the runtime close to pure 2D object detection. The additional\nparameters are transformed to 3D bounding box keypoints within the network\nunder geometric constraints. Our proposed method features a full 3D description\nincluding all three angles of rotation without supervision by any labeled\nground truth data for the object's orientation, as it focuses on certain\nkeypoints within the image plane. While our approach can be combined with any\nmodern object detection framework with only little computational overhead, we\nexemplify the extension of SSD for the prediction of 3D bounding boxes. We test\nour approach on different datasets for autonomous driving and evaluate it using\nthe challenging KITTI 3D Object Detection as well as the novel nuScenes Object\nDetection benchmarks. While we achieve competitive results on both benchmarks\nwe outperform current state-of-the-art methods in terms of speed with more than\n20 FPS for all tested datasets and image resolutions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 15:10:19 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["G\u00e4hlert", "Nils", ""], ["Wan", "Jun-Jun", ""], ["Jourdan", "Nicolas", ""], ["Finkbeiner", "Jan", ""], ["Franke", "Uwe", ""], ["Denzler", "Joachim", ""]]}, {"id": "2006.13108", "submitter": "Ruoyu Sun", "authors": "Ruoyu Sun, Fuhui Tang, Xiaopeng Zhang, Hongkai Xiong, Qi Tian", "title": "Distilling Object Detectors with Task Adaptive Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art object detectors are at the expense of high\ncomputational costs and are hard to deploy to low-end devices. Knowledge\ndistillation, which aims at training a smaller student network by transferring\nknowledge from a larger teacher model, is one of the promising solutions for\nmodel miniaturization. In this paper, we investigate each module of a typical\ndetector in depth, and propose a general distillation framework that adaptively\ntransfers knowledge from teacher to student according to the task specific\npriors. The intuition is that simply distilling all information from teacher to\nstudent is not advisable, instead we should only borrow priors from the teacher\nmodel where the student cannot perform well. Towards this goal, we propose a\nregion proposal sharing mechanism to interflow region responses between the\nteacher and student models. Based on this, we adaptively transfer knowledge at\nthree levels, \\emph{i.e.}, feature backbone, classification head, and bounding\nbox regression head, according to which model performs more reasonably.\nFurthermore, considering that it would introduce optimization dilemma when\nminimizing distillation loss and detection loss simultaneously, we propose a\ndistillation decay strategy to help improve model generalization via gradually\nreducing the distillation penalty. Experiments on widely used detection\nbenchmarks demonstrate the effectiveness of our method. In particular, using\nFaster R-CNN with FPN as an instantiation, we achieve an accuracy of $39.0\\%$\nwith Resnet-50 on COCO dataset, which surpasses the baseline $36.3\\%$ by\n$2.7\\%$ points, and even better than the teacher model with $38.5\\%$ mAP.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 15:58:22 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Sun", "Ruoyu", ""], ["Tang", "Fuhui", ""], ["Zhang", "Xiaopeng", ""], ["Xiong", "Hongkai", ""], ["Tian", "Qi", ""]]}, {"id": "2006.13144", "submitter": "Elias Kassapis", "authors": "Elias Kassapis, Georgi Dikov, Deepak K. Gupta, Cedric Nugteren", "title": "Calibrated Adversarial Refinement for Stochastic Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambiguities in images or unsystematic annotation can lead to multiple valid\nsolutions in semantic segmentation. To learn a distribution over predictions,\nrecent work has explored the use of probabilistic networks. However, these do\nnot necessarily capture the empirical distribution accurately. In this work, we\naim to learn a multimodal predictive distribution, where the empirical\nfrequency of the sampled predictions closely reflects that of the corresponding\nlabels in the training set. To this end, we propose a novel two-stage, cascaded\nstrategy for calibrated adversarial refinement. In the first stage, we\nexplicitly model the data with a categorical likelihood. In the second, we\ntrain an adversarial network to sample from it an arbitrary number of coherent\npredictions. The model can be used independently or integrated into any\nblack-box segmentation framework to facilitate learning of calibrated\nstochastic mappings. We demonstrate the utility and versatility of the approach\nby attaining state-of-the-art results on the multigrader LIDC dataset and a\nmodified Cityscapes dataset. In addition, we use a toy regression dataset to\nshow that our framework is not confined to semantic segmentation, and the core\ndesign can be adapted to other tasks requiring learning a calibrated predictive\ndistribution.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 16:39:59 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 17:28:27 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kassapis", "Elias", ""], ["Dikov", "Georgi", ""], ["Gupta", "Deepak K.", ""], ["Nugteren", "Cedric", ""]]}, {"id": "2006.13163", "submitter": "Mauricio Neira", "authors": "Mauricio Neira, Catalina G\\'omez, John F. Su\\'arez-P\\'erez, Diego A.\n  G\\'omez, Juan Pablo Reyes, Marcela Hern\\'andez Hoyos, Pablo Arbel\\'aez, Jaime\n  E. Forero-Romero", "title": "MANTRA: A Machine Learning reference lightcurve dataset for astronomical\n  transient event recognition", "comments": "ApJS accepted, 17 pages, 14 figures", "journal-ref": null, "doi": "10.3847/1538-4365/aba267", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce MANTRA, an annotated dataset of 4869 transient and 71207\nnon-transient object lightcurves built from the Catalina Real Time Transient\nSurvey. We provide public access to this dataset as a plain text file to\nfacilitate standardized quantitative comparison of astronomical transient event\nrecognition algorithms. Some of the classes included in the dataset are:\nsupernovae, cataclysmic variables, active galactic nuclei, high proper motion\nstars, blazars and flares. As an example of the tasks that can be performed on\nthe dataset we experiment with multiple data pre-processing methods, feature\nselection techniques and popular machine learning algorithms (Support Vector\nMachines, Random Forests and Neural Networks). We assess quantitative\nperformance in two classification tasks: binary (transient/non-transient) and\neight-class classification. The best performing algorithm in both tasks is the\nRandom Forest Classifier. It achieves an F1-score of 96.25% in the binary\nclassification and 52.79% in the eight-class classification. For the\neight-class classification, non-transients ( 96.83% ) is the class with the\nhighest F1-score, while the lowest corresponds to high-proper-motion stars (\n16.79% ); for supernovae it achieves a value of 54.57% , close to the average\nacross classes. The next release of MANTRA includes images and benchmarks with\ndeep learning models.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:06:49 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 17:45:58 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Neira", "Mauricio", ""], ["G\u00f3mez", "Catalina", ""], ["Su\u00e1rez-P\u00e9rez", "John F.", ""], ["G\u00f3mez", "Diego A.", ""], ["Reyes", "Juan Pablo", ""], ["Hoyos", "Marcela Hern\u00e1ndez", ""], ["Arbel\u00e1ez", "Pablo", ""], ["Forero-Romero", "Jaime E.", ""]]}, {"id": "2006.13164", "submitter": "Xinshuo Weng", "authors": "Yongxin Wang and Kris Kitani and Xinshuo Weng", "title": "Joint Object Detection and Multi-Object Tracking with Graph Neural\n  Networks", "comments": "Published in International Conference on Robotics and Automation\n  (ICRA), 2021. Code is released here: https://github.com/yongxinw/GSDT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and data association are critical components in multi-object\ntracking (MOT) systems. Despite the fact that the two components are dependent\non each other, prior works often design detection and data association modules\nseparately which are trained with separate objectives. As a result, one cannot\nback-propagate the gradients and optimize the entire MOT system, which leads to\nsub-optimal performance. To address this issue, recent works simultaneously\noptimize detection and data association modules under a joint MOT framework,\nwhich has shown improved performance in both modules. In this work, we propose\na new instance of joint MOT approach based on Graph Neural Networks (GNNs). The\nkey idea is that GNNs can model relations between variable-sized objects in\nboth the spatial and temporal domains, which is essential for learning\ndiscriminative features for detection and data association. Through extensive\nexperiments on the MOT15/16/17/20 datasets, we demonstrate the effectiveness of\nour GNN-based joint MOT approach and show state-of-the-art performance for both\ndetection and MOT tasks. Our code is available at:\nhttps://github.com/yongxinw/GSDT\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:07:00 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 17:03:24 GMT"}, {"version": "v3", "created": "Sat, 3 Apr 2021 13:32:03 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Yongxin", ""], ["Kitani", "Kris", ""], ["Weng", "Xinshuo", ""]]}, {"id": "2006.13171", "submitter": "Roozbeh Mottaghi", "authors": "Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets,\n  Roozbeh Mottaghi, Manolis Savva, Alexander Toshev, Erik Wijmans", "title": "ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to\n  Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of Object-Goal Navigation (ObjectNav). In its simplest\nform, ObjectNav is defined as the task of navigating to an object, specified by\nits label, in an unexplored environment. In particular, the agent is\ninitialized at a random location and pose in an environment and asked to find\nan instance of an object category, e.g., find a chair, by navigating to it.\n  As the community begins to show increased interest in semantic goal\nspecification for navigation tasks, a number of different often-inconsistent\ninterpretations of this task are emerging. This document summarizes the\nconsensus recommendations of this working group on ObjectNav. In particular, we\nmake recommendations on subtle but important details of evaluation criteria\n(for measuring success when navigating towards a target object), the agent's\nembodiment parameters, and the characteristics of the environments within which\nthe task is carried out. Finally, we provide a detailed description of the\ninstantiation of these recommendations in challenges organized at the Embodied\nAI workshop at CVPR 2020 http://embodied-ai.org .\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:18:54 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 04:28:13 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Batra", "Dhruv", ""], ["Gokaslan", "Aaron", ""], ["Kembhavi", "Aniruddha", ""], ["Maksymets", "Oleksandr", ""], ["Mottaghi", "Roozbeh", ""], ["Savva", "Manolis", ""], ["Toshev", "Alexander", ""], ["Wijmans", "Erik", ""]]}, {"id": "2006.13176", "submitter": "Muhammad Kamran", "authors": "Kang Zhao, Muhammad Kamran, Gunho Sohn", "title": "Boundary Regularized Building Footprint Extraction From Satellite Images\n  Using Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, an ever-increasing number of remote satellites are orbiting\nthe Earth which streams vast amount of visual data to support a wide range of\ncivil, public and military applications. One of the key information obtained\nfrom satellite imagery is to produce and update spatial maps of built\nenvironment due to its wide coverage with high resolution data. However,\nreconstructing spatial maps from satellite imagery is not a trivial vision task\nas it requires reconstructing a scene or object with high-level representation\nsuch as primitives. For the last decade, significant advancement in object\ndetection and representation using visual data has been achieved, but the\nprimitive-based object representation still remains as a challenging vision\ntask. Thus, a high-quality spatial map is mainly produced through complex\nlabour-intensive processes. In this paper, we propose a novel deep neural\nnetwork, which enables to jointly detect building instance and regularize noisy\nbuilding boundary shapes from a single satellite imagery. The proposed deep\nlearning method consists of a two-stage object detection network to produce\nregion of interest (RoI) features and a building boundary extraction network\nusing graph models to learn geometric information of the polygon shapes.\nExtensive experiments show that our model can accomplish multi-tasks of object\nlocalization, recognition, semantic labelling and geometric shape extraction\nsimultaneously. In terms of building extraction accuracy, computation\nefficiency and boundary regularization performance, our model outperforms the\nstate-of-the-art baseline models.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:24:09 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Zhao", "Kang", ""], ["Kamran", "Muhammad", ""], ["Sohn", "Gunho", ""]]}, {"id": "2006.13188", "submitter": "Thomas Mitchel", "authors": "Thomas W. Mitchel, Benedict Brown, David Koller, Tim Weyrich, Szymon\n  Rusinkiewicz, Michael Kazhdan", "title": "Efficient Spatially Adaptive Convolution and Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast methods for convolution and correlation underlie a variety of\napplications in computer vision and graphics, including efficient filtering,\nanalysis, and simulation. However, standard convolution and correlation are\ninherently limited to fixed filters: spatial adaptation is impossible without\nsacrificing efficient computation. In early work, Freeman and Adelson have\nshown how steerable filters can address this limitation, providing a way for\nrotating the filter as it is passed over the signal. In this work, we provide a\ngeneral, representation-theoretic, framework that allows for spatially varying\nlinear transformations to be applied to the filter. This framework allows for\nefficient implementation of extended convolution and correlation for\ntransformation groups such as rotation (in 2D and 3D) and scale, and provides a\nnew interpretation for previous methods including steerable filters and the\ngeneralized Hough transform. We present applications to pattern matching, image\nfeature description, vector field visualization, and adaptive image filtering.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:41:10 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 16:36:04 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Mitchel", "Thomas W.", ""], ["Brown", "Benedict", ""], ["Koller", "David", ""], ["Weyrich", "Tim", ""], ["Rusinkiewicz", "Szymon", ""], ["Kazhdan", "Michael", ""]]}, {"id": "2006.13190", "submitter": "Connor Anderson", "authors": "Connor Anderson, Matt Gwilliam, Adam Teuscher, Andrew Merrill, Ryan\n  Farrell", "title": "Facing the Hard Problems in FGVC", "comments": "17 pages, 6 figures, 2 tables; fixed typo, minor adjustment to\n  format, added equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fine-grained visual categorization (FGVC), there is a near-singular focus\nin pursuit of attaining state-of-the-art (SOTA) accuracy. This work carefully\nanalyzes the performance of recent SOTA methods, quantitatively, but more\nimportantly, qualitatively. We show that these models universally struggle with\ncertain \"hard\" images, while also making complementary mistakes. We underscore\nthe importance of such analysis, and demonstrate that combining complementary\nmodels can improve accuracy on the popular CUB-200 dataset by over 5%. In\naddition to detailed analysis and characterization of the errors made by these\nSOTA methods, we provide a clear set of recommended directions for future FGVC\nresearchers.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:44:05 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 20:24:37 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Anderson", "Connor", ""], ["Gwilliam", "Matt", ""], ["Teuscher", "Adam", ""], ["Merrill", "Andrew", ""], ["Farrell", "Ryan", ""]]}, {"id": "2006.13192", "submitter": "Shaojie Wang", "authors": "Shaojie Wang, Tong Wu, Yevgeniy Vorobeychik", "title": "Towards Robust Sensor Fusion in Visual Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of robust sensor fusion in visual perception, especially\nunder the autonomous driving settings. We evaluate the robustness of RGB camera\nand LiDAR sensor fusion for binary classification and object detection. In this\nwork, we are interested in the behavior of different fusion methods under\nadversarial attacks on different sensors. We first train both classification\nand detection models with early fusion and late fusion, then apply different\ncombinations of adversarial attacks on both sensor inputs for evaluation. We\nalso study the effectiveness of adversarial attacks with varying budgets.\nExperiment results show that while sensor fusion models are generally\nvulnerable to adversarial attacks, late fusion method is more robust than early\nfusion. The results also provide insights on further obtaining robust sensor\nfusion models.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:46:16 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Wang", "Shaojie", ""], ["Wu", "Tong", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "2006.13194", "submitter": "Seyed Nematollah Ahmadyan", "authors": "Adel Ahmadyan, Tingbo Hou, Jianing Wei, Liangkai Zhang, Artsiom\n  Ablavatski, Matthias Grundmann", "title": "Instant 3D Object Tracking with Applications in Augmented Reality", "comments": "4 pages, five figures, CVPR Fourth Workshop on Computer Vision for\n  AR/VR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking object poses in 3D is a crucial building block for Augmented Reality\napplications. We propose an instant motion tracking system that tracks an\nobject's pose in space (represented by its 3D bounding box) in real-time on\nmobile devices. Our system does not require any prior sensory calibration or\ninitialization to function. We employ a deep neural network to detect objects\nand estimate their initial 3D pose. Then the estimated pose is tracked using a\nrobust planar tracker. Our tracker is capable of performing relative-scale\n9-DoF tracking in real-time on mobile devices. By combining use of CPU and GPU\nefficiently, we achieve 26-FPS+ performance on mobile devices.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:48:29 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Ahmadyan", "Adel", ""], ["Hou", "Tingbo", ""], ["Wei", "Jianing", ""], ["Zhang", "Liangkai", ""], ["Ablavatski", "Artsiom", ""], ["Grundmann", "Matthias", ""]]}, {"id": "2006.13202", "submitter": "Oleh Rybkin", "authors": "Oleh Rybkin, Kostas Daniilidis, Sergey Levine", "title": "Simple and Effective VAE Training with Calibrated Decoders", "comments": "International Conference on Machine Learning (ICML), 2021. Project\n  website is at https://orybkin.github.io/sigma-vae/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAEs) provide an effective and simple method for\nmodeling complex distributions. However, training VAEs often requires\nconsiderable hyperparameter tuning to determine the optimal amount of\ninformation retained by the latent variable. We study the impact of calibrated\ndecoders, which learn the uncertainty of the decoding distribution and can\ndetermine this amount of information automatically, on the VAE performance.\nWhile many methods for learning calibrated decoders have been proposed, many of\nthe recent papers that employ VAEs rely on heuristic hyperparameters and ad-hoc\nmodifications instead. We perform the first comprehensive comparative analysis\nof calibrated decoder and provide recommendations for simple and effective VAE\ntraining. Our analysis covers a range of image and video datasets and several\nsingle-image and sequential VAE models. We further propose a simple but novel\nmodification to the commonly used Gaussian decoder, which computes the\nprediction variance analytically. We observe empirically that using heuristic\nmodifications is not necessary with our method. Project website is at\nhttps://orybkin.github.io/sigma-vae/\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:57:47 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 01:09:15 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 04:06:41 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Rybkin", "Oleh", ""], ["Daniilidis", "Kostas", ""], ["Levine", "Sergey", ""]]}, {"id": "2006.13205", "submitter": "Oleh Rybkin", "authors": "Karl Pertsch, Oleh Rybkin, Frederik Ebert, Chelsea Finn, Dinesh\n  Jayaraman, Sergey Levine", "title": "Long-Horizon Visual Planning with Goal-Conditioned Hierarchical\n  Predictors", "comments": "Project page: orybkin.github.io/video-gcp. KP and OR contributed\n  equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to predict and plan into the future is fundamental for agents\nacting in the world. To reach a faraway goal, we predict trajectories at\nmultiple timescales, first devising a coarse plan towards the goal and then\ngradually filling in details. In contrast, current learning approaches for\nvisual prediction and planning fail on long-horizon tasks as they generate\npredictions (1) without considering goal information, and (2) at the finest\ntemporal resolution, one step at a time. In this work we propose a framework\nfor visual prediction and planning that is able to overcome both of these\nlimitations. First, we formulate the problem of predicting towards a goal and\npropose the corresponding class of latent space goal-conditioned predictors\n(GCPs). GCPs significantly improve planning efficiency by constraining the\nsearch space to only those trajectories that reach the goal. Further, we show\nhow GCPs can be naturally formulated as hierarchical models that, given two\nobservations, predict an observation between them, and by recursively\nsubdividing each part of the trajectory generate complete sequences. This\ndivide-and-conquer strategy is effective at long-term prediction, and enables\nus to design an effective hierarchical planning algorithm that optimizes\ntrajectories in a coarse-to-fine manner. We show that by using both\ngoal-conditioning and hierarchical prediction, GCPs enable us to solve visual\nplanning tasks with much longer horizon than previously possible.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:58:56 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 22:34:30 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Pertsch", "Karl", ""], ["Rybkin", "Oleh", ""], ["Ebert", "Frederik", ""], ["Finn", "Chelsea", ""], ["Jayaraman", "Dinesh", ""], ["Levine", "Sergey", ""]]}, {"id": "2006.13211", "submitter": "Dung Nguyen", "authors": "Dung Nguyen, Sridha Sridharan, Duc Thanh Nguyen, Simon Denman, David\n  Dean, Clinton Fookes", "title": "Meta Transfer Learning for Emotion Recognition", "comments": "Revision under Journal of Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been widely adopted in automatic emotion recognition and\nhas lead to significant progress in the field. However, due to insufficient\nannotated emotion datasets, pre-trained models are limited in their\ngeneralization capability and thus lead to poor performance on novel test sets.\nTo mitigate this challenge, transfer learning performing fine-tuning on\npre-trained models has been applied. However, the fine-tuned knowledge may\noverwrite and/or discard important knowledge learned from pre-trained models.\nIn this paper, we address this issue by proposing a PathNet-based transfer\nlearning method that is able to transfer emotional knowledge learned from one\nvisual/audio emotion domain to another visual/audio emotion domain, and\ntransfer the emotional knowledge learned from multiple audio emotion domains\ninto one another to improve overall emotion recognition accuracy. To show the\nrobustness of our proposed system, various sets of experiments for facial\nexpression recognition and speech emotion recognition task on three emotion\ndatasets: SAVEE, EMODB, and eNTERFACE have been carried out. The experimental\nresults indicate that our proposed system is capable of improving the\nperformance of emotion recognition, making its performance substantially\nsuperior to the recent proposed fine-tuning/pre-trained models based transfer\nlearning methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 00:25:28 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Nguyen", "Dung", ""], ["Sridharan", "Sridha", ""], ["Nguyen", "Duc Thanh", ""], ["Denman", "Simon", ""], ["Dean", "David", ""], ["Fookes", "Clinton", ""]]}, {"id": "2006.13212", "submitter": "Viraj Kulkarni", "authors": "Rohit Lokwani, Ashrika Gaikwad, Viraj Kulkarni, Aniruddha Pant, Amit\n  Kharat", "title": "Automated Detection of COVID-19 from CT Scans Using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 is an infectious disease that causes respiratory problems similar to\nthose caused by SARS-CoV (2003). Currently, swab samples are being used for its\ndiagnosis. The most common testing method used is the RT-PCR method, which has\nhigh specificity but variable sensitivity. AI-based detection has the\ncapability to overcome this drawback. In this paper, we propose a prospective\nmethod wherein we use chest CT scans to diagnose the patients for COVID-19\npneumonia. We use a set of open-source images, available as individual CT\nslices, and full CT scans from a private Indian Hospital to train our model. We\nbuild a 2D segmentation model using the U-Net architecture, which gives the\noutput by marking out the region of infection. Our model achieves a sensitivity\nof 96.428% (95% CI: 88%-100%) and a specificity of 88.39% (95% CI: 82%-94%).\nAdditionally, we derive a logic for converting our slice-level predictions to\nscan-level, which helps us reduce the false positives.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 06:50:41 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Lokwani", "Rohit", ""], ["Gaikwad", "Ashrika", ""], ["Kulkarni", "Viraj", ""], ["Pant", "Aniruddha", ""], ["Kharat", "Amit", ""]]}, {"id": "2006.13240", "submitter": "Pablo Palafox", "authors": "Alja\\v{z} Bo\\v{z}i\\v{c}, Pablo Palafox, Michael Zollh\\\"ofer, Angela\n  Dai, Justus Thies, Matthias Nie{\\ss}ner", "title": "Neural Non-Rigid Tracking", "comments": "Video: https://youtu.be/nqYaxM6Rj8I, Code:\n  https://github.com/DeformableFriends/NeuralTracking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel, end-to-end learnable, differentiable non-rigid tracker\nthat enables state-of-the-art non-rigid reconstruction by a learned robust\noptimization. Given two input RGB-D frames of a non-rigidly moving object, we\nemploy a convolutional neural network to predict dense correspondences and\ntheir confidences. These correspondences are used as constraints in an\nas-rigid-as-possible (ARAP) optimization problem. By enabling gradient\nback-propagation through the weighted non-linear least squares solver, we are\nable to learn correspondences and confidences in an end-to-end manner such that\nthey are optimal for the task of non-rigid tracking. Under this formulation,\ncorrespondence confidences can be learned via self-supervision, informing a\nlearned robust optimization, where outliers and wrong correspondences are\nautomatically down-weighted to enable effective tracking. Compared to\nstate-of-the-art approaches, our algorithm shows improved reconstruction\nperformance, while simultaneously achieving 85 times faster correspondence\nprediction than comparable deep-learning based methods. We make our code\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 18:00:39 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 18:15:37 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Bo\u017ei\u010d", "Alja\u017e", ""], ["Palafox", "Pablo", ""], ["Zollh\u00f6fer", "Michael", ""], ["Dai", "Angela", ""], ["Thies", "Justus", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2006.13252", "submitter": "Zhaoyuan Fang", "authors": "Aidan Boyd, Zhaoyuan Fang, Adam Czajka, Kevin W. Bowyer", "title": "Iris Presentation Attack Detection: Where Are We Now?", "comments": "Under revision for Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the popularity of iris recognition systems increases, the importance of\neffective security measures against presentation attacks becomes paramount.\nThis work presents an overview of the most important advances in the area of\niris presentation attack detection published in recent two years.\nNewly-released, publicly-available datasets for development and evaluation of\niris presentation attack detection are discussed. Recent literature can be seen\nto be broken into three categories: traditional \"hand-crafted\" feature\nextraction and classification, deep learning-based solutions, and hybrid\napproaches fusing both methodologies. Conclusions of modern approaches\nunderscore the difficulty of this task. Finally, commentary on possible\ndirections for future research is provided.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 18:11:29 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 19:49:43 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Boyd", "Aidan", ""], ["Fang", "Zhaoyuan", ""], ["Czajka", "Adam", ""], ["Bowyer", "Kevin W.", ""]]}, {"id": "2006.13253", "submitter": "Thao Nguyen", "authors": "Thao Nguyen, Nakul Gopalan, Roma Patel, Matt Corsaro, Ellie Pavlick,\n  Stefanie Tellex", "title": "Robot Object Retrieval with Contextual Natural Language Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language object retrieval is a highly useful yet challenging task for\nrobots in human-centric environments. Previous work has primarily focused on\ncommands specifying the desired object's type such as \"scissors\" and/or visual\nattributes such as \"red,\" thus limiting the robot to only known object classes.\nWe develop a model to retrieve objects based on descriptions of their usage.\nThe model takes in a language command containing a verb, for example \"Hand me\nsomething to cut,\" and RGB images of candidate objects and selects the object\nthat best satisfies the task specified by the verb. Our model directly predicts\nan object's appearance from the object's use specified by a verb phrase. We do\nnot need to explicitly specify an object's class label. Our approach allows us\nto predict high level concepts like an object's utility based on the language\nquery. Based on contextual information present in the language commands, our\nmodel can generalize to unseen object classes and unknown nouns in the\ncommands. Our model correctly selects objects out of sets of five candidates to\nfulfill natural language commands, and achieves an average accuracy of 62.3% on\na held-out test set of unseen ImageNet object classes and 53.0% on unseen\nobject classes and unknown nouns. Our model also achieves an average accuracy\nof 54.7% on unseen YCB object classes, which have a different image\ndistribution from ImageNet objects. We demonstrate our model on a KUKA LBR iiwa\nrobot arm, enabling the robot to retrieve objects based on natural language\ndescriptions of their usage. We also present a new dataset of 655 verb-object\npairs denoting object usage over 50 verbs and 216 object classes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 18:13:40 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Nguyen", "Thao", ""], ["Gopalan", "Nakul", ""], ["Patel", "Roma", ""], ["Corsaro", "Matt", ""], ["Pavlick", "Ellie", ""], ["Tellex", "Stefanie", ""]]}, {"id": "2006.13256", "submitter": "Dima Damen", "authors": "Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari,\n  Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett,\n  Will Price, Michael Wray", "title": "Rescaling Egocentric Vision", "comments": "Dataset available from: http://epic-kitchens.github.io/", "journal-ref": null, "doi": "10.5523/bris.2g1n6qdydwa9u22shpxqzp0t8m", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces the pipeline to scale the largest dataset in egocentric\nvision EPIC-KITCHENS. The effort culminates in EPIC-KITCHENS-100, a collection\nof 100~hours, 20M frames, 90K actions in 700 variable-length videos, capturing\nlong-term unscripted activities in 45 environments, using head-mounted cameras.\nCompared to its previous version, EPIC-KITCHENS-100 has been annotated using a\nnovel pipeline that allows denser (54\\% more actions per minute) and more\ncomplete annotations of fine-grained actions (+128\\% more action segments).\nThis collection also enables evaluating the \"test of time\" - i.e. whether\nmodels trained on data collected in 2018 can generalise to new footage\ncollected under the same hypotheses albeit \"two years on\".\n  The dataset is aligned with 6 challenges: action recognition (full and weak\nsupervision), action detection, action anticipation, cross-modal retrieval\n(from captions), as well as unsupervised domain adaptation for action\nrecognition. For each challenge, we define the task, provide baselines and\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 18:28:04 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 20:11:27 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2021 11:11:01 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Damen", "Dima", ""], ["Doughty", "Hazel", ""], ["Farinella", "Giovanni Maria", ""], ["Furnari", "Antonino", ""], ["Kazakos", "Evangelos", ""], ["Ma", "Jian", ""], ["Moltisanti", "Davide", ""], ["Munro", "Jonathan", ""], ["Perrett", "Toby", ""], ["Price", "Will", ""], ["Wray", "Michael", ""]]}, {"id": "2006.13262", "submitter": "Imon Banerjee", "authors": "Imon Banerjee, Priyanshu Sinha, Saptarshi Purkayastha, Nazanin\n  Mashhaditafreshi, Amara Tariq, Jiwoong Jeong, Hari Trivedi, Judy W. Gichoya", "title": "Was there COVID-19 back in 2012? Challenge for AI in Diagnosis with\n  Similar Indications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Since the recent COVID-19 outbreak, there has been an avalanche of\nresearch papers applying deep learning based image processing to chest\nradiographs for detection of the disease. To test the performance of the two\ntop models for CXR COVID-19 diagnosis on external datasets to assess model\ngeneralizability. Methods: In this paper, we present our argument regarding the\nefficiency and applicability of existing deep learning models for COVID-19\ndiagnosis. We provide results from two popular models - COVID-Net and CoroNet\nevaluated on three publicly available datasets and an additional institutional\ndataset collected from EMORY Hospital between January and May 2020, containing\npatients tested for COVID-19 infection using RT-PCR. Results: There is a large\nfalse positive rate (FPR) for COVID-Net on both ChexPert (55.3%) and MIMIC-CXR\n(23.4%) dataset. On the EMORY Dataset, COVID-Net has 61.4% sensitivity, 0.54\nF1-score and 0.49 precision value. The FPR of the CoroNet model is\nsignificantly lower across all the datasets as compared to COVID-Net -\nEMORY(9.1%), ChexPert (1.3%), ChestX-ray14 (0.02%), MIMIC-CXR (0.06%).\nConclusion: The models reported good to excellent performance on their internal\ndatasets, however we observed from our testing that their performance\ndramatically worsened on external data. This is likely from several causes\nincluding overfitting models due to lack of appropriate control patients and\nground truth labels. The fourth institutional dataset was labeled using RT-PCR,\nwhich could be positive without radiographic findings and vice versa.\nTherefore, a fusion model of both clinical and radiographic data may have\nbetter performance and generalization.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 18:35:57 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Banerjee", "Imon", ""], ["Sinha", "Priyanshu", ""], ["Purkayastha", "Saptarshi", ""], ["Mashhaditafreshi", "Nazanin", ""], ["Tariq", "Amara", ""], ["Jeong", "Jiwoong", ""], ["Trivedi", "Hari", ""], ["Gichoya", "Judy W.", ""]]}, {"id": "2006.13265", "submitter": "Nina Tuluptceva", "authors": "Nina Tuluptceva, Bart Bakker, Irina Fedulova, Heinrich Schulz, and\n  Dmitry V. Dylov", "title": "Anomaly Detection with Deep Perceptual Autoencoders", "comments": "A preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is the problem of recognizing abnormal inputs based on the\nseen examples of normal data. Despite recent advances of deep learning in\nrecognizing image anomalies, these methods still prove incapable of handling\ncomplex medical images, such as barely visible abnormalities in chest X-rays\nand metastases in lymph nodes. To address this problem, we introduce a new\npowerful method of image anomaly detection. It relies on the classical\nautoencoder approach with a re-designed training pipeline to handle\nhigh-resolution, complex images and a robust way of computing an image\nabnormality score. We revisit the very problem statement of fully unsupervised\nanomaly detection, where no abnormal examples at all are provided during the\nmodel setup. We propose to relax this unrealistic assumption by using a very\nsmall number of anomalies of confined variability merely to initiate the search\nof hyperparameters of the model. We evaluate our solution on natural image\ndatasets with a known benchmark, as well as on two medical datasets containing\nradiology and digital pathology images. The proposed approach suggests a new\nstrong baseline for image anomaly detection and outperforms state-of-the-art\napproaches in complex medical image analysis tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 18:45:55 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 17:26:07 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Tuluptceva", "Nina", ""], ["Bakker", "Bart", ""], ["Fedulova", "Irina", ""], ["Schulz", "Heinrich", ""], ["Dylov", "Dmitry V.", ""]]}, {"id": "2006.13276", "submitter": "Xiaocong Chen", "authors": "Xiaocong Chen and Lina Yao and Tao Zhou and Jinming Dong and Yu Zhang", "title": "Momentum Contrastive Learning for Few-Shot COVID-19 Diagnosis from Chest\n  CT Images", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2021.107826", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current pandemic, caused by the outbreak of a novel coronavirus\n(COVID-19) in December 2019, has led to a global emergency that has\nsignificantly impacted economies, healthcare systems and personal wellbeing all\naround the world. Controlling the rapidly evolving disease requires highly\nsensitive and specific diagnostics. While real-time RT-PCR is the most commonly\nused, these can take up to 8 hours, and require significant effort from\nhealthcare professionals. As such, there is a critical need for a quick and\nautomatic diagnostic system. Diagnosis from chest CT images is a promising\ndirection. However, current studies are limited by the lack of sufficient\ntraining samples, as acquiring annotated CT images is time-consuming. To this\nend, we propose a new deep learning algorithm for the automated diagnosis of\nCOVID-19, which only requires a few samples for training. Specifically, we use\ncontrastive learning to train an encoder which can capture expressive feature\nrepresentations on large and publicly available lung datasets and adopt the\nprototypical network for classification. We validate the efficacy of the\nproposed model in comparison with other competing methods on two publicly\navailable and annotated COVID-19 CT datasets. Our results demonstrate the\nsuperior performance of our model for the accurate diagnosis of COVID-19 based\non chest CT images.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 10:14:58 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chen", "Xiaocong", ""], ["Yao", "Lina", ""], ["Zhou", "Tao", ""], ["Dong", "Jinming", ""], ["Zhang", "Yu", ""]]}, {"id": "2006.13291", "submitter": "Matt Amodio", "authors": "Matthew Amodio, Rim Assouel, Victor Schmidt, Tristan Sylvain, Smita\n  Krishnaswamy, Yoshua Bengio", "title": "Image-to-image Mapping with Many Domains by Sparse Attribute Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised image-to-image translation consists of learning a pair of\nmappings between two domains without known pairwise correspondences between\npoints. The current convention is to approach this task with cycle-consistent\nGANs: using a discriminator to encourage the generator to change the image to\nmatch the target domain, while training the generator to be inverted with\nanother mapping. While ending up with paired inverse functions may be a good\nend result, enforcing this restriction at all times during training can be a\nhindrance to effective modeling. We propose an alternate approach that directly\nrestricts the generator to performing a simple sparse transformation in a\nlatent layer, motivated by recent work from cognitive neuroscience suggesting\nan architectural prior on representations corresponding to consciousness. Our\nbiologically motivated approach leads to representations more amenable to\ntransformation by disentangling high-level abstract concepts in the latent\nspace. We demonstrate that image-to-image domain translation with many\ndifferent domains can be learned more effectively with our architecturally\nconstrained, simple transformation than with previous unconstrained\narchitectures that rely on a cycle-consistency loss.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 19:52:23 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Amodio", "Matthew", ""], ["Assouel", "Rim", ""], ["Schmidt", "Victor", ""], ["Sylvain", "Tristan", ""], ["Krishnaswamy", "Smita", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2006.13311", "submitter": "Jesper S\\\"oren Dramsch", "authors": "Jesper S\\\"oren Dramsch", "title": "70 years of machine learning in geoscience in review", "comments": "36 pages, 17 figures, book chapter", "journal-ref": null, "doi": "10.1016/bs.agph.2020.08.002", "report-no": null, "categories": "physics.geo-ph cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This review gives an overview of the development of machine learning in\ngeoscience. A thorough analysis of the co-developments of machine learning\napplications throughout the last 70 years relates the recent enthusiasm for\nmachine learning to developments in geoscience. I explore the shift of kriging\ntowards a mainstream machine learning method and the historic application of\nneural networks in geoscience, following the general trend of machine learning\nenthusiasm through the decades. Furthermore, this chapter explores the shift\nfrom mathematical fundamentals and knowledge in software development towards\nskills in model validation, applied statistics, and integrated subject matter\nexpertise. The review is interspersed with code examples to complement the\ntheoretical foundations and illustrate model validation and machine learning\nexplainability for science. The scope of this review includes various shallow\nmachine learning methods, e.g. Decision Trees, Random Forests, Support-Vector\nMachines, and Gaussian Processes, as well as, deep neural networks, including\nfeed-forward neural networks, convolutional neural networks, recurrent neural\nnetworks and generative adversarial networks. Regarding geoscience, the review\nhas a bias towards geophysics but aims to strike a balance with geochemistry,\ngeostatistics, and geology, however excludes remote sensing, as this would\nexceed the scope. In general, I aim to provide context for the recent\nenthusiasm surrounding deep learning with respect to research, hardware, and\nsoftware developments that enable successful application of shallow and deep\nmachine learning in all disciplines of Earth science.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 16:32:27 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 06:26:24 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 12:34:24 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Dramsch", "Jesper S\u00f6ren", ""]]}, {"id": "2006.13314", "submitter": "Michele Merler", "authors": "Rameswar Panda, Michele Merler, Mayoore Jaiswal, Hui Wu, Kandan\n  Ramakrishnan, Ulrich Finkler, Chun-Fu Chen, Minsik Cho, David Kung, Rogerio\n  Feris, Bishwaranjan Bhattacharjee", "title": "NASTransfer: Analyzing Architecture Transferability in Large Scale\n  Neural Architecture Search", "comments": "19 pages, 19 Figures, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) is an open and challenging problem in\nmachine learning. While NAS offers great promise, the prohibitive computational\ndemand of most of the existing NAS methods makes it difficult to directly\nsearch the architectures on large-scale tasks. The typical way of conducting\nlarge scale NAS is to search for an architectural building block on a small\ndataset (either using a proxy set from the large dataset or a completely\ndifferent small scale dataset) and then transfer the block to a larger dataset.\nDespite a number of recent results that show the promise of transfer from proxy\ndatasets, a comprehensive evaluation of different NAS methods studying the\nimpact of different source datasets has not yet been addressed. In this work,\nwe propose to analyze the architecture transferability of different NAS methods\nby performing a series of experiments on large scale benchmarks such as\nImageNet1K and ImageNet22K. We find that: (i) The size and domain of the proxy\nset does not seem to influence architecture performance on the target dataset.\nOn average, transfer performance of architectures searched using completely\ndifferent small datasets (e.g., CIFAR10) perform similarly to the architectures\nsearched directly on proxy target datasets. However, design of proxy sets has\nconsiderable impact on rankings of different NAS methods. (ii) While different\nNAS methods show similar performance on a source dataset (e.g., CIFAR10), they\nsignificantly differ on the transfer performance to a large dataset (e.g.,\nImageNet1K). (iii) Even on large datasets, random sampling baseline is very\ncompetitive, but the choice of the appropriate combination of proxy set and\nsearch strategy can provide significant improvement over it. We believe that\nour extensive empirical analysis will prove useful for future design of NAS\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 20:28:42 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 02:55:35 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Panda", "Rameswar", ""], ["Merler", "Michele", ""], ["Jaiswal", "Mayoore", ""], ["Wu", "Hui", ""], ["Ramakrishnan", "Kandan", ""], ["Finkler", "Ulrich", ""], ["Chen", "Chun-Fu", ""], ["Cho", "Minsik", ""], ["Kung", "David", ""], ["Feris", "Rogerio", ""], ["Bhattacharjee", "Bishwaranjan", ""]]}, {"id": "2006.13322", "submitter": "Chen Chen", "authors": "Chen Chen, Chen Qin, Huaqi Qiu, Cheng Ouyang, Shuo Wang, Liang Chen,\n  Giacomo Tarroni, Wenjia Bai, Daniel Rueckert", "title": "Realistic Adversarial Data Augmentation for MR Image Segmentation", "comments": "13 pages. This paper is accepted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based approaches can achieve high accuracy in various medical\nimage segmentation tasks. However, they generally require large labelled\ndatasets for supervised learning. Acquiring and manually labelling a large\nmedical dataset is expensive and sometimes impractical due to data sharing and\nprivacy issues. In this work, we propose an adversarial data augmentation\nmethod for training neural networks for medical image segmentation. Instead of\ngenerating pixel-wise adversarial attacks, our model generates plausible and\nrealistic signal corruptions, which models the intensity inhomogeneities caused\nby a common type of artefacts in MR imaging: bias field. The proposed method\ndoes not rely on generative networks, and can be used as a plug-in module for\ngeneral segmentation networks in both supervised and semi-supervised learning.\nUsing cardiac MR imaging we show that such an approach can improve the\ngeneralization ability and robustness of models as well as provide significant\nimprovements in low-data scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 20:43:18 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Chen", "Chen", ""], ["Qin", "Chen", ""], ["Qiu", "Huaqi", ""], ["Ouyang", "Cheng", ""], ["Wang", "Shuo", ""], ["Chen", "Liang", ""], ["Tarroni", "Giacomo", ""], ["Bai", "Wenjia", ""], ["Rueckert", "Daniel", ""]]}, {"id": "2006.13341", "submitter": "Liliane Almeida", "authors": "Liliane Rodrigues de Almeida, Gilson A. Giraldi, Marcelo Bernardes\n  Vieira", "title": "Applying Lie Groups Approaches for Rigid Registration of Point Clouds", "comments": "29 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last decades, some literature appeared using the Lie groups theory to\nsolve problems in computer vision. On the other hand, Lie algebraic\nrepresentations of the transformations therein were introduced to overcome the\ndifficulties behind group structure by mapping the transformation groups to\nlinear spaces. In this paper we focus on application of Lie groups and Lie\nalgebras to find the rigid transformation that best register two surfaces\nrepresented by point clouds. The so called pairwise rigid registration can be\nformulated by comparing intrinsic second-order orientation tensors that encode\nlocal geometry. These tensors can be (locally) represented by symmetric\nnon-negative definite matrices. In this paper we interpret the obtained tensor\nfield as a multivariate normal model. So, we start with the fact that the space\nof Gaussians can be equipped with a Lie group structure, that is isomorphic to\na subgroup of the upper triangular matrices. Consequently, the associated Lie\nalgebra structure enables us to handle Gaussians, and consequently, to compare\norientation tensors, with Euclidean operations. We apply this methodology to\nvariants of the Iterative Closest Point (ICP), a known technique for pairwise\nregistration. We compare the obtained results with the original implementations\nthat apply the comparative tensor shape factor (CTSF), which is a similarity\nnotion based on the eigenvalues of the orientation tensors. We notice that the\nsimilarity measure in tensor spaces directly derived from Lie's approach is not\ninvariant under rotations, which is a problem in terms of rigid registration.\nDespite of this, the performed computational experiments show promising results\nwhen embedding orientation tensor fields in Lie algebras.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 21:26:57 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["de Almeida", "Liliane Rodrigues", ""], ["Giraldi", "Gilson A.", ""], ["Vieira", "Marcelo Bernardes", ""]]}, {"id": "2006.13352", "submitter": "Bo Li", "authors": "Bo Li, Yezhen Wang, Tong Che, Shanghang Zhang, Sicheng Zhao, Pengfei\n  Xu, Wei Zhou, Yoshua Bengio, Kurt Keutzer", "title": "Rethinking Distributional Matching Based Domain Adaptation", "comments": "Preprint version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain adaptation (DA) is a technique that transfers predictive models\ntrained on a labeled source domain to an unlabeled target domain, with the core\ndifficulty of resolving distributional shift between domains. Currently, most\npopular DA algorithms are based on distributional matching (DM). However in\npractice, realistic domain shifts (RDS) may violate their basic assumptions and\nas a result these methods will fail. In this paper, in order to devise robust\nDA algorithms, we first systematically analyze the limitations of DM based\nmethods, and then build new benchmarks with more realistic domain shifts to\nevaluate the well-accepted DM methods. We further propose InstaPBM, a novel\nInstance-based Predictive Behavior Matching method for robust DA. Extensive\nexperiments on both conventional and RDS benchmarks demonstrate both the\nlimitations of DM methods and the efficacy of InstaPBM: Compared with the best\nbaselines, InstaPBM improves the classification accuracy respectively by\n$4.5\\%$, $3.9\\%$ on Digits5, VisDA2017, and $2.2\\%$, $2.9\\%$, $3.6\\%$ on\nDomainNet-LDS, DomainNet-ILDS, ID-TwO. We hope our intuitive yet effective\nmethod will serve as a useful new direction and increase the robustness of DA\nin real scenarios. Code will be available at anonymous link:\nhttps://github.com/pikachusocute/InstaPBM-RobustDA.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 21:55:14 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 07:00:54 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Li", "Bo", ""], ["Wang", "Yezhen", ""], ["Che", "Tong", ""], ["Zhang", "Shanghang", ""], ["Zhao", "Sicheng", ""], ["Xu", "Pengfei", ""], ["Zhou", "Wei", ""], ["Bengio", "Yoshua", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2006.13377", "submitter": "Thiago Rateke", "authors": "Thiago Rateke and Aldo von Wangenheim", "title": "Road surface detection and differentiation considering surface damages", "comments": "13 pages", "journal-ref": "Autonomous Robots, 2021", "doi": "10.1007/s10514-020-09964-3", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A challenge still to be overcome in the field of visual perception for\nvehicle and robotic navigation on heavily damaged and unpaved roads is the task\nof reliable path and obstacle detection. The vast majority of the researches\nhave as scenario roads in good condition, from developed countries. These works\ncope with few situations of variation on the road surface and even fewer\nsituations presenting surface damages. In this paper we present an approach for\nroad detection considering variation in surface types, identifying paved and\nunpaved surfaces and also detecting damage and other information on other road\nsurface that may be relevant to driving safety. We also present a new Ground\nTruth with image segmentation, used in our approach and that allowed us to\nevaluate our results. Our results show that it is possible to use passive\nvision for these purposes, even using images captured with low cost cameras.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 23:11:26 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Rateke", "Thiago", ""], ["von Wangenheim", "Aldo", ""]]}, {"id": "2006.13379", "submitter": "Shuo Wang", "authors": "Shuo Wang, Giacomo Tarroni, Chen Qin, Yuanhan Mo, Chengliang Dai, Chen\n  Chen, Ben Glocker, Yike Guo, Daniel Rueckert and Wenjia Bai", "title": "Deep Generative Model-based Quality Control for Cardiac MRI Segmentation", "comments": "The paper is accepted to MICCAI 2020", "journal-ref": null, "doi": "10.1007/978-3-030-59719-1_9", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, convolutional neural networks have demonstrated promising\nperformance in a variety of medical image segmentation tasks. However, when a\ntrained segmentation model is deployed into the real clinical world, the model\nmay not perform optimally. A major challenge is the potential poor-quality\nsegmentations generated due to degraded image quality or domain shift issues.\nThere is a timely need to develop an automated quality control method that can\ndetect poor segmentations and feedback to clinicians. Here we propose a novel\ndeep generative model-based framework for quality control of cardiac MRI\nsegmentation. It first learns a manifold of good-quality image-segmentation\npairs using a generative model. The quality of a given test segmentation is\nthen assessed by evaluating the difference from its projection onto the\ngood-quality manifold. In particular, the projection is refined through\niterative search in the latent space. The proposed method achieves high\nprediction accuracy on two publicly available cardiac MRI datasets. Moreover,\nit shows better generalisation ability than traditional regression-based\nmethods. Our approach provides a real-time and model-agnostic quality control\nfor cardiac MRI segmentation, which has the potential to be integrated into\nclinical image analysis workflows.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 23:15:54 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Wang", "Shuo", ""], ["Tarroni", "Giacomo", ""], ["Qin", "Chen", ""], ["Mo", "Yuanhan", ""], ["Dai", "Chengliang", ""], ["Chen", "Chen", ""], ["Glocker", "Ben", ""], ["Guo", "Yike", ""], ["Rueckert", "Daniel", ""], ["Bai", "Wenjia", ""]]}, {"id": "2006.13391", "submitter": "Armand Comas-Massague", "authors": "Armand Comas-Massagu\\'e, Chi Zhang, Zlatan Feric, Octavia Camps, Rose\n  Yu", "title": "Learning Disentangled Representations of Video with Missing Data", "comments": "Published at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data poses significant challenges while learning representations of\nvideo sequences. We present Disentangled Imputed Video autoEncoder (DIVE), a\ndeep generative model that imputes and predicts future video frames in the\npresence of missing data. Specifically, DIVE introduces a missingness latent\nvariable, disentangles the hidden video representations into static and dynamic\nappearance, pose, and missingness factors for each object. DIVE imputes each\nobject's trajectory where data is missing. On a moving MNIST dataset with\nvarious missing scenarios, DIVE outperforms the state of the art baselines by a\nsubstantial margin. We also present comparisons for real-world MOTSChallenge\npedestrian dataset, which demonstrates the practical value of our method in a\nmore realistic setting. Our code and data can be found at\nhttps://github.com/Rose-STL-Lab/DIVE.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 23:54:49 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 20:56:04 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Comas-Massagu\u00e9", "Armand", ""], ["Zhang", "Chi", ""], ["Feric", "Zlatan", ""], ["Camps", "Octavia", ""], ["Yu", "Rose", ""]]}, {"id": "2006.13434", "submitter": "Innfarn Yoo", "authors": "Innfarn Yoo and Xiyang Luo and Yilin Wang and Feng Yang and Peyman\n  Milanfar", "title": "GIFnets: Differentiable GIF Encoding Framework", "comments": null, "journal-ref": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020, pp. 14473-14482", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphics Interchange Format (GIF) is a widely used image file format. Due to\nthe limited number of palette colors, GIF encoding often introduces color\nbanding artifacts. Traditionally, dithering is applied to reduce color banding,\nbut introducing dotted-pattern artifacts. To reduce artifacts and provide a\nbetter and more efficient GIF encoding, we introduce a differentiable GIF\nencoding pipeline, which includes three novel neural networks: PaletteNet,\nDitherNet, and BandingNet. Each of these three networks provides an important\nfunctionality within the GIF encoding pipeline. PaletteNet predicts a\nnear-optimal color palette given an input image. DitherNet manipulates the\ninput image to reduce color banding artifacts and provides an alternative to\ntraditional dithering. Finally, BandingNet is designed to detect color banding,\nand provides a new perceptual loss specifically for GIF images. As far as we\nknow, this is the first fully differentiable GIF encoding pipeline based on\ndeep neural networks and compatible with existing GIF decoders. User study\nshows that our algorithm is better than Floyd-Steinberg based GIF encoding.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 02:39:37 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Yoo", "Innfarn", ""], ["Luo", "Xiyang", ""], ["Wang", "Yilin", ""], ["Yang", "Feng", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2006.13457", "submitter": "Wei Luo", "authors": "Wei Luo and Hengmin Zhang and Jun Li and Xiu-Shen Wei", "title": "Learning Semantically Enhanced Feature for Fine-Grained Image\n  Classification", "comments": "Accepted by IEEE Signal Processing Letters. 5 pages, 4 figures, 4\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to provide a computationally cheap yet effective approach for\nfine-grained image classification (FGIC) in this letter. Unlike previous\nmethods that rely on complex part localization modules, our approach learns\nfine-grained features by enhancing the semantics of sub-features of a global\nfeature. Specifically, we first achieve the sub-feature semantic by arranging\nfeature channels of a CNN into different groups through channel permutation.\nMeanwhile, to enhance the discriminability of sub-features, the groups are\nguided to be activated on object parts with strong discriminability by a\nweighted combination regularization. Our approach is parameter parsimonious and\ncan be easily integrated into the backbone model as a plug-and-play module for\nend-to-end training with only image-level supervision. Experiments verified the\neffectiveness of our approach and validated its comparable performance to the\nstate-of-the-art methods. Code is available at https://github.com/cswluo/SEF\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 03:41:12 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 08:59:38 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 14:08:45 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Luo", "Wei", ""], ["Zhang", "Hengmin", ""], ["Li", "Jun", ""], ["Wei", "Xiu-Shen", ""]]}, {"id": "2006.13458", "submitter": "Yizhou Wang", "authors": "Jiarui Cai, Yizhou Wang, Haotian Zhang, Hung-Min Hsu, Chengqian Ma,\n  Jenq-Neng Hwang", "title": "IA-MOT: Instance-Aware Multi-Object Tracking with Motion Consistency", "comments": "The 5th Benchmarking Multi-Target Tracking (BMTT) Workshop, CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple object tracking (MOT) is a crucial task in computer vision society.\nHowever, most tracking-by-detection MOT methods, with available detected\nbounding boxes, cannot effectively handle static, slow-moving and fast-moving\ncamera scenarios simultaneously due to ego-motion and frequent occlusion. In\nthis work, we propose a novel tracking framework, called \"instance-aware MOT\"\n(IA-MOT), that can track multiple objects in either static or moving cameras by\njointly considering the instance-level features and object motions. First,\nrobust appearance features are extracted from a variant of Mask R-CNN detector\nwith an additional embedding head, by sending the given detections as the\nregion proposals. Meanwhile, the spatial attention, which focuses on the\nforeground within the bounding boxes, is generated from the given instance\nmasks and applied to the extracted embedding features. In the tracking stage,\nobject instance masks are aligned by feature similarity and motion consistency\nusing the Hungarian association algorithm. Moreover, object re-identification\n(ReID) is incorporated to recover ID switches caused by long-term occlusion or\nmissing detection. Overall, when evaluated on the MOTS20 and KITTI-MOTS\ndataset, our proposed method won the first place in Track 3 of the BMTT\nChallenge in CVPR2020 workshops.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 03:53:36 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Cai", "Jiarui", ""], ["Wang", "Yizhou", ""], ["Zhang", "Haotian", ""], ["Hsu", "Hung-Min", ""], ["Ma", "Chengqian", ""], ["Hwang", "Jenq-Neng", ""]]}, {"id": "2006.13461", "submitter": "Xinyue Huo", "authors": "Xinyue Huo, Lingxi Xie, Jianzhong He, Zijie Yang and Qi Tian", "title": "ATSO: Asynchronous Teacher-Student Optimization for Semi-Supervised\n  Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical image analysis, semi-supervised learning is an effective method to\nextract knowledge from a small amount of labeled data and a large amount of\nunlabeled data. This paper focuses on a popular pipeline known as self\nlearning, and points out a weakness named lazy learning that refers to the\ndifficulty for a model to learn from the pseudo labels generated by itself. To\nalleviate this issue, we propose ATSO, an asynchronous version of\nteacher-student optimization. ATSO partitions the unlabeled data into two\nsubsets and alternately uses one subset to fine-tune the model and updates the\nlabel on the other subset. We evaluate ATSO on two popular medical image\nsegmentation datasets and show its superior performance in various\nsemi-supervised settings. With slight modification, ATSO transfers well to\nnatural image segmentation for autonomous driving data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 04:05:12 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 04:29:36 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 01:18:45 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Huo", "Xinyue", ""], ["Xie", "Lingxi", ""], ["He", "Jianzhong", ""], ["Yang", "Zijie", ""], ["Tian", "Qi", ""]]}, {"id": "2006.13491", "submitter": "Amir Bar", "authors": "Muhamedrahimov Raouf, Bar Amir and Akselrod-Ballin Ayelet", "title": "Learning Interclass Relations for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In standard classification, we typically treat class categories as\nindependent of one-another. In many problems, however, we would be neglecting\nthe natural relations that exist between categories, which are often dictated\nby an underlying biological or physical process. In this work, we propose novel\nformulations of the classification problem, based on a realization that the\nassumption of class-independence is a limiting factor that leads to the\nrequirement of more training data. First, we propose manual ways to reduce our\ndata needs by reintroducing knowledge about problem-specific interclass\nrelations into the training process. Second, we propose a general approach to\njointly learn categorical label representations that can implicitly encode\nnatural interclass relations, alleviating the need for strong prior\nassumptions, which are not always available. We demonstrate this in the domain\nof medical images, where access to large amounts of labelled data is not\ntrivial. Specifically, our experiments show the advantages of this approach in\nthe classification of Intravenous Contrast enhancement phases in CT images,\nwhich encapsulate multiple interesting inter-class relations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 05:32:54 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Raouf", "Muhamedrahimov", ""], ["Amir", "Bar", ""], ["Ayelet", "Akselrod-Ballin", ""]]}, {"id": "2006.13500", "submitter": "Jiazhi Du", "authors": "Jiazhi Du, Xin Qiao, Zifei Yan, Hongzhi Zhang, and Wangmeng Zuo", "title": "Flexible Image Denoising with Multi-layer Conditional Feature Modulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For flexible non-blind image denoising, existing deep networks usually take\nboth noisy image and noise level map as the input to handle various noise\nlevels with a single model. However, in this kind of solution, the noise\nvariance (i.e., noise level) is only deployed to modulate the first layer of\nconvolution feature with channel-wise shifting, which is limited in balancing\nnoise removal and detail preservation. In this paper, we present a novel\nflexible image enoising network (CFMNet) by equipping an U-Net backbone with\nmulti-layer conditional feature modulation (CFM) modules. In comparison to\nchannel-wise shifting only in the first layer, CFMNet can make better use of\nnoise level information by deploying multiple layers of CFM. Moreover, each CFM\nmodule takes onvolutional features from both noisy image and noise level map as\ninput for better trade-off between noise removal and detail preservation.\nExperimental results show that our CFMNet is effective in exploiting noise\nlevel information for flexible non-blind denoising, and performs favorably\nagainst the existing deep image denoising methods in terms of both quantitative\nmetrics and visual quality.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 06:00:00 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Du", "Jiazhi", ""], ["Qiao", "Xin", ""], ["Yan", "Zifei", ""], ["Zhang", "Hongzhi", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "2006.13510", "submitter": "Zhou Yutao", "authors": "Xingwei An, Yutao Zhou, Yang Di, Dong Ming", "title": "Dynamic Functional Connectivity and Graph Convolution Network for\n  Alzheimer's Disease Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease (AD) is the most prevalent form of dementia. Traditional\nmethods cannot achieve efficient and accurate diagnosis of AD. In this paper,\nwe introduce a novel method based on dynamic functional connectivity (dFC) that\ncan effectively capture changes in the brain. We compare and combine four\ndifferent types of features including amplitude of low-frequency fluctuation\n(ALFF), regional homogeneity (ReHo), dFC and the adjacency matrix of different\nbrain structures between subjects. We use graph convolution network (GCN) which\nconsider the similarity of brain structure between patients to solve the\nclassification problem of non-Euclidean domains. The proposed method's accuracy\nand the area under the receiver operating characteristic curve achieved 91.3%\nand 98.4%. This result demonstrated that our proposed method can be used for\ndetecting AD.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 06:45:25 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["An", "Xingwei", ""], ["Zhou", "Yutao", ""], ["Di", "Yang", ""], ["Ming", "Dong", ""]]}, {"id": "2006.13511", "submitter": "Kangfu Mei", "authors": "Kangfu Mei, Yao Lu, Qiaosi Yi, Haoyu Wu, Juncheng Li, Rui Huang", "title": "Disentangle Perceptual Learning through Online Contrastive Learning", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pursuing realistic results according to human visual perception is the\ncentral concern in the image transformation tasks. Perceptual learning\napproaches like perceptual loss are empirically powerful for such tasks but\nthey usually rely on the pre-trained classification network to provide\nfeatures, which are not necessarily optimal in terms of visual perception of\nimage transformation. In this paper, we argue that, among the features\nrepresentation from the pre-trained classification network, only limited\ndimensions are related to human visual perception, while others are irrelevant,\nalthough both will affect the final image transformation results. Under such an\nassumption, we try to disentangle the perception-relevant dimensions from the\nrepresentation through our proposed online contrastive learning. The resulted\nnetwork includes the pre-training part and a feature selection layer, followed\nby the contrastive learning module, which utilizes the transformed results,\ntarget images, and task-oriented distorted images as the positive, negative,\nand anchor samples, respectively. The contrastive learning aims at activating\nthe perception-relevant dimensions and suppressing the irrelevant ones by using\nthe triplet loss, so that the original representation can be disentangled for\nbetter perceptual quality. Experiments on various image transformation tasks\ndemonstrate the superiority of our framework, in terms of human visual\nperception, to the existing approaches using pre-trained networks and\nempirically designed losses.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 06:48:38 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Mei", "Kangfu", ""], ["Lu", "Yao", ""], ["Yi", "Qiaosi", ""], ["Wu", "Haoyu", ""], ["Li", "Juncheng", ""], ["Huang", "Rui", ""]]}, {"id": "2006.13517", "submitter": "Justin Wang", "authors": "Justin Wang, Edward Xu, Kangrui Xue, Lukasz Kidzinski", "title": "3D Pose Detection in Videos: Focusing on Occlusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we build upon existing methods for occlusion-aware 3D pose\ndetection in videos. We implement a two stage architecture that consists of the\nstacked hourglass network to produce 2D pose predictions, which are then\ninputted into a temporal convolutional network to produce 3D pose predictions.\nTo facilitate prediction on poses with occluded joints, we introduce an\nintuitive generalization of the cylinder man model used to generate occlusion\nlabels. We find that the occlusion-aware network is able to achieve a\nmean-per-joint-position error 5 mm less than our linear baseline model on the\nHuman3.6M dataset. Compared to our temporal convolutional network baseline, we\nachieve a comparable mean-per-joint-position error of 0.1 mm less at reduced\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 07:01:17 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Wang", "Justin", ""], ["Xu", "Edward", ""], ["Xue", "Kangrui", ""], ["Kidzinski", "Lukasz", ""]]}, {"id": "2006.13527", "submitter": "Pengqian Yu", "authors": "Xinhan Di, Pengqian Yu, Hong Zhu, Lei Cai, Qiuyan Sheng, Changyu Sun", "title": "Adversarial Model for Rotated Indoor Scenes Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an adversarial model for producing furniture layout\nfor interior scene synthesis when the interior room is rotated. The proposed\nmodel combines a conditional adversarial network, a rotation module, a mode\nmodule, and a rotation discriminator module. As compared with the prior work on\nscene synthesis, our proposed three modules enhance the ability of auto-layout\ngeneration and reduce the mode collapse during the rotation of the interior\nroom. We conduct our experiments on a proposed real-world interior layout\ndataset that contains 14400 designs from the professional designers. Our\nnumerical results demonstrate that the proposed model yields higher-quality\nlayouts for four types of rooms, including the bedroom, the bathroom, the study\nroom, and the tatami room.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 07:29:07 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 03:43:11 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Di", "Xinhan", ""], ["Yu", "Pengqian", ""], ["Zhu", "Hong", ""], ["Cai", "Lei", ""], ["Sheng", "Qiuyan", ""], ["Sun", "Changyu", ""]]}, {"id": "2006.13542", "submitter": "Yang Zhang", "authors": "Yang Zhang, Moyun Liu, Jingwu He, Fei Pan, and Yanwen Guo", "title": "Affinity Fusion Graph-based Framework for Natural Image Segmentation", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an affinity fusion graph framework to effectively connect\ndifferent graphs with highly discriminating power and nonlinearity for natural\nimage segmentation. The proposed framework combines adjacency-graphs and kernel\nspectral clustering based graphs (KSC-graphs) according to a new definition\nnamed affinity nodes of multi-scale superpixels. These affinity nodes are\nselected based on a better affiliation of superpixels, namely\nsubspace-preserving representation which is generated by sparse subspace\nclustering based on subspace pursuit. Then a KSC-graph is built via a novel\nkernel spectral clustering to explore the nonlinear relationships among these\naffinity nodes. Moreover, an adjacency-graph at each scale is constructed,\nwhich is further used to update the proposed KSC-graph at affinity nodes. The\nfusion graph is built across different scales, and it is partitioned to obtain\nfinal segmentation result. Experimental results on the Berkeley segmentation\ndataset and Microsoft Research Cambridge dataset show the superiority of our\nframework in comparison with the state-of-the-art methods. The code is\navailable at https://github.com/Yangzhangcst/AF-graph.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 08:01:10 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 03:56:16 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2021 03:33:23 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Zhang", "Yang", ""], ["Liu", "Moyun", ""], ["He", "Jingwu", ""], ["Pan", "Fei", ""], ["Guo", "Yanwen", ""]]}, {"id": "2006.13554", "submitter": "Xingjun Ma", "authors": "Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani,\n  James Bailey", "title": "Normalized Loss Functions for Deep Learning with Noisy Labels", "comments": "Accepted to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust loss functions are essential for training accurate deep neural\nnetworks (DNNs) in the presence of noisy (incorrect) labels. It has been shown\nthat the commonly used Cross Entropy (CE) loss is not robust to noisy labels.\nWhilst new loss functions have been designed, they are only partially robust.\nIn this paper, we theoretically show by applying a simple normalization that:\nany loss can be made robust to noisy labels. However, in practice, simply being\nrobust is not sufficient for a loss function to train accurate DNNs. By\ninvestigating several robust loss functions, we find that they suffer from a\nproblem of underfitting. To address this, we propose a framework to build\nrobust loss functions called Active Passive Loss (APL). APL combines two robust\nloss functions that mutually boost each other. Experiments on benchmark\ndatasets demonstrate that the family of new loss functions created by our APL\nframework can consistently outperform state-of-the-art methods by large\nmargins, especially under large noise rates such as 60% or 80% incorrect\nlabels.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 08:25:46 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Ma", "Xingjun", ""], ["Huang", "Hanxun", ""], ["Wang", "Yisen", ""], ["Romano", "Simone", ""], ["Erfani", "Sarah", ""], ["Bailey", "James", ""]]}, {"id": "2006.13555", "submitter": "Xin Li", "authors": "Xin Li, Deng Pan, Dongxiao Zhu", "title": "Defending against adversarial attacks on medical imaging AI system,\n  classification or detection?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging AI systems such as disease classification and segmentation\nare increasingly inspired and transformed from computer vision based AI\nsystems. Although an array of adversarial training and/or loss function based\ndefense techniques have been developed and proved to be effective in computer\nvision, defending against adversarial attacks on medical images remains largely\nan uncharted territory due to the following unique challenges: 1) label\nscarcity in medical images significantly limits adversarial generalizability of\nthe AI system; 2) vastly similar and dominant fore- and background in medical\nimages make it hard samples for learning the discriminating features between\ndifferent disease classes; and 3) crafted adversarial noises added to the\nentire medical image as opposed to the focused organ target can make clean and\nadversarial examples more discriminate than that between different disease\nclasses. In this paper, we propose a novel robust medical imaging AI framework\nbased on Semi-Supervised Adversarial Training (SSAT) and Unsupervised\nAdversarial Detection (UAD), followed by designing a new measure for assessing\nsystems adversarial risk. We systematically demonstrate the advantages of our\nrobust medical imaging AI system over the existing adversarial defense\ntechniques under diverse real-world settings of adversarial attacks using a\nbenchmark OCT imaging data set.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 08:26:49 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Li", "Xin", ""], ["Pan", "Deng", ""], ["Zhu", "Dongxiao", ""]]}, {"id": "2006.13556", "submitter": "Antonio Foncubierta-Rodriguez", "authors": "Ting-An Yen, Hung-Chun Hsu, Pushpak Pati, Maria Gabrani, Antonio\n  Foncubierta-Rodr\\'iguez, Pau-Choo Chung", "title": "NINEPINS: Nuclei Instance Segmentation with Point Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based methods are gaining traction in digital pathology, with\nan increasing number of publications and challenges that aim at easing the work\nof systematically and exhaustively analyzing tissue slides. These methods often\nachieve very high accuracies, at the cost of requiring large annotated datasets\nto train. This requirement is especially difficult to fulfill in the medical\nfield, where expert knowledge is essential. In this paper we focus on nuclei\nsegmentation, which generally requires experienced pathologists to annotate the\nnuclear areas in gigapixel histological images. We propose an algorithm for\ninstance segmentation that uses pseudo-label segmentations generated\nautomatically from point annotations, as a method to reduce the burden for\npathologists. With the generated segmentation masks, the proposed method trains\na modified version of HoVer-Net model to achieve instance segmentation.\nExperimental results show that the proposed method is robust to inaccuracies in\npoint annotations and comparison with Hover-Net trained with fully annotated\ninstance masks shows that a degradation in segmentation performance does not\nalways imply a degradation in higher order tasks such as tissue classification.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 08:28:52 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Yen", "Ting-An", ""], ["Hsu", "Hung-Chun", ""], ["Pati", "Pushpak", ""], ["Gabrani", "Maria", ""], ["Foncubierta-Rodr\u00edguez", "Antonio", ""], ["Chung", "Pau-Choo", ""]]}, {"id": "2006.13560", "submitter": "Ren Yang", "authors": "Ren Yang, Fabian Mentzer, Luc Van Gool and Radu Timofte", "title": "Learning for Video Compression with Recurrent Auto-Encoder and Recurrent\n  Probability Model", "comments": "Accepted for publication in IEEE Journal of Selected Topics in Signal\n  Processing (J-STSP)", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, 2021", "doi": "10.1109/JSTSP.2020.3043590", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed increasing interests in applying deep\nlearning to video compression. However, the existing approaches compress a\nvideo frame with only a few number of reference frames, which limits their\nability to fully exploit the temporal correlation among video frames. To\novercome this shortcoming, this paper proposes a Recurrent Learned Video\nCompression (RLVC) approach with the Recurrent Auto-Encoder (RAE) and Recurrent\nProbability Model (RPM). Specifically, the RAE employs recurrent cells in both\nthe encoder and decoder. As such, the temporal information in a large range of\nframes can be used for generating latent representations and reconstructing\ncompressed outputs. Furthermore, the proposed RPM network recurrently estimates\nthe Probability Mass Function (PMF) of the latent representation, conditioned\non the distribution of previous latent representations. Due to the correlation\namong consecutive frames, the conditional cross entropy can be lower than the\nindependent cross entropy, thus reducing the bit-rate. The experiments show\nthat our approach achieves the state-of-the-art learned video compression\nperformance in terms of both PSNR and MS-SSIM. Moreover, our approach\noutperforms the default Low-Delay P (LDP) setting of x265 on PSNR, and also has\nbetter performance on MS-SSIM than the SSIM-tuned x265 and the slowest setting\nof x265. The codes are available at https://github.com/RenYang-home/RLVC.git.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 08:46:33 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 09:02:42 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 08:14:34 GMT"}, {"version": "v4", "created": "Sun, 6 Dec 2020 10:07:34 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Yang", "Ren", ""], ["Mentzer", "Fabian", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2006.13566", "submitter": "Micha{\\l} Tyszkiewicz", "authors": "Micha{\\l} J. Tyszkiewicz, Pascal Fua, Eduard Trulls", "title": "DISK: Learning local features with policy gradient", "comments": "Camera-ready version for NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local feature frameworks are difficult to learn in an end-to-end fashion, due\nto the discreteness inherent to the selection and matching of sparse keypoints.\nWe introduce DISK (DIScrete Keypoints), a novel method that overcomes these\nobstacles by leveraging principles from Reinforcement Learning (RL), optimizing\nend-to-end for a high number of correct feature matches. Our simple yet\nexpressive probabilistic model lets us keep the training and inference regimes\nclose, while maintaining good enough convergence properties to reliably train\nfrom scratch. Our features can be extracted very densely while remaining\ndiscriminative, challenging commonly held assumptions about what constitutes a\ngood keypoint, as showcased in Fig. 1, and deliver state-of-the-art results on\nthree public benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 08:57:38 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 10:32:54 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Tyszkiewicz", "Micha\u0142 J.", ""], ["Fua", "Pascal", ""], ["Trulls", "Eduard", ""]]}, {"id": "2006.13575", "submitter": "Filippo Maria Bianchi", "authors": "Filippo Maria Bianchi, Martine M. Espeseth, Nj{\\aa}l Borch", "title": "Large-scale detection and categorization of oil spills from SAR images\n  with deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning framework to detect and categorize oil spills in\nsynthetic aperture radar (SAR) images at a large scale. By means of a carefully\ndesigned neural network model for image segmentation trained on an extensive\ndataset, we are able to obtain state-of-the-art performance in oil spill\ndetection, achieving results that are comparable to results produced by human\noperators. We also introduce a classification task, which is novel in the\ncontext of oil spill detection in SAR. Specifically, after being detected, each\noil spill is also classified according to different categories pertaining to\nits shape and texture characteristics. The classification results provide\nvaluable insights for improving the design of oil spill services by\nworld-leading providers. As the last contribution, we present our operational\npipeline and a visualization tool for large-scale data, which allows to detect\nand analyze the historical presence of oil spills worldwide.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 09:32:31 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Bianchi", "Filippo Maria", ""], ["Espeseth", "Martine M.", ""], ["Borch", "Nj\u00e5l", ""]]}, {"id": "2006.13593", "submitter": "Ayush Chopra", "authors": "Surgan Jandial, Ayush Chopra, Mausoom Sarkar, Piyush Gupta, Balaji\n  Krishnamurthy, Vineeth Balasubramanian", "title": "Retrospective Loss: Looking Back to Improve Training of Deep Neural\n  Networks", "comments": "Accepted at KDD 2020; The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks (DNNs) are powerful learning machines that have enabled\nbreakthroughs in several domains. In this work, we introduce a new\nretrospective loss to improve the training of deep neural network models by\nutilizing the prior experience available in past model states during training.\nMinimizing the retrospective loss, along with the task-specific loss, pushes\nthe parameter state at the current training step towards the optimal parameter\nstate while pulling it away from the parameter state at a previous training\nstep. Although a simple idea, we analyze the method as well as to conduct\ncomprehensive sets of experiments across domains - images, speech, text, and\ngraphs - to show that the proposed loss results in improved performance across\ninput domains, tasks, and architectures.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 10:16:36 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Jandial", "Surgan", ""], ["Chopra", "Ayush", ""], ["Sarkar", "Mausoom", ""], ["Gupta", "Piyush", ""], ["Krishnamurthy", "Balaji", ""], ["Balasubramanian", "Vineeth", ""]]}, {"id": "2006.13608", "submitter": "Shengyu Zhang", "authors": "Shengyu Zhang, Ziqi Tan, Jin Yu, Zhou Zhao, Kun Kuang, Tan Jiang,\n  Jingren Zhou, Hongxia Yang, Fei Wu", "title": "Comprehensive Information Integration Modeling Framework for Video\n  Titling", "comments": "11 pages, 6 figures, to appear in KDD 2020 proceedings", "journal-ref": null, "doi": "10.1145/3394486.3403325", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In e-commerce, consumer-generated videos, which in general deliver consumers'\nindividual preferences for the different aspects of certain products, are\nmassive in volume. To recommend these videos to potential consumers more\neffectively, diverse and catchy video titles are critical. However,\nconsumer-generated videos seldom accompany appropriate titles. To bridge this\ngap, we integrate comprehensive sources of information, including the content\nof consumer-generated videos, the narrative comment sentences supplied by\nconsumers, and the product attributes, in an end-to-end modeling framework.\nAlthough automatic video titling is very useful and demanding, it is much less\naddressed than video captioning. The latter focuses on generating sentences\nthat describe videos as a whole while our task requires the product-aware\nmulti-grained video analysis. To tackle this issue, the proposed method\nconsists of two processes, i.e., granular-level interaction modeling and\nabstraction-level story-line summarization. Specifically, the granular-level\ninteraction modeling first utilizes temporal-spatial landmark cues, descriptive\nwords, and abstractive attributes to builds three individual graphs and\nrecognizes the intra-actions in each graph through Graph Neural Networks (GNN).\nThen the global-local aggregation module is proposed to model inter-actions\nacross graphs and aggregate heterogeneous graphs into a holistic graph\nrepresentation. The abstraction-level story-line summarization further\nconsiders both frame-level video features and the holistic graph to utilize the\ninteractions between products and backgrounds, and generate the story-line\ntopic of the video. We collect a large-scale dataset accordingly from\nreal-world data in Taobao, a world-leading e-commerce platform, and will make\nthe desensitized version publicly available to nourish further development of\nthe research community...\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 10:38:15 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Zhang", "Shengyu", ""], ["Tan", "Ziqi", ""], ["Yu", "Jin", ""], ["Zhao", "Zhou", ""], ["Kuang", "Kun", ""], ["Jiang", "Tan", ""], ["Zhou", "Jingren", ""], ["Yang", "Hongxia", ""], ["Wu", "Fei", ""]]}, {"id": "2006.13611", "submitter": "Yang Wang", "authors": "Dan Guo, Yang Wang, Peipei Song, Meng Wang", "title": "Recurrent Relational Memory Network for Unsupervised Image Captioning", "comments": "Appearing at IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image captioning with no annotations is an emerging challenge in\ncomputer vision, where the existing arts usually adopt GAN (Generative\nAdversarial Networks) models. In this paper, we propose a novel memory-based\nnetwork rather than GAN, named Recurrent Relational Memory Network ($R^2M$).\nUnlike complicated and sensitive adversarial learning that non-ideally performs\nfor long sentence generation, $R^2M$ implements a concepts-to-sentence memory\ntranslator through two-stage memory mechanisms: fusion and recurrent memories,\ncorrelating the relational reasoning between common visual concepts and the\ngenerated words for long periods. $R^2M$ encodes visual context through\nunsupervised training on images, while enabling the memory to learn from\nirrelevant textual corpus via supervised fashion. Our solution enjoys less\nlearnable parameters and higher computational efficiency than GAN-based\nmethods, which heavily bear parameter sensitivity. We experimentally validate\nthe superiority of $R^2M$ than state-of-the-arts on all benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 10:44:35 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Guo", "Dan", ""], ["Wang", "Yang", ""], ["Song", "Peipei", ""], ["Wang", "Meng", ""]]}, {"id": "2006.13622", "submitter": "Yuteng Zhu", "authors": "Graham Finlayson and Yuteng Zhu", "title": "Unifying Optimization Methods for Color Filter Design", "comments": "11 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through optimization we can solve for a filter that when the camera views the\nworld through this filter, it is more colorimetric. Previous work solved for\nthe filter that best satisfied the Luther condition: the camera spectral\nsensitivities after filtering were approximately a linear transform from the\nCIE XYZ color matching functions. A more recent method optimized for the filter\nthat maximized the Vora-Value (a measure which relates to the closeness of the\nvector spaces spanned by the camera sensors and human vision sensors). The\noptimized Luther- and Vora-filters are different from one another.\n  In this paper we begin by observing that the function defining the Vora-Value\nis equivalent to the Luther-condition optimization if we use the orthonormal\nbasis of the XYZ color matching functions, i.e. we linearly transform the XYZ\nsensitivities to a set of orthonormal basis. In this formulation, the\nLuther-optimization algorithm is shown to almost optimize the Vora-Value.\nMoreover, experiments demonstrate that the modified orthonormal Luther-method\nfinds the same color filter compared to the Vora-Value filter optimization.\nSignificantly, our modified algorithm is simpler in formulation and also\nconverges faster than the direct Vora-Value method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 11:01:56 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Finlayson", "Graham", ""], ["Zhu", "Yuteng", ""]]}, {"id": "2006.13645", "submitter": "Maxim Samarin", "authors": "Maxim Samarin, Volker Roth, David Belius", "title": "On the Empirical Neural Tangent Kernel of Standard Finite-Width\n  Convolutional Neural Network Architectures", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Neural Tangent Kernel (NTK) is an important milestone in the ongoing\neffort to build a theory for deep learning. Its prediction that sufficiently\nwide neural networks behave as kernel methods, or equivalently as random\nfeature models, has been confirmed empirically for certain wide architectures.\nIt remains an open question how well NTK theory models standard neural network\narchitectures of widths common in practice, trained on complex datasets such as\nImageNet. We study this question empirically for two well-known convolutional\nneural network architectures, namely AlexNet and LeNet, and find that their\nbehavior deviates significantly from their finite-width NTK counterparts. For\nwider versions of these networks, where the number of channels and widths of\nfully-connected layers are increased, the deviation decreases.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 11:40:36 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Samarin", "Maxim", ""], ["Roth", "Volker", ""], ["Belius", "David", ""]]}, {"id": "2006.13662", "submitter": "Yuki Asano", "authors": "Yuki M. Asano, Mandela Patrick, Christian Rupprecht, Andrea Vedaldi", "title": "Labelling unlabelled videos from scratch with multi-modal\n  self-supervision", "comments": "Accepted to NeurIPS 2020. Project page:\n  https://www.robots.ox.ac.uk/~vgg/research/selavi, code:\n  https://github.com/facebookresearch/selavi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large part of the current success of deep learning lies in the\neffectiveness of data -- more precisely: labelled data. Yet, labelling a\ndataset with human annotation continues to carry high costs, especially for\nvideos. While in the image domain, recent methods have allowed to generate\nmeaningful (pseudo-) labels for unlabelled datasets without supervision, this\ndevelopment is missing for the video domain where learning feature\nrepresentations is the current focus. In this work, we a) show that\nunsupervised labelling of a video dataset does not come for free from strong\nfeature encoders and b) propose a novel clustering method that allows\npseudo-labelling of a video dataset without any human annotations, by\nleveraging the natural correspondence between the audio and visual modalities.\nAn extensive analysis shows that the resulting clusters have high semantic\noverlap to ground truth human labels. We further introduce the first\nbenchmarking results on unsupervised labelling of common video datasets\nKinetics, Kinetics-Sound, VGG-Sound and AVE.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 12:28:17 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 18:01:13 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 14:45:24 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Asano", "Yuki M.", ""], ["Patrick", "Mandela", ""], ["Rupprecht", "Christian", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2006.13670", "submitter": "Huaiyang Huang", "authors": "Huaiyang Huang, Haoyang Ye, Yuxiang Sun, Ming Liu", "title": "GMMLoc: Structure Consistent Visual Localization with Gaussian Mixture\n  Models", "comments": "IEEE Robotics and Automation Letters (RA-L); 8 pages, 6 figures; for\n  demos and source code, see: http://sites.google.com/view/gmmloc/", "journal-ref": null, "doi": "10.1109/LRA.2020.3005130", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating prior structure information into the visual state estimation\ncould generally improve the localization performance. In this letter, we aim to\naddress the paradox between accuracy and efficiency in coupling visual factors\nwith structure constraints. To this end, we present a cross-modality method\nthat tracks a camera in a prior map modelled by the Gaussian Mixture Model\n(GMM). With the pose estimated by the front-end initially, the local visual\nobservations and map components are associated efficiently, and the visual\nstructure from the triangulation is refined simultaneously. By introducing the\nhybrid structure factors into the joint optimization, the camera poses are\nbundle-adjusted with the local visual structure. By evaluating our complete\nsystem, namely GMMLoc, on the public dataset, we show how our system can\nprovide a centimeter-level localization accuracy with only trivial\ncomputational overhead. In addition, the comparative studies with the\nstate-of-the-art vision-dominant state estimators demonstrate the competitive\nperformance of our method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 12:41:03 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 11:07:53 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Huang", "Huaiyang", ""], ["Ye", "Haoyang", ""], ["Sun", "Yuxiang", ""], ["Liu", "Ming", ""]]}, {"id": "2006.13681", "submitter": "Siyi Hu", "authors": "Siyi Hu and Xiaojun Chang", "title": "Multi-view Drone-based Geo-localization via Style and Spatial Alignment", "comments": "9 pages 9 figures. arXiv admin note: text overlap with\n  arXiv:2002.12186 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the task of multi-view multi-source\ngeo-localization, which serves as an important auxiliary method of GPS\npositioning by matching drone-view image and satellite-view image with\npre-annotated GPS tag. To solve this problem, most existing methods adopt\nmetric loss with an weighted classification block to force the generation of\ncommon feature space shared by different view points and view sources. However,\nthese methods fail to pay sufficient attention to spatial information\n(especially viewpoint variances). To address this drawback, we propose an\nelegant orientation-based method to align the patterns and introduce a new\nbranch to extract aligned partial feature. Moreover, we provide a style\nalignment strategy to reduce the variance in image style and enhance the\nfeature unification. To demonstrate the performance of the proposed approach,\nwe conduct extensive experiments on the large-scale benchmark dataset. The\nexperimental results confirm the superiority of the proposed approach compared\nto state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 15:44:02 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 03:29:57 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Hu", "Siyi", ""], ["Chang", "Xiaojun", ""]]}, {"id": "2006.13714", "submitter": "Bihan Wen Dr", "authors": "Lanqing Guo, Zhiyuan Zha, Saiprasad Ravishankar and Bihan Wen", "title": "Exploiting Non-Local Priors via Self-Convolution For Highly-Efficient\n  Image Restoration", "comments": "Submitted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing effective image priors is critical to solving ill-posed inverse\nproblems in image processing and imaging. Recent works proposed to exploit\nimage non-local similarity for inverse problems by grouping similar patches and\ndemonstrated state-of-the-art results in many applications. However, compared\nto classic methods based on filtering or sparsity, most of the non-local\nalgorithms are time-consuming, mainly due to the highly inefficient and\nredundant block matching step, where the distance between each pair of\noverlapping patches needs to be computed. In this work, we propose a novel\nSelf-Convolution operator to exploit image non-local similarity in a\nself-supervised way. The proposed Self-Convolution can generalize the\ncommonly-used block matching step and produce equivalent results with much\ncheaper computation. Furthermore, by applying Self-Convolution, we propose an\neffective multi-modality image restoration scheme, which is much more efficient\nthan conventional block matching for non-local modeling. Experimental results\ndemonstrate that (1) Self-Convolution can significantly speed up most of the\npopular non-local image restoration algorithms, with two-fold to nine-fold\nfaster block matching, and (2) the proposed multi-modality image restoration\nscheme achieves superior denoising results in both efficiency and effectiveness\non RGB-NIR images. The code is publicly available at\n\\href{https://github.com/GuoLanqing/Self-Convolution}.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 13:24:37 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 06:11:12 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Guo", "Lanqing", ""], ["Zha", "Zhiyuan", ""], ["Ravishankar", "Saiprasad", ""], ["Wen", "Bihan", ""]]}, {"id": "2006.13717", "submitter": "Harrish Thasarathan", "authors": "Harrish Thasarathan and Mehran Ebrahimi", "title": "Artist-Guided Semiautomatic Animation Colorization", "comments": "This article supersedes our previous work arXiv:1904.09527", "journal-ref": "The IEEE International Conference on Computer Vision (ICCV)\n  Workshops, 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a delicate balance between automating repetitive work in creative\ndomains while staying true to an artist's vision. The animation industry\nregularly outsources large animation workloads to foreign countries where labor\nis inexpensive and long hours are common. Automating part of this process can\nbe incredibly useful for reducing costs and creating manageable workloads for\nmajor animation studios and outsourced artists. We present a method for\nautomating line art colorization by keeping artists in the loop to successfully\nreduce this workload while staying true to an artist's vision. By incorporating\ncolor hints and temporal information to an adversarial image-to-image\nframework, we show that it is possible to meet the balance between automation\nand authenticity through artist's input to generate colored frames with\ntemporal consistency.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 22:30:39 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 00:50:09 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Thasarathan", "Harrish", ""], ["Ebrahimi", "Mehran", ""]]}, {"id": "2006.13725", "submitter": "Swathikiran Sudhakaran", "authors": "Swathikiran Sudhakaran, Sergio Escalera, Oswald Lanz", "title": "FBK-HUPBA Submission to the EPIC-Kitchens Action Recognition 2020\n  Challenge", "comments": "Ranked 3rd in the EPIC-Kitchens action recognition challenge @ CVPR\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we describe the technical details of our submission to the\nEPIC-Kitchens Action Recognition 2020 Challenge. To participate in the\nchallenge we deployed spatio-temporal feature extraction and aggregation models\nwe have developed recently: Gate-Shift Module (GSM) [1] and EgoACO, an\nextension of Long Short-Term Attention (LSTA) [2]. We design an ensemble of GSM\nand EgoACO model families with different backbones and pre-training to generate\nthe prediction scores. Our submission, visible on the public leaderboard with\nteam name FBK-HUPBA, achieved a top-1 action recognition accuracy of 40.0% on\nS1 setting, and 25.71% on S2 setting, using only RGB.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 13:41:17 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Sudhakaran", "Swathikiran", ""], ["Escalera", "Sergio", ""], ["Lanz", "Oswald", ""]]}, {"id": "2006.13726", "submitter": "Xingjun Ma", "authors": "Linxi Jiang, Xingjun Ma, Zejia Weng, James Bailey, Yu-Gang Jiang", "title": "Imbalanced Gradients: A New Cause of Overestimated Adversarial\n  Robustness", "comments": "17 pages, 7 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the robustness of a defense model is a challenging task in\nadversarial robustness research. Obfuscated gradients, a type of gradient\nmasking, have previously been found to exist in many defense methods and cause\na false signal of robustness. In this paper, we identify a more subtle\nsituation called \\emph{Imbalanced Gradients} that can also cause overestimated\nadversarial robustness. The phenomenon of imbalanced gradients occurs when the\ngradient of one term of the margin loss dominates and pushes the attack towards\nto a suboptimal direction. To exploit imbalanced gradients, we formulate a\n\\emph{Margin Decomposition (MD)} attack that decomposes a margin loss into\nindividual terms and then explores the attackability of these terms separately\nvia a two-stage process. We examine 12 state-of-the-art defense models, and\nfind that models exploiting label smoothing easily cause imbalanced gradients,\nand on which our MD attacks can decrease their PGD robustness (evaluated by PGD\nattack) by over 23%. For 6 out of the 12 defenses, our attack can reduce their\nPGD robustness by at least 9%. The results suggest that imbalanced gradients\nneed to be carefully addressed for more reliable adversarial robustness.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 13:41:37 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 05:56:02 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Jiang", "Linxi", ""], ["Ma", "Xingjun", ""], ["Weng", "Zejia", ""], ["Bailey", "James", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "2006.13742", "submitter": "Joon Sern Lee", "authors": "Joon Sern Lee, Gui Peng David Yam, Jin Hao Chan", "title": "PhishGAN: Data Augmentation and Identification of Homoglpyh Attacks", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": "10.1109/CCCI49893.2020.9256804", "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homoglyph attacks are a common technique used by hackers to conduct phishing.\nDomain names or links that are visually similar to actual ones are created via\npunycode to obfuscate the attack, making the victim more susceptible to\nphishing. For example, victims may mistake \"|inkedin.com\" for \"linkedin.com\"\nand in the process, divulge personal details to the fake website. Current State\nof The Art (SOTA) typically make use of string comparison algorithms (e.g.\nLevenshtein Distance), which are computationally heavy. One reason for this is\nthe lack of publicly available datasets thus hindering the training of more\nadvanced Machine Learning (ML) models. Furthermore, no one font is able to\nrender all types of punycode correctly, posing a significant challenge to the\ncreation of a dataset that is unbiased toward any particular font. This coupled\nwith the vast number of internet domains pose a challenge in creating a dataset\nthat can capture all possible variations. Here, we show how a conditional\nGenerative Adversarial Network (GAN), PhishGAN, can be used to generate images\nof hieroglyphs, conditioned on non-homoglpyh input text images. Practical\nchanges to current SOTA were required to facilitate the generation of more\nvaried homoglyph text-based images. We also demonstrate a workflow of how\nPhishGAN together with a Homoglyph Identifier (HI) model can be used to\nidentify the domain the homoglyph was trying to imitate. Furthermore, we\ndemonstrate how PhishGAN's ability to generate datasets on the fly facilitate\nthe quick adaptation of cybersecurity systems to detect new threats as they\nemerge.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 13:59:09 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 03:18:57 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 09:04:11 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Lee", "Joon Sern", ""], ["Yam", "Gui Peng David", ""], ["Chan", "Jin Hao", ""]]}, {"id": "2006.13748", "submitter": "Arthur Douillard", "authors": "Arthur Douillard and Eduardo Valle and Charles Ollion and Thomas\n  Robert and Matthieu Cord", "title": "Insights from the Future for Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Continual learning aims to learn tasks sequentially, with (often severe)\nconstraints on the storage of old learning samples, without suffering from\ncatastrophic forgetting. In this work, we propose prescient continual learning,\na novel experimental setting, to incorporate existing information about the\nclasses, prior to any training data. Usually, each task in a traditional\ncontinual learning setting evaluates the model on present and past classes, the\nlatter with a limited number of training samples. Our setting adds future\nclasses, with no training samples at all. We introduce Ghost Model, a\nrepresentation-learning-based model for continual learning using ideas from\nzero-shot learning. A generative model of the representation space in concert\nwith a careful adjustment of the losses allows us to exploit insights from\nfuture classes to constraint the spatial arrangement of the past and current\nclasses. Quantitative results on the AwA2 and aP\\&Y datasets and detailed\nvisualizations showcase the interest of this new setting and the method we\npropose to address it.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 14:05:45 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Douillard", "Arthur", ""], ["Valle", "Eduardo", ""], ["Ollion", "Charles", ""], ["Robert", "Thomas", ""], ["Cord", "Matthieu", ""]]}, {"id": "2006.13772", "submitter": "Guillaume Hocquet", "authors": "G. Hocquet, O. Bichler, D. Querlioz", "title": "OvA-INN: Continual Learning with Invertible Neural Networks", "comments": "to be published in IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of Continual Learning, the objective is to learn several tasks\none after the other without access to the data from previous tasks. Several\nsolutions have been proposed to tackle this problem but they usually assume\nthat the user knows which of the tasks to perform at test time on a particular\nsample, or rely on small samples from previous data and most of them suffer of\na substantial drop in accuracy when updated with batches of only one class at a\ntime. In this article, we propose a new method, OvA-INN, which is able to learn\none class at a time and without storing any of the previous data. To achieve\nthis, for each class, we train a specific Invertible Neural Network to extract\nthe relevant features to compute the likelihood on this class. At test time, we\ncan predict the class of a sample by identifying the network which predicted\nthe highest likelihood. With this method, we show that we can take advantage of\npretrained models by stacking an Invertible Network on top of a feature\nextractor. This way, we are able to outperform state-of-the-art approaches that\nrely on features learning for the Continual Learning of MNIST and CIFAR-100\ndatasets. In our experiments, we reach 72% accuracy on CIFAR-100 after training\nour model one class at a time.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 14:40:05 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Hocquet", "G.", ""], ["Bichler", "O.", ""], ["Querlioz", "D.", ""]]}, {"id": "2006.13782", "submitter": "Francis Williams", "authors": "Francis Williams, Matthew Trager, Joan Bruna, Denis Zorin", "title": "Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Neural Splines, a technique for 3D surface reconstruction that is\nbased on random feature kernels arising from infinitely-wide shallow ReLU\nnetworks. Our method achieves state-of-the-art results, outperforming recent\nneural network-based techniques and widely used Poisson Surface Reconstruction\n(which, as we demonstrate, can also be viewed as a type of kernel method).\nBecause our approach is based on a simple kernel formulation, it is easy to\nanalyze and can be accelerated by general techniques designed for kernel-based\nlearning. We provide explicit analytical expressions for our kernel and argue\nthat our formulation can be seen as a generalization of cubic spline\ninterpolation to higher dimensions. In particular, the RKHS norm associated\nwith Neural Splines biases toward smooth interpolants.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 14:54:59 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 21:57:44 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 13:56:24 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Williams", "Francis", ""], ["Trager", "Matthew", ""], ["Bruna", "Joan", ""], ["Zorin", "Denis", ""]]}, {"id": "2006.13791", "submitter": "Enzo Ferrante", "authors": "Agostina J Larrazabal and C\\'esar Mart\\'inez and Ben Glocker and Enzo\n  Ferrante", "title": "Post-DAE: Anatomically Plausible Segmentation via Post-Processing with\n  Denoising Autoencoders", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging\n  (IEEE TMI)", "journal-ref": "IEEE Transactions on Medical Imaging (IEEE TMI), 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Post-DAE, a post-processing method based on denoising\nautoencoders (DAE) to improve the anatomical plausibility of arbitrary\nbiomedical image segmentation algorithms. Some of the most popular segmentation\nmethods (e.g. based on convolutional neural networks or random forest\nclassifiers) incorporate additional post-processing steps to ensure that the\nresulting masks fulfill expected connectivity constraints. These methods\noperate under the hypothesis that contiguous pixels with similar aspect should\nbelong to the same class. Even if valid in general, this assumption does not\nconsider more complex priors like topological restrictions or convexity, which\ncannot be easily incorporated into these methods. Post-DAE leverages the latest\ndevelopments in manifold learning via denoising autoencoders. First, we learn a\ncompact and non-linear embedding that represents the space of anatomically\nplausible segmentations. Then, given a segmentation mask obtained with an\narbitrary method, we reconstruct its anatomically plausible version by\nprojecting it onto the learnt manifold. The proposed method is trained using\nunpaired segmentation mask, what makes it independent of intensity information\nand image modality. We performed experiments in binary and multi-label\nsegmentation of chest X-ray and cardiac magnetic resonance images. We show how\nerroneous and noisy segmentation masks can be improved using Post-DAE. With\nalmost no additional computation cost, our method brings erroneous\nsegmentations back to a feasible space.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 15:05:03 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Larrazabal", "Agostina J", ""], ["Mart\u00ednez", "C\u00e9sar", ""], ["Glocker", "Ben", ""], ["Ferrante", "Enzo", ""]]}, {"id": "2006.13804", "submitter": "Michael Rotman", "authors": "Michael Rotman, Rafi Brada, Israel Beniaminy, Sangtae Ahn, Christopher\n  J. Hardy, Lior Wolf", "title": "A Novel Approach for Correcting Multiple Discrete Rigid In-Plane Motions\n  Artefacts in MRI Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion artefacts created by patient motion during an MRI scan occur\nfrequently in practice, often rendering the scans clinically unusable and\nrequiring a re-scan. While many methods have been employed to ameliorate the\neffects of patient motion, these often fall short in practice. In this paper we\npropose a novel method for removing motion artefacts using a deep neural\nnetwork with two input branches that discriminates between patient poses using\nthe motion's timing. The first branch receives a subset of the $k$-space data\ncollected during a single patient pose, and the second branch receives the\nremaining part of the collected $k$-space data. The proposed method can be\napplied to artefacts generated by multiple movements of the patient.\nFurthermore, it can be used to correct motion for the case where $k$-space has\nbeen under-sampled, to shorten the scan time, as is common when using methods\nsuch as parallel imaging or compressed sensing. Experimental results on both\nsimulated and real MRI data show the efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 15:25:11 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 17:54:13 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Rotman", "Michael", ""], ["Brada", "Rafi", ""], ["Beniaminy", "Israel", ""], ["Ahn", "Sangtae", ""], ["Hardy", "Christopher J.", ""], ["Wolf", "Lior", ""]]}, {"id": "2006.13806", "submitter": "Danfeng Hong", "authors": "Danfeng Hong, Naoto Yokoya, Gui-Song Xia, Jocelyn Chanussot, Xiao\n  Xiang Zhu", "title": "X-ModalNet: A Semi-Supervised Deep Cross-Modal Network for\n  Classification of Remote Sensing Data", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing,2020,167:12-23", "doi": "10.1016/j.isprsjprs.2020.06.014", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of semi-supervised transfer learning with\nlimited cross-modality data in remote sensing. A large amount of multi-modal\nearth observation images, such as multispectral imagery (MSI) or synthetic\naperture radar (SAR) data, are openly available on a global scale, enabling\nparsing global urban scenes through remote sensing imagery. However, their\nability in identifying materials (pixel-wise classification) remains limited,\ndue to the noisy collection environment and poor discriminative information as\nwell as limited number of well-annotated training images. To this end, we\npropose a novel cross-modal deep-learning framework, called X-ModalNet, with\nthree well-designed modules: self-adversarial module, interactive learning\nmodule, and label propagation module, by learning to transfer more\ndiscriminative information from a small-scale hyperspectral image (HSI) into\nthe classification task using a large-scale MSI or SAR data. Significantly,\nX-ModalNet generalizes well, owing to propagating labels on an updatable graph\nconstructed by high-level features on the top of the network, yielding\nsemi-supervised cross-modality learning. We evaluate X-ModalNet on two\nmulti-modal remote sensing datasets (HSI-MSI and HSI-SAR) and achieve a\nsignificant improvement in comparison with several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 15:29:41 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 18:26:47 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Hong", "Danfeng", ""], ["Yokoya", "Naoto", ""], ["Xia", "Gui-Song", ""], ["Chanussot", "Jocelyn", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2006.13807", "submitter": "Mahdiyar Molahasani Majdabadi", "authors": "Arman Haghanifar, Mahdiyar Molahasani Majdabadi, Younhee Choi, S.\n  Deivalakshmi, Seokbum Ko", "title": "COVID-CXNet: Detecting COVID-19 in Frontal Chest X-ray Images using Deep\n  Learning", "comments": "The editor has asked for confidence intervals. In this version,\n  confidence intervals are added to the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary clinical observations for screening the infectious by the\nnovel coronavirus is capturing a chest x-ray image. In most of the patients, a\nchest x-ray contains abnormalities, such as consolidation, which are the\nresults of COVID-19 viral pneumonia. In this study, research is conducted on\nefficiently detecting imaging features of this type of pneumonia using deep\nconvolutional neural networks in a large dataset. It is demonstrated that\nsimple models, alongside the majority of pretrained networks in the literature,\nfocus on irrelevant features for decision-making. In this paper, numerous chest\nx-ray images from various sources are collected, and the largest publicly\naccessible dataset is prepared. Finally, using the transfer learning paradigm,\nthe well-known CheXNet model is utilized for developing COVID-CXNet. This\npowerful model is capable of detecting the novel coronavirus pneumonia based on\nrelevant and meaningful features with precise localization. COVID-CXNet is a\nstep towards a fully automated and robust COVID-19 detection system.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 21:31:02 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 01:53:46 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Haghanifar", "Arman", ""], ["Majdabadi", "Mahdiyar Molahasani", ""], ["Choi", "Younhee", ""], ["Deivalakshmi", "S.", ""], ["Ko", "Seokbum", ""]]}, {"id": "2006.13811", "submitter": "Esther Puyol-Anton Dr", "authors": "Esther Puyol-Ant\\'on, Chen Chen, James R. Clough, Bram Ruijsink,\n  Baldeep S. Sidhu, Justin Gould, Bradley Porter, Mark Elliott, Vishal Mehta,\n  Daniel Rueckert, Christopher A. Rinaldi, and Andrew P. King", "title": "Interpretable Deep Models for Cardiac Resynchronisation Therapy Response\n  Prediction", "comments": "MICCAI 2020 conference", "journal-ref": null, "doi": "10.1007/978-3-030-59710-8_28", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep learning (DL) have resulted in impressive accuracy in some\nmedical image classification tasks, but often deep models lack\ninterpretability. The ability of these models to explain their decisions is\nimportant for fostering clinical trust and facilitating clinical translation.\nFurthermore, for many problems in medicine there is a wealth of existing\nclinical knowledge to draw upon, which may be useful in generating\nexplanations, but it is not obvious how this knowledge can be encoded into DL\nmodels - most models are learnt either from scratch or using transfer learning\nfrom a different domain. In this paper we address both of these issues. We\npropose a novel DL framework for image-based classification based on a\nvariational autoencoder (VAE). The framework allows prediction of the output of\ninterest from the latent space of the autoencoder, as well as visualisation (in\nthe image domain) of the effects of crossing the decision boundary, thus\nenhancing the interpretability of the classifier. Our key contribution is that\nthe VAE disentangles the latent space based on `explanations' drawn from\nexisting clinical knowledge. The framework can predict outputs as well as\nexplanations for these outputs, and also raises the possibility of discovering\nnew biomarkers that are separate (or disentangled) from the existing knowledge.\nWe demonstrate our framework on the problem of predicting response of patients\nwith cardiomyopathy to cardiac resynchronization therapy (CRT) from cine\ncardiac magnetic resonance images. The sensitivity and specificity of the\nproposed model on the task of CRT response prediction are 88.43% and 84.39%\nrespectively, and we showcase the potential of our model in enhancing\nunderstanding of the factors contributing to CRT response.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 15:35:47 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 10:59:55 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Puyol-Ant\u00f3n", "Esther", ""], ["Chen", "Chen", ""], ["Clough", "James R.", ""], ["Ruijsink", "Bram", ""], ["Sidhu", "Baldeep S.", ""], ["Gould", "Justin", ""], ["Porter", "Bradley", ""], ["Elliott", "Mark", ""], ["Mehta", "Vishal", ""], ["Rueckert", "Daniel", ""], ["Rinaldi", "Christopher A.", ""], ["King", "Andrew P.", ""]]}, {"id": "2006.13817", "submitter": "Mahesh Gour", "authors": "Mahesh Gour, Sweta Jain", "title": "Stacked Convolutional Neural Network for Diagnosis of COVID-19 Disease\n  from X-ray Images", "comments": "6 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic and rapid screening of COVID-19 from the chest X-ray images has\nbecome an urgent need in this pandemic situation of SARS-CoV-2 worldwide in\n2020. However, accurate and reliable screening of patients is a massive\nchallenge due to the discrepancy between COVID-19 and other viral pneumonia in\nX-ray images. In this paper, we design a new stacked convolutional neural\nnetwork model for the automatic diagnosis of COVID-19 disease from the chest\nX-ray images. We obtain different sub-models from the VGG19 and developed a\n30-layered CNN model (named as CovNet30) during the training, and obtained\nsub-models are stacked together using logistic regression. The proposed CNN\nmodel combines the discriminating power of the different CNN`s sub-models and\nclassifies chest X-ray images into COVID-19, Normal, and Pneumonia classes. In\naddition, we generate X-ray images dataset referred to as COVID19CXr, which\nincludes 2764 chest x-ray images of 1768 patients from the three publicly\navailable data repositories. The proposed stacked CNN achieves an accuracy of\n92.74%, the sensitivity of 93.33%, PPV of 92.13%, and F1-score of 0.93 for the\nclassification of X-ray images. Our proposed approach shows its superiority\nover the existing methods for the diagnosis of the COVID-19 from the X-ray\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:55:16 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Gour", "Mahesh", ""], ["Jain", "Sweta", ""]]}, {"id": "2006.13848", "submitter": "Yi Fang", "authors": "Shuaihang Yuan, Xiang Li, Yi Fang", "title": "DeepTracking-Net: 3D Tracking with Unsupervised Learning of Continuous\n  Flow", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of 3D tracking, i.e., to find dense\ncorrespondences in a sequence of time-varying 3D shapes. Despite deep learning\napproaches have achieved promising performance for pairwise dense 3D shapes\nmatching, it is a great challenge to generalize those approaches for the\ntracking of 3D time-varying geometries. In this paper, we aim at handling the\nproblem of 3D tracking, which provides the tracking of the consecutive frames\nof 3D shapes. We propose a novel unsupervised 3D shape registration framework\nnamed DeepTracking-Net, which uses the deep neural networks (DNNs) as auxiliary\nfunctions to produce spatially and temporally continuous displacement fields\nfor 3D tracking of objects in a temporal order. Our key novelty is that we\npresent a novel temporal-aware correspondence descriptor (TCD) that captures\nspatio-temporal essence from consecutive 3D point cloud frames. Specifically,\nour DeepTracking-Net starts with optimizing a randomly initialized latent TCD.\nThe TCD is then decoded to regress a continuous flow (i.e. a displacement\nvector field) which assigns a motion vector to every point of time-varying 3D\nshapes. Our DeepTracking-Net jointly optimizes TCDs and DNNs' weights towards\nthe minimization of an unsupervised alignment loss. Experiments on both\nsimulated and real data sets demonstrate that our unsupervised DeepTracking-Net\noutperforms the current supervised state-of-the-art method. In addition, we\nprepare a new synthetic 3D data, named SynMotions, to the 3D tracking and\nrecognition community.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 16:20:48 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Yuan", "Shuaihang", ""], ["Li", "Xiang", ""], ["Fang", "Yi", ""]]}, {"id": "2006.13856", "submitter": "Lassi Meronen", "authors": "Lassi Meronen, William J. Wilkinson, Arno Solin", "title": "Movement Tracking by Optical Flow Assisted Inertial Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust and accurate six degree-of-freedom tracking on portable devices\nremains a challenging problem, especially on small hand-held devices such as\nsmartphones. For improved robustness and accuracy, complementary movement\ninformation from an IMU and a camera is often fused. Conventional\nvisual-inertial methods fuse information from IMUs with a sparse cloud of\nfeature points tracked by the device camera. We consider a visually dense\napproach, where the IMU data is fused with the dense optical flow field\nestimated from the camera data. Learning-based methods applied to the full\nimage frames can leverage visual cues and global consistency of the flow field\nto improve the flow estimates. We show how a learning-based optical flow model\ncan be combined with conventional inertial navigation, and how ideas from\nprobabilistic deep learning can aid the robustness of the measurement updates.\nThe practical applicability is demonstrated on real-world data acquired by an\niPad in a challenging low-texture environment.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 16:36:13 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Meronen", "Lassi", ""], ["Wilkinson", "William J.", ""], ["Solin", "Arno", ""]]}, {"id": "2006.13863", "submitter": "Xiaobin Hu", "authors": "Xiaobin Hu, Yanyang Yan, Wenqi Ren, Hongwei Li, Yu Zhao, Amirhossein\n  Bayat, Bjoern Menze", "title": "Feedback Graph Attention Convolutional Network for Medical Image\n  Enhancement", "comments": "The description of the experiments is not accurate and complete, and\n  some details of equations and expressions should be corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artifacts, blur and noise are the common distortions degrading MRI images\nduring the acquisition process, and deep neural networks have been demonstrated\nto help in improving image quality. To well exploit global structural\ninformation and texture details, we propose a novel biomedical image\nenhancement network, named Feedback Graph Attention Convolutional Network\n(FB-GACN). As a key innovation, we consider the global structure of an image by\nbuilding a graph network from image sub-regions that we consider to be node\nfeatures, linking them non-locally according to their similarity. The proposed\nmodel consists of three main parts: 1) The parallel graph similarity branch and\ncontent branch, where the graph similarity branch aims at exploiting the\nsimilarity and symmetry across different image sub-regions in low-resolution\nfeature space and provides additional priors for the content branch to enhance\ntexture details. 2) A feedback mechanism with a recurrent structure to refine\nlow-level representations with high-level information and generate powerful\nhigh-level texture details by handling the feedback connections. 3) A\nreconstruction to remove the artifacts and recover super-resolution images by\nusing the estimated sub-region correlation priors obtained from the graph\nsimilarity branch. We evaluate our method on two image enhancement tasks: i)\ncross-protocol super resolution of diffusion MRI; ii) artifact removal of FLAIR\nMR images. Experimental results demonstrate that the proposed algorithm\noutperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 16:46:05 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 19:25:12 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Hu", "Xiaobin", ""], ["Yan", "Yanyang", ""], ["Ren", "Wenqi", ""], ["Li", "Hongwei", ""], ["Zhao", "Yu", ""], ["Bayat", "Amirhossein", ""], ["Menze", "Bjoern", ""]]}, {"id": "2006.13873", "submitter": "Manu Siddhartha", "authors": "Manu Siddhartha and Avik Santra", "title": "COVIDLite: A depth-wise separable deep neural network with white balance\n  and CLAHE for detection of COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Objective:Currently, the whole world is facing a pandemic\ndisease, novel Coronavirus also known as COVID-19, which spread in more than\n200 countries with around 3.3 million active cases and 4.4 lakh deaths\napproximately. Due to rapid increase in number of cases and limited supply of\ntesting kits, availability of alternative diagnostic method is necessary for\ncontaining the spread of COVID-19 cases at an early stage and reducing the\ndeath count. For making available an alternative diagnostic method, we proposed\na deep neural network based diagnostic method which can be easily integrated\nwith mobile devices for detection of COVID-19 and viral pneumonia using Chest\nX-rays (CXR) images. Methods:In this study, we have proposed a method named\nCOVIDLite, which is a combination of white balance followed by Contrast Limited\nAdaptive Histogram Equalization (CLAHE) and depth-wise separable convolutional\nneural network (DSCNN). In this method, white balance followed by CLAHE is used\nas an image preprocessing step for enhancing the visibility of CXR images and\nDSCNN trained using sparse cross entropy is used for image classification with\nlesser parameters and significantly lighter in size, i.e., 8.4 MB without\nquantization. Results:The proposed COVIDLite method resulted in improved\nperformance in comparison to vanilla DSCNN with no pre-processing. The proposed\nmethod achieved higher accuracy of 99.58% for binary classification, whereas\n96.43% for multiclass classification and out-performed various state-of-the-art\nmethods. Conclusion:Our proposed method, COVIDLite achieved exceptional results\non various performance metrics. With detailed model interpretations, COVIDLite\ncan assist radiologists in detecting COVID-19 patients from CXR images and can\nreduce the diagnosis time significantly.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 02:30:34 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Siddhartha", "Manu", ""], ["Santra", "Avik", ""]]}, {"id": "2006.13877", "submitter": "Yixin Wang", "authors": "Yixin Wang, Yao Zhang, Yang Liu, Jiang Tian, Cheng Zhong, Zhongchao\n  Shi, Yang Zhang, Zhiqiang He", "title": "Does Non-COVID19 Lung Lesion Help? Investigating Transferability in\n  COVID-19 CT Image Segmentation", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) is a highly contagious virus spreading\nall around the world. Deep learning has been adopted as an effective technique\nto aid COVID-19 detection and segmentation from computed tomography (CT)\nimages. The major challenge lies in the inadequate public COVID-19 datasets.\nRecently, transfer learning has become a widely used technique that leverages\nthe knowledge gained while solving one problem and applying it to a different\nbut related problem. However, it remains unclear whether various non-COVID19\nlung lesions could contribute to segmenting COVID-19 infection areas and how to\nbetter conduct this transfer procedure. This paper provides a way to understand\nthe transferability of non-COVID19 lung lesions. Based on a publicly available\nCOVID-19 CT dataset and three public non-COVID19 datasets, we evaluate four\ntransfer learning methods using 3D U-Net as a standard encoder-decoder method.\nThe results reveal the benefits of transferring knowledge from non-COVID19 lung\nlesions, and learning from multiple lung lesion datasets can extract more\ngeneral features, leading to accurate and robust pre-trained models. We further\nshow the capability of the encoder to learn feature representations of lung\nlesions, which improves segmentation accuracy and facilitates training\nconvergence. In addition, our proposed Hybrid-encoder learning method\nincorporates transferred lung lesion features from non-COVID19 datasets\neffectively and achieves significant improvement. These findings promote new\ninsights into transfer learning for COVID-19 CT image segmentation, which can\nalso be further generalized to other medical tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 04:40:51 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 05:03:12 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Yixin", ""], ["Zhang", "Yao", ""], ["Liu", "Yang", ""], ["Tian", "Jiang", ""], ["Zhong", "Cheng", ""], ["Shi", "Zhongchao", ""], ["Zhang", "Yang", ""], ["He", "Zhiqiang", ""]]}, {"id": "2006.13882", "submitter": "Mohammed Daoudi", "authors": "Benjamin Szczapa, Mohamed Daoudi, Stefano Berretti, Pietro Pala,\n  Alberto Del Bimbo, Zakia Hammal", "title": "Automatic Estimation of Self-Reported Pain by Interpretable\n  Representations of Motion Dynamics", "comments": "accepted at ICPR 2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automatic method for pain intensity measurement from video. For\neach video, pain intensity was measured using the dynamics of facial movement\nusing 66 facial points. Gram matrices formulation was used for facial points\ntrajectory representations on the Riemannian manifold of symmetric positive\nsemi-definite matrices of fixed rank. Curve fitting and temporal alignment were\nthen used to smooth the extracted trajectories. A Support Vector Regression\nmodel was then trained to encode the extracted trajectories into ten pain\nintensity levels consistent with the Visual Analogue Scale for pain intensity\nmeasurement. The proposed approach was evaluated using the UNBC McMaster\nShoulder Pain Archive and was compared to the state-of-the-art on the same\ndata. Using both 5-fold cross-validation and leave-one-subject-out\ncross-validation, our results are competitive with respect to state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:08:16 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Szczapa", "Benjamin", ""], ["Daoudi", "Mohamed", ""], ["Berretti", "Stefano", ""], ["Pala", "Pietro", ""], ["Del Bimbo", "Alberto", ""], ["Hammal", "Zakia", ""]]}, {"id": "2006.13886", "submitter": "Elizabeth Holm", "authors": "Tim Hsu, William K. Epting, Hokon Kim, Harry W. Abernathy, Gregory A.\n  Hackett, Anthony D. Rollett, Paul A. Salvador, and Elizabeth A. Holm", "title": "Microstructure Generation via Generative Adversarial Network for\n  Heterogeneous, Topologically Complex 3D Materials", "comments": "submitted to JOM", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cond-mat.mtrl-sci cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a large-scale, experimentally captured 3D microstructure dataset, we\nimplement the generative adversarial network (GAN) framework to learn and\ngenerate 3D microstructures of solid oxide fuel cell electrodes. The generated\nmicrostructures are visually, statistically, and topologically realistic, with\ndistributions of microstructural parameters, including volume fraction,\nparticle size, surface area, tortuosity, and triple phase boundary density,\nbeing highly similar to those of the original microstructure. These results are\ncompared and contrasted with those from an established, grain-based generation\nalgorithm (DREAM.3D). Importantly, simulations of electrochemical performance,\nusing a locally resolved finite element model, demonstrate that the GAN\ngenerated microstructures closely match the performance distribution of the\noriginal, while DREAM.3D leads to significant differences. The ability of the\ngenerative machine learning model to recreate microstructures with high\nfidelity suggests that the essence of complex microstructures may be captured\nand represented in a compact and manipulatable form.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 21:52:01 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Hsu", "Tim", ""], ["Epting", "William K.", ""], ["Kim", "Hokon", ""], ["Abernathy", "Harry W.", ""], ["Hackett", "Gregory A.", ""], ["Rollett", "Anthony D.", ""], ["Salvador", "Paul A.", ""], ["Holm", "Elizabeth A.", ""]]}, {"id": "2006.13890", "submitter": "Yamin Li", "authors": "Yamin Li, Jiancheng Yang, Yi Xu, Jingwei Xu, Xiaodan Ye, Guangyu Tao,\n  Xueqian Xie, Guixue Liu", "title": "Learning Tumor Growth via Follow-Up Volume Prediction for Lung Nodules", "comments": "MICCAI 2020", "journal-ref": null, "doi": "10.1007/978-3-030-59725-2_49", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Follow-up serves an important role in the management of pulmonary nodules for\nlung cancer. Imaging diagnostic guidelines with expert consensus have been made\nto help radiologists make clinical decision for each patient. However, tumor\ngrowth is such a complicated process that it is difficult to stratify high-risk\nnodules from low-risk ones based on morphologic characteristics. On the other\nhand, recent deep learning studies using convolutional neural networks (CNNs)\nto predict the malignancy score of nodules, only provides clinicians with\nblack-box predictions. To this end, we propose a unified framework, named\nNodule Follow-Up Prediction Network (NoFoNet), which predicts the growth of\npulmonary nodules with high-quality visual appearances and accurate\nquantitative results, given any time interval from baseline observations. It is\nachieved by predicting future displacement field of each voxel with a WarpNet.\nA TextureNet is further developed to refine textural details of WarpNet\noutputs. We also introduce techniques including Temporal Encoding Module and\nWarp Segmentation Loss to encourage time-aware and shape-aware representation\nlearning. We build an in-house follow-up dataset from two medical centers to\nvalidate the effectiveness of the proposed method. NoFoNet significantly\noutperforms direct prediction by a U-Net in terms of visual quality; more\nimportantly, it demonstrates accurate differentiating performance between high-\nand low-risk nodules. Our promising results suggest the potentials in computer\naided intervention for lung nodule management.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:18:46 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 04:27:42 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Li", "Yamin", ""], ["Yang", "Jiancheng", ""], ["Xu", "Yi", ""], ["Xu", "Jingwei", ""], ["Ye", "Xiaodan", ""], ["Tao", "Guangyu", ""], ["Xie", "Xueqian", ""], ["Liu", "Guixue", ""]]}, {"id": "2006.13895", "submitter": "Mohammed Daoudi", "authors": "Ettore Maria Celozzi, Luca Ciabini, Luca Cultrera, Pietro Pala,\n  Stefano Berretti, Mohamed Daoudi, Alberto Del Bimbo", "title": "Modelling the Statistics of Cyclic Activities by Trajectory Analysis on\n  the Manifold of Positive-Semi-Definite Matrices", "comments": "accepted at 15th IEEE International Conference on Automatic Face and\n  Gesture Recognition 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a model is presented to extract statistical summaries to\ncharacterize the repetition of a cyclic body action, for instance a gym\nexercise, for the purpose of checking the compliance of the observed action to\na template one and highlighting the parts of the action that are not correctly\nexecuted (if any). The proposed system relies on a Riemannian metric to compute\nthe distance between two poses in such a way that the geometry of the manifold\nwhere the pose descriptors lie is preserved; a model to detect the begin and\nend of each cycle; a model to temporally align the poses of different cycles so\nas to accurately estimate the \\emph{cross-sectional} mean and variance of poses\nacross different cycles. The proposed model is demonstrated using gym videos\ntaken from the Internet.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:29:43 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Celozzi", "Ettore Maria", ""], ["Ciabini", "Luca", ""], ["Cultrera", "Luca", ""], ["Pala", "Pietro", ""], ["Berretti", "Stefano", ""], ["Daoudi", "Mohamed", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2006.13904", "submitter": "Dumindu Tissera", "authors": "Dumindu Tissera, Kasun Vithanage, Rukshan Wijesinghe, Kumara\n  Kahatapitiya, Subha Fernando, Ranga Rodrigo", "title": "Feature-Dependent Cross-Connections in Multi-Path Neural Networks", "comments": "International Conference on Pattern Recognition (ICPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a particular task from a dataset, samples in which originate from\ndiverse contexts, is challenging, and usually addressed by deepening or\nwidening standard neural networks. As opposed to conventional network widening,\nmulti-path architectures restrict the quadratic increment of complexity to a\nlinear scale. However, existing multi-column/path networks or model ensembling\nmethods do not consider any feature-dependent allocation of parallel resources,\nand therefore, tend to learn redundant features. Given a layer in a multi-path\nnetwork, if we restrict each path to learn a context-specific set of features\nand introduce a mechanism to intelligently allocate incoming feature maps to\nsuch paths, each path can specialize in a certain context, reducing the\nredundancy and improving the quality of extracted features. This eventually\nleads to better-optimized usage of parallel resources. To do this, we propose\ninserting feature-dependent cross-connections between parallel sets of feature\nmaps in successive layers. The weighting coefficients of these\ncross-connections are computed from the input features of the particular layer.\nOur multi-path networks show improved image recognition accuracy at a similar\ncomplexity compared to conventional and state-of-the-art methods for deepening,\nwidening and adaptive feature extracting, in both small and large scale\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:38:03 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 16:49:47 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Tissera", "Dumindu", ""], ["Vithanage", "Kasun", ""], ["Wijesinghe", "Rukshan", ""], ["Kahatapitiya", "Kumara", ""], ["Fernando", "Subha", ""], ["Rodrigo", "Ranga", ""]]}, {"id": "2006.13906", "submitter": "Yi Fang", "authors": "Shuaihang Yuan, Xiang Li, Anthony Tzes, Yi Fang", "title": "3DMotion-Net: Learning Continuous Flow Function for 3D Motion Prediction", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we deal with the problem to predict the future 3D motions of\n3D object scans from previous two consecutive frames. Previous methods mostly\nfocus on sparse motion prediction in the form of skeletons. While in this paper\nwe focus on predicting dense 3D motions in the from of 3D point clouds. To\napproach this problem, we propose a self-supervised approach that leverages the\npower of the deep neural network to learn a continuous flow function of 3D\npoint clouds that can predict temporally consistent future motions and\nnaturally bring out the correspondences among consecutive point clouds at the\nsame time. More specifically, in our approach, to eliminate the unsolved and\nchallenging process of defining a discrete point convolution on 3D point cloud\nsequences to encode spatial and temporal information, we introduce a learnable\nlatent code to represent the temporal-aware shape descriptor which is optimized\nduring model training. Moreover, a temporally consistent motion Morpher is\nproposed to learn a continuous flow field which deforms a 3D scan from the\ncurrent frame to the next frame. We perform extensive experiments on D-FAUST,\nSCAPE and TOSCA benchmark data sets and the results demonstrate that our\napproach is capable of handling temporally inconsistent input and produces\nconsistent future 3D motion while requiring no ground truth supervision.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:39:19 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Yuan", "Shuaihang", ""], ["Li", "Xiang", ""], ["Tzes", "Anthony", ""], ["Fang", "Yi", ""]]}, {"id": "2006.13919", "submitter": "Aayush Bansal", "authors": "Aayush Bansal", "title": "Improving task-specific representation via 1M unlabelled images without\n  any extra knowledge", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a case-study to improve the task-specific representation by\nleveraging a million unlabelled images without any extra knowledge. We propose\nan exceedingly simple method of conditioning an existing representation on a\ndiverse data distribution and observe that a model trained on diverse examples\nacts as a better initialization. We extensively study our findings for the task\nof surface normal estimation and semantic segmentation from a single image. We\nimprove surface normal estimation on NYU-v2 depth dataset and semantic\nsegmentation on PASCAL VOC by 4% over base model. We did not use any\ntask-specific knowledge or auxiliary tasks, neither changed hyper-parameters\nnor made any modification in the underlying neural network architecture.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:49:05 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Bansal", "Aayush", ""]]}, {"id": "2006.13944", "submitter": "Moritz Platscher", "authors": "Alejandro Ungr\\'ia Hirte, Moritz Platscher, Thomas Joyce, Jeremy J.\n  Heit, Eric Tranvinh, Christian Federau", "title": "Diffusion-Weighted Magnetic Resonance Brain Images Generation with\n  Generative Adversarial Networks and Variational Autoencoders: A Comparison\n  Study", "comments": "20 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that high quality, diverse and realistic-looking diffusion-weighted\nmagnetic resonance images can be synthesized using deep generative models.\nBased on professional neuroradiologists' evaluations and diverse metrics with\nrespect to quality and diversity of the generated synthetic brain images, we\npresent two networks, the Introspective Variational Autoencoder and the\nStyle-Based GAN, that qualify for data augmentation in the medical field, where\ninformation is saved in a dispatched and inhomogeneous way and access to it is\nin many aspects restricted.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 18:00:01 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Hirte", "Alejandro Ungr\u00eda", ""], ["Platscher", "Moritz", ""], ["Joyce", "Thomas", ""], ["Heit", "Jeremy J.", ""], ["Tranvinh", "Eric", ""], ["Federau", "Christian", ""]]}, {"id": "2006.13977", "submitter": "David Stutz", "authors": "David Stutz, Nandhini Chandramoorthy, Matthias Hein, Bernt Schiele", "title": "Bit Error Robustness for Energy-Efficient DNN Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) accelerators received considerable attention in\npast years due to saved energy compared to mainstream hardware. Low-voltage\noperation of DNN accelerators allows to further reduce energy consumption\nsignificantly, however, causes bit-level failures in the memory storing the\nquantized DNN weights. In this paper, we show that a combination of robust\nfixed-point quantization, weight clipping, and random bit error training\n(RandBET) improves robustness against random bit errors in (quantized) DNN\nweights significantly. This leads to high energy savings from both low-voltage\noperation as well as low-precision quantization. Our approach generalizes\nacross operating voltages and accelerators, as demonstrated on bit errors from\nprofiled SRAM arrays. We also discuss why weight clipping alone is already a\nquite effective way to achieve robustness against bit errors. Moreover, we\nspecifically discuss the involved trade-offs regarding accuracy, robustness and\nprecision: Without losing more than 1% in accuracy compared to a normally\ntrained 8-bit DNN, we can reduce energy consumption on CIFAR-10 by 20%. Higher\nenergy savings of, e.g., 30%, are possible at the cost of 2.5% accuracy, even\nfor 4-bit DNNs.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 18:23:10 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 16:00:52 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 15:24:12 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Stutz", "David", ""], ["Chandramoorthy", "Nandhini", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "2006.13980", "submitter": "Rafael Redondo", "authors": "Rafael Redondo and Jaume Gibert", "title": "Extended Labeled Faces in-the-Wild (ELFW): Augmenting Classes for Face\n  Segmentation", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Existing face datasets often lack sufficient representation of occluding\nobjects, which can hinder recognition, but also supply meaningful information\nto understand the visual context. In this work, we introduce Extended Labeled\nFaces in-the-Wild (ELFW), a dataset supplementing with additional face-related\ncategories -- and also additional faces -- the originally released semantic\nlabels in the vastly used Labeled Faces in-the-Wild (LFW) dataset.\nAdditionally, two object-based data augmentation techniques are deployed to\nsynthetically enrich under-represented categories which, in benchmarking\nexperiments, reveal that not only segmenting the augmented categories improves,\nbut also the remaining ones benefit.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 18:26:17 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Redondo", "Rafael", ""], ["Gibert", "Jaume", ""]]}, {"id": "2006.13999", "submitter": "Hang Qiu", "authors": "Hang Qiu, Krishna Chintalapudi, Ramesh Govindan", "title": "Minimum Cost Active Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeling a data set completely is important for groundtruth generation. In\nthis paper, we consider the problem of minimum-cost labeling: classifying all\nimages in a large data set with a target accuracy bound at minimum dollar cost.\nHuman labeling can be prohibitive, so we train a classifier to accurately label\npart of the data set. However, training the classifier can be expensive too,\nparticularly with active learning. Our min-cost labeling uses a variant of\nactive learning to learn a model to predict the optimal training set size for\nthe classifier that minimizes overall cost, then uses active learning to train\nthe classifier to maximize the number of samples the classifier can correctly\nlabel. We validate our approach on well-known public data sets such as Fashion,\nCIFAR-10, and CIFAR-100. In some cases, our approach has 6X lower overall cost\nrelative to human labeling, and is always cheaper than the cheapest active\nlearning strategy.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 19:01:05 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Qiu", "Hang", ""], ["Chintalapudi", "Krishna", ""], ["Govindan", "Ramesh", ""]]}, {"id": "2006.14011", "submitter": "Thiago Rateke", "authors": "Thiago Rateke and Aldo von Wangenheim", "title": "Road obstacles positional and dynamic features extraction combining\n  object detection, stereo disparity maps and optical flow data", "comments": "11 pages", "journal-ref": "Machine Vision and Applications, 2020", "doi": "10.1007/s00138-020-01126-w", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the most relevant tasks in an intelligent vehicle navigation system is\nthe detection of obstacles. It is important that a visual perception system for\nnavigation purposes identifies obstacles, and it is also important that this\nsystem can extract essential information that may influence the vehicle's\nbehavior, whether it will be generating an alert for a human driver or guide an\nautonomous vehicle in order to be able to make its driving decisions. In this\npaper we present an approach for the identification of obstacles and extraction\nof class, position, depth and motion information from these objects that\nemploys data gained exclusively from passive vision. We performed our\nexperiments on two different data-sets and the results obtained shown a good\nefficacy from the use of depth and motion patterns to assess the obstacles'\npotential threat status.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 19:29:06 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Rateke", "Thiago", ""], ["von Wangenheim", "Aldo", ""]]}, {"id": "2006.14032", "submitter": "Jesse Mu", "authors": "Jesse Mu, Jacob Andreas", "title": "Compositional Explanations of Neurons", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a procedure for explaining neurons in deep representations by\nidentifying compositional logical concepts that closely approximate neuron\nbehavior. Compared to prior work that uses atomic labels as explanations,\nanalyzing neurons compositionally allows us to more precisely and expressively\ncharacterize their behavior. We use this procedure to answer several questions\non interpretability in models for vision and natural language processing.\nFirst, we examine the kinds of abstractions learned by neurons. In image\nclassification, we find that many neurons learn highly abstract but\nsemantically coherent visual concepts, while other polysemantic neurons detect\nmultiple unrelated features; in natural language inference (NLI), neurons learn\nshallow lexical heuristics from dataset biases. Second, we see whether\ncompositional explanations give us insight into model performance: vision\nneurons that detect human-interpretable concepts are positively correlated with\ntask performance, while NLI neurons that fire for shallow heuristics are\nnegatively correlated with task performance. Finally, we show how compositional\nexplanations provide an accessible way for end users to produce simple\n\"copy-paste\" adversarial examples that change model behavior in predictable\nways.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 20:37:05 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 23:46:51 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Mu", "Jesse", ""], ["Andreas", "Jacob", ""]]}, {"id": "2006.14042", "submitter": "Huiying Li", "authors": "Huiying Li, Shawn Shan, Emily Wenger, Jiayun Zhang, Haitao Zheng, Ben\n  Y. Zhao", "title": "Blacklight: Defending Black-Box Adversarial Attacks on Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vulnerability of deep neural networks (DNNs) to adversarial examples is\nwell documented. Under the strong white-box threat model, where attackers have\nfull access to DNN internals, recent work has produced continual advancements\nin defenses, often followed by more powerful attacks that break them.\nMeanwhile, research on the more realistic black-box threat model has focused\nalmost entirely on reducing the query-cost of attacks, making them increasingly\npractical for ML models already deployed today.\n  This paper proposes and evaluates Blacklight, a new defense against black-box\nadversarial attacks. Blacklight targets a key property of black-box attacks: to\ncompute adversarial examples, they produce sequences of highly similar images\nwhile trying to minimize the distance from some initial benign input. To detect\nan attack, Blacklight computes for each query image a compact set of one-way\nhash values that form a probabilistic fingerprint. Variants of an image produce\nnearly identical fingerprints, and fingerprint generation is robust against\nmanipulation. We evaluate Blacklight on 5 state-of-the-art black-box attacks,\nacross a variety of models and classification tasks. While the most efficient\nattacks take thousands or tens of thousands of queries to complete, Blacklight\nidentifies them all, often after only a handful of queries. Blacklight is also\nrobust against several powerful countermeasures, including an optimal black-box\nattack that approximates white-box attacks in efficiency. Finally, Blacklight\nsignificantly outperforms the only known alternative in both detection coverage\nof attack queries and resistance against persistent attackers.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 20:52:24 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Li", "Huiying", ""], ["Shan", "Shawn", ""], ["Wenger", "Emily", ""], ["Zhang", "Jiayun", ""], ["Zheng", "Haitao", ""], ["Zhao", "Ben Y.", ""]]}, {"id": "2006.14077", "submitter": "Vikash Sehwag", "authors": "Vikash Sehwag, Rajvardhan Oak, Mung Chiang, Prateek Mittal", "title": "Time for a Background Check! Uncovering the impact of Background\n  Features on Deep Neural Networks", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing expressive power, deep neural networks have significantly\nimproved the state-of-the-art on image classification datasets, such as\nImageNet. In this paper, we investigate to what extent the increasing\nperformance of deep neural networks is impacted by background features? In\nparticular, we focus on background invariance, i.e., accuracy unaffected by\nswitching background features and background influence, i.e., predictive power\nof background features itself when foreground is masked. We perform experiments\nwith 32 different neural networks ranging from small-size networks to\nlarge-scale networks trained with up to one Billion images. Our investigations\nreveal that increasing expressive power of DNNs leads to higher influence of\nbackground features, while simultaneously, increases their ability to make the\ncorrect prediction when background features are removed or replaced with a\nrandomly selected texture-based background.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 22:17:50 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Sehwag", "Vikash", ""], ["Oak", "Rajvardhan", ""], ["Chiang", "Mung", ""], ["Mittal", "Prateek", ""]]}, {"id": "2006.14086", "submitter": "Xiaofeng Ma", "authors": "Xiaofeng Ma, Michael Kirby, Chris Peterson", "title": "The flag manifold as a tool for analyzing and comparing data sets", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shape and orientation of data clouds reflect variability in observations\nthat can confound pattern recognition systems. Subspace methods, utilizing\nGrassmann manifolds, have been a great aid in dealing with such variability.\nHowever, this usefulness begins to falter when the data cloud contains\nsufficiently many outliers corresponding to stray elements from another class\nor when the number of data points is larger than the number of features. We\nillustrate how nested subspace methods, utilizing flag manifolds, can help to\ndeal with such additional confounding factors. Flag manifolds, which are\nparameter spaces for nested subspaces, are a natural geometric generalization\nof Grassmann manifolds. To make practical comparisons on a flag manifold,\nalgorithms are proposed for determining the distances between points $[A], [B]$\non a flag manifold, where $A$ and $B$ are arbitrary orthogonal matrix\nrepresentatives for $[A]$ and $[B]$, and for determining the initial direction\nof these minimal length geodesics. The approach is illustrated in the context\nof (hyper) spectral imagery showing the impact of ambient dimension, sample\ndimension, and flag structure.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 22:29:02 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Ma", "Xiaofeng", ""], ["Kirby", "Michael", ""], ["Peterson", "Chris", ""]]}, {"id": "2006.14090", "submitter": "Ming Lin", "authors": "Ming Lin, Hesen Chen, Xiuyu Sun, Qi Qian, Hao Li, Rong Jin", "title": "Neural Architecture Design for GPU-Efficient Networks", "comments": "update training setting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many mission-critical systems are based on GPU for inference. It requires not\nonly high recognition accuracy but also low latency in responding time.\nAlthough many studies are devoted to optimizing the structure of deep models\nfor efficient inference, most of them do not leverage the architecture of\n\\textbf{modern GPU} for fast inference, leading to suboptimal performance. To\naddress this issue, we propose a general principle for designing GPU-efficient\nnetworks based on extensive empirical studies. This design principle enables us\nto search for GPU-efficient network structures effectively by a simple and\nlightweight method as opposed to most Neural Architecture Search (NAS) methods\nthat are complicated and computationally expensive. Based on the proposed\nframework, we design a family of GPU-Efficient Networks, or GENets in short. We\ndid extensive evaluations on multiple GPU platforms and inference engines.\nWhile achieving $\\geq 81.3\\%$ top-1 accuracy on ImageNet, GENet is up to $6.4$\ntimes faster than EfficienNet on GPU. It also outperforms most state-of-the-art\nmodels that are more efficient than EfficientNet in high precision regimes. Our\nsource code and pre-trained models are available from\n\\url{https://github.com/idstcv/GPU-Efficient-Networks}.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 22:42:18 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 18:11:08 GMT"}, {"version": "v3", "created": "Sun, 12 Jul 2020 05:11:21 GMT"}, {"version": "v4", "created": "Tue, 11 Aug 2020 22:54:26 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Lin", "Ming", ""], ["Chen", "Hesen", ""], ["Sun", "Xiuyu", ""], ["Qian", "Qi", ""], ["Li", "Hao", ""], ["Jin", "Rong", ""]]}, {"id": "2006.14105", "submitter": "Rafael Ulises Pizarro Solar", "authors": "Rafael Pizarro Solar and Michal Pleskowicz", "title": "Block-matching in FPGA", "comments": "19 pages, 15 figures, paper submitted in \"CS413 - Computational\n  Photography\" at EPFL, for project repository see\n  $\\href{https://github.com/UlisesLuzius/ImageProcessingPipeline/}{\\text{link}}$", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Block-matching and 3D filtering (BM3D) is an image denoising algorithm that\nworks in two similar steps. Both of these steps need to perform grouping by\nblock-matching. We implement the block-matching in an FPGA, leveraging its\nability to perform parallel computations. Our goal is to enable other\nresearchers to use our solution in the future for real-time video denoising in\nvideo cameras that use FPGAs (such as the AXIOM Beta).\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 23:53:28 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Solar", "Rafael Pizarro", ""], ["Pleskowicz", "Michal", ""]]}, {"id": "2006.14107", "submitter": "Jogendra Nath Kundu", "authors": "Jogendra Nath Kundu, Siddharth Seth, Rahul M V, Mugalodi Rakesh, R.\n  Venkatesh Babu, Anirban Chakraborty", "title": "Kinematic-Structure-Preserved Representation for Unsupervised 3D Human\n  Pose Estimation", "comments": "AAAI 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of 3D human pose from monocular image has gained considerable\nattention, as a key step to several human-centric applications. However,\ngeneralizability of human pose estimation models developed using supervision on\nlarge-scale in-studio datasets remains questionable, as these models often\nperform unsatisfactorily on unseen in-the-wild environments. Though\nweakly-supervised models have been proposed to address this shortcoming,\nperformance of such models relies on availability of paired supervision on some\nrelated tasks, such as 2D pose or multi-view image pairs. In contrast, we\npropose a novel kinematic-structure-preserved unsupervised 3D pose estimation\nframework, which is not restrained by any paired or unpaired weak supervisions.\nOur pose estimation framework relies on a minimal set of prior knowledge that\ndefines the underlying kinematic 3D structure, such as skeletal joint\nconnectivity information with bone-length ratios in a fixed canonical scale.\nThe proposed model employs three consecutive differentiable transformations\nnamed as forward-kinematics, camera-projection and spatial-map transformation.\nThis design not only acts as a suitable bottleneck stimulating effective pose\ndisentanglement but also yields interpretable latent pose representations\navoiding training of an explicit latent embedding to pose mapper. Furthermore,\ndevoid of unstable adversarial setup, we re-utilize the decoder to formalize an\nenergy-based loss, which enables us to learn from in-the-wild videos, beyond\nlaboratory settings. Comprehensive experiments demonstrate our state-of-the-art\nunsupervised and weakly-supervised pose estimation performance on both\nHuman3.6M and MPI-INF-3DHP datasets. Qualitative results on unseen environments\nfurther establish our superior generalization ability.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 23:56:33 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Kundu", "Jogendra Nath", ""], ["Seth", "Siddharth", ""], ["M", "Rahul", "V"], ["Rakesh", "Mugalodi", ""], ["Babu", "R. Venkatesh", ""], ["Chakraborty", "Anirban", ""]]}, {"id": "2006.14200", "submitter": "Andreas Lugmayr", "authors": "Andreas Lugmayr and Martin Danelljan and Luc Van Gool and Radu Timofte", "title": "SRFlow: Learning the Super-Resolution Space with Normalizing Flow", "comments": "ECCV 2020 Spotlight | git.io/SRFlow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution is an ill-posed problem, since it allows for multiple\npredictions for a given low-resolution image. This fundamental fact is largely\nignored by state-of-the-art deep learning based approaches. These methods\ninstead train a deterministic mapping using combinations of reconstruction and\nadversarial losses. In this work, we therefore propose SRFlow: a normalizing\nflow based super-resolution method capable of learning the conditional\ndistribution of the output given the low-resolution input. Our model is trained\nin a principled manner using a single loss, namely the negative log-likelihood.\nSRFlow therefore directly accounts for the ill-posed nature of the problem, and\nlearns to predict diverse photo-realistic high-resolution images. Moreover, we\nutilize the strong image posterior learned by SRFlow to design flexible image\nmanipulation techniques, capable of enhancing super-resolved images by, e.g.,\ntransferring content from other images. We perform extensive experiments on\nfaces, as well as on super-resolution in general. SRFlow outperforms\nstate-of-the-art GAN-based approaches in terms of both PSNR and perceptual\nquality metrics, while allowing for diversity through the exploration of the\nspace of super-resolved solutions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 06:34:04 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 14:55:35 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Lugmayr", "Andreas", ""], ["Danelljan", "Martin", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2006.14208", "submitter": "Peng Zhou", "authors": "Peng Zhou, Lingxi Xie, Xiaopeng Zhang, Bingbing Ni, Qi Tian", "title": "Searching towards Class-Aware Generators for Conditional Generative\n  Adversarial Networks", "comments": "Code is available at \\url{https://github.com/PeterouZh/NAS_cGAN}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Generative Adversarial Networks (cGAN) were designed to generate\nimages based on the provided conditions, \\eg, class-level distributions.\nHowever, existing methods have used the same generating architecture for all\nclasses. This paper presents a novel idea that adopts NAS to find a distinct\narchitecture for each class. The search space contains regular and\nclass-modulated convolutions, where the latter is designed to introduce\nclass-specific information while avoiding the reduction of training data for\neach class generator. The search algorithm follows a weight-sharing pipeline\nwith mixed-architecture optimization so that the search cost does not grow with\nthe number of classes. To learn the sampling policy, a Markov decision process\nis embedded into the search algorithm and a moving average is applied for\nbetter stability. We evaluate our approach on CIFAR10 and CIFAR100. Besides\nachieving better image generation quality in terms of FID scores, we discover\nseveral insights that are helpful in designing cGAN models. Code is available\nat https://github.com/PeterouZh/NAS_cGAN.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 07:05:28 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 02:07:12 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhou", "Peng", ""], ["Xie", "Lingxi", ""], ["Zhang", "Xiaopeng", ""], ["Ni", "Bingbing", ""], ["Tian", "Qi", ""]]}, {"id": "2006.14215", "submitter": "Alexandr G. Rassadin", "authors": "Alexandr G. Rassadin", "title": "Deep Residual 3D U-Net for Joint Segmentation and Texture Classification\n  of Nodules in Lung", "comments": "10 pages, 5 figures, 2 tables, accepted for publication at ICIAR 2020\n  (LNDb Grand Challenge)", "journal-ref": "ICIAR 2020: Image Analysis and Recognition pp 419-427", "doi": "10.1007/978-3-030-50516-5_37", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a method for lung nodules segmentation, their texture\nclassification and subsequent follow-up recommendation from the CT image of\nlung. Our method consists of neural network model based on popular U-Net\narchitecture family but modified for the joint nodule segmentation and its\ntexture classification tasks and an ensemble-based model for the follow-up\nrecommendation. This solution was evaluated within the LNDb medical imaging\nchallenge and produced the best nodule segmentation result on the final\nleaderboard.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 07:20:41 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 05:08:18 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Rassadin", "Alexandr G.", ""]]}, {"id": "2006.14239", "submitter": "Navid Mahmoudian Bidgoli", "authors": "Navid Mahmoudian Bidgoli, Thomas Maugey, Aline Roumy", "title": "Fine granularity access in interactive compression of 360-degree images\n  based on rate-adaptive channel codes", "comments": "accepted to be published in IEEE Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2020.3017890", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new interactive compression scheme for\nomnidirectional images. This requires two characteristics: efficient\ncompression of data, to lower the storage cost, and random access ability to\nextract part of the compressed stream requested by the user (for reducing the\ntransmission rate). For efficient compression, data needs to be predicted by a\nseries of references that have been pre-defined and compressed. This contrasts\nwith the spirit of random accessibility. We propose a solution for this problem\nbased on incremental codes implemented by rate-adaptive channel codes. This\nscheme encodes the image while adapting to any user request and leads to an\nefficient coding that is flexible in extracting data depending on the available\ninformation at the decoder. Therefore, only the information that is needed to\nbe displayed at the user's side is transmitted during the user's request, as if\nthe request was already known at the encoder. The experimental results\ndemonstrate that our coder obtains a better transmission rate than the\nstate-of-the-art tile-based methods at a small cost in storage. Moreover, the\ntransmission rate grows gradually with the size of the request and avoids a\nstaircase effect, which shows the perfect suitability of our coder for\ninteractive transmission.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 08:13:48 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 13:45:21 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Bidgoli", "Navid Mahmoudian", ""], ["Maugey", "Thomas", ""], ["Roumy", "Aline", ""]]}, {"id": "2006.14255", "submitter": "Rakshit Naidu", "authors": "Haofan Wang, Rakshit Naidu, Joy Michael and Soumya Snigdha Kundu", "title": "SS-CAM: Smoothed Score-CAM for Sharper Visual Feature Localization", "comments": "7 pages, 4 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interpretation of the underlying mechanisms of Deep Convolutional Neural\nNetworks has become an important aspect of research in the field of deep\nlearning due to their applications in high-risk environments. To explain these\nblack-box architectures there have been many methods applied so the internal\ndecisions can be analyzed and understood. In this paper, built on the top of\nScore-CAM, we introduce an enhanced visual explanation in terms of visual\nsharpness called SS-CAM, which produces centralized localization of object\nfeatures within an image through a smooth operation. We evaluate our method on\nthe ILSVRC 2012 Validation dataset, which outperforms Score-CAM on both\nfaithfulness and localization tasks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 08:51:54 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 22:32:22 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 13:02:55 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Wang", "Haofan", ""], ["Naidu", "Rakshit", ""], ["Michael", "Joy", ""], ["Kundu", "Soumya Snigdha", ""]]}, {"id": "2006.14262", "submitter": "Chiranjib Sur", "authors": "Chiranjib Sur", "title": "SACT: Self-Aware Multi-Space Feature Composition Transformer for\n  Multinomial Attention for Video Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video captioning works on the two fundamental concepts, feature detection and\nfeature composition. While modern day transformers are beneficial in composing\nfeatures, they lack the fundamental problems of selecting and understanding of\nthe contents. As the feature length increases, it becomes increasingly\nimportant to include provisions for improved capturing of the pertinent\ncontents. In this work, we have introduced a new concept of Self-Aware\nComposition Transformer (SACT) that is capable of generating Multinomial\nAttention (MultAtt) which is a way of generating distributions of various\ncombinations of frames. Also, multi-head attention transformer works on the\nprinciple of combining all possible contents for attention, which is good for\nnatural language classification, but has limitations for video captioning.\nVideo contents have repetitions and require parsing of important contents for\nbetter content composition. In this work, we have introduced SACT for more\nselective attention and combined them for different attention heads for better\ncapturing of the usable contents for any applications. To address the problem\nof diversification and encourage selective utilization, we propose the\nSelf-Aware Composition Transformer model for dense video captioning and apply\nthe technique on two benchmark datasets like ActivityNet and YouCookII.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 09:11:49 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Sur", "Chiranjib", ""]]}, {"id": "2006.14264", "submitter": "Chiranjib Sur", "authors": "Chiranjib Sur", "title": "Self-Segregating and Coordinated-Segregating Transformer for Focused\n  Deep Multi-Modular Network for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention mechanism has gained huge popularity due to its effectiveness in\nachieving high accuracy in different domains. But attention is opportunistic\nand is not justified by the content or usability of the content. Transformer\nlike structure creates all/any possible attention(s). We define segregating\nstrategies that can prioritize the contents for the applications for\nenhancement of performance. We defined two strategies: Self-Segregating\nTransformer (SST) and Coordinated-Segregating Transformer (CST) and used it to\nsolve visual question answering application. Self-segregation strategy for\nattention contributes in better understanding and filtering the information\nthat can be most helpful for answering the question and create diversity of\nvisual-reasoning for attention. This work can easily be used in many other\napplications that involve repetition and multiple frames of features and would\nreduce the commonality of the attentions to a great extent. Visual Question\nAnswering (VQA) requires understanding and coordination of both images and\ntextual interpretations. Experiments demonstrate that segregation strategies\nfor cascaded multi-head transformer attention outperforms many previous works\nand achieved considerable improvement for VQA-v2 dataset benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 09:17:03 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Sur", "Chiranjib", ""]]}, {"id": "2006.14265", "submitter": "Yasin Yazici", "authors": "Yasin Yazici, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Vijay\n  Chandrasekhar", "title": "Empirical Analysis of Overfitting and Mode Drop in GAN Training", "comments": "To appear in ICIP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine two key questions in GAN training, namely overfitting and mode\ndrop, from an empirical perspective. We show that when stochasticity is removed\nfrom the training procedure, GANs can overfit and exhibit almost no mode drop.\nOur results shed light on important characteristics of the GAN training\nprocedure. They also provide evidence against prevailing intuitions that GANs\ndo not memorize the training set, and that mode dropping is mainly due to\nproperties of the GAN objective rather than how it is optimized during\ntraining.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 09:17:32 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Yazici", "Yasin", ""], ["Foo", "Chuan-Sheng", ""], ["Winkler", "Stefan", ""], ["Yap", "Kim-Hui", ""], ["Chandrasekhar", "Vijay", ""]]}, {"id": "2006.14308", "submitter": "Xiehe Huang", "authors": "Xiehe Huang, Weihong Deng, Haifeng Shen, Xiubao Zhang, Jieping Ye", "title": "PropagationNet: Propagate Points to Curve to Learn Structure Information", "comments": "10 pages, 8 figures, 8 tables, CVPR2020", "journal-ref": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020, pp. 7265-7274", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning technique has dramatically boosted the performance of face\nalignment algorithms. However, due to large variability and lack of samples,\nthe alignment problem in unconstrained situations, \\emph{e.g}\\onedot large head\nposes, exaggerated expression, and uneven illumination, is still largely\nunsolved. In this paper, we explore the instincts and reasons behind our two\nproposals, \\emph{i.e}\\onedot Propagation Module and Focal Wing Loss, to tackle\nthe problem. Concretely, we present a novel structure-infused face alignment\nalgorithm based on heatmap regression via propagating landmark heatmaps to\nboundary heatmaps, which provide structure information for further attention\nmap generation. Moreover, we propose a Focal Wing Loss for mining and\nemphasizing the difficult samples under in-the-wild condition. In addition, we\nadopt methods like CoordConv and Anti-aliased CNN from other fields that\naddress the shift-variance problem of CNN for face alignment. When implementing\nextensive experiments on different benchmarks, \\emph{i.e}\\onedot WFLW, 300W,\nand COFW, our method outperforms state-of-the-arts by a significant margin. Our\nproposed approach achieves 4.05\\% mean error on WFLW, 2.93\\% mean error on 300W\nfull-set, and 3.71\\% mean error on COFW.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 11:08:59 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Huang", "Xiehe", ""], ["Deng", "Weihong", ""], ["Shen", "Haifeng", ""], ["Zhang", "Xiubao", ""], ["Ye", "Jieping", ""]]}, {"id": "2006.14319", "submitter": "Ivan Snozzi", "authors": "Toussain Cardot, Pilar Marxer, and Ivan Snozzi", "title": "Deep Learning for Cornea Microscopy Blind Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this project is to build a deep-learning solution that deblurs\ncornea scans, used for medical examination. The spherical shape of the eye\nprevents ophtamologist from having completely sharp image. Provided with a\nstack of corneas from confocal images, our approach is to build a model that\nperforms an upscaling of the images using an SR (Super Resolution) Network.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 11:50:35 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Cardot", "Toussain", ""], ["Marxer", "Pilar", ""], ["Snozzi", "Ivan", ""]]}, {"id": "2006.14321", "submitter": "Jonathan P. Epperlein PhD", "authors": "Sergiy Zhuk, Jonathan P. Epperlein, Rahul Nair, Seshu Thirupati, Pol\n  Mac Aonghusa, Ronan Cahill, Donal O'Shea", "title": "Perfusion Quantification from Endoscopic Videos: Learning to Read Tumor\n  Signatures", "comments": "To be published in 23rd International Conference on Medical Image\n  Computing & Computer Assisted Intervention (MICCAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intra-operative identification of malignant versus benign or healthy tissue\nis a major challenge in fluorescence guided cancer surgery. We propose a\nperfusion quantification method for computer-aided interpretation of subtle\ndifferences in dynamic perfusion patterns which can be used to distinguish\nbetween normal tissue and benign or malignant tumors intra-operatively in\nreal-time by using multispectral endoscopic videos. The method exploits the\nfact that vasculature arising from cancer angiogenesis gives tumors differing\nperfusion patterns from the surrounding tissue, and defines a signature of\ntumor which could be used to differentiate tumors from normal tissues.\nExperimental evaluation of our method on a cohort of colorectal cancer surgery\nendoscopic videos suggests that the proposed tumor signature is able to\nsuccessfully discriminate between healthy, cancerous and benign tissue with 95%\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 11:53:20 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Zhuk", "Sergiy", ""], ["Epperlein", "Jonathan P.", ""], ["Nair", "Rahul", ""], ["Thirupati", "Seshu", ""], ["Mac Aonghusa", "Pol", ""], ["Cahill", "Ronan", ""], ["O'Shea", "Donal", ""]]}, {"id": "2006.14345", "submitter": "Zhenxi Zhang", "authors": "Zhenxi Zhang, Chunna Tian, Jie Li, Zhusi Zhong, Zhicheng Jiao, and\n  Xinbo Gao", "title": "Collaborative Boundary-aware Context Encoding Networks for Error Map\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation is usually regarded as one of the most important\nintermediate steps in clinical situations and medical imaging research. Thus,\naccurately assessing the segmentation quality of the automatically generated\npredictions is essential for guaranteeing the reliability of the results of the\ncomputer-assisted diagnosis (CAD). Many researchers apply neural networks to\ntrain segmentation quality regression models to estimate the segmentation\nquality of a new data cohort without labeled ground truth. Recently, a novel\nidea is proposed that transforming the segmentation quality assessment (SQA)\nproblem intothe pixel-wise error map prediction task in the form of\nsegmentation. However, the simple application of vanilla segmentation\nstructures in medical image fails to detect some small and thin error regions\nof the auto-generated masks with complex anatomical structures. In this paper,\nwe propose collaborative boundaryaware context encoding networks called AEP-Net\nfor error prediction task. Specifically, we propose a collaborative feature\ntransformation branch for better feature fusion between images and masks, and\nprecise localization of error regions. Further, we propose a context encoding\nmodule to utilize the global predictor from the error map to enhance the\nfeature representation and regularize the networks. We perform experiments on\nIBSR v2.0 dataset and ACDC dataset. The AEP-Net achieves an average DSC of\n0.8358, 0.8164 for error prediction task,and shows a high Pearson correlation\ncoefficient of 0.9873 between the actual segmentation accuracy and the\npredicted accuracy inferred from the predicted error map on IBSR v2.0 dataset,\nwhich verifies the efficacy of our AEP-Net.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 12:42:01 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Zhang", "Zhenxi", ""], ["Tian", "Chunna", ""], ["Li", "Jie", ""], ["Zhong", "Zhusi", ""], ["Jiao", "Zhicheng", ""], ["Gao", "Xinbo", ""]]}, {"id": "2006.14347", "submitter": "Xi Li", "authors": "Jiabao Cui, Xuewei Li, Bin Li, Hanbin Zhao, Bourahla Omar, and Xi Li", "title": "Epoch-evolving Gaussian Process Guided Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel learning scheme called epoch-evolving\nGaussian Process Guided Learning (GPGL), which aims at characterizing the\ncorrelation information between the batch-level distribution and the global\ndata distribution. Such correlation information is encoded as context labels\nand needs renewal every epoch. With the guidance of the context label and\nground truth label, GPGL scheme provides a more efficient optimization through\nupdating the model parameters with a triangle consistency loss. Furthermore,\nour GPGL scheme can be further generalized and naturally applied to the current\ndeep models, outperforming the existing batch-based state-of-the-art models on\nmainstream datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) remarkably.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 12:45:17 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Cui", "Jiabao", ""], ["Li", "Xuewei", ""], ["Li", "Bin", ""], ["Zhao", "Hanbin", ""], ["Omar", "Bourahla", ""], ["Li", "Xi", ""]]}, {"id": "2006.14348", "submitter": "Eli Shlizerman", "authors": "Kun Su, Xiulong Liu, Eli Shlizerman", "title": "Audeo: Audio Generation for a Silent Performance Video", "comments": "Please see associated video at\n  https://www.youtube.com/watch?v=8rS3VgjG7_c", "journal-ref": "Advances in neural information processing 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel system that gets as an input video frames of a musician\nplaying the piano and generates the music for that video. Generation of music\nfrom visual cues is a challenging problem and it is not clear whether it is an\nattainable goal at all. Our main aim in this work is to explore the\nplausibility of such a transformation and to identify cues and components able\nto carry the association of sounds with visual events. To achieve the\ntransformation we built a full pipeline named `\\textit{Audeo}' containing three\ncomponents. We first translate the video frames of the keyboard and the\nmusician hand movements into raw mechanical musical symbolic representation\nPiano-Roll (Roll) for each video frame which represents the keys pressed at\neach time step. We then adapt the Roll to be amenable for audio synthesis by\nincluding temporal correlations. This step turns out to be critical for\nmeaningful audio generation. As a last step, we implement Midi synthesizers to\ngenerate realistic music. \\textit{Audeo} converts video to audio smoothly and\nclearly with only a few setup constraints. We evaluate \\textit{Audeo} on `in\nthe wild' piano performance videos and obtain that their generated music is of\nreasonable audio quality and can be successfully recognized with high precision\nby popular music identification software.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 00:58:59 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Su", "Kun", ""], ["Liu", "Xiulong", ""], ["Shlizerman", "Eli", ""]]}, {"id": "2006.14374", "submitter": "Yasuhiro Yao", "authors": "Yasuhiro Yao, Menandro Roxas, Ryoichi Ishikawa, Shingo Ando, Jun\n  Shimamura, Takeshi Oishi", "title": "Discontinuous and Smooth Depth Completion with Binary Anisotropic\n  Diffusion Tensor", "comments": "8 pages 6 figures", "journal-ref": null, "doi": "10.1109/LRA.2020.3005890", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised real-time dense depth completion from a sparse\ndepth map guided by a single image. Our method generates a smooth depth map\nwhile preserving discontinuity between different objects. Our key idea is a\nBinary Anisotropic Diffusion Tensor (B-ADT) which can completely eliminate\nsmoothness constraint at intended positions and directions by applying it to\nvariational regularization. We also propose an Image-guided Nearest Neighbor\nSearch (IGNNS) to derive a piecewise constant depth map which is used for B-ADT\nderivation and in the data term of the variational energy. Our experiments show\nthat our method can outperform previous unsupervised and semi-supervised depth\ncompletion methods in terms of accuracy. Moreover, since our resulting depth\nmap preserves the discontinuity between objects, the result can be converted to\na visually plausible point cloud. This is remarkable since previous methods\ngenerate unnatural surface-like artifacts between discontinuous objects.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 13:16:47 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Yao", "Yasuhiro", ""], ["Roxas", "Menandro", ""], ["Ishikawa", "Ryoichi", ""], ["Ando", "Shingo", ""], ["Shimamura", "Jun", ""], ["Oishi", "Takeshi", ""]]}, {"id": "2006.14380", "submitter": "Dong Hui Kim", "authors": "Dong Hui Kim", "title": "Deep Convolutional GANs for Car Image Generation", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the application of deep convolutional GANs on\ncar image generation. We improve upon the commonly used DCGAN architecture by\nimplementing Wasserstein loss to decrease mode collapse and introducing dropout\nat the end of the discrimiantor to introduce stochasticity. Furthermore, we\nintroduce convolutional layers at the end of the generator to improve\nexpressiveness and smooth noise. All of these improvements upon the DCGAN\narchitecture comprise our proposal of the novel BoolGAN architecture, which is\nable to decrease the FID from 195.922 (baseline) to 165.966.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 06:56:56 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Kim", "Dong Hui", ""]]}, {"id": "2006.14395", "submitter": "Orcun Goksel", "authors": "Melanie Bernhardt, Valery Vishnevskiy, Richard Rau, Orcun Goksel", "title": "Training Variational Networks with Multi-Domain Simulations:\n  Speed-of-Sound Image Reconstruction", "comments": null, "journal-ref": null, "doi": "10.1109/TUFFC.2020.3010186", "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speed-of-sound has been shown as a potential biomarker for breast cancer\nimaging, successfully differentiating malignant tumors from benign ones.\nSpeed-of-sound images can be reconstructed from time-of-flight measurements\nfrom ultrasound images acquired using conventional handheld ultrasound\ntransducers. Variational Networks (VN) have recently been shown to be a\npotential learning-based approach for optimizing inverse problems in image\nreconstruction. Despite earlier promising results, these methods however do not\ngeneralize well from simulated to acquired data, due to the domain shift. In\nthis work, we present for the first time a VN solution for a pulse-echo SoS\nimage reconstruction problem using diverging waves with conventional\ntransducers and single-sided tissue access. This is made possible by\nincorporating simulations with varying complexity into training. We use loop\nunrolling of gradient descent with momentum, with an exponentially weighted\nloss of outputs at each unrolled iteration in order to regularize training. We\nlearn norms as activation functions regularized to have smooth forms for\nrobustness to input distribution variations. We evaluate reconstruction quality\non ray-based and full-wave simulations as well as on tissue-mimicking phantom\ndata, in comparison to a classical iterative (L-BFGS) optimization of this\nimage reconstruction problem. We show that the proposed regularization\ntechniques combined with multi-source domain training yield substantial\nimprovements in the domain adaptation capabilities of VN, reducing median RMSE\nby 54% on a wave-based simulation dataset compared to the baseline VN. We also\nshow that on data acquired from a tissue-mimicking breast phantom the proposed\nVN provides improved reconstruction in 12 milliseconds.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 13:32:08 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Bernhardt", "Melanie", ""], ["Vishnevskiy", "Valery", ""], ["Rau", "Richard", ""], ["Goksel", "Orcun", ""]]}, {"id": "2006.14419", "submitter": "Abdolkarim Saeedi", "authors": "Abdolkarim Saeedi, Maryam Saeedi, Arash Maghsoudi", "title": "A Novel and Reliable Deep Learning Web-Based Tool to Detect COVID-19\n  Infection from Chest CT-Scan", "comments": "9 pages, 8 figures, Improved English writing in the abstract,\n  introduction and conclusion sections. Removed DenseNet typos underneath\n  figures. Fixed some other minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The corona virus is already spread around the world in many countries, and it\nhas taken many lives. Furthermore, the world health organization (WHO) has\nannounced that COVID-19 has reached the global epidemic stage. Early and\nreliable diagnosis using chest CT-scan can assist medical specialists in vital\ncircumstances. In this work, we introduce a computer aided diagnosis (CAD) web\nservice to detect COVID- 19 online. One of the largest public chest CT-scan\ndatabases, containing 746 participants was used in this experiment. A number of\nwell-known deep neural network architectures consisting of ResNet, Inception\nand MobileNet were inspected to find the most efficient model for the hybrid\nsystem. A combination of the Densely connected convolutional network (DenseNet)\nin order to reduce image dimensions and Nu-SVM as an anti-overfitting\nbottleneck was chosen to distinguish between COVID-19 and healthy controls. The\nproposed methodology achieved 90.80% recall, 89.76% precision and 90.61%\naccuracy. The method also yields an AUC of 95.05%. Ultimately a flask web\nservice is made public through ngrok using the trained models to provide a\nRESTful COVID-19 detector, which takes only 39 milliseconds to process one\nimage. The source code is also available at\nhttps://github.com/KiLJ4EdeN/COVID_WEB. Based on the findings, it can be\ninferred that it is feasible to use the proposed technique as an automated tool\nfor diagnosis of COVID-19.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 13:47:54 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 13:26:14 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Saeedi", "Abdolkarim", ""], ["Saeedi", "Maryam", ""], ["Maghsoudi", "Arash", ""]]}, {"id": "2006.14435", "submitter": "Wenbin Gao", "authors": "Wenbin Gao, Lei Zhang, Qi Teng, Jun He, Hao Wu", "title": "DanHAR: Dual Attention Network For Multimodal Human Activity Recognition\n  Using Wearable Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition (HAR) in ubiquitous computing has been beginning\nto incorporate attention into the context of deep neural networks (DNNs), in\nwhich the rich sensing data from multimodal sensors such as accelerometer and\ngyroscope is used to infer human activities. Recently, two attention methods\nare proposed via combining with Gated Recurrent Units (GRU) and Long Short-Term\nMemory (LSTM) network, which can capture the dependencies of sensing signals in\nboth spatial and temporal domains simultaneously. However, recurrent networks\noften have a weak feature representing power compared with convolutional neural\nnetworks (CNNs). On the other hand, two attention, i.e., hard attention and\nsoft attention, are applied in temporal domains via combining with CNN, which\npay more attention to the target activity from a long sequence. However, they\ncan only tell where to focus and miss channel information, which plays an\nimportant role in deciding what to focus. As a result, they fail to address the\nspatial-temporal dependencies of multimodal sensing signals, compared with\nattention-based GRU or LSTM. In the paper, we propose a novel dual attention\nmethod called DanHAR, which introduces the framework of blending channel\nattention and temporal attention on a CNN, demonstrating superiority in\nimproving the comprehensibility for multimodal HAR. Extensive experiments on\nfour public HAR datasets and weakly labeled dataset show that DanHAR achieves\nstate-of-the-art performance with negligible overhead of parameters.\nFurthermore, visualizing analysis is provided to show that our attention can\namplifies more important sensor modalities and timesteps during classification,\nwhich agrees well with human common intuition.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 14:17:33 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 07:54:58 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 01:33:26 GMT"}, {"version": "v4", "created": "Wed, 21 Jul 2021 08:21:37 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Gao", "Wenbin", ""], ["Zhang", "Lei", ""], ["Teng", "Qi", ""], ["He", "Jun", ""], ["Wu", "Hao", ""]]}, {"id": "2006.14470", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Farhad Pourkamali-Anaraki", "title": "Scalable Spectral Clustering with Nystrom Approximation: Practical and\n  Theoretical Aspects", "comments": "Published in IEEE Open Journal of Signal Processing", "journal-ref": "in IEEE Open Journal of Signal Processing, vol. 1, pp. 242-256,\n  2020", "doi": "10.1109/OJSP.2020.3039330", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spectral clustering techniques are valuable tools in signal processing and\nmachine learning for partitioning complex data sets. The effectiveness of\nspectral clustering stems from constructing a non-linear embedding based on\ncreating a similarity graph and computing the spectral decomposition of the\nLaplacian matrix. However, spectral clustering methods fail to scale to large\ndata sets because of high computational cost and memory usage. A popular\napproach for addressing these problems utilizes the Nystrom method, an\nefficient sampling-based algorithm for computing low-rank approximations to\nlarge positive semi-definite matrices. This paper demonstrates how the\npreviously popular approach of Nystrom-based spectral clustering has severe\nlimitations. Existing time-efficient methods ignore critical information by\nprematurely reducing the rank of the similarity matrix associated with sampled\npoints. Also, current understanding is limited regarding how utilizing the\nNystrom approximation will affect the quality of spectral embedding\napproximations. To address the limitations, this work presents a principled\nspectral clustering algorithm that exploits spectral properties of the\nsimilarity matrix associated with sampled points to regulate\naccuracy-efficiency trade-offs. We provide theoretical results to reduce the\ncurrent gap and present numerical experiments with real and synthetic data.\nEmpirical results demonstrate the efficacy and efficiency of the proposed\nmethod compared to existing spectral clustering techniques based on the Nystrom\nmethod and other efficient methods. The overarching goal of this work is to\nprovide an improved baseline for future research directions to accelerate\nspectral clustering.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 15:10:56 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 02:10:30 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Pourkamali-Anaraki", "Farhad", ""]]}, {"id": "2006.14480", "submitter": "Peter Ondruska", "authors": "John Houston, Guido Zuidhof, Luca Bergamini, Yawei Ye, Long Chen,\n  Ashesh Jain, Sammy Omari, Vladimir Iglovikov, Peter Ondruska", "title": "One Thousand and One Hours: Self-driving Motion Prediction Dataset", "comments": "Presente at CoRL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the impact of large-scale datasets on ML systems we present the\nlargest self-driving dataset for motion prediction to date, containing over\n1,000 hours of data. This was collected by a fleet of 20 autonomous vehicles\nalong a fixed route in Palo Alto, California, over a four-month period. It\nconsists of 170,000 scenes, where each scene is 25 seconds long and captures\nthe perception output of the self-driving system, which encodes the precise\npositions and motions of nearby vehicles, cyclists, and pedestrians over time.\nOn top of this, the dataset contains a high-definition semantic map with 15,242\nlabelled elements and a high-definition aerial view over the area. We show that\nusing a dataset of this size dramatically improves performance for key\nself-driving problems. Combined with the provided software kit, this collection\nforms the largest and most detailed dataset to date for the development of\nself-driving machine learning tasks, such as motion forecasting, motion\nplanning and simulation. The full dataset is available at\nhttp://level5.lyft.com/.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 15:23:41 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 21:16:49 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Houston", "John", ""], ["Zuidhof", "Guido", ""], ["Bergamini", "Luca", ""], ["Ye", "Yawei", ""], ["Chen", "Long", ""], ["Jain", "Ashesh", ""], ["Omari", "Sammy", ""], ["Iglovikov", "Vladimir", ""], ["Ondruska", "Peter", ""]]}, {"id": "2006.14512", "submitter": "Kaizhao Liang", "authors": "Kaizhao Liang, Jacky Y. Zhang, Boxin Wang, Zhuolin Yang, Oluwasanmi\n  Koyejo, Bo Li", "title": "Uncovering the Connections Between Adversarial Transferability and\n  Knowledge Transferability", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Knowledge transferability, or transfer learning, has been widely adopted to\nallow a pre-trained model in the source domain to be effectively adapted to\ndownstream tasks in the target domain. It is thus important to explore and\nunderstand the factors affecting knowledge transferability. In this paper, as\nthe first work, we analyze and demonstrate the connections between knowledge\ntransferability and another important phenomenon--adversarial transferability,\n\\emph{i.e.}, adversarial examples generated against one model can be\ntransferred to attack other models. Our theoretical studies show that\nadversarial transferability indicates knowledge transferability and vice versa.\nMoreover, based on the theoretical insights, we propose two practical\nadversarial transferability metrics to characterize this process, serving as\nbidirectional indicators between adversarial and knowledge transferability. We\nconduct extensive experiments for different scenarios on diverse datasets,\nshowing a positive correlation between adversarial transferability and\nknowledge transferability. Our findings will shed light on future research\nabout effective knowledge transfer learning and adversarial transferability\nanalyses.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 16:04:47 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 19:42:53 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 17:14:16 GMT"}, {"version": "v4", "created": "Thu, 8 Jul 2021 19:17:09 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Liang", "Kaizhao", ""], ["Zhang", "Jacky Y.", ""], ["Wang", "Boxin", ""], ["Yang", "Zhuolin", ""], ["Koyejo", "Oluwasanmi", ""], ["Li", "Bo", ""]]}, {"id": "2006.14536", "submitter": "Cihang Xie", "authors": "Cihang Xie, Mingxing Tan, Boqing Gong, Alan Yuille, Quoc V. Le", "title": "Smooth Adversarial Training", "comments": "tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly believed that networks cannot be both accurate and robust,\nthat gaining robustness means losing accuracy. It is also generally believed\nthat, unless making networks larger, network architectural elements would\notherwise matter little in improving adversarial robustness. Here we present\nevidence to challenge these common beliefs by a careful study about adversarial\ntraining. Our key observation is that the widely-used ReLU activation function\nsignificantly weakens adversarial training due to its non-smooth nature. Hence\nwe propose smooth adversarial training (SAT), in which we replace ReLU with its\nsmooth approximations to strengthen adversarial training. The purpose of smooth\nactivation functions in SAT is to allow it to find harder adversarial examples\nand compute better gradient updates during adversarial training.\n  Compared to standard adversarial training, SAT improves adversarial\nrobustness for \"free\", i.e., no drop in accuracy and no increase in\ncomputational cost. For example, without introducing additional computations,\nSAT significantly enhances ResNet-50's robustness from 33.0% to 42.3%, while\nalso improving accuracy by 0.9% on ImageNet. SAT also works well with larger\nnetworks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6%\nrobustness on ImageNet, outperforming the previous state-of-the-art defense by\n9.5% for accuracy and 11.6% for robustness. Models are available at\nhttps://github.com/cihangxie/SmoothAdversarialTraining.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 16:34:39 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 00:56:58 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Xie", "Cihang", ""], ["Tan", "Mingxing", ""], ["Gong", "Boqing", ""], ["Yuille", "Alan", ""], ["Le", "Quoc V.", ""]]}, {"id": "2006.14547", "submitter": "Armin Hadzic", "authors": "Armin Hadzic, Gordon Christie, Jeffrey Freeman, Amber Dismer, Stevan\n  Bullard, Ashley Greiner, Nathan Jacobs, Ryan Mukherjee", "title": "Estimating Displaced Populations from Overhead", "comments": "Fixed typo in abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep learning approach to perform fine-grained population\nestimation for displacement camps using high-resolution overhead imagery. We\ntrain and evaluate our approach on drone imagery cross-referenced with\npopulation data for refugee camps in Cox's Bazar, Bangladesh in 2018 and 2019.\nOur proposed approach achieves 7.02% mean absolute percent error on sequestered\ncamp imagery. We believe our experiments with real-world displacement camp data\nconstitute an important step towards the development of tools that enable the\nhumanitarian community to effectively and rapidly respond to the global\ndisplacement crisis.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 16:45:11 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 17:41:21 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Hadzic", "Armin", ""], ["Christie", "Gordon", ""], ["Freeman", "Jeffrey", ""], ["Dismer", "Amber", ""], ["Bullard", "Stevan", ""], ["Greiner", "Ashley", ""], ["Jacobs", "Nathan", ""], ["Mukherjee", "Ryan", ""]]}, {"id": "2006.14550", "submitter": "Roberto Henschel", "authors": "Andrea Hornakova, Roberto Henschel, Bodo Rosenhahn, Paul Swoboda", "title": "Lifted Disjoint Paths with Application in Multiple Object Tracking", "comments": "ICML 2020, Codebase available at\n  https://github.com/AndreaHor/LifT_Solver", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extension to the disjoint paths problem in which additional\n\\emph{lifted} edges are introduced to provide path connectivity priors. We call\nthe resulting optimization problem the lifted disjoint paths problem. We show\nthat this problem is NP-hard by reduction from integer multicommodity flow and\n3-SAT. To enable practical global optimization, we propose several classes of\nlinear inequalities that produce a high-quality LP-relaxation. Additionally, we\npropose efficient cutting plane algorithms for separating the proposed linear\ninequalities. The lifted disjoint path problem is a natural model for multiple\nobject tracking and allows an elegant mathematical formulation for long range\ntemporal interactions. Lifted edges help to prevent id switches and to\nre-identify persons. Our lifted disjoint paths tracker achieves nearly optimal\nassignments with respect to input detections. As a consequence, it leads on all\nthree main benchmarks of the MOT challenge, improving significantly over\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 16:49:08 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Hornakova", "Andrea", ""], ["Henschel", "Roberto", ""], ["Rosenhahn", "Bodo", ""], ["Swoboda", "Paul", ""]]}, {"id": "2006.14556", "submitter": "Dumindu Tissera", "authors": "Nadarasar Bahavan, Navaratnarajah Suman, Sulhi Cader, Ruwinda\n  Ranganayake, Damitha Seneviratne, Vinu Maddumage, Gershom Seneviratne,\n  Yasinha Supun, Isuru Wijesiri, Suchitha Dehigaspitiya, Dumindu Tissera,\n  Chamira Edussooriya", "title": "Anomaly Detection using Deep Reconstruction and Forecasting for\n  Autonomous Systems", "comments": "Runners Up - IEEE Signal Processing Cup 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose self-supervised deep algorithms to detect anomalies in\nheterogeneous autonomous systems using frontal camera video and IMU readings.\nGiven that the video and IMU data are not synchronized, each of them are\nanalyzed separately. The vision-based system, which utilizes a conditional GAN,\nanalyzes immediate-past three frames and attempts to predict the next frame.\nThe frame is classified as either an anomalous case or a normal case based on\nthe degree of difference estimated using the prediction error and a threshold.\nThe IMU-based system utilizes two approaches to classify the timestamps; the\nfirst being an LSTM autoencoder which reconstructs three consecutive IMU\nvectors and the second being an LSTM forecaster which is utilized to predict\nthe next vector using the previous three IMU vectors. Based on the\nreconstruction error, the prediction error, and a threshold, the timestamp is\nclassified as either an anomalous case or a normal case. The composition of\nalgorithms won runners up at the IEEE Signal Processing Cup anomaly detection\nchallenge 2020. In the competition dataset of camera frames consisting of both\nnormal and anomalous cases, we achieve a test accuracy of 94% and an F1-score\nof 0.95. Furthermore, we achieve an accuracy of 100% on a test set containing\nnormal IMU data, and an F1-score of 0.98 on the test set of abnormal IMU data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 17:00:01 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Bahavan", "Nadarasar", ""], ["Suman", "Navaratnarajah", ""], ["Cader", "Sulhi", ""], ["Ranganayake", "Ruwinda", ""], ["Seneviratne", "Damitha", ""], ["Maddumage", "Vinu", ""], ["Seneviratne", "Gershom", ""], ["Supun", "Yasinha", ""], ["Wijesiri", "Isuru", ""], ["Dehigaspitiya", "Suchitha", ""], ["Tissera", "Dumindu", ""], ["Edussooriya", "Chamira", ""]]}, {"id": "2006.14563", "submitter": "Yongqiang Dou", "authors": "Yongqiang Dou, Haocheng Yang, Maolin Yang, Yanyan Xu and Dengfeng Ke", "title": "Dynamically Mitigating Data Discrepancy with Balanced Focal Loss for\n  Replay Attack Detection", "comments": "This work has been accepted by the 25th International Conference on\n  Pattern Recognition (ICPR2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It becomes urgent to design effective anti-spoofing algorithms for vulnerable\nautomatic speaker verification systems due to the advancement of high-quality\nplayback devices. Current studies mainly treat anti-spoofing as a binary\nclassification problem between bonafide and spoofed utterances, while lack of\nindistinguishable samples makes it difficult to train a robust spoofing\ndetector. In this paper, we argue that for anti-spoofing, it needs more\nattention for indistinguishable samples over easily-classified ones in the\nmodeling process, to make correct discrimination a top priority. Therefore, to\nmitigate the data discrepancy between training and inference, we propose to\nleverage a balanced focal loss function as the training objective to\ndynamically scale the loss based on the traits of the sample itself. Besides,\nin the experiments, we select three kinds of features that contain both\nmagnitude-based and phase-based information to form complementary and\ninformative features. Experimental results on the ASVspoof2019 dataset\ndemonstrate the superiority of the proposed methods by comparison between our\nsystems and top-performing ones. Systems trained with the balanced focal loss\nperform significantly better than conventional cross-entropy loss. With\ncomplementary features, our fusion system with only three kinds of features\noutperforms other systems containing five or more complex single models by\n22.5% for min-tDCF and 7% for EER, achieving a min-tDCF and an EER of 0.0124\nand 0.55% respectively. Furthermore, we present and discuss the evaluation\nresults on real replay data apart from the simulated ASVspoof2019 data,\nindicating that research for anti-spoofing still has a long way to go.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 17:06:47 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Dou", "Yongqiang", ""], ["Yang", "Haocheng", ""], ["Yang", "Maolin", ""], ["Xu", "Yanyan", ""], ["Ke", "Dengfeng", ""]]}, {"id": "2006.14566", "submitter": "Saad Nadeem", "authors": "Saad Nadeem, Travis Hollmann and Allen Tannenbaum", "title": "Multimarginal Wasserstein Barycenter for Stain Normalization and\n  Augmentation", "comments": "To appear in MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variations in hematoxylin and eosin (H&E) stained images (due to clinical lab\nprotocols, scanners, etc) directly impact the quality and accuracy of clinical\ndiagnosis, and hence it is important to control for these variations for a\nreliable diagnosis. In this work, we present a new approach based on the\nmultimarginal Wasserstein barycenter to normalize and augment H&E stained\nimages given one or more references. Specifically, we provide a mathematically\nrobust way of naturally incorporating additional images as intermediate\nreferences to drive stain normalization and augmentation simultaneously. The\npresented approach showed superior results quantitatively and qualitatively as\ncompared to state-of-the-art methods for stain normalization. We further\nvalidated our stain normalization and augmentations in the nuclei segmentation\ntask on a publicly available dataset, achieving state-of-the-art results\nagainst competing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 17:09:40 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Nadeem", "Saad", ""], ["Hollmann", "Travis", ""], ["Tannenbaum", "Allen", ""]]}, {"id": "2006.14580", "submitter": "Emily Wenger", "authors": "Emily Wenger, Josephine Passananti, Arjun Bhagoji, Yuanshun Yao,\n  Haitao Zheng, Ben Y. Zhao", "title": "Backdoor Attacks Against Deep Learning Systems in the Physical World", "comments": "Accepted to the 2021 Conference on Computer Vision and Pattern\n  Recognition (CVPR 2021); 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Backdoor attacks embed hidden malicious behaviors into deep learning models,\nwhich only activate and cause misclassifications on model inputs containing a\nspecific trigger. Existing works on backdoor attacks and defenses, however,\nmostly focus on digital attacks that use digitally generated patterns as\ntriggers. A critical question remains unanswered: can backdoor attacks succeed\nusing physical objects as triggers, thus making them a credible threat against\ndeep learning systems in the real world? We conduct a detailed empirical study\nto explore this question for facial recognition, a critical deep learning task.\nUsing seven physical objects as triggers, we collect a custom dataset of 3205\nimages of ten volunteers and use it to study the feasibility of physical\nbackdoor attacks under a variety of real-world conditions. Our study reveals\ntwo key findings. First, physical backdoor attacks can be highly successful if\nthey are carefully configured to overcome the constraints imposed by physical\nobjects. In particular, the placement of successful triggers is largely\nconstrained by the target model's dependence on key facial features. Second,\nfour of today's state-of-the-art defenses against (digital) backdoors are\nineffective against physical backdoors, because the use of physical objects\nbreaks core assumptions used to construct these defenses. Our study confirms\nthat (physical) backdoor attacks are not a hypothetical phenomenon but rather\npose a serious real-world threat to critical classification tasks. We need new\nand more robust defenses against backdoors in the physical world.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 17:26:20 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 22:12:56 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 16:41:55 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Wenger", "Emily", ""], ["Passananti", "Josephine", ""], ["Bhagoji", "Arjun", ""], ["Yao", "Yuanshun", ""], ["Zheng", "Haitao", ""], ["Zhao", "Ben Y.", ""]]}, {"id": "2006.14582", "submitter": "Xianhang Li", "authors": "Xianhang Li, Yali Wang, Zhipeng Zhou, Yu Qiao", "title": "SmallBigNet: Integrating Core and Contextual Views for Video\n  Classification", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal convolution has been widely used for video classification. However,\nit is performed on spatio-temporal contexts in a limited view, which often\nweakens its capacity of learning video representation. To alleviate this\nproblem, we propose a concise and novel SmallBig network, with the cooperation\nof small and big views. For the current time step, the small view branch is\nused to learn the core semantics, while the big view branch is used to capture\nthe contextual semantics. Unlike traditional temporal convolution, the big view\nbranch can provide the small view branch with the most activated video features\nfrom a broader 3D receptive field. Via aggregating such big-view contexts, the\nsmall view branch can learn more robust and discriminative spatio-temporal\nrepresentations for video classification. Furthermore, we propose to share\nconvolution in the small and big view branch, which improves model compactness\nas well as alleviates overfitting. As a result, our SmallBigNet achieves a\ncomparable model size like 2D CNNs, while boosting accuracy like 3D CNNs. We\nconduct extensive experiments on the large-scale video benchmarks, e.g.,\nKinetics400, Something-Something V1 and V2. Our SmallBig network outperforms a\nnumber of recent state-of-the-art approaches, in terms of accuracy and/or\nefficiency. The codes and models will be available on\nhttps://github.com/xhl-video/SmallBigNet.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 17:29:57 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Li", "Xianhang", ""], ["Wang", "Yali", ""], ["Zhou", "Zhipeng", ""], ["Qiao", "Yu", ""]]}, {"id": "2006.14610", "submitter": "Yuval Atzmon", "authors": "Yuval Atzmon, Felix Kreuk, Uri Shalit, Gal Chechik", "title": "A causal view of compositional zero-shot recognition", "comments": "(1) Accepted to NeurIPS 2020 (Spotlight) (2) Project page is at\n  https://github.com/nv-research-israel/causal_comp (3) A video of our\n  spotlight talk is at https://www.youtube.com/watch?v=IUAmwBylvyc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People easily recognize new visual categories that are new combinations of\nknown components. This compositional generalization capacity is critical for\nlearning in real-world domains like vision and language because the long tail\nof new combinations dominates the distribution. Unfortunately, learning systems\nstruggle with compositional generalization because they often build on features\nthat are correlated with class labels even if they are not \"essential\" for the\nclass. This leads to consistent misclassification of samples from a new\ndistribution, like new combinations of known components.\n  Here we describe an approach for compositional generalization that builds on\ncausal ideas. First, we describe compositional zero-shot learning from a causal\nperspective, and propose to view zero-shot inference as finding \"which\nintervention caused the image?\". Second, we present a causal-inspired embedding\nmodel that learns disentangled representations of elementary components of\nvisual objects from correlated (confounded) training data. We evaluate this\napproach on two datasets for predicting new combinations of attribute-object\npairs: A well-controlled synthesized images dataset and a real-world dataset\nwhich consists of fine-grained types of shoes. We show improvements compared to\nstrong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 17:51:22 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 17:26:29 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Atzmon", "Yuval", ""], ["Kreuk", "Felix", ""], ["Shalit", "Uri", ""], ["Chechik", "Gal", ""]]}, {"id": "2006.14611", "submitter": "Zhenfeng Xue", "authors": "Zhenfeng Xue, Weijie Mao, Liang Zheng", "title": "Learning to simulate complex scenes", "comments": "13 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data simulation engines like Unity are becoming an increasingly important\ndata source that allows us to acquire ground truth labels conveniently.\nMoreover, we can flexibly edit the content of an image in the engine, such as\nobjects (position, orientation) and environments (illumination, occlusion).\nWhen using simulated data as training sets, its editable content can be\nleveraged to mimic the distribution of real-world data, and thus reduce the\ncontent difference between the synthetic and real domains. This paper explores\ncontent adaptation in the context of semantic segmentation, where the complex\nstreet scenes are fully synthesized using 19 classes of virtual objects from a\nfirst person driver perspective and controlled by 23 attributes. To optimize\nthe attribute values and obtain a training set of similar content to real-world\ndata, we propose a scalable discretization-and-relaxation (SDR) approach. Under\na reinforcement learning framework, we formulate attribute optimization as a\nrandom-to-optimized mapping problem using a neural network. Our method has\nthree characteristics. 1) Instead of editing attributes of individual objects,\nwe focus on global attributes that have large influence on the scene structure,\nsuch as object density and illumination. 2) Attributes are quantized to\ndiscrete values, so as to reduce search space and training complexity. 3)\nCorrelated attributes are jointly optimized in a group, so as to avoid\nmeaningless scene structures and find better convergence points. Experiment\nshows our system can generate reasonable and useful scenes, from which we\nobtain promising real-world segmentation accuracy compared with existing\nsynthetic training sets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 17:51:34 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Xue", "Zhenfeng", ""], ["Mao", "Weijie", ""], ["Zheng", "Liang", ""]]}, {"id": "2006.14613", "submitter": "Allan Jabri", "authors": "Allan Jabri, Andrew Owens, Alexei A. Efros", "title": "Space-Time Correspondence as a Contrastive Random Walk", "comments": "NeurIPS 2020 camera ready version -- Code at\n  github.com/ajabri/videowalk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple self-supervised approach for learning a\nrepresentation for visual correspondence from raw video. We cast correspondence\nas prediction of links in a space-time graph constructed from video. In this\ngraph, the nodes are patches sampled from each frame, and nodes adjacent in\ntime can share a directed edge. We learn a representation in which pairwise\nsimilarity defines transition probability of a random walk, so that long-range\ncorrespondence is computed as a walk along the graph. We optimize the\nrepresentation to place high probability along paths of similarity. Targets for\nlearning are formed without supervision, by cycle-consistency: the objective is\nto maximize the likelihood of returning to the initial node when walking along\na graph constructed from a palindrome of frames. Thus, a single path-level\nconstraint implicitly supervises chains of intermediate comparisons. When used\nas a similarity metric without adaptation, the learned representation\noutperforms the self-supervised state-of-the-art on label propagation tasks\ninvolving objects, semantic parts, and pose. Moreover, we demonstrate that a\ntechnique we call edge dropout, as well as self-supervised adaptation at\ntest-time, further improve transfer for object-centric correspondence.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 17:56:05 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 18:59:03 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Jabri", "Allan", ""], ["Owens", "Andrew", ""], ["Efros", "Alexei A.", ""]]}, {"id": "2006.14615", "submitter": "Kamal Gupta", "authors": "Kamal Gupta, Alessandro Achille, Justin Lazarow, Larry Davis, Vijay\n  Mahadevan, Abhinav Shrivastava", "title": "Layout Generation and Completion with Self-attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of layout generation for diverse domains such as\nimages, documents, and mobile applications. A layout is a set of graphical\nelements, belonging to one or more categories, placed together in a meaningful\nway. Generating a new layout or extending an existing layout requires\nunderstanding the relationships between these graphical elements. To do this,\nwe propose a novel framework, LayoutTransformer, that leverages a\nself-attention based approach to learn contextual relationships between layout\nelements and generate layouts in a given domain. The proposed model improves\nupon the state-of-the-art approaches in layout generation in four ways. First,\nour model can generate a new layout either from an empty set or add more\nelements to a partial layout starting from an initial set of elements. Second,\nas the approach is attention-based, we can visualize which previous elements\nthe model is attending to predict the next element, thereby providing an\ninterpretable sequence of layout elements. Third, our model can easily scale to\nsupport both a large number of element categories and a large number of\nelements per layout. Finally, the model also produces an embedding for various\nelement categories, which can be used to explore the relationships between the\ncategories. We demonstrate with experiments that our model can produce\nmeaningful layouts in diverse settings such as object bounding boxes in scenes\n(COCO bounding boxes), documents (PubLayNet), and mobile applications (RICO\ndataset).\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 17:56:34 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Gupta", "Kamal", ""], ["Achille", "Alessandro", ""], ["Lazarow", "Justin", ""], ["Davis", "Larry", ""], ["Mahadevan", "Vijay", ""], ["Shrivastava", "Abhinav", ""]]}, {"id": "2006.14616", "submitter": "Ameesh Makadia", "authors": "Jake Levinson, Carlos Esteves, Kefan Chen, Noah Snavely, Angjoo\n  Kanazawa, Afshin Rostamizadeh, Ameesh Makadia", "title": "An Analysis of SVD for Deep Rotation Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric orthogonalization via SVD, and closely related procedures, are\nwell-known techniques for projecting matrices onto $O(n)$ or $SO(n)$. These\ntools have long been used for applications in computer vision, for example\noptimal 3D alignment problems solved by orthogonal Procrustes, rotation\naveraging, or Essential matrix decomposition. Despite its utility in different\nsettings, SVD orthogonalization as a procedure for producing rotation matrices\nis typically overlooked in deep learning models, where the preferences tend\ntoward classic representations like unit quaternions, Euler angles, and\naxis-angle, or more recently-introduced methods. Despite the importance of 3D\nrotations in computer vision and robotics, a single universally effective\nrepresentation is still missing. Here, we explore the viability of SVD\northogonalization for 3D rotations in neural networks. We present a theoretical\nanalysis that shows SVD is the natural choice for projecting onto the rotation\ngroup. Our extensive quantitative analysis shows simply replacing existing\nrepresentations with the SVD orthogonalization procedure obtains state of the\nart performance in many deep learning applications covering both supervised and\nunsupervised training.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 17:58:28 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Levinson", "Jake", ""], ["Esteves", "Carlos", ""], ["Chen", "Kefan", ""], ["Snavely", "Noah", ""], ["Kanazawa", "Angjoo", ""], ["Rostamizadeh", "Afshin", ""], ["Makadia", "Ameesh", ""]]}, {"id": "2006.14618", "submitter": "Yue Cao", "authors": "Yue Cao, Zhenda Xie, Bin Liu, Yutong Lin, Zheng Zhang, Han Hu", "title": "Parametric Instance Classification for Unsupervised Visual Feature\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents parametric instance classification (PIC) for unsupervised\nvisual feature learning. Unlike the state-of-the-art approaches which do\ninstance discrimination in a dual-branch non-parametric fashion, PIC directly\nperforms a one-branch parametric instance classification, revealing a simple\nframework similar to supervised classification and without the need to address\nthe information leakage issue. We show that the simple PIC framework can be as\neffective as the state-of-the-art approaches, i.e. SimCLR and MoCo v2, by\nadapting several common component settings used in the state-of-the-art\napproaches. We also propose two novel techniques to further improve\neffectiveness and practicality of PIC: 1) a sliding-window data scheduler,\ninstead of the previous epoch-based data scheduler, which addresses the\nextremely infrequent instance visiting issue in PIC and improves the\neffectiveness; 2) a negative sampling and weight update correction approach to\nreduce the training time and GPU memory consumption, which also enables\napplication of PIC to almost unlimited training images. We hope that the PIC\nframework can serve as a simple baseline to facilitate future study.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 17:59:13 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Cao", "Yue", ""], ["Xie", "Zhenda", ""], ["Liu", "Bin", ""], ["Lin", "Yutong", ""], ["Zhang", "Zheng", ""], ["Hu", "Han", ""]]}, {"id": "2006.14644", "submitter": "Vibhor Singh", "authors": "Vibhor Singh, Vishesh Devgan, Ishu Anand", "title": "Determining Image similarity with Quasi-Euclidean Metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image similarity is a core concept in Image Analysis due to its extensive\napplication in computer vision, image processing, and pattern recognition. The\nobjective of our study is to evaluate Quasi-Euclidean metric as an image\nsimilarity measure and analyze how it fares against the existing standard ways\nlike SSIM and Euclidean metric. In this paper, we analyzed the similarity\nbetween two images from our own novice dataset and assessed its performance\nagainst the Euclidean distance metric and SSIM. We also present experimental\nresults along with evidence indicating that our proposed implementation when\napplied to our novice dataset, furnished different results than standard\nmetrics in terms of effectiveness and accuracy. In some cases, our methodology\nprojected remarkable performance and it is also interesting to note that our\nimplementation proves to be a step ahead in recognizing similarity when\ncompared to\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 18:12:21 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Singh", "Vibhor", ""], ["Devgan", "Vishesh", ""], ["Anand", "Ishu", ""]]}, {"id": "2006.14655", "submitter": "Tianlong Chen", "authors": "Yi Wang, Jingyang Zhou, Tianlong Chen, Sijia Liu, Shiyu Chang,\n  Chandrajit Bajaj, Zhangyang Wang", "title": "Can 3D Adversarial Logos Cloak Humans?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the trend of adversarial attacks, researchers attempt to fool trained\nobject detectors in 2D scenes. Among many of them, an intriguing new form of\nattack with potential real-world usage is to append adversarial patches (e.g.\nlogos) to images. Nevertheless, much less have we known about adversarial\nattacks from 3D rendering views, which is essential for the attack to be\npersistently strong in the physical world. This paper presents a new 3D\nadversarial logo attack: we construct an arbitrary shape logo from a 2D texture\nimage and map this image into a 3D adversarial logo via a texture mapping\ncalled logo transformation. The resulting 3D adversarial logo is then viewed as\nan adversarial texture enabling easy manipulation of its shape and position.\nThis greatly extends the versatility of adversarial training for computer\ngraphics synthesized imagery. Contrary to the traditional adversarial patch,\nthis new form of attack is mapped into the 3D object world and back-propagates\nto the 2D image domain through differentiable rendering. In addition, and\nunlike existing adversarial patches, our new 3D adversarial logo is shown to\nfool state-of-the-art deep object detectors robustly under model rotations,\nleading to one step further for realistic attacks in the physical world. Our\ncodes are available at https://github.com/TAMU-VITA/3D_Adversarial_Logo.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 18:34:33 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 07:18:55 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Wang", "Yi", ""], ["Zhou", "Jingyang", ""], ["Chen", "Tianlong", ""], ["Liu", "Sijia", ""], ["Chang", "Shiyu", ""], ["Bajaj", "Chandrajit", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2006.14660", "submitter": "Angela Dai", "authors": "Angela Dai, Yawar Siddiqui, Justus Thies, Julien Valentin, Matthias\n  Nie{\\ss}ner", "title": "SPSG: Self-Supervised Photometric Scene Generation from RGB-D Scans", "comments": "Video: https://youtu.be/1cj962m9zqo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SPSG, a novel approach to generate high-quality, colored 3D models\nof scenes from RGB-D scan observations by learning to infer unobserved scene\ngeometry and color in a self-supervised fashion. Our self-supervised approach\nlearns to jointly inpaint geometry and color by correlating an incomplete RGB-D\nscan with a more complete version of that scan. Notably, rather than relying on\n3D reconstruction losses to inform our 3D geometry and color reconstruction, we\npropose adversarial and perceptual losses operating on 2D renderings in order\nto achieve high-resolution, high-quality colored reconstructions of scenes.\nThis exploits the high-resolution, self-consistent signal from individual raw\nRGB-D frames, in contrast to fused 3D reconstructions of the frames which\nexhibit inconsistencies from view-dependent effects, such as color balancing or\npose inconsistencies. Thus, by informing our 3D scene generation directly\nthrough 2D signal, we produce high-quality colored reconstructions of 3D\nscenes, outperforming state of the art on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 18:58:23 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 15:15:45 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Dai", "Angela", ""], ["Siddiqui", "Yawar", ""], ["Thies", "Justus", ""], ["Valentin", "Julien", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2006.14673", "submitter": "Hugo Oliveira", "authors": "Hugo Oliveira, Caio Silva, Gabriel L. S. Machado, Keiller Nogueira,\n  Jefersson A. dos Santos", "title": "Fully Convolutional Open Set Segmentation", "comments": "Submitted to the Machine Learning Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In semantic segmentation knowing about all existing classes is essential to\nyield effective results with the majority of existing approaches. However,\nthese methods trained in a Closed Set of classes fail when new classes are\nfound in the test phase. It means that they are not suitable for Open Set\nscenarios, which are very common in real-world computer vision and remote\nsensing applications. In this paper, we discuss the limitations of Closed Set\nsegmentation and propose two fully convolutional approaches to effectively\naddress Open Set semantic segmentation: OpenFCN and OpenPCS. OpenFCN is based\non the well-known OpenMax algorithm, configuring a new application of this\napproach in segmentation settings. OpenPCS is a fully novel approach based on\nfeature-space from DNN activations that serve as features for computing PCA and\nmulti-variate gaussian likelihood in a lower dimensional space. Experiments\nwere conducted on the well-known Vaihingen and Potsdam segmentation datasets.\nOpenFCN showed little-to-no improvement when compared to the simpler and much\nmore time efficient SoftMax thresholding, while being between some orders of\nmagnitude slower. OpenPCS achieved promising results in almost all experiments\nby overcoming both OpenFCN and SoftMax thresholding. OpenPCS is also a\nreasonable compromise between the runtime performances of the extremely fast\nSoftMax thresholding and the extremely slow OpenFCN, being close able to run\nclose to real-time. Experiments also indicate that OpenPCS is effective, robust\nand suitable for Open Set segmentation, being able to improve the recognition\nof unknown class pixels without reducing the accuracy on the known class\npixels.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 19:40:20 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Oliveira", "Hugo", ""], ["Silva", "Caio", ""], ["Machado", "Gabriel L. S.", ""], ["Nogueira", "Keiller", ""], ["Santos", "Jefersson A. dos", ""]]}, {"id": "2006.14691", "submitter": "Ilya Chugunov", "authors": "Ilya Chugunov and Avideh Zakhor", "title": "Duodepth: Static Gesture Recognition Via Dual Depth Sensors", "comments": "26th International Conference on Image Processing", "journal-ref": "2019 IEEE International Conference on Image Processing (ICIP),\n  Taipei, Taiwan, 2019, pp. 3467-3471", "doi": "10.1109/ICIP.2019.8803665", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static gesture recognition is an effective non-verbal communication channel\nbetween a user and their devices; however many modern methods are sensitive to\nthe relative pose of the user's hands with respect to the capture device, as\nparts of the gesture can become occluded. We present two methodologies for\ngesture recognition via synchronized recording from two depth cameras to\nalleviate this occlusion problem. One is a more classic approach using\niterative closest point registration to accurately fuse point clouds and a\nsingle PointNet architecture for classification, and the other is a dual\nPoint-Net architecture for classification without registration. On a manually\ncollected data-set of 20,100 point clouds we show a 39.2% reduction in\nmisclassification for the fused point cloud method, and 53.4% for the dual\nPointNet, when compared to a standard single camera pipeline.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 20:41:47 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Chugunov", "Ilya", ""], ["Zakhor", "Avideh", ""]]}, {"id": "2006.14693", "submitter": "Istvan Fehervari", "authors": "Istvan Fehervari and Ives Macedo", "title": "Adaptive additive classification-based loss for deep metric learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that deep metric learning algorithms can benefit from\nweak supervision from another input modality. This additional modality can be\nincorporated directly into the popular triplet-based loss function as\ndistances. Also recently, classification loss and proxy-based metric learning\nhave been observed to lead to faster convergence as well as better retrieval\nresults, all the while without requiring complex and costly sampling\nstrategies. In this paper we propose an extension to the existing adaptive\nmargin for classification-based deep metric learning. Our extension introduces\na separate margin for each negative proxy per sample. These margins are\ncomputed during training from precomputed distances of the classes in the other\nmodality. Our results set a new state-of-the-art on both on the Amazon fashion\nretrieval dataset as well as on the public DeepFashion dataset. This was\nobserved with both fastText- and BERT-based embeddings for the additional\ntextual modality. Our results were achieved with faster convergence and lower\ncode complexity than the prior state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 20:45:22 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Fehervari", "Istvan", ""], ["Macedo", "Ives", ""]]}, {"id": "2006.14699", "submitter": "Saypraseuth Mounsaveng", "authors": "Saypraseuth Mounsaveng, Issam Laradji, Ismail Ben Ayed, David Vazquez,\n  Marco Pedersoli", "title": "Learning Data Augmentation with Online Bilevel Optimization for Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a key practice in machine learning for improving\ngeneralization performance. However, finding the best data augmentation\nhyperparameters requires domain knowledge or a computationally demanding\nsearch. We address this issue by proposing an efficient approach to\nautomatically train a network that learns an effective distribution of\ntransformations to improve its generalization. Using bilevel optimization, we\ndirectly optimize the data augmentation parameters using a validation set. This\nframework can be used as a general solution to learn the optimal data\naugmentation jointly with an end task model like a classifier. Results show\nthat our joint training method produces an image classification accuracy that\nis comparable to or better than carefully hand-crafted data augmentation. Yet,\nit does not need an expensive external validation loop on the data augmentation\nhyperparameters.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 21:01:52 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 16:11:57 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Mounsaveng", "Saypraseuth", ""], ["Laradji", "Issam", ""], ["Ayed", "Ismail Ben", ""], ["Vazquez", "David", ""], ["Pedersoli", "Marco", ""]]}, {"id": "2006.14702", "submitter": "Hongxu Yang", "authors": "Hongxu Yang, Caifeng Shan, Alexander F. Kolen, Peter H. N. de With", "title": "Deep Q-Network-Driven Catheter Segmentation in 3D US by Hybrid\n  Constrained Semi-Supervised Learning and Dual-UNet", "comments": "Accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catheter segmentation in 3D ultrasound is important for computer-assisted\ncardiac intervention. However, a large amount of labeled images are required to\ntrain a successful deep convolutional neural network (CNN) to segment the\ncatheter, which is expensive and time-consuming. In this paper, we propose a\nnovel catheter segmentation approach, which requests fewer annotations than the\nsupervised learning method, but nevertheless achieves better performance. Our\nscheme considers a deep Q learning as the pre-localization step, which avoids\nvoxel-level annotation and which can efficiently localize the target catheter.\nWith the detected catheter, patch-based Dual-UNet is applied to segment the\ncatheter in 3D volumetric data. To train the Dual-UNet with limited labeled\nimages and leverage information of unlabeled images, we propose a novel\nsemi-supervised scheme, which exploits unlabeled images based on hybrid\nconstraints from predictions. Experiments show the proposed scheme achieves a\nhigher performance than state-of-the-art semi-supervised methods, while it\ndemonstrates that our method is able to learn from large-scale unlabeled\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 21:10:04 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Yang", "Hongxu", ""], ["Shan", "Caifeng", ""], ["Kolen", "Alexander F.", ""], ["de With", "Peter H. N.", ""]]}, {"id": "2006.14708", "submitter": "Jiayuan Mao", "authors": "Yikai Li, Jiayuan Mao, Xiuming Zhang, William T. Freeman, Joshua B.\n  Tenenbaum, Jiajun Wu", "title": "Perspective Plane Program Induction from a Single Image", "comments": "CVPR 2020. First two authors contributed equally. Project page:\n  http://p3i.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the inverse graphics problem of inferring a holistic representation\nfor natural images. Given an input image, our goal is to induce a\nneuro-symbolic, program-like representation that jointly models camera poses,\nobject locations, and global scene structures. Such high-level, holistic scene\nrepresentations further facilitate low-level image manipulation tasks such as\ninpainting. We formulate this problem as jointly finding the camera pose and\nscene structure that best describe the input image. The benefits of such joint\ninference are two-fold: scene regularity serves as a new cue for perspective\ncorrection, and in turn, correct perspective correction leads to a simplified\nscene structure, similar to how the correct shape leads to the most regular\ntexture in shape from texture. Our proposed framework, Perspective Plane\nProgram Induction (P3I), combines search-based and gradient-based algorithms to\nefficiently solve the problem. P3I outperforms a set of baselines on a\ncollection of Internet images, across tasks including camera pose estimation,\nglobal structure inference, and down-stream image manipulation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 21:18:58 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Li", "Yikai", ""], ["Mao", "Jiayuan", ""], ["Zhang", "Xiuming", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""], ["Wu", "Jiajun", ""]]}, {"id": "2006.14715", "submitter": "Amirreza Mahbod", "authors": "Amirreza Mahbod, Gerald Schaefer, Chunliang Wang, Rupert Ecker, Georg\n  Dorffner, Isabella Ellinger", "title": "Investigating and Exploiting Image Resolution for Transfer\n  Learning-based Skin Lesion Classification", "comments": "Accepted for the 25th International Conference on Pattern Recognition\n  (ICPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin cancer is among the most common cancer types. Dermoscopic image analysis\nimproves the diagnostic accuracy for detection of malignant melanoma and other\npigmented skin lesions when compared to unaided visual inspection. Hence,\ncomputer-based methods to support medical experts in the diagnostic procedure\nare of great interest. Fine-tuning pre-trained convolutional neural networks\n(CNNs) has been shown to work well for skin lesion classification. Pre-trained\nCNNs are usually trained with natural images of a fixed image size which is\ntypically significantly smaller than captured skin lesion images and\nconsequently dermoscopic images are downsampled for fine-tuning. However,\nuseful medical information may be lost during this transformation. In this\npaper, we explore the effect of input image size on skin lesion classification\nperformance of fine-tuned CNNs. For this, we resize dermoscopic images to\ndifferent resolutions, ranging from 64x64 to 768x768 pixels and investigate the\nresulting classification performance of three well-established CNNs, namely\nDenseNet-121, ResNet-18, and ResNet-50. Our results show that using very small\nimages (of size 64x64 pixels) degrades the classification performance, while\nimages of size 128x128 pixels and above support good performance with larger\nimage sizes leading to slightly improved classification. We further propose a\nnovel fusion approach based on a three-level ensemble strategy that exploits\nmultiple fine-tuned networks trained with dermoscopic images at various sizes.\nWhen applied on the ISIC 2017 skin lesion classification challenge, our fusion\napproach yields an area under the receiver operating characteristic curve of\n89.2% and 96.6% for melanoma classification and seborrheic keratosis\nclassification, respectively, outperforming state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 21:51:24 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Mahbod", "Amirreza", ""], ["Schaefer", "Gerald", ""], ["Wang", "Chunliang", ""], ["Ecker", "Rupert", ""], ["Dorffner", "Georg", ""], ["Ellinger", "Isabella", ""]]}, {"id": "2006.14722", "submitter": "Satyam Mohla Mr.", "authors": "Satyam Mohla, Anshul Nasery, Biplab Banerjee and Subhasis Chaudhari", "title": "CognitiveCNN: Mimicking Human Cognitive Models to resolve Texture-Shape\n  Bias", "comments": "5 Pages; LaTeX; Published at ICLR 2020 Workshop on Bridging AI and\n  Cognitive Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works demonstrate the texture bias in Convolutional Neural Networks\n(CNNs), conflicting with early works claiming that networks identify objects\nusing shape. It is commonly believed that the cost function forces the network\nto take a greedy route to increase accuracy using texture, failing to explore\nany global statistics. We propose a novel intuitive architecture, namely\nCognitiveCNN, inspired from feature integration theory in psychology to utilise\nhuman-interpretable feature like shape, texture, edges etc. to reconstruct, and\nclassify the image. We define two metrics, namely TIC and RIC to quantify the\nimportance of each stream using attention maps. We introduce a regulariser\nwhich ensures that the contribution of each feature is same for any task, as it\nis for reconstruction; and perform experiments to show the resulting boost in\naccuracy and robustness besides imparting explainability. Lastly, we adapt\nthese ideas to conventional CNNs and propose Augmented Cognitive CNN to achieve\nsuperior performance in object recognition.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 22:32:54 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Mohla", "Satyam", ""], ["Nasery", "Anshul", ""], ["Banerjee", "Biplab", ""], ["Chaudhari", "Subhasis", ""]]}, {"id": "2006.14727", "submitter": "Polina Zablotskaia", "authors": "Polina Zablotskaia, Edoardo A. Dominici, Leonid Sigal, Andreas M.\n  Lehrmann", "title": "Unsupervised Video Decomposition using Spatio-temporal Iterative\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised multi-object scene decomposition is a fast-emerging problem in\nrepresentation learning. Despite significant progress in static scenes, such\nmodels are unable to leverage important dynamic cues present in video. We\npropose a novel spatio-temporal iterative inference framework that is powerful\nenough to jointly model complex multi-object representations and explicit\ntemporal dependencies between latent variables across frames. This is achieved\nby leveraging 2D-LSTM, temporally conditioned inference and generation within\nthe iterative amortized inference for posterior refinement. Our method improves\nthe overall quality of decompositions, encodes information about the objects'\ndynamics, and can be used to predict trajectories of each object separately.\nAdditionally, we show that our model has a high accuracy even without color\ninformation. We demonstrate the decomposition, segmentation, and prediction\ncapabilities of our model and show that it outperforms the state-of-the-art on\nseveral benchmark datasets, one of which was curated for this work and will be\nmade publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 22:57:17 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Zablotskaia", "Polina", ""], ["Dominici", "Edoardo A.", ""], ["Sigal", "Leonid", ""], ["Lehrmann", "Andreas M.", ""]]}, {"id": "2006.14738", "submitter": "Sepehr Ataei", "authors": "Sepehr Ataei, Dr. Javad Alirezaie, Dr. Paul Babyn", "title": "Cascaded Convolutional Neural Networks with Perceptual Loss for Low Dose\n  CT Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low Dose CT Denoising research aims to reduce the risks of radiation exposure\nto patients. Recently researchers have used deep learning to denoise low dose\nCT images with promising results. However, approaches that use\nmean-squared-error (MSE) tend to over smooth the image resulting in loss of\nfine structural details in low contrast regions of the image. These regions are\noften crucial for diagnosis and must be preserved in order for Low dose CT to\nbe used effectively in practice. In this work we use a cascade of two neural\nnetworks, the first of which aims to reconstruct normal dose CT from low dose\nCT by minimizing perceptual loss, and the second which predicts the difference\nbetween the ground truth and prediction from the perceptual loss network. We\nshow that our method outperforms related works and more effectively\nreconstructs fine structural details in low contrast regions of the image.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 00:35:26 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Ataei", "Sepehr", ""], ["Alirezaie", "Dr. Javad", ""], ["Babyn", "Dr. Paul", ""]]}, {"id": "2006.14744", "submitter": "Liqun Chen", "authors": "Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, Jingjing Liu", "title": "Graph Optimal Transport for Cross-Domain Alignment", "comments": null, "journal-ref": "ICML 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain alignment between two sets of entities (e.g., objects in an\nimage, words in a sentence) is fundamental to both computer vision and natural\nlanguage processing. Existing methods mainly focus on designing advanced\nattention mechanisms to simulate soft alignment, with no training signals to\nexplicitly encourage alignment. The learned attention matrices are also dense\nand lacks interpretability. We propose Graph Optimal Transport (GOT), a\nprincipled framework that germinates from recent advances in Optimal Transport\n(OT). In GOT, cross-domain alignment is formulated as a graph matching problem,\nby representing entities into a dynamically-constructed graph. Two types of OT\ndistances are considered: (i) Wasserstein distance (WD) for node (entity)\nmatching; and (ii) Gromov-Wasserstein distance (GWD) for edge (structure)\nmatching. Both WD and GWD can be incorporated into existing neural network\nmodels, effectively acting as a drop-in regularizer. The inferred transport\nplan also yields sparse and self-normalized alignment, enhancing the\ninterpretability of the learned model. Experiments show consistent\noutperformance of GOT over baselines across a wide range of tasks, including\nimage-text retrieval, visual question answering, image captioning, machine\ntranslation, and text summarization.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 01:14:23 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 15:58:36 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 20:04:49 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Chen", "Liqun", ""], ["Gan", "Zhe", ""], ["Cheng", "Yu", ""], ["Li", "Linjie", ""], ["Carin", "Lawrence", ""], ["Liu", "Jingjing", ""]]}, {"id": "2006.14745", "submitter": "Laura Domin\\'e", "authors": "Laura Domin\\'e, Pierre C\\^ote de Soux, Fran\\c{c}ois Drielsma, Dae Heun\n  Koh, Ran Itay, Qing Lin, Kazuhiro Terao, Ka Vang Tsang, Tracy L. Usher", "title": "Point Proposal Network for Reconstructing 3D Particle Endpoints with\n  Sub-Pixel Precision in Liquid Argon Time Projection Chambers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex cs.CV physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liquid Argon Time Projection Chambers (LArTPC) are particle imaging detectors\nrecording 2D or 3D images of trajectories of charged particles. Identifying\npoints of interest in these images, namely the initial and terminal points of\ntrack-like particle trajectories such as muons and protons, and the initial\npoints of electromagnetic shower-like particle trajectories such as electrons\nand gamma rays, is a crucial step of identifying and analyzing these particles\nand impacts the inference of physics signals such as neutrino interaction. The\nPoint Proposal Network is designed to discover these specific points of\ninterest. The algorithm predicts with a sub-voxel precision their spatial\nlocation, and also determines the category of the identified points of\ninterest. Using as a benchmark the PILArNet public LArTPC data sample in which\nthe voxel resolution is 3mm/voxel, our algorithm successfully predicted 96.8%\nand 97.8% of 3D points within a distance of 3 and 10~voxels from the provided\ntrue point locations respectively. For the predicted 3D points within 3 voxels\nof the closest true point locations, the median distance is found to be 0.25\nvoxels, achieving the sub-voxel level precision. In addition, we report our\nanalysis of the mistakes where our algorithm prediction differs from the\nprovided true point positions by more than 10~voxels. Among 50 mistakes\nvisually scanned, 25 were due to the definition of true position location, 15\nwere legitimate mistakes where a physicist cannot visually disagree with the\nalgorithm's prediction, and 10 were genuine mistakes that we wish to improve in\nthe future. Further, using these predicted points, we demonstrate a simple\nalgorithm to cluster 3D voxels into individual track-like particle trajectories\nwith a clustering efficiency, purity, and Adjusted Rand Index of 96%, 93%, and\n91% respectively.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 01:18:43 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 17:47:41 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 15:30:44 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Domin\u00e9", "Laura", ""], ["de Soux", "Pierre C\u00f4te", ""], ["Drielsma", "Fran\u00e7ois", ""], ["Koh", "Dae Heun", ""], ["Itay", "Ran", ""], ["Lin", "Qing", ""], ["Terao", "Kazuhiro", ""], ["Tsang", "Ka Vang", ""], ["Usher", "Tracy L.", ""]]}, {"id": "2006.14749", "submitter": "Oscar De Lima", "authors": "Oscar de Lima, Sean Franklin, Shreshtha Basu, Blake Karwoski, Annet\n  George", "title": "Deepfake Detection using Spatiotemporal Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Better generative models and larger datasets have led to more realistic fake\nvideos that can fool the human eye but produce temporal and spatial artifacts\nthat deep learning approaches can detect. Most current Deepfake detection\nmethods only use individual video frames and therefore fail to learn from\ntemporal information. We created a benchmark of the performance of\nspatiotemporal convolutional methods using the Celeb-DF dataset. Our methods\noutperformed state-of-the-art frame-based detection methods. Code for our paper\nis publicly available at https://github.com/oidelima/Deepfake-Detection.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 01:32:31 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["de Lima", "Oscar", ""], ["Franklin", "Sean", ""], ["Basu", "Shreshtha", ""], ["Karwoski", "Blake", ""], ["George", "Annet", ""]]}, {"id": "2006.14758", "submitter": "Yi Fang", "authors": "Daohan Lu, Yi Fang", "title": "Meta Deformation Network: Meta Functionals for Shape Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new technique named \"Meta Deformation Network\" for 3D shape\nmatching via deformation, in which a deep neural network maps a reference shape\nonto the parameters of a second neural network whose task is to give the\ncorrespondence between a learned template and query shape via deformation. We\ncategorize the second neural network as a meta-function, or a function\ngenerated by another function, as its parameters are dynamically given by the\nfirst network on a per-input basis. This leads to a straightforward overall\narchitecture and faster execution speeds, without loss in the quality of the\ndeformation of the template. We show in our experiments that Meta Deformation\nNetwork leads to improvements on the MPI-FAUST Inter Challenge over designs\nthat utilized a conventional decoder design that has non-dynamic parameters.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 02:28:51 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Lu", "Daohan", ""], ["Fang", "Yi", ""]]}, {"id": "2006.14761", "submitter": "Pengfei Guo", "authors": "Pengfei Guo, Puyang Wang, Jinyuan Zhou, Vishal M. Patel, Shanshan\n  Jiang", "title": "Lesion Mask-based Simultaneous Synthesis of Anatomic and MolecularMR\n  Images using a GAN", "comments": "MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven automatic approaches have demonstrated their great potential in\nresolving various clinical diagnostic dilemmas for patients with malignant\ngliomas in neuro-oncology with the help of conventional and advanced molecular\nMR images. However, the lack of sufficient annotated MRI data has vastly\nimpeded the development of such automatic methods. Conventional data\naugmentation approaches, including flipping, scaling, rotation, and distortion\nare not capable of generating data with diverse image content. In this paper,\nwe propose a method, called synthesis of anatomic and molecular MR images\nnetwork (SAMR), which can simultaneously synthesize data from arbitrary\nmanipulated lesion information on multiple anatomic and molecular MRI\nsequences, including T1-weighted (T1w), gadolinium enhanced T1w (Gd-T1w),\nT2-weighted (T2w), fluid-attenuated inversion recovery (FLAIR), and amide\nproton transfer-weighted (APTw). The proposed framework consists of a\nstretch-out up-sampling module, a brain atlas encoder, a segmentation\nconsistency module, and multi-scale label-wise discriminators. Extensive\nexperiments on real clinical data demonstrate that the proposed model can\nperform significantly better than the state-of-the-art synthesis methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 02:50:09 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 21:34:12 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 19:03:46 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Guo", "Pengfei", ""], ["Wang", "Puyang", ""], ["Zhou", "Jinyuan", ""], ["Patel", "Vishal M.", ""], ["Jiang", "Shanshan", ""]]}, {"id": "2006.14773", "submitter": "Jong Chul Ye", "authors": "Shujaat Khan, Jaeyoung Huh, Jong Chul Ye", "title": "Pushing the Limit of Unsupervised Learning for Ultrasound Image Artifact\n  Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound (US) imaging is a fast and non-invasive imaging modality which is\nwidely used for real-time clinical imaging applications without concerning\nabout radiation hazard. Unfortunately, it often suffers from poor visual\nquality from various origins, such as speckle noises, blurring, multi-line\nacquisition (MLA), limited RF channels, small number of view angles for the\ncase of plane wave imaging, etc. Classical methods to deal with these problems\ninclude image-domain signal processing approaches using various adaptive\nfiltering and model-based approaches. Recently, deep learning approaches have\nbeen successfully used for ultrasound imaging field. However, one of the\nlimitations of these approaches is that paired high quality images for\nsupervised training are difficult to obtain in many practical applications. In\nthis paper, inspired by the recent theory of unsupervised learning using\noptimal transport driven cycleGAN (OT-cycleGAN), we investigate applicability\nof unsupervised deep learning for US artifact removal problems without matched\nreference data. Experimental results for various tasks such as deconvolution,\nspeckle removal, limited data artifact removal, etc. confirmed that our\nunsupervised learning method provides comparable results to supervised learning\nfor many practical applications.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 03:21:56 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Khan", "Shujaat", ""], ["Huh", "Jaeyoung", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2006.14780", "submitter": "Insu Jeon", "authors": "In S. Jeon, Deokyoung Kang, Suk I. Yoo", "title": "Blind Image Deconvolution using Student's-t Prior with Overlapping Group\n  Sparsity", "comments": null, "journal-ref": "2017 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP)", "doi": "10.1109/ICASSP.2017.7952470", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we solve blind image deconvolution problem that is to remove\nblurs form a signal degraded image without any knowledge of the blur kernel.\nSince the problem is ill-posed, an image prior plays a significant role in\naccurate blind deconvolution. Traditional image prior assumes coefficients in\nfiltered domains are sparse. However, it is assumed here that there exist\nadditional structures over the sparse coefficients. Accordingly, we propose new\nproblem formulation for the blind image deconvolution, which utilizes the\nstructural information by coupling Student's-t image prior with overlapping\ngroup sparsity. The proposed method resulted in an effective blind\ndeconvolution algorithm that outperforms other state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 03:34:44 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Jeon", "In S.", ""], ["Kang", "Deokyoung", ""], ["Yoo", "Suk I.", ""]]}, {"id": "2006.14787", "submitter": "Zezhou Cheng", "authors": "Zezhou Cheng, Jong-Chyi Su, Subhransu Maji", "title": "On Equivariant and Invariant Learning of Object Landmark Representations", "comments": "Project Page:\n  https://people.cs.umass.edu/~zezhoucheng/contrastive_landmark Code:\n  https://github.com/cvl-umass/ContrastLandmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a collection of images, humans are able to discover landmarks by\nmodeling the shared geometric structure across instances. This idea of\ngeometric equivariance has been widely used for the unsupervised discovery of\nobject landmark representations. In this paper, we develop a simple and\neffective approach by combining instance-discriminative and\nspatially-discriminative contrastive learning. We show that when a deep network\nis trained to be invariant to geometric and photometric transformations,\nrepresentations emerge from its intermediate layers that are highly predictive\nof object landmarks. Stacking these across layers in a \"hypercolumn\" and\nprojecting them using spatially-contrastive learning further improves their\nperformance on matching and few-shot landmark regression tasks. We also present\na unified view of existing equivariant and invariant representation learning\napproaches through the lens of contrastive learning, shedding light on the\nnature of invariances learned. Experiments on standard benchmarks for landmark\nlearning, as well as a new challenging one we propose, show that the proposed\napproach surpasses prior state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 04:06:56 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 20:23:49 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Cheng", "Zezhou", ""], ["Su", "Jong-Chyi", ""], ["Maji", "Subhransu", ""]]}, {"id": "2006.14788", "submitter": "Jisui Huang", "authors": "Na Lei, Jisui Huang, Yuxue Ren, Emil Saucan, Zhenchang Wang", "title": "Ricci Curvature Based Volumetric Segmentation of the Auditory Ossicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The auditory ossicles that are located in the middle ear are the smallest\nbones in the human body. Their damage will result in hearing loss. It is\ntherefore important to be able to automatically diagnose ossicles' diseases\nbased on Computed Tomography (CT) 3D imaging. However CT images usually include\nthe whole head area, which is much larger than the bones of interest, thus the\nlocalization of the ossicles, followed by segmentation, both play a significant\nrole in automatic diagnosis. The commonly employed local segmentation methods\nrequire manually selected initial points, which is a highly time consuming\nprocess. We therefore propose a completely automatic method to locate the\nossicles which requires neither templates, nor manual labels. It relies solely\non the connective properties of the auditory ossicles themselves, and their\nrelationship with the surrounding tissue fluid. For the segmentation task, we\ndefine a novel energy function and obtain the shape of the ossicles from the 3D\nCT image by minimizing this new energy. Compared to the state-of-the-art\nmethods which usually use the gradient operator and some normalization terms,\nwe propose to add a Ricci curvature term to the commonly employed energy\nfunction. We compare our proposed method with the state-of-the-art methods and\nshow that the performance of discrete Forman-Ricci curvature is superior to the\nothers.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 04:09:15 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 08:56:31 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Lei", "Na", ""], ["Huang", "Jisui", ""], ["Ren", "Yuxue", ""], ["Saucan", "Emil", ""], ["Wang", "Zhenchang", ""]]}, {"id": "2006.14808", "submitter": "Masayoshi Aritsugi", "authors": "Riku Anegawa and Masayoshi Aritsugi", "title": "Text Detection on Roughly Placed Books by Leveraging a Learning-based\n  Model Trained with Another Domain Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text detection enables us to extract rich information from images. In this\npaper, we focus on how to generate bounding boxes that are appropriate to grasp\ntext areas on books to help implement automatic text detection. We attempt not\nto improve a learning-based model by training it with an enough amount of data\nin the target domain but to leverage it, which has been already trained with\nanother domain data. We develop algorithms that construct the bounding boxes by\nimproving and leveraging the results of a learning-based method. Our algorithms\ncan utilize different learning-based approaches to detect scene texts.\nExperimental evaluations demonstrate that our algorithms work well in various\nsituations where books are roughly placed.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 05:53:23 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Anegawa", "Riku", ""], ["Aritsugi", "Masayoshi", ""]]}, {"id": "2006.14811", "submitter": "Yu Tian", "authors": "Yu Tian, Gabriel Maicas, Leonardo Zorron Cheng Tao Pu, Rajvinder\n  Singh, Johan W. Verjans, Gustavo Carneiro", "title": "Few-Shot Anomaly Detection for Polyp Frames from Colonoscopy", "comments": "Accept at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection methods generally target the learning of a normal image\ndistribution (i.e., inliers showing healthy cases) and during testing, samples\nrelatively far from the learned distribution are classified as anomalies (i.e.,\noutliers showing disease cases). These approaches tend to be sensitive to\noutliers that lie relatively close to inliers (e.g., a colonoscopy image with a\nsmall polyp). In this paper, we address the inappropriate sensitivity to\noutliers by also learning from inliers. We propose a new few-shot anomaly\ndetection method based on an encoder trained to maximise the mutual information\nbetween feature embeddings and normal images, followed by a few-shot score\ninference network, trained with a large set of inliers and a substantially\nsmaller set of outliers. We evaluate our proposed method on the clinical\nproblem of detecting frames containing polyps from colonoscopy video sequences,\nwhere the training set has 13350 normal images (i.e., without polyps) and less\nthan 100 abnormal images (i.e., with polyps). The results of our proposed model\non this data set reveal a state-of-the-art detection result, while the\nperformance based on different number of anomaly samples is relatively stable\nafter approximately 40 abnormal training images.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 06:08:46 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Tian", "Yu", ""], ["Maicas", "Gabriel", ""], ["Pu", "Leonardo Zorron Cheng Tao", ""], ["Singh", "Rajvinder", ""], ["Verjans", "Johan W.", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2006.14822", "submitter": "Shruti Jadon", "authors": "Shruti Jadon", "title": "A survey of loss functions for semantic segmentation", "comments": "5 pages, 5 figures, 2 tables", "journal-ref": "2020 IEEE International Conference on Computational Intelligence\n  in Bioinformatics and Computational Biology", "doi": "10.1109/CIBCB48159.2020.9277638", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image Segmentation has been an active field of research as it has a wide\nrange of applications, ranging from automated disease detection to self-driving\ncars. In the past five years, various papers came up with different objective\nloss functions used in different cases such as biased data, sparse\nsegmentation, etc. In this paper, we have summarized some of the well-known\nloss functions widely used for Image Segmentation and listed out the cases\nwhere their usage can help in fast and better convergence of a model.\nFurthermore, we have also introduced a new log-cosh dice loss function and\ncompared its performance on the NBFS skull-segmentation open-source data-set\nwith widely used loss functions. We also showcased that certain loss functions\nperform well across all data-sets and can be taken as a good baseline choice in\nunknown data distribution scenarios. Our code is available at Github:\nhttps://github.com/shruti-jadon/Semantic-Segmentation-Loss-Functions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 06:49:18 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 18:24:31 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 06:43:50 GMT"}, {"version": "v4", "created": "Thu, 3 Sep 2020 01:14:34 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Jadon", "Shruti", ""]]}, {"id": "2006.14837", "submitter": "Masahiro Takahashi", "authors": "Masahiro Takahashi, Alessandro Moro, Yonghoon Ji and Kazunori Umeda", "title": "Expandable YOLO: 3D Object Detection from RGB-D Images", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at constructing a light-weight object detector that inputs a\ndepth and a color image from a stereo camera. Specifically, by extending the\nnetwork architecture of YOLOv3 to 3D in the middle, it is possible to output in\nthe depth direction. In addition, Intersection over Uninon (IoU) in 3D space is\nintroduced to confirm the accuracy of region extraction results. In the field\nof deep learning, object detectors that use distance information as input are\nactively studied for utilizing automated driving. However, the conventional\ndetector has a large network structure, and the real-time property is impaired.\nThe effectiveness of the detector constructed as described above is verified\nusing datasets. As a result of this experiment, the proposed model is able to\noutput 3D bounding boxes and detect people whose part of the body is hidden.\nFurther, the processing speed of the model is 44.35 fps.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 07:32:30 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Takahashi", "Masahiro", ""], ["Moro", "Alessandro", ""], ["Ji", "Yonghoon", ""], ["Umeda", "Kazunori", ""]]}, {"id": "2006.14841", "submitter": "Sailik Sengupta", "authors": "Alberto Olmo, Sailik Sengupta, Subbarao Kambhampati", "title": "Not all Failure Modes are Created Equal: Training Deep Neural Networks\n  for Explicable (Mis)Classification", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks are often brittle on image classification tasks and\nknown to misclassify inputs. While these misclassifications may be inevitable,\nall failure modes cannot be considered equal. Certain misclassifications (eg.\nclassifying the image of a dog to an airplane) can create surprise and result\nin the loss of human trust in the system. Even worse, certain errors (eg. a\nperson misclassified as a primate) can have societal impacts. Thus, in this\nwork, we aim to reduce inexplicable errors. To address this challenge, we first\ndiscuss how to obtain the class-level semantics that captures the human's\nexpectation ($M^h$) regarding which classes are semantically close vs. ones\nthat are far away. We show that for data-sets like CIFAR-10 and CIFAR-100,\nclass-level semantics can be obtained by leveraging human subject studies\n(significantly inexpensive compared to existing works) and, whenever possible,\nby utilizing publicly available human-curated knowledge. Second, we propose the\nuse of Weighted Loss Functions to penalize misclassifications by the weight of\ntheir inexplicability. Finally, we show that training (or even fine-tuning)\nexisting classifiers with the two proposed methods lead to Deep Neural Networks\nthat have (1) comparable top-1 accuracy, an important metric in operational\ncontexts, (2) more explicable failure modes and (3) require significantly less\ncost in teams of additional human labels compared to existing work.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 07:37:33 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Olmo", "Alberto", ""], ["Sengupta", "Sailik", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "2006.14853", "submitter": "Marco Scarpetta", "authors": "Filippo Attivissimo, Nicola Giaquinto, Marco Scarpetta, Maurizio\n  Spadavecchia", "title": "An Automatic Reader of Identity Documents", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": "10.1109/SMC.2019.8914438", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identity documents automatic reading and verification is an appealing\ntechnology for nowadays service industry, since this task is still mostly\nperformed manually, leading to waste of economic and time resources. In this\npaper the prototype of a novel automatic reading system of identity documents\nis presented. The system has been thought to extract data of the main Italian\nidentity documents from photographs of acceptable quality, like those usually\nrequired to online subscribers of various services. The document is first\nlocalized inside the photo, and then classified; finally, text recognition is\nexecuted. A synthetic dataset has been used, both for neural networks training,\nand for performance evaluation of the system. The synthetic dataset avoided\nprivacy issues linked to the use of real photos of real documents, which will\nbe used, instead, for future developments of the system.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 08:22:40 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Attivissimo", "Filippo", ""], ["Giaquinto", "Nicola", ""], ["Scarpetta", "Marco", ""], ["Spadavecchia", "Maurizio", ""]]}, {"id": "2006.14856", "submitter": "Mohammad Jalwana", "authors": "Mohammad A. A. K. Jalwana, Naveed Akhtar, Mohammed Bennamoun, Ajmal\n  Mian", "title": "Orthogonal Deep Models As Defense Against Black-Box Attacks", "comments": "Accepted in IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has demonstrated state-of-the-art performance for a variety of\nchallenging computer vision tasks. On one hand, this has enabled deep visual\nmodels to pave the way for a plethora of critical applications like disease\nprognostics and smart surveillance. On the other, deep learning has also been\nfound vulnerable to adversarial attacks, which calls for new techniques to\ndefend deep models against these attacks. Among the attack algorithms, the\nblack-box schemes are of serious practical concern since they only need\npublicly available knowledge of the targeted model. We carefully analyze the\ninherent weakness of deep models in black-box settings where the attacker may\ndevelop the attack using a model similar to the targeted model. Based on our\nanalysis, we introduce a novel gradient regularization scheme that encourages\nthe internal representation of a deep model to be orthogonal to another, even\nif the architectures of the two models are similar. Our unique constraint\nallows a model to concomitantly endeavour for higher accuracy while maintaining\nnear orthogonal alignment of gradients with respect to a reference model.\nDetailed empirical study verifies that controlled misalignment of gradients\nunder our orthogonality objective significantly boosts a model's robustness\nagainst transferable black-box adversarial attacks. In comparison to regular\nmodels, the orthogonal models are significantly more robust to a range of $l_p$\nnorm bounded perturbations. We verify the effectiveness of our technique on a\nvariety of large-scale models.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 08:29:05 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Jalwana", "Mohammad A. A. K.", ""], ["Akhtar", "Naveed", ""], ["Bennamoun", "Mohammed", ""], ["Mian", "Ajmal", ""]]}, {"id": "2006.14858", "submitter": "David K\\\"ugler", "authors": "David K\\\"ugler, Marc Uecker, Arjan Kuijper, Anirban Mukhopadhyay", "title": "AutoSNAP: Automatically Learning Neural Architectures for Instrument\n  Pose Estimation", "comments": "Accepted at MICCAI 2020 Preparing code for release at\n  https://github.com/MECLabTUDA/AutoSNAP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Despite recent successes, the advances in Deep Learning have not yet been\nfully translated to Computer Assisted Intervention (CAI) problems such as pose\nestimation of surgical instruments. Currently, neural architectures for\nclassification and segmentation tasks are adopted ignoring significant\ndiscrepancies between CAI and these tasks. We propose an automatic framework\n(AutoSNAP) for instrument pose estimation problems, which discovers and learns\nthe architectures for neural networks. We introduce 1)~an efficient testing\nenvironment for pose estimation, 2)~a powerful architecture representation\nbased on novel Symbolic Neural Architecture Patterns (SNAPs), and 3)~an\noptimization of the architecture using an efficient search scheme. Using\nAutoSNAP, we discover an improved architecture (SNAPNet) which outperforms both\nthe hand-engineered i3PosNet and the state-of-the-art architecture search\nmethod DARTS.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 08:34:47 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["K\u00fcgler", "David", ""], ["Uecker", "Marc", ""], ["Kuijper", "Arjan", ""], ["Mukhopadhyay", "Anirban", ""]]}, {"id": "2006.14859", "submitter": "Bruno Lecouat", "authors": "Bruno Lecouat, Jean Ponce, Julien Mairal", "title": "A Flexible Framework for Designing Trainable Priors with Adaptive\n  Smoothing and Game Encoding", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework for designing and training neural network\nlayers whose forward passes can be interpreted as solving non-smooth convex\noptimization problems, and whose architectures are derived from an optimization\nalgorithm. We focus on convex games, solved by local agents represented by the\nnodes of a graph and interacting through regularization functions. This\napproach is appealing for solving imaging problems, as it allows the use of\nclassical image priors within deep models that are trainable end to end. The\npriors used in this presentation include variants of total variation, Laplacian\nregularization, bilateral filtering, sparse coding on learned dictionaries, and\nnon-local self similarities. Our models are fully interpretable as well as\nparameter and data efficient. Our experiments demonstrate their effectiveness\non a large diversity of tasks ranging from image denoising and compressed\nsensing for fMRI to dense stereo matching.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 08:34:54 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 10:00:10 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Lecouat", "Bruno", ""], ["Ponce", "Jean", ""], ["Mairal", "Julien", ""]]}, {"id": "2006.14863", "submitter": "Qixiang Ye", "authors": "Feng Liu, Xiaoxong Zhang, Fang Wan, Xiangyang Ji, Qixiang Ye", "title": "Domain Contrast for Domain Adaptive Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Domain Contrast (DC), a simple yet effective approach inspired by\ncontrastive learning for training domain adaptive detectors. DC is deduced from\nthe error bound minimization perspective of a transferred model, and is\nimplemented with cross-domain contrast loss which is plug-and-play. By\nminimizing cross-domain contrast loss, DC guarantees the transferability of\ndetectors while naturally alleviating the class imbalance issue in the target\ndomain. DC can be applied at either image level or region level, consistently\nimproving detectors' transferability and discriminability. Extensive\nexperiments on commonly used benchmarks show that DC improves the baseline and\nstate-of-the-art by significant margins, while demonstrating great potential\nfor large domain divergence.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 08:45:36 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Liu", "Feng", ""], ["Zhang", "Xiaoxong", ""], ["Wan", "Fang", ""], ["Ji", "Xiangyang", ""], ["Ye", "Qixiang", ""]]}, {"id": "2006.14865", "submitter": "Zihao Yan", "authors": "Zihao Yan, Ruizhen Hu, Xingguang Yan, Luanmin Chen, Oliver van Kaick,\n  Hao Zhang, Hui Huang", "title": "RPM-Net: Recurrent Prediction of Motion and Parts from Point Cloud", "comments": "Accepted to SIGGRAPH Asia 2019, project page at\n  https://vcc.tech/research/2019/RPMNet", "journal-ref": "ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia),\n  volume 38, number 6, pages 240:1--240:15, year 2019", "doi": "10.1145/3355089.3356573", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce RPM-Net, a deep learning-based approach which simultaneously\ninfers movable parts and hallucinates their motions from a single,\nun-segmented, and possibly partial, 3D point cloud shape. RPM-Net is a novel\nRecurrent Neural Network (RNN), composed of an encoder-decoder pair with\ninterleaved Long Short-Term Memory (LSTM) components, which together predict a\ntemporal sequence of pointwise displacements for the input point cloud. At the\nsame time, the displacements allow the network to learn movable parts,\nresulting in a motion-based shape segmentation. Recursive applications of\nRPM-Net on the obtained parts can predict finer-level part motions, resulting\nin a hierarchical object segmentation. Furthermore, we develop a separate\nnetwork to estimate part mobilities, e.g., per-part motion parameters, from the\nsegmented motion sequence. Both networks learn deep predictive models from a\ntraining set that exemplifies a variety of mobilities for diverse objects. We\nshow results of simultaneous motion and part predictions from synthetic and\nreal scans of 3D objects exhibiting a variety of part mobilities, possibly\ninvolving multiple movable parts.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 08:51:11 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Yan", "Zihao", ""], ["Hu", "Ruizhen", ""], ["Yan", "Xingguang", ""], ["Chen", "Luanmin", ""], ["van Kaick", "Oliver", ""], ["Zhang", "Hao", ""], ["Huang", "Hui", ""]]}, {"id": "2006.14882", "submitter": "Fan Zuo", "authors": "Fan Zuo, Jingxing Wang, Jingqin Gao, Kaan Ozbay, Xuegang Jeff Ban,\n  Yubin Shen, Hong Yang, Shri Iyer", "title": "An Interactive Data Visualization and Analytics Tool to Evaluate\n  Mobility and Sociability Trends During COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 outbreak has dramatically changed travel behavior in affected\ncities. The C2SMART research team has been investigating the impact of COVID-19\non mobility and sociability. New York City (NYC) and Seattle, two of the cities\nmost affected by COVID-19 in the U.S. were included in our initial study. An\nall-in-one dashboard with data mining and cloud computing capabilities was\ndeveloped for interactive data analytics and visualization to facilitate the\nunderstanding of the impact of the outbreak and corresponding policies such as\nsocial distancing on transportation systems. This platform is updated regularly\nand continues to evolve with the addition of new data, impact metrics, and\nvisualizations to assist public and decision-makers to make informed decisions.\nThis paper presents the architecture of the COVID related mobility data\ndashboard and preliminary mobility and sociability metrics for NYC and Seattle.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 09:27:53 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Zuo", "Fan", ""], ["Wang", "Jingxing", ""], ["Gao", "Jingqin", ""], ["Ozbay", "Kaan", ""], ["Ban", "Xuegang Jeff", ""], ["Shen", "Yubin", ""], ["Yang", "Hong", ""], ["Iyer", "Shri", ""]]}, {"id": "2006.14887", "submitter": "Andreas Klos", "authors": "Andreas Klos, Marius Rosenbaum, Wolfram Schiffmann", "title": "Ensemble Transfer Learning for Emergency Landing Field Identification on\n  Moderate Resource Heterogeneous Kubernetes Cluster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The full loss of thrust of an aircraft requires fast and reliable decisions\nof the pilot. If no published landing field is within reach, an emergency\nlanding field must be selected. The choice of a suitable emergency landing\nfield denotes a crucial task to avoid unnecessary damage of the aircraft, risk\nfor the civil population as well as the crew and all passengers on board.\nEspecially in case of instrument meteorological conditions it is indispensable\nto use a database of suitable emergency landing fields. Thus, based on public\navailable digital orthographic photos and digital surface models, we created\nvarious datasets with different sample sizes to facilitate training and testing\nof neural networks. Each dataset consists of a set of data layers. The best\ncompositions of these data layers as well as the best performing transfer\nlearning models are selected. Subsequently, certain hyperparameters of the\nchosen models for each sample size are optimized with Bayesian and Bandit\noptimization. The hyperparameter tuning is performed with a self-made\nKubernetes cluster. The models outputs were investigated with respect to the\ninput data by the utilization of layer-wise relevance propagation. With\noptimized models we created an ensemble model to improve the segmentation\nperformance. Finally, an area around the airport of Arnsberg in North\nRhine-Westphalia was segmented and emergency landing fields are identified,\nwhile the verification of the final approach's obstacle clearance is left\nunconsidered. These emergency landing fields are stored in a PostgreSQL\ndatabase.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 09:40:32 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 09:35:34 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Klos", "Andreas", ""], ["Rosenbaum", "Marius", ""], ["Schiffmann", "Wolfram", ""]]}, {"id": "2006.14970", "submitter": "Thomas Germer", "authors": "Thomas Germer, Tobias Uelwer, Stefan Conrad, Stefan Harmeling", "title": "Fast Multi-Level Foreground Estimation", "comments": "Accepted at the 25th International Conference on Pattern Recognition\n  2020 (ICPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alpha matting aims to estimate the translucency of an object in a given\nimage. The resulting alpha matte describes pixel-wise to what amount foreground\nand background colors contribute to the color of the composite image. While\nmost methods in literature focus on estimating the alpha matte, the process of\nestimating the foreground colors given the input image and its alpha matte is\noften neglected, although foreground estimation is an essential part of many\nimage editing workflows. In this work, we propose a novel method for foreground\nestimation given the alpha matte. We demonstrate that our fast multi-level\napproach yields results that are comparable with the state-of-the-art while\noutperforming those methods in computational runtime and memory usage.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 13:16:13 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Germer", "Thomas", ""], ["Uelwer", "Tobias", ""], ["Conrad", "Stefan", ""], ["Harmeling", "Stefan", ""]]}, {"id": "2006.14984", "submitter": "Chengliang Dai", "authors": "Chengliang Dai, Shuo Wang, Yuanhan Mo, Kaichen Zhou, Elsa Angelini,\n  Yike Guo, and Wenjia Bai", "title": "Suggestive Annotation of Brain Tumour Images with Gradient-guided\n  Sampling", "comments": "Paper accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has been widely adopted for medical image analysis in recent\nyears given its promising performance in image segmentation and classification\ntasks. As a data-driven science, the success of machine learning, in particular\nsupervised learning, largely depends on the availability of manually annotated\ndatasets. For medical imaging applications, such annotated datasets are not\neasy to acquire. It takes a substantial amount of time and resource to curate\nan annotated medical image set. In this paper, we propose an efficient\nannotation framework for brain tumour images that is able to suggest\ninformative sample images for human experts to annotate. Our experiments show\nthat training a segmentation model with only 19% suggestively annotated patient\nscans from BraTS 2019 dataset can achieve a comparable performance to training\na model on the full dataset for whole tumour segmentation task. It demonstrates\na promising way to save manual annotation cost and improve data efficiency in\nmedical imaging applications.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 13:39:49 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 11:34:10 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Dai", "Chengliang", ""], ["Wang", "Shuo", ""], ["Mo", "Yuanhan", ""], ["Zhou", "Kaichen", ""], ["Angelini", "Elsa", ""], ["Guo", "Yike", ""], ["Bai", "Wenjia", ""]]}, {"id": "2006.15015", "submitter": "Jingang Tan", "authors": "Jingang Tan, Lili Chen, Kangru Wang, Jingquan Peng, Jiamao Li, Xiaolin\n  Zhang", "title": "SASO: Joint 3D Semantic-Instance Segmentation via Multi-scale Semantic\n  Association and Salient Point Clustering Optimization", "comments": "8 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel 3D point cloud segmentation framework named SASO, which\njointly performs semantic and instance segmentation tasks. For semantic\nsegmentation task, inspired by the inherent correlation among objects in\nspatial context, we propose a Multi-scale Semantic Association (MSA) module to\nexplore the constructive effects of the semantic context information. For\ninstance segmentation task, different from previous works that utilize\nclustering only in inference procedure, we propose a Salient Point Clustering\nOptimization (SPCO) module to introduce a clustering procedure into the\ntraining process and impel the network focusing on points that are difficult to\nbe distinguished. In addition, because of the inherent structures of indoor\nscenes, the imbalance problem of the category distribution is rarely considered\nbut severely limits the performance of 3D scene perception. To address this\nissue, we introduce an adaptive Water Filling Sampling (WFS) algorithm to\nbalance the category distribution of training data. Extensive experiments\ndemonstrate that our method outperforms the state-of-the-art methods on\nbenchmark datasets in both semantic segmentation and instance segmentation\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 08:55:25 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Tan", "Jingang", ""], ["Chen", "Lili", ""], ["Wang", "Kangru", ""], ["Peng", "Jingquan", ""], ["Li", "Jiamao", ""], ["Zhang", "Xiaolin", ""]]}, {"id": "2006.15031", "submitter": "Stephan Garbin Mr", "authors": "Stephan J. Garbin, Marek Kowalski, Matthew Johnson, and Jamie Shotton", "title": "High Resolution Zero-Shot Domain Adaptation of Synthetically Rendered\n  Face Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating photorealistic images of human faces at scale remains a\nprohibitively difficult task using computer graphics approaches. This is\nbecause these require the simulation of light to be photorealistic, which in\nturn requires physically accurate modelling of geometry, materials, and light\nsources, for both the head and the surrounding scene. Non-photorealistic\nrenders however are increasingly easy to produce. In contrast to computer\ngraphics approaches, generative models learned from more readily available 2D\nimage data have been shown to produce samples of human faces that are hard to\ndistinguish from real data. The process of learning usually corresponds to a\nloss of control over the shape and appearance of the generated images. For\ninstance, even simple disentangling tasks such as modifying the hair\nindependently of the face, which is trivial to accomplish in a computer\ngraphics approach, remains an open research question. In this work, we propose\nan algorithm that matches a non-photorealistic, synthetically generated image\nto a latent vector of a pretrained StyleGAN2 model which, in turn, maps the\nvector to a photorealistic image of a person of the same pose, expression,\nhair, and lighting. In contrast to most previous work, we require no synthetic\ntraining data. To the best of our knowledge, this is the first algorithm of its\nkind to work at a resolution of 1K and represents a significant leap forward in\nvisual realism.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 15:00:04 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Garbin", "Stephan J.", ""], ["Kowalski", "Marek", ""], ["Johnson", "Matthew", ""], ["Shotton", "Jamie", ""]]}, {"id": "2006.15037", "submitter": "Emanuele Dalsasso", "authors": "Emanuele Dalsasso, Lo\\\"ic Denis, Florence Tupin", "title": "SAR2SAR: a semi-supervised despeckling algorithm for SAR images", "comments": "The manuscript is the accepted version of IEEE STARS. Code is made\n  available at https://gitlab.telecom-paris.fr/RING/SAR2SAR", "journal-ref": "IEEE Journal of Selected Topics in Applied Earth Observations and\n  Remote Sensing (Early Access), 2020", "doi": "10.1109/JSTARS.2021.3071864", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Speckle reduction is a key step in many remote sensing applications. By\nstrongly affecting synthetic aperture radar (SAR) images, it makes them\ndifficult to analyse. Due to the difficulty to model the spatial correlation of\nspeckle, a deep learning algorithm with self-supervision is proposed in this\npaper: SAR2SAR. Multi-temporal time series are leveraged and the neural network\nlearns to restore SAR images by only looking at noisy acquisitions. To this\npurpose, the recently proposed noise2noise framework has been employed. The\nstrategy to adapt it to SAR despeckling is presented, based on a compensation\nof temporal changes and a loss function adapted to the statistics of speckle.\n  A study with synthetic speckle noise is presented to compare the performances\nof the proposed method with other state-of-the-art filters. Then, results on\nreal images are discussed, to show the potential of the proposed algorithm. The\ncode is made available to allow testing and reproducible research in this\nfield.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 15:07:28 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 07:06:50 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 09:41:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Dalsasso", "Emanuele", ""], ["Denis", "Lo\u00efc", ""], ["Tupin", "Florence", ""]]}, {"id": "2006.15055", "submitter": "Thomas Kipf", "authors": "Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh\n  Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, Thomas Kipf", "title": "Object-Centric Learning with Slot Attention", "comments": "NeurIPS 2020. Code available at\n  https://github.com/google-research/google-research/tree/master/slot_attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning object-centric representations of complex scenes is a promising step\ntowards enabling efficient abstract reasoning from low-level perceptual\nfeatures. Yet, most deep learning approaches learn distributed representations\nthat do not capture the compositional properties of natural scenes. In this\npaper, we present the Slot Attention module, an architectural component that\ninterfaces with perceptual representations such as the output of a\nconvolutional neural network and produces a set of task-dependent abstract\nrepresentations which we call slots. These slots are exchangeable and can bind\nto any object in the input by specializing through a competitive procedure over\nmultiple rounds of attention. We empirically demonstrate that Slot Attention\ncan extract object-centric representations that enable generalization to unseen\ncompositions when trained on unsupervised object discovery and supervised\nproperty prediction tasks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 15:31:57 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 08:51:40 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Locatello", "Francesco", ""], ["Weissenborn", "Dirk", ""], ["Unterthiner", "Thomas", ""], ["Mahendran", "Aravindh", ""], ["Heigold", "Georg", ""], ["Uszkoreit", "Jakob", ""], ["Dosovitskiy", "Alexey", ""], ["Kipf", "Thomas", ""]]}, {"id": "2006.15056", "submitter": "Zitian Chen", "authors": "Zitian Chen, Zhiqiang Shen, Jiahui Yu, Erik Learned-Miller", "title": "Cross-Supervised Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After learning a new object category from image-level annotations (with no\nobject bounding boxes), humans are remarkably good at precisely localizing\nthose objects. However, building good object localizers (i.e., detectors)\ncurrently requires expensive instance-level annotations. While some work has\nbeen done on learning detectors from weakly labeled samples (with only class\nlabels), these detectors do poorly at localization. In this work, we show how\nto build better object detectors from weakly labeled images of new categories\nby leveraging knowledge learned from fully labeled base categories. We call\nthis novel learning paradigm cross-supervised object detection. We propose a\nunified framework that combines a detection head trained from instance-level\nannotations and a recognition head learned from image-level annotations,\ntogether with a spatial correlation module that bridges the gap between\ndetection and recognition. These contributions enable us to better detect novel\nobjects with image-level annotations in complex multi-object scenes such as the\nCOCO dataset.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 15:33:48 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 03:11:46 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Chen", "Zitian", ""], ["Shen", "Zhiqiang", ""], ["Yu", "Jiahui", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "2006.15057", "submitter": "Steffen Czolbe", "authors": "Steffen Czolbe, Oswin Krause, Ingemar Cox, Christian Igel", "title": "A Loss Function for Generative Neural Networks Based on Watson's\n  Perceptual Model", "comments": "Published at the 34th Conference on Neural Information Processing\n  Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To train Variational Autoencoders (VAEs) to generate realistic imagery\nrequires a loss function that reflects human perception of image similarity. We\npropose such a loss function based on Watson's perceptual model, which computes\na weighted distance in frequency space and accounts for luminance and contrast\nmasking. We extend the model to color images, increase its robustness to\ntranslation by using the Fourier Transform, remove artifacts due to splitting\nthe image into blocks, and make it differentiable. In experiments, VAEs trained\nwith the new loss function generated realistic, high-quality image samples.\nCompared to using the Euclidean distance and the Structural Similarity Index,\nthe images were less blurry; compared to deep neural network based losses, the\nnew approach required less computational resources and generated images with\nless artifacts.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 15:36:11 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 16:15:05 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 11:16:21 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Czolbe", "Steffen", ""], ["Krause", "Oswin", ""], ["Cox", "Ingemar", ""], ["Igel", "Christian", ""]]}, {"id": "2006.15059", "submitter": "Jos Stam", "authors": "Jos Stam", "title": "Computing Light Transport Gradients using the Adjoint Method", "comments": "23 pages, 8 figures, unpublished manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new equation from continuous adjoint theory to compute\nthe gradient of quantities governed by the Transport Theory of light. Unlike\ndiscrete gradients ala autograd, which work at the code level, we first\nformulate the continuous theory and then discretize it. The key insight of this\npaper is that computing gradients in Transport Theory is akin to computing the\nimportance, a quantity adjoint to radiance that satisfies an adjoint equation.\nImportance tells us where to look for light that matters. This is one of the\nkey insights of this paper. In fact, this mathematical journey started from a\nwhimsical thought that these adjoints might be related. Computing gradients is\ntherefore no more complicated than computing the importance field. This insight\nand the following paper hopefully will shed some light on this complicated\nproblem and ease the implementations of gradient computations in existing path\ntracers.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 15:38:14 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Stam", "Jos", ""]]}, {"id": "2006.15088", "submitter": "Hichem Sahbi", "authors": "Mingyuan Jiu and Hichem Sahbi", "title": "End-to-end training of deep kernel map networks for image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep kernel map networks have shown excellent performances in various\nclassification problems including image annotation. Their general recipe\nconsists in aggregating several layers of singular value decompositions (SVDs)\n-- that map data from input spaces into high dimensional spaces -- while\npreserving the similarity of the underlying kernels. However, the potential of\nthese deep map networks has not been fully explored as the original setting of\nthese networks focuses mainly on the approximation quality of their kernels and\nignores their discrimination power. In this paper, we introduce a novel\n\"end-to-end\" design for deep kernel map learning that balances the\napproximation quality of kernels and their discrimination power. Our method\nproceeds in two steps; first, layerwise SVD is applied in order to build\ninitial deep kernel map approximations and then an \"end-to-end\" supervised\nlearning is employed to further enhance their discrimination power while\nmaintaining their efficiency. Extensive experiments, conducted on the\nchallenging ImageCLEF annotation benchmark, show the high efficiency and the\nout-performance of this two-step process with respect to different related\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 16:37:41 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Jiu", "Mingyuan", ""], ["Sahbi", "Hichem", ""]]}, {"id": "2006.15102", "submitter": "Nandan Kumar Jha", "authors": "Rajat Saini, Nandan Kumar Jha, Bedanta Das, Sparsh Mittal, C. Krishna\n  Mohan", "title": "ULSAM: Ultra-Lightweight Subspace Attention Module for Compact\n  Convolutional Neural Networks", "comments": "Accepted as a conference paper in 2020 IEEE Winter Conference on\n  Applications of Computer Vision (WACV)", "journal-ref": "WACV (2020) 1627-1636", "doi": "10.1109/WACV45572.2020.9093341", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capability of the self-attention mechanism to model the long-range\ndependencies has catapulted its deployment in vision models. Unlike convolution\noperators, self-attention offers infinite receptive field and enables\ncompute-efficient modeling of global dependencies. However, the existing\nstate-of-the-art attention mechanisms incur high compute and/or parameter\noverheads, and hence unfit for compact convolutional neural networks (CNNs). In\nthis work, we propose a simple yet effective \"Ultra-Lightweight Subspace\nAttention Mechanism\" (ULSAM), which infers different attention maps for each\nfeature map subspace. We argue that leaning separate attention maps for each\nfeature subspace enables multi-scale and multi-frequency feature\nrepresentation, which is more desirable for fine-grained image classification.\nOur method of subspace attention is orthogonal and complementary to the\nexisting state-of-the-arts attention mechanisms used in vision models. ULSAM is\nend-to-end trainable and can be deployed as a plug-and-play module in the\npre-existing compact CNNs. Notably, our work is the first attempt that uses a\nsubspace attention mechanism to increase the efficiency of compact CNNs. To\nshow the efficacy of ULSAM, we perform experiments with MobileNet-V1 and\nMobileNet-V2 as backbone architectures on ImageNet-1K and three fine-grained\nimage classification datasets. We achieve $\\approx$13% and $\\approx$25%\nreduction in both the FLOPs and parameter counts of MobileNet-V2 with a 0.27%\nand more than 1% improvement in top-1 accuracy on the ImageNet-1K and\nfine-grained image classification datasets (respectively). Code and trained\nmodels are available at https://github.com/Nandan91/ULSAM.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 17:05:43 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Saini", "Rajat", ""], ["Jha", "Nandan Kumar", ""], ["Das", "Bedanta", ""], ["Mittal", "Sparsh", ""], ["Mohan", "C. Krishna", ""]]}, {"id": "2006.15109", "submitter": "Sandesh Bharadwaj", "authors": "Sandesh Bharadwaj (1,2) and Kunal Chanda (2) ((1) Indian Institute of\n  Information Technology, Design and Manufacturing, Kancheepuram, (2) Center\n  for Development of Advanced Computing, Kolkata)", "title": "Person Re-identification by analyzing Dynamic Variations in Gait\n  Sequences", "comments": "Presented at ETCCS 2020, accepted for publication in Springer LNEE\n  Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait recognition is a biometric technology that identifies individuals in a\nvideo sequence by analysing their style of walking or limb movement. However,\nthis identification is generally sensitive to appearance changes and\nconventional feature descriptors such as Gait Energy Image (GEI) lose some of\nthe dynamic information in the gait sequence. Active Energy Image (AEI) focuses\nmore on dynamic motion changes than GEI and is more suited to deal with\nappearance changes. We propose a new approach, which allows recognizing people\nby analysing the dynamic motion variations and identifying people without using\na database of predicted changes. In the proposed method, the active energy\nimage is calculated by averaging the difference frames of the silhouette\nsequence and divided into multiple segments. Affine moment invariants are\ncomputed as gait features for each section. Next, matching weights are\ncalculated based on the similarity between extracted features and those in the\ndatabase. Finally, the subject is identified by the weighted combination of\nsimilarities in all segments. The CASIA-B Gait Database is used as the\nprincipal dataset for the experimental analysis.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 17:16:37 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Bharadwaj", "Sandesh", ""], ["Chanda", "Kunal", ""]]}, {"id": "2006.15131", "submitter": "Soumyabrata Dev", "authors": "Ivan Bacher, Hossein Javidnia, Soumyabrata Dev, Rahul Agrahari, Murhaf\n  Hossari, Matthew Nicholson, Clare Conran, Jian Tang, Peng Song, David\n  Corrigan, Fran\\c{c}ois Piti\\'e", "title": "An Advert Creation System for 3D Product Placements", "comments": "Published in Proc. European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, the evolution of video-sharing platforms has attracted\na significant amount of investments on contextual advertising. The common\ncontextual advertising platforms utilize the information provided by users to\nintegrate 2D visual ads into videos. The existing platforms face many technical\nchallenges such as ad integration with respect to occluding objects and 3D ad\nplacement. This paper presents a Video Advertisement Placement & Integration\n(Adverts) framework, which is capable of perceiving the 3D geometry of the\nscene and camera motion to blend 3D virtual objects in videos and create the\nillusion of reality. The proposed framework contains several modules such as\nmonocular depth estimation, object segmentation, background-foreground\nseparation, alpha matting and camera tracking. Our experiments conducted using\nAdverts framework indicates the significant potential of this system in\ncontextual ad integration, and pushing the limits of advertising industry using\nmixed reality technologies.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 17:41:50 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Bacher", "Ivan", ""], ["Javidnia", "Hossein", ""], ["Dev", "Soumyabrata", ""], ["Agrahari", "Rahul", ""], ["Hossari", "Murhaf", ""], ["Nicholson", "Matthew", ""], ["Conran", "Clare", ""], ["Tang", "Jian", ""], ["Song", "Peng", ""], ["Corrigan", "David", ""], ["Piti\u00e9", "Fran\u00e7ois", ""]]}, {"id": "2006.15176", "submitter": "Kai Wang", "authors": "Kai Wang, Luis Herranz, Anjan Dutta, Joost van de Weijer", "title": "Bookworm continual learning: beyond zero-shot learning and continual\n  learning", "comments": "Accepted by TASK-CV workshop at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose bookworm continual learning(BCL), a flexible setting where unseen\nclasses can be inferred via a semantic model, and the visual model can be\nupdated continually. Thus BCL generalizes both continual learning (CL) and\nzero-shot learning (ZSL). We also propose the bidirectional imagination (BImag)\nframework to address BCL where features of both past and future classes are\ngenerated. We observe that conditioning the feature generator on attributes can\nactually harm the continual learning ability, and propose two variants (joint\nclass-attribute conditioning and asymmetric generation) to alleviate this\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 19:07:18 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 18:38:42 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 13:07:23 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Wang", "Kai", ""], ["Herranz", "Luis", ""], ["Dutta", "Anjan", ""], ["van de Weijer", "Joost", ""]]}, {"id": "2006.15186", "submitter": "Subhradeep Kayal", "authors": "Subhradeep Kayal, Shuai Chen, Marleen de Bruijne", "title": "Region-of-interest guided Supervoxel Inpainting for Self-supervision", "comments": "Accepted at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning has proven to be invaluable in making best use of\nall of the available data in biomedical image segmentation. One particularly\nsimple and effective mechanism to achieve self-supervision is inpainting, the\ntask of predicting arbitrary missing areas based on the rest of an image. In\nthis work, we focus on image inpainting as the self-supervised proxy task, and\npropose two novel structural changes to further enhance the performance of a\ndeep neural network. We guide the process of generating images to inpaint by\nusing supervoxel-based masking instead of random masking, and also by focusing\non the area to be segmented in the primary task, which we term as the\nregion-of-interest. We postulate that these additions force the network to\nlearn semantics that are more attuned to the primary task, and test our\nhypotheses on two applications: brain tumour and white matter hyperintensities\nsegmentation. We empirically show that our proposed approach consistently\noutperforms both supervised CNNs, without any self-supervision, and\nconventional inpainting-based self-supervision methods on both large and small\ntraining set sizes.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 19:28:20 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kayal", "Subhradeep", ""], ["Chen", "Shuai", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "2006.15190", "submitter": "Ruslan Rakhimov", "authors": "Ruslan Rakhimov, Emil Bogomolov, Alexandr Notchenko, Fung Mao, Alexey\n  Artemov, Denis Zorin, Evgeny Burnaev", "title": "Making DensePose fast and light", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DensePose estimation task is a significant step forward for enhancing user\nexperience computer vision applications ranging from augmented reality to cloth\nfitting. Existing neural network models capable of solving this task are\nheavily parameterized and a long way from being transferred to an embedded or\nmobile device. To enable Dense Pose inference on the end device with current\nmodels, one needs to support an expensive server-side infrastructure and have a\nstable internet connection. To make things worse, mobile and embedded devices\ndo not always have a powerful GPU inside. In this work, we target the problem\nof redesigning the DensePose R-CNN model's architecture so that the final\nnetwork retains most of its accuracy but becomes more light-weight and fast. To\nachieve that, we tested and incorporated many deep learning innovations from\nrecent years, specifically performing an ablation study on 23 efficient\nbackbone architectures, multiple two-stage detection pipeline modifications,\nand custom model quantization methods. As a result, we achieved $17\\times$\nmodel size reduction and $2\\times$ latency improvement compared to the baseline\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 19:42:20 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 12:38:28 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 11:33:27 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Rakhimov", "Ruslan", ""], ["Bogomolov", "Emil", ""], ["Notchenko", "Alexandr", ""], ["Mao", "Fung", ""], ["Artemov", "Alexey", ""], ["Zorin", "Denis", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2006.15264", "submitter": "Hajar Emami Gohari", "authors": "Hajar Emami, Ming Dong, Carri K. Glide-Hurst", "title": "Attention-Guided Generative Adversarial Network to Address Atypical\n  Anatomy in Modality Transfer", "comments": "IEEE 21st International Conference on Information Reuse and\n  Integration for Data Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, interest in MR-only treatment planning using synthetic CTs (synCTs)\nhas grown rapidly in radiation therapy. However, developing class solutions for\nmedical images that contain atypical anatomy remains a major limitation. In\nthis paper, we propose a novel spatial attention-guided generative adversarial\nnetwork (attention-GAN) model to generate accurate synCTs using T1-weighted MRI\nimages as the input to address atypical anatomy. Experimental results on\nfifteen brain cancer patients show that attention-GAN outperformed existing\nsynCT models and achieved an average MAE of 85.22$\\pm$12.08, 232.41$\\pm$60.86,\n246.38$\\pm$42.67 Hounsfield units between synCT and CT-SIM across the entire\nhead, bone and air regions, respectively. Qualitative analysis shows that\nattention-GAN has the ability to use spatially focused areas to better handle\noutliers, areas with complex anatomy or post-surgical regions, and thus offer\nstrong potential for supporting near real-time MR-only treatment planning.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 02:50:39 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 16:42:35 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 22:26:39 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Emami", "Hajar", ""], ["Dong", "Ming", ""], ["Glide-Hurst", "Carri K.", ""]]}, {"id": "2006.15271", "submitter": "Dongdong Chen", "authors": "Dongdong Chen, Mike E. Davies and Mohammad Golbabaee", "title": "Compressive MR Fingerprinting reconstruction with Neural Proximal\n  Gradient iterations", "comments": "To appear in MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistency of the predictions with respect to the physical forward model is\npivotal for reliably solving inverse problems. This consistency is mostly\nun-controlled in the current end-to-end deep learning methodologies proposed\nfor the Magnetic Resonance Fingerprinting (MRF) problem. To address this, we\npropose ProxNet, a learned proximal gradient descent framework that directly\nincorporates the forward acquisition and Bloch dynamic models within a\nrecurrent learning mechanism. The ProxNet adopts a compact neural proximal\nmodel for de-aliasing and quantitative inference, that can be flexibly trained\non scarce MRF training datasets. Our numerical experiments show that the\nProxNet can achieve a superior quantitative inference accuracy, much smaller\nstorage requirement, and a comparable runtime to the recent deep learning MRF\nbaselines, while being much faster than the dictionary matching schemes. Code\nhas been released at https://github.com/edongdongchen/PGD-Net.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 03:52:22 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 19:05:51 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 11:51:40 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Chen", "Dongdong", ""], ["Davies", "Mike E.", ""], ["Golbabaee", "Mohammad", ""]]}, {"id": "2006.15304", "submitter": "Suren Sritharan", "authors": "Harshana Weligampola, Gihan Jayatilaka, Suren Sritharan, Roshan\n  Godaliyadda, Parakrama Ekanayaka, Roshan Ragel, Vijitha Herath", "title": "A Retinex based GAN Pipeline to Utilize Paired and Unpaired Datasets for\n  Enhancing Low Light Images", "comments": null, "journal-ref": null, "doi": "10.1109/MERCon50084.2020.9185373", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low light image enhancement is an important challenge for the development of\nrobust computer vision algorithms. The machine learning approaches to this have\nbeen either unsupervised, supervised based on paired dataset or supervised\nbased on unpaired dataset. This paper presents a novel deep learning pipeline\nthat can learn from both paired and unpaired datasets. Convolution Neural\nNetworks (CNNs) that are optimized to minimize standard loss, and Generative\nAdversarial Networks (GANs) that are optimized to minimize the adversarial loss\nare used to achieve different steps of the low light image enhancement process.\nCycle consistency loss and a patched discriminator are utilized to further\nimprove the performance. The paper also analyses the functionality and the\nperformance of different components, hidden layers, and the entire pipeline.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 07:12:21 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Weligampola", "Harshana", ""], ["Jayatilaka", "Gihan", ""], ["Sritharan", "Suren", ""], ["Godaliyadda", "Roshan", ""], ["Ekanayaka", "Parakrama", ""], ["Ragel", "Roshan", ""], ["Herath", "Vijitha", ""]]}, {"id": "2006.15319", "submitter": "Hung Le", "authors": "Hung Le, Steven C.H. Hoi", "title": "Video-Grounded Dialogues with Pretrained Generation Language Models", "comments": "Accepted at ACL 2020 (Short Paper)", "journal-ref": "Association for Computational Linguistics (2020) 5842-5848", "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models have shown remarkable success in improving\nvarious downstream NLP tasks due to their ability to capture dependencies in\ntextual data and generate natural responses. In this paper, we leverage the\npower of pre-trained language models for improving video-grounded dialogue,\nwhich is very challenging and involves complex features of different dynamics:\n(1) Video features which can extend across both spatial and temporal\ndimensions; and (2) Dialogue features which involve semantic dependencies over\nmultiple dialogue turns. We propose a framework by extending GPT-2 models to\ntackle these challenges by formulating video-grounded dialogue tasks as a\nsequence-to-sequence task, combining both visual and textual representation\ninto a structured sequence, and fine-tuning a large pre-trained GPT-2 network.\nOur framework allows fine-tuning language models to capture dependencies across\nmultiple modalities over different levels of information: spatio-temporal level\nin video and token-sentence level in dialogue context. We achieve promising\nimprovement on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark from\nDSTC7, which supports a potential direction in this line of research.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 08:24:26 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Le", "Hung", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2006.15320", "submitter": "Titinunt Kitrungrotsakul", "authors": "Titinunt Kitrungrotsakul, Iwamoto Yutaro, Lanfen Lin, Ruofeng Tong,\n  Jingsong Li, Yen-Wei Chen", "title": "Interactive Deep Refinement Network for Medical Image Segmentation", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have successfully been employed in numerous computer\nvision tasks including image segmentation. The techniques have also been\napplied to medical image segmentation, one of the most critical tasks in\ncomputer-aided diagnosis. Compared with natural images, the medical image is a\ngray-scale image with low-contrast (even with some invisible parts). Because\nsome organs have similar intensity and texture with neighboring organs, there\nis usually a need to refine automatic segmentation results. In this paper, we\npropose an interactive deep refinement framework to improve the traditional\nsemantic segmentation networks such as U-Net and fully convolutional network.\nIn the proposed framework, we added a refinement network to traditional\nsegmentation network to refine the segmentation results.Experimental results\nwith public dataset revealed that the proposed method could achieve higher\naccuracy than other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 08:24:46 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kitrungrotsakul", "Titinunt", ""], ["Yutaro", "Iwamoto", ""], ["Lin", "Lanfen", ""], ["Tong", "Ruofeng", ""], ["Li", "Jingsong", ""], ["Chen", "Yen-Wei", ""]]}, {"id": "2006.15327", "submitter": "Amir Bar", "authors": "Amir Bar, Roei Herzig, Xiaolong Wang, Anna Rohrbach, Gal Chechik,\n  Trevor Darrell, Amir Globerson", "title": "Compositional Video Synthesis with Action Graphs", "comments": "ICML 2021 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos of actions are complex signals containing rich compositional structure\nin space and time. Current video generation methods lack the ability to\ncondition the generation on multiple coordinated and potentially simultaneous\ntimed actions. To address this challenge, we propose to represent the actions\nin a graph structure called Action Graph and present the new ``Action Graph To\nVideo'' synthesis task. Our generative model for this task (AG2Vid)\ndisentangles motion and appearance features, and by incorporating a scheduling\nmechanism for actions facilitates a timely and coordinated video generation. We\ntrain and evaluate AG2Vid on the CATER and Something-Something V2 datasets, and\nshow that the resulting videos have better visual quality and semantic\nconsistency compared to baselines. Finally, our model demonstrates zero-shot\nabilities by synthesizing novel compositions of the learned actions. For code\nand pretrained models, see the project page https://roeiherz.github.io/AG2Video\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 09:39:04 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 11:31:18 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 11:01:21 GMT"}, {"version": "v4", "created": "Thu, 10 Jun 2021 21:07:15 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Bar", "Amir", ""], ["Herzig", "Roei", ""], ["Wang", "Xiaolong", ""], ["Rohrbach", "Anna", ""], ["Chechik", "Gal", ""], ["Darrell", "Trevor", ""], ["Globerson", "Amir", ""]]}, {"id": "2006.15330", "submitter": "Roman Jakub\\'i\\v{c}ek", "authors": "Roman Jakubicek and Tomas Vicar and Jiri Chmelik", "title": "A Tool for Automatic Estimation of Patient Position in Spinal CT Data", "comments": "6 pages, 3 figures, submitted on EMBEC 2020; the paper has not been\n  reviewed yet", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the recently available research and challenge data lack the meta-data\ncontaining any information about the patient position. This paper presents a\ntool for automatic rotation of CT data into a standardized (HFS) patient\nposition. The proposed method is based on the prediction of rotation angle with\nCNN, and it achieved nearly perfect results with an accuracy of 99.55 %. We\nprovide implementations with easy to use an example for both Matlab and Python\n(PyTorch), which can be used, for example, for automatic rotation correction of\nVerSe2020 challenge data.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 09:48:49 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Jakubicek", "Roman", ""], ["Vicar", "Tomas", ""], ["Chmelik", "Jiri", ""]]}, {"id": "2006.15349", "submitter": "Marc G\\'orriz Blanch", "authors": "Marc G\\'orriz, Saverio Blasi, Alan F. Smeaton, Noel E. O'Connor, Marta\n  Mrak", "title": "Chroma Intra Prediction with attention-based CNN architectures", "comments": "27th IEEE International Conference on Image Processing, 25-28 Oct\n  2020, Abu Dhabi, United Arab Emirates", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CC cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural networks can be used in video coding to improve chroma\nintra-prediction. In particular, usage of fully-connected networks has enabled\nbetter cross-component prediction with respect to traditional linear models.\nNonetheless, state-of-the-art architectures tend to disregard the location of\nindividual reference samples in the prediction process. This paper proposes a\nnew neural network architecture for cross-component intra-prediction. The\nnetwork uses a novel attention module to model spatial relations between\nreference and predicted samples. The proposed approach is integrated into the\nVersatile Video Coding (VVC) prediction pipeline. Experimental results\ndemonstrate compression gains over the latest VVC anchor compared with\nstate-of-the-art chroma intra-prediction methods based on neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 12:11:17 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["G\u00f3rriz", "Marc", ""], ["Blasi", "Saverio", ""], ["Smeaton", "Alan F.", ""], ["O'Connor", "Noel E.", ""], ["Mrak", "Marta", ""]]}, {"id": "2006.15350", "submitter": "Jun Liu", "authors": "Jun Liu, Qing Li, Rui Cao, Wenming Tang, Guoping Qiu", "title": "MiniNet: An extremely lightweight convolutional neural network for\n  real-time unsupervised monocular depth estimation", "comments": null, "journal-ref": "Volume 166, August 2020, Pages 255-267", "doi": "10.1016/j.isprsjprs.2020.06.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting depth from a single image is an attractive research topic since it\nprovides one more dimension of information to enable machines to better\nperceive the world. Recently, deep learning has emerged as an effective\napproach to monocular depth estimation. As obtaining labeled data is costly,\nthere is a recent trend to move from supervised learning to unsupervised\nlearning to obtain monocular depth. However, most unsupervised learning methods\ncapable of achieving high depth prediction accuracy will require a deep network\narchitecture which will be too heavy and complex to run on embedded devices\nwith limited storage and memory spaces. To address this issue, we propose a new\npowerful network with a recurrent module to achieve the capability of a deep\nnetwork while at the same time maintaining an extremely lightweight size for\nreal-time high performance unsupervised monocular depth prediction from video\nsequences. Besides, a novel efficient upsample block is proposed to fuse the\nfeatures from the associated encoder layer and recover the spatial size of\nfeatures with the small number of model parameters. We validate the\neffectiveness of our approach via extensive experiments on the KITTI dataset.\nOur new model can run at a speed of about 110 frames per second (fps) on a\nsingle GPU, 37 fps on a single CPU, and 2 fps on a Raspberry Pi 3. Moreover, it\nachieves higher depth accuracy with nearly 33 times fewer model parameters than\nstate-of-the-art models. To the best of our knowledge, this work is the first\nextremely lightweight neural network trained on monocular video sequences for\nreal-time unsupervised monocular depth estimation, which opens up the\npossibility of implementing deep learning-based real-time unsupervised\nmonocular depth prediction on low-cost embedded devices.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 12:13:22 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Liu", "Jun", ""], ["Li", "Qing", ""], ["Cao", "Rui", ""], ["Tang", "Wenming", ""], ["Qiu", "Guoping", ""]]}, {"id": "2006.15351", "submitter": "Hongwei Dong", "authors": "Lamei Zhang and Siyu Zhang and Bin Zou and Hongwei Dong", "title": "Unsupervised Deep Representation Learning and Few-Shot Classification of\n  PolSAR Images", "comments": "16 pages, 16 figures", "journal-ref": null, "doi": "10.1109/TGRS.2020.3043191", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning and convolutional neural networks (CNNs) have made progress in\npolarimetric synthetic aperture radar (PolSAR) image classification over the\npast few years. However, a crucial issue has not been addressed, i.e., the\nrequirement of CNNs for abundant labeled samples versus the insufficient human\nannotations of PolSAR images. It is well-known that following the supervised\nlearning paradigm may lead to the overfitting of training data, and the lack of\nsupervision information of PolSAR images undoubtedly aggravates this problem,\nwhich greatly affects the generalization performance of CNN-based classifiers\nin large-scale applications. To handle this problem, in this paper, learning\ntransferrable representations from unlabeled PolSAR data through convolutional\narchitectures is explored for the first time. Specifically, a PolSAR-tailored\ncontrastive learning network (PCLNet) is proposed for unsupervised deep PolSAR\nrepresentation learning and few-shot classification. Different from the\nutilization of optical processing methods, a diversity stimulation mechanism is\nconstructed to narrow the application gap between optics and PolSAR. Beyond the\nconventional supervised methods, PCLNet develops an unsupervised pre-training\nphase based on the proxy objective of instance discrimination to learn useful\nrepresentations from unlabeled PolSAR data. The acquired representations are\ntransferred to the downstream task, i.e., few-shot PolSAR classification.\nExperiments on two widely-used PolSAR benchmark datasets confirm the validity\nof PCLNet. Besides, this work may enlighten how to efficiently utilize the\nmassive unlabeled PolSAR data to alleviate the greedy demands of CNN-based\nmethods for human annotations.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 12:15:32 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 01:27:15 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhang", "Lamei", ""], ["Zhang", "Siyu", ""], ["Zou", "Bin", ""], ["Dong", "Hongwei", ""]]}, {"id": "2006.15357", "submitter": "Zehong Cao Dr.", "authors": "Xianglin Zheng, Zehong Cao, Quan Bai", "title": "An Evoked Potential-Guided Deep Learning Brain Representation For Visual\n  Classification", "comments": "This paper is submitting to ICONIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new perspective in visual classification aims to decode the feature\nrepresentation of visual objects from human brain activities. Recording\nelectroencephalogram (EEG) from the brain cortex has been seen as a prevalent\napproach to understand the cognition process of an image classification task.\nIn this study, we proposed a deep learning framework guided by the visual\nevoked potentials, called the Event-Related Potential (ERP)-Long short-term\nmemory (LSTM) framework, extracted by EEG signals for visual classification. In\nspecific, we first extracted the ERP sequences from multiple EEG channels to\nresponse image stimuli-related information. Then, we trained an LSTM network to\nlearn the feature representation space of visual objects for classification. In\nthe experiment, 10 subjects were recorded by over 50,000 EEG trials from an\nimage dataset with 6 categories, including a total of 72 exemplars. Our results\nshowed that our proposed ERP-LSTM framework could achieve classification\naccuracies of cross-subject of 66.81% and 27.08% for categories (6 classes) and\nexemplars (72 classes), respectively. Our results outperformed that of using\nthe existing visual classification frameworks, by improving classification\naccuracies in the range of 12.62% - 53.99%. Our findings suggested that\ndecoding visual evoked potentials from EEG signals is an effective strategy to\nlearn discriminative brain representations for visual classification.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 12:46:31 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Zheng", "Xianglin", ""], ["Cao", "Zehong", ""], ["Bai", "Quan", ""]]}, {"id": "2006.15366", "submitter": "Xiaoxu Li", "authors": "Xiaoxu Li, Liyun Yu, Xiaochen Yang, Zhanyu Ma, Jing-Hao Xue, Jie Cao,\n  Jun Guo", "title": "ReMarNet: Conjoint Relation and Margin Learning for Small-Sample Image\n  Classification", "comments": "IEEE TCSVT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite achieving state-of-the-art performance, deep learning methods\ngenerally require a large amount of labeled data during training and may suffer\nfrom overfitting when the sample size is small. To ensure good generalizability\nof deep networks under small sample sizes, learning discriminative features is\ncrucial. To this end, several loss functions have been proposed to encourage\nlarge intra-class compactness and inter-class separability. In this paper, we\npropose to enhance the discriminative power of features from a new perspective\nby introducing a novel neural network termed Relation-and-Margin learning\nNetwork (ReMarNet). Our method assembles two networks of different backbones so\nas to learn the features that can perform excellently in both of the\naforementioned two classification mechanisms. Specifically, a relation network\nis used to learn the features that can support classification based on the\nsimilarity between a sample and a class prototype; at the meantime, a fully\nconnected network with the cross entropy loss is used for classification via\nthe decision boundary. Experiments on four image datasets demonstrate that our\napproach is effective in learning discriminative features from a small set of\nlabeled samples and achieves competitive performance against state-of-the-art\nmethods. Codes are available at https://github.com/liyunyu08/ReMarNet.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 13:50:20 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Li", "Xiaoxu", ""], ["Yu", "Liyun", ""], ["Yang", "Xiaochen", ""], ["Ma", "Zhanyu", ""], ["Xue", "Jing-Hao", ""], ["Cao", "Jie", ""], ["Guo", "Jun", ""]]}, {"id": "2006.15373", "submitter": "Nicola Strisciuglio", "authors": "Rafael Brandt, Nicola Strisciuglio, Nicolai Petkov", "title": "MTStereo 2.0: improved accuracy of stereo depth estimation withMax-trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Efficient yet accurate extraction of depth from stereo image pairs is\nrequired by systems with low power resources, such as robotics and embedded\nsystems. State-of-the-art stereo matching methods based on convolutional neural\nnetworks require intensive computations on GPUs and are difficult to deploy on\nembedded systems. In this paper, we propose a stereo matching method, called\nMTStereo 2.0, for limited-resource systems that require efficient and accurate\ndepth estimation. It is based on a Max-tree hierarchical representation of\nimage pairs, which we use to identify matching regions along image scan-lines.\nThe method includes a cost function that considers similarity of region\ncontextual information based on the Max-trees and a disparity border preserving\ncost aggregation approach. MTStereo 2.0 improves on its predecessor MTStereo\n1.0 as it a) deploys a more robust cost function, b) performs more thorough\ndetection of incorrect matches, c) computes disparity maps with pixel-level\nrather than node-level precision. MTStereo provides accurate sparse and\nsemi-dense depth estimation and does not require intensive GPU computations\nlike methods based on CNNs. Thus it can run on embedded and robotics devices\nwith low-power requirements. We tested the proposed approach on several\nbenchmark data sets, namely KITTI 2015, Driving, FlyingThings3D, Middlebury\n2014, Monkaa and the TrimBot2020 garden data sets, and achieved competitive\naccuracy and efficiency. The code is available at\nhttps://github.com/rbrandt1/MaxTreeS.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 14:33:04 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Brandt", "Rafael", ""], ["Strisciuglio", "Nicola", ""], ["Petkov", "Nicolai", ""]]}, {"id": "2006.15389", "submitter": "Yifan Song", "authors": "Yifan Song, Furkan Elibol, Mengkun She, David Nakath and Kevin K\\\"oser", "title": "Light Pose Calibration for Camera-light Vision Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Illuminating a scene with artificial light is a prerequisite for seeing in\ndark environments. However, nonuniform and dynamic illumination can deteriorate\nor even break computer vision approaches, for instance when operating a robot\nwith headlights in the darkness. This paper presents a novel light calibration\napproach by taking multi-view and -distance images of a reference plane in\norder to provide pose information of the employed light sources to the computer\nvision system. By following a physical light propagation approach, under\nconsideration of energy preservation, the estimation of light poses is solved\nby minimizing of the differences between real and rendered pixel intensities.\nDuring the evaluation we show the robustness and consistency of this method by\nstatistically analyzing the light pose estimation results with different\nsetups. Although the results are demonstrated using a rotationally-symmetric\nnon-isotropic light, the method is suited also for non-symmetric lights.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 15:56:13 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Song", "Yifan", ""], ["Elibol", "Furkan", ""], ["She", "Mengkun", ""], ["Nakath", "David", ""], ["K\u00f6ser", "Kevin", ""]]}, {"id": "2006.15398", "submitter": "Yifan Song", "authors": "Yifan Song, David Nakath, Mengkun She, Furkan Elibol and Kevin K\\\"oser", "title": "Deep Sea Robotic Imaging Simulator", "comments": "Accepted to ICPR 2021 Workshop: CVAUI 2020", "journal-ref": null, "doi": "10.1007/978-3-030-68790-8_29", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays underwater vision systems are being widely applied in ocean\nresearch. However, the largest portion of the ocean - the deep sea - still\nremains mostly unexplored. Only relatively few image sets have been taken from\nthe deep sea due to the physical limitations caused by technical challenges and\nenormous costs. Deep sea images are very different from the images taken in\nshallow waters and this area did not get much attention from the community. The\nshortage of deep sea images and the corresponding ground truth data for\nevaluation and training is becoming a bottleneck for the development of\nunderwater computer vision methods. Thus, this paper presents a physical\nmodel-based image simulation solution, which uses an in-air texture and depth\ninformation as inputs, to generate underwater image sequences taken by robots\nin deep ocean scenarios. Different from shallow water conditions, artificial\nillumination plays a vital role in deep sea image formation as it strongly\naffects the scene appearance. Our radiometric image formation model considers\nboth attenuation and scattering effects with co-moving spotlights in the dark.\nBy detailed analysis and evaluation of the underwater image formation model, we\npropose a 3D lookup table structure in combination with a novel rendering\nstrategy to improve simulation performance. This enables us to integrate an\ninteractive deep sea robotic vision simulation in the Unmanned Underwater\nVehicles simulator. To inspire further deep sea vision research by the\ncommunity, we will release the source code of our deep sea image converter to\nthe public.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 16:18:32 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 10:03:19 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 12:07:18 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Song", "Yifan", ""], ["Nakath", "David", ""], ["She", "Mengkun", ""], ["Elibol", "Furkan", ""], ["K\u00f6ser", "Kevin", ""]]}, {"id": "2006.15417", "submitter": "Ruihan Zhang", "authors": "Ruihan Zhang, Prashan Madumal, Tim Miller, Krista A. Ehinger, Benjamin\n  I. P. Rubinstein", "title": "Invertible Concept-based Explanations for CNN Models with Non-negative\n  Concept Activation Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) models for computer vision are powerful\nbut lack explainability in their most basic form. This deficiency remains a key\nchallenge when applying CNNs in important domains. Recent work on explanations\nthrough feature importance of approximate linear models has moved from\ninput-level features (pixels or segments) to features from mid-layer feature\nmaps in the form of concept activation vectors (CAVs). CAVs contain\nconcept-level information and could be learned via clustering. In this work, we\nrethink the ACE algorithm of Ghorbani et~al., proposing an alternative\ninvertible concept-based explanation (ICE) framework to overcome its\nshortcomings. Based on the requirements of fidelity (approximate models to\ntarget models) and interpretability (being meaningful to people), we design\nmeasurements and evaluate a range of matrix factorization methods with our\nframework. We find that non-negative concept activation vectors (NCAVs) from\nnon-negative matrix factorization provide superior performance in\ninterpretability and fidelity based on computational and human subject\nexperiments. Our framework provides both local and global concept-level\nexplanations for pre-trained CNN models.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 17:57:26 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 10:13:10 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 13:54:53 GMT"}, {"version": "v4", "created": "Thu, 17 Jun 2021 12:31:21 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhang", "Ruihan", ""], ["Madumal", "Prashan", ""], ["Miller", "Tim", ""], ["Ehinger", "Krista A.", ""], ["Rubinstein", "Benjamin I. P.", ""]]}, {"id": "2006.15418", "submitter": "Debidatta Dwibedi", "authors": "Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet,\n  Andrew Zisserman", "title": "Counting Out Time: Class Agnostic Video Repetition Counting in the Wild", "comments": "Accepted at CVPR 2020. Project webpage:\n  https://sites.google.com/view/repnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for estimating the period with which an action is\nrepeated in a video. The crux of the approach lies in constraining the period\nprediction module to use temporal self-similarity as an intermediate\nrepresentation bottleneck that allows generalization to unseen repetitions in\nvideos in the wild. We train this model, called Repnet, with a synthetic\ndataset that is generated from a large unlabeled video collection by sampling\nshort clips of varying lengths and repeating them with different periods and\ncounts. This combination of synthetic data and a powerful yet constrained\nmodel, allows us to predict periods in a class-agnostic fashion. Our model\nsubstantially exceeds the state of the art performance on existing periodicity\n(PERTUBE) and repetition counting (QUVA) benchmarks. We also collect a new\nchallenging dataset called Countix (~90 times larger than existing datasets)\nwhich captures the challenges of repetition counting in real-world videos.\nProject webpage: https://sites.google.com/view/repnet .\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 18:00:42 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Dwibedi", "Debidatta", ""], ["Aytar", "Yusuf", ""], ["Tompson", "Jonathan", ""], ["Sermanet", "Pierre", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2006.15427", "submitter": "Miguel \\'Angel Bautista Martin", "authors": "Miguel Angel Bautista, Walter Talbott, Shuangfei Zhai, Nitish\n  Srivastava, Joshua M Susskind", "title": "On the generalization of learning-based 3D reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art learning-based monocular 3D reconstruction methods learn\npriors over object categories on the training set, and as a result struggle to\nachieve reasonable generalization to object categories unseen during training.\nIn this paper we study the inductive biases encoded in the model architecture\nthat impact the generalization of learning-based 3D reconstruction methods. We\nfind that 3 inductive biases impact performance: the spatial extent of the\nencoder, the use of the underlying geometry of the scene to describe point\nfeatures, and the mechanism to aggregate information from multiple views.\nAdditionally, we propose mechanisms to enforce those inductive biases: a point\nrepresentation that is aware of camera position, and a variance cost to\naggregate information across views. Our model achieves state-of-the-art results\non the standard ShapeNet 3D reconstruction benchmark in various settings.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 18:53:41 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Bautista", "Miguel Angel", ""], ["Talbott", "Walter", ""], ["Zhai", "Shuangfei", ""], ["Srivastava", "Nitish", ""], ["Susskind", "Joshua M", ""]]}, {"id": "2006.15457", "submitter": "Maximilian Kraus", "authors": "Maximilian Kraus, Seyed Majid Azimi, Emec Ercelik, Reza Bahmanyar,\n  Peter Reinartz, Alois Knoll", "title": "AerialMPTNet: Multi-Pedestrian Tracking in Aerial Imagery Using Temporal\n  and Graphical Features", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-pedestrian tracking in aerial imagery has several applications such as\nlarge-scale event monitoring, disaster management, search-and-rescue missions,\nand as input into predictive crowd dynamic models. Due to the challenges such\nas the large number and the tiny size of the pedestrians (e.g., 4 x 4 pixels)\nwith their similar appearances as well as different scales and atmospheric\nconditions of the images with their extremely low frame rates (e.g., 2 fps),\ncurrent state-of-the-art algorithms including the deep learning-based ones are\nunable to perform well. In this paper, we propose AerialMPTNet, a novel\napproach for multi-pedestrian tracking in geo-referenced aerial imagery by\nfusing appearance features from a Siamese Neural Network, movement predictions\nfrom a Long Short-Term Memory, and pedestrian interconnections from a GraphCNN.\nIn addition, to address the lack of diverse aerial pedestrian tracking\ndatasets, we introduce the Aerial Multi-Pedestrian Tracking (AerialMPT) dataset\nconsisting of 307 frames and 44,740 pedestrians annotated. We believe that\nAerialMPT is the largest and most diverse dataset to this date and will be\nreleased publicly. We evaluate AerialMPTNet on AerialMPT and KIT AIS, and\nbenchmark with several state-of-the-art tracking methods. Results indicate that\nAerialMPTNet significantly outperforms other methods on accuracy and\ntime-efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 22:02:29 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kraus", "Maximilian", ""], ["Azimi", "Seyed Majid", ""], ["Ercelik", "Emec", ""], ["Bahmanyar", "Reza", ""], ["Reinartz", "Peter", ""], ["Knoll", "Alois", ""]]}, {"id": "2006.15473", "submitter": "Loc Trinh", "authors": "Loc Trinh, Michael Tsang, Sirisha Rambhatla, Yan Liu", "title": "Interpretable and Trustworthy Deepfake Detection via Dynamic Prototypes", "comments": "To appear in the 2021 IEEE Winter Conference on Applications of\n  Computer Vision (WACV 21')", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel human-centered approach for detecting\nforgery in face images, using dynamic prototypes as a form of visual\nexplanations. Currently, most state-of-the-art deepfake detections are based on\nblack-box models that process videos frame-by-frame for inference, and few\nclosely examine their temporal inconsistencies. However, the existence of such\ntemporal artifacts within deepfake videos is key in detecting and explaining\ndeepfakes to a supervising human. To this end, we propose Dynamic Prototype\nNetwork (DPNet) -- an interpretable and effective solution that utilizes\ndynamic representations (i.e., prototypes) to explain deepfake temporal\nartifacts. Extensive experimental results show that DPNet achieves competitive\npredictive performance, even on unseen testing datasets such as Google's\nDeepFakeDetection, DeeperForensics, and Celeb-DF, while providing easy\nreferential explanations of deepfake dynamics. On top of DPNet's prototypical\nframework, we further formulate temporal logic specifications based on these\ndynamics to check our model's compliance to desired temporal behaviors, hence\nproviding trustworthiness for such critical detection systems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 00:25:34 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 02:13:45 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Trinh", "Loc", ""], ["Tsang", "Michael", ""], ["Rambhatla", "Sirisha", ""], ["Liu", "Yan", ""]]}, {"id": "2006.15476", "submitter": "Jos\\'e Augusto Stuchi Mr", "authors": "Jos\\'e Augusto Stuchi, Levy Boccato, Romis Attux", "title": "Frequency learning for image classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning applied to computer vision and signal processing is\nachieving results comparable to the human brain on specific tasks due to the\ngreat improvements brought by the deep neural networks (DNN). The majority of\nstate-of-the-art architectures nowadays are DNN related, but only a few explore\nthe frequency domain to extract useful information and improve the results,\nlike in the image processing field. In this context, this paper presents a new\napproach for exploring the Fourier transform of the input images, which is\ncomposed of trainable frequency filters that boost discriminative components in\nthe spectrum. Additionally, we propose a slicing procedure to allow the network\nto learn both global and local features from the frequency-domain\nrepresentations of the image blocks. The proposed method proved to be\ncompetitive with respect to well-known DNN architectures in the selected\nexperiments, with the advantage of being a simpler and lightweight model. This\nwork also raises the discussion on how the state-of-the-art DNNs architectures\ncan exploit not only spatial features, but also the frequency, in order to\nimprove its performance when solving real world problems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 00:32:47 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Stuchi", "Jos\u00e9 Augusto", ""], ["Boccato", "Levy", ""], ["Attux", "Romis", ""]]}, {"id": "2006.15478", "submitter": "Riza Rae Pineda", "authors": "Riza Rae Pineda, Kristofer delas Pe\\~nas, Dana Manogan", "title": "Automated Stitching of Coral Reef Images and Extraction of Features for\n  Damselfish Shoaling Behavior Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavior analysis of animals involves the observation of intraspecific and\ninterspecific interactions among various organisms in the environment.\nCollective behavior such as herding in farm animals, flocking of birds, and\nshoaling and schooling of fish provide information on its benefits on\ncollective survival, fitness, reproductive patterns, group decision-making, and\neffects in animal epidemiology. In marine ethology, the investigation of\nbehavioral patterns in schooling species can provide supplemental information\nin the planning and management of marine resources. Currently, damselfish\nspecies, although prevalent in tropical waters, have no adequate established\nbase behavior information. This limits reef managers in efficiently planning\nfor stress and disaster responses in protecting the reef. Visual marine data\ncaptured in the wild are scarce and prone to multiple scene variations,\nprimarily caused by motion and changes in the natural environment. The gathered\nvideos of damselfish by this research exhibit several scene distortions caused\nby erratic camera motions during acquisition. To effectively analyze shoaling\nbehavior given the issues posed by capturing data in the wild, we propose a\npre-processing system that utilizes color correction and image stitching\ntechniques and extracts behavior features for manual analysis.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 00:56:51 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Pineda", "Riza Rae", ""], ["Pe\u00f1as", "Kristofer delas", ""], ["Manogan", "Dana", ""]]}, {"id": "2006.15480", "submitter": "Ke Sun", "authors": "Ke Sun, Zigang Geng, Depu Meng, Bin Xiao, Dong Liu, Zhaoxiang Zhang,\n  Jingdong Wang", "title": "Bottom-Up Human Pose Estimation by Ranking Heatmap-Guided Adaptive\n  Keypoint Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The typical bottom-up human pose estimation framework includes two stages,\nkeypoint detection and grouping. Most existing works focus on developing\ngrouping algorithms, e.g., associative embedding, and pixel-wise keypoint\nregression that we adopt in our approach. We present several schemes that are\nrarely or unthoroughly studied before for improving keypoint detection and\ngrouping (keypoint regression) performance. First, we exploit the keypoint\nheatmaps for pixel-wise keypoint regression instead of separating them for\nimproving keypoint regression. Second, we adopt a pixel-wise spatial\ntransformer network to learn adaptive representations for handling the scale\nand orientation variance to further improve keypoint regression quality. Last,\nwe present a joint shape and heatvalue scoring scheme to promote the estimated\nposes that are more likely to be true poses. Together with the tradeoff heatmap\nestimation loss for balancing the background and keypoint pixels and thus\nimproving heatmap estimation quality, we get the state-of-the-art bottom-up\nhuman pose estimation result. Code is available at\nhttps://github.com/HRNet/HRNet-Bottom-up-Pose-Estimation.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 01:14:59 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Sun", "Ke", ""], ["Geng", "Zigang", ""], ["Meng", "Depu", ""], ["Xiao", "Bin", ""], ["Liu", "Dong", ""], ["Zhang", "Zhaoxiang", ""], ["Wang", "Jingdong", ""]]}, {"id": "2006.15486", "submitter": "Imtiaz Ziko", "authors": "Imtiaz Masud Ziko, Jose Dolz, Eric Granger and Ismail Ben Ayed", "title": "Laplacian Regularized Few-Shot Learning", "comments": "ICML 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a transductive Laplacian-regularized inference for few-shot tasks.\nGiven any feature embedding learned from the base classes, we minimize a\nquadratic binary-assignment function containing two terms: (1) a unary term\nassigning query samples to the nearest class prototype, and (2) a pairwise\nLaplacian term encouraging nearby query samples to have consistent label\nassignments. Our transductive inference does not re-train the base model, and\ncan be viewed as a graph clustering of the query set, subject to supervision\nconstraints from the support set. We derive a computationally efficient bound\noptimizer of a relaxation of our function, which computes independent\n(parallel) updates for each query sample, while guaranteeing convergence.\nFollowing a simple cross-entropy training on the base classes, and without\ncomplex meta-learning strategies, we conducted comprehensive experiments over\nfive few-shot learning benchmarks. Our LaplacianShot consistently outperforms\nstate-of-the-art methods by significant margins across different models,\nsettings, and data sets. Furthermore, our transductive inference is very fast,\nwith computational times that are close to inductive inference, and can be used\nfor large-scale few-shot tasks.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 02:17:52 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 16:11:40 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 15:17:38 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Ziko", "Imtiaz Masud", ""], ["Dolz", "Jose", ""], ["Granger", "Eric", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "2006.15489", "submitter": "Ceyuan Yang", "authors": "Ceyuan Yang, Yinghao Xu, Bo Dai, Bolei Zhou", "title": "Video Representation Learning with Visual Tempo Consistency", "comments": "Technical report. Models are available at\n  https://github.com/decisionforce/VTHCL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tempo, which describes how fast an action goes, has shown its\npotential in supervised action recognition. In this work, we demonstrate that\nvisual tempo can also serve as a self-supervision signal for video\nrepresentation learning. We propose to maximize the mutual information between\nrepresentations of slow and fast videos via hierarchical contrastive learning\n(VTHCL). Specifically, by sampling the same instance at slow and fast frame\nrates respectively, we can obtain slow and fast video frames which share the\nsame semantics but contain different visual tempos. Video representations\nlearned from VTHCL achieve the competitive performances under the\nself-supervision evaluation protocol for action recognition on UCF-101 (82.1\\%)\nand HMDB-51 (49.2\\%). Moreover, comprehensive experiments suggest that the\nlearned representations are generalized well to other downstream tasks\nincluding action detection on AVA and action anticipation on Epic-Kitchen.\nFinally, we propose Instance Correspondence Map (ICM) to visualize the shared\nsemantics captured by contrastive learning.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 02:46:44 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 03:02:33 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Yang", "Ceyuan", ""], ["Xu", "Yinghao", ""], ["Dai", "Bo", ""], ["Zhou", "Bolei", ""]]}, {"id": "2006.15505", "submitter": "Runzhou Ge", "authors": "Zhuangzhuang Ding, Yihan Hu, Runzhou Ge, Li Huang, Sijia Chen, Yu\n  Wang, Jie Liao", "title": "1st Place Solution for Waymo Open Dataset Challenge -- 3D Detection and\n  Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we introduce our winning solution \"HorizonLiDAR3D\"\nfor the 3D detection track and the domain adaptation track in Waymo Open\nDataset Challenge at CVPR 2020. Many existing 3D object detectors include\nprior-based anchor box design to account for different scales and aspect ratios\nand classes of objects, which limits its capability of generalization to a\ndifferent dataset or domain and requires post-processing (e.g. Non-Maximum\nSuppression (NMS)). We proposed a one-stage, anchor-free and NMS-free 3D point\ncloud object detector AFDet, using object key-points to encode the 3D\nattributes, and to learn an end-to-end point cloud object detection without the\nneed of hand-engineering or learning the anchors. AFDet serves as a strong\nbaseline in our winning solution and significant improvements are made over\nthis baseline during the challenges. Specifically, we design stronger networks\nand enhance the point cloud data using densification and point painting. To\nleverage camera information, we append/paint additional attributes to each\npoint by projecting them to camera space and gathering image-based perception\ninformation. The final detection performance also benefits from model ensemble\nand Test-Time Augmentation (TTA) in both the 3D detection track and the domain\nadaptation track. Our solution achieves the 1st place with 77.11% mAPH/L2 and\n69.49% mAPH/L2 respectively on the 3D detection track and the domain adaptation\ntrack.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 04:49:39 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Ding", "Zhuangzhuang", ""], ["Hu", "Yihan", ""], ["Ge", "Runzhou", ""], ["Huang", "Li", ""], ["Chen", "Sijia", ""], ["Wang", "Yu", ""], ["Liao", "Jie", ""]]}, {"id": "2006.15506", "submitter": "Runzhou Ge", "authors": "Yu Wang, Sijia Chen, Li Huang, Runzhou Ge, Yihan Hu, Zhuangzhuang\n  Ding, Jie Liao", "title": "1st Place Solutions for Waymo Open Dataset Challenges -- 2D and 3D\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report presents the online and real-time 2D and 3D\nmulti-object tracking (MOT) algorithms that reached the 1st places on both\nWaymo Open Dataset 2D tracking and 3D tracking challenges. An efficient and\npragmatic online tracking-by-detection framework named HorizonMOT is proposed\nfor camera-based 2D tracking in the image space and LiDAR-based 3D tracking in\nthe 3D world space. Within the tracking-by-detection paradigm, our trackers\nleverage our high-performing detectors used in the 2D/3D detection challenges\nand achieved 45.13% 2D MOTA/L2 and 63.45% 3D MOTA/L2 in the 2D/3D tracking\nchallenges.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 04:49:59 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Wang", "Yu", ""], ["Chen", "Sijia", ""], ["Huang", "Li", ""], ["Ge", "Runzhou", ""], ["Hu", "Yihan", ""], ["Ding", "Zhuangzhuang", ""], ["Liao", "Jie", ""]]}, {"id": "2006.15507", "submitter": "Runzhou Ge", "authors": "Sijia Chen, Yu Wang, Li Huang, Runzhou Ge, Yihan Hu, Zhuangzhuang\n  Ding, Jie Liao", "title": "2nd Place Solution for Waymo Open Dataset Challenge -- 2D Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A practical autonomous driving system urges the need to reliably and\naccurately detect vehicles and persons. In this report, we introduce a\nstate-of-the-art 2D object detection system for autonomous driving scenarios.\nSpecifically, we integrate both popular two-stage detector and one-stage\ndetector with anchor free fashion to yield a robust detection. Furthermore, we\ntrain multiple expert models and design a greedy version of the auto ensemble\nscheme that automatically merges detections from different models. Notably, our\noverall detection system achieves 70.28 L2 mAP on the Waymo Open Dataset v1.2,\nranking the 2nd place in the 2D detection track of the Waymo Open Dataset\nChallenges.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 04:50:16 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Chen", "Sijia", ""], ["Wang", "Yu", ""], ["Huang", "Li", ""], ["Ge", "Runzhou", ""], ["Hu", "Yihan", ""], ["Ding", "Zhuangzhuang", ""], ["Liao", "Jie", ""]]}, {"id": "2006.15517", "submitter": "Rui Zhao", "authors": "Rui Zhao, Kin-Man Lam, Daniel P.K. Lun", "title": "Enhancement of a CNN-Based Denoiser Based on Spatial and Spectral\n  Analysis", "comments": "ICIP 2019", "journal-ref": null, "doi": "10.1109/ICIP.2019.8804295", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN)-based image denoising methods have been\nwidely studied recently, because of their high-speed processing capability and\ngood visual quality. However, most of the existing CNN-based denoisers learn\nthe image prior from the spatial domain, and suffer from the problem of\nspatially variant noise, which limits their performance in real-world image\ndenoising tasks. In this paper, we propose a discrete wavelet denoising CNN\n(WDnCNN), which restores images corrupted by various noise with a single model.\nSince most of the content or energy of natural images resides in the\nlow-frequency spectrum, their transformed coefficients in the frequency domain\nare highly imbalanced. To address this issue, we present a band normalization\nmodule (BNM) to normalize the coefficients from different parts of the\nfrequency spectrum. Moreover, we employ a band discriminative training (BDT)\ncriterion to enhance the model regression. We evaluate the proposed WDnCNN, and\ncompare it with other state-of-the-art denoisers. Experimental results show\nthat WDnCNN achieves promising performance in both synthetic and real noise\nreduction, making it a potential solution to many practical image denoising\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 05:25:50 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Zhao", "Rui", ""], ["Lam", "Kin-Man", ""], ["Lun", "Daniel P. K.", ""]]}, {"id": "2006.15520", "submitter": "Zihao Yan", "authors": "Ruizhen Hu, Zihao Yan, Jingwen Zhang, Oliver van Kaick, Ariel Shamir,\n  Hao Zhang, Hui Huang", "title": "Predictive and Generative Neural Networks for Object Functionality", "comments": "Accepted to SIGGRAPH 2018, project page at\n  https://vcc.tech/research/2018/ICON4", "journal-ref": "ACM Transactions on Graphics (Proc. SIGGRAPH), volume 37, number\n  4, pages 151:1--151:14, year 2018", "doi": "10.1145/3197517.3201287", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can predict the functionality of an object even without any\nsurroundings, since their knowledge and experience would allow them to\n\"hallucinate\" the interaction or usage scenarios involving the object. We\ndevelop predictive and generative deep convolutional neural networks to\nreplicate this feat. Specifically, our work focuses on functionalities of\nman-made 3D objects characterized by human-object or object-object\ninteractions. Our networks are trained on a database of scene contexts, called\ninteraction contexts, each consisting of a central object and one or more\nsurrounding objects, that represent object functionalities. Given a 3D object\nin isolation, our functional similarity network (fSIM-NET), a variation of the\ntriplet network, is trained to predict the functionality of the object by\ninferring functionality-revealing interaction contexts. fSIM-NET is\ncomplemented by a generative network (iGEN-NET) and a segmentation network\n(iSEG-NET). iGEN-NET takes a single voxelized 3D object with a functionality\nlabel and synthesizes a voxelized surround, i.e., the interaction context which\nvisually demonstrates the corresponding functionality. iSEG-NET further\nseparates the interacting objects into different groups according to their\ninteraction types.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 05:40:05 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Hu", "Ruizhen", ""], ["Yan", "Zihao", ""], ["Zhang", "Jingwen", ""], ["van Kaick", "Oliver", ""], ["Shamir", "Ariel", ""], ["Zhang", "Hao", ""], ["Huang", "Hui", ""]]}, {"id": "2006.15524", "submitter": "Xi Li", "authors": "Hanbin Zhao, Yongjian Fu, Mintong Kang, Qi Tian, Fei Wu, Xi Li", "title": "MgSvF: Multi-Grained Slow vs. Fast Framework for Few-Shot\n  Class-Incremental Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a challenging problem, few-shot class-incremental learning (FSCIL)\ncontinually learns a sequence of tasks, confronting the dilemma between slow\nforgetting of old knowledge and fast adaptation to new knowledge. In this\npaper, we concentrate on this \"slow vs. fast\" (SvF) dilemma to determine which\nknowledge components to be updated in a slow fashion or a fast fashion, and\nthereby balance old-knowledge preservation and new-knowledge adaptation. We\npropose a multi-grained SvF learning strategy to cope with the SvF dilemma from\ntwo different grains: intra-space (within the same feature space) and\ninter-space (between two different feature spaces). The proposed strategy\ndesigns a novel frequency-aware regularization to boost the intra-space SvF\ncapability, and meanwhile develops a new feature space composition operation to\nenhance the inter-space SvF learning performance. With the multi-grained SvF\nlearning strategy, our method outperforms the state-of-the-art approaches by a\nlarge margin.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 06:12:49 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 11:55:55 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 16:47:28 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Zhao", "Hanbin", ""], ["Fu", "Yongjian", ""], ["Kang", "Mintong", ""], ["Tian", "Qi", ""], ["Wu", "Fei", ""], ["Li", "Xi", ""]]}, {"id": "2006.15528", "submitter": "Li Xiao", "authors": "Chunlong Luo, Tianqi Yu, Yufan Luo, Manqing Wang, Fuhai Yu, Yinhao Li,\n  Chan Tian, Jie Qiao, Li Xiao", "title": "DeepACC:Automate Chromosome Classification based on Metaphase Images\n  using Deep Learning Framework Fused with Prior Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chromosome classification is an important but difficult and tedious task in\nkaryotyping. Previous methods only classify manually segmented single\nchromosome, which is far from clinical practice. In this work, we propose a\ndetection based method, DeepACC, to locate and fine classify chromosomes\nsimultaneously based on the whole metaphase image. We firstly introduce the\nAdditive Angular Margin Loss to enhance the discriminative power of model. To\nalleviate batch effects, we transform decision boundary of each class\ncase-by-case through a siamese network which make full use of prior knowledges\nthat chromosomes usually appear in pairs. Furthermore, we take the clinically\nseven group criterion as a prior knowledge and design an additional Group\nInner-Adjacency Loss to further reduce inter-class similarities. 3390 metaphase\nimages from clinical laboratory are collected and labelled to evaluate the\nperformance. Results show that the new design brings encouraging performance\ngains comparing to the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 07:04:41 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Luo", "Chunlong", ""], ["Yu", "Tianqi", ""], ["Luo", "Yufan", ""], ["Wang", "Manqing", ""], ["Yu", "Fuhai", ""], ["Li", "Yinhao", ""], ["Tian", "Chan", ""], ["Qiao", "Jie", ""], ["Xiao", "Li", ""]]}, {"id": "2006.15538", "submitter": "Adam Kortylewski", "authors": "Adam Kortylewski and Qing Liu and Angtian Wang and Yihong Sun and Alan\n  Yuille", "title": "Compositional Convolutional Neural Networks: A Robust and Interpretable\n  Model for Object Recognition under Occlusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision systems in real-world applications need to be robust to\npartial occlusion while also being explainable. In this work, we show that\nblack-box deep convolutional neural networks (DCNNs) have only limited\nrobustness to partial occlusion. We overcome these limitations by unifying\nDCNNs with part-based models into Compositional Convolutional Neural Networks\n(CompositionalNets) - an interpretable deep architecture with innate robustness\nto partial occlusion. Specifically, we propose to replace the fully connected\nclassification head of DCNNs with a differentiable compositional model that can\nbe trained end-to-end. The structure of the compositional model enables\nCompositionalNets to decompose images into objects and context, as well as to\nfurther decompose object representations in terms of individual parts and the\nobjects' pose. The generative nature of our compositional model enables it to\nlocalize occluders and to recognize objects based on their non-occluded parts.\nWe conduct extensive experiments in terms of image classification and object\ndetection on images of artificially occluded objects from the PASCAL3D+ and\nImageNet dataset, and real images of partially occluded vehicles from the\nMS-COCO dataset. Our experiments show that CompositionalNets made from several\npopular DCNN backbones (VGG-16, ResNet50, ResNext) improve by a large margin\nover their non-compositional counterparts at classifying and detecting\npartially occluded objects. Furthermore, they can localize occluders accurately\ndespite being trained with class-level supervision only. Finally, we\ndemonstrate that CompositionalNets provide human interpretable predictions as\ntheir individual components can be understood as detecting parts and estimating\nan objects' viewpoint.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 08:18:19 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Kortylewski", "Adam", ""], ["Liu", "Qing", ""], ["Wang", "Angtian", ""], ["Sun", "Yihong", ""], ["Yuille", "Alan", ""]]}, {"id": "2006.15553", "submitter": "Laifeng Hu", "authors": "Kaide Li, Bingyan Liao, Laifeng Hu, Yaonong Wang", "title": "DHARI Report to EPIC-Kitchens 2020 Object Detection Challenge", "comments": "Challenge Winner in the EPIC-Kitchens 2020 Object Detection\n  Challenge(EPIC@CVPR2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we describe the technical details of oursubmission to the\nEPIC-Kitchens Object Detection Challenge.Duck filling and mix-up techniques are\nfirstly introduced to augment the data and significantly improve the robustness\nof the proposed method. Then we propose GRE-FPN and Hard IoU-imbalance Sampler\nmethods to extract more representative global object features. To bridge the\ngap of category imbalance, Class Balance Sampling is utilized and greatly\nimproves the test results. Besides, some training and testing strategies are\nalso exploited, such as Stochastic Weight Averaging and multi-scale testing.\nExperimental results demonstrate that our approach can significantly improve\nthe mean Average Precision (mAP) of object detection on both the seen and\nunseen test sets of EPICKitchens.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 09:29:48 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Li", "Kaide", ""], ["Liao", "Bingyan", ""], ["Hu", "Laifeng", ""], ["Wang", "Yaonong", ""]]}, {"id": "2006.15555", "submitter": "Aviad Aberdam", "authors": "Aviad Aberdam, Dror Simon, Michael Elad", "title": "When and How Can Deep Generative Models be Inverted?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models (e.g. GANs and VAEs) have been developed quite\nextensively in recent years. Lately, there has been an increased interest in\nthe inversion of such a model, i.e. given a (possibly corrupted) signal, we\nwish to recover the latent vector that generated it. Building upon sparse\nrepresentation theory, we define conditions that are applicable to any\ninversion algorithm (gradient descent, deep encoder, etc.), under which such\ngenerative models are invertible with a unique solution. Importantly, the\nproposed analysis is applicable to any trained model, and does not depend on\nGaussian i.i.d. weights. Furthermore, we introduce two layer-wise inversion\npursuit algorithms for trained generative networks of arbitrary depth, and\naccompany these with recovery guarantees. Finally, we validate our theoretical\nresults numerically and show that our method outperforms gradient descent when\ninverting such generators, both for clean and corrupted signals.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 09:37:52 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Aberdam", "Aviad", ""], ["Simon", "Dror", ""], ["Elad", "Michael", ""]]}, {"id": "2006.15559", "submitter": "Emanuele Dalsasso", "authors": "Emanuele Dalsasso, Xiangli Yang, Lo\\\"ic Denis, Florence Tupin, Wen\n  Yang", "title": "SAR Image Despeckling by Deep Neural Networks: from a pre-trained model\n  to an end-to-end training strategy", "comments": "Article accepted for publication on Remote Sensing, MDPI. Notebook\n  with Colab compatibility is available at\n  https://gitlab.telecom-paris.fr/RING/SAR-CNN", "journal-ref": "Remote Sens. 2020, 12(16), 2636", "doi": "10.3390/rs12162636", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speckle reduction is a longstanding topic in synthetic aperture radar (SAR)\nimages. Many different schemes have been proposed for the restoration of\nintensity SAR images. Among the different possible approaches, methods based on\nconvolutional neural networks (CNNs) have recently shown to reach\nstate-of-the-art performance for SAR image restoration. CNN training requires\ngood training data: many pairs of speckle-free / speckle-corrupted images. This\nis an issue in SAR applications, given the inherent scarcity of speckle-free\nimages. To handle this problem, this paper analyzes different strategies one\ncan adopt, depending on the speckle removal task one wishes to perform and the\navailability of multitemporal stacks of SAR data. The first strategy applies a\nCNN model, trained to remove additive white Gaussian noise from natural images,\nto a recently proposed SAR speckle removal framework: MuLoG (MUlti-channel\nLOgarithm with Gaussian denoising). No training on SAR images is performed, the\nnetwork is readily applied to speckle reduction tasks. The second strategy\nconsiders a novel approach to construct a reliable dataset of speckle-free SAR\nimages necessary to train a CNN model. Finally, a hybrid approach is also\nanalyzed: the CNN used to remove additive white Gaussian noise is trained on\nspeckle-free SAR images. The proposed methods are compared to other\nstate-of-the-art speckle removal filters, to evaluate the quality of denoising\nand to discuss the pros and cons of the different strategies. Along with the\npaper, we make available the weights of the trained network to allow its usage\nby other researchers.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 09:47:31 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 07:55:03 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 07:04:39 GMT"}, {"version": "v4", "created": "Mon, 21 Sep 2020 12:24:43 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Dalsasso", "Emanuele", ""], ["Yang", "Xiangli", ""], ["Denis", "Lo\u00efc", ""], ["Tupin", "Florence", ""], ["Yang", "Wen", ""]]}, {"id": "2006.15560", "submitter": "Limin Wang", "authors": "Yin-Dong Zheng, Zhaoyang Liu, Tong Lu, Limin Wang", "title": "Dynamic Sampling Networks for Efficient Action Recognition in Videos", "comments": "To appear in IEEE Transaction on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2020.3007826", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing action recognition methods are mainly based on clip-level\nclassifiers such as two-stream CNNs or 3D CNNs, which are trained from the\nrandomly selected clips and applied to densely sampled clips during testing.\nHowever, this standard setting might be suboptimal for training classifiers and\nalso requires huge computational overhead when deployed in practice. To address\nthese issues, we propose a new framework for action recognition in videos,\ncalled {\\em Dynamic Sampling Networks} (DSN), by designing a dynamic sampling\nmodule to improve the discriminative power of learned clip-level classifiers\nand as well increase the inference efficiency during testing. Specifically, DSN\nis composed of a sampling module and a classification module, whose objective\nis to learn a sampling policy to on-the-fly select which clips to keep and\ntrain a clip-level classifier to perform action recognition based on these\nselected clips, respectively. In particular, given an input video, we train an\nobservation network in an associative reinforcement learning setting to\nmaximize the rewards of the selected clips with a correct prediction. We\nperform extensive experiments to study different aspects of the DSN framework\non four action recognition datasets: UCF101, HMDB51, THUMOS14, and ActivityNet\nv1.3. The experimental results demonstrate that DSN is able to greatly improve\nthe inference efficiency by only using less than half of the clips, which can\nstill obtain a slightly better or comparable recognition accuracy to the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 09:48:29 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Zheng", "Yin-Dong", ""], ["Liu", "Zhaoyang", ""], ["Lu", "Tong", ""], ["Wang", "Limin", ""]]}, {"id": "2006.15561", "submitter": "Yujin Chen", "authors": "Yujin Chen, Zhigang Tu, Di Kang, Ruizhi Chen, Linchao Bao, Zhengyou\n  Zhang, Junsong Yuan", "title": "Joint Hand-object 3D Reconstruction from a Single Image with\n  Cross-branch Feature Fusion", "comments": "Accepted by IEEE Transactions on Image Processing (TIP)", "journal-ref": null, "doi": "10.1109/TIP.2021.3068645", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate 3D reconstruction of the hand and object shape from a hand-object\nimage is important for understanding human-object interaction as well as human\ndaily activities. Different from bare hand pose estimation, hand-object\ninteraction poses a strong constraint on both the hand and its manipulated\nobject, which suggests that hand configuration may be crucial contextual\ninformation for the object, and vice versa. However, current approaches address\nthis task by training a two-branch network to reconstruct the hand and object\nseparately with little communication between the two branches. In this work, we\npropose to consider hand and object jointly in feature space and explore the\nreciprocity of the two branches. We extensively investigate cross-branch\nfeature fusion architectures with MLP or LSTM units. Among the investigated\narchitectures, a variant with LSTM units that enhances object feature with hand\nfeature shows the best performance gain. Moreover, we employ an auxiliary depth\nestimation module to augment the input RGB image with the estimated depth map,\nwhich further improves the reconstruction accuracy. Experiments conducted on\npublic datasets demonstrate that our approach significantly outperforms\nexisting approaches in terms of the reconstruction accuracy of objects.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 09:50:25 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 07:38:29 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Chen", "Yujin", ""], ["Tu", "Zhigang", ""], ["Kang", "Di", ""], ["Chen", "Ruizhi", ""], ["Bao", "Linchao", ""], ["Zhang", "Zhengyou", ""], ["Yuan", "Junsong", ""]]}, {"id": "2006.15573", "submitter": "Xinzhe Luo", "authors": "Xinzhe Luo and Xiahai Zhuang", "title": "MvMM-RegNet: A new image registration framework based on multivariate\n  mixture model and neural network estimation", "comments": "Accepted for publication at MICCAI 2020; Code is available from\n  https://github.com/xzluo97/MvMM-RegNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep-learning-based registration algorithms often exploit\nintensity-based similarity measures as the loss function, where dense\ncorrespondence between a pair of moving and fixed images is optimized through\nbackpropagation during training. However, intensity-based metrics can be\nmisleading when the assumption of intensity class correspondence is violated,\nespecially in cross-modality or contrast-enhanced images. Moreover, existing\nlearning-based registration methods are predominantly applicable to pairwise\nregistration and are rarely extended to groupwise registration or simultaneous\nregistration with multiple images. In this paper, we propose a new image\nregistration framework based on multivariate mixture model (MvMM) and neural\nnetwork estimation. A generative model consolidating both appearance and\nanatomical information is established to derive a novel loss function capable\nof implementing groupwise registration. We highlight the versatility of the\nproposed framework for various applications on multimodal cardiac images,\nincluding single-atlas-based segmentation (SAS) via pairwise registration and\nmulti-atlas segmentation (MAS) unified by groupwise registration. We evaluated\nperformance on two publicly available datasets, i.e. MM-WHS-2017 and\nMS-CMRSeg-2019. The results show that the proposed framework achieved an\naverage Dice score of $0.871\\pm 0.025$ for whole-heart segmentation on MR\nimages and $0.783\\pm 0.082$ for myocardium segmentation on LGE MR images.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 11:19:15 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 04:38:16 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Luo", "Xinzhe", ""], ["Zhuang", "Xiahai", ""]]}, {"id": "2006.15576", "submitter": "Huixin Miao", "authors": "Junqi Lin, Huixin Miao, Junjie Cao, Zhixun Su, Risheng Liu", "title": "SMPR: Single-Stage Multi-Person Pose Regression", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing multi-person pose estimators can be roughly divided into two-stage\napproaches (top-down and bottom-up approaches) and one-stage approaches. The\ntwo-stage methods either suffer high computational redundancy for additional\nperson detectors or group keypoints heuristically after predicting all the\ninstance-free keypoints. The recently proposed single-stage methods do not rely\non the above two extra stages but have lower performance than the latest\nbottom-up approaches. In this work, a novel single-stage multi-person pose\nregression, termed SMPR, is presented. It follows the paradigm of dense\nprediction and predicts instance-aware keypoints from every location. Besides\nfeature aggregation, we propose better strategies to define positive pose\nhypotheses for training which all play an important role in dense pose\nestimation. The network also learns the scores of estimated poses. The pose\nscoring strategy further improves the pose estimation performance by\nprioritizing superior poses during non-maximum suppression (NMS). We show that\nour method not only outperforms existing single-stage methods and but also be\ncompetitive with the latest bottom-up methods, with 70.2 AP and 77.5 AP75 on\nthe COCO test-dev pose benchmark. Code is available at\nhttps://github.com/cmdi-dlut/SMPR.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 11:26:38 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 15:57:52 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lin", "Junqi", ""], ["Miao", "Huixin", ""], ["Cao", "Junjie", ""], ["Su", "Zhixun", ""], ["Liu", "Risheng", ""]]}, {"id": "2006.15578", "submitter": "Siyu Liu", "authors": "Siyu Liu, Wei Dai, Craig Engstrom, Jurgen Fripp, Peter B. Greer,\n  Stuart Crozier, Jason A. Dowling and Shekhar S. Chandra", "title": "Fabric Image Representation Encoding Networks for Large-scale 3D Medical\n  Image Analysis", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are parameterised by weights that encode feature\nrepresentations, whose performance is dictated through generalisation by using\nlarge-scale feature-rich datasets. The lack of large-scale labelled 3D medical\nimaging datasets restrict constructing such generalised networks. In this work,\na novel 3D segmentation network, Fabric Image Representation Networks\n(FIRENet), is proposed to extract and encode generalisable feature\nrepresentations from multiple medical image datasets in a large-scale manner.\nFIRENet learns image specific feature representations by way of 3D fabric\nnetwork architecture that contains exponential number of sub-architectures to\nhandle various protocols and coverage of anatomical regions and structures. The\nfabric network uses Atrous Spatial Pyramid Pooling (ASPP) extended to 3D to\nextract local and image-level features at a fine selection of scales. The\nfabric is constructed with weighted edges allowing the learnt features to\ndynamically adapt to the training data at an architecture level. Conditional\npadding modules, which are integrated into the network to reinsert voxels\ndiscarded by feature pooling, allow the network to inherently process\ndifferent-size images at their original resolutions. FIRENet was trained for\nfeature learning via automated semantic segmentation of pelvic structures and\nobtained a state-of-the-art median DSC score of 0.867. FIRENet was also\nsimultaneously trained on MR (Magnatic Resonance) images acquired from 3D\nexaminations of musculoskeletal elements in the (hip, knee, shoulder) joints\nand a public OAI knee dataset to perform automated segmentation of bone across\nanatomy. Transfer learning was used to show that the features learnt through\nthe pelvic segmentation helped achieve improved mean DSC scores of 0.962,\n0.963, 0.945 and 0.986 for automated segmentation of bone across datasets.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 11:35:23 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 05:35:51 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Liu", "Siyu", ""], ["Dai", "Wei", ""], ["Engstrom", "Craig", ""], ["Fripp", "Jurgen", ""], ["Greer", "Peter B.", ""], ["Crozier", "Stuart", ""], ["Dowling", "Jason A.", ""], ["Chandra", "Shekhar S.", ""]]}, {"id": "2006.15588", "submitter": "Peng Fu", "authors": "Xiaoguang Li, Peng Fu, Hongxia Yin, ZhenChang Wang, Li Zhuo, Hui Zhang", "title": "A lateral semicircular canal segmentation based geometric calibration\n  for human temporal bone CT Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed Tomography (CT) of the temporal bone has become an important method\nfor diagnosing ear diseases. Due to the different posture of the subject and\nthe settings of CT scanners, the CT image of the human temporal bone should be\ngeometrically calibrated to ensure the symmetry of the bilateral anatomical\nstructure. Manual calibration is a time-consuming task for radiologists and an\nimportant pre-processing step for further computer-aided CT analysis. We\npropose an automatic calibration algorithm for temporal bone CT images. The\nlateral semicircular canals (LSCs) are segmented as anchors at first. Then, we\ndefine a standard 3D coordinate system. The key step is the LSC segmentation.\nWe design a novel 3D LSC segmentation encoder-decoder network, which introduces\na 3D dilated convolution and a multi-pooling scheme for feature fusion in the\nencoding stage. The experimental results show that our LSC segmentation network\nachieved a higher segmentation accuracy. Our proposed method can help to\nperform calibration of temporal bone CT images efficiently.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 12:36:08 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Li", "Xiaoguang", ""], ["Fu", "Peng", ""], ["Yin", "Hongxia", ""], ["Wang", "ZhenChang", ""], ["Zhuo", "Li", ""], ["Zhang", "Hui", ""]]}, {"id": "2006.15607", "submitter": "Youngwan Lee", "authors": "Youngwan Lee, Joong-won Hwang, Hyung-Il Kim, Kimin Yun, Yongjin Kwon", "title": "Localization Uncertainty Estimation for Anchor-Free Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since many safety-critical systems, such as surgical robots and autonomous\ndriving cars, are in unstable environments with sensor noise and incomplete\ndata, it is desirable for object detectors to take into account the confidence\nof localization prediction. There are three limitations of the prior\nuncertainty estimation methods for anchor-based object detection. 1) They model\nthe uncertainty based on object properties having different characteristics,\nsuch as location (center point) and scale (width, height). 2) they model a box\noffset and ground-truth as Gaussian distribution and Dirac delta distribution,\nwhich leads to the model misspecification problem. Because the Dirac delta\ndistribution is not exactly represented as Gaussian, i.e., for any $\\mu$ and\n$\\Sigma$. 3) Since anchor-based methods are sensitive to hyper-parameters of\nanchor, the localization uncertainty modeling is also sensitive to these\nparameters. Therefore, we propose a new localization uncertainty estimation\nmethod called Gaussian-FCOS for anchor-free object detection. Our method\ncaptures the uncertainty based on four directions of box offsets~(left, right,\ntop, bottom) that have similar properties, which enables to capture which\ndirection is uncertain and provide a quantitative value in range~[0, 1]. To\nthis end, we design a new uncertainty loss, negative power log-likelihood loss,\nto measure uncertainty by weighting IoU to the likelihood loss, which\nalleviates the model misspecification problem. Experiments on COCO datasets\ndemonstrate that our Gaussian-FCOS reduces false positives and finds more\nmissing-objects by mitigating over-confidence scores with the estimated\nuncertainty. We hope Gaussian-FCOS serves as a crucial component for the\nreliability-required task.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 13:49:30 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 09:51:51 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 00:24:14 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 00:18:43 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lee", "Youngwan", ""], ["Hwang", "Joong-won", ""], ["Kim", "Hyung-Il", ""], ["Yun", "Kimin", ""], ["Kwon", "Yongjin", ""]]}, {"id": "2006.15617", "submitter": "Zhihao Liu", "authors": "Zhihao Liu, Hui Yin, Yang Mi, Mengyang Pu, and Song Wang", "title": "Shadow Removal by a Lightness-Guided Network with Training on Unpaired\n  Data", "comments": "Submitted to IEEE TIP", "journal-ref": null, "doi": "10.1109/TIP.2020.3048677", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shadow removal can significantly improve the image visual quality and has\nmany applications in computer vision. Deep learning methods based on CNNs have\nbecome the most effective approach for shadow removal by training on either\npaired data, where both the shadow and underlying shadow-free versions of an\nimage are known, or unpaired data, where shadow and shadow-free training images\nare totally different with no correspondence. In practice, CNN training on\nunpaired data is more preferred given the easiness of training data collection.\nIn this paper, we present a new Lightness-Guided Shadow Removal Network\n(LG-ShadowNet) for shadow removal by training on unpaired data. In this method,\nwe first train a CNN module to compensate for the lightness and then train a\nsecond CNN module with the guidance of lightness information from the first CNN\nmodule for final shadow removal. We also introduce a loss function to further\nutilise the colour prior of existing data. Extensive experiments on widely used\nISTD, adjusted ISTD and USR datasets demonstrate that the proposed method\noutperforms the state-of-the-art methods with training on unpaired data.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 14:31:18 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Liu", "Zhihao", ""], ["Yin", "Hui", ""], ["Mi", "Yang", ""], ["Pu", "Mengyang", ""], ["Wang", "Song", ""]]}, {"id": "2006.15618", "submitter": "Rui Gong", "authors": "Rui Gong, Dengxin Dai, Yuhua Chen, Wen Li, Luc Van Gool", "title": "Analogical Image Translation for Fog Generation", "comments": "18 pages, 9 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation is to map images from a given \\emph{style} to\nanother given \\emph{style}. While exceptionally successful, current methods\nassume the availability of training images in both source and target domains,\nwhich does not always hold in practice. Inspired by humans' reasoning\ncapability of analogy, we propose analogical image translation (AIT). Given\nimages of two styles in the source domain: $\\mathcal{A}$ and\n$\\mathcal{A}^\\prime$, along with images $\\mathcal{B}$ of the first style in the\ntarget domain, learn a model to translate $\\mathcal{B}$ to $\\mathcal{B}^\\prime$\nin the target domain, such that $\\mathcal{A}:\\mathcal{A}^\\prime\n::\\mathcal{B}:\\mathcal{B}^\\prime$. AIT is especially useful for translation\nscenarios in which training data of one style is hard to obtain but training\ndata of the same two styles in another domain is available. For instance, in\nthe case from normal conditions to extreme, rare conditions, obtaining real\ntraining images for the latter case is challenging but obtaining synthetic data\nfor both cases is relatively easy. In this work, we are interested in adding\nadverse weather effects, more specifically fog effects, to images taken in\nclear weather. To circumvent the challenge of collecting real foggy images, AIT\nlearns with synthetic clear-weather images, synthetic foggy images and real\nclear-weather images to add fog effects onto real clear-weather images without\nseeing any real foggy images during training. AIT achieves this zero-shot image\ntranslation capability by coupling a supervised training scheme in the\nsynthetic domain, a cycle consistency strategy in the real domain, an\nadversarial training scheme between the two domains, and a novel network\ndesign. Experiments show the effectiveness of our method for zero-short image\ntranslation and its benefit for downstream tasks such as semantic foggy scene\nunderstanding.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 14:33:31 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Gong", "Rui", ""], ["Dai", "Dengxin", ""], ["Chen", "Yuhua", ""], ["Li", "Wen", ""], ["Van Gool", "Luc", ""]]}, {"id": "2006.15619", "submitter": "Bo Liu", "authors": "Brian Liu, Xianchao Xu, Yu Zhang", "title": "Offline Handwritten Chinese Text Recognition with Convolutional Neural\n  Networks", "comments": "6 pages, 5 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning based methods have been dominating the text recognition tasks\nin different and multilingual scenarios. The offline handwritten Chinese text\nrecognition (HCTR) is one of the most challenging tasks because it involves\nthousands of characters, variant writing styles and complex data collection\nprocess. Recently, the recurrent-free architectures for text recognition\nappears to be competitive as its highly parallelism and comparable results. In\nthis paper, we build the models using only the convolutional neural networks\nand use CTC as the loss function. To reduce the overfitting, we apply dropout\nafter each max-pooling layer and with extreme high rate on the last one before\nthe linear layer. The CASIA-HWDB database is selected to tune and evaluate the\nproposed models. With the existing text samples as templates, we randomly\nchoose isolated character samples to synthesis more text samples for training.\nWe finally achieve 6.81% character error rate (CER) on the ICDAR 2013\ncompetition set, which is the best published result without language model\ncorrection.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 14:34:38 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Liu", "Brian", ""], ["Xu", "Xianchao", ""], ["Zhang", "Yu", ""]]}, {"id": "2006.15631", "submitter": "Jialin Wu", "authors": "Jialin Wu, Liyan Chen and Raymond J. Mooney", "title": "Improving VQA and its Explanations \\\\ by Comparing Competing\n  Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent state-of-the-art Visual Question Answering (VQA) systems are\nopaque black boxes that are only trained to fit the answer distribution given\nthe question and visual content. As a result, these systems frequently take\nshortcuts, focusing on simple visual concepts or question priors. This\nphenomenon becomes more problematic as the questions become complex that\nrequires more reasoning and commonsense knowledge. To address this issue, we\npresent a novel framework that uses explanations for competing answers to help\nVQA systems select the correct answer. By training on human textual\nexplanations, our framework builds better representations for the questions and\nvisual content, and then reweights confidences in the answer candidates using\neither generated or retrieved explanations from the training set. We evaluate\nour framework on the VQA-X dataset, which has more difficult questions with\nhuman explanations, achieving new state-of-the-art results on both VQA and its\nexplanations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 15:11:40 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Wu", "Jialin", ""], ["Chen", "Liyan", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "2006.15657", "submitter": "Dave Epstein", "authors": "Dave Epstein and Carl Vondrick", "title": "Learning Goals from Failure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework that predicts the goals behind observable human\naction in video. Motivated by evidence in developmental psychology, we leverage\nvideo of unintentional action to learn video representations of goals without\ndirect supervision. Our approach models videos as contextual trajectories that\nrepresent both low-level motion and high-level action features. Experiments and\nvisualizations show our trained model is able to predict the underlying goals\nin video of unintentional action. We also propose a method to \"automatically\ncorrect\" unintentional action by leveraging gradient signals of our model to\nadjust latent trajectories. Although the model is trained with minimal\nsupervision, it is competitive with or outperforms baselines trained on large\n(supervised) datasets of successfully executed goals, showing that observing\nunintentional action is crucial to learning about goals in video. Project page:\nhttps://aha.cs.columbia.edu/\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 17:16:49 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 01:44:08 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Epstein", "Dave", ""], ["Vondrick", "Carl", ""]]}, {"id": "2006.15669", "submitter": "Nurislam Tursynbek", "authors": "Nurislam Tursynbek, Aleksandr Petiushko, and Ivan Oseledets", "title": "Geometry-Inspired Top-k Adversarial Perturbations", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are vulnerable to adversarial examples, which endangers\ntheir usage in real-world applications. The main target of existing adversarial\nperturbations is primarily limited to change the correct Top-1 predicted class\nby the incorrect one, which does not intend changing the Top-$k$ prediction.\nHowever, in many real-world scenarios, especially dealing with digital images,\nTop-$k$ predictions are more important. In this work, we propose a simple yet\neffective geometry-inspired method of computing Top-$k$ adversarial examples\nfor any $k$. We evaluate its effectiveness and efficiency by comparing it with\nother adversarial example crafting techniques. Moreover, based on this method,\nwe propose Top-$k$ Universal Adversarial Perturbations, image-agnostic tiny\nperturbations that cause true class to be absent among the Top-$k$ prediction\nfor most inputs in the dataset. We experimentally show that our approach\noutperforms baseline methods and even improves existing techniques of\ngenerating Universal Adversarial Perturbations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 18:05:57 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 09:25:38 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 03:54:29 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Tursynbek", "Nurislam", ""], ["Petiushko", "Aleksandr", ""], ["Oseledets", "Ivan", ""]]}, {"id": "2006.15691", "submitter": "Yuankai Huo", "authors": "Yuankai Huo, Jinzheng Cai, Chi-Tung Cheng, Ashwin Raju, Ke Yan,\n  Bennett A. Landman, Jing Xiao, Le Lu, Chien-Hung Liao, Adam P. Harrison", "title": "Harvesting, Detecting, and Characterizing Liver Lesions from Large-scale\n  Multi-phase CT Data via Deep Dynamic Texture Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-invasive radiological-based lesion characterization and identification,\ne.g., to differentiate cancer subtypes, has long been a major aim to enhance\noncological diagnosis and treatment procedures. Here we study a specific\npopulation of human subjects, with the hope of reducing the need for invasive\nsurgical biopsies of liver cancer patients, which can cause many harmful\nside-effects. To this end, we propose a fully-automated and multi-stage liver\ntumor characterization framework designed for dynamic contrast computed\ntomography (CT). Our system comprises four sequential processes of tumor\nproposal detection, tumor harvesting, primary tumor site selection, and deep\ntexture-based tumor characterization. Our main contributions are that, (1) we\npropose a 3D non-isotropic anchor-free detection method for liver lesions; (2)\nwe present and validate spatially adaptivedeep texture (SaDT) learning, which\nallows for more precise characterization of liver lesions; (3) using a\nsemi-automatic process, we bootstrap off of 200 gold standard annotations to\ncurate another 1001 patients. Experimental evaluations demonstrate that our new\ndata curation strategy, combined with the SaDT deep dynamic texture analysis,\ncan effectively improve the mean F1 scores by >8.6% compared with baselines, in\ndifferentiating four major liver lesion types. Our F1 score of (hepatocellular\ncarcinoma versus remaining subclasses) is 0.763, which is higher than reported\nhuman observer performance using dynamic CT and comparable to an advanced\nmagnetic resonance imagery protocol. Apart from demonstrating the benefits of\nour data curation approach and physician-inspired workflow, these results also\nindicate that analyzing texture features, instead of standard object-based\nanalysis, is a promising strategy for lesion differentiation.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 19:55:34 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 16:51:39 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Huo", "Yuankai", ""], ["Cai", "Jinzheng", ""], ["Cheng", "Chi-Tung", ""], ["Raju", "Ashwin", ""], ["Yan", "Ke", ""], ["Landman", "Bennett A.", ""], ["Xiao", "Jing", ""], ["Lu", "Le", ""], ["Liao", "Chien-Hung", ""], ["Harrison", "Adam P.", ""]]}, {"id": "2006.15693", "submitter": "Fernando P\\'erez-Garc\\'ia", "authors": "Fernando P\\'erez-Garc\\'ia (1 and 2), Roman Rodionov (3 and 4), Ali\n  Alim-Marvasti (1, 3 and 4), Rachel Sparks (2), John S. Duncan (3 and 4), and\n  S\\'ebastien Ourselin (2) ((1) Wellcome EPSRC Centre for Interventional and\n  Surgical Sciences (WEISS), University College London, (2) School of\n  Biomedical Engineering and Imaging Sciences (BMEIS), King's College London,\n  (3) Department of Clinical and Experimental Epilepsy, UCL Queen Square\n  Institute of Neurology, (4) National Hospital for Neurology and Neurosurgery,\n  Queen Square, London, UK)", "title": "Simulation of Brain Resection for Cavity Segmentation Using\n  Self-Supervised and Semi-Supervised Learning", "comments": "13 pages, 6 figures, accepted at the International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI) 2020", "journal-ref": "Medical Image Computing and Computer Assisted Intervention -\n  MICCAI 2020. Lecture Notes in Computer Science, vol 12263. Springer, Cham", "doi": "10.1007/978-3-030-59716-0_12", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Resective surgery may be curative for drug-resistant focal epilepsy, but only\n40% to 70% of patients achieve seizure freedom after surgery. Retrospective\nquantitative analysis could elucidate patterns in resected structures and\npatient outcomes to improve resective surgery. However, the resection cavity\nmust first be segmented on the postoperative MR image. Convolutional neural\nnetworks (CNNs) are the state-of-the-art image segmentation technique, but\nrequire large amounts of annotated data for training. Annotation of medical\nimages is a time-consuming process requiring highly-trained raters, and often\nsuffering from high inter-rater variability. Self-supervised learning can be\nused to generate training instances from unlabeled data. We developed an\nalgorithm to simulate resections on preoperative MR images. We curated a new\ndataset, EPISURG, comprising 431 postoperative and 269 preoperative MR images\nfrom 431 patients who underwent resective surgery. In addition to EPISURG, we\nused three public datasets comprising 1813 preoperative MR images for training.\nWe trained a 3D CNN on artificially resected images created on the fly during\ntraining, using images from 1) EPISURG, 2) public datasets and 3) both. To\nevaluate trained models, we calculate Dice score (DSC) between model\nsegmentations and 200 manual annotations performed by three human raters. The\nmodel trained on data with manual annotations obtained a median (interquartile\nrange) DSC of 65.3 (30.6). The DSC of our best-performing model, trained with\nno manual annotations, is 81.7 (14.2). For comparison, inter-rater agreement\nbetween human annotators was 84.0 (9.9). We demonstrate a training method for\nCNNs using simulated resection cavities that can accurately segment real\nresection cavities, without manual annotations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 20:03:39 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["P\u00e9rez-Garc\u00eda", "Fernando", "", "1 and 2"], ["Rodionov", "Roman", "", "3 and 4"], ["Alim-Marvasti", "Ali", "", "1, 3 and 4"], ["Sparks", "Rachel", "", "3 and 4"], ["Duncan", "John S.", "", "3 and 4"], ["Ourselin", "S\u00e9bastien", ""]]}, {"id": "2006.15699", "submitter": "Patrick Geneva", "authors": "Kevin Eckenhoff, Patrick Geneva, and Guoquan Huang", "title": "MIMC-VINS: A Versatile and Resilient Multi-IMU Multi-Camera\n  Visual-Inertial Navigation System", "comments": "20 pages, 10 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As cameras and inertial sensors are becoming ubiquitous in mobile devices and\nrobots, it holds great potential to design visual-inertial navigation systems\n(VINS) for efficient versatile 3D motion tracking which utilize any (multiple)\navailable cameras and inertial measurement units (IMUs) and are resilient to\nsensor failures or measurement depletion. To this end, rather than the standard\nVINS paradigm using a minimal sensing suite of a single camera and IMU, in this\npaper we design a real-time consistent multi-IMU multi-camera (MIMC)-VINS\nestimator that is able to seamlessly fuse multi-modal information from an\narbitrary number of uncalibrated cameras and IMUs. Within an efficient\nmulti-state constraint Kalman filter (MSCKF) framework, the proposed MIMC-VINS\nalgorithm optimally fuses asynchronous measurements from all sensors, while\nproviding smooth, uninterrupted, and accurate 3D motion tracking even if some\nsensors fail. The key idea of the proposed MIMC-VINS is to perform high-order\non-manifold state interpolation to efficiently process all available visual\nmeasurements without increasing the computational burden due to estimating\nadditional sensors' poses at asynchronous imaging times. In order to fuse the\ninformation from multiple IMUs, we propagate a joint system consisting of all\nIMU states while enforcing rigid-body constraints between the IMUs during the\nfilter update stage. Lastly, we estimate online both spatiotemporal extrinsic\nand visual intrinsic parameters to make our system robust to errors in prior\nsensor calibration. The proposed system is extensively validated in both\nMonte-Carlo simulations and real-world experiments.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 20:16:08 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Eckenhoff", "Kevin", ""], ["Geneva", "Patrick", ""], ["Huang", "Guoquan", ""]]}, {"id": "2006.15710", "submitter": "Hanchao Yu", "authors": "Hanchao Yu, Xiao Chen, Humphrey Shi, Terrence Chen, Thomas S. Huang,\n  Shanhui Sun", "title": "Motion Pyramid Networks for Accurate and Efficient Cardiac Motion\n  Estimation", "comments": "Accepted by MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac motion estimation plays a key role in MRI cardiac feature tracking\nand function assessment such as myocardium strain. In this paper, we propose\nMotion Pyramid Networks, a novel deep learning-based approach for accurate and\nefficient cardiac motion estimation. We predict and fuse a pyramid of motion\nfields from multiple scales of feature representations to generate a more\nrefined motion field. We then use a novel cyclic teacher-student training\nstrategy to make the inference end-to-end and further improve the tracking\nperformance. Our teacher model provides more accurate motion estimation as\nsupervision through progressive motion compensations. Our student model learns\nfrom the teacher model to estimate motion in a single step while maintaining\naccuracy. The teacher-student knowledge distillation is performed in a cyclic\nway for a further performance boost. Our proposed method outperforms a strong\nbaseline model on two public available clinical datasets significantly,\nevaluated by a variety of metrics and the inference time. New evaluation\nmetrics are also proposed to represent errors in a clinically meaningful\nmanner.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 21:03:19 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 18:13:20 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 23:13:18 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Yu", "Hanchao", ""], ["Chen", "Xiao", ""], ["Shi", "Humphrey", ""], ["Chen", "Terrence", ""], ["Huang", "Thomas S.", ""], ["Sun", "Shanhui", ""]]}, {"id": "2006.15713", "submitter": "Saad Nadeem", "authors": "Sadegh R Alam, Tianfang Li, Pengpeng Zhang, Si-Yuan Zhang, and Saad\n  Nadeem", "title": "Generalizable Cone Beam CT Esophagus Segmentation Using Physics-Based\n  Data Augmentation", "comments": "Accepted to Physics in Medicine & Biology 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated segmentation of esophagus is critical in image guided/adaptive\nradiotherapy of lung cancer to minimize radiation-induced toxicities such as\nacute esophagitis. We developed a semantic physics-based data augmentation\nmethod for segmenting esophagus in both planning CT (pCT) and cone-beam CT\n(CBCT) using 3D convolutional neural networks. 191 cases with their pCT and\nCBCTs from four independent datasets were used to train a modified 3D-Unet\narchitecture with a multi-objective loss function specifically designed for\nsoft-tissue organs such as esophagus. Scatter artifacts and noise were\nextracted from week 1 CBCTs using power law adaptive histogram equalization\nmethod and induced to the corresponding pCT followed by reconstruction using\nCBCT reconstruction parameters. Moreover, we leverage physics-based artifact\ninduced pCTs to drive the esophagus segmentation in real weekly CBCTs.\nSegmentations were evaluated using geometric Dice and Hausdorff distance as\nwell as dosimetrically using mean esophagus dose and D5cc. Due to the\nphysics-based data augmentation, our model trained just on the synthetic CBCTs\nwas robust and generalizable enough to also produce state-of-the-art results on\nthe pCTs and CBCTs, achieving 0.81 and 0.74 Dice overlap. Our physics-based\ndata augmentation spans the realistic noise/artifact spectrum across patient\nCBCT/pCT data and can generalize well across modalities with the potential to\nimprove the accuracy of treatment setup and response analysis.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 21:12:09 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 22:33:15 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Alam", "Sadegh R", ""], ["Li", "Tianfang", ""], ["Zhang", "Pengpeng", ""], ["Zhang", "Si-Yuan", ""], ["Nadeem", "Saad", ""]]}, {"id": "2006.15731", "submitter": "Pavel Tokmakov", "authors": "Pavel Tokmakov, Martial Hebert, Cordelia Schmid", "title": "Unsupervised Learning of Video Representations via Dense Trajectory\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of unsupervised learning of representations for\naction recognition in videos. Previous works proposed to utilize future\nprediction, or other domain-specific objectives to train a network, but\nachieved only limited success. In contrast, in the relevant field of image\nrepresentation learning, simpler, discrimination-based methods have recently\nbridged the gap to fully-supervised performance. We first propose to adapt two\ntop performing objectives in this class - instance recognition and local\naggregation, to the video domain. In particular, the latter approach iterates\nbetween clustering the videos in the feature space of a network and updating it\nto respect the cluster with a non-parametric classification loss. We observe\npromising performance, but qualitative analysis shows that the learned\nrepresentations fail to capture motion patterns, grouping the videos based on\nappearance. To mitigate this issue, we turn to the heuristic-based IDT\ndescriptors, that were manually designed to encode motion patterns in videos.\nWe form the clusters in the IDT space, using these descriptors as a an\nunsupervised prior in the iterative local aggregation algorithm. Our\nexperiments demonstrates that this approach outperform prior work on UCF101 and\nHMDB51 action recognition benchmarks. We also qualitatively analyze the learned\nrepresentations and show that they successfully capture video dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 22:23:03 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Tokmakov", "Pavel", ""], ["Hebert", "Martial", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2006.15736", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Fakhri Karray, Mark Crowley", "title": "Roweisposes, Including Eigenposes, Supervised Eigenposes, and\n  Fisherposes, for 3D Action Recognition", "comments": "key-words: Roweisposes, Roweis discriminant analysis, Fisherposes,\n  eigenposes, supervised eigenposes, action recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition is one of the important fields of computer vision\nand machine learning. Although various methods have been proposed for 3D action\nrecognition, some of which are basic and some use deep learning, the need of\nbasic methods based on generalized eigenvalue problem is sensed for action\nrecognition. This need is especially sensed because of having similar basic\nmethods in the field of face recognition such as eigenfaces and Fisherfaces. In\nthis paper, we propose Roweisposes which uses Roweis discriminant analysis for\ngeneralized subspace learning. This method includes Fisherposes, eigenposes,\nsupervised eigenposes, and double supervised eigenposes as its special cases.\nRoweisposes is a family of infinite number of action recongition methods which\nlearn a discriminative subspace for embedding the body poses. Experiments on\nthe TST, UTKinect, and UCFKinect datasets verify the effectiveness of the\nproposed method for action recognition.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 22:46:12 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2006.15754", "submitter": "Eric Heiden", "authors": "Ali-akbar Agha-mohammadi, Eric Heiden, Karol Hausman, Gaurav S.\n  Sukhatme", "title": "Confidence-rich grid mapping", "comments": "Published at International Journal of Robotics Research (IJRR) 2019\n  (https://journals.sagepub.com/doi/10.1177/0278364919839762)", "journal-ref": "The International Journal of Robotics Research, 38(12-13),\n  1352-1374 (2019)", "doi": "10.1177/0278364919839762", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing the environment is a fundamental task in enabling robots to act\nautonomously in unknown environments. In this work, we present confidence-rich\nmapping (CRM), a new algorithm for spatial grid-based mapping of the 3D\nenvironment. CRM augments the occupancy level at each voxel by its confidence\nvalue. By explicitly storing and evolving confidence values using the CRM\nfilter, CRM extends traditional grid mapping in three ways: first, it partially\nmaintains the probabilistic dependence among voxels. Second, it relaxes the\nneed for hand-engineering an inverse sensor model and proposes the concept of\nsensor cause model that can be derived in a principled manner from the forward\nsensor model. Third, and most importantly, it provides consistent confidence\nvalues over the occupancy estimation that can be reliably used in collision\nrisk evaluation and motion planning. CRM runs online and enables mapping\nenvironments where voxels might be partially occupied. We demonstrate the\nperformance of the method on various datasets and environments in simulation\nand on physical systems. We show in real-world experiments that, in addition to\nachieving maps that are more accurate than traditional methods, the proposed\nfiltering scheme demonstrates a much higher level of consistency between its\nerror and the reported confidence, hence, enabling a more reliable collision\nrisk evaluation for motion planning.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 00:21:30 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Agha-mohammadi", "Ali-akbar", ""], ["Heiden", "Eric", ""], ["Hausman", "Karol", ""], ["Sukhatme", "Gaurav S.", ""]]}, {"id": "2006.15759", "submitter": "Alexander Wong", "authors": "James Ren Hou Lee, Linda Wang, and Alexander Wong", "title": "EmotionNet Nano: An Efficient Deep Convolutional Neural Network Design\n  for Real-time Facial Expression Recognition", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent advances in deep learning have led to significant improvements\nin facial expression classification (FEC), a major challenge that remains a\nbottleneck for the widespread deployment of such systems is their high\narchitectural and computational complexities. This is especially challenging\ngiven the operational requirements of various FEC applications, such as safety,\nmarketing, learning, and assistive living, where real-time requirements on\nlow-cost embedded devices is desired. Motivated by this need for a compact, low\nlatency, yet accurate system capable of performing FEC in real-time on low-cost\nembedded devices, this study proposes EmotionNet Nano, an efficient deep\nconvolutional neural network created through a human-machine collaborative\ndesign strategy, where human experience is combined with machine meticulousness\nand speed in order to craft a deep neural network design catered towards\nreal-time embedded usage. Two different variants of EmotionNet Nano are\npresented, each with a different trade-off between architectural and\ncomputational complexity and accuracy. Experimental results using the CK+\nfacial expression benchmark dataset demonstrate that the proposed EmotionNet\nNano networks demonstrated accuracies comparable to state-of-the-art in FEC\nnetworks, while requiring significantly fewer parameters (e.g., 23$\\times$\nfewer at a higher accuracy). Furthermore, we demonstrate that the proposed\nEmotionNet Nano networks achieved real-time inference speeds (e.g. $>25$ FPS\nand $>70$ FPS at 15W and 30W, respectively) and high energy efficiency (e.g.\n$>1.7$ images/sec/watt at 15W) on an ARM embedded processor, thus further\nillustrating the efficacy of EmotionNet Nano for deployment on embedded\ndevices.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 00:48:05 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Lee", "James Ren Hou", ""], ["Wang", "Linda", ""], ["Wong", "Alexander", ""]]}, {"id": "2006.15771", "submitter": "Shengjie Liu", "authors": "Sheng-Jie Liu, Haowen Luo, Qian Shi", "title": "Active Ensemble Deep Learning for Polarimetric Synthetic Aperture Radar\n  Image Classification", "comments": "Accepted by GRSL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning has achieved great success in image classification\ntasks, its performance is subject to the quantity and quality of training\nsamples. For classification of polarimetric synthetic aperture radar (PolSAR)\nimages, it is nearly impossible to annotate the images from visual\ninterpretation. Therefore, it is urgent for remote sensing scientists to\ndevelop new techniques for PolSAR image classification under the condition of\nvery few training samples. In this letter, we take the advantage of active\nlearning and propose active ensemble deep learning (AEDL) for PolSAR image\nclassification. We first show that only 35\\% of the predicted labels of a deep\nlearning model's snapshots near its convergence were exactly the same. The\ndisagreement between snapshots is non-negligible. From the perspective of\nmultiview learning, the snapshots together serve as a good committee to\nevaluate the importance of unlabeled instances. Using the snapshots committee\nto give out the informativeness of unlabeled data, the proposed AEDL achieved\nbetter performance on two real PolSAR images compared with standard active\nlearning strategies. It achieved the same classification accuracy with only 86%\nand 55% of the training samples compared with breaking ties active learning and\nrandom selection for the Flevoland dataset.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 01:40:54 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Liu", "Sheng-Jie", ""], ["Luo", "Haowen", ""], ["Shi", "Qian", ""]]}, {"id": "2006.15789", "submitter": "Long Chen", "authors": "Long Chen, Lei Tong, Feixiang Zhou, Zheheng Jiang, Zhenyang Li, Jialin\n  Lv, Junyu Dong, and Huiyu Zhou", "title": "A Benchmark dataset for both underwater image enhancement and underwater\n  object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater image enhancement is such an important vision task due to its\nsignificance in marine engineering and aquatic robot. It is usually work as a\npre-processing step to improve the performance of high level vision tasks such\nas underwater object detection. Even though many previous works show the\nunderwater image enhancement algorithms can boost the detection accuracy of the\ndetectors, no work specially focus on investigating the relationship between\nthese two tasks. This is mainly because existing underwater datasets lack\neither bounding box annotations or high quality reference images, based on\nwhich detection accuracy or image quality assessment metrics are calculated. To\ninvestigate how the underwater image enhancement methods influence the\nfollowing underwater object detection tasks, in this paper, we provide a\nlarge-scale underwater object detection dataset with both bounding box\nannotations and high quality reference images, namely OUC dataset. The OUC\ndataset provides a platform for researchers to comprehensive study the\ninfluence of underwater image enhancement algorithms on the underwater object\ndetection task.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 03:12:50 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Chen", "Long", ""], ["Tong", "Lei", ""], ["Zhou", "Feixiang", ""], ["Jiang", "Zheheng", ""], ["Li", "Zhenyang", ""], ["Lv", "Jialin", ""], ["Dong", "Junyu", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2006.15833", "submitter": "Jung Hee Kim", "authors": "Jung Hee Kim, Siyeong Lee, and Suk-Ju Kang", "title": "End-to-End Differentiable Learning to HDR Image Synthesis for\n  Multi-exposure Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, high dynamic range (HDR) image reconstruction based on the multiple\nexposure stack from a given single exposure utilizes a deep learning framework\nto generate high-quality HDR images. These conventional networks focus on the\nexposure transfer task to reconstruct the multi-exposure stack. Therefore, they\noften fail to fuse the multi-exposure stack into a perceptually pleasant HDR\nimage as the inversion artifacts occur. We tackle the problem in stack\nreconstruction-based methods by proposing a novel framework with a fully\ndifferentiable high dynamic range imaging (HDRI) process. By explicitly using\nthe loss, which compares the network's output with the ground truth HDR image,\nour framework enables a neural network that generates the multiple exposure\nstack for HDRI to train stably. In other words, our differentiable HDR\nsynthesis layer helps the deep neural network to train to create multi-exposure\nstacks while reflecting the precise correlations between multi-exposure images\nin the HDRI process. In addition, our network uses the image decomposition and\nthe recursive process to facilitate the exposure transfer task and to\nadaptively respond to recursion frequency. The experimental results show that\nthe proposed network outperforms the state-of-the-art quantitative and\nqualitative results in terms of both the exposure transfer tasks and the whole\nHDRI process.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 06:47:07 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 07:19:13 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Kim", "Jung Hee", ""], ["Lee", "Siyeong", ""], ["Kang", "Suk-Ju", ""]]}, {"id": "2006.15862", "submitter": "Ren Yang", "authors": "Ren Yang, Luc Van Gool, Radu Timofte", "title": "OpenDVC: An Open Source Implementation of the DVC Video Compression\n  Method", "comments": "Technical report of OpenDVC; the project page is at\n  https://github.com/RenYang-home/OpenDVC", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an open source Tensorflow implementation of the Deep Video\nCompression (DVC) method in this technical report. DVC is the first end-to-end\noptimized learned video compression method, achieving better MS-SSIM\nperformance than the Low-Delay P (LDP) very fast setting of x265 and comparable\nPSNR performance with x265 (LDP very fast). At the time of writing this report,\nseveral learned video compression methods are superior to DVC, but currently\nnone of them provides open source codes. We hope that our OpenDVC codes are\nable to provide a useful model for further development, and facilitate future\nresearches on learned video compression. Different from the original DVC, which\nis only optimized for PSNR, we release not only the PSNR-optimized\nre-implementation, denoted by OpenDVC (PSNR), but also the MS-SSIM-optimized\nmodel OpenDVC (MS-SSIM). Our OpenDVC (MS-SSIM) model provides a more convincing\nbaseline for MS-SSIM optimized methods, which can only compare with the PSNR\noptimized DVC in the past. The OpenDVC source codes and pre-trained models are\npublicly released at https://github.com/RenYang-home/OpenDVC.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 08:22:08 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 18:45:14 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Yang", "Ren", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2006.15873", "submitter": "Hui Huang", "authors": "Chunhua Jia, Wenhai Yi, Yu Wu, Hui Huang, Lei Zhang, Leilei Wu", "title": "Abnormal activity capture from passenger flow of elevator based on\n  unsupervised learning and fine-grained multi-label recognition", "comments": "9 pages, 8 figures, submitted to 34th Conference on Neural\n  Information Processing System(NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a work-flow which aims at capturing residents' abnormal activities\nthrough the passenger flow of elevator in multi-storey residence buildings.\nCamera and sensors (hall sensor, photoelectric sensor, gyro, accelerometer,\nbarometer, and thermometer) with internet connection are mounted in elevator to\ncollect image and data. Computer vision algorithms such as instance\nsegmentation, multi-label recognition, embedding and clustering are applied to\ngeneralize passenger flow of elevator, i.e. how many people and what kinds of\npeople get in and out of the elevator on each floor. More specifically in our\nimplementation we propose GraftNet, a solution for fine-grained multi-label\nrecognition task, to recognize human attributes, e.g. gender, age, appearance,\nand occupation. Then anomaly detection of unsupervised learning is\nhierarchically applied on the passenger flow data to capture abnormal or even\nillegal activities of the residents which probably bring safety hazard, e.g.\ndrug dealing, pyramid sale gathering, prostitution, and over crowded residence.\nExperiment shows effects are there, and the captured records will be directly\nreported to our customer(property managers) for further confirmation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 08:50:20 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Jia", "Chunhua", ""], ["Yi", "Wenhai", ""], ["Wu", "Yu", ""], ["Huang", "Hui", ""], ["Zhang", "Lei", ""], ["Wu", "Leilei", ""]]}, {"id": "2006.15919", "submitter": "Pratik Mazumder", "authors": "Pratik Mazumder, Pravendra Singh and Vinay P. Namboodiri", "title": "Improving Few-Shot Learning using Composite Rotation based Auxiliary\n  Task", "comments": "Accepted in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an approach to improve few-shot classification\nperformance using a composite rotation based auxiliary task. Few-shot\nclassification methods aim to produce neural networks that perform well for\nclasses with a large number of training samples and classes with less number of\ntraining samples. They employ techniques to enable the network to produce\nhighly discriminative features that are also very generic. Generally, the\nbetter the quality and generic-nature of the features produced by the network,\nthe better is the performance of the network on few-shot learning. Our approach\naims to train networks to produce such features by using a self-supervised\nauxiliary task. Our proposed composite rotation based auxiliary task performs\nrotation at two levels, i.e., rotation of patches inside the image (inner\nrotation) and rotation of the whole image (outer rotation) and assigns one out\nof 16 rotation classes to the modified image. We then simultaneously train for\nthe composite rotation prediction task along with the original classification\ntask, which forces the network to learn high-quality generic features that help\nimprove the few-shot classification performance. We experimentally show that\nour approach performs better than existing few-shot learning methods on\nmultiple benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 10:21:35 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 17:39:51 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Mazumder", "Pratik", ""], ["Singh", "Pravendra", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2006.15920", "submitter": "Quanshi Zhang", "authors": "Jie Ren, Mingjie Li, Zexu Liu, Quanshi Zhang", "title": "Interpreting and Disentangling Feature Components of Various Complexity\n  from DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to define, quantify, and analyze the feature complexity that\nis learned by a DNN. We propose a generic definition for the feature\ncomplexity. Given the feature of a certain layer in the DNN, our method\ndisentangles feature components of different complexity orders from the\nfeature. We further design a set of metrics to evaluate the reliability, the\neffectiveness, and the significance of over-fitting of these feature\ncomponents. Furthermore, we successfully discover a close relationship between\nthe feature complexity and the performance of DNNs. As a generic mathematical\ntool, the feature complexity and the proposed metrics can also be used to\nanalyze the success of network compression and knowledge distillation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 10:24:27 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Ren", "Jie", ""], ["Li", "Mingjie", ""], ["Liu", "Zexu", ""], ["Zhang", "Quanshi", ""]]}, {"id": "2006.15938", "submitter": "Bijiao Wu", "authors": "Bijiao Wu, Dingheng Wang, Guangshe Zhao, Lei Deng and Guoqi Li", "title": "Hybrid Tensor Decomposition in Neural Network Compression", "comments": "submitted to <<Neural Networks>> on Apr.18,2020; accepted on\n  Sep.08,2020", "journal-ref": null, "doi": "10.1016/j.neunet.2020.09.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have enabled impressive breakthroughs in various\nartificial intelligence (AI) applications recently due to its capability of\nlearning high-level features from big data. However, the current demand of DNNs\nfor computational resources especially the storage consumption is growing due\nto that the increasing sizes of models are being required for more and more\ncomplicated applications. To address this problem, several tensor decomposition\nmethods including tensor-train (TT) and tensor-ring (TR) have been applied to\ncompress DNNs and shown considerable compression effectiveness. In this work,\nwe introduce the hierarchical Tucker (HT), a classical but rarely-used tensor\ndecomposition method, to investigate its capability in neural network\ncompression. We convert the weight matrices and convolutional kernels to both\nHT and TT formats for comparative study, since the latter is the most widely\nused decomposition method and the variant of HT. We further theoretically and\nexperimentally discover that the HT format has better performance on\ncompressing weight matrices, while the TT format is more suited for compressing\nconvolutional kernels. Based on this phenomenon we propose a strategy of hybrid\ntensor decomposition by combining TT and HT together to compress convolutional\nand fully connected parts separately and attain better accuracy than only using\nthe TT or HT format on convolutional neural networks (CNNs). Our work\nilluminates the prospects of hybrid tensor decomposition for neural network\ncompression.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 11:16:22 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 02:01:57 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 02:14:21 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Wu", "Bijiao", ""], ["Wang", "Dingheng", ""], ["Zhao", "Guangshe", ""], ["Deng", "Lei", ""], ["Li", "Guoqi", ""]]}, {"id": "2006.15940", "submitter": "Maxime De Bois", "authors": "Maxime De Bois, Moun\\^im A. El Yacoubi, and Mehdi Ammi", "title": "Adversarial Multi-Source Transfer Learning in Healthcare: Application to\n  Glucose Prediction for Diabetic People", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has yet to revolutionize general practices in healthcare,\ndespite promising results for some specific tasks. This is partly due to data\nbeing in insufficient quantities hurting the training of the models. To address\nthis issue, data from multiple health actors or patients could be combined by\ncapitalizing on their heterogeneity through the use of transfer learning.\n  To improve the quality of the transfer between multiple sources of data, we\npropose a multi-source adversarial transfer learning framework that enables the\nlearning of a feature representation that is similar across the sources, and\nthus more general and more easily transferable. We apply this idea to glucose\nforecasting for diabetic people using a fully convolutional neural network. The\nevaluation is done by exploring various transfer scenarios with three datasets\ncharacterized by their high inter and intra variability.\n  While transferring knowledge is beneficial in general, we show that the\nstatistical and clinical accuracies can be further improved by using of the\nadversarial training methodology, surpassing the current state-of-the-art\nresults. In particular, it shines when using data from different datasets, or\nwhen there is too little data in an intra-dataset situation. To understand the\nbehavior of the models, we analyze the learnt feature representations and\npropose a new metric in this regard. Contrary to a standard transfer, the\nadversarial transfer does not discriminate the patients and datasets, helping\nthe learning of a more general feature representation.\n  The adversarial training framework improves the learning of a general feature\nrepresentation in a multi-source environment, enhancing the knowledge transfer\nto an unseen target.\n  The proposed method can help improve the efficiency of data shared by\ndifferent health actors in the training of deep models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 11:17:50 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["De Bois", "Maxime", ""], ["Yacoubi", "Moun\u00eem A. El", ""], ["Ammi", "Mehdi", ""]]}, {"id": "2006.15954", "submitter": "Chuang Zhu", "authors": "Chuang Zhu, Ke Mei, Ting Peng, Yihao Luo, Jun Liu, Ying Wang and Mulan\n  Jin", "title": "Multi-level colonoscopy malignant tissue detection with adversarial\n  CAC-UNet", "comments": "accepted by Neurocomputing; winner of the MICCAI DigestPath 2019\n  challenge on colonoscopy tissue segmentation and classification task", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic and objective medical diagnostic model can be valuable to\nachieve early cancer detection, and thus reducing the mortality rate. In this\npaper, we propose a highly efficient multi-level malignant tissue detection\nthrough the designed adversarial CAC-UNet. A patch-level model with a\npre-prediction strategy and a malignancy area guided label smoothing is adopted\nto remove the negative WSIs, with which to lower the risk of false positive\ndetection. For the selected key patches by multi-model ensemble, an adversarial\ncontext-aware and appearance consistency UNet (CAC-UNet) is designed to achieve\nrobust segmentation. In CAC-UNet, mirror designed discriminators are able to\nseamlessly fuse the whole feature maps of the skillfully designed powerful\nbackbone network without any information loss. Besides, a mask prior is further\nadded to guide the accurate segmentation mask prediction through an extra\nmask-domain discriminator. The proposed scheme achieves the best results in\nMICCAI DigestPath2019 challenge on colonoscopy tissue segmentation and\nclassification task. The full implementation details and the trained models are\navailable at https://github.com/Raykoooo/CAC-UNet.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 11:49:58 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 13:43:25 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Zhu", "Chuang", ""], ["Mei", "Ke", ""], ["Peng", "Ting", ""], ["Luo", "Yihao", ""], ["Liu", "Jun", ""], ["Wang", "Ying", ""], ["Jin", "Mulan", ""]]}, {"id": "2006.15969", "submitter": "Ekaterina Kondrateva", "authors": "Maxim Kan, Ruslan Aliev, Anna Rudenko, Nikita Drobyshev, Nikita\n  Petrashen, Ekaterina Kondrateva, Maxim Sharaev, Alexander Bernstein, Evgeny\n  Burnaev", "title": "Interpretation of 3D CNNs for Brain MRI Data Classification", "comments": "12 pages, 3 figures", "journal-ref": "AIST2020", "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning shows high potential for many medical image analysis tasks.\nNeural networks can work with full-size data without extensive preprocessing\nand feature generation and, thus, information loss. Recent work has shown that\nthe morphological difference in specific brain regions can be found on MRI with\nthe means of Convolution Neural Networks (CNN). However, interpretation of the\nexisting models is based on a region of interest and can not be extended to\nvoxel-wise image interpretation on a whole image. In the current work, we\nconsider the classification task on a large-scale open-source dataset of young\nhealthy subjects -- an exploration of brain differences between men and women.\nIn this paper, we extend the previous findings in gender differences from\ndiffusion-tensor imaging on T1 brain MRI scans. We provide the voxel-wise 3D\nCNN interpretation comparing the results of three interpretation methods:\nMeaningful Perturbations, Grad CAM and Guided Backpropagation, and contribute\nwith the open-source library.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 17:56:46 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 16:14:44 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Kan", "Maxim", ""], ["Aliev", "Ruslan", ""], ["Rudenko", "Anna", ""], ["Drobyshev", "Nikita", ""], ["Petrashen", "Nikita", ""], ["Kondrateva", "Ekaterina", ""], ["Sharaev", "Maxim", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2006.15983", "submitter": "Gabrielle Ras", "authors": "Gabri\\\"elle Ras, Luca Ambrogioni, Pim Haselager, Marcel A.J. van\n  Gerven, Umut G\\\"u\\c{c}l\\\"u", "title": "Explainable 3D Convolutional Neural Networks by Learning Temporal\n  Transformations", "comments": "10 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the temporally factorized 3D convolution (3TConv)\nas an interpretable alternative to the regular 3D convolution (3DConv). In a\n3TConv the 3D convolutional filter is obtained by learning a 2D filter and a\nset of temporal transformation parameters, resulting in a sparse filter where\nthe 2D slices are sequentially dependent on each other in the temporal\ndimension. We demonstrate that 3TConv learns temporal transformations that\nafford a direct interpretation. The temporal parameters can be used in\ncombination with various existing 2D visualization methods. We also show that\ninsight about what the model learns can be achieved by analyzing the\ntransformation parameter statistics on a layer and model level. Finally, we\nimplicitly demonstrate that, in popular ConvNets, the 2DConv can be replaced\nwith a 3TConv and that the weights can be transferred to yield pretrained\n3TConvs. pretrained 3TConvnets leverage more than a decade of work on\ntraditional 2DConvNets by being able to make use of features that have been\nproven to deliver excellent results on image classification benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 12:29:30 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Ras", "Gabri\u00eblle", ""], ["Ambrogioni", "Luca", ""], ["Haselager", "Pim", ""], ["van Gerven", "Marcel A. J.", ""], ["G\u00fc\u00e7l\u00fc", "Umut", ""]]}, {"id": "2006.16007", "submitter": "Xichuan Zhou", "authors": "Xichuan Zhou, Yicong Peng, Chunqiao Long, Fengbo Ren, Cong Shi", "title": "MoNet3D: Towards Accurate Monocular 3D Object Localization in Real Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular multi-object detection and localization in 3D space has been proven\nto be a challenging task. The MoNet3D algorithm is a novel and effective\nframework that can predict the 3D position of each object in a monocular image\nand draw a 3D bounding box for each object. The MoNet3D method incorporates\nprior knowledge of the spatial geometric correlation of neighbouring objects\ninto the deep neural network training process to improve the accuracy of 3D\nobject localization. Experiments on the KITTI dataset show that the accuracy\nfor predicting the depth and horizontal coordinates of objects in 3D space can\nreach 96.25\\% and 94.74\\%, respectively. Moreover, the method can realize the\nreal-time image processing at 27.85 FPS, showing promising potential for\nembedded advanced driving-assistance system applications. Our code is publicly\navailable at https://github.com/CQUlearningsystemgroup/YicongPeng.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 12:48:57 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Zhou", "Xichuan", ""], ["Peng", "Yicong", ""], ["Long", "Chunqiao", ""], ["Ren", "Fengbo", ""], ["Shi", "Cong", ""]]}, {"id": "2006.16011", "submitter": "Hassan Abu Alhaija", "authors": "Hassan Abu Alhaija, Siva Karthik Mustikovela, Justus Thies, Varun\n  Jampani, Matthias Nie{\\ss}ner, Andreas Geiger, Carsten Rother", "title": "Intrinsic Autoencoders for Joint Neural Rendering and Intrinsic Image\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural rendering techniques promise efficient photo-realistic image synthesis\nwhile at the same time providing rich control over scene parameters by learning\nthe physical image formation process. While several supervised methods have\nbeen proposed for this task, acquiring a dataset of images with accurately\naligned 3D models is very difficult. The main contribution of this work is to\nlift this restriction by training a neural rendering algorithm from unpaired\ndata. More specifically, we propose an autoencoder for joint generation of\nrealistic images from synthetic 3D models while simultaneously decomposing real\nimages into their intrinsic shape and appearance properties. In contrast to a\ntraditional graphics pipeline, our approach does not require to specify all\nscene properties, such as material parameters and lighting by hand. Instead, we\nlearn photo-realistic deferred rendering from a small set of 3D models and a\nlarger set of unaligned real images, both of which are easy to acquire in\npractice. Simultaneously, we obtain accurate intrinsic decompositions of real\nimages while not requiring paired ground truth. Our experiments confirm that a\njoint treatment of rendering and decomposition is indeed beneficial and that\nour approach outperforms state-of-the-art image-to-image translation baselines\nboth qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 12:53:58 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 07:45:04 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 10:27:41 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Alhaija", "Hassan Abu", ""], ["Mustikovela", "Siva Karthik", ""], ["Thies", "Justus", ""], ["Jampani", "Varun", ""], ["Nie\u00dfner", "Matthias", ""], ["Geiger", "Andreas", ""], ["Rother", "Carsten", ""]]}, {"id": "2006.16028", "submitter": "Oleg Grinchuk", "authors": "Aleksandr Parkin and Oleg Grinchuk", "title": "Creating Artificial Modalities to Solve RGB Liveness", "comments": "CVPRW2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Special cameras that provide useful features for face anti-spoofing are\ndesirable, but not always an option. In this work we propose a method to\nutilize the difference in dynamic appearance between bona fide and spoof\nsamples by creating artificial modalities from RGB videos. We introduce two\ntypes of artificial transforms: rank pooling and optical flow, combined in\nend-to-end pipeline for spoof detection. We demonstrate that using intermediate\nrepresentations that contain less identity and fine-grained features increase\nmodel robustness to unseen attacks as well as to unseen ethnicities. The\nproposed method achieves state-of-the-art on the largest cross-ethnicity face\nanti-spoofing dataset CASIA-SURF CeFA (RGB).\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 13:19:22 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Parkin", "Aleksandr", ""], ["Grinchuk", "Oleg", ""]]}, {"id": "2006.16033", "submitter": "Joseph Robinson", "authors": "Joseph P Robinson and Ming Shao and Yun Fu", "title": "Survey on the Analysis and Modeling of Visual Kinship: A Decade in the\n  Making", "comments": null, "journal-ref": "IEEE Transactions on pattern analysis and machine intelligence\n  (2021)", "doi": "10.1109/tpami.2021.3063078", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kinship recognition is a challenging problem with many practical\napplications. With much progress and milestones having been reached after ten\nyears - we are now able to survey the research and create new milestones. We\nreview the public resources and data challenges that enabled and inspired many\nto hone-in on the views of automatic kinship recognition in the visual domain.\nThe different tasks are described in technical terms and syntax consistent\nacross the problem domain and the practical value of each discussed and\nmeasured. State-of-the-art methods for visual kinship recognition problems,\nwhether to discriminate between or generate from, are examined. As part of\nsuch, we review systems proposed as part of a recent data challenge held in\nconjunction with the 2020 IEEE Conference on Automatic Face and Gesture\nRecognition. We establish a stronghold for the state of progress for the\ndifferent problems in a consistent manner. This survey will serve as the\ncentral resource for the work of the next decade to build upon. For the tenth\nanniversary, the demo code is provided for the various kin-based tasks.\nDetecting relatives with visual recognition and classifying the relationship is\nan area with high potential for impact in research and practice.IEEE\nTransactions on pattern analysis and machine intelligence\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 13:25:45 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 14:21:22 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 23:28:10 GMT"}, {"version": "v4", "created": "Wed, 24 Feb 2021 04:04:20 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Robinson", "Joseph P", ""], ["Shao", "Ming", ""], ["Fu", "Yun", ""]]}, {"id": "2006.16057", "submitter": "Rammal Aftab", "authors": "Maria Yaseen, Rammal Aftab Ahmed, Rimsha Mahrukh", "title": "Forgery Detection in a Questioned Hyperspectral Document Image using\n  K-means Clustering", "comments": "5 pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging allows for analysis of images in several hundred of\nspectral bands depending on the spectral resolution of the imaging sensor.\nHyperspectral document image is the one which has been captured by a\nhyperspectral camera so that the document can be observed in the different\nbands on the basis of their unique spectral signatures. To detect the forgery\nin a document various Ink mismatch detection techniques based on hyperspectral\nimaging have presented vast potential in differentiating visually similar inks.\nInks of different materials exhibit different spectral signature even if they\nhave the same color. Hyperspectral analysis of document images allows\nidentification and discrimination of visually similar inks. Based on this\nanalysis forensic experts can identify the authenticity of the document. In\nthis paper an extensive ink mismatch detection technique is presented which\nuses KMean Clustering to identify different inks on the basis of their unique\nspectral response and separates them into different clusters.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 13:51:24 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Yaseen", "Maria", ""], ["Ahmed", "Rammal Aftab", ""], ["Mahrukh", "Rimsha", ""]]}, {"id": "2006.16067", "submitter": "Jihun Yi", "authors": "Jihun Yi and Sungroh Yoon", "title": "Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of image anomaly detection and\nsegmentation. Anomaly detection involves making a binary decision as to whether\nan input image contains an anomaly, and anomaly segmentation aims to locate the\nanomaly on the pixel level. Support vector data description (SVDD) is a\nlong-standing algorithm used for an anomaly detection, and we extend its deep\nlearning variant to the patch-based method using self-supervised learning. This\nextension enables anomaly segmentation and improves detection performance. As a\nresult, anomaly detection and segmentation performances measured in AUROC on\nMVTec AD dataset increased by 9.8% and 7.0%, respectively, compared to the\nprevious state-of-the-art methods. Our results indicate the efficacy of the\nproposed method and its potential for industrial application. Detailed analysis\nof the proposed method offers insights regarding its behavior, and the code is\navailable online.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 14:19:47 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 10:26:14 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Yi", "Jihun", ""], ["Yoon", "Sungroh", ""]]}, {"id": "2006.16094", "submitter": "Jialiang Wang", "authors": "Jialiang Wang and Todd Zickler", "title": "Level Set Stereo for Cooperative Grouping with Occlusion", "comments": "ICIP 2021 Code and data: https://github.com/jialiangw/levelsetstereo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing stereo boundaries is difficult because matching cues are absent in\nthe occluded regions that are adjacent to them. We introduce an energy and\nlevel-set optimizer that improves boundaries by encoding the essential geometry\nof occlusions: The spatial extent of an occlusion must equal the amplitude of\nthe disparity jump that causes it. In a collection of figure-ground scenes from\nMiddlebury and Falling Things stereo datasets, the model provides more accurate\nboundaries than previous occlusion-handling techniques.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 14:51:08 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 05:20:52 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 05:16:35 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Wang", "Jialiang", ""], ["Zickler", "Todd", ""]]}, {"id": "2006.16098", "submitter": "Chen Wu", "authors": "Chen Wu, Yinong Guo, Haonan Guo, Jingwen Yuan, Lixiang Ru, Hongruixuan\n  Chen, Bo Du, Liangpei Zhang", "title": "An Investigation of Traffic Density Changes inside Wuhan during the\n  COVID-19 Epidemic with GF-2 Time-Series Images", "comments": "35 pages, 9 figures, submitted to International Journal of Applied\n  Earth Observation and Geoinformation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to mitigate the spread of COVID-19, Wuhan was the first city to\nimplement strict lockdown policy in 2020. Even though numerous researches have\ndiscussed the travel restriction between cities and provinces, few studies\nfocus on the effect of transportation control inside the city due to the lack\nof the measurement and available data in Wuhan. Since the public transports\nhave been shut down in the beginning of city lockdown, the change of traffic\ndensity is a good indicator to reflect the intracity population flow.\nTherefore, in this paper, we collected time-series high-resolution remote\nsensing images with the resolution of 1m acquired before, during and after\nWuhan lockdown by GF-2 satellite. Vehicles on the road were extracted and\ncounted for the statistics of traffic density to reflect the changes of human\ntransmissions in the whole period of Wuhan lockdown. Open Street Map was used\nto obtain observation road surfaces, and a vehicle detection method combing\nmorphology filter and deep learning was utilized to extract vehicles with the\naccuracy of 62.56%. According to the experimental results, the traffic density\nof Wuhan dropped with the percentage higher than 80%, and even higher than 90%\non main roads during city lockdown; after lockdown lift, the traffic density\nrecovered to the normal rate. Traffic density distributions also show the\nobvious reduction and increase throughout the whole study area. The significant\nreduction and recovery of traffic density indicates that the lockdown policy in\nWuhan show effectiveness in controlling human transmission inside the city, and\nthe city returned to normal after lockdown lift.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 10:30:12 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 07:26:58 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Wu", "Chen", ""], ["Guo", "Yinong", ""], ["Guo", "Haonan", ""], ["Yuan", "Jingwen", ""], ["Ru", "Lixiang", ""], ["Chen", "Hongruixuan", ""], ["Du", "Bo", ""], ["Zhang", "Liangpei", ""]]}, {"id": "2006.16106", "submitter": "Vishal Sharma", "authors": "Vishal Sharma, Curtis Dyreson", "title": "COVID-19 Screening Using Residual Attention Network an Artificial\n  Intelligence Approach", "comments": null, "journal-ref": "2020 19th IEEE International Conference on Machine Learning and\n  Applications (ICMLA)", "doi": "10.1109/ICMLA51294.2020.00211", "report-no": "978-1-7281-8470-8/20", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus Disease 2019 (COVID-19) is caused by severe acute respiratory\nsyndrome coronavirus 2 virus (SARS-CoV-2). The virus transmits rapidly; it has\na basic reproductive number R of 2.2-2.7. In March 2020, the World Health\nOrganization declared the COVID-19 outbreak a pandemic. COVID-19 is currently\naffecting more than 200 countries with 6M active cases. An effective testing\nstrategy for COVID-19 is crucial to controlling the outbreak but the demand for\ntesting surpasses the availability of test kits that use Reverse Transcription\nPolymerase Chain Reaction (RT-PCR). In this paper, we present a technique to\nscreen for COVID-19 using artificial intelligence. Our technique takes only\nseconds to screen for the presence of the virus in a patient. We collected a\ndataset of chest X-ray images and trained several popular deep convolution\nneural network-based models (VGG, MobileNet, Xception, DenseNet,\nInceptionResNet) to classify the chest X-rays. Unsatisfied with these models,\nwe then designed and built a Residual Attention Network that was able to screen\nCOVID-19 with a testing accuracy of 98% and a validation accuracy of 100%. A\nfeature maps visual of our model show areas in a chest X-ray which are\nimportant for classification. Our work can help to increase the adaptation of\nAI-assisted applications in clinical practice. The code and dataset used in\nthis project are available at\nhttps://github.com/vishalshar/covid-19-screening-using-RAN-on-X-ray-images.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 16:33:01 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 03:29:15 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 16:54:38 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Sharma", "Vishal", ""], ["Dyreson", "Curtis", ""]]}, {"id": "2006.16107", "submitter": "J. Michael Rozmus", "authors": "J. Michael Rozmus (Eyelock LLC)", "title": "Iris Recognition: Inherent Binomial Degrees of Freedom", "comments": "4 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distinctiveness of the human iris has been measured by first extracting a\nset of features from the iris, an encoding, and then comparing these encoded\nfeature sets to determine how distinct they are from one another. For example,\nJohn Daugman measures the distinctiveness of the human iris at 244 degrees of\nfreedom, that is, Daugman's encoding maps irises into the equivalent of 2 ^ 244\ndistinct possibilities [2]. This paper shows by direct pixel-by-pixel\ncomparison of high-quality iris images that the inherent number of degrees of\nfreedom embodied in the human iris, independent of any encoding, is at least\n536. When the resolution of these images is gradually reduced, the number of\ndegrees of freedom decreases smoothly to 123 for the lowest resolution images\ntested.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 15:15:49 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Rozmus", "J. Michael", "", "Eyelock LLC"]]}, {"id": "2006.16112", "submitter": "Tiziano Portenier", "authors": "Tiziano Portenier, Siavash Bigdeli, Orcun Goksel", "title": "GramGAN: Deep 3D Texture Synthesis From 2D Exemplars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel texture synthesis framework, enabling the generation of\ninfinite, high-quality 3D textures given a 2D exemplar image. Inspired by\nrecent advances in natural texture synthesis, we train deep neural models to\ngenerate textures by non-linearly combining learned noise frequencies. To\nachieve a highly realistic output conditioned on an exemplar patch, we propose\na novel loss function that combines ideas from both style transfer and\ngenerative adversarial networks. In particular, we train the synthesis network\nto match the Gram matrices of deep features from a discriminator network. In\naddition, we propose two architectural concepts and an extrapolation strategy\nthat significantly improve generalization performance. In particular, we inject\nboth model input and condition into hidden network layers by learning to scale\nand bias hidden activations. Quantitative and qualitative evaluations on a\ndiverse set of exemplars motivate our design decisions and show that our system\nperforms superior to previous state of the art. Finally, we conduct a user\nstudy that confirms the benefits of our framework.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 15:22:03 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 10:33:59 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Portenier", "Tiziano", ""], ["Bigdeli", "Siavash", ""], ["Goksel", "Orcun", ""]]}, {"id": "2006.16120", "submitter": "Jakeoung Koo", "authors": "Jakeoung Koo, Anders B. Dahl, J. Andreas B{\\ae}rentzen, Qiongyang\n  Chen, Sara Bals, Vedrana A. Dahl", "title": "Shape from Projections via Differentiable Forward Projector for Computed\n  Tomography", "comments": "Accepted in Ultramicroscopy", "journal-ref": null, "doi": "10.1016/j.ultramic.2021.113239", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In computed tomography, the reconstruction is typically obtained on a voxel\ngrid. In this work, however, we propose a mesh-based reconstruction method. For\ntomographic problems, 3D meshes have mostly been studied to simulate data\nacquisition, but not for reconstruction, for which a 3D mesh means the inverse\nprocess of estimating shapes from projections. In this paper, we propose a\ndifferentiable forward model for 3D meshes that bridge the gap between the\nforward model for 3D surfaces and optimization. We view the forward projection\nas a rendering process, and make it differentiable by extending recent work in\ndifferentiable rendering. We use the proposed forward model to reconstruct 3D\nshapes directly from projections. Experimental results for single-object\nproblems show that the proposed method outperforms traditional voxel-based\nmethods on noisy simulated data. We also apply the proposed method on electron\ntomography images of nanoparticles to demonstrate the applicability of the\nmethod on real data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 15:33:30 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 16:34:15 GMT"}, {"version": "v3", "created": "Sun, 6 Dec 2020 18:15:46 GMT"}, {"version": "v4", "created": "Thu, 11 Mar 2021 08:31:41 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Koo", "Jakeoung", ""], ["Dahl", "Anders B.", ""], ["B\u00e6rentzen", "J. Andreas", ""], ["Chen", "Qiongyang", ""], ["Bals", "Sara", ""], ["Dahl", "Vedrana A.", ""]]}, {"id": "2006.16132", "submitter": "Zhenyu Liu", "authors": "Zhenyu Liu, Yaqiang Yao, Yan Liu, Yuening Zhu, Zhenchao Tao, Lei Wang,\n  Yuhong Feng", "title": "Human Activity Recognition based on Dynamic Spatio-Temporal Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity, which usually consists of several actions, generally covers\ninteractions among persons and or objects. In particular, human actions involve\ncertain spatial and temporal relationships, are the components of more\ncomplicated activity, and evolve dynamically over time. Therefore, the\ndescription of a single human action and the modeling of the evolution of\nsuccessive human actions are two major issues in human activity recognition. In\nthis paper, we develop a method for human activity recognition that tackles\nthese two issues. In the proposed method, an activity is divided into several\nsuccessive actions represented by spatio temporal patterns, and the evolution\nof these actions are captured by a sequential model. A refined comprehensive\nspatio temporal graph is utilized to represent a single action, which is a\nqualitative representation of a human action incorporating both the spatial and\ntemporal relations of the participant objects. Next, a discrete hidden Markov\nmodel is applied to model the evolution of action sequences. Moreover, a fully\nautomatic partition method is proposed to divide a long-term human activity\nvideo into several human actions based on variational objects and qualitative\nspatial relations. Finally, a hierarchical decomposition of the human body is\nintroduced to obtain a discriminative representation for a single action.\nExperimental results on the Cornell Activity Dataset demonstrate the efficiency\nand effectiveness of the proposed approach, which will enable long videos of\nhuman activity to be better recognized.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 15:49:34 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Liu", "Zhenyu", ""], ["Yao", "Yaqiang", ""], ["Liu", "Yan", ""], ["Zhu", "Yuening", ""], ["Tao", "Zhenchao", ""], ["Wang", "Lei", ""], ["Feng", "Yuhong", ""]]}, {"id": "2006.16148", "submitter": "Tony C. W. Mok", "authors": "Tony C. W. Mok and Albert C. S. Chung", "title": "Large Deformation Diffeomorphic Image Registration with Laplacian\n  Pyramid Networks", "comments": "Paper accepted by MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based methods have recently demonstrated promising results in\ndeformable image registration for a wide range of medical image analysis tasks.\nHowever, existing deep learning-based methods are usually limited to small\ndeformation settings, and desirable properties of the transformation including\nbijective mapping and topology preservation are often being ignored by these\napproaches. In this paper, we propose a deep Laplacian Pyramid Image\nRegistration Network, which can solve the image registration optimization\nproblem in a coarse-to-fine fashion within the space of diffeomorphic maps.\nExtensive quantitative and qualitative evaluations on two MR brain scan\ndatasets show that our method outperforms the existing methods by a significant\nmargin while maintaining desirable diffeomorphic properties and promising\nregistration speed.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 16:10:40 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 07:23:39 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Mok", "Tony C. W.", ""], ["Chung", "Albert C. S.", ""]]}, {"id": "2006.16166", "submitter": "Aidean Sharghi", "authors": "Aidean Sharghi, Helene Haugerud, Daniel Oh, Omid Mohareri", "title": "Automatic Operating Room Surgical Activity Recognition for\n  Robot-Assisted Surgery", "comments": "International Conference on Medical Image Computing and Computer\n  Assisted Intervention (MICCAI'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic recognition of surgical activities in the operating room (OR) is a\nkey technology for creating next generation intelligent surgical devices and\nworkflow monitoring/support systems. Such systems can potentially enhance\nefficiency in the OR, resulting in lower costs and improved care delivery to\nthe patients. In this paper, we investigate automatic surgical activity\nrecognition in robot-assisted operations. We collect the first large-scale\ndataset including 400 full-length multi-perspective videos from a variety of\nrobotic surgery cases captured using Time-of-Flight cameras. We densely\nannotate the videos with 10 most recognized and clinically relevant classes of\nactivities. Furthermore, we investigate state-of-the-art computer vision action\nrecognition techniques and adapt them for the OR environment and the dataset.\nFirst, we fine-tune the Inflated 3D ConvNet (I3D) for clip-level activity\nrecognition on our dataset and use it to extract features from the videos.\nThese features are then fed to a stack of 3 Temporal Gaussian Mixture layers\nwhich extracts context from neighboring clips, and eventually go through a Long\nShort Term Memory network to learn the order of activities in full-length\nvideos. We extensively assess the model and reach a peak performance of 88%\nmean Average Precision.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 16:30:31 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Sharghi", "Aidean", ""], ["Haugerud", "Helene", ""], ["Oh", "Daniel", ""], ["Mohareri", "Omid", ""]]}, {"id": "2006.16177", "submitter": "Lazhar Khelifi", "authors": "Lazhar Khelifi and Max Mignotte", "title": "Unsupervised Learning Consensus Model for Dynamic Texture Videos\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic texture (DT) segmentation, and video processing in general, is\ncurrently widely dominated by methods based on deep neural networks that\nrequire the deployment of a large number of layers. Although this parametric\napproach has shown superior performances for the dynamic texture segmentation,\nall current deep learning methods suffer from a significant main weakness\nrelated to the lack of a sufficient reference annotation to train models and to\nmake them functional. This study explores the unsupervised segmentation\napproach that can be used in the absence of training data to segment new\nvideos. We present an effective unsupervised learning consensus model for the\nsegmentation of dynamic texture (ULCM). This model is designed to merge\ndifferent segmentation maps that contain multiple and weak quality regions in\norder to achieve a more accurate final result of segmentation. The diverse\nlabeling fields required for the combination process are obtained by a\nsimplified grouping scheme applied to an input video (on the basis of a three\northogonal planes: xy, yt and xt). In the proposed model, the set of values of\nthe requantized local binary patterns (LBP) histogram around the pixel to be\nclassified are used as features which represent both the spatial and temporal\ninformation replicated in the video. Experiments conducted on the challenging\nSynthDB dataset show that, contrary to current dynamic texture segmentation\napproaches that either require parameter estimation or a training step, ULCM is\nsignificantly faster, easier to code, simple and has limited parameters.\nFurther qualitative experiments based on the YUP++ dataset prove the\nefficiently and competitively of the ULCM.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 16:40:59 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Khelifi", "Lazhar", ""], ["Mignotte", "Max", ""]]}, {"id": "2006.16228", "submitter": "Jean-Baptiste Alayrac", "authors": "Jean-Baptiste Alayrac, Adri\\`a Recasens, Rosalia Schneider, Relja\n  Arandjelovi\\'c, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander\n  Dieleman, Andrew Zisserman", "title": "Self-Supervised MultiModal Versatile Networks", "comments": "To appear in the Thirty-Fourth Annual Conference on Neural\n  Information Processing Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos are a rich source of multi-modal supervision. In this work, we learn\nrepresentations using self-supervision by leveraging three modalities naturally\npresent in videos: visual, audio and language streams. To this end, we\nintroduce the notion of a multimodal versatile network -- a network that can\ningest multiple modalities and whose representations enable downstream tasks in\nmultiple modalities. In particular, we explore how best to combine the\nmodalities, such that fine-grained representations of the visual and audio\nmodalities can be maintained, whilst also integrating text into a common\nembedding. Driven by versatility, we also introduce a novel process of\ndeflation, so that the networks can be effortlessly applied to the visual data\nin the form of video or a static image. We demonstrate how such networks\ntrained on large collections of unlabelled video data can be applied on video,\nvideo-text, image and audio tasks. Equipped with these representations, we\nobtain state-of-the-art performance on multiple challenging benchmarks\nincluding UCF101, HMDB51, Kinetics600, AudioSet and ESC-50 when compared to\nprevious self-supervised work. Our models are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 17:50:23 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 17:53:59 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Alayrac", "Jean-Baptiste", ""], ["Recasens", "Adri\u00e0", ""], ["Schneider", "Rosalia", ""], ["Arandjelovi\u0107", "Relja", ""], ["Ramapuram", "Jason", ""], ["De Fauw", "Jeffrey", ""], ["Smaira", "Lucas", ""], ["Dieleman", "Sander", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2006.16241", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang,\n  Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song,\n  Jacob Steinhardt, Justin Gilmer", "title": "The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution\n  Generalization", "comments": "ICCV 2021; Datasets, code, and models available at\n  https://github.com/hendrycks/imagenet-r", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce four new real-world distribution shift datasets consisting of\nchanges in image style, image blurriness, geographic location, camera\noperation, and more. With our new datasets, we take stock of previously\nproposed methods for improving out-of-distribution robustness and put them to\nthe test. We find that using larger models and artificial data augmentations\ncan improve robustness on real-world distribution shifts, contrary to claims in\nprior work. We find improvements in artificial robustness benchmarks can\ntransfer to real-world distribution shifts, contrary to claims in prior work.\nMotivated by our observation that data augmentations can help with real-world\ndistribution shifts, we also introduce a new data augmentation method which\nadvances the state-of-the-art and outperforms models pretrained with 1000 times\nmore labeled data. Overall we find that some methods consistently help with\ndistribution shifts in texture and local image statistics, but these methods do\nnot help with some other distribution shifts like geographic changes. Our\nresults show that future research must study multiple distribution shifts\nsimultaneously, as we demonstrate that no evaluated method consistently\nimproves robustness.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 17:59:10 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 15:43:39 GMT"}, {"version": "v3", "created": "Sat, 24 Jul 2021 04:28:58 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Hendrycks", "Dan", ""], ["Basart", "Steven", ""], ["Mu", "Norman", ""], ["Kadavath", "Saurav", ""], ["Wang", "Frank", ""], ["Dorundo", "Evan", ""], ["Desai", "Rahul", ""], ["Zhu", "Tyler", ""], ["Parajuli", "Samyak", ""], ["Guo", "Mike", ""], ["Song", "Dawn", ""], ["Steinhardt", "Jacob", ""], ["Gilmer", "Justin", ""]]}, {"id": "2006.16242", "submitter": "Yawei Li", "authors": "Yawei Li, Wen Li, Martin Danelljan, Kai Zhang, Shuhang Gu, Luc Van\n  Gool, Radu Timofte", "title": "The Heterogeneity Hypothesis: Finding Layer-Wise Differentiated Network\n  Architectures", "comments": "CVPR2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of convolutional neural network design.\nInstead of focusing on the design of the overall architecture, we investigate a\ndesign space that is usually overlooked, i.e. adjusting the channel\nconfigurations of predefined networks. We find that this adjustment can be\nachieved by shrinking widened baseline networks and leads to superior\nperformance. Based on that, we articulate the heterogeneity hypothesis: with\nthe same training protocol, there exists a layer-wise differentiated network\narchitecture (LW-DNA) that can outperform the original network with regular\nchannel configurations but with a lower level of model complexity.\n  The LW-DNA models are identified without extra computational cost or training\ntime compared with the original network. This constraint leads to controlled\nexperiments which direct the focus to the importance of layer-wise specific\nchannel configurations. LW-DNA models come with advantages related to\noverfitting, i.e. the relative relationship between model complexity and\ndataset size. Experiments are conducted on various networks and datasets for\nimage classification, visual tracking and image restoration. The resultant\nLW-DNA models consistently outperform the baseline models. Code is available at\nhttps://github.com/ofsoundof/Heterogeneity_Hypothesis.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 17:59:26 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 21:23:58 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Li", "Yawei", ""], ["Li", "Wen", ""], ["Danelljan", "Martin", ""], ["Zhang", "Kai", ""], ["Gu", "Shuhang", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2006.16322", "submitter": "Subham Sekhar Sahoo", "authors": "Subham Sekhar Sahoo, Subhashini Venugopalan, Li Li, Rishabh Singh,\n  Patrick Riley", "title": "Scaling Symbolic Methods using Gradients for Neural Model Explanation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic techniques based on Satisfiability Modulo Theory (SMT) solvers have\nbeen proposed for analyzing and verifying neural network properties, but their\nusage has been fairly limited owing to their poor scalability with larger\nnetworks. In this work, we propose a technique for combining gradient-based\nmethods with symbolic techniques to scale such analyses and demonstrate its\napplication for model explanation. In particular, we apply this technique to\nidentify minimal regions in an input that are most relevant for a neural\nnetwork's prediction. Our approach uses gradient information (based on\nIntegrated Gradients) to focus on a subset of neurons in the first layer, which\nallows our technique to scale to large networks. The corresponding SMT\nconstraints encode the minimal input mask discovery problem such that after\nmasking the input, the activations of the selected neurons are still above a\nthreshold. After solving for the minimal masks, our approach scores the mask\nregions to generate a relative ordering of the features within the mask. This\nproduces a saliency map which explains \"where a model is looking\" when making a\nprediction. We evaluate our technique on three datasets - MNIST, ImageNet, and\nBeer Reviews, and demonstrate both quantitatively and qualitatively that the\nregions generated by our approach are sparser and achieve higher saliency\nscores compared to the gradient-based methods alone. Code and examples are at -\nhttps://github.com/google-research/google-research/tree/master/smug_saliency\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 19:12:22 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 00:45:28 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 19:19:35 GMT"}, {"version": "v4", "created": "Wed, 5 May 2021 14:13:39 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Sahoo", "Subham Sekhar", ""], ["Venugopalan", "Subhashini", ""], ["Li", "Li", ""], ["Singh", "Rishabh", ""], ["Riley", "Patrick", ""]]}, {"id": "2006.16331", "submitter": "Mateusz Budnik", "authors": "Mateusz Budnik and Yannis Avrithis", "title": "Asymmetric metric learning for knowledge transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge transfer from large teacher models to smaller student models has\nrecently been studied for metric learning, focusing on fine-grained\nclassification. In this work, focusing on instance-level image retrieval, we\nstudy an asymmetric testing task, where the database is represented by the\nteacher and queries by the student. Inspired by this task, we introduce\nasymmetric metric learning, a novel paradigm of using asymmetric\nrepresentations at training. This acts as a simple combination of knowledge\ntransfer with the original metric learning task.\n  We systematically evaluate different teacher and student models, metric\nlearning and knowledge transfer loss functions on the new asymmetric testing as\nwell as the standard symmetric testing task, where database and queries are\nrepresented by the same model. We find that plain regression is surprisingly\neffective compared to more complex knowledge transfer mechanisms, working best\nin asymmetric testing. Interestingly, our asymmetric metric learning approach\nworks best in symmetric testing, allowing the student to even outperform the\nteacher.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 19:28:36 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Budnik", "Mateusz", ""], ["Avrithis", "Yannis", ""]]}, {"id": "2006.16344", "submitter": "Roohallah Alizadehsani", "authors": "Hadi Mahami, Navid Ghassemi, Mohammad Tayarani Darbandy, Afshin\n  Shoeibi, Sadiq Hussain, Farnad Nasirzadeh, Roohallah Alizadehsani, Darius\n  Nahavandi, Abbas Khosravi, Saeid Nahavandi", "title": "Material Recognition for Automated Progress Monitoring using Deep\n  Learning Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in Artificial intelligence, especially deep learning, has\nchanged many fields irreversibly by introducing state of the art methods for\nautomation. Construction monitoring has not been an exception; as a part of\nconstruction monitoring systems, material classification and recognition have\ndrawn the attention of deep learning and machine vision researchers. However,\nto create production-ready systems, there is still a long path to cover.\nReal-world problems such as varying illuminations and reaching acceptable\naccuracies need to be addressed in order to create robust systems. In this\npaper, we have addressed these issues and reached a state of the art\nperformance, i.e., 97.35% accuracy rate for this task. Also, a new dataset\ncontaining 1231 images of 11 classes taken from several construction sites is\ngathered and publicly published to help other researchers in this field.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 20:06:26 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 01:18:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mahami", "Hadi", ""], ["Ghassemi", "Navid", ""], ["Darbandy", "Mohammad Tayarani", ""], ["Shoeibi", "Afshin", ""], ["Hussain", "Sadiq", ""], ["Nasirzadeh", "Farnad", ""], ["Alizadehsani", "Roohallah", ""], ["Nahavandi", "Darius", ""], ["Khosravi", "Abbas", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "2006.16400", "submitter": "Xingyang Ni", "authors": "Xingyang Ni, Heikki Huttunen", "title": "Vehicle Attribute Recognition by Appearance: Computer Vision Methods for\n  Vehicle Type, Make and Model Classification", "comments": "Published in Journal of Signal Processing Systems", "journal-ref": null, "doi": "10.1007/s11265-020-01567-6", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies vehicle attribute recognition by appearance. In the\nliterature, image-based target recognition has been extensively investigated in\nmany use cases, such as facial recognition, but less so in the field of vehicle\nattribute recognition. We survey a number of algorithms that identify vehicle\nproperties ranging from coarse-grained level (vehicle type) to fine-grained\nlevel (vehicle make and model). Moreover, we discuss two alternative approaches\nfor these tasks, including straightforward classification and a more flexible\nmetric learning method. Furthermore, we design a simulated real-world scenario\nfor vehicle attribute recognition and present an experimental comparison of the\ntwo approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 21:33:06 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Ni", "Xingyang", ""], ["Huttunen", "Heikki", ""]]}, {"id": "2006.16424", "submitter": "Wei-Lin Hsiao", "authors": "Nicole D. Payntar, Wei-Lin Hsiao, R. Alan Covey, Kristen Grauman", "title": "Learning Patterns of Tourist Movement and Photography from Geotagged\n  Photos at Archaeological Heritage Sites in Cuzco, Peru", "comments": "Accepted to Tourism Management", "journal-ref": null, "doi": "10.1016/j.tourman.2020.104165", "report-no": null, "categories": "cs.SI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of media sharing platforms in recent decades has provided an\nabundance of open source data that remains underutilized by heritage scholars.\nBy pairing geotagged internet photographs with machine learning and computer\nvision algorithms, we build upon the current theoretical discourse of\nanthropology associated with visuality and heritage tourism to identify travel\npatterns across a known archaeological heritage circuit, and quantify visual\nculture and experiences in Cuzco, Peru. Leveraging large-scale in-the-wild\ntourist photos, our goals are to (1) understand how the intensification of\ntourism intersects with heritage regulations and social media, aiding in the\narticulation of travel patterns across Cuzco's heritage landscape; and to (2)\nassess how aesthetic preferences and visuality become entangled with the\nrapidly evolving expectations of tourists, whose travel narratives are curated\non social media and grounded in historic site representations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 22:49:59 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Payntar", "Nicole D.", ""], ["Hsiao", "Wei-Lin", ""], ["Covey", "R. Alan", ""], ["Grauman", "Kristen", ""]]}, {"id": "2006.16427", "submitter": "Andrzej Banburski", "authors": "Manish V. Reddy, Andrzej Banburski, Nishka Pant, Tomaso Poggio", "title": "Biologically Inspired Mechanisms for Adversarial Robustness", "comments": "25 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A convolutional neural network strongly robust to adversarial perturbations\nat reasonable computational and performance cost has not yet been demonstrated.\nThe primate visual ventral stream seems to be robust to small perturbations in\nvisual stimuli but the underlying mechanisms that give rise to this robust\nperception are not understood. In this work, we investigate the role of two\nbiologically plausible mechanisms in adversarial robustness. We demonstrate\nthat the non-uniform sampling performed by the primate retina and the presence\nof multiple receptive fields with a range of receptive field sizes at each\neccentricity improve the robustness of neural networks to small adversarial\nperturbations. We verify that these two mechanisms do not suffer from gradient\nobfuscation and study their contribution to adversarial robustness through\nablation studies.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 23:07:34 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Reddy", "Manish V.", ""], ["Banburski", "Andrzej", ""], ["Pant", "Nishka", ""], ["Poggio", "Tomaso", ""]]}, {"id": "2006.16434", "submitter": "Pingchuan Ma", "authors": "Pingchuan Ma, Tao Du, Wojciech Matusik", "title": "Efficient Continuous Pareto Exploration in Multi-Task Learning", "comments": "ICML 2020 camera-ready. Code:\n  https://github.com/mit-gfx/ContinuousParetoMTL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tasks in multi-task learning often correlate, conflict, or even compete with\neach other. As a result, a single solution that is optimal for all tasks rarely\nexists. Recent papers introduced the concept of Pareto optimality to this field\nand directly cast multi-task learning as multi-objective optimization problems,\nbut solutions returned by existing methods are typically finite, sparse, and\ndiscrete. We present a novel, efficient method that generates locally\ncontinuous Pareto sets and Pareto fronts, which opens up the possibility of\ncontinuous analysis of Pareto optimal solutions in machine learning problems.\nWe scale up theoretical results in multi-objective optimization to modern\nmachine learning problems by proposing a sample-based sparse linear system, for\nwhich standard Hessian-free solvers in machine learning can be applied. We\ncompare our method to the state-of-the-art algorithms and demonstrate its usage\nof analyzing local Pareto sets on various multi-task classification and\nregression problems. The experimental results confirm that our algorithm\nreveals the primary directions in local Pareto sets for trade-off balancing,\nfinds more solutions with different trade-offs efficiently, and scales well to\ntasks with millions of parameters.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 23:36:20 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 20:48:16 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Ma", "Pingchuan", ""], ["Du", "Tao", ""], ["Matusik", "Wojciech", ""]]}, {"id": "2006.16471", "submitter": "Mazin Hnewa", "authors": "Mazin Hnewa and Hayder Radha", "title": "Object Detection Under Rainy Conditions for Autonomous Vehicles: A\n  Review of State-of-the-Art and Emerging Techniques", "comments": null, "journal-ref": "IEEE Signal Processing Magazine, vol. 38, no. 1, pp. 53-67, Jan.\n  2021", "doi": "10.1109/MSP.2020.2984801", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced automotive active-safety systems, in general, and autonomous\nvehicles, in particular, rely heavily on visual data to classify and localize\nobjects such as pedestrians, traffic signs and lights, and other nearby cars,\nto assist the corresponding vehicles maneuver safely in their environments.\nHowever, the performance of object detection methods could degrade rather\nsignificantly under challenging weather scenarios including rainy conditions.\nDespite major advancements in the development of deraining approaches, the\nimpact of rain on object detection has largely been understudied, especially in\nthe context of autonomous driving. The main objective of this paper is to\npresent a tutorial on state-of-the-art and emerging techniques that represent\nleading candidates for mitigating the influence of rainy conditions on an\nautonomous vehicle's ability to detect objects. Our goal includes surveying and\nanalyzing the performance of object detection methods trained and tested using\nvisual data captured under clear and rainy conditions. Moreover, we survey and\nevaluate the efficacy and limitations of leading deraining approaches,\ndeep-learning based domain adaptation, and image translation frameworks that\nare being considered for addressing the problem of object detection under rainy\nconditions. Experimental results of a variety of the surveyed techniques are\npresented as part of this tutorial.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 02:05:10 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 19:06:43 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 19:51:52 GMT"}, {"version": "v4", "created": "Fri, 12 Feb 2021 02:16:15 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Hnewa", "Mazin", ""], ["Radha", "Hayder", ""]]}, {"id": "2006.16479", "submitter": "Xiaoyu Zhu", "authors": "Xiaoyu Zhu, Junwei Liang, Alexander Hauptmann", "title": "MSNet: A Multilevel Instance Segmentation Network for Natural Disaster\n  Damage Assessment in Aerial Videos", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of efficiently assessing building damage\nafter natural disasters like hurricanes, floods or fires, through aerial video\nanalysis. We make two main contributions. The first contribution is a new\ndataset, consisting of user-generated aerial videos from social media with\nannotations of instance-level building damage masks. This provides the first\nbenchmark for quantitative evaluation of models to assess building damage using\naerial videos. The second contribution is a new model, namely MSNet, which\ncontains novel region proposal network designs and an unsupervised score\nrefinement network for confidence score calibration in both bounding box and\nmask branches. We show that our model achieves state-of-the-art results\ncompared to previous methods in our dataset. We will release our data, models\nand code.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 02:23:05 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2020 23:06:25 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhu", "Xiaoyu", ""], ["Liang", "Junwei", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "2006.16500", "submitter": "Hyungki Kim", "authors": "Hyungki Kim, Moohyun Cha, Duhwan Mun", "title": "Method for the generation of depth images for view-based shape retrieval\n  of 3D CAD model from partial point cloud", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A laser scanner can easily acquire the geometric data of physical\nenvironments in the form of a point cloud. Recognizing objects from a point\ncloud is often required for industrial 3D reconstruction, which should include\nnot only geometry information but also semantic information. However,\nrecognition process is often a bottleneck in 3D reconstruction because it\nrequires expertise on domain knowledge and intensive labor. To address this\nproblem, various methods have been developed to recognize objects by retrieving\nthe corresponding model in the database from an input geometry query. In recent\nyears, the technique of converting geometric data into an image and applying\nview-based 3D shape retrieval has demonstrated high accuracy. Depth image which\nencodes depth value as intensity of pixel is frequently used for view-based 3D\nshape retrieval. However, geometric data collected from objects is often\nincomplete due to the occlusions and the limit of line of sight. Image\ngenerated by occluded point clouds lowers the performance of view-based 3D\nobject retrieval due to loss of information. In this paper, we propose a method\nof viewpoint and image resolution estimation method for view-based 3D shape\nretrieval from point cloud query. Automatic selection of viewpoint and image\nresolution by calculating the data acquisition rate and density from the\nsampled viewpoints and image resolutions are proposed. The retrieval\nperformance from the images generated by the proposed method is experimented\nand compared for various dataset. Additionally, view-based 3D shape retrieval\nperformance with deep convolutional neural network has been experimented with\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 03:18:16 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Kim", "Hyungki", ""], ["Cha", "Moohyun", ""], ["Mun", "Duhwan", ""]]}, {"id": "2006.16503", "submitter": "Weiwei Sun", "authors": "Zizhang Wu, Man Wang, Lingxiao Yin, Weiwei Sun, Jason Wang, Huangbin\n  Wu", "title": "Vehicle Re-ID for Surround-view Camera System", "comments": "CVPR 2020 workshop on Scalability in Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vehicle re-identification (ReID) plays a critical role in the perception\nsystem of autonomous driving, which attracts more and more attention in recent\nyears. However, to our best knowledge, there is no existing complete solution\nfor the surround-view system mounted on the vehicle. In this paper, we argue\ntwo main challenges in above scenario: i) In single camera view, it is\ndifficult to recognize the same vehicle from the past image frames due to the\nfisheye distortion, occlusion, truncation, etc. ii) In multi-camera view, the\nappearance of the same vehicle varies greatly from different camera's\nviewpoints. Thus, we present an integral vehicle Re-ID solution to address\nthese problems. Specifically, we propose a novel quality evaluation mechanism\nto balance the effect of tracking box's drift and target's consistency.\nBesides, we take advantage of the Re-ID network based on attention mechanism,\nthen combined with a spatial constraint strategy to further boost the\nperformance between different cameras. The experiments demonstrate that our\nsolution achieves state-of-the-art accuracy while being real-time in practice.\nBesides, we will release the code and annotated fisheye dataset for the benefit\nof community.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 03:25:10 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Wu", "Zizhang", ""], ["Wang", "Man", ""], ["Yin", "Lingxiao", ""], ["Sun", "Weiwei", ""], ["Wang", "Jason", ""], ["Wu", "Huangbin", ""]]}, {"id": "2006.16524", "submitter": "Samarth Sinha", "authors": "Samarth Sinha, Karsten Roth, Anirudh Goyal, Marzyeh Ghassemi, Hugo\n  Larochelle, Animesh Garg", "title": "Uniform Priors for Data-Efficient Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have shown great promise on a variety of downstream\napplications; but their ability to adapt and generalize to new data and tasks\nremains a challenge. However, the ability to perform few or zero-shot\nadaptation to novel tasks is important for the scalability and deployment of\nmachine learning models. It is therefore crucial to understand what makes for\ngood, transfer-able features in deep networks that best allow for such\nadaptation. In this paper, we shed light on this by showing that features that\nare most transferable have high uniformity in the embedding space and propose a\nuniformity regularization scheme that encourages better transfer and feature\nreuse. We evaluate the regularization on its ability to facilitate adaptation\nto unseen tasks and data, for which we conduct a thorough experimental study\ncovering four relevant, and distinct domains: few-shot Meta-Learning, Deep\nMetric Learning, Zero-Shot Domain Adaptation, as well as Out-of-Distribution\nclassification. Across all experiments, we show that uniformity regularization\nconsistently offers benefits over baseline methods and is able to achieve\nstate-of-the-art performance in Deep Metric Learning and Meta-Learning.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 04:39:36 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 12:21:07 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Sinha", "Samarth", ""], ["Roth", "Karsten", ""], ["Goyal", "Anirudh", ""], ["Ghassemi", "Marzyeh", ""], ["Larochelle", "Hugo", ""], ["Garg", "Animesh", ""]]}, {"id": "2006.16533", "submitter": "Shusen Liu", "authors": "Shusen Liu, Bhavya Kailkhura, Jize Zhang, Anna M. Hiszpanski, Emily\n  Robertson, Donald Loveland, T. Yong-Jin Han", "title": "Actionable Attribution Maps for Scientific Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scientific community has been increasingly interested in harnessing the\npower of deep learning to solve various domain challenges. However, despite the\neffectiveness in building predictive models, fundamental challenges exist in\nextracting actionable knowledge from the deep neural network due to their\nopaque nature. In this work, we propose techniques for exploring the behavior\nof deep learning models by injecting domain-specific actionable concepts as\ntunable ``knobs'' in the analysis pipeline. By incorporating the domain\nknowledge with generative modeling, we are not only able to better understand\nthe behavior of these black-box models, but also provide scientists with\nactionable insights that can potentially lead to fundamental discoveries.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 05:12:29 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Liu", "Shusen", ""], ["Kailkhura", "Bhavya", ""], ["Zhang", "Jize", ""], ["Hiszpanski", "Anna M.", ""], ["Robertson", "Emily", ""], ["Loveland", "Donald", ""], ["Han", "T. Yong-Jin", ""]]}, {"id": "2006.16537", "submitter": "Pan Zhou", "authors": "Pan Zhou, Caiming Xiong, Richard Socher, Steven C.H. Hoi", "title": "Theory-Inspired Path-Regularized Differential Network Architecture\n  Search", "comments": "NeurIPS 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its high search efficiency, differential architecture search (DARTS)\noften selects network architectures with dominated skip connections which lead\nto performance degradation. However, theoretical understandings on this issue\nremain absent, hindering the development of more advanced methods in a\nprincipled way. In this work, we solve this problem by theoretically analyzing\nthe effects of various types of operations, e.g. convolution, skip connection\nand zero operation, to the network optimization. We prove that the\narchitectures with more skip connections can converge faster than the other\ncandidates, and thus are selected by DARTS. This result, for the first time,\ntheoretically and explicitly reveals the impact of skip connections to fast\nnetwork optimization and its competitive advantage over other types of\noperations in DARTS. Then we propose a theory-inspired path-regularized DARTS\nthat consists of two key modules: (i) a differential group-structured sparse\nbinary gate introduced for each operation to avoid unfair competition among\noperations, and (ii) a path-depth-wise regularization used to incite search\nexploration for deep architectures that often converge slower than shallow ones\nas shown in our theory and are not well explored during the search.\nExperimental results on image classification tasks validate its advantages.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 05:28:23 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 12:12:55 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zhou", "Pan", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2006.16546", "submitter": "William Adorno", "authors": "William Adorno III, Angela Yi, Marcel Durieux, Donald Brown", "title": "Hand-drawn Symbol Recognition of Surgical Flowsheet Graphs with Deep\n  Image Segmentation", "comments": "8 pages, 4 figures, BioInformatics And BioEngineering 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perioperative data are essential to investigating the causes of adverse\nsurgical outcomes. In some low to middle income countries, these data are\ncomputationally inaccessible due to a lack of digitization of surgical\nflowsheets. In this paper, we present a deep image segmentation approach using\na U-Net architecture that can detect hand-drawn symbols on a flowsheet graph.\nThe segmentation mask outputs are post-processed with techniques unique to each\nsymbol to convert into numeric values. The U-Net method can detect, at the\nappropriate time intervals, the symbols for heart rate and blood pressure with\nover 99 percent accuracy. Over 95 percent of the predictions fall within an\nabsolute error of five when compared to the actual value. The deep learning\nmodel outperformed template matching even with a small size of annotated images\navailable for the training set.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 05:56:59 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Adorno", "William", "III"], ["Yi", "Angela", ""], ["Durieux", "Marcel", ""], ["Brown", "Donald", ""]]}, {"id": "2006.16571", "submitter": "Deepak Gupta", "authors": "Deepak K. Gupta, Efstratios Gavves and Arnold W. M. Smeulders", "title": "Tackling Occlusion in Siamese Tracking with Structured Dropouts", "comments": "Accepted at ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion is one of the most difficult challenges in object tracking to\nmodel. This is because unlike other challenges, where data augmentation can be\nof help, occlusion is hard to simulate as the occluding object can be anything\nin any shape. In this paper, we propose a simple solution to simulate the\neffects of occlusion in the latent space. Specifically, we present structured\ndropout to mimick the change in latent codes under occlusion. We present three\nforms of dropout (channel dropout, segment dropout and slice dropout) with the\nvarious forms of occlusion in mind. To demonstrate its effectiveness, the\ndropouts are incorporated into two modern Siamese trackers (SiamFC and\nSiamRPN++). The outputs from multiple dropouts are combined using an encoder\nnetwork to obtain the final prediction. Experiments on several tracking\nbenchmarks show the benefits of structured dropouts, while due to their\nsimplicity requiring only small changes to the existing tracker models.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 07:09:33 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Gupta", "Deepak K.", ""], ["Gavves", "Efstratios", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "2006.16581", "submitter": "Qunliang Xing", "authors": "Qunliang Xing, Mai Xu, Tianyi Li, Zhenyu Guan", "title": "Early Exit or Not: Resource-Efficient Blind Quality Enhancement for\n  Compressed Images", "comments": "Accepted by ECCV 2020. v5 updates: enlarge character size; correct\n  titlerunning; add publishment reference; add open-sourced url", "journal-ref": null, "doi": "10.1007/978-3-030-58517-4_17", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy image compression is pervasively conducted to save communication\nbandwidth, resulting in undesirable compression artifacts. Recently, extensive\napproaches have been proposed to reduce image compression artifacts at the\ndecoder side; however, they require a series of architecture-identical models\nto process images with different quality, which are inefficient and\nresource-consuming. Besides, it is common in practice that compressed images\nare with unknown quality and it is intractable for existing approaches to\nselect a suitable model for blind quality enhancement. In this paper, we\npropose a resource-efficient blind quality enhancement (RBQE) approach for\ncompressed images. Specifically, our approach blindly and progressively\nenhances the quality of compressed images through a dynamic deep neural network\n(DNN), in which an early-exit strategy is embedded. Then, our approach can\nautomatically decide to terminate or continue enhancement according to the\nassessed quality of enhanced images. Consequently, slight artifacts can be\nremoved in a simpler and faster process, while the severe artifacts can be\nfurther removed in a more elaborate process. Extensive experiments demonstrate\nthat our RBQE approach achieves state-of-the-art performance in terms of both\nblind quality enhancement and resource efficiency. The code is available at\nhttps://github.com/RyanXingQL/RBQE.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 07:38:47 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 06:27:15 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 02:47:20 GMT"}, {"version": "v4", "created": "Tue, 22 Sep 2020 07:13:45 GMT"}, {"version": "v5", "created": "Mon, 12 Oct 2020 08:23:58 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Xing", "Qunliang", ""], ["Xu", "Mai", ""], ["Li", "Tianyi", ""], ["Guan", "Zhenyu", ""]]}, {"id": "2006.16589", "submitter": "Nandan Kumar Jha", "authors": "Nandan Kumar Jha, Rajat Saini, Sparsh Mittal", "title": "On the Demystification of Knowledge Distillation: A Residual Network\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation (KD) is generally considered as a technique for\nperforming model compression and learned-label smoothing. However, in this\npaper, we study and investigate the KD approach from a new perspective: we\nstudy its efficacy in training a deeper network without any residual\nconnections. We find that in most of the cases, non-residual student networks\nperform equally or better than their residual versions trained on raw data\nwithout KD (baseline network). Surprisingly, in some cases, they surpass the\naccuracy of baseline networks even with the inferior teachers. After a certain\ndepth of non-residual student network, the accuracy drop, coming from the\nremoval of residual connections, is substantial, and training with KD boosts\nthe accuracy of the student up to a great extent; however, it does not fully\nrecover the accuracy drop. Furthermore, we observe that the conventional\nteacher-student view of KD is incomplete and does not adequately explain our\nfindings. We propose a novel interpretation of KD with the Trainee-Mentor\nhypothesis, which provides a holistic view of KD. We also present two\nviewpoints, loss landscape, and feature reuse, to explain the interplay between\nresidual connections and KD. We substantiate our claims through extensive\nexperiments on residual networks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 08:00:13 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Jha", "Nandan Kumar", ""], ["Saini", "Rajat", ""], ["Mittal", "Sparsh", ""]]}, {"id": "2006.16607", "submitter": "Cristian Axenie", "authors": "Du Xiaorui, Yavuzhan Erdem, Immanuel Schweizer, Cristian Axenie", "title": "A Framework for Learning Invariant Physical Relations in Multimodal\n  Sensory Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Perceptual learning enables humans to recognize and represent stimuli\ninvariant to various transformations and build a consistent representation of\nthe self and physical world. Such representations preserve the invariant\nphysical relations among the multiple perceived sensory cues. This work is an\nattempt to exploit these principles in an engineered system. We design a novel\nneural network architecture capable of learning, in an unsupervised manner,\nrelations among multiple sensory cues. The system combines computational\nprinciples, such as competition, cooperation, and correlation, in a neurally\nplausible computational substrate. It achieves that through a parallel and\ndistributed processing architecture in which the relations among the multiple\nsensory quantities are extracted from time-sequenced data. We describe the core\nsystem functionality when learning arbitrary non-linear relations in\nlow-dimensional sensory data. Here, an initial benefit rises from the fact that\nsuch a network can be engineered in a relatively straightforward way without\nprior information about the sensors and their interactions. Moreover,\nalleviating the need for tedious modelling and parametrization, the network\nconverges to a consistent description of any arbitrary high-dimensional\nmultisensory setup. We demonstrate this through a real-world learning problem,\nwhere, from standard RGB camera frames, the network learns the relations\nbetween physical quantities such as light intensity, spatial gradient, and\noptical flow, describing a visual scene. Overall, the benefits of such a\nframework lie in the capability to learn non-linear pairwise relations among\nsensory streams in an architecture that is stable under noise and missing\nsensor input.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 08:42:48 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Xiaorui", "Du", ""], ["Erdem", "Yavuzhan", ""], ["Schweizer", "Immanuel", ""], ["Axenie", "Cristian", ""]]}, {"id": "2006.16621", "submitter": "Kanchana Vaishnavi Gandikota", "authors": "Guruprasad Hegde, Avinash Nittur Ramesh, Kanchana Vaishnavi Gandikota,\n  Roman Obermaisser, Michael Moeller", "title": "A Simple Domain Shifting Networkfor Generating Low Quality Images", "comments": "accepted ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning systems have proven to be extremely successful for image\nrecognition tasks for which significant amounts of training data is available,\ne.g., on the famous ImageNet dataset. We demonstrate that for robotics\napplications with cheap camera equipment, the low image quality,\nhowever,influences the classification accuracy, and freely available databases\ncannot be exploited in a straight forward way to train classifiers to be used\non a robot. As a solution we propose to train a network on degrading the\nquality images in order to mimic specific low quality imaging systems.\nNumerical experiments demonstrate that classification networks trained by using\nimages produced by our quality degrading network along with the high quality\nimages outperform classification networks trained only on high quality data\nwhen used on a real robot system, while being significantly easier to use than\ncompeting zero-shot domain adaptation techniques.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 09:19:54 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Hegde", "Guruprasad", ""], ["Ramesh", "Avinash Nittur", ""], ["Gandikota", "Kanchana Vaishnavi", ""], ["Obermaisser", "Roman", ""], ["Moeller", "Michael", ""]]}, {"id": "2006.16625", "submitter": "In-Jae Yu", "authors": "In-Jae Yu, Wonhyuk Ahn, Seung-Hun Nam, Heung-Kyu Lee", "title": "BitMix: Data Augmentation for Image Steganalysis", "comments": null, "journal-ref": null, "doi": "10.1049/el.2020.1951", "report-no": null, "categories": "eess.IV cs.CR cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) for image steganalysis demonstrate better\nperformances with employing concepts from high-level vision tasks. The major\nemployed concept is to use data augmentation to avoid overfitting due to\nlimited data. To augment data without damaging the message embedding, only\nrotating multiples of 90 degrees or horizontally flipping are used in\nsteganalysis, which generates eight fixed results from one sample. To overcome\nthis limitation, we propose BitMix, a data augmentation method for spatial\nimage steganalysis. BitMix mixes a cover and stego image pair by swapping the\nrandom patch and generates an embedding adaptive label with the ratio of the\nnumber of pixels modified in the swapped patch to those in the cover-stego\npair. We explore optimal hyperparameters, the ratio of applying BitMix in the\nmini-batch, and the size of the bounding box for swapping patch. The results\nreveal that using BitMix improves the performance of spatial image steganalysis\nand better than other data augmentation methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 09:36:21 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Yu", "In-Jae", ""], ["Ahn", "Wonhyuk", ""], ["Nam", "Seung-Hun", ""], ["Lee", "Heung-Kyu", ""]]}, {"id": "2006.16633", "submitter": "Veronika Cheplygina", "authors": "Linde S. Hesse, Pim A. de Jong, Josien P.W. Pluim, Veronika Cheplygina", "title": "Primary Tumor Origin Classification of Lung Nodules in Spectral CT using\n  Transfer Learning", "comments": "MSc thesis Linde Hesse", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of lung cancer has been proven to decrease mortality\nsignificantly. A recent development in computed tomography (CT), spectral CT,\ncan potentially improve diagnostic accuracy, as it yields more information per\nscan than regular CT. However, the shear workload involved with analyzing a\nlarge number of scans drives the need for automated diagnosis methods.\nTherefore, we propose a detection and classification system for lung nodules in\nCT scans. Furthermore, we want to observe whether spectral images can increase\nclassifier performance. For the detection of nodules we trained a VGG-like 3D\nconvolutional neural net (CNN). To obtain a primary tumor classifier for our\ndataset we pre-trained a 3D CNN with similar architecture on nodule\nmalignancies of a large publicly available dataset, the LIDC-IDRI dataset.\nSubsequently we used this pre-trained network as feature extractor for the\nnodules in our dataset. The resulting feature vectors were classified into two\n(benign/malignant) and three (benign/primary lung cancer/metastases) classes\nusing support vector machine (SVM). This classification was performed both on\nnodule- and scan-level. We obtained state-of-the art performance for detection\nand malignancy regression on the LIDC-IDRI database. Classification performance\non our own dataset was higher for scan- than for nodule-level predictions. For\nthe three-class scan-level classification we obtained an accuracy of 78\\%.\nSpectral features did increase classifier performance, but not significantly.\nOur work suggests that a pre-trained feature extractor can be used as primary\ntumor origin classifier for lung nodules, eliminating the need for elaborate\nfine-tuning of a new network and large datasets. Code is available at\n\\url{https://github.com/tueimage/lung-nodule-msc-2018}.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 09:56:18 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Hesse", "Linde S.", ""], ["de Jong", "Pim A.", ""], ["Pluim", "Josien P. W.", ""], ["Cheplygina", "Veronika", ""]]}, {"id": "2006.16637", "submitter": "Shuaicheng Liu Prof.", "authors": "Kunming Luo, Chuan Wang, Nianjin Ye, Shuaicheng Liu, Jue Wang", "title": "OccInpFlow: Occlusion-Inpainting Optical Flow Estimation by Unsupervised\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occlusion is an inevitable and critical problem in unsupervised optical flow\nlearning. Existing methods either treat occlusions equally as non-occluded\nregions or simply remove them to avoid incorrectness. However, the occlusion\nregions can provide effective information for optical flow learning. In this\npaper, we present OccInpFlow, an occlusion-inpainting framework to make full\nuse of occlusion regions. Specifically, a new appearance-flow network is\nproposed to inpaint occluded flows based on the image content. Moreover, a\nboundary warp is proposed to deal with occlusions caused by displacement beyond\nimage border. We conduct experiments on multiple leading flow benchmark data\nsets such as Flying Chairs, KITTI and MPI-Sintel, which demonstrate that the\nperformance is significantly improved by our proposed occlusion handling\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 10:01:32 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Luo", "Kunming", ""], ["Wang", "Chuan", ""], ["Ye", "Nianjin", ""], ["Liu", "Shuaicheng", ""], ["Wang", "Jue", ""]]}, {"id": "2006.16644", "submitter": "Furkan Ozcelik", "authors": "Furkan Ozcelik, Ugur Alganci, Elif Sertel, Gozde Unal", "title": "Rethinking CNN-Based Pansharpening: Guided Colorization of Panchromatic\n  Images via GANs", "comments": "F. Ozcelik, U. Alganci, E. Sertel, G. Unal, \"Rethinking CNN-Based\n  Pansharpening: Guided Colorization of Panchromatic Images via GANs\", IEEE\n  Transactions on Geoscience and Remote Sensing (TGRS), in press, 2020", "journal-ref": null, "doi": "10.1109/TGRS.2020.3010441", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN)-based approaches have shown promising\nresults in pansharpening of satellite images in recent years. However, they\nstill exhibit limitations in producing high-quality pansharpening outputs. To\nthat end, we propose a new self-supervised learning framework, where we treat\npansharpening as a colorization problem, which brings an entirely novel\nperspective and solution to the problem compared to existing methods that base\ntheir solution solely on producing a super-resolution version of the\nmultispectral image. Whereas CNN-based methods provide a reduced resolution\npanchromatic image as input to their model along with reduced resolution\nmultispectral images, hence learn to increase their resolution together, we\ninstead provide the grayscale transformed multispectral image as input, and\ntrain our model to learn the colorization of the grayscale input. We further\naddress the fixed downscale ratio assumption during training, which does not\ngeneralize well to the full-resolution scenario. We introduce a noise injection\ninto the training by randomly varying the downsampling ratios. Those two\ncritical changes, along with the addition of adversarial training in the\nproposed PanColorization Generative Adversarial Networks (PanColorGAN)\nframework, help overcome the spatial detail loss and blur problems that are\nobserved in CNN-based pansharpening. The proposed approach outperforms the\nprevious CNN-based and traditional methods as demonstrated in our experiments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 10:12:37 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ozcelik", "Furkan", ""], ["Alganci", "Ugur", ""], ["Sertel", "Elif", ""], ["Unal", "Gozde", ""]]}, {"id": "2006.16669", "submitter": "Yongle Zhao", "authors": "Di Wu, Qi Tang, Yongle Zhao, Ming Zhang, Ying Fu and Debing Zhang", "title": "EasyQuant: Post-training Quantization via Scale Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 8 bits quantization has been widely applied to accelerate network\ninference in various deep learning applications. There are two kinds of\nquantization methods, training-based quantization and post-training\nquantization. Training-based approach suffers from a cumbersome training\nprocess, while post-training quantization may lead to unacceptable accuracy\ndrop. In this paper, we present an efficient and simple post-training method\nvia scale optimization, named EasyQuant (EQ),that could obtain comparable\naccuracy with the training-based method.Specifically, we first alternately\noptimize scales of weights and activations for all layers target at\nconvolutional outputs to further obtain the high quantization precision. Then,\nwe lower down bit width to INT7 both for weights and activations, and adopt\nINT16 intermediate storage and integer Winograd convolution implementation to\naccelerate inference.Experimental results on various computer vision tasks show\nthat EQ outperforms the TensorRT method and can achieve near INT8 accuracy in 7\nbits width post-training.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 10:43:02 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Wu", "Di", ""], ["Tang", "Qi", ""], ["Zhao", "Yongle", ""], ["Zhang", "Ming", ""], ["Fu", "Ying", ""], ["Zhang", "Debing", ""]]}, {"id": "2006.16670", "submitter": "Kutsev Bengisu Ozyoruk", "authors": "Kutsev Bengisu Ozyoruk, Guliz Irem Gokceler, Gulfize Coskun, Kagan\n  Incetan, Yasin Almalioglu, Faisal Mahmood, Eva Curto, Luis Perdigoto, Marina\n  Oliveira, Hasan Sahin, Helder Araujo, Henrique Alexandrino, Nicholas J. Durr,\n  Hunter B. Gilbert, and Mehmet Turan", "title": "EndoSLAM Dataset and An Unsupervised Monocular Visual Odometry and Depth\n  Estimation Approach for Endoscopic Videos: Endo-SfMLearner", "comments": "27 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques hold promise to develop dense topography\nreconstruction and pose estimation methods for endoscopic videos. However,\ncurrently available datasets do not support effective quantitative\nbenchmarking. In this paper, we introduce a comprehensive endoscopic SLAM\ndataset consisting of 3D point cloud data for six porcine organs, capsule and\nstandard endoscopy recordings as well as synthetically generated data. A Panda\nrobotic arm, two commercially available capsule endoscopes, two conventional\nendoscopes with different camera properties, and two high precision 3D scanners\nwere employed to collect data from 8 ex-vivo porcine gastrointestinal\n(GI)-tract organs. In total, 35 sub-datasets are provided with 6D pose ground\ntruth for the ex-vivo part: 18 sub-dataset for colon, 12 sub-datasets for\nstomach and 5 sub-datasets for small intestine, while four of these contain\npolyp-mimicking elevations carried out by an expert gastroenterologist.\nSynthetic capsule endoscopy frames from GI-tract with both depth and pose\nannotations are included to facilitate the study of simulation-to-real transfer\nlearning algorithms. Additionally, we propound Endo-SfMLearner, an unsupervised\nmonocular depth and pose estimation method that combines residual networks with\nspatial attention module in order to dictate the network to focus on\ndistinguishable and highly textured tissue regions. The proposed approach makes\nuse of a brightness-aware photometric loss to improve the robustness under fast\nframe-to-frame illumination changes. To exemplify the use-case of the EndoSLAM\ndataset, the performance of Endo-SfMLearner is extensively compared with the\nstate-of-the-art. The codes and the link for the dataset are publicly available\nat https://github.com/CapsuleEndoscope/EndoSLAM. A video demonstrating the\nexperimental setup and procedure is accessible through\nhttps://www.youtube.com/watch?v=G_LCe0aWWdQ.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 10:43:27 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 21:45:25 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 13:44:32 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Ozyoruk", "Kutsev Bengisu", ""], ["Gokceler", "Guliz Irem", ""], ["Coskun", "Gulfize", ""], ["Incetan", "Kagan", ""], ["Almalioglu", "Yasin", ""], ["Mahmood", "Faisal", ""], ["Curto", "Eva", ""], ["Perdigoto", "Luis", ""], ["Oliveira", "Marina", ""], ["Sahin", "Hasan", ""], ["Araujo", "Helder", ""], ["Alexandrino", "Henrique", ""], ["Durr", "Nicholas J.", ""], ["Gilbert", "Hunter B.", ""], ["Turan", "Mehmet", ""]]}, {"id": "2006.16673", "submitter": "Shangchen Zhou", "authors": "Shangchen Zhou, Jiawei Zhang, Wangmeng Zuo, Chen Change Loy", "title": "Cross-Scale Internal Graph Neural Network for Image Super-Resolution", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-local self-similarity in natural images has been well studied as an\neffective prior in image restoration. However, for single image\nsuper-resolution (SISR), most existing deep non-local methods (e.g., non-local\nneural networks) only exploit similar patches within the same scale of the\nlow-resolution (LR) input image. Consequently, the restoration is limited to\nusing the same-scale information while neglecting potential high-resolution\n(HR) cues from other scales. In this paper, we explore the cross-scale patch\nrecurrence property of a natural image, i.e., similar patches tend to recur\nmany times across different scales. This is achieved using a novel cross-scale\ninternal graph neural network (IGNN). Specifically, we dynamically construct a\ncross-scale graph by searching k-nearest neighboring patches in the downsampled\nLR image for each query patch in the LR image. We then obtain the corresponding\nk HR neighboring patches in the LR image and aggregate them adaptively in\naccordance to the edge label of the constructed graph. In this way, the HR\ninformation can be passed from k HR neighboring patches to the LR query patch\nto help it recover more detailed textures. Besides, these internal\nimage-specific LR/HR exemplars are also significant complements to the external\ninformation learned from the training dataset. Extensive experiments\ndemonstrate the effectiveness of IGNN against the state-of-the-art SISR methods\nincluding existing non-local networks on standard benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 10:48:40 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 13:59:29 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Zhou", "Shangchen", ""], ["Zhang", "Jiawei", ""], ["Zuo", "Wangmeng", ""], ["Loy", "Chen Change", ""]]}, {"id": "2006.16675", "submitter": "Nils Gessert", "authors": "M. Gromniak and N. Gessert and T. Saathoff and A. Schlaefer", "title": "Needle tip force estimation by deep learning from raw spectral OCT data", "comments": "Accepted for publication in IJCARS", "journal-ref": null, "doi": "10.1007/s11548-020-02224-w", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose. Needle placement is a challenging problem for applications such as\nbiopsy or brachytherapy. Tip force sensing can provide valuable feedback for\nneedle navigation inside the tissue. For this purpose, fiber-optical sensors\ncan be directly integrated into the needle tip. Optical coherence tomography\n(OCT) can be used to image tissue. Here, we study how to calibrate OCT to sense\nforces, e.g. during robotic needle placement.\n  Methods. We investigate whether using raw spectral OCT data without a typical\nimage reconstruction can improve a deep learning-based calibration between\noptical signal and forces. For this purpose, we consider three different\nneedles with a new, more robust design which are calibrated using convolutional\nneural networks (CNNs). We compare training the CNNs with the raw OCT signal\nand the reconstructed depth profiles.\n  Results. We find that using raw data as an input for the largest CNN model\noutperforms the use of reconstructed data with a mean absolute error of 5.81 mN\ncompared to 8.04 mN.\n  Conclusions. We find that deep learning with raw spectral OCT data can\nimprove learning for the task of force estimation. Our needle design and\ncalibration approach constitute a very accurate fiber-optical sensor for\nmeasuring forces at the needle tip.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 10:49:54 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Gromniak", "M.", ""], ["Gessert", "N.", ""], ["Saathoff", "T.", ""], ["Schlaefer", "A.", ""]]}, {"id": "2006.16705", "submitter": "Yuval Bahat", "authors": "Yuval Bahat and Gregory Shakhnarovich", "title": "Classification Confidence Estimation with Test-Time Data-Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning plays an increasingly significant role in many aspects of\nour lives (including medicine, transportation, security, justice and other\ndomains), making the potential consequences of false predictions increasingly\ndevastating. These consequences may be mitigated if we can automatically flag\nsuch false predictions and potentially assign them to alternative, more\nreliable mechanisms, that are possibly more costly and involve human attention.\nThis suggests the task of detecting errors, which we tackle in this paper for\nthe case of visual classification. To this end, we propose a novel approach for\nclassification confidence estimation. We apply a set of semantics-preserving\nimage transformations to the input image, and show how the resulting image sets\ncan be used to estimate confidence in the classifier's prediction. We\ndemonstrate the potential of our approach by extensively evaluating it on a\nwide variety of classifier architectures and datasets, including\nResNext/ImageNet, achieving state of the art performance. This paper\nconstitutes a significant revision of our earlier work in this direction (Bahat\n& Shakhnarovich, 2018).\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 11:59:53 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Bahat", "Yuval", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "2006.16736", "submitter": "Robert Geirhos", "authors": "Robert Geirhos, Kristof Meding, Felix A. Wichmann", "title": "Beyond accuracy: quantifying trial-by-trial behaviour of CNNs and humans\n  by measuring error consistency", "comments": "NeurIPS 2020 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in cognitive science and behavioural neuroscience as well\nas in machine learning and artificial intelligence research is to ascertain\nwhether two or more decision makers (be they brains or algorithms) use the same\nstrategy. Accuracy alone cannot distinguish between strategies: two systems may\nachieve similar accuracy with very different strategies. The need to\ndifferentiate beyond accuracy is particularly pressing if two systems are near\nceiling performance, like Convolutional Neural Networks (CNNs) and humans on\nvisual object recognition. Here we introduce trial-by-trial error consistency,\na quantitative analysis for measuring whether two decision making systems\nsystematically make errors on the same inputs. Making consistent errors on a\ntrial-by-trial basis is a necessary condition for similar processing strategies\nbetween decision makers. Our analysis is applicable to compare algorithms with\nalgorithms, humans with humans, and algorithms with humans. When applying error\nconsistency to object recognition we obtain three main findings: (1.)\nIrrespective of architecture, CNNs are remarkably consistent with one another.\n(2.) The consistency between CNNs and human observers, however, is little above\nwhat can be expected by chance alone -- indicating that humans and CNNs are\nlikely implementing very different strategies. (3.) CORnet-S, a recurrent model\ntermed the \"current best model of the primate ventral visual stream\", fails to\ncapture essential characteristics of human behavioural data and behaves\nessentially like a standard purely feedforward ResNet-50 in our analysis. Taken\ntogether, error consistency analysis suggests that the strategies used by human\nand machine vision are still very different -- but we envision our\ngeneral-purpose error consistency analysis to serve as a fruitful tool for\nquantifying future progress.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 12:47:17 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 08:43:53 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 15:39:48 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Geirhos", "Robert", ""], ["Meding", "Kristof", ""], ["Wichmann", "Felix A.", ""]]}, {"id": "2006.16741", "submitter": "Ben Glocker", "authors": "R. Robinson, Q. Dou, D.C. Castro, K. Kamnitsas, M. de Groot, R.M.\n  Summers, D. Rueckert, B. Glocker", "title": "Image-level Harmonization of Multi-Site Data using Image-and-Spatial\n  Transformer Networks", "comments": "Accepted at MICCAI 2020", "journal-ref": "Medical Image Computing and Computer-Assisted Intervention (2020),\n  pp. 710-719, LNCS 12267", "doi": "10.1007/978-3-030-59728-3_69", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of image-and-spatial transformer networks (ISTNs) to\ntackle domain shift in multi-site medical imaging data. Commonly, domain\nadaptation (DA) is performed with little regard for explainability of the\ninter-domain transformation and is often conducted at the feature-level in the\nlatent space. We employ ISTNs for DA at the image-level which constrains\ntransformations to explainable appearance and shape changes. As\nproof-of-concept we demonstrate that ISTNs can be trained adversarially on a\nclassification problem with simulated 2D data. For real-data validation, we\nconstruct two 3D brain MRI datasets from the Cam-CAN and UK Biobank studies to\ninvestigate domain shift due to acquisition and population differences. We show\nthat age regression and sex classification models trained on ISTN output\nimprove generalization when training on data from one and testing on the other\nsite.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 12:58:41 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Robinson", "R.", ""], ["Dou", "Q.", ""], ["Castro", "D. C.", ""], ["Kamnitsas", "K.", ""], ["de Groot", "M.", ""], ["Summers", "R. M.", ""], ["Rueckert", "D.", ""], ["Glocker", "B.", ""]]}, {"id": "2006.16777", "submitter": "Taro Langner", "authors": "Taro Langner, Robin Strand, H{\\aa}kan Ahlstr\\\"om, Joel Kullberg", "title": "Large-scale inference of liver fat with neural networks on UK Biobank\n  body MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The UK Biobank Imaging Study has acquired medical scans of more than 40,000\nvolunteer participants. The resulting wealth of anatomical information has been\nmade available for research, together with extensive metadata including\nmeasurements of liver fat. These values play an important role in metabolic\ndisease, but are only available for a minority of imaged subjects as their\ncollection requires the careful work of image analysts on dedicated liver MRI.\nAnother UK Biobank protocol is neck-to-knee body MRI for analysis of body\ncomposition. The resulting volumes can also quantify fat fractions, even though\nthey were reconstructed with a two- instead of a three-point Dixon technique.\nIn this work, a novel framework for automated inference of liver fat from UK\nBiobank neck-to-knee body MRI is proposed. A ResNet50 was trained for\nregression on two-dimensional slices from these scans and the reference values\nas target, without any need for ground truth segmentations. Once trained, it\nperforms fast, objective, and fully automated predictions that require no\nmanual intervention. On the given data, it closely emulates the reference\nmethod, reaching a level of agreement comparable to different gold standard\ntechniques. The network learned to rectify non-linearities in the fat fraction\nvalues and identified several outliers in the reference. It outperformed a\nmulti-atlas segmentation baseline and inferred new estimates for all imaged\nsubjects lacking reference values, expanding the total number of liver fat\nmeasurements by factor six.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 13:33:30 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Langner", "Taro", ""], ["Strand", "Robin", ""], ["Ahlstr\u00f6m", "H\u00e5kan", ""], ["Kullberg", "Joel", ""]]}, {"id": "2006.16795", "submitter": "Sonia Poltoratski", "authors": "J\\~nani Crawford, Eshed Margalit, Kalanit Grill-Spector, and Sonia\n  Poltoratski", "title": "Validation and generalization of pixel-wise relevance in convolutional\n  neural networks trained for face classification", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased use of convolutional neural networks for face recognition in\nscience, governance, and broader society has created an acute need for methods\nthat can show how these 'black box' decisions are made. To be interpretable and\nuseful to humans, such a method should convey a model's learned classification\nstrategy in a way that is robust to random initializations or spurious\ncorrelations in input data. To this end, we applied the decompositional\npixel-wise attribution method of layer-wise relevance propagation (LRP) to\nresolve the decisions of several classes of VGG-16 models trained for face\nrecognition. We then quantified how these relevance measures vary with and\ngeneralize across key model parameters, such as the pretraining dataset\n(ImageNet or VGGFace), the finetuning task (gender or identity classification),\nand random initializations of model weights. Using relevance-based image\nmasking, we find that relevance maps for face classification prove generally\nstable across random initializations, and can generalize across finetuning\ntasks. However, there is markedly less generalization across pretraining\ndatasets, indicating that ImageNet- and VGGFace-trained models sample face\ninformation differently even as they achieve comparably high classification\nperformance. Fine-grained analyses of relevance maps across models revealed\nasymmetries in generalization that point to specific benefits of choice\nparameters, and suggest that it may be possible to find an underlying set of\nimportant face image pixels that drive decisions across convolutional neural\nnetworks and tasks. Finally, we evaluated model decision weighting against\nhuman measures of similarity, providing a novel framework for interpreting face\nrecognition decisions across human and machine.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 23:20:40 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Crawford", "J\u00f1ani", ""], ["Margalit", "Eshed", ""], ["Grill-Spector", "Kalanit", ""], ["Poltoratski", "Sonia", ""]]}, {"id": "2006.16796", "submitter": "Zhongang Cai", "authors": "Cunjun Yu, Zhongang Cai, Daxuan Ren, Haiyu Zhao", "title": "Leveraging Temporal Information for 3D Detection and Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ever since the prevalent use of the LiDARs in autonomous driving, tremendous\nimprovements have been made to the learning on the point clouds. However,\nrecent progress largely focuses on detecting objects in a single 360-degree\nsweep, without extensively exploring the temporal information. In this report,\nwe describe a simple way to pass such information in the learning pipeline by\nadding timestamps to the point clouds, which shows consistent improvements\nacross all three classes.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 13:47:28 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Yu", "Cunjun", ""], ["Cai", "Zhongang", ""], ["Ren", "Daxuan", ""], ["Zhao", "Haiyu", ""]]}, {"id": "2006.16806", "submitter": "Yingda Xia", "authors": "Yingda Xia, Dong Yang, Zhiding Yu, Fengze Liu, Jinzheng Cai, Lequan\n  Yu, Zhuotun Zhu, Daguang Xu, Alan Yuille, Holger Roth", "title": "Uncertainty-aware multi-view co-training for semi-supervised medical\n  image segmentation and domain adaptation", "comments": "19 pages, 6 figures, to appear in Medical Image Analysis. This\n  article is an extension of the conference paper arXiv:1811.12506", "journal-ref": "Medical Image Analysis, 2020", "doi": "10.1016/j.media.2020.101766", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although having achieved great success in medical image segmentation, deep\nlearning-based approaches usually require large amounts of well-annotated data,\nwhich can be extremely expensive in the field of medical image analysis.\nUnlabeled data, on the other hand, is much easier to acquire. Semi-supervised\nlearning and unsupervised domain adaptation both take the advantage of\nunlabeled data, and they are closely related to each other. In this paper, we\npropose uncertainty-aware multi-view co-training (UMCT), a unified framework\nthat addresses these two tasks for volumetric medical image segmentation. Our\nframework is capable of efficiently utilizing unlabeled data for better\nperformance. We firstly rotate and permute the 3D volumes into multiple views\nand train a 3D deep network on each view. We then apply co-training by\nenforcing multi-view consistency on unlabeled data, where an uncertainty\nestimation of each view is utilized to achieve accurate labeling. Experiments\non the NIH pancreas segmentation dataset and a multi-organ segmentation dataset\nshow state-of-the-art performance of the proposed framework on semi-supervised\nmedical image segmentation. Under unsupervised domain adaptation settings, we\nvalidate the effectiveness of this work by adapting our multi-organ\nsegmentation model to two pathological organs from the Medical Segmentation\nDecathlon Datasets. Additionally, we show that our UMCT-DA model can even\neffectively handle the challenging situation where labeled source data is\ninaccessible, demonstrating strong potentials for real-world applications.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 22:04:54 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Xia", "Yingda", ""], ["Yang", "Dong", ""], ["Yu", "Zhiding", ""], ["Liu", "Fengze", ""], ["Cai", "Jinzheng", ""], ["Yu", "Lequan", ""], ["Zhu", "Zhuotun", ""], ["Xu", "Daguang", ""], ["Yuille", "Alan", ""], ["Roth", "Holger", ""]]}, {"id": "2006.16829", "submitter": "Xi Peng", "authors": "Boyun Li, Yuanbiao Gou, Shuhang Gu, Jerry Zitao Liu, Joey Tianyi Zhou,\n  Xi Peng", "title": "You Only Look Yourself: Unsupervised and Untrained Single Image Dehazing\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study two challenging and less-touched problems in single\nimage dehazing, namely, how to make deep learning achieve image dehazing\nwithout training on the ground-truth clean image (unsupervised) and a image\ncollection (untrained). An unsupervised neural network will avoid the intensive\nlabor collection of hazy-clean image pairs, and an untrained model is a\n``real'' single image dehazing approach which could remove haze based on only\nthe observed hazy image itself and no extra images is used. Motivated by the\nlayer disentanglement idea, we propose a novel method, called you only look\nyourself (\\textbf{YOLY}) which could be one of the first unsupervised and\nuntrained neural networks for image dehazing. In brief, YOLY employs three\njointly subnetworks to separate the observed hazy image into several latent\nlayers, \\textit{i.e.}, scene radiance layer, transmission map layer, and\natmospheric light layer. After that, these three layers are further composed to\nthe hazy image in a self-supervised manner. Thanks to the unsupervised and\nuntrained characteristics of YOLY, our method bypasses the conventional\ntraining paradigm of deep models on hazy-clean pairs or a large scale dataset,\nthus avoids the labor-intensive data collection and the domain shift issue.\nBesides, our method also provides an effective learning-based haze transfer\nsolution thanks to its layer disentanglement mechanism. Extensive experiments\nshow the promising performance of our method in image dehazing compared with 14\nmethods on four databases.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 14:05:47 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Li", "Boyun", ""], ["Gou", "Yuanbiao", ""], ["Gu", "Shuhang", ""], ["Liu", "Jerry Zitao", ""], ["Zhou", "Joey Tianyi", ""], ["Peng", "Xi", ""]]}, {"id": "2006.16836", "submitter": "Anjith George", "authors": "Anjith George and Sebastien Marcel", "title": "Can Your Face Detector Do Anti-spoofing? Face Presentation Attack\n  Detection with a Multi-Channel Face Detector", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": "Idiap-RR-12-2020", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a typical face recognition pipeline, the task of the face detector is to\nlocalize the face region. However, the face detector localizes regions that\nlook like a face, irrespective of the liveliness of the face, which makes the\nentire system susceptible to presentation attacks. In this work, we try to\nreformulate the task of the face detector to detect real faces, thus\neliminating the threat of presentation attacks. While this task could be\nchallenging with visible spectrum images alone, we leverage the multi-channel\ninformation available from off the shelf devices (such as color, depth, and\ninfrared channels) to design a multi-channel face detector. The proposed system\ncan be used as a live-face detector obviating the need for a separate\npresentation attack detection module, making the system reliable in practice\nwithout any additional computational overhead. The main idea is to leverage a\nsingle-stage object detection framework, with a joint representation obtained\nfrom different channels for the PAD task. We have evaluated our approach in the\nmulti-channel WMCA dataset containing a wide variety of attacks to show the\neffectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 14:22:46 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 09:14:54 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["George", "Anjith", ""], ["Marcel", "Sebastien", ""]]}, {"id": "2006.16841", "submitter": "Adam Kosiorek", "authors": "Adam R Kosiorek, Hyunjik Kim, Danilo J Rezende", "title": "Conditional Set Generation with Transformers", "comments": "6 pages, 6 figures, ICML 2020 Workshop on Object-Oriented Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set is an unordered collection of unique elements--and yet many machine\nlearning models that generate sets impose an implicit or explicit ordering.\nSince model performance can depend on the choice of order, any particular\nordering can lead to sub-optimal results. An alternative solution is to use a\npermutation-equivariant set generator, which does not specify an order-ing. An\nexample of such a generator is the DeepSet Prediction Network (DSPN). We\nintroduce the Transformer Set Prediction Network (TSPN), a flexible\npermutation-equivariant model for set prediction based on the transformer, that\nbuilds upon and outperforms DSPN in the quality of predicted set elements and\nin the accuracy of their predicted sizes. We test our model on\nMNIST-as-point-clouds (SET-MNIST) for point-cloud generation and on CLEVR for\nobject detection.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 17:52:27 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 06:00:00 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Kosiorek", "Adam R", ""], ["Kim", "Hyunjik", ""], ["Rezende", "Danilo J", ""]]}, {"id": "2006.16866", "submitter": "Hannah Kerner", "authors": "Hannah Kerner, Gabriel Tseng, Inbal Becker-Reshef, Catherine\n  Nakalembe, Brian Barker, Blake Munshell, Madhava Paliyam, and Mehdi Hosseini", "title": "Rapid Response Crop Maps in Data Sparse Regions", "comments": "Presented at KDD 2020 Humanitarian Mapping Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial information on cropland distribution, often called cropland or crop\nmaps, are critical inputs for a wide range of agriculture and food security\nanalyses and decisions. However, high-resolution cropland maps are not readily\navailable for most countries, especially in regions dominated by smallholder\nfarming (e.g., sub-Saharan Africa). These maps are especially critical in times\nof crisis when decision makers need to rapidly design and enact\nagriculture-related policies and mitigation strategies, including providing\nhumanitarian assistance, dispersing targeted aid, or boosting productivity for\nfarmers. A major challenge for developing crop maps is that many regions do not\nhave readily accessible ground truth data on croplands necessary for training\nand validating predictive models, and field campaigns are not feasible for\ncollecting labels for rapid response. We present a method for rapid mapping of\ncroplands in regions where little to no ground data is available. We present\nresults for this method in Togo, where we delivered a high-resolution (10 m)\ncropland map in under 10 days to facilitate rapid response to the COVID-19\npandemic by the Togolese government. This demonstrated a successful transition\nof machine learning applications research to operational rapid response in a\nreal humanitarian crisis. All maps, data, and code are publicly available to\nenable future research and operational systems in data-sparse regions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:19:26 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Kerner", "Hannah", ""], ["Tseng", "Gabriel", ""], ["Becker-Reshef", "Inbal", ""], ["Nakalembe", "Catherine", ""], ["Barker", "Brian", ""], ["Munshell", "Blake", ""], ["Paliyam", "Madhava", ""], ["Hosseini", "Mehdi", ""]]}, {"id": "2006.16867", "submitter": "Matthias Rath", "authors": "Matthias Rath and Alexandru Paul Condurache", "title": "Boosting Deep Neural Networks with Geometrical Prior Knowledge: A Survey", "comments": "Survey Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Deep Neural Networks (DNNs) achieve state-of-the-art results in many\ndifferent problem settings, they are affected by some crucial weaknesses. On\nthe one hand, DNNs depend on exploiting a vast amount of training data, whose\nlabeling process is time-consuming and expensive. On the other hand, DNNs are\noften treated as black box systems, which complicates their evaluation and\nvalidation. Both problems can be mitigated by incorporating prior knowledge\ninto the DNN.\n  One promising field, inspired by the success of convolutional neural networks\n(CNNs) in computer vision tasks, is to incorporate knowledge about symmetric\ngeometrical transformations of the problem to solve. This promises an increased\ndata-efficiency and filter responses that are interpretable more easily. In\nthis survey, we try to give a concise overview about different approaches to\nincorporate geometrical prior knowledge into DNNs. Additionally, we try to\nconnect those methods to the field of 3D object detection for autonomous\ndriving, where we expect promising results applying those methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 14:56:05 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Rath", "Matthias", ""], ["Condurache", "Alexandru Paul", ""]]}, {"id": "2006.16868", "submitter": "Tuan Tran", "authors": "Tuan Tran, Jory Denny, Chinwe Ekenna", "title": "Predicting Sample Collision with Neural Networks", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-art robotics applications require fast and efficient motion\nplanning algorithms. Existing motion planning methods become less effective as\nthe dimensionality of the robot and its workspace increases, especially the\ncomputational cost of collision detection routines. In this work, we present a\nframework to address the cost of expensive primitive operations in\nsampling-based motion planning. This framework determines the validity of a\nsample robot configuration through a novel combination of a Contractive\nAutoEncoder (CAE), which captures a occupancy grids representation of the\nrobot's workspace, and a Multilayer Perceptron, which efficiently predicts the\ncollision state of the robot from the CAE and the robot's configuration. We\nevaluate our framework on multiple planning problems with a variety of robots\nin 2D and 3D workspaces. The results show that (1) the framework is\ncomputationally efficient in all investigated problems, and (2) the framework\ngeneralizes well to new workspaces.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 14:56:14 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Tran", "Tuan", ""], ["Denny", "Jory", ""], ["Ekenna", "Chinwe", ""]]}, {"id": "2006.16893", "submitter": "Daniel Berj\\'on", "authors": "Daniel Berj\\'on, Pablo Carballeira, Juli\\'an Cabrera, Carlos Carmona,\n  Daniel Corregidor, C\\'esar D\\'iaz, Francisco Mor\\'an, Narciso Garc\\'ia", "title": "FVV Live: Real-Time, Low-Cost, Free Viewpoint Video", "comments": null, "journal-ref": null, "doi": "10.1109/ICMEW46912.2020.9105977", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FVV Live is a novel real-time, low-latency, end-to-end free viewpoint system\nincluding capture, transmission, synthesis on an edge server and visualization\nand control on a mobile terminal. The system has been specially designed for\nlow-cost and real-time operation, only using off-the-shelf components.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 15:21:53 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Berj\u00f3n", "Daniel", ""], ["Carballeira", "Pablo", ""], ["Cabrera", "Juli\u00e1n", ""], ["Carmona", "Carlos", ""], ["Corregidor", "Daniel", ""], ["D\u00edaz", "C\u00e9sar", ""], ["Mor\u00e1n", "Francisco", ""], ["Garc\u00eda", "Narciso", ""]]}, {"id": "2006.16934", "submitter": "Jiji Tang", "authors": "Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, Haifeng\n  Wang", "title": "ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through\n  Scene Graph", "comments": "Paper has been published in the AAAI2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a knowledge-enhanced approach, ERNIE-ViL, which incorporates\nstructured knowledge obtained from scene graphs to learn joint representations\nof vision-language. ERNIE-ViL tries to build the detailed semantic connections\n(objects, attributes of objects and relationships between objects) across\nvision and language, which are essential to vision-language cross-modal tasks.\nUtilizing scene graphs of visual scenes, ERNIE-ViL constructs Scene Graph\nPrediction tasks, i.e., Object Prediction, Attribute Prediction and\nRelationship Prediction tasks in the pre-training phase. Specifically, these\nprediction tasks are implemented by predicting nodes of different types in the\nscene graph parsed from the sentence. Thus, ERNIE-ViL can learn the joint\nrepresentations characterizing the alignments of the detailed semantics across\nvision and language. After pre-training on large scale image-text aligned\ndatasets, we validate the effectiveness of ERNIE-ViL on 5 cross-modal\ndownstream tasks. ERNIE-ViL achieves state-of-the-art performances on all these\ntasks and ranks the first place on the VCR leaderboard with an absolute\nimprovement of 3.7%.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 16:03:12 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 06:49:34 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 05:17:32 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Yu", "Fei", ""], ["Tang", "Jiji", ""], ["Yin", "Weichong", ""], ["Sun", "Yu", ""], ["Tian", "Hao", ""], ["Wu", "Hua", ""], ["Wang", "Haifeng", ""]]}, {"id": "2006.16956", "submitter": "Leonardo de Melo Joao", "authors": "Leonardo de Melo Joao, Felipe de Castro Belem, Alexandre Xavier Falcao", "title": "ITSELF: Iterative Saliency Estimation fLexible Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency object detection estimates the objects that most stand out in an\nimage. The available unsupervised saliency estimators rely on a pre-determined\nset of assumptions of how humans perceive saliency to create discriminating\nfeatures. By fixing the pre-selected assumptions as an integral part of their\nmodels, these methods cannot be easily extended for specific settings and\ndifferent image domains. We then propose a superpixel-based ITerative Saliency\nEstimation fLexible Framework (ITSELF) that allows any user-defined assumptions\nto be added to the model when required. Thanks to recent advancements in\nsuperpixel segmentation algorithms, saliency-maps can be used to improve\nsuperpixel delineation. By combining a saliency-based superpixel algorithm to a\nsuperpixel-based saliency estimator, we propose a novel saliency/superpixel\nself-improving loop to iteratively enhance saliency maps. We compare ITSELF to\ntwo state-of-the-art saliency estimators on five metrics and six datasets, four\nof which are composed of natural-images, and two of biomedical-images.\nExperiments show that our approach is more robust than the compared methods,\npresenting competitive results on natural-image datasets and outperforming them\non biomedical-image datasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 16:51:31 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 19:16:37 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Joao", "Leonardo de Melo", ""], ["Belem", "Felipe de Castro", ""], ["Falcao", "Alexandre Xavier", ""]]}, {"id": "2006.16971", "submitter": "Steffen Schneider", "authors": "Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland\n  Brendel, Matthias Bethge", "title": "Improving robustness against common corruptions by covariate shift\n  adaptation", "comments": "Accepted at the Thirty-fourth Conference on Neural Information\n  Processing Systems. Web: https://domainadaptation.org/batchnorm/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's state-of-the-art machine vision models are vulnerable to image\ncorruptions like blurring or compression artefacts, limiting their performance\nin many real-world applications. We here argue that popular benchmarks to\nmeasure model robustness against common corruptions (like ImageNet-C)\nunderestimate model robustness in many (but not all) application scenarios. The\nkey insight is that in many scenarios, multiple unlabeled examples of the\ncorruptions are available and can be used for unsupervised online adaptation.\nReplacing the activation statistics estimated by batch normalization on the\ntraining set with the statistics of the corrupted images consistently improves\nthe robustness across 25 different popular computer vision models. Using the\ncorrected statistics, ResNet-50 reaches 62.2% mCE on ImageNet-C compared to\n76.7% without adaptation. With the more robust DeepAugment+AugMix model, we\nimprove the state of the art achieved by a ResNet50 model up to date from 53.6%\nmCE to 45.4% mCE. Even adapting to a single sample improves robustness for the\nResNet-50 and AugMix models, and 32 samples are sufficient to improve the\ncurrent state of the art for a ResNet-50 architecture. We argue that results\nwith adapted statistics should be included whenever reporting scores in\ncorruption benchmarks and other out-of-distribution generalization settings.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 17:01:37 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 04:37:23 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Schneider", "Steffen", ""], ["Rusak", "Evgenia", ""], ["Eck", "Luisa", ""], ["Bringmann", "Oliver", ""], ["Brendel", "Wieland", ""], ["Bethge", "Matthias", ""]]}, {"id": "2006.16974", "submitter": "Jiachen Sun", "authors": "Jiachen Sun, Yulong Cao, Qi Alfred Chen, Z. Morley Mao", "title": "Towards Robust LiDAR-based Perception in Autonomous Driving: General\n  Black-box Adversarial Sensor Attack and Countermeasures", "comments": "18 pages, 27 figures, to be published in USENIX Security 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perception plays a pivotal role in autonomous driving systems, which utilizes\nonboard sensors like cameras and LiDARs (Light Detection and Ranging) to assess\nsurroundings. Recent studies have demonstrated that LiDAR-based perception is\nvulnerable to spoofing attacks, in which adversaries spoof a fake vehicle in\nfront of a victim self-driving car by strategically transmitting laser signals\nto the victim's LiDAR sensor. However, existing attacks suffer from\neffectiveness and generality limitations. In this work, we perform the first\nstudy to explore the general vulnerability of current LiDAR-based perception\narchitectures and discover that the ignored occlusion patterns in LiDAR point\nclouds make self-driving cars vulnerable to spoofing attacks. We construct the\nfirst black-box spoofing attack based on our identified vulnerability, which\nuniversally achieves around 80% mean success rates on all target models. We\nperform the first defense study, proposing CARLO to mitigate LiDAR spoofing\nattacks. CARLO detects spoofed data by treating ignored occlusion patterns as\ninvariant physical features, which reduces the mean attack success rate to\n5.5%. Meanwhile, we take the first step towards exploring a general\narchitecture for robust LiDAR-based perception, and propose SVF that embeds the\nneglected physical features into end-to-end learning. SVF further reduces the\nmean attack success rate to around 2.3%.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 17:07:45 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Sun", "Jiachen", ""], ["Cao", "Yulong", ""], ["Chen", "Qi Alfred", ""], ["Mao", "Z. Morley", ""]]}, {"id": "2006.16976", "submitter": "Nikhil Parthasarathy", "authors": "Nikhil Parthasarathy and Eero P. Simoncelli", "title": "Self-Supervised Learning of a Biologically-Inspired Visual Texture Model", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a model for representing visual texture in a low-dimensional\nfeature space, along with a novel self-supervised learning objective that is\nused to train it on an unlabeled database of texture images. Inspired by the\narchitecture of primate visual cortex, the model uses a first stage of oriented\nlinear filters (corresponding to cortical area V1), consisting of both\nrectified units (simple cells) and pooled phase-invariant units (complex\ncells). These responses are processed by a second stage (analogous to cortical\narea V2) consisting of convolutional filters followed by half-wave\nrectification and pooling to generate V2 'complex cell' responses. The second\nstage filters are trained on a set of unlabeled homogeneous texture images,\nusing a novel contrastive objective that maximizes the distance between the\ndistribution of V2 responses to individual images and the distribution of\nresponses across all images. When evaluated on texture classification, the\ntrained model achieves substantially greater data-efficiency than a variety of\ndeep hierarchical model architectures. Moreover, we show that the learned model\nexhibits stronger representational similarity to texture responses of neural\npopulations recorded in primate V2 than pre-trained deep CNNs.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 17:12:09 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Parthasarathy", "Nikhil", ""], ["Simoncelli", "Eero P.", ""]]}, {"id": "2006.16990", "submitter": "Shuyang Gu", "authors": "Shuyang Gu, Jianmin Bao, Dong Chen, Fang Wen", "title": "PriorGAN: Real Data Prior for Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have achieved rapid progress in\nlearning rich data distributions. However, we argue about two main issues in\nexisting techniques. First, the low quality problem where the learned\ndistribution has massive low quality samples. Second, the missing modes problem\nwhere the learned distribution misses some certain regions of the real data\ndistribution. To address these two issues, we propose a novel prior that\ncaptures the whole real data distribution for GANs, which are called PriorGANs.\nTo be specific, we adopt a simple yet elegant Gaussian Mixture Model (GMM) to\nbuild an explicit probability distribution on the feature level for the whole\nreal data. By maximizing the probability of generated data, we can push the low\nquality samples to high quality. Meanwhile, equipped with the prior, we can\nestimate the missing modes in the learned distribution and design a sampling\nstrategy on the real data to solve the problem. The proposed real data prior\ncan generalize to various training settings of GANs, such as LSGAN, WGAN-GP,\nSNGAN, and even the StyleGAN. Our experiments demonstrate that PriorGANs\noutperform the state-of-the-art on the CIFAR-10, FFHQ, LSUN-cat, and LSUN-bird\ndatasets by large margins.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 17:51:47 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Gu", "Shuyang", ""], ["Bao", "Jianmin", ""], ["Chen", "Dong", ""], ["Wen", "Fang", ""]]}, {"id": "2006.16992", "submitter": "Haozhi Qi", "authors": "Haozhi Qi, Chong You, Xiaolong Wang, Yi Ma, Jitendra Malik", "title": "Deep Isometric Learning for Visual Recognition", "comments": "ICML 2020; Code: https://github.com/HaozhiQi/ISONet; Website:\n  https://haozhiqi.github.io/ISONet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initialization, normalization, and skip connections are believed to be three\nindispensable techniques for training very deep convolutional neural networks\nand obtaining state-of-the-art performance. This paper shows that deep vanilla\nConvNets without normalization nor skip connections can also be trained to\nachieve surprisingly good performance on standard image recognition benchmarks.\nThis is achieved by enforcing the convolution kernels to be near isometric\nduring initialization and training, as well as by using a variant of ReLU that\nis shifted towards being isometric. Further experiments show that if combined\nwith skip connections, such near isometric networks can achieve performances on\npar with (for ImageNet) and better than (for COCO) the standard ResNet, even\nwithout normalization at all. Our code is available at\nhttps://github.com/HaozhiQi/ISONet.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 17:53:13 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 04:39:34 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Qi", "Haozhi", ""], ["You", "Chong", ""], ["Wang", "Xiaolong", ""], ["Ma", "Yi", ""], ["Malik", "Jitendra", ""]]}]