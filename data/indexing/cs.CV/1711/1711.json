[{"id": "1711.00002", "submitter": "Hanzhang Hu", "authors": "Hanzhang Hu, Debadeepta Dey, Allison Del Giorno, Martial Hebert, J.\n  Andrew Bagnell", "title": "Log-DenseNet: How to Sparsify a DenseNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skip connections are increasingly utilized by deep neural networks to improve\naccuracy and cost-efficiency. In particular, the recent DenseNet is efficient\nin computation and parameters, and achieves state-of-the-art predictions by\ndirectly connecting each feature layer to all previous ones. However,\nDenseNet's extreme connectivity pattern may hinder its scalability to high\ndepths, and in applications like fully convolutional networks, full DenseNet\nconnections are prohibitively expensive. This work first experimentally shows\nthat one key advantage of skip connections is to have short distances among\nfeature layers during backpropagation. Specifically, using a fixed number of\nskip connections, the connection patterns with shorter backpropagation distance\namong layers have more accurate predictions. Following this insight, we propose\na connection template, Log-DenseNet, which, in comparison to DenseNet, only\nslightly increases the backpropagation distances among layers from 1 to ($1 +\n\\log_2 L$), but uses only $L\\log_2 L$ total connections instead of $O(L^2)$.\nHence, Log-DenseNets are easier than DenseNets to implement and to scale. We\ndemonstrate the effectiveness of our design principle by showing better\nperformance than DenseNets on tabula rasa semantic segmentation, and\ncompetitive results on visual recognition.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 22:01:08 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Hu", "Hanzhang", ""], ["Dey", "Debadeepta", ""], ["Del Giorno", "Allison", ""], ["Hebert", "Martial", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1711.00003", "submitter": "Gaurav Bhatt", "authors": "Gaurav Bhatt, Piyush Jha, and Balasubramanian Raman", "title": "Common Representation Learning Using Step-based Correlation Multi-Modal\n  CNN", "comments": "Accepted in Asian Conference of Pattern Recognition (ACPR-2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques have been successfully used in learning a common\nrepresentation for multi-view data, wherein the different modalities are\nprojected onto a common subspace. In a broader perspective, the techniques used\nto investigate common representation learning falls under the categories of\ncanonical correlation-based approaches and autoencoder based approaches. In\nthis paper, we investigate the performance of deep autoencoder based methods on\nmulti-view data. We propose a novel step-based correlation multi-modal CNN\n(CorrMCNN) which reconstructs one view of the data given the other while\nincreasing the interaction between the representations at each hidden layer or\nevery intermediate step. Finally, we evaluate the performance of the proposed\nmodel on two benchmark datasets - MNIST and XRMB. Through extensive\nexperiments, we find that the proposed model achieves better performance than\nthe current state-of-the-art techniques on joint common representation learning\nand transfer learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 07:43:34 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Bhatt", "Gaurav", ""], ["Jha", "Piyush", ""], ["Raman", "Balasubramanian", ""]]}, {"id": "1711.00049", "submitter": "Xiang Li", "authors": "Zhe Guo, Xiang Li, Heng Huang, Ning Guo, Quanzheng Li", "title": "Medical Image Segmentation Based on Multi-Modal Convolutional Neural\n  Network: Study on Image Fusion Schemes", "comments": "Zhe Guo and Xiang Li contribute equally to this work", "journal-ref": "2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI\n  2018), Washington, DC, 2018, pp. 903-907", "doi": "10.1109/ISBI.2018.8363717", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image analysis using more than one modality (i.e. multi-modal) has been\nincreasingly applied in the field of biomedical imaging. One of the challenges\nin performing the multimodal analysis is that there exist multiple schemes for\nfusing the information from different modalities, where such schemes are\napplication-dependent and lack a unified framework to guide their designs. In\nthis work we firstly propose a conceptual architecture for the image fusion\nschemes in supervised biomedical image analysis: fusing at the feature level,\nfusing at the classifier level, and fusing at the decision-making level.\nFurther, motivated by the recent success in applying deep learning for natural\nimage analysis, we implement the three image fusion schemes above based on the\nConvolutional Neural Network (CNN) with varied structures, and combined into a\nsingle framework. The proposed image segmentation framework is capable of\nanalyzing the multi-modality images using different fusing schemes\nsimultaneously. The framework is applied to detect the presence of soft tissue\nsarcoma from the combination of Magnetic Resonance Imaging (MRI), Computed\nTomography (CT) and Positron Emission Tomography (PET) images. It is found from\nthe results that while all the fusion schemes outperform the single-modality\nschemes, fusing at the feature level can generally achieve the best performance\nin terms of both accuracy and computational cost, but also suffers from the\ndecreased robustness in the presence of large errors in any image modalities.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 18:37:28 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 16:02:28 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Guo", "Zhe", ""], ["Li", "Xiang", ""], ["Huang", "Heng", ""], ["Guo", "Ning", ""], ["Li", "Quanzheng", ""]]}, {"id": "1711.00088", "submitter": "Melanie Mitchell", "authors": "Max H. Quinn, Erik Conser, Jordan M. Witte, and Melanie Mitchell", "title": "Semantic Image Retrieval via Active Grounding of Visual Situations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel architecture for semantic image retrieval---in\nparticular, retrieval of instances of visual situations. Visual situations are\nconcepts such as \"a boxing match,\" \"walking the dog,\" \"a crowd waiting for a\nbus,\" or \"a game of ping-pong,\" whose instantiations in images are linked more\nby their common spatial and semantic structure than by low-level visual\nsimilarity. Given a query situation description, our architecture---called\nSituate---learns models capturing the visual features of expected objects as\nwell the expected spatial configuration of relationships among objects. Given a\nnew image, Situate uses these models in an attempt to ground (i.e., to create a\nbounding box locating) each expected component of the situation in the image\nvia an active search procedure. Situate uses the resulting grounding to compute\na score indicating the degree to which the new image is judged to contain an\ninstance of the situation. Such scores can be used to rank images in a\ncollection as part of a retrieval system. In the preliminary study described\nhere, we demonstrate the promise of this system by comparing Situate's\nperformance with that of two baseline methods, as well as with a related\nsemantic image-retrieval system based on \"scene graphs.\"\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 20:15:49 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Quinn", "Max H.", ""], ["Conser", "Erik", ""], ["Witte", "Jordan M.", ""], ["Mitchell", "Melanie", ""]]}, {"id": "1711.00107", "submitter": "James Goldfarb", "authors": "James W Goldfarb", "title": "Separation of Water and Fat Magnetic Resonance Imaging Signals Using\n  Deep Learning with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: A new method for magnetic resonance (MR) imaging water-fat\nseparation using a convolutional neural network (ConvNet) and deep learning\n(DL) is presented. Feasibility of the method with complex and magnitude images\nis demonstrated with a series of patient studies and accuracy of predicted\nquantitative values is analyzed.\n  Methods: Water-fat separation of 1200 gradient-echo acquisitions from 90\nimaging sessions (normal, acute and chronic myocardial infarction) was\nperformed using a conventional model based method with modeling of R2* and\noff-resonance and a multi-peak fat spectrum. A U-Net convolutional neural\nnetwork for calculation of water-only, fat-only, R2* and off-resonance images\nwas trained with 900 gradient-echo Multiple and single-echo complex and\nmagnitude input data algorithms were studied and compared to conventional\nextended echo modeling.\n  Results: The U-Net ConvNet was easily trained and provided water-fat\nseparation results visually comparable to conventional methods. Myocardial fat\ndeposition in chronic myocardial infarction and intramyocardial hemorrhage in\nacute myocardial infarction were well visualized in the DL results. Predicted\nvalues for R2*, off-resonance, water and fat signal intensities were well\ncorrelated with conventional model based water fat separation (R2>=0.97,\np<0.001). DL images had a 14% higher signal-to-noise ratio (p<0.001) when\ncompared to the conventional method.\n  Conclusion: Deep learning utilizing ConvNets is a feasible method for MR\nwater-fat separationimaging with complex, magnitude and single echo image data.\nA trained U-Net can be efficiently used for MR water-fat separation, providing\nresults comparable to conventional model based methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 17:36:36 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Goldfarb", "James W", ""]]}, {"id": "1711.00111", "submitter": "Ludovic Trottier", "authors": "Ludovic Trottier, Philippe Gigu\\`ere, Brahim Chaib-draa", "title": "Multi-Task Learning by Deep Collaboration and Application in Facial\n  Landmark Detection", "comments": "Under review at the 15th European Conference on Computer Vision\n  (ECCV) (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have become the most successful approach\nin many vision-related domains. However, they are limited to domains where data\nis abundant. Recent works have looked at multi-task learning (MTL) to mitigate\ndata scarcity by leveraging domain-specific information from related tasks. In\nthis paper, we present a novel soft-parameter sharing mechanism for CNNs in a\nMTL setting, which we refer to as Deep Collaboration. We propose taking into\naccount the notion that task relevance depends on depth by using lateral\ntransformation blocs with skip connections. This allows extracting\ntask-specific features at various depth without sacrificing features relevant\nto all tasks. We show that CNNs connected with our Deep Collaboration obtain\nbetter accuracy on facial landmark detection with related tasks. We finally\nverify that our approach effectively allows knowledge sharing by showing\ndepth-specific influence of tasks that we know are related.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 03:51:16 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 16:48:22 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Trottier", "Ludovic", ""], ["Gigu\u00e8re", "Philippe", ""], ["Chaib-draa", "Brahim", ""]]}, {"id": "1711.00112", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl, Thiago Santini, Gjergji Kasneci, Wolfgang Rosenstiel,\n  Enkelejda Kasneci", "title": "PupilNet v2.0: Convolutional Neural Networks for CPU based real time\n  Robust Pupil Detection", "comments": "Pupil detection, pupil center estimation, image processing, CNN.\n  arXiv admin note: substantial text overlap with arXiv:1601.04902", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time, accurate, and robust pupil detection is an essential prerequisite\nfor pervasive video-based eye-tracking. However, automated pupil detection in\nrealworld scenarios has proven to be an intricate challenge due to fast\nillumination changes, pupil occlusion, non-centered and off-axis eye recording,\nas well as physiological eye characteristics. In this paper, we approach this\nchallenge through: I) a convolutional neural network (CNN) running in real time\non a single core, II) a novel computational intensive two stage CNN for\naccuracy improvement, and III) a fast propability distribution based refinement\nmethod as a practical alternative to II. We evaluate the proposed approaches\nagainst the state-of-the-art pupil detection algorithms, improving the\ndetection rate up to ~9% percent points on average over all data sets (~7% on\none CPU core 7ms). This evaluation was performed on over 135,000 images: 94,000\nimages from the literature, and 41,000 new hand-labeled and challenging images\ncontributed by this work (v1.0).\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 10:55:00 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Santini", "Thiago", ""], ["Kasneci", "Gjergji", ""], ["Rosenstiel", "Wolfgang", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "1711.00117", "submitter": "Chuan Guo", "authors": "Chuan Guo, Mayank Rana, Moustapha Cisse, Laurens van der Maaten", "title": "Countering Adversarial Images using Input Transformations", "comments": "12 pages, 6 figures, submitted to ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates strategies that defend against adversarial-example\nattacks on image-classification systems by transforming the inputs before\nfeeding them to the system. Specifically, we study applying image\ntransformations such as bit-depth reduction, JPEG compression, total variance\nminimization, and image quilting before feeding the image to a convolutional\nnetwork classifier. Our experiments on ImageNet show that total variance\nminimization and image quilting are very effective defenses in practice, in\nparticular, when the network is trained on transformed images. The strength of\nthose defenses lies in their non-differentiable nature and their inherent\nrandomness, which makes it difficult for an adversary to circumvent the\ndefenses. Our best defense eliminates 60% of strong gray-box and 90% of strong\nblack-box attacks by a variety of major attack methods\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 21:22:16 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 20:59:58 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 19:04:48 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Guo", "Chuan", ""], ["Rana", "Mayank", ""], ["Cisse", "Moustapha", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1711.00126", "submitter": "Abolfazl Hashemi", "authors": "Abolfazl Hashemi and Haris Vikalo", "title": "Accelerated Sparse Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art algorithms for sparse subspace clustering perform spectral\nclustering on a similarity matrix typically obtained by representing each data\npoint as a sparse combination of other points using either basis pursuit (BP)\nor orthogonal matching pursuit (OMP). BP-based methods are often prohibitive in\npractice while the performance of OMP-based schemes are unsatisfactory,\nespecially in settings where data points are highly similar. In this paper, we\npropose a novel algorithm that exploits an accelerated variant of orthogonal\nleast-squares to efficiently find the underlying subspaces. We show that under\ncertain conditions the proposed algorithm returns a subspace-preserving\nsolution. Simulation results illustrate that the proposed method compares\nfavorably with BP-based method in terms of running time while being\nsignificantly more accurate than OMP-based schemes.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 22:05:47 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Hashemi", "Abolfazl", ""], ["Vikalo", "Haris", ""]]}, {"id": "1711.00139", "submitter": "Min Tang", "authors": "Min Tang, Zichen Zhang, Dana Cobzas, Martin Jagersand, Jacob L.\n  Jaremko", "title": "Segmentation-by-Detection: A Cascade Network for Volumetric Medical\n  Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an attention mechanism for 3D medical image segmentation. The\nmethod, named segmentation-by-detection, is a cascade of a detection module\nfollowed by a segmentation module. The detection module enables a region of\ninterest to come to attention and produces a set of object region candidates\nwhich are further used as an attention model. Rather than dealing with the\nentire volume, the segmentation module distills the information from the\npotential region. This scheme is an efficient solution for volumetric data as\nit reduces the influence of the surrounding noise which is especially important\nfor medical data with low signal-to-noise ratio. Experimental results on 3D\nultrasound data of the femoral head shows superiority of the proposed method\nwhen compared with a standard fully convolutional network like the U-Net.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 23:04:28 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Tang", "Min", ""], ["Zhang", "Zichen", ""], ["Cobzas", "Dana", ""], ["Jagersand", "Martin", ""], ["Jaremko", "Jacob L.", ""]]}, {"id": "1711.00146", "submitter": "Miquel Mart\\'i I Rabad\\'an", "authors": "Miquel Mart\\'i and Atsuto Maki", "title": "A multitask deep learning model for real-time deployment in embedded\n  systems", "comments": "2 pages, 5 figures. Poster presentation at Swedish Symposium on Deep\n  Learning SSDL2017, Stockholm, Sweden. June 20-21, 2017", "journal-ref": "Swedish Symposium on Deep Learning 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to Multitask Learning (MTL) to make deep learning\nmodels faster and lighter for applications in which multiple tasks need to be\nsolved simultaneously, which is particularly useful in embedded, real-time\nsystems. We develop a multitask model for both Object Detection and Semantic\nSegmentation and analyze the challenges that appear during its training. Our\nmultitask network is 1.6x faster, lighter and uses less memory than deploying\nthe single-task models in parallel. We conclude that MTL has the potential to\ngive superior performance in exchange of a more complex training process that\nintroduces challenges not present in single-task models.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 23:59:23 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Mart\u00ed", "Miquel", ""], ["Maki", "Atsuto", ""]]}, {"id": "1711.00164", "submitter": "Lachlan Tychsen-Smith", "authors": "Lachlan Tychsen-Smith, Lars Petersson", "title": "Improving Object Localization with Fitness NMS and Bounded IoU Loss", "comments": "CVPR2018 Main Conference (Poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that many detection methods are designed to identify only a\nsufficently accurate bounding box, rather than the best available one. To\naddress this issue we propose a simple and fast modification to the existing\nmethods called Fitness NMS. This method is tested with the DeNet model and\nobtains a significantly improved MAP at greater localization accuracies without\na loss in evaluation rate, and can be used in conjunction with Soft NMS for\nadditional improvements. Next we derive a novel bounding box regression loss\nbased on a set of IoU upper bounds that better matches the goal of IoU\nmaximization while still providing good convergence properties. Following these\nnovelties we investigate RoI clustering schemes for improving evaluation rates\nfor the DeNet wide model variants and provide an analysis of localization\nperformance at various input image dimensions. We obtain a MAP of 33.6%@79Hz\nand 41.8%@5Hz for MSCOCO and a Titan X (Maxwell). Source code available from:\nhttps://github.com/lachlants/denet\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 02:02:23 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 02:01:13 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 23:54:45 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Tychsen-Smith", "Lachlan", ""], ["Petersson", "Lars", ""]]}, {"id": "1711.00199", "submitter": "Yu Xiang", "authors": "Yu Xiang, Tanner Schmidt, Venkatraman Narayanan and Dieter Fox", "title": "PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation in\n  Cluttered Scenes", "comments": "Accepted to RSS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the 6D pose of known objects is important for robots to interact\nwith the real world. The problem is challenging due to the variety of objects\nas well as the complexity of a scene caused by clutter and occlusions between\nobjects. In this work, we introduce PoseCNN, a new Convolutional Neural Network\nfor 6D object pose estimation. PoseCNN estimates the 3D translation of an\nobject by localizing its center in the image and predicting its distance from\nthe camera. The 3D rotation of the object is estimated by regressing to a\nquaternion representation. We also introduce a novel loss function that enables\nPoseCNN to handle symmetric objects. In addition, we contribute a large scale\nvideo dataset for 6D object pose estimation named the YCB-Video dataset. Our\ndataset provides accurate 6D poses of 21 objects from the YCB dataset observed\nin 92 videos with 133,827 frames. We conduct extensive experiments on our\nYCB-Video dataset and the OccludedLINEMOD dataset to show that PoseCNN is\nhighly robust to occlusions, can handle symmetric objects, and provide accurate\npose estimation using only color images as input. When using depth data to\nfurther refine the poses, our approach achieves state-of-the-art results on the\nchallenging OccludedLINEMOD dataset. Our code and dataset are available at\nhttps://rse-lab.cs.washington.edu/projects/posecnn/.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 04:10:58 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 02:50:26 GMT"}, {"version": "v3", "created": "Sat, 26 May 2018 07:34:09 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Xiang", "Yu", ""], ["Schmidt", "Tanner", ""], ["Narayanan", "Venkatraman", ""], ["Fox", "Dieter", ""]]}, {"id": "1711.00205", "submitter": "Chunhua Shen", "authors": "Bohan Zhuang, Chunhua Shen, Mingkui Tan, Lingqiao Liu, Ian Reid", "title": "Towards Effective Low-bitwidth Convolutional Neural Networks", "comments": "11 pages. Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of training a deep convolutional neural\nnetwork with both low-precision weights and low-bitwidth activations.\nOptimizing a low-precision network is very challenging since the training\nprocess can easily get trapped in a poor local minima, which results in\nsubstantial accuracy loss. To mitigate this problem, we propose three\nsimple-yet-effective approaches to improve the network training. First, we\npropose to use a two-stage optimization strategy to progressively find good\nlocal minima. Specifically, we propose to first optimize a net with quantized\nweights and then quantized activations. This is in contrast to the traditional\nmethods which optimize them simultaneously. Second, following a similar spirit\nof the first method, we propose another progressive optimization approach which\nprogressively decreases the bit-width from high-precision to low-precision\nduring the course of training. Third, we adopt a novel learning scheme to\njointly train a full-precision model alongside the low-precision one. By doing\nso, the full-precision model provides hints to guide the low-precision model\ntraining. Extensive experiments on various datasets ( i.e., CIFAR-100 and\nImageNet) show the effectiveness of the proposed methods. To highlight, using\nour methods to train a 4-bit precision network leads to no performance decrease\nin comparison with its full-precision counterpart with standard network\narchitectures ( i.e., AlexNet and ResNet-50).\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 04:41:29 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 01:35:27 GMT"}], "update_date": "2021-06-05", "authors_parsed": [["Zhuang", "Bohan", ""], ["Shen", "Chunhua", ""], ["Tan", "Mingkui", ""], ["Liu", "Lingqiao", ""], ["Reid", "Ian", ""]]}, {"id": "1711.00207", "submitter": "Doguk Kim", "authors": "Do-Guk Kim, Jong-Uk Hou and Heung-Kyu Lee", "title": "Learning deep features for source color laser printer identification\n  based on cascaded learning", "comments": "11 pages, 16 figures, submitted to Signal Processing: Image\n  Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color laser printers have fast printing speed and high resolution, and\nforgeries using color laser printers can cause significant harm to society. A\nsource printer identification technique can be employed as a countermeasure to\nthose forgeries. This paper presents a color laser printer identification\nmethod based on cascaded learning of deep neural networks. The refiner network\nis trained by adversarial training to refine the synthetic dataset for halftone\ncolor decomposition. The halftone color decomposing ConvNet is trained with the\nrefined dataset, and the trained knowledge is transferred to the printer\nidentifying ConvNet to enhance the accuracy. The robustness about rotation and\nscaling is considered in training process, which is not considered in existing\nmethods. Experiments are performed on eight color laser printers, and the\nperformance is compared with several existing methods. The experimental results\nclearly show that the proposed method outperforms existing source color laser\nprinter identification methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 04:48:05 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Kim", "Do-Guk", ""], ["Hou", "Jong-Uk", ""], ["Lee", "Heung-Kyu", ""]]}, {"id": "1711.00238", "submitter": "Qianhui Luo", "authors": "Qianhui Luo, Huifang Ma, Yue Wang, Li Tang and Rong Xiong", "title": "3D-SSD: Learning Hierarchical Features from RGB-D Images for Amodal 3D\n  Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at developing a faster and a more accurate solution to the\namodal 3D object detection problem for indoor scenes. It is achieved through a\nnovel neural network that takes a pair of RGB-D images as the input and\ndelivers oriented 3D bounding boxes as the output. The network, named 3D-SSD,\ncomposed of two parts: hierarchical feature fusion and multi-layer prediction.\nThe hierarchical feature fusion combines appearance and geometric features from\nRGB-D images while the multi-layer prediction utilizes multi-scale features for\nobject detection. As a result, the network can exploit 2.5D representations in\na synergetic way to improve the accuracy and efficiency. The issue of object\nsizes is addressed by attaching a set of 3D anchor boxes with varying sizes to\nevery location of the prediction layers. At the end stage, the category scores\nfor 3D anchor boxes are generated with adjusted positions, sizes and\norientations respectively, leading to the final detections using non-maximum\nsuppression. In the training phase, the positive samples are identified with\nthe aid of 2D ground truth to avoid the noisy estimation of depth from raw\ndata, which guide to a better converged model. Experiments performed on the\nchallenging SUN RGB-D dataset show that our algorithm outperforms the\nstate-of-the-art Deep Sliding Shape by 10.2% mAP and 88x faster. Further,\nexperiments also suggest our approach achieves comparable accuracy and is 386x\nfaster than the state-of-art method on the NYUv2 dataset even with a smaller\ninput image size.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 07:57:25 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 09:06:33 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Luo", "Qianhui", ""], ["Ma", "Huifang", ""], ["Wang", "Yue", ""], ["Tang", "Li", ""], ["Xiong", "Rong", ""]]}, {"id": "1711.00248", "submitter": "Ya Zhang", "authors": "Zhuoxiang Chen, Zhe Xu, Ya Zhang, Xiao Gu", "title": "Query-free Clothing Retrieval via Implicit Relevance Feedback", "comments": "12 pages, under review at IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based clothing retrieval is receiving increasing interest with the\ngrowth of online shopping. In practice, users may often have a desired piece of\nclothing in mind (e.g., either having seen it before on the street or requiring\ncertain specific clothing attributes) but may be unable to supply an image as a\nquery. We model this problem as a new type of image retrieval task in which the\ntarget image resides only in the user's mind (called \"mental image retrieval\"\nhereafter). Because of the absence of an explicit query image, we propose to\nsolve this problem through relevance feedback. Specifically, a new Bayesian\nformulation is proposed that simultaneously models the retrieval target and its\nhigh-level representation in the mind of the user (called the \"user metric\"\nhereafter) as posterior distributions of pre-fetched shop images and\nheterogeneous features extracted from multiple clothing attributes,\nrespectively. Requiring only clicks as user feedback, the proposed algorithm is\nable to account for the variability in human decision-making. Experiments with\nreal users demonstrate the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 08:33:39 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Chen", "Zhuoxiang", ""], ["Xu", "Zhe", ""], ["Zhang", "Ya", ""], ["Gu", "Xiao", ""]]}, {"id": "1711.00253", "submitter": "Chunhua Shen", "authors": "Yu Chen, Chunhua Shen, Hao Chen, Xiu-Shen Wei, Lingqiao Liu, Jian Yang", "title": "Adversarial Learning of Structure-Aware Fully Convolutional Networks for\n  Landmark Localization", "comments": "18 pages. Extended version of arXiv:1705.00389. Accepted to IEEE\n  Trans. Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landmark/pose estimation in single monocular images have received much effort\nin computer vision due to its important applications. It remains a challenging\ntask when input images severe occlusions caused by, e.g., adverse camera views.\nUnder such circumstances, biologically implausible pose predictions may be\nproduced. In contrast, human vision is able to predict poses by exploiting\ngeometric constraints of landmark point inter-connectivity. To address the\nproblem, by incorporating priors about the structure of pose components, we\npropose a novel structure-aware fully convolutional network to implicitly take\nsuch priors into account during training of the deep network. Explicit learning\nof such constraints is typically challenging. Instead, inspired by how human\nidentifies implausible poses, we design discriminators to distinguish the real\nposes from the fake ones (such as biologically implausible ones). If the pose\ngenerator G generates results that the discriminator fails to distinguish from\nreal ones, the network successfully learns the priors. Training of the network\nfollows the strategy of conditional Generative Adversarial Networks (GANs). The\neffectiveness of the proposed network is evaluated on three pose-related tasks:\n2D single human pose estimation, 2D facial landmark estimation and 3D single\nhuman pose estimation. The proposed approach significantly outperforms the\nstate-of-the-art methods and almost always generates plausible pose\npredictions, demonstrating the usefulness of implicit learning of structures\nusing GANs.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 08:54:50 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 08:20:17 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2018 06:14:42 GMT"}, {"version": "v4", "created": "Tue, 12 Feb 2019 01:00:33 GMT"}, {"version": "v5", "created": "Sun, 24 Feb 2019 01:24:02 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Chen", "Yu", ""], ["Shen", "Chunhua", ""], ["Chen", "Hao", ""], ["Wei", "Xiu-Shen", ""], ["Liu", "Lingqiao", ""], ["Yang", "Jian", ""]]}, {"id": "1711.00267", "submitter": "Wenbin Li", "authors": "Wenbin Li, Jeannette Bohg, Mario Fritz", "title": "Acquiring Target Stacking Skills by Goal-Parameterized Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding physical phenomena is a key component of human intelligence and\nenables physical interaction with previously unseen environments. In this\npaper, we study how an artificial agent can autonomously acquire this intuition\nthrough interaction with the environment. We created a synthetic block stacking\nenvironment with physics simulation in which the agent can learn a policy\nend-to-end through trial and error. Thereby, we bypass to explicitly model\nphysical knowledge within the policy. We are specifically interested in tasks\nthat require the agent to reach a given goal state that may be different for\nevery new trial. To this end, we propose a deep reinforcement learning\nframework that learns policies which are parametrized by a goal. We validated\nthe model on a toy example navigating in a grid world with different target\npositions and in a block stacking task with different target structures of the\nfinal tower. In contrast to prior work, our policies show better generalization\nacross different goals.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 10:04:29 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 11:38:17 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Li", "Wenbin", ""], ["Bohg", "Jeannette", ""], ["Fritz", "Mario", ""]]}, {"id": "1711.00305", "submitter": "Mickael Chen", "authors": "Micka\\\"el Chen, Ludovic Denoyer and Thierry Arti\\`eres", "title": "Multi-View Data Generation Without View Supervision", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of high-dimensional generative models has recently gained a\ngreat surge of interest with the introduction of variational auto-encoders and\ngenerative adversarial neural networks. Different variants have been proposed\nwhere the underlying latent space is structured, for example, based on\nattributes describing the data to generate. We focus on a particular problem\nwhere one aims at generating samples corresponding to a number of objects under\nvarious views. We assume that the distribution of the data is driven by two\nindependent latent factors: the content, which represents the intrinsic\nfeatures of an object, and the view, which stands for the settings of a\nparticular observation of that object. Therefore, we propose a generative model\nand a conditional variant built on such a disentangled latent space. This\napproach allows us to generate realistic samples corresponding to various\nobjects in a high variety of views. Unlike many multi-view approaches, our\nmodel doesn't need any supervision on the views but only on the content.\nCompared to other conditional generation approaches that are mostly based on\nbinary or categorical attributes, we make no such assumption about the factors\nof variations. Our model can be used on problems with a huge, potentially\ninfinite, number of categories. We experiment it on four image datasets on\nwhich we demonstrate the effectiveness of the model and its ability to\ngeneralize.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 12:18:26 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 11:27:10 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Chen", "Micka\u00ebl", ""], ["Denoyer", "Ludovic", ""], ["Arti\u00e8res", "Thierry", ""]]}, {"id": "1711.00322", "submitter": "Chunbiao Zhu", "authors": "Kan Huang, Chunbiao Zhu, Ge Li", "title": "Robust Saliency Detection via Fusing Foreground and Background Priors", "comments": "Project website: https://github.com/ChunbiaoZhu/FBP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic Salient object detection has received tremendous attention from\nresearch community and has been an increasingly important tool in many computer\nvision tasks. This paper proposes a novel bottom-up salient object detection\nframework which considers both foreground and background cues. First, A series\nof background and foreground seeds are selected from an image reliably, and\nthen used for calculation of saliency map separately. Next, a combination of\nforeground and background saliency map is performed. Last, a refinement step\nbased on geodesic distance is utilized to enhance salient regions, thus\nderiving the final saliency map. Particularly we provide a robust scheme for\nseeds selection which contributes a lot to accuracy improvement in saliency\ndetection. Extensive experimental evaluations demonstrate the effectiveness of\nour proposed method against other outstanding methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 12:56:42 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Huang", "Kan", ""], ["Zhu", "Chunbiao", ""], ["Li", "Ge", ""]]}, {"id": "1711.00349", "submitter": "Nikolas Lessmann", "authors": "Nikolas Lessmann, Bram van Ginneken, Majd Zreik, Pim A. de Jong, Bob\n  D. de Vos, Max A. Viergever, Ivana I\\v{s}gum", "title": "Automatic calcium scoring in low-dose chest CT using deep neural\n  networks with dilated convolutions", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging 37(2), pp 615-625, 2018", "doi": "10.1109/TMI.2017.2769839", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavy smokers undergoing screening with low-dose chest CT are affected by\ncardiovascular disease as much as by lung cancer. Low-dose chest CT scans\nacquired in screening enable quantification of atherosclerotic calcifications\nand thus enable identification of subjects at increased cardiovascular risk.\nThis paper presents a method for automatic detection of coronary artery,\nthoracic aorta and cardiac valve calcifications in low-dose chest CT using two\nconsecutive convolutional neural networks. The first network identifies and\nlabels potential calcifications according to their anatomical location and the\nsecond network identifies true calcifications among the detected candidates.\nThis method was trained and evaluated on a set of 1744 CT scans from the\nNational Lung Screening Trial. To determine whether any reconstruction or only\nimages reconstructed with soft tissue filters can be used for calcification\ndetection, we evaluated the method on soft and medium/sharp filter\nreconstructions separately. On soft filter reconstructions, the method achieved\nF1 scores of 0.89, 0.89, 0.67, and 0.55 for coronary artery, thoracic aorta,\naortic valve and mitral valve calcifications, respectively. On sharp filter\nreconstructions, the F1 scores were 0.84, 0.81, 0.64, and 0.66, respectively.\nLinearly weighted kappa coefficients for risk category assignment based on per\nsubject coronary artery calcium were 0.91 and 0.90 for soft and sharp filter\nreconstructions, respectively. These results demonstrate that the presented\nmethod enables reliable automatic cardiovascular risk assessment in all\nlow-dose chest CT scans acquired for lung cancer screening.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 13:54:55 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 21:56:31 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Lessmann", "Nikolas", ""], ["van Ginneken", "Bram", ""], ["Zreik", "Majd", ""], ["de Jong", "Pim A.", ""], ["de Vos", "Bob D.", ""], ["Viergever", "Max A.", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1711.00362", "submitter": "Mykola Ponomarenko", "authors": "Vladimir Katkovnik, Mykola Ponomarenko and Karen Egiazarian", "title": "Complex-valued image denosing based on group-wise complex-domain\n  sparsity", "comments": "Submitted to Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase imaging and wavefront reconstruction from noisy observations of complex\nexponent is a topic of this paper. It is a highly non-linear problem because\nthe exponent is a 2{\\pi}-periodic function of phase. The reconstruction of\nphase and amplitude is difficult. Even with an additive Gaussian noise in\nobservations distributions of noisy components in phase and amplitude are\nsignal dependent and non-Gaussian. Additional difficulties follow from a prior\nunknown correlation of phase and amplitude in real life scenarios. In this\npaper, we propose a new class of non-iterative and iterative complex domain\nfilters based on group-wise sparsity in complex domain. This sparsity is based\non the techniques implemented in Block-Matching 3D filtering (BM3D) and 3D/4D\nHigh-Order Singular Decomposition (HOSVD) exploited for spectrum design,\nanalysis and filtering. The introduced algorithms are a generalization of the\nideas used in the CD-BM3D algorithms presented in our previous publications.\nThe algorithms are implemented as a MATLAB Toolbox. The efficiency of the\nalgorithms is demonstrated by simulation tests.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 14:30:31 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Katkovnik", "Vladimir", ""], ["Ponomarenko", "Mykola", ""], ["Egiazarian", "Karen", ""]]}, {"id": "1711.00414", "submitter": "Xun Jia", "authors": "Chenyang Shen, Yesenia Gonzalez, Liyuan Chen, Steve B. Jiang, Xun Jia", "title": "Intelligent Parameter Tuning in Optimization-based Iterative CT\n  Reconstruction via Deep Reinforcement Learning", "comments": "8 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of image-processing problems can be formulated as optimization\nproblems. The objective function typically contains several terms specifically\ndesigned for different purposes. Parameters in front of these terms are used to\ncontrol the relative weights among them. It is of critical importance to tune\nthese parameters, as quality of the solution depends on their values. Tuning\nparameter is a relatively straightforward task for a human, as one can\nintelligently determine the direction of parameter adjustment based on the\nsolution quality. Yet manual parameter tuning is not only tedious in many\ncases, but becomes impractical when a number of parameters exist in a problem.\nAiming at solving this problem, this paper proposes an approach that employs\ndeep reinforcement learning to train a system that can automatically adjust\nparameters in a human-like manner. We demonstrate our idea in an example\nproblem of optimization-based iterative CT reconstruction with a pixel-wise\ntotal-variation regularization term. We set up a parameter tuning policy\nnetwork (PTPN), which maps an CT image patch to an output that specifies the\ndirection and amplitude by which the parameter at the patch center is adjusted.\nWe train the PTPN via an end-to-end reinforcement learning procedure. We\ndemonstrate that under the guidance of the trained PTPN for parameter tuning at\neach pixel, reconstructed CT images attain quality similar or better than in\nthose reconstructed with manually tuned parameters.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 16:04:24 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Shen", "Chenyang", ""], ["Gonzalez", "Yesenia", ""], ["Chen", "Liyuan", ""], ["Jiang", "Steve B.", ""], ["Jia", "Xun", ""]]}, {"id": "1711.00436", "submitter": "Hanxiao Liu", "authors": "Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, Koray\n  Kavukcuoglu", "title": "Hierarchical Representations for Efficient Architecture Search", "comments": "Accepted as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore efficient neural architecture search methods and show that a\nsimple yet powerful evolutionary algorithm can discover new architectures with\nexcellent performance. Our approach combines a novel hierarchical genetic\nrepresentation scheme that imitates the modularized design pattern commonly\nadopted by human experts, and an expressive search space that supports complex\ntopologies. Our algorithm efficiently discovers architectures that outperform a\nlarge number of manually designed models for image classification, obtaining\ntop-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which\nis competitive with the best existing neural architecture search approaches. We\nalso present results using random search, achieving 0.3% less top-1 accuracy on\nCIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36\nhours down to 1 hour.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 16:46:27 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 22:31:30 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Liu", "Hanxiao", ""], ["Simonyan", "Karen", ""], ["Vinyals", "Oriol", ""], ["Fernando", "Chrisantha", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1711.00441", "submitter": "Sandra Avila", "authors": "Eduardo Valle, Michel Fornaciali, Afonso Menegola, Julia Tavares,\n  Fl\\'avia Vasques Bittencourt, Lin Tzy Li, Sandra Avila", "title": "Data, Depth, and Design: Learning Reliable Models for Skin Lesion\n  Analysis", "comments": "12 pages, 6 figures, 3 tables. Article accepted at Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning fostered a leap ahead in automated skin lesion analysis in the\nlast two years. Those models are expensive to train and difficult to\nparameterize. Objective: We investigate methodological issues for designing and\nevaluating deep learning models for skin lesion analysis. We explore 10 choices\nfaced by researchers: use of transfer learning, model architecture, train\ndataset, image resolution, type of data augmentation, input normalization, use\nof segmentation, duration of training, additional use of SVMs, and test data\naugmentation. Methods: We perform two full factorial experiments, for five\ndifferent test datasets, resulting in 2560 exhaustive trials in our main\nexperiment, and 1280 trials in our assessment of transfer learning. We analyze\nboth with multi-way ANOVA. We use the exhaustive trials to simulate sequential\ndecisions and ensembles, with and without the use of privileged information\nfrom the test set. Results -- main experiment: Amount of train data has\ndisproportionate influence, explaining almost half the variation in\nperformance. Of the other factors, test data augmentation and input resolution\nare the most influential. Deeper models, when combined, with extra data, also\nhelp. -- transfer experiment: Transfer learning is critical, its absence brings\nhuge performance penalties. -- simulations: Ensembles of models are the best\noption to provide reliable results with limited resources, without using\nprivileged information and sacrificing methodological rigor. Conclusions and\nSignificance: Advancing research on automated skin lesion analysis requires\ncurating larger public datasets. Indirect use of privileged information from\nthe test set to design the models is a subtle, but frequent methodological\nmistake that leads to overoptimistic results. Ensembles of models are a\ncost-effective alternative to the expensive full-factorial and to the unstable\nsequential designs.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 17:02:35 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 16:49:34 GMT"}, {"version": "v3", "created": "Fri, 23 Mar 2018 01:59:59 GMT"}, {"version": "v4", "created": "Sun, 18 Aug 2019 02:00:32 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Valle", "Eduardo", ""], ["Fornaciali", "Michel", ""], ["Menegola", "Afonso", ""], ["Tavares", "Julia", ""], ["Bittencourt", "Fl\u00e1via Vasques", ""], ["Li", "Lin Tzy", ""], ["Avila", "Sandra", ""]]}, {"id": "1711.00457", "submitter": "Alex Fedorov", "authors": "Alex Fedorov, Eswar Damaraju, Vince Calhoun and Sergey Plis", "title": "Almost instant brain atlas segmentation for large-scale studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale studies of group differences in healthy controls and patients and\nscreenings for early stage disease prevention programs require processing and\nanalysis of extensive multisubject datasets. Complexity of the task increases\neven further when segmenting structural MRI of the brain into an atlas with\nmore than 50 regions. Current automatic approaches are time-consuming and\nhardly scalable; they often involve many error prone intermediate steps and\ndon't utilize other available modalities. To alleviate these problems, we\npropose a feedforward fully convolutional neural network trained on the output\nproduced by the state of the art models. Incredible speed due to available\npowerful GPUs neural network makes this analysis much easier and faster (from\n$>10$ hours to a minute). The proposed model is more than two orders of\nmagnitudes faster than the state of the art and yet as accurate. We have\nevaluated the network's performance by comparing it with the state of the art\nin the task of differentiating region volumes of healthy controls and patients\nwith schizophrenia on a dataset with 311 subjects. This comparison provides a\nstrong evidence that speed did not harm the accuracy. The overall quality may\nalso be increased by utilizing multi-modal datasets (not an easy task for other\nmodels) by simple adding more modalities as an input. Our model will be useful\nin large-scale studies as well as in clinical care solutions, where it can\nsignificantly reduce delay between the patient screening and the result.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 17:44:11 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Fedorov", "Alex", ""], ["Damaraju", "Eswar", ""], ["Calhoun", "Vince", ""], ["Plis", "Sergey", ""]]}, {"id": "1711.00489", "submitter": "Samuel L. Smith", "authors": "Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying and Quoc V. Le", "title": "Don't Decay the Learning Rate, Increase the Batch Size", "comments": "11 pages, 8 figures. Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common practice to decay the learning rate. Here we show one can\nusually obtain the same learning curve on both training and test sets by\ninstead increasing the batch size during training. This procedure is successful\nfor stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum,\nand Adam. It reaches equivalent test accuracies after the same number of\ntraining epochs, but with fewer parameter updates, leading to greater\nparallelism and shorter training times. We can further reduce the number of\nparameter updates by increasing the learning rate $\\epsilon$ and scaling the\nbatch size $B \\propto \\epsilon$. Finally, one can increase the momentum\ncoefficient $m$ and scale $B \\propto 1/(1-m)$, although this tends to slightly\nreduce the test accuracy. Crucially, our techniques allow us to repurpose\nexisting training schedules for large batch training with no hyper-parameter\ntuning. We train ResNet-50 on ImageNet to $76.1\\%$ validation accuracy in under\n30 minutes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 18:04:31 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 00:16:12 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Smith", "Samuel L.", ""], ["Kindermans", "Pieter-Jan", ""], ["Ying", "Chris", ""], ["Le", "Quoc V.", ""]]}, {"id": "1711.00499", "submitter": "Patrick Brandao", "authors": "Patrick Brandao and Evangelos Mazomenos and Danail Stoyanov", "title": "Widening siamese architectures for stereo matching", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational stereo is one of the classical problems in computer vision.\nNumerous algorithms and solutions have been reported in recent years focusing\non developing methods for computing similarity, aggregating it to obtain\nspatial support and finally optimizing an energy function to find the final\ndisparity. In this paper, we focus on the feature extraction component of\nstereo matching architecture and we show standard CNNs operation can be used to\nimprove the quality of the features used to find point correspondences.\nFurthermore, we propose a simple space aggregation that hugely simplifies the\ncorrelation learning problem. Our results on benchmark data are compelling and\nshow promising potential even without refining the solution.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 18:24:31 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Brandao", "Patrick", ""], ["Mazomenos", "Evangelos", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1711.00536", "submitter": "Rossano Schifanella", "authors": "Luca M. Aiello, Rossano Schifanella, Miriam Redi, Stacey Svetlichnaya,\n  Frank Liu, Simon Osindero", "title": "Beautiful and damned. Combined effect of content quality and social ties\n  on user engagement", "comments": "13 pages, 12 figures, final version published in IEEE Transactions on\n  Knowledge and Data Engineering (Volume: PP, Issue: 99)", "journal-ref": null, "doi": "10.1109/TKDE.2017.2747552", "report-no": null, "categories": "cs.SI cs.AI cs.CV cs.MM physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User participation in online communities is driven by the intertwinement of\nthe social network structure with the crowd-generated content that flows along\nits links. These aspects are rarely explored jointly and at scale. By looking\nat how users generate and access pictures of varying beauty on Flickr, we\ninvestigate how the production of quality impacts the dynamics of online social\nsystems. We develop a deep learning computer vision model to score images\naccording to their aesthetic value and we validate its output through\ncrowdsourcing. By applying it to over 15B Flickr photos, we study for the first\ntime how image beauty is distributed over a large-scale social system.\nBeautiful images are evenly distributed in the network, although only a small\ncore of people get social recognition for them. To study the impact of exposure\nto quality on user engagement, we set up matching experiments aimed at\ndetecting causality from observational data. Exposure to beauty is\ndouble-edged: following people who produce high-quality content increases one's\nprobability of uploading better photos; however, an excessive imbalance between\nthe quality generated by a user and the user's neighbors leads to a decline in\nengagement. Our analysis has practical implications for improving link\nrecommender systems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 20:48:30 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Aiello", "Luca M.", ""], ["Schifanella", "Rossano", ""], ["Redi", "Miriam", ""], ["Svetlichnaya", "Stacey", ""], ["Liu", "Frank", ""], ["Osindero", "Simon", ""]]}, {"id": "1711.00558", "submitter": "Shubham Jain", "authors": "Shubham Jain and Marco Gruteser", "title": "Recognizing Textures with Mobile Cameras for Pedestrian Safety\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As smartphone rooted distractions become commonplace, the lack of compelling\nsafety measures has led to a rise in the number of injuries to distracted\nwalkers. Various solutions address this problem by sensing a pedestrian's\nwalking environment. Existing camera-based approaches have been largely limited\nto obstacle detection and other forms of object detection. Instead, we present\nTerraFirma, an approach that performs material recognition on the pedestrian's\nwalking surface. We explore, first, how well commercial off-the-shelf\nsmartphone cameras can learn texture to distinguish among paving materials in\nuncontrolled outdoor urban settings. Second, we aim at identifying when a\ndistracted user is about to enter the street, which can be used to support\nsafety functions such as warning the user to be cautious. To this end, we\ngather a unique dataset of street/sidewalk imagery from a pedestrian's\nperspective, that spans major cities like New York, Paris, and London. We\ndemonstrate that modern phone cameras can be enabled to distinguish materials\nof walking surfaces in urban areas with more than 90% accuracy, and accurately\nidentify when pedestrians transition from sidewalk to street.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 23:09:22 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Jain", "Shubham", ""], ["Gruteser", "Marco", ""]]}, {"id": "1711.00575", "submitter": "Garrett Bingham", "authors": "Garrett Bingham", "title": "Random Subspace Two-dimensional LDA for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel technique named random subspace two-dimensional LDA\n(RS-2DLDA) is developed for face recognition. This approach offers a number of\nimprovements over the random subspace two-dimensional PCA (RS2DPCA) framework\nintroduced by Nguyen et al. [5]. Firstly, the eigenvectors from 2DLDA have more\ndiscriminative power than those from 2DPCA, resulting in higher accuracy for\nthe RS-2DLDA method over RS-2DPCA. Various distance metrics are evaluated, and\na weighting scheme is developed to further boost accuracy. A series of\nexperiments on the MORPH-II and ORL datasets are conducted to demonstrate the\neffectiveness of this approach.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 00:27:01 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Bingham", "Garrett", ""]]}, {"id": "1711.00583", "submitter": "Jiangchao Yao", "authors": "Jiangchao Yao, Jiajie Wang, Ivor Tsang, Ya Zhang, Jun Sun, Chengqi\n  Zhang, Rui Zhang", "title": "Deep Learning from Noisy Image Labels with Quality Embedding", "comments": "Under review for Transactions on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an emerging trend to leverage noisy image datasets in many visual\nrecognition tasks. However, the label noise among the datasets severely\ndegenerates the \\mbox{performance of deep} learning approaches. Recently, one\nmainstream is to introduce the latent label to handle label noise, which has\nshown promising improvement in the network designs. Nevertheless, the mismatch\nbetween latent labels and noisy labels still affects the predictions in such\nmethods. To address this issue, we propose a quality embedding model, which\nexplicitly introduces a quality variable to represent the trustworthiness of\nnoisy labels. Our key idea is to identify the mismatch between the latent and\nnoisy labels by embedding the quality variables into different subspaces, which\neffectively minimizes the noise effect. At the same time, the high-quality\nlabels is still able to be applied for training. To instantiate the model, we\nfurther propose a Contrastive-Additive Noise network (CAN), which consists of\ntwo important layers: (1) the contrastive layer estimates the quality variable\nin the embedding space to reduce noise effect; and (2) the additive layer\naggregates the prior predictions and noisy labels as the posterior to train the\nclassifier. Moreover, to tackle the optimization difficulty, we deduce an SGD\nalgorithm with the reparameterization tricks, which makes our method scalable\nto big data. We conduct the experimental evaluation of the proposed method over\na range of noisy image datasets. Comprehensive results have demonstrated CAN\noutperforms the state-of-the-art deep learning approaches.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 01:19:25 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Yao", "Jiangchao", ""], ["Wang", "Jiajie", ""], ["Tsang", "Ivor", ""], ["Zhang", "Ya", ""], ["Sun", "Jun", ""], ["Zhang", "Chengqi", ""], ["Zhang", "Rui", ""]]}, {"id": "1711.00591", "submitter": "Zhenqiang Ying", "authors": "Zhenqiang Ying and Ge Li and Wen Gao", "title": "A Bio-Inspired Multi-Exposure Fusion Framework for Low-light Image\n  Enhancement", "comments": "Project website: https://baidut.github.io/BIMEF/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Low-light images are not conducive to human observation and computer vision\nalgorithms due to their low visibility. Although many image enhancement\ntechniques have been proposed to solve this problem, existing methods\ninevitably introduce contrast under- and over-enhancement. Inspired by human\nvisual system, we design a multi-exposure fusion framework for low-light image\nenhancement. Based on the framework, we propose a dual-exposure fusion\nalgorithm to provide an accurate contrast and lightness enhancement.\nSpecifically, we first design the weight matrix for image fusion using\nillumination estimation techniques. Then we introduce our camera response model\nto synthesize multi-exposure images. Next, we find the best exposure ratio so\nthat the synthetic image is well-exposed in the regions where the original\nimage is under-exposed. Finally, the enhanced result is obtained by fusing the\ninput image and the synthetic image according to the weight matrix. Experiments\nshow that our method can obtain results with less contrast and lightness\ndistortion compared to that of several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 02:18:03 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Ying", "Zhenqiang", ""], ["Li", "Ge", ""], ["Gao", "Wen", ""]]}, {"id": "1711.00648", "submitter": "Xinyue Zhu", "authors": "Xinyue Zhu, Yifan Liu, Zengchang Qin and Jiahong Li", "title": "Data Augmentation in Emotion Classification Using Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a difficult task to classify images with multiple class labels using\nonly a small number of labeled examples, especially when the label (class)\ndistribution is imbalanced. Emotion classification is such an example of\nimbalanced label distribution, because some classes of emotions like\n\\emph{disgusted} are relatively rare comparing to other labels like {\\it happy\nor sad}. In this paper, we propose a data augmentation method using generative\nadversarial networks (GAN). It can complement and complete the data manifold\nand find better margins between neighboring classes. Specifically, we design a\nframework with a CNN model as the classifier and a cycle-consistent adversarial\nnetworks (CycleGAN) as the generator. In order to avoid gradient vanishing\nproblem, we employ the least-squared loss as adversarial loss. We also propose\nseveral evaluation methods on three benchmark datasets to validate GAN's\nperformance. Empirical results show that we can obtain 5%~10% increase in the\nclassification accuracy after employing the GAN-based data augmentation\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 08:35:07 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 08:26:13 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 02:15:23 GMT"}, {"version": "v4", "created": "Tue, 21 Nov 2017 10:38:00 GMT"}, {"version": "v5", "created": "Thu, 14 Dec 2017 06:27:58 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Zhu", "Xinyue", ""], ["Liu", "Yifan", ""], ["Qin", "Zengchang", ""], ["Li", "Jiahong", ""]]}, {"id": "1711.00671", "submitter": "Karteek Popuri", "authors": "Karteek Popuri, Rakesh Balachandar, Kathryn Alpert, Donghuan Lu,\n  Mahadev Bhalla, Ian Mackenzie, Robin Ging-Yuek Hsiung, Lei Wang, Mirza Faisal\n  Beg, and the Alzhemier's Disease Neuroimaging Initiative", "title": "Development and validation of a novel dementia of Alzheimer's type (DAT)\n  score based on metabolism FDG-PET imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluorodeoxyglucose positron emission tomography (FDG-PET) imaging based 3D\ntopographic brain glucose metabolism patterns from normal controls (NC) and\nindividuals with dementia of Alzheimer's type (DAT) are used to train a novel\nmulti-scale ensemble classification model. This ensemble model outputs a\nFDG-PET DAT score (FPDS) between 0 and 1 denoting the probability of a subject\nto be clinically diagnosed with DAT based on their metabolism profile. A novel\n7 group image stratification scheme is devised that groups images not only\nbased on their associated clinical diagnosis but also on past and future\ntrajectories of the clinical diagnoses, yielding a more continuous\nrepresentation of the different stages of DAT spectrum that mimics a real-world\nclinical setting. The potential for using FPDS as a DAT biomarker was validated\non a large number of FDG-PET images (N=2984) obtained from the Alzheimer's\nDisease Neuroimaging Initiative (ADNI) database taken across the proposed\nstratification, and a good classification AUC (area under the curve) of 0.78\nwas achieved in distinguishing between images belonging to subjects on a DAT\ntrajectory and those images taken from subjects not progressing to a DAT\ndiagnosis. Further, the FPDS biomarker achieved state-of-the-art performance on\nthe mild cognitive impairment (MCI) to DAT conversion prediction task with an\nAUC of 0.81, 0.80, 0.77 for the 2, 3, 5 years to conversion windows\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 10:06:53 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Popuri", "Karteek", ""], ["Balachandar", "Rakesh", ""], ["Alpert", "Kathryn", ""], ["Lu", "Donghuan", ""], ["Bhalla", "Mahadev", ""], ["Mackenzie", "Ian", ""], ["Hsiung", "Robin Ging-Yuek", ""], ["Wang", "Lei", ""], ["Beg", "Mirza Faisal", ""], ["Initiative", "the Alzhemier's Disease Neuroimaging", ""]]}, {"id": "1711.00677", "submitter": "Bin Dai", "authors": "Bin Dai, Baoyuan Wang, Gang Hua", "title": "Understanding and Predicting The Attractiveness of Human Action Shot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting attractive photos from a human action shot sequence is quite\nchallenging, because of the subjective nature of the \"attractiveness\", which is\nmainly a combined factor of human pose in action and the background. Prior\nworks have actively studied high-level image attributes including\ninterestingness, memorability, popularity, and aesthetics. However, none of\nthem has ever studied the \"attractiveness\" of human action shot. In this paper,\nwe present the first study of the \"attractiveness\" of human action shots by\ntaking a systematic data-driven approach. Specifically, we create a new\naction-shot dataset composed of about 8000 high quality action-shot photos. We\nfurther conduct rich crowd-sourced human judge studies on Amazon Mechanical\nTurk(AMT) in terms of global attractiveness of a single photo, and relative\nattractiveness of a pair of photos. A deep Siamese network with a novel hybrid\ndistribution matching loss was further proposed to fully exploit both types of\nratings. Extensive experiments reveal that (1) the property of action shot\nattractiveness is subjective but predicable (2) our proposed method is both\nefficient and effective for predicting the attractive human action shots.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 10:31:08 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Dai", "Bin", ""], ["Wang", "Baoyuan", ""], ["Hua", "Gang", ""]]}, {"id": "1711.00693", "submitter": "Mykola Ponomarenko", "authors": "Karen Egiazarian, Mykola Ponomarenko, Vladimir Lukin, Oleg Ieremeiem", "title": "Statistical evaluation of visual quality metrics for image denoising", "comments": "Submitted to ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of full reference visual quality assessment of\ndenoised images with a special emphasis on images with low contrast and\nnoise-like texture. Denoising of such images together with noise removal often\nresults in image details loss or smoothing. A new test image database, FLT,\ncontaining 75 noise-free \"reference\" images and 300 filtered (\"distorted\")\nimages is developed. Each reference image, corrupted by an additive white\nGaussian noise, is denoised by the BM3D filter with four different values of\nthreshold parameter (four levels of noise suppression). After carrying out a\nperceptual quality assessment of distorted images, the mean opinion scores\n(MOS) are obtained and compared with the values of known full reference quality\nmetrics. As a result, the Spearman Rank Order Correlation Coefficient (SROCC)\nbetween PSNR values and MOS has a value close to zero, and SROCC between values\nof known full-reference image visual quality metrics and MOS does not exceed\n0.82 (which is reached by a new visual quality metric proposed in this paper).\nThe FLT dataset is more complex than earlier datasets used for assessment of\nvisual quality for image denoising. Thus, it can be effectively used to design\nnew image visual quality metrics for image denoising.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 11:36:51 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Egiazarian", "Karen", ""], ["Ponomarenko", "Mykola", ""], ["Lukin", "Vladimir", ""], ["Ieremeiem", "Oleg", ""]]}, {"id": "1711.00848", "submitter": "Abhishek Kumar", "authors": "Abhishek Kumar, Prasanna Sattigeri, Avinash Balakrishnan", "title": "Variational Inference of Disentangled Latent Concepts from Unlabeled\n  Observations", "comments": "ICLR 2018 Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentangled representations, where the higher level data generative factors\nare reflected in disjoint latent dimensions, offer several benefits such as\nease of deriving invariant representations, transferability to other tasks,\ninterpretability, etc. We consider the problem of unsupervised learning of\ndisentangled representations from large pool of unlabeled observations, and\npropose a variational inference based approach to infer disentangled latent\nfactors. We introduce a regularizer on the expectation of the approximate\nposterior over observed data that encourages the disentanglement. We also\npropose a new disentanglement metric which is better aligned with the\nqualitative disentanglement observed in the decoder's output. We empirically\nobserve significant improvement over existing methods in terms of both\ndisentanglement and data likelihood (reconstruction quality).\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 17:57:43 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 21:29:36 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 19:25:22 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Kumar", "Abhishek", ""], ["Sattigeri", "Prasanna", ""], ["Balakrishnan", "Avinash", ""]]}, {"id": "1711.00888", "submitter": "I-Hong Jhuo", "authors": "I-Hong Jhuo, Jun Wang", "title": "Set-to-Set Hashing with Applications in Visual Recognition", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual data, such as an image or a sequence of video frames, is often\nnaturally represented as a point set. In this paper, we consider the\nfundamental problem of finding a nearest set from a collection of sets, to a\nquery set. This problem has obvious applications in large-scale visual\nretrieval and recognition, and also in applied fields beyond computer vision.\nOne challenge stands out in solving the problem---set representation and\nmeasure of similarity. Particularly, the query set and the sets in dataset\ncollection can have varying cardinalities. The training collection is large\nenough such that linear scan is impractical. We propose a simple representation\nscheme that encodes both statistical and structural information of the sets.\nThe derived representations are integrated in a kernel framework for flexible\nsimilarity measurement. For the query set process, we adopt a learning-to-hash\npipeline that turns the kernel representations into hash bits based on simple\nlearners, using multiple kernel learning. Experiments on two visual retrieval\ndatasets show unambiguously that our set-to-set hashing framework outperforms\nprior methods that do not take the set-to-set search setting.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 18:57:43 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 02:16:43 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Jhuo", "I-Hong", ""], ["Wang", "Jun", ""]]}, {"id": "1711.00889", "submitter": "Hao Zhang", "authors": "Zhijie Deng, Hao Zhang, Xiaodan Liang, Luona Yang, Shizhen Xu, Jun\n  Zhu, Eric P. Xing", "title": "Structured Generative Adversarial Networks", "comments": "To appear in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of conditional generative modeling based on designated\nsemantics or structures. Existing models that build conditional generators\neither require massive labeled instances as supervision or are unable to\naccurately control the semantics of generated samples. We propose structured\ngenerative adversarial networks (SGANs) for semi-supervised conditional\ngenerative modeling. SGAN assumes the data x is generated conditioned on two\nindependent latent variables: y that encodes the designated semantics, and z\nthat contains other factors of variation. To ensure disentangled semantics in y\nand z, SGAN builds two collaborative games in the hidden space to minimize the\nreconstruction error of y and z, respectively. Training SGAN also involves\nsolving two adversarial games that have their equilibrium concentrating at the\ntrue joint data distributions p(x, z) and p(x, y), avoiding distributing the\nprobability mass diffusely over data space that MLE-based methods may suffer.\nWe assess SGAN by evaluating its trained networks, and its performance on\ndownstream tasks. We show that SGAN delivers a highly controllable generator,\nand disentangled representations; it also establishes start-of-the-art results\nacross multiple datasets when applied for semi-supervised image classification\n(1.27%, 5.73%, 17.26% error rates on MNIST, SVHN and CIFAR-10 using 50, 1000\nand 4000 labels, respectively). Benefiting from the separate modeling of y and\nz, SGAN can generate images with high visual quality and strictly following the\ndesignated semantic, and can be extended to a wide spectrum of applications,\nsuch as style transfer.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 19:00:56 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Deng", "Zhijie", ""], ["Zhang", "Hao", ""], ["Liang", "Xiaodan", ""], ["Yang", "Luona", ""], ["Xu", "Shizhen", ""], ["Zhu", "Jun", ""], ["Xing", "Eric P.", ""]]}, {"id": "1711.00953", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Joachim Denzler", "title": "Automatic Query Image Disambiguation for Content-Based Image Retrieval", "comments": "VISAPP 2018 paper, 8 pages, 5 figures. Source code:\n  https://github.com/cvjena/aid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query images presented to content-based image retrieval systems often have\nvarious different interpretations, making it difficult to identify the search\nobjective pursued by the user. We propose a technique for overcoming this\nambiguity, while keeping the amount of required user interaction at a minimum.\nTo achieve this, the neighborhood of the query image is divided into coherent\nclusters from which the user may choose the relevant ones. A novel feedback\nintegration technique is then employed to re-rank the entire database with\nregard to both the user feedback and the original query. We evaluate our\napproach on the publicly available MIRFLICKR-25K dataset, where it leads to a\nrelative improvement of average precision by 23% over the baseline retrieval,\nwhich does not distinguish between different image senses.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 21:55:11 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Denzler", "Joachim", ""]]}, {"id": "1711.00970", "submitter": "Shibani Santurkar", "authors": "Shibani Santurkar, Ludwig Schmidt and Aleksander M\\k{a}dry", "title": "A Classification-Based Study of Covariate Shift in GAN Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic, and still largely unanswered, question in the context of Generative\nAdversarial Networks (GANs) is whether they are truly able to capture all the\nfundamental characteristics of the distributions they are trained on. In\nparticular, evaluating the diversity of GAN distributions is challenging and\nexisting methods provide only a partial understanding of this issue. In this\npaper, we develop quantitative and scalable tools for assessing the diversity\nof GAN distributions. Specifically, we take a classification-based perspective\nand view loss of diversity as a form of covariate shift introduced by GANs. We\nexamine two specific forms of such shift: mode collapse and boundary\ndistortion. In contrast to prior work, our methods need only minimal human\nsupervision and can be readily applied to state-of-the-art GANs on large,\ncanonical datasets. Examining popular GANs using our tools indicates that these\nGANs have significant problems in reproducing the more distributional\nproperties of their training dataset.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 23:13:39 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 14:41:30 GMT"}, {"version": "v3", "created": "Sun, 19 Nov 2017 16:46:20 GMT"}, {"version": "v4", "created": "Wed, 11 Apr 2018 02:07:59 GMT"}, {"version": "v5", "created": "Wed, 30 May 2018 03:23:47 GMT"}, {"version": "v6", "created": "Sat, 2 Jun 2018 03:49:25 GMT"}, {"version": "v7", "created": "Wed, 6 Jun 2018 02:51:11 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Santurkar", "Shibani", ""], ["Schmidt", "Ludwig", ""], ["M\u0105dry", "Aleksander", ""]]}, {"id": "1711.00972", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi and Khaled F. Hussain", "title": "The Achievement of Higher Flexibility in Multiple Choice-based Tests\n  Using Image Classification Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the high accuracy of the existing optical mark reading (OMR)\nsystems and devices, a few restrictions remain existent. In this work, we aim\nto reduce the restrictions of multiple choice questions (MCQ) within tests. We\nuse an image registration technique to extract the answer boxes from answer\nsheets. Unlike other systems that rely on simple image processing steps to\nrecognize the extracted answer boxes, we address the problem from another\nperspective by training a machine learning classifier to recognize the class of\neach answer box (i.e., confirmed, crossed out, or blank answer). This gives us\nthe ability to deal with a variety of shading and mark patterns, and\ndistinguish between chosen (i.e., confirmed) and canceled answers (i.e.,\ncrossed out). All existing machine learning techniques require a large number\nof examples in order to train a model for classification, therefore we present\na dataset including six real MCQ assessments with different answer sheet\ntemplates. We evaluate two strategies of classification: a straight-forward\napproach and a two-stage classifier approach. We test two handcrafted feature\nmethods and a convolutional neural network. In the end, we present an\neasy-to-use graphical user interface of the proposed system. Compared with\nexisting OMR systems, the proposed system has the least constraints and\nachieves a high accuracy. We believe that the presented work will further\ndirect the development of OMR systems towards reducing the restrictions of the\nMCQ tests.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 23:36:13 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 02:09:11 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 14:56:41 GMT"}, {"version": "v4", "created": "Wed, 15 Nov 2017 06:34:59 GMT"}, {"version": "v5", "created": "Wed, 29 Nov 2017 03:44:19 GMT"}, {"version": "v6", "created": "Sat, 2 Dec 2017 17:56:51 GMT"}, {"version": "v7", "created": "Wed, 18 Jul 2018 04:34:37 GMT"}, {"version": "v8", "created": "Sat, 4 Aug 2018 19:54:08 GMT"}, {"version": "v9", "created": "Sat, 12 Jan 2019 03:34:13 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Afifi", "Mahmoud", ""], ["Hussain", "Khaled F.", ""]]}, {"id": "1711.01004", "submitter": "Aldo Zaimi", "authors": "Aldo Zaimi, Maxime Wabartha, Victor Herman, Pierre-Louis Antonsanti,\n  Christian Samuel Perone, Julien Cohen-Adad", "title": "AxonDeepSeg: automatic axon and myelin segmentation from microscopy data\n  using convolutional neural networks", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": "10.1038/s41598-018-22181-4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of axon and myelin from microscopy images of the nervous system\nprovides useful quantitative information about the tissue microstructure, such\nas axon density and myelin thickness. This could be used for instance to\ndocument cell morphometry across species, or to validate novel non-invasive\nquantitative magnetic resonance imaging techniques. Most currently-available\nsegmentation algorithms are based on standard image processing and usually\nrequire multiple processing steps and/or parameter tuning by the user to adapt\nto different modalities. Moreover, only few methods are publicly available. We\nintroduce AxonDeepSeg, an open-source software that performs axon and myelin\nsegmentation of microscopic images using deep learning. AxonDeepSeg features:\n(i) a convolutional neural network architecture; (ii) an easy training\nprocedure to generate new models based on manually-labelled data and (iii) two\nready-to-use models trained from scanning electron microscopy (SEM) and\ntransmission electron microscopy (TEM). Results show high pixel-wise accuracy\nacross various species: 85% on rat SEM, 81% on human SEM, 95% on mice TEM and\n84% on macaque TEM. Segmentation of a full rat spinal cord slice is computed\nand morphological metrics are extracted and compared against the literature.\nAxonDeepSeg is freely available at https://github.com/neuropoly/axondeepseg\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 03:04:23 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 16:10:04 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Zaimi", "Aldo", ""], ["Wabartha", "Maxime", ""], ["Herman", "Victor", ""], ["Antonsanti", "Pierre-Louis", ""], ["Perone", "Christian Samuel", ""], ["Cohen-Adad", "Julien", ""]]}, {"id": "1711.01005", "submitter": "Sarah Ostadabbas", "authors": "Shuangjun Liu, Yu Yin, and Sarah Ostadabbas", "title": "In-Bed Pose Estimation: Deep Learning with Shallow Dataset", "comments": null, "journal-ref": "IEEE Journal of Translational Engineering in Health and Medicine\n  2019", "doi": "10.1109/JTEHM.2019.2892970", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although human pose estimation for various computer vision (CV) applications\nhas been studied extensively in the last few decades, yet in-bed pose\nestimation using camera-based vision methods has been ignored by the CV\ncommunity because it is assumed to be identical to the general purpose pose\nestimation methods. However, in-bed pose estimation has its own specialized\naspects and comes with specific challenges including the notable differences in\nlighting conditions throughout a day and also having different pose\ndistribution from the common human surveillance viewpoint. In this paper, we\ndemonstrate that these challenges significantly lessen the effectiveness of\nexisting general purpose pose estimation models. In order to address the\nlighting variation challenge, infrared selective (IRS) image acquisition\ntechnique is proposed to provide uniform quality data under various lighting\nconditions. In addition, to deal with unconventional pose perspective, a 2-end\nhistogram of oriented gradient (HOG) rectification method is presented. In this\nwork, we explored the idea of employing a pre-trained convolutional neural\nnetwork (CNN) model trained on large public datasets of general human poses and\nfine-tuning the model using our own shallow in-bed IRS dataset. We developed an\nIRS imaging system and collected IRS image data from several realistic\nlife-size mannequins in a simulated hospital room environment. A pre-trained\nCNN called convolutional pose machine (CPM) was repurposed for in-bed pose\nestimation by fine-tuning its specific intermediate layers. Using the HOG\nrectification method, the pose estimation performance of CPM significantly\nimproved by 26.4% in PCK0.1 criteria compared to the model without such\nrectification.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 03:05:05 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 03:32:39 GMT"}, {"version": "v3", "created": "Sat, 7 Jul 2018 20:37:06 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Liu", "Shuangjun", ""], ["Yin", "Yu", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "1711.01043", "submitter": "Chi-Hao Wu", "authors": "Chi-Hao Wu, Qin Huang, Siyang Li, C.-C. Jay Kuo", "title": "A Taught-Obesrve-Ask (TOA) Method for Object Detection with Critical\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being inspired by child's learning experience - taught first and followed by\nobservation and questioning, we investigate a critically supervised learning\nmethodology for object detection in this work. Specifically, we propose a\ntaught-observe-ask (TOA) method that consists of several novel components such\nas negative object proposal, critical example mining, and machine-guided\nquestion-answer (QA) labeling. To consider labeling time and performance\njointly, new evaluation methods are developed to compare the performance of the\nTOA method, with the fully and weakly supervised learning methods. Extensive\nexperiments are conducted on the PASCAL VOC and the Caltech benchmark datasets.\nThe TOA method provides significantly improved performance of weakly\nsupervision yet demands only about 3-6% of labeling time of full supervision.\nThe effectiveness of each novel component is also analyzed.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 07:19:59 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Wu", "Chi-Hao", ""], ["Huang", "Qin", ""], ["Li", "Siyang", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1711.01062", "submitter": "Hengduo Li", "authors": "Hengduo Li, Jun Liu, Guyue Zhang, Yuan Gao, Yirui Wu", "title": "Multi-Glimpse LSTM with Color-Depth Feature Fusion for Human Detection", "comments": "ICIP 2017 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of depth cameras such as Kinect and Intel Realsense,\nRGB-D based human detection receives continuous research attention due to its\nusage in a variety of applications. In this paper, we propose a new\nMulti-Glimpse LSTM (MG-LSTM) network, in which multi-scale contextual\ninformation is sequentially integrated to promote the human detection\nperformance. Furthermore, we propose a feature fusion strategy based on our\nMG-LSTM network to better incorporate the RGB and depth information. To the\nbest of our knowledge, this is the first attempt to utilize LSTM structure for\nRGB-D based human detection. Our method achieves superior performance on two\npublicly available datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 08:52:42 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Li", "Hengduo", ""], ["Liu", "Jun", ""], ["Zhang", "Guyue", ""], ["Gao", "Yuan", ""], ["Wu", "Yirui", ""]]}, {"id": "1711.01094", "submitter": "Weidi Xie", "authors": "Davis M. Vigneault, Weidi Xie, Carolyn Y. Ho, David A. Bluemke, J.\n  Alison Noble", "title": "{\\Omega}-Net (Omega-Net): Fully Automatic, Multi-View Cardiac MR\n  Detection, Orientation, and Segmentation with Deep Neural Networks", "comments": "First two authors contributed equally to this work, result for MICCAI\n  Automated Cardiac Diagnosis Challenge (ACDC) dataset is added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pixelwise segmentation of the left ventricular (LV) myocardium and the four\ncardiac chambers in 2-D steady state free precession (SSFP) cine sequences is\nan essential preprocessing step for a wide range of analyses. Variability in\ncontrast, appearance, orientation, and placement of the heart between patients,\nclinical views, scanners, and protocols makes fully automatic semantic\nsegmentation a notoriously difficult problem. Here, we present ${\\Omega}$-Net\n(Omega-Net): a novel convolutional neural network (CNN) architecture for\nsimultaneous localization, transformation into a canonical orientation, and\nsemantic segmentation. First, an initial segmentation is performed on the input\nimage, second, the features learned during this initial segmentation are used\nto predict the parameters needed to transform the input image into a canonical\norientation, and third, a final segmentation is performed on the transformed\nimage. In this work, ${\\Omega}$-Nets of varying depths were trained to detect\nfive foreground classes in any of three clinical views (short axis, SA,\nfour-chamber, 4C, two-chamber, 2C), without prior knowledge of the view being\nsegmented. The architecture was trained on a cohort of patients with\nhypertrophic cardiomyopathy and healthy control subjects. Network performance\nas measured by weighted foreground intersection-over-union (IoU) was\nsubstantially improved in the best-performing ${\\Omega}$- Net compared with\nU-Net segmentation without localization or orientation. In addition,\n{\\Omega}-Net was retrained from scratch on the 2017 MICCAI ACDC dataset, and\nachieves state-of-the-art results on the LV and RV bloodpools, and performed\nslightly worse in segmentation of the LV myocardium. We conclude this\narchitecture represents a substantive advancement over prior approaches, with\nimplications for biomedical image segmentation more generally.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 10:31:27 GMT"}, {"version": "v2", "created": "Sun, 26 Nov 2017 17:45:06 GMT"}, {"version": "v3", "created": "Tue, 20 Mar 2018 10:32:33 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Vigneault", "Davis M.", ""], ["Xie", "Weidi", ""], ["Ho", "Carolyn Y.", ""], ["Bluemke", "David A.", ""], ["Noble", "J. Alison", ""]]}, {"id": "1711.01117", "submitter": "Maike Stoeve", "authors": "Maike P. Stoeve, Marc Aubreville, Nicolai Oetter, Christian Knipfer,\n  Helmut Neumann, Florian Stelzle, Andreas Maier", "title": "Motion Artifact Detection in Confocal Laser Endomicroscopy Images", "comments": null, "journal-ref": "Bildverarbeitung f\\\"ur die Medizin 2018 (2018) 328-333", "doi": "10.1007/978-3-662-56537-7_85", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Confocal Laser Endomicroscopy (CLE), an optical imaging technique allowing\nnon-invasive examination of the mucosa on a (sub)cellular level, has proven to\nbe a valuable diagnostic tool in gastroenterology and shows promising results\nin various anatomical regions including the oral cavity. Recently, the\nfeasibility of automatic carcinoma detection for CLE images of sufficient\nquality was shown. However, in real world data sets a high amount of CLE images\nis corrupted by artifacts. Amongst the most prevalent artifact types are\nmotion-induced image deteriorations. In the scope of this work, algorithmic\napproaches for the automatic detection of motion artifact-tainted image regions\nwere developed. Hence, this work provides an important step towards clinical\napplicability of automatic carcinoma detection. Both, conventional machine\nlearning and novel, deep learning-based approaches were assessed. The deep\nlearning-based approach outperforms the conventional approaches, attaining an\nAUC of 0.90.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 12:02:46 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 12:47:13 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Stoeve", "Maike P.", ""], ["Aubreville", "Marc", ""], ["Oetter", "Nicolai", ""], ["Knipfer", "Christian", ""], ["Neumann", "Helmut", ""], ["Stelzle", "Florian", ""], ["Maier", "Andreas", ""]]}, {"id": "1711.01124", "submitter": "Zheng Zhu", "authors": "Zheng Zhu, Wei Wu, Wei Zou and Junjie Yan", "title": "End-to-end Flow Correlation Tracking with Spatial-temporal Attention", "comments": "Accepted in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative correlation filters (DCF) with deep convolutional features\nhave achieved favorable performance in recent tracking benchmarks. However,\nmost of existing DCF trackers only consider appearance features of current\nframe, and hardly benefit from motion and inter-frame information. The lack of\ntemporal information degrades the tracking performance during challenges such\nas partial occlusion and deformation. In this work, we focus on making use of\nthe rich flow information in consecutive frames to improve the feature\nrepresentation and the tracking accuracy. Firstly, individual components,\nincluding optical flow estimation, feature extraction, aggregation and\ncorrelation filter tracking are formulated as special layers in network. To the\nbest of our knowledge, this is the first work to jointly train flow and\ntracking task in a deep learning framework. Then the historical feature maps at\npredefined intervals are warped and aggregated with current ones by the guiding\nof flow. For adaptive aggregation, we propose a novel spatial-temporal\nattention mechanism. Extensive experiments are performed on four challenging\ntracking datasets: OTB2013, OTB2015, VOT2015 and VOT2016, and the proposed\nmethod achieves superior results on these benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 12:20:50 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 07:50:24 GMT"}, {"version": "v3", "created": "Tue, 14 Nov 2017 16:59:36 GMT"}, {"version": "v4", "created": "Tue, 27 Feb 2018 12:08:55 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Zhu", "Zheng", ""], ["Wu", "Wei", ""], ["Zou", "Wei", ""], ["Yan", "Junjie", ""]]}, {"id": "1711.01201", "submitter": "Dhireesha Kudithipudi", "authors": "Dillon Graham, Seyed Hamed Fatemi Langroudi, Christopher Kanan, and\n  Dhireesha Kudithipudi", "title": "Convolutional Drift Networks for Video Classification", "comments": "Published in IEEE Rebooting Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing spatio-temporal data like video is a challenging task that requires\nprocessing visual and temporal information effectively. Convolutional Neural\nNetworks have shown promise as baseline fixed feature extractors through\ntransfer learning, a technique that helps minimize the training cost on visual\ninformation. Temporal information is often handled using hand-crafted features\nor Recurrent Neural Networks, but this can be overly specific or prohibitively\ncomplex. Building a fully trainable system that can efficiently analyze\nspatio-temporal data without hand-crafted features or complex training is an\nopen challenge. We present a new neural network architecture to address this\nchallenge, the Convolutional Drift Network (CDN). Our CDN architecture combines\nthe visual feature extraction power of deep Convolutional Neural Networks with\nthe intrinsically efficient temporal processing provided by Reservoir\nComputing. In this introductory paper on the CDN, we provide a very simple\nbaseline implementation tested on two egocentric (first-person) video activity\ndatasets.We achieve video-level activity classification results on-par with\nstate-of-the art methods. Notably, performance on this complex spatio-temporal\ntask was produced by only training a single feed-forward layer in the CDN.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 15:07:24 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Graham", "Dillon", ""], ["Langroudi", "Seyed Hamed Fatemi", ""], ["Kanan", "Christopher", ""], ["Kudithipudi", "Dhireesha", ""]]}, {"id": "1711.01218", "submitter": "Sarah Ostadabbas", "authors": "Behnaz Rezaei and Sarah Ostadabbas", "title": "Background Subtraction via Fast Robust Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background subtraction is the primary task of the majority of video\ninspection systems. The most important part of the background subtraction which\nis common among different algorithms is background modeling. In this regard,\nour paper addresses the problem of background modeling in a computationally\nefficient way, which is important for current eruption of \"big data\" processing\ncoming from high resolution multi-channel videos. Our model is based on the\nassumption that background in natural images lies on a low-dimensional\nsubspace. We formulated and solved this problem in a low-rank matrix completion\nframework. In modeling the background, we benefited from the in-face extended\nFrank-Wolfe algorithm for solving a defined convex optimization problem. We\nevaluated our fast robust matrix completion (fRMC) method on both background\nmodels challenge (BMC) and Stuttgart artificial background subtraction (SABS)\ndatasets. The results were compared with the robust principle component\nanalysis (RPCA) and low-rank robust matrix completion (RMC) methods, both\nsolved by inexact augmented Lagrangian multiplier (IALM). The results showed\nfaster computation, at least twice as when IALM solver is used, while having a\ncomparable accuracy even better in some challenges, in subtracting the\nbackgrounds in order to detect moving objects in the scene.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 15:55:49 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Rezaei", "Behnaz", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "1711.01239", "submitter": "Clemens Rosenbaum", "authors": "Clemens Rosenbaum, Tim Klinger and Matthew Riemer", "title": "Routing Networks: Adaptive Selection of Non-linear Functions for\n  Multi-Task Learning", "comments": "Under Review at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) with neural networks leverages commonalities in\ntasks to improve performance, but often suffers from task interference which\nreduces the benefits of transfer. To address this issue we introduce the\nrouting network paradigm, a novel neural network and training algorithm. A\nrouting network is a kind of self-organizing neural network consisting of two\ncomponents: a router and a set of one or more function blocks. A function block\nmay be any neural network - for example a fully-connected or a convolutional\nlayer. Given an input the router makes a routing decision, choosing a function\nblock to apply and passing the output back to the router recursively,\nterminating when a fixed recursion depth is reached. In this way the routing\nnetwork dynamically composes different function blocks for each input. We\nemploy a collaborative multi-agent reinforcement learning (MARL) approach to\njointly train the router and function blocks. We evaluate our model against\ncross-stitch networks and shared-layer baselines on multi-task settings of the\nMNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a\nsignificant improvement in accuracy, with sharper convergence. In addition,\nrouting networks have nearly constant per-task training cost while cross-stitch\nnetworks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we\nobtain cross-stitch performance levels with an 85% reduction in training time.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 17:07:51 GMT"}, {"version": "v2", "created": "Sun, 31 Dec 2017 14:53:00 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Rosenbaum", "Clemens", ""], ["Klinger", "Tim", ""], ["Riemer", "Matthew", ""]]}, {"id": "1711.01243", "submitter": "Mohammad Ghasemzadeh", "authors": "Mohammad Ghasemzadeh, Mohammad Samragh, Farinaz Koushanfar", "title": "ReBNet: Residual Binarized Neural Network", "comments": "To Appear In The 26th IEEE International Symposium on\n  Field-Programmable Custom Computing Machines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes ReBNet, an end-to-end framework for training\nreconfigurable binary neural networks on software and developing efficient\naccelerators for execution on FPGA. Binary neural networks offer an intriguing\nopportunity for deploying large-scale deep learning models on\nresource-constrained devices. Binarization reduces the memory footprint and\nreplaces the power-hungry matrix-multiplication with light-weight XnorPopcount\noperations. However, binary networks suffer from a degraded accuracy compared\nto their fixed-point counterparts. We show that the state-of-the-art methods\nfor optimizing binary networks accuracy, significantly increase the\nimplementation cost and complexity. To compensate for the degraded accuracy\nwhile adhering to the simplicity of binary networks, we devise the first\nreconfigurable scheme that can adjust the classification accuracy based on the\napplication. Our proposition improves the classification accuracy by\nrepresenting features with multiple levels of residual binarization. Unlike\nprevious methods, our approach does not exacerbate the area cost of the\nhardware accelerator. Instead, it provides a tradeoff between throughput and\naccuracy while the area overhead of multi-level binarization is negligible.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 17:12:15 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 00:45:57 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 20:58:01 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Ghasemzadeh", "Mohammad", ""], ["Samragh", "Mohammad", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1711.01249", "submitter": "Roozbeh Rajabi", "authors": "Sara Khoshsokhan, Roozbeh Rajabi, Hadi Zayyani", "title": "Distributed Unmixing of Hyperspectral Data With Sparsity Constraint", "comments": "6 pages, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral unmixing (SU) is a data processing problem in hyperspectral remote\nsensing. The significant challenge in the SU problem is how to identify\nendmembers and their weights, accurately. For estimation of signature and\nfractional abundance matrices in a blind problem, nonnegative matrix\nfactorization (NMF) and its developments are used widely in the SU problem. One\nof the constraints which was added to NMF is sparsity constraint that was\nregularized by L 1/2 norm. In this paper, a new algorithm based on distributed\noptimization has been used for spectral unmixing. In the proposed algorithm, a\nnetwork including single-node clusters has been employed. Each pixel in\nhyperspectral images considered as a node in this network. The distributed\nunmixing with sparsity constraint has been optimized with diffusion LMS\nstrategy, and then the update equations for fractional abundance and signature\nmatrices are obtained. Simulation results based on defined performance metrics,\nillustrate advantage of the proposed algorithm in spectral unmixing of\nhyperspectral data compared with other methods. The results show that the AAD\nand SAD of the proposed approach are improved respectively about 6 and 27\npercent toward distributed unmixing in SNR=25dB.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 17:23:49 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Khoshsokhan", "Sara", ""], ["Rajabi", "Roozbeh", ""], ["Zayyani", "Hadi", ""]]}, {"id": "1711.01345", "submitter": "Matthieu Le", "authors": "Matthieu Le, Jesse Lieman-Sifry, Felix Lau, Sean Sall, Albert Hsiao,\n  Daniel Golden", "title": "Computationally efficient cardiac views projection using 3D\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  4D Flow is an MRI sequence which allows acquisition of 3D images of the\nheart. The data is typically acquired volumetrically, so it must be reformatted\nto generate cardiac long axis and short axis views for diagnostic\ninterpretation. These views may be generated by placing 6 landmarks: the left\nand right ventricle apex, and the aortic, mitral, pulmonary, and tricuspid\nvalves. In this paper, we propose an automatic method to localize landmarks in\norder to compute the cardiac views. Our approach consists of first calculating\na bounding box that tightly crops the heart, followed by a landmark\nlocalization step within this bounded region. Both steps are based on a 3D\nextension of the recently introduced ENet. We demonstrate that the long and\nshort axis projections computed with our automated method are of equivalent\nquality to projections created with landmarks placed by an experienced cardiac\nradiologist, based on a blinded test administered to a different cardiac\nradiologist.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 21:47:05 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Le", "Matthieu", ""], ["Lieman-Sifry", "Jesse", ""], ["Lau", "Felix", ""], ["Sall", "Sean", ""], ["Hsiao", "Albert", ""], ["Golden", "Daniel", ""]]}, {"id": "1711.01357", "submitter": "Katherine Bouman", "authors": "Katherine L. Bouman, Michael D. Johnson, Adrian V. Dalca, Andrew A.\n  Chael, Freek Roelofs, Sheperd S. Doeleman, William T. Freeman", "title": "Reconstructing Video from Interferometric Measurements of Time-Varying\n  Sources", "comments": "Submitted to Transactions on Computational Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very long baseline interferometry (VLBI) makes it possible to recover images\nof astronomical sources with extremely high angular resolution. Most recently,\nthe Event Horizon Telescope (EHT) has extended VLBI to short millimeter\nwavelengths with a goal of achieving angular resolution sufficient for imaging\nthe event horizons of nearby supermassive black holes. VLBI provides\nmeasurements related to the underlying source image through a sparse set\nspatial frequencies. An image can then be recovered from these measurements by\nmaking assumptions about the underlying image. One of the most important\nassumptions made by conventional imaging methods is that over the course of a\nnight's observation the image is static. However, for quickly evolving sources,\nsuch as the galactic center's supermassive black hole (Sgr A*) targeted by the\nEHT, this assumption is violated and these conventional imaging approaches\nfail. In this work we propose a new way to model VLBI measurements that allows\nus to recover both the appearance and dynamics of an evolving source by\nreconstructing a video rather than a static image. By modeling VLBI\nmeasurements using a Gaussian Markov Model, we are able to propagate\ninformation across observations in time to reconstruct a video, while\nsimultaneously learning about the dynamics of the source's emission region. We\ndemonstrate our proposed Expectation-Maximization (EM) algorithm, StarWarps, on\nrealistic synthetic observations of black holes, and show how it substantially\nimproves results compared to conventional imaging algorithms. Additionally, we\ndemonstrate StarWarps on real VLBI data of the M87 Jet from the VLBA.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 23:09:17 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 23:37:42 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Bouman", "Katherine L.", ""], ["Johnson", "Michael D.", ""], ["Dalca", "Adrian V.", ""], ["Chael", "Andrew A.", ""], ["Roelofs", "Freek", ""], ["Doeleman", "Sheperd S.", ""], ["Freeman", "William T.", ""]]}, {"id": "1711.01371", "submitter": "Runmin Cong", "authors": "Runmin Cong, Jianjun Lei, Huazhu Fu, Weisi Lin, Qingming Huang,\n  Xiaochun Cao, and Chunping Hou", "title": "An Iterative Co-Saliency Framework for RGBD Images", "comments": "13 pages, 13 figures, Accepted by IEEE Transactions on Cybernetics\n  2017. Project URL: https://rmcong.github.io/proj_RGBD_cosal_tcyb.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a newly emerging and significant topic in computer vision community,\nco-saliency detection aims at discovering the common salient objects in\nmultiple related images. The existing methods often generate the co-saliency\nmap through a direct forward pipeline which is based on the designed cues or\ninitialization, but lack the refinement-cycle scheme. Moreover, they mainly\nfocus on RGB image and ignore the depth information for RGBD images. In this\npaper, we propose an iterative RGBD co-saliency framework, which utilizes the\nexisting single saliency maps as the initialization, and generates the final\nRGBD cosaliency map by using a refinement-cycle model. Three schemes are\nemployed in the proposed RGBD co-saliency framework, which include the addition\nscheme, deletion scheme, and iteration scheme. The addition scheme is used to\nhighlight the salient regions based on intra-image depth propagation and\nsaliency propagation, while the deletion scheme filters the saliency regions\nand removes the non-common salient regions based on interimage constraint. The\niteration scheme is proposed to obtain more homogeneous and consistent\nco-saliency map. Furthermore, a novel descriptor, named depth shape prior, is\nproposed in the addition scheme to introduce the depth information to enhance\nidentification of co-salient objects. The proposed method can effectively\nexploit any existing 2D saliency model to work well in RGBD co-saliency\nscenarios. The experiments on two RGBD cosaliency datasets demonstrate the\neffectiveness of our proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 00:41:06 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Cong", "Runmin", ""], ["Lei", "Jianjun", ""], ["Fu", "Huazhu", ""], ["Lin", "Weisi", ""], ["Huang", "Qingming", ""], ["Cao", "Xiaochun", ""], ["Hou", "Chunping", ""]]}, {"id": "1711.01458", "submitter": "Jonathan Binas", "authors": "Jonathan Binas, Daniel Neil, Shih-Chii Liu, Tobi Delbruck", "title": "DDD17: End-To-End DAVIS Driving Dataset", "comments": "Presented at the ICML 2017 Workshop on Machine Learning for\n  Autonomous Vehicles", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras, such as dynamic vision sensors (DVS), and dynamic and\nactive-pixel vision sensors (DAVIS) can supplement other autonomous driving\nsensors by providing a concurrent stream of standard active pixel sensor (APS)\nimages and DVS temporal contrast events. The APS stream is a sequence of\nstandard grayscale global-shutter image sensor frames. The DVS events represent\nbrightness changes occurring at a particular moment, with a jitter of about a\nmillisecond under most lighting conditions. They have a dynamic range of >120\ndB and effective frame rates >1 kHz at data rates comparable to 30 fps\n(frames/second) image sensors. To overcome some of the limitations of current\nimage acquisition technology, we investigate in this work the use of the\ncombined DVS and APS streams in end-to-end driving applications. The dataset\nDDD17 accompanying this paper is the first open dataset of annotated DAVIS\ndriving recordings. DDD17 has over 12 h of a 346x260 pixel DAVIS sensor\nrecording highway and city driving in daytime, evening, night, dry and wet\nweather conditions, along with vehicle speed, GPS position, driver steering,\nthrottle, and brake captured from the car's on-board diagnostics interface. As\nan example application, we performed a preliminary end-to-end learning study of\nusing a convolutional neural network that is trained to predict the\ninstantaneous steering angle from DVS and APS visual data.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 16:19:56 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Binas", "Jonathan", ""], ["Neil", "Daniel", ""], ["Liu", "Shih-Chii", ""], ["Delbruck", "Tobi", ""]]}, {"id": "1711.01467", "submitter": "Rohit Girdhar", "authors": "Rohit Girdhar and Deva Ramanan", "title": "Attentional Pooling for Action Recognition", "comments": "In NIPS 2017. Project page:\n  https://rohitgirdhar.github.io/AttentionalPoolingAction/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple yet surprisingly powerful model to incorporate\nattention in action recognition and human object interaction tasks. Our\nproposed attention module can be trained with or without extra supervision, and\ngives a sizable boost in accuracy while keeping the network size and\ncomputational cost nearly the same. It leads to significant improvements over\nstate of the art base architecture on three standard action recognition\nbenchmarks across still images and videos, and establishes new state of the art\non MPII dataset with 12.5% relative improvement. We also perform an extensive\nanalysis of our attention module both empirically and analytically. In terms of\nthe latter, we introduce a novel derivation of bottom-up and top-down attention\nas low-rank approximations of bilinear pooling methods (typically used for\nfine-grained classification). From this perspective, our attention formulation\nsuggests a novel characterization of action recognition as a fine-grained\nrecognition problem.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 17:37:15 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 04:08:11 GMT"}, {"version": "v3", "created": "Sat, 30 Dec 2017 01:07:10 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Girdhar", "Rohit", ""], ["Ramanan", "Deva", ""]]}, {"id": "1711.01468", "submitter": "Konstantinos Kamnitsas", "authors": "Konstantinos Kamnitsas, Wenjia Bai, Enzo Ferrante, Steven McDonagh,\n  Matthew Sinclair, Nick Pawlowski, Martin Rajchl, Matthew Lee, Bernhard Kainz,\n  Daniel Rueckert, Ben Glocker", "title": "Ensembles of Multiple Models and Architectures for Robust Brain Tumour\n  Segmentation", "comments": "The method won the 1st-place in the Brain Tumour Segmentation (BRATS)\n  2017 competition (segmentation task)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches such as convolutional neural nets have consistently\noutperformed previous methods on challenging tasks such as dense, semantic\nsegmentation. However, the various proposed networks perform differently, with\nbehaviour largely influenced by architectural choices and training settings.\nThis paper explores Ensembles of Multiple Models and Architectures (EMMA) for\nrobust performance through aggregation of predictions from a wide range of\nmethods. The approach reduces the influence of the meta-parameters of\nindividual models and the risk of overfitting the configuration to a particular\ndatabase. EMMA can be seen as an unbiased, generic deep learning model which is\nshown to yield excellent performance, winning the first position in the BRATS\n2017 competition among 50+ participating teams.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 17:43:07 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Kamnitsas", "Konstantinos", ""], ["Bai", "Wenjia", ""], ["Ferrante", "Enzo", ""], ["McDonagh", "Steven", ""], ["Sinclair", "Matthew", ""], ["Pawlowski", "Nick", ""], ["Rajchl", "Martin", ""], ["Lee", "Matthew", ""], ["Kainz", "Bernhard", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""]]}, {"id": "1711.01470", "submitter": "Rui Zhu", "authors": "Rui Zhu, Chaoyang Wang, Chen-Hsuan Lin, Ziyan Wang, Simon Lucey", "title": "Object-Centric Photometric Bundle Adjustment with Deep Shape Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing 3D shapes from a sequence of images has long been a problem of\ninterest in computer vision. Classical Structure from Motion (SfM) methods have\nattempted to solve this problem through projected point displacement \\& bundle\nadjustment. More recently, deep methods have attempted to solve this problem by\ndirectly learning a relationship between geometry and appearance. There is,\nhowever, a significant gap between these two strategies. SfM tackles the\nproblem from purely a geometric perspective, taking no account of the object\nshape prior. Modern deep methods more often throw away geometric constraints\naltogether, rendering the results unreliable. In this paper we make an effort\nto bring these two seemingly disparate strategies together. We introduce\nlearned shape prior in the form of deep shape generators into Photometric\nBundle Adjustment (PBA) and propose to accommodate full 3D shape generated by\nthe shape prior within the optimization-based inference framework,\ndemonstrating impressive results.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 17:57:57 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Zhu", "Rui", ""], ["Wang", "Chaoyang", ""], ["Lin", "Chen-Hsuan", ""], ["Wang", "Ziyan", ""], ["Lucey", "Simon", ""]]}, {"id": "1711.01506", "submitter": "Xiao-Yun Zhou", "authors": "Xiao-Yun Zhou Celia Riga, Su-Lin Lee, Guang-Zhong Yang", "title": "Towards Automatic 3D Shape Instantiation for Deployed Stent Grafts: 2D\n  Multiple-class and Class-imbalance Marker Segmentation with Equally-weighted\n  Focal U-Net", "comments": "7 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot-assisted Fenestrated Endovascular Aortic Repair (FEVAR) is currently\nnavigated by 2D fluoroscopy which is insufficiently informative. Previously, a\nsemi-automatic 3D shape instantiation method was developed to instantiate the\n3D shape of a main, deployed, and fenestrated stent graft from a single\nfluoroscopy projection in real-time, which could help 3D FEVAR navigation and\nrobotic path planning. This proposed semi-automatic method was based on the\nRobust Perspective-5-Point (RP5P) method, graft gap interpolation and\nsemi-automatic multiple-class marker center determination. In this paper, an\nautomatic 3D shape instantiation could be achieved by automatic multiple-class\nmarker segmentation and hence automatic multiple-class marker center\ndetermination. Firstly, the markers were designed into five different shapes.\nThen, Equally-weighted Focal U-Net was proposed to segment the fluoroscopy\nprojections of customized markers into five classes and hence to determine the\nmarker centers. The proposed Equally-weighted Focal U-Net utilized U-Net as the\nnetwork architecture, equally-weighted loss function for initial marker\nsegmentation, and then equally-weighted focal loss function for improving the\ninitial marker segmentation. This proposed network outperformed traditional\nWeighted U-Net on the class-imbalance segmentation in this paper with reducing\none hyper-parameter - the weight. An overall mean Intersection over Union\n(mIoU) of 0.6943 was achieved on 78 testing images, where 81.01% markers were\nsegmented with a center position error <1.6mm. Comparable accuracy of 3D shape\ninstantiation was also achieved and stated. The data, trained models and\nTensorFlow codes are available on-line.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 22:59:18 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 19:29:55 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 12:32:24 GMT"}, {"version": "v4", "created": "Tue, 31 Jul 2018 15:53:41 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Riga", "Xiao-Yun Zhou Celia", ""], ["Lee", "Su-Lin", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1711.01543", "submitter": "Nati Ofir", "authors": "Nati Ofir, Shai Silberstein, Dani Rozenbaum, Yosi Keller, Sharon\n  Duvdevani Bar", "title": "Registration and Fusion of Multi-Spectral Images Using a Novel Edge\n  Descriptor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a fully end-to-end approach for multi-spectral\nimage registration and fusion. Our method for fusion combines images from\ndifferent spectral channels into a single fused image by different approaches\nfor low and high frequency signals. A prerequisite of fusion is a stage of\ngeometric alignment between the spectral bands, commonly referred to as\nregistration. Unfortunately, common methods for image registration of a single\nspectral channel do not yield reasonable results on images from different\nmodalities. For that end, we introduce a new algorithm for multi-spectral image\nregistration, based on a novel edge descriptor of feature points. Our method\nachieves an accurate alignment of a level that allows us to further fuse the\nimages. As our experiments show, we produce a high quality of multi-spectral\nimage registration and fusion under many challenging scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 07:35:23 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 07:09:03 GMT"}, {"version": "v3", "created": "Sun, 21 Jan 2018 06:00:14 GMT"}, {"version": "v4", "created": "Wed, 24 Jan 2018 12:00:13 GMT"}, {"version": "v5", "created": "Mon, 28 May 2018 04:53:53 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Ofir", "Nati", ""], ["Silberstein", "Shai", ""], ["Rozenbaum", "Dani", ""], ["Keller", "Yosi", ""], ["Bar", "Sharon Duvdevani", ""]]}, {"id": "1711.01573", "submitter": "Kun He Prof.", "authors": "Mengxiao Zhang, Wangquan Wu, Yanren Zhang, Kun He, Tao Yu, Huan Long,\n  John E. Hopcroft", "title": "The Local Dimension of Deep Manifold", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on our observation that there exists a dramatic drop for the singular\nvalues of the fully connected layers or a single feature map of the\nconvolutional layer, and that the dimension of the concatenated feature vector\nalmost equals the summation of the dimension on each feature map, we propose a\nsingular value decomposition (SVD) based approach to estimate the dimension of\nthe deep manifolds for a typical convolutional neural network VGG19. We choose\nthree categories from the ImageNet, namely Persian Cat, Container Ship and\nVolcano, and determine the local dimension of the deep manifolds of the deep\nlayers through the tangent space of a target image. Through several\naugmentation methods, we found that the Gaussian noise method is closer to the\nintrinsic dimension, as by adding random noise to an image we are moving in an\narbitrary dimension, and when the rank of the feature matrix of the augmented\nimages does not increase we are very close to the local dimension of the\nmanifold. We also estimate the dimension of the deep manifold based on the\ntangent space for each of the maxpooling layers. Our results show that the\ndimensions of different categories are close to each other and decline quickly\nalong the convolutional layers and fully connected layers. Furthermore, we show\nthat the dimensions decline quickly inside the Conv5 layer. Our work provides\nnew insights for the intrinsic structure of deep neural networks and helps\nunveiling the inner organization of the black box of deep neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 12:17:35 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Zhang", "Mengxiao", ""], ["Wu", "Wangquan", ""], ["Zhang", "Yanren", ""], ["He", "Kun", ""], ["Yu", "Tao", ""], ["Long", "Huan", ""], ["Hopcroft", "John E.", ""]]}, {"id": "1711.01575", "submitter": "Kuniaki Saito Saito Kuniaki", "authors": "Kuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada and Kate Saenko", "title": "Adversarial Dropout Regularization", "comments": "TBA on ICLR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for transferring neural representations from label-rich\nsource domains to unlabeled target domains. Recent adversarial methods proposed\nfor this task learn to align features across domains by fooling a special\ndomain critic network. However, a drawback of this approach is that the critic\nsimply labels the generated features as in-domain or not, without considering\nthe boundaries between classes. This can lead to ambiguous features being\ngenerated near class boundaries, reducing target classification accuracy. We\npropose a novel approach, Adversarial Dropout Regularization (ADR), to\nencourage the generator to output more discriminative features for the target\ndomain. Our key idea is to replace the critic with one that detects\nnon-discriminative features, using dropout on the classifier network. The\ngenerator then learns to avoid these areas of the feature space and thus\ncreates better features. We apply our ADR approach to the problem of\nunsupervised domain adaptation for image classification and semantic\nsegmentation tasks, and demonstrate significant improvement over the state of\nthe art. We also show that our approach can be used to train Generative\nAdversarial Networks for semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 12:26:09 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 03:21:32 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 00:40:13 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Saito", "Kuniaki", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""], ["Saenko", "Kate", ""]]}, {"id": "1711.01589", "submitter": "Saeed Ghodsi", "authors": "Saeed Ghodsi, Hoda Mohammadzade, Erfan Korki", "title": "Simultaneous Joint and Object Trajectory Templates for Human Activity\n  Recognition from 3-D Data", "comments": null, "journal-ref": null, "doi": "10.1016/j.jvcir.2018.08.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of low-cost range sensors and the development of relatively\nrobust algorithms for the extraction of skeleton joint locations have inspired\nmany researchers to develop human activity recognition methods using the 3-D\ndata. In this paper, an effective method for the recognition of human\nactivities from the normalized joint trajectories is proposed. We represent the\nactions as multidimensional signals and introduce a novel method for generating\naction templates by averaging the samples in a \"dynamic time\" sense. Then in\norder to deal with the variations in the speed and style of performing actions,\nwe warp the samples to the action templates by an efficient algorithm and\nemploy wavelet filters to extract meaningful spatiotemporal features. The\nproposed method is also capable of modeling the human-object interactions, by\nperforming the template generation and temporal warping procedure via the joint\nand object trajectories simultaneously. The experimental evaluation on several\nchallenging datasets demonstrates the effectiveness of our method compared to\nthe state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 13:52:55 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Ghodsi", "Saeed", ""], ["Mohammadzade", "Hoda", ""], ["Korki", "Erfan", ""]]}, {"id": "1711.01656", "submitter": "Mahdieh Poostchi", "authors": "Mahdieh Poostchi", "title": "Spatial Pyramid Context-Aware Moving Object Detection and Tracking for\n  Full Motion Video and Wide Aerial Motion Imagery", "comments": "PhD Dissertation (162 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust and fast automatic moving object detection and tracking system is\nessential to characterize target object and extract spatial and temporal\ninformation for different functionalities including video surveillance systems,\nurban traffic monitoring and navigation, robotic. In this dissertation, I\npresent a collaborative Spatial Pyramid Context-aware moving object detection\nand Tracking system. The proposed visual tracker is composed of one master\ntracker that usually relies on visual object features and two auxiliary\ntrackers based on object temporal motion information that will be called\ndynamically to assist master tracker. SPCT utilizes image spatial context at\ndifferent level to make the video tracking system resistant to occlusion,\nbackground noise and improve target localization accuracy and robustness. We\nchose a pre-selected seven-channel complementary features including RGB color,\nintensity and spatial pyramid of HoG to encode object color, shape and spatial\nlayout information. We exploit integral histogram as building block to meet the\ndemands of real-time performance. A novel fast algorithm is presented to\naccurately evaluate spatially weighted local histograms in constant time\ncomplexity using an extension of the integral histogram method. Different\ntechniques are explored to efficiently compute integral histogram on GPU\narchitecture and applied for fast spatio-temporal median computations and 3D\nface reconstruction texturing. We proposed a multi-component framework based on\nsemantic fusion of motion information with projected building footprint map to\nsignificantly reduce the false alarm rate in urban scenes with many tall\nstructures. The experiments on extensive VOTC2016 benchmark dataset and aerial\nvideo confirm that combining complementary tracking cues in an intelligent\nfusion framework enables persistent tracking for Full Motion Video and Wide\nAerial Motion Imagery.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 20:07:17 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Poostchi", "Mahdieh", ""]]}, {"id": "1711.01666", "submitter": "Yipeng Hu", "authors": "Yipeng Hu, Marc Modat, Eli Gibson, Nooshin Ghavami, Ester Bonmati,\n  Caroline M. Moore, Mark Emberton, J. Alison Noble, Dean C. Barratt, Tom\n  Vercauteren", "title": "Label-driven weakly-supervised learning for multimodal deformable image\n  registration", "comments": "Accepted to ISBI 2018", "journal-ref": null, "doi": "10.1109/ISBI.2018.8363756", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially aligning medical images from different modalities remains a\nchallenging task, especially for intraoperative applications that require fast\nand robust algorithms. We propose a weakly-supervised, label-driven formulation\nfor learning 3D voxel correspondence from higher-level label correspondence,\nthereby bypassing classical intensity-based image similarity measures. During\ntraining, a convolutional neural network is optimised by outputting a dense\ndisplacement field (DDF) that warps a set of available anatomical labels from\nthe moving image to match their corresponding counterparts in the fixed image.\nThese label pairs, including solid organs, ducts, vessels, point landmarks and\nother ad hoc structures, are only required at training time and can be\nspatially aligned by minimising a cross-entropy function of the warped moving\nlabel and the fixed label. During inference, the trained network takes a new\nimage pair to predict an optimal DDF, resulting in a fully-automatic,\nlabel-free, real-time and deformable registration. For interventional\napplications where large global transformation prevails, we also propose a\nneural network architecture to jointly optimise the global- and local\ndisplacements. Experiment results are presented based on cross-validating\nregistrations of 111 pairs of T2-weighted magnetic resonance images and 3D\ntransrectal ultrasound images from prostate cancer patients with a total of\nover 4000 anatomical labels, yielding a median target registration error of 4.2\nmm on landmark centroids and a median Dice of 0.88 on prostate glands.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 22:01:57 GMT"}, {"version": "v2", "created": "Sun, 24 Dec 2017 22:23:19 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Hu", "Yipeng", ""], ["Modat", "Marc", ""], ["Gibson", "Eli", ""], ["Ghavami", "Nooshin", ""], ["Bonmati", "Ester", ""], ["Moore", "Caroline M.", ""], ["Emberton", "Mark", ""], ["Noble", "J. Alison", ""], ["Barratt", "Dean C.", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1711.01714", "submitter": "Jie Lin", "authors": "Fang Yuan, Zhe Wang, Jie Lin, Luis Fernando D'Haro, Kim Jung Jae, Zeng\n  Zeng, Vijay Chandrasekhar", "title": "End-to-End Video Classification with Knowledge Graphs", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video understanding has attracted much research attention especially since\nthe recent availability of large-scale video benchmarks. In this paper, we\naddress the problem of multi-label video classification. We first observe that\nthere exists a significant knowledge gap between how machines and humans learn.\nThat is, while current machine learning approaches including deep neural\nnetworks largely focus on the representations of the given data, humans often\nlook beyond the data at hand and leverage external knowledge to make better\ndecisions. Towards narrowing the gap, we propose to incorporate external\nknowledge graphs into video classification. In particular, we unify traditional\n\"knowledgeless\" machine learning models and knowledge graphs in a novel\nend-to-end framework. The framework is flexible to work with most existing\nvideo classification algorithms including state-of-the-art deep models.\nFinally, we conduct extensive experiments on the largest public video dataset\nYouTube-8M. The results are promising across the board, improving mean average\nprecision by up to 2.9%.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 03:50:35 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Yuan", "Fang", ""], ["Wang", "Zhe", ""], ["Lin", "Jie", ""], ["D'Haro", "Luis Fernando", ""], ["Jae", "Kim Jung", ""], ["Zeng", "Zeng", ""], ["Chandrasekhar", "Vijay", ""]]}, {"id": "1711.01732", "submitter": "Xiao Lin", "authors": "Xiao Lin, Devi Parikh", "title": "Active Learning for Visual Question Answering: An Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an empirical study of active learning for Visual Question\nAnswering, where a deep VQA model selects informative question-image pairs from\na pool and queries an oracle for answers to maximally improve its performance\nunder a limited query budget. Drawing analogies from human learning, we explore\ncramming (entropy), curiosity-driven (expected model change), and goal-driven\n(expected error reduction) active learning approaches, and propose a fast and\neffective goal-driven active learning scoring function to pick question-image\npairs for deep VQA models under the Bayesian Neural Network framework. We find\nthat deep VQA models need large amounts of training data before they can start\nasking informative questions. But once they do, all three approaches outperform\nthe random selection baseline and achieve significant query savings. For the\nscenario where the model is allowed to ask generic questions about images but\nis evaluated only on specific questions (e.g., questions whose answer is either\nyes or no), our proposed goal-driven scoring function performs the best.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 05:28:38 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Lin", "Xiao", ""], ["Parikh", "Devi", ""]]}, {"id": "1711.01768", "submitter": "Seong Joon Oh", "authors": "Seong Joon Oh, Max Augustin, Bernt Schiele, Mario Fritz", "title": "Towards Reverse-Engineering Black-Box Neural Networks", "comments": "20 pages, 12 figures, to appear at ICLR'18. Code:\n  https://goo.gl/MbYfsv", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many deployed learned models are black boxes: given input, returns output.\nInternal information about the model, such as the architecture, optimisation\nprocedure, or training data, is not disclosed explicitly as it might contain\nproprietary information or make the system more vulnerable. This work shows\nthat such attributes of neural networks can be exposed from a sequence of\nqueries. This has multiple implications. On the one hand, our work exposes the\nvulnerability of black-box neural networks to different types of attacks -- we\nshow that the revealed internal information helps generate more effective\nadversarial examples against the black box model. On the other hand, this\ntechnique can be used for better protection of private content from automatic\nrecognition models using adversarial examples. Our paper suggests that it is\nactually hard to draw a line between white box and black box models.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 07:58:48 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 14:04:23 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 18:50:53 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Oh", "Seong Joon", ""], ["Augustin", "Max", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1711.01791", "submitter": "Zhun Sun", "authors": "Zhun Sun, Mete Ozay, Takayuki Okatani", "title": "HyperNetworks with statistical filtering for defending adversarial\n  examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms have been known to be vulnerable to adversarial\nperturbations in various tasks such as image classification. This problem was\naddressed by employing several defense methods for detection and rejection of\nparticular types of attacks. However, training and manipulating networks\naccording to particular defense schemes increases computational complexity of\nthe learning algorithms. In this work, we propose a simple yet effective method\nto improve robustness of convolutional neural networks (CNNs) to adversarial\nattacks by using data dependent adaptive convolution kernels. To this end, we\npropose a new type of HyperNetwork in order to employ statistical properties of\ninput data and features for computation of statistical adaptive maps. Then, we\nfilter convolution weights of CNNs with the learned statistical maps to compute\ndynamic kernels. Thereby, weights and kernels are collectively optimized for\nlearning of image classification models robust to adversarial attacks without\nemployment of additional target detection and rejection algorithms. We\nempirically demonstrate that the proposed method enables CNNs to spontaneously\ndefend against different types of attacks, e.g. attacks generated by Gaussian\nnoise, fast gradient sign methods (Goodfellow et al., 2014) and a black-box\nattack(Narodytska & Kasiviswanathan, 2016).\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 09:11:47 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Sun", "Zhun", ""], ["Ozay", "Mete", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1711.01889", "submitter": "Jianshu Zhang", "authors": "Jianshu Zhang, Yixing Zhu, Jun Du and Lirong Dai", "title": "Radical analysis network for zero-shot learning in printed Chinese\n  character recognition", "comments": "Accepted by ICME2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese characters have a huge set of character categories, more than 20,000\nand the number is still increasing as more and more novel characters continue\nbeing created. However, the enormous characters can be decomposed into a\ncompact set of about 500 fundamental and structural radicals. This paper\nintroduces a novel radical analysis network (RAN) to recognize printed Chinese\ncharacters by identifying radicals and analyzing two-dimensional spatial\nstructures among them. The proposed RAN first extracts visual features from\ninput by employing convolutional neural networks as an encoder. Then a decoder\nbased on recurrent neural networks is employed, aiming at generating captions\nof Chinese characters by detecting radicals and two-dimensional structures\nthrough a spatial attention mechanism. The manner of treating a Chinese\ncharacter as a composition of radicals rather than a single character class\nlargely reduces the size of vocabulary and enables RAN to possess the ability\nof recognizing unseen Chinese character classes, namely zero-shot learning.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 14:40:28 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 05:25:29 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Zhang", "Jianshu", ""], ["Zhu", "Yixing", ""], ["Du", "Jun", ""], ["Dai", "Lirong", ""]]}, {"id": "1711.01970", "submitter": "Eirikur Agustsson", "authors": "Eirikur Agustsson, Alexander Sage, Radu Timofte, Luc Van Gool", "title": "Optimal transport maps for distribution preserving operations on latent\n  spaces of Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models such as Variational Auto Encoders (VAEs) and Generative\nAdversarial Networks (GANs) are typically trained for a fixed prior\ndistribution in the latent space, such as uniform or Gaussian. After a trained\nmodel is obtained, one can sample the Generator in various forms for\nexploration and understanding, such as interpolating between two samples,\nsampling in the vicinity of a sample or exploring differences between a pair of\nsamples applied to a third sample. In this paper, we show that the latent space\noperations used in the literature so far induce a distribution mismatch between\nthe resulting outputs and the prior distribution the model was trained on. To\naddress this, we propose to use distribution matching transport maps to ensure\nthat such latent space operations preserve the prior distribution, while\nminimally modifying the original operation. Our experimental results validate\nthat the proposed operations give higher quality samples compared to the\noriginal operations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 15:51:29 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 21:55:52 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Agustsson", "Eirikur", ""], ["Sage", "Alexander", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1711.01984", "submitter": "Wei-Hong Li", "authors": "Wei-Hong Li, Benchao Li and Wei-Shi Zheng", "title": "PersonRank: Detecting Important People in Images", "comments": "8 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Always, some individuals in images are more important/attractive than others\nin some events such as presentation, basketball game or speech. However, it is\nchallenging to find important people among all individuals in images directly\nbased on their spatial or appearance information due to the existence of\ndiverse variations of pose, action, appearance of persons and various changes\nof occasions. We overcome this difficulty by constructing a multiple\nHyper-Interaction Graph to treat each individual in an image as a node and\ninferring the most active node referring to interactions estimated by various\ntypes of clews. We model pairwise interactions between persons as the edge\nmessage communicated between nodes, resulting in a bidirectional\npairwise-interaction graph. To enrich the personperson interaction estimation,\nwe further introduce a unidirectional hyper-interaction graph that models the\nconsensus of interaction between a focal person and any person in a local\nregion around. Finally, we modify the PageRank algorithm to infer the\nactiveness of persons on the multiple Hybrid-Interaction Graph (HIG), the union\nof the pairwise-interaction and hyperinteraction graphs, and we call our\nalgorithm the PersonRank. In order to provide publicable datasets for\nevaluation, we have contributed a new dataset called Multi-scene Important\nPeople Image Dataset and gathered a NCAA Basketball Image Dataset from sports\ngame sequences. We have demonstrated that the proposed PersonRank outperforms\nrelated methods clearly and substantially.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 16:09:50 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Li", "Wei-Hong", ""], ["Li", "Benchao", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1711.01991", "submitter": "Cihang Xie", "authors": "Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan Yuille", "title": "Mitigating Adversarial Effects Through Randomization", "comments": "To appear in ICLR 2018, code available at\n  https://github.com/cihangxie/NIPS2017_adv_challenge_defense", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have demonstrated high accuracy on various\ntasks in recent years. However, they are extremely vulnerable to adversarial\nexamples. For example, imperceptible perturbations added to clean images can\ncause convolutional neural networks to fail. In this paper, we propose to\nutilize randomization at inference time to mitigate adversarial effects.\nSpecifically, we use two randomization operations: random resizing, which\nresizes the input images to a random size, and random padding, which pads zeros\naround the input images in a random manner. Extensive experiments demonstrate\nthat the proposed randomization method is very effective at defending against\nboth single-step and iterative attacks. Our method provides the following\nadvantages: 1) no additional training or fine-tuning, 2) very few additional\ncomputations, 3) compatible with other adversarial defense methods. By\ncombining the proposed randomization method with an adversarially trained\nmodel, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense\nteams) in the NIPS 2017 adversarial examples defense challenge, which is far\nbetter than using adversarial training alone with a normalized score of 0.773\n(ranked No.56). The code is public available at\nhttps://github.com/cihangxie/NIPS2017_adv_challenge_defense.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 16:22:54 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 22:45:32 GMT"}, {"version": "v3", "created": "Wed, 28 Feb 2018 22:39:15 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Xie", "Cihang", ""], ["Wang", "Jianyu", ""], ["Zhang", "Zhishuai", ""], ["Ren", "Zhou", ""], ["Yuille", "Alan", ""]]}, {"id": "1711.02010", "submitter": "Dimitrios Marmanis", "authors": "Dimitrios Marmanis, Wei Yao, Fathalrahman Adam, Mihai Datcu, Peter\n  Reinartz, Konrad Schindler, Jan Dirk Wegner, Uwe Stilla", "title": "Artificial Generation of Big Data for Improving Image Classification: A\n  Generative Adversarial Network Approach on SAR Data", "comments": "Submitted for review in \"Big Data from Space 2017\" conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very High Spatial Resolution (VHSR) large-scale SAR image databases are still\nan unresolved issue in the Remote Sensing field. In this work, we propose such\na dataset and use it to explore patch-based classification in urban and\nperiurban areas, considering 7 distinct semantic classes. In this context, we\ninvestigate the accuracy of large CNN classification models and pre-trained\nnetworks for SAR imaging systems. Furthermore, we propose a Generative\nAdversarial Network (GAN) for SAR image generation and test, whether the\nsynthetic data can actually improve classification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 16:52:51 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Marmanis", "Dimitrios", ""], ["Yao", "Wei", ""], ["Adam", "Fathalrahman", ""], ["Datcu", "Mihai", ""], ["Reinartz", "Peter", ""], ["Schindler", "Konrad", ""], ["Wegner", "Jan Dirk", ""], ["Stilla", "Uwe", ""]]}, {"id": "1711.02017", "submitter": "Xiaoliang Dai", "authors": "Xiaoliang Dai, Hongxu Yin, Niraj K. Jha", "title": "NeST: A Neural Network Synthesis Tool Based on a Grow-and-Prune Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have begun to have a pervasive impact on various\napplications of machine learning. However, the problem of finding an optimal\nDNN architecture for large applications is challenging. Common approaches go\nfor deeper and larger DNN architectures but may incur substantial redundancy.\nTo address these problems, we introduce a network growth algorithm that\ncomplements network pruning to learn both weights and compact DNN architectures\nduring training. We propose a DNN synthesis tool (NeST) that combines both\nmethods to automate the generation of compact and accurate DNNs. NeST starts\nwith a randomly initialized sparse network called the seed architecture. It\niteratively tunes the architecture with gradient-based growth and\nmagnitude-based pruning of neurons and connections. Our experimental results\nshow that NeST yields accurate, yet very compact DNNs, with a wide range of\nseed architecture selection. For the LeNet-300-100 (LeNet-5) architecture, we\nreduce network parameters by 70.2x (74.3x) and floating-point operations\n(FLOPs) by 79.4x (43.7x). For the AlexNet and VGG-16 architectures, we reduce\nnetwork parameters (FLOPs) by 15.7x (4.6x) and 30.2x (8.6x), respectively.\nNeST's grow-and-prune paradigm delivers significant additional parameter and\nFLOPs reduction relative to pruning-only methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 17:03:39 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 18:45:01 GMT"}, {"version": "v3", "created": "Fri, 1 Jun 2018 04:04:09 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Dai", "Xiaoliang", ""], ["Yin", "Hongxu", ""], ["Jha", "Niraj K.", ""]]}, {"id": "1711.02037", "submitter": "N. Benjamin Erichson", "authors": "N. Benjamin Erichson and Ariana Mendible and Sophie Wihlborn and J.\n  Nathan Kutz", "title": "Randomized Nonnegative Matrix Factorization", "comments": "This is an extended and revised version of the paper which appeared\n  in JPRL", "journal-ref": "Pattern Recognition Letters, Volume 104, 2018, Pages 1-7", "doi": "10.1016/j.patrec.2018.01.007", "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) is a powerful tool for data mining.\nHowever, the emergence of `big data' has severely challenged our ability to\ncompute this fundamental decomposition using deterministic algorithms. This\npaper presents a randomized hierarchical alternating least squares (HALS)\nalgorithm to compute the NMF. By deriving a smaller matrix from the nonnegative\ninput data, a more efficient nonnegative decomposition can be computed. Our\nalgorithm scales to big data applications while attaining a near-optimal\nfactorization. The proposed algorithm is evaluated using synthetic and real\nworld data and shows substantial speedups compared to deterministic HALS.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 17:42:47 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 19:20:32 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Erichson", "N. Benjamin", ""], ["Mendible", "Ariana", ""], ["Wihlborn", "Sophie", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "1711.02074", "submitter": "Dufan Wu", "authors": "Dufan Wu, Kyungsang Kim, Bin Dong, Georges El Fakhri and Quanzheng Li", "title": "End-to-end Lung Nodule Detection in Computed Tomography", "comments": "published at MLMI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer aided diagnostic (CAD) system is crucial for modern med-ical\nimaging. But almost all CAD systems operate on reconstructed images, which were\noptimized for radiologists. Computer vision can capture features that is subtle\nto human observers, so it is desirable to design a CAD system op-erating on the\nraw data. In this paper, we proposed a deep-neural-network-based detection\nsystem for lung nodule detection in computed tomography (CT). A\nprimal-dual-type deep reconstruction network was applied first to convert the\nraw data to the image space, followed by a 3-dimensional convolutional neural\nnetwork (3D-CNN) for the nodule detection. For efficient network training, the\ndeep reconstruction network and the CNN detector was trained sequentially\nfirst, then followed by one epoch of end-to-end fine tuning. The method was\nevaluated on the Lung Image Database Consortium image collection (LIDC-IDRI)\nwith simulated forward projections. With 144 multi-slice fanbeam pro-jections,\nthe proposed end-to-end detector could achieve comparable sensitivity with the\nreference detector, which was trained and applied on the fully-sampled image\ndata. It also demonstrated superior detection performance compared to detectors\ntrained on the reconstructed images. The proposed method is general and could\nbe expanded to most detection tasks in medical imaging.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 18:41:18 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 18:15:45 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Wu", "Dufan", ""], ["Kim", "Kyungsang", ""], ["Dong", "Bin", ""], ["Fakhri", "Georges El", ""], ["Li", "Quanzheng", ""]]}, {"id": "1711.02131", "submitter": "Sourya Dey", "authors": "Sourya Dey, Kuan-Wen Huang, Peter A. Beerel, Keith M. Chugg", "title": "Characterizing Sparse Connectivity Patterns in Neural Networks", "comments": "Presented at the 2018 Information Theory and Applications Workshop,\n  San Diego, California", "journal-ref": "in 2018 Information Theory and Applications Workshop (ITA), pp.\n  1--8, Feb 2018", "doi": "10.1109/ITA.2018.8502950", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel way of reducing the number of parameters in the\nstorage-hungry fully connected layers of a neural network by using pre-defined\nsparsity, where the majority of connections are absent prior to starting\ntraining. Our results indicate that convolutional neural networks can operate\nwithout any loss of accuracy at less than half percent classification layer\nconnection density, or less than 5 percent overall network connection density.\nWe also investigate the effects of pre-defining the sparsity of networks with\nonly fully connected layers. Based on our sparsifying technique, we introduce\nthe `scatter' metric to characterize the quality of a particular connection\npattern. As proof of concept, we show results on CIFAR, MNIST and a new dataset\non classifying Morse code symbols, which highlights some interesting trends and\nlimits of sparse connection patterns.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 19:26:21 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 02:45:55 GMT"}, {"version": "v3", "created": "Tue, 30 Jan 2018 00:33:48 GMT"}, {"version": "v4", "created": "Fri, 23 Feb 2018 03:31:44 GMT"}, {"version": "v5", "created": "Thu, 25 Apr 2019 23:42:23 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Dey", "Sourya", ""], ["Huang", "Kuan-Wen", ""], ["Beerel", "Peter A.", ""], ["Chugg", "Keith M.", ""]]}, {"id": "1711.02144", "submitter": "Suvam Patra", "authors": "Suvam Patra and Pranjal Maheshwari and Shashank Yadav and Chetan Arora\n  and Subhashis Banerjee", "title": "A Joint 3D-2D based Method for Free Space Detection on Roads", "comments": "Accepted for publication at IEEE WACV 2018", "journal-ref": null, "doi": "10.1109/WACV.2018.00076", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of road segmentation and free space\ndetection in the context of autonomous driving. Traditional methods either use\n3-dimensional (3D) cues such as point clouds obtained from LIDAR, RADAR or\nstereo cameras or 2-dimensional (2D) cues such as lane markings, road\nboundaries and object detection. Typical 3D point clouds do not have enough\nresolution to detect fine differences in heights such as between road and\npavement. Image based 2D cues fail when encountering uneven road textures such\nas due to shadows, potholes, lane markings or road restoration. We propose a\nnovel free road space detection technique combining both 2D and 3D cues. In\nparticular, we use CNN based road segmentation from 2D images and plane/box\nfitting on sparse depth data obtained from SLAM as priors to formulate an\nenergy minimization using conditional random field (CRF), for road pixels\nclassification. While the CNN learns the road texture and is unaffected by\ndepth boundaries, the 3D information helps in overcoming texture based\nclassification failures. Finally, we use the obtained road segmentation with\nthe 3D depth data from monocular SLAM to detect the free space for the\nnavigation purposes. Our experiments on KITTI odometry dataset, Camvid dataset,\nas well as videos captured by us, validate the superiority of the proposed\napproach over the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 20:04:20 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 09:50:59 GMT"}, {"version": "v3", "created": "Mon, 15 Jan 2018 20:22:10 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Patra", "Suvam", ""], ["Maheshwari", "Pranjal", ""], ["Yadav", "Shashank", ""], ["Arora", "Chetan", ""], ["Banerjee", "Subhashis", ""]]}, {"id": "1711.02217", "submitter": "Kumar Abhinav", "authors": "Kumar Abhinav, Jaideep Singh Chauhan and Debasis Sarkar", "title": "Image Segmentation of Multi-Shaped Overlapping Objects", "comments": "Accepted at VISAPP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new segmentation algorithm for images containing\nconvex objects present in multiple shapes with a high degree of overlap. The\nproposed algorithm is carried out in two steps, first we identify the visible\ncontours, segment them using concave points and finally group the segments\nbelonging to the same object. The next step is to assign a shape identity to\nthese grouped contour segments. For images containing objects in multiple\nshapes we begin first by identifying shape classes of the contours followed by\nassigning a shape entity to these classes. We provide a comprehensive\nexperimentation of our algorithm on two crystal image datasets. One dataset\ncomprises of images containing objects in multiple shapes overlapping each\nother and the other dataset contains standard images with objects present in a\nsingle shape. We test our algorithm against two baselines, with our proposed\nalgorithm outperforming both the baselines.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 23:14:46 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Abhinav", "Kumar", ""], ["Chauhan", "Jaideep Singh", ""], ["Sarkar", "Debasis", ""]]}, {"id": "1711.02231", "submitter": "Wang-Cheng Kang", "authors": "Wang-Cheng Kang, Chen Fang, Zhaowen Wang, Julian McAuley", "title": "Visually-Aware Fashion Recommendation and Design with Generative Image\n  Models", "comments": "10 pages, 6 figures. Accepted by ICDM'17 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building effective recommender systems for domains like fashion is\nchallenging due to the high level of subjectivity and the semantic complexity\nof the features involved (i.e., fashion styles). Recent work has shown that\napproaches to `visual' recommendation (e.g.~clothing, art, etc.) can be made\nmore accurate by incorporating visual signals directly into the recommendation\nobjective, using `off-the-shelf' feature representations derived from deep\nnetworks. Here, we seek to extend this contribution by showing that\nrecommendation performance can be significantly improved by learning `fashion\naware' image representations directly, i.e., by training the image\nrepresentation (from the pixel level) and the recommender system jointly; this\ncontribution is related to recent work using Siamese CNNs, though we are able\nto show improvements over state-of-the-art recommendation techniques such as\nBPR and variants that make use of pre-trained visual features. Furthermore, we\nshow that our model can be used \\emph{generatively}, i.e., given a user and a\nproduct category, we can generate new images (i.e., clothing items) that are\nmost consistent with their personal taste. This represents a first step towards\nbuilding systems that go beyond recommending existing items from a product\ncorpus, but which can be used to suggest styles and aid the design of new\nproducts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 00:17:51 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Kang", "Wang-Cheng", ""], ["Fang", "Chen", ""], ["Wang", "Zhaowen", ""], ["McAuley", "Julian", ""]]}, {"id": "1711.02245", "submitter": "Attila Szab\\'o", "authors": "Attila Szab\\'o, Qiyang Hu, Tiziano Portenier, Matthias Zwicker, Paolo\n  Favaro", "title": "Challenges in Disentangling Independent Factors of Variation", "comments": "Submitted to ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of building models that disentangle independent factors\nof variation. Such models could be used to encode features that can efficiently\nbe used for classification and to transfer attributes between different images\nin image synthesis. As data we use a weakly labeled training set. Our weak\nlabels indicate what single factor has changed between two data samples,\nalthough the relative value of the change is unknown. This labeling is of\nparticular interest as it may be readily available without annotation costs. To\nmake use of weak labels we introduce an autoencoder model and train it through\nconstraints on image pairs and triplets. We formally prove that without\nadditional knowledge there is no guarantee that two images with the same factor\nof variation will be mapped to the same feature. We call this issue the\nreference ambiguity. Moreover, we show the role of the feature dimensionality\nand adversarial training. We demonstrate experimentally that the proposed model\ncan successfully transfer attributes on several datasets, but show also cases\nwhen the reference ambiguity occurs.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 01:13:04 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Szab\u00f3", "Attila", ""], ["Hu", "Qiyang", ""], ["Portenier", "Tiziano", ""], ["Zwicker", "Matthias", ""], ["Favaro", "Paolo", ""]]}, {"id": "1711.02254", "submitter": "Jiajun Zhang", "authors": "Jiajun Zhang, Jinkun Tao, Jiangtao Huangfu and Zhiguo Shi", "title": "Doppler-Radar Based Hand Gesture Recognition System Using Convolutional\n  Neural Networks", "comments": "Best Paper Award of International Conference on Communications,\n  Signal Processing, and Systems 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand gesture recognition has long been a hot topic in human computer\ninteraction. Traditional camera-based hand gesture recognition systems cannot\nwork properly under dark circumstances. In this paper, a Doppler Radar based\nhand gesture recognition system using convolutional neural networks is\nproposed. A cost-effective Doppler radar sensor with dual receiving channels at\n5.8GHz is used to acquire a big database of four standard gestures. The\nreceived hand gesture signals are then processed with time-frequency analysis.\nConvolutional neural networks are used to classify different gestures.\nExperimental results verify the effectiveness of the system with an accuracy of\n98%. Besides, related factors such as recognition distance and gesture scale\nare investigated.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 01:58:11 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 10:13:21 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 11:53:47 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Zhang", "Jiajun", ""], ["Tao", "Jinkun", ""], ["Huangfu", "Jiangtao", ""], ["Shi", "Zhiguo", ""]]}, {"id": "1711.02257", "submitter": "Zhao Chen", "authors": "Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee and Andrew Rabinovich", "title": "GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep\n  Multitask Networks", "comments": "ICML 2018", "journal-ref": "Proceedings of the 35th International Conference on Machine\n  Learning (2018), 793-802", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep multitask networks, in which one neural network produces multiple\npredictive outputs, can offer better speed and performance than their\nsingle-task counterparts but are challenging to train properly. We present a\ngradient normalization (GradNorm) algorithm that automatically balances\ntraining in deep multitask models by dynamically tuning gradient magnitudes. We\nshow that for various network architectures, for both regression and\nclassification tasks, and on both synthetic and real datasets, GradNorm\nimproves accuracy and reduces overfitting across multiple tasks when compared\nto single-task networks, static baselines, and other adaptive multitask loss\nbalancing techniques. GradNorm also matches or surpasses the performance of\nexhaustive grid search methods, despite only involving a single asymmetry\nhyperparameter $\\alpha$. Thus, what was once a tedious search process that\nincurred exponentially more compute for each task added can now be accomplished\nwithin a few training runs, irrespective of the number of tasks. Ultimately, we\nwill demonstrate that gradient manipulation affords us great control over the\ntraining dynamics of multitask networks and may be one of the keys to unlocking\nthe potential of multitask learning.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 02:08:12 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 01:00:22 GMT"}, {"version": "v3", "created": "Sun, 8 Apr 2018 21:25:49 GMT"}, {"version": "v4", "created": "Tue, 12 Jun 2018 06:45:49 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Chen", "Zhao", ""], ["Badrinarayanan", "Vijay", ""], ["Lee", "Chen-Yu", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1711.02321", "submitter": "Jae-Seok Choi", "authors": "Jae-Seok Choi, Munchurl Kim", "title": "Single Image Super-Resolution Using Lightweight CNN with Maxout Units", "comments": "ACCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rectified linear units (ReLU) are well-known to be helpful in obtaining\nfaster convergence and thus higher performance for many deep-learning-based\napplications. However, networks with ReLU tend to perform poorly when the\nnumber of filter parameters is constrained to a small number. To overcome it,\nin this paper, we propose a novel network utilizing maxout units (MU), and show\nits effectiveness on super-resolution (SR) applications. In general, the MU has\nbeen known to make the filter sizes doubled in generating the feature maps of\nthe same sizes in classification problems. In this paper, we first reveal that\nthe MU can even make the filter sizes halved in restoration problems thus\nleading to compaction of the network sizes. To show this, our SR network is\ndesigned without increasing the filter sizes with MU, which outperforms the\nstate of the art SR methods with a smaller number of filter parameters. To the\nbest of our knowledge, we are the first to incorporate MU into SR applications\nand show promising performance results. In MU, feature maps from a previous\nconvolutional layer are divided into two parts along channels, which are then\ncompared element-wise and only their max values are passed to a next layer.\nAlong with some interesting properties of MU to be analyzed, we further\ninvestigate other variants of MU and their effects. In addition, while ReLU\nhave a trouble for learning in networks with a very small number of\nconvolutional filter parameters, MU do not. For SR applications, our MU-based\nnetwork reconstructs high-resolution images with comparable quality compared to\nprevious deep-learning-based SR methods, with lower filter parameters.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 07:37:06 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 02:30:29 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Choi", "Jae-Seok", ""], ["Kim", "Munchurl", ""]]}, {"id": "1711.02329", "submitter": "Reza Abbasi-Asl", "authors": "Reza Abbasi-Asl, Bin Yu", "title": "Interpreting Convolutional Neural Networks Through Compression", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) achieve state-of-the-art performance in\na wide variety of tasks in computer vision. However, interpreting CNNs still\nremains a challenge. This is mainly due to the large number of parameters in\nthese networks. Here, we investigate the role of compression and particularly\npruning filters in the interpretation of CNNs. We exploit our recently-proposed\ngreedy structural compression scheme that prunes filters in a trained CNN. In\nour compression, the filter importance index is defined as the classification\naccuracy reduction (CAR) of the network after pruning that filter. The filters\nare then iteratively pruned based on the CAR index. We demonstrate the\ninterpretability of CAR-compressed CNNs by showing that our algorithm prunes\nfilters with visually redundant pattern selectivity. Specifically, we show the\nimportance of shape-selective filters for object recognition, as opposed to\ncolor-selective filters. Out of top 20 CAR-pruned filters in AlexNet, 17 of\nthem in the first layer and 14 of them in the second layer are color-selective\nfilters. Finally, we introduce a variant of our CAR importance index that\nquantifies the importance of each image class to each CNN filter. We show that\nthe most and the least important class labels present a meaningful\ninterpretation of each filter that is consistent with the visualized pattern\nselectivity of that filter.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 08:10:52 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Abbasi-Asl", "Reza", ""], ["Yu", "Bin", ""]]}, {"id": "1711.02383", "submitter": "Viral Parekh", "authors": "Viral Parekh, Ramanathan Subramanian, Dipanjan Roy, and C.V. Jawahar", "title": "An EEG-based Image Annotation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning in computer vision has greatly increased the\nneed for annotated image datasets. We propose an EEG\n(Electroencephalogram)-based image annotation system. While humans can\nrecognize objects in 20-200 milliseconds, the need to manually label images\nresults in a low annotation throughput. Our system employs brain signals\ncaptured via a consumer EEG device to achieve an annotation rate of up to 10\nimages per second. We exploit the P300 event-related potential (ERP) signature\nto identify target images during a rapid serial visual presentation (RSVP)\ntask. We further perform unsupervised outlier removal to achieve an F1-score of\n0.88 on the test set. The proposed system does not depend on category-specific\nEEG signatures enabling the annotation of any new image category without any\nmodel pre-training.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 10:38:21 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Parekh", "Viral", ""], ["Subramanian", "Ramanathan", ""], ["Roy", "Dipanjan", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1711.02396", "submitter": "Mohit Jain", "authors": "Mohit Jain, Minesh Mathew and C.V. Jawahar", "title": "Unconstrained Scene Text and Video Text Recognition for Arabic Script", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building robust recognizers for Arabic has always been challenging. We\ndemonstrate the effectiveness of an end-to-end trainable CNN-RNN hybrid\narchitecture in recognizing Arabic text in videos and natural scenes. We\noutperform previous state-of-the-art on two publicly available video text\ndatasets - ALIF and ACTIV. For the scene text recognition task, we introduce a\nnew Arabic scene text dataset and establish baseline results. For scripts like\nArabic, a major challenge in developing robust recognizers is the lack of large\nquantity of annotated data. We overcome this by synthesising millions of Arabic\ntext images from a large vocabulary of Arabic words and phrases. Our\nimplementation is built on top of the model introduced here [37] which is\nproven quite effective for English scene text recognition. The model follows a\nsegmentation-free, sequence to sequence transcription approach. The network\ntranscribes a sequence of convolutional features from the input image to a\nsequence of target labels. This does away with the need for segmenting input\nimage into constituent characters/glyphs, which is often difficult for Arabic\nscript. Further, the ability of RNNs to model contextual dependencies yields\nsuperior recognition results.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 11:07:48 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Jain", "Mohit", ""], ["Mathew", "Minesh", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1711.02441", "submitter": "Serag Eldin Habib Prof.", "authors": "Al-Hussein A. El-Shafie and S. E. D. Habib", "title": "A Survey on Hardware Implementations of Visual Object Trackers", "comments": "17 pages, 14 Figures, 6 tables, 84 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object tracking is an active topic in the computer vision domain with\napplications extending over numerous fields. The main sub-tasks required to\nbuild an object tracker (e.g. object detection, feature extraction and object\ntracking) are computation-intensive. In addition, real-time operation of the\ntracker is indispensable for almost all of its applications. Therefore,\ncomplete hardware or hardware/software co-design approaches are pursued for\nbetter tracker implementations. This paper presents a literature survey of the\nhardware implementations of object trackers over the last two decades. Although\nseveral tracking surveys exist in literature, a survey addressing the hardware\nimplementations of the different trackers is missing. We believe this survey\nwould fill the gap and complete the picture with the existing surveys of how to\ndesign an efficient tracker and point out the future directions researchers can\nfollow in this field. We highlight the lack of hardware implementations for\nstate-of-the-art tracking algorithms as well as for enhanced classical\nalgorithms. We also stress the need for measuring the tracking performance of\nthe hardware-based trackers. Additionally, enough details of the hardware-based\ntrackers need to be provided to allow reasonable comparison between the\ndifferent implementations.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 12:45:12 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["El-Shafie", "Al-Hussein A.", ""], ["Habib", "S. E. D.", ""]]}, {"id": "1711.02488", "submitter": "Liang Shen", "authors": "Liang Shen, Zihan Yue, Fan Feng, Quan Chen, Shihao Liu, and Jie Ma", "title": "MSR-net:Low-light Image Enhancement Using Deep Convolutional Network", "comments": "9pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images captured in low-light conditions usually suffer from very low\ncontrast, which increases the difficulty of subsequent computer vision tasks in\na great extent. In this paper, a low-light image enhancement model based on\nconvolutional neural network and Retinex theory is proposed. Firstly, we show\nthat multi-scale Retinex is equivalent to a feedforward convolutional neural\nnetwork with different Gaussian convolution kernels. Motivated by this fact, we\nconsider a Convolutional Neural Network(MSR-net) that directly learns an\nend-to-end mapping between dark and bright images. Different fundamentally from\nexisting approaches, low-light image enhancement in this paper is regarded as a\nmachine learning problem. In this model, most of the parameters are optimized\nby back-propagation, while the parameters of traditional models depend on the\nartificial setting. Experiments on a number of challenging images reveal the\nadvantages of our method in comparison with other state-of-the-art methods from\nthe qualitative and quantitative perspective.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 14:32:25 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Shen", "Liang", ""], ["Yue", "Zihan", ""], ["Feng", "Fan", ""], ["Chen", "Quan", ""], ["Liu", "Shihao", ""], ["Ma", "Jie", ""]]}, {"id": "1711.02512", "submitter": "Filip Radenovi\\'c", "authors": "Filip Radenovi\\'c, Giorgos Tolias, Ond\\v{r}ej Chum", "title": "Fine-tuning CNN Image Retrieval with No Human Annotation", "comments": "TPAMI 2018. arXiv admin note: substantial text overlap with\n  arXiv:1604.02426", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image descriptors based on activations of Convolutional Neural Networks\n(CNNs) have become dominant in image retrieval due to their discriminative\npower, compactness of representation, and search efficiency. Training of CNNs,\neither from scratch or fine-tuning, requires a large amount of annotated data,\nwhere a high quality of annotation is often crucial. In this work, we propose\nto fine-tune CNNs for image retrieval on a large collection of unordered images\nin a fully automated manner. Reconstructed 3D models obtained by the\nstate-of-the-art retrieval and structure-from-motion methods guide the\nselection of the training data. We show that both hard-positive and\nhard-negative examples, selected by exploiting the geometry and the camera\npositions available from the 3D models, enhance the performance of\nparticular-object retrieval. CNN descriptor whitening discriminatively learned\nfrom the same training data outperforms commonly used PCA whitening. We propose\na novel trainable Generalized-Mean (GeM) pooling layer that generalizes max and\naverage pooling and show that it boosts retrieval performance. Applying the\nproposed method to the VGG network achieves state-of-the-art performance on the\nstandard benchmarks: Oxford Buildings, Paris, and Holidays datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 21:29:55 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 05:25:34 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Radenovi\u0107", "Filip", ""], ["Tolias", "Giorgos", ""], ["Chum", "Ond\u0159ej", ""]]}, {"id": "1711.02536", "submitter": "Saeid Motiian", "authors": "Saeid Motiian, Quinn Jones, Seyed Mehdi Iranmanesh, Gianfranco Doretto", "title": "Few-Shot Adversarial Domain Adaptation", "comments": "Accepted to NIPS 2017. arXiv admin note: text overlap with\n  arXiv:1709.10190", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides a framework for addressing the problem of supervised\ndomain adaptation with deep models. The main idea is to exploit adversarial\nlearning to learn an embedded subspace that simultaneously maximizes the\nconfusion between two domains while semantically aligning their embedding. The\nsupervised setting becomes attractive especially when there are only a few\ntarget data samples that need to be labeled. In this few-shot learning\nscenario, alignment and separation of semantic probability distributions is\ndifficult because of the lack of data. We found that by carefully designing a\ntraining scheme whereby the typical binary adversarial discriminator is\naugmented to distinguish between four different classes, it is possible to\neffectively address the supervised adaptation problem. In addition, the\napproach has a high speed of adaptation, i.e. it requires an extremely low\nnumber of labeled target training samples, even one per category can be\neffective. We then extensively compare this approach to the state of the art in\ndomain adaptation in two experiments: one using datasets for handwritten digit\nrecognition, and one using datasets for visual object recognition.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 17:08:46 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Motiian", "Saeid", ""], ["Jones", "Quinn", ""], ["Iranmanesh", "Seyed Mehdi", ""], ["Doretto", "Gianfranco", ""]]}, {"id": "1711.02549", "submitter": "Qingjie Liu", "authors": "Xiangyu Liu, Qingjie Liu, Yunhong Wang", "title": "Remote Sensing Image Fusion Based on Two-stream Fusion Network", "comments": "An extension of MMM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing image fusion (also known as pan-sharpening) aims at generating\nhigh resolution multi-spectral (MS) image from inputs of a high spatial\nresolution single band panchromatic (PAN) image and a low spatial resolution\nmulti-spectral image. Inspired by the astounding achievements of convolutional\nneural networks (CNNs) in a variety of computer vision tasks, in this paper, we\npropose a two-stream fusion network (TFNet) to address the problem of\npan-sharpening. Unlike previous CNN based methods that consider pan-sharpening\nas a super resolution problem and perform pan-sharpening in pixel level, the\nproposed TFNet aims to fuse PAN and MS images in feature level and reconstruct\nthe pan-sharpened image from the fused features. The TFNet mainly consists of\nthree parts. The first part is comprised of two networks extracting features\nfrom PAN and MS images, respectively. The subsequent network fuses them\ntogether to form compact features that represent both spatial and spectral\ninformation of PAN and MS images, simultaneously. Finally, the desired high\nspatial resolution MS image is recovered from the fused features through an\nimage reconstruction network. Experiments on Quickbird and \\mbox{GaoFen-1}\nsatellite images demonstrate that the proposed TFNet can fuse PAN and MS\nimages, effectively, and produce pan-sharpened images competitive with even\nsuperior to state of the arts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 15:33:29 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 05:21:33 GMT"}, {"version": "v3", "created": "Fri, 26 Jan 2018 09:44:19 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Liu", "Xiangyu", ""], ["Liu", "Qingjie", ""], ["Wang", "Yunhong", ""]]}, {"id": "1711.02578", "submitter": "Octavio Arriaga", "authors": "Octavio Arriaga, Paul Pl\\\"oger, Matias Valdenegro-Toro", "title": "Image Captioning and Classification of Dangerous Situations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current robot platforms are being employed to collaborate with humans in a\nwide range of domestic and industrial tasks. These environments require\nautonomous systems that are able to classify and communicate anomalous\nsituations such as fires, injured persons, car accidents; or generally, any\npotentially dangerous situation for humans. In this paper we introduce an\nanomaly detection dataset for the purpose of robot applications as well as the\ndesign and implementation of a deep learning architecture that classifies and\ndescribes dangerous situations using only a single image as input. We report a\nclassification accuracy of 97 % and METEOR score of 16.2. We will make the\ndataset publicly available after this paper is accepted.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 16:02:09 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Arriaga", "Octavio", ""], ["Pl\u00f6ger", "Paul", ""], ["Valdenegro-Toro", "Matias", ""]]}, {"id": "1711.02613", "submitter": "Elliot J. Crowley", "authors": "Elliot J. Crowley, Gavin Gray, Amos Storkey", "title": "Moonshine: Distilling with Cheap Convolutions", "comments": "32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many engineers wish to deploy modern neural networks in memory-limited\nsettings; but the development of flexible methods for reducing memory use is in\nits infancy, and there is little knowledge of the resulting cost-benefit. We\npropose structural model distillation for memory reduction using a strategy\nthat produces a student architecture that is a simple transformation of the\nteacher architecture: no redesign is needed, and the same hyperparameters can\nbe used. Using attention transfer, we provide Pareto curves/tables for\ndistillation of residual networks with four benchmark datasets, indicating the\nmemory versus accuracy payoff. We show that substantial memory savings are\npossible with very little loss of accuracy, and confirm that distillation\nprovides student network performance that is better than training that student\narchitecture directly on data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 17:21:06 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 11:43:02 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 16:47:40 GMT"}, {"version": "v4", "created": "Thu, 17 Jan 2019 12:26:19 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Crowley", "Elliot J.", ""], ["Gray", "Gavin", ""], ["Storkey", "Amos", ""]]}, {"id": "1711.02638", "submitter": "Jose M. Alvarez", "authors": "Jose M. Alvarez, Mathieu Salzmann", "title": "Compression-aware Training of Deep Networks", "comments": "Accepted at NIPS 2017", "journal-ref": null, "doi": null, "report-no": "Advances in Neural Information Processing Systems 30", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, great progress has been made in a variety of application\ndomains thanks to the development of increasingly deeper neural networks.\nUnfortunately, the huge number of units of these networks makes them expensive\nboth computationally and memory-wise. To overcome this, exploiting the fact\nthat deep networks are over-parametrized, several compression strategies have\nbeen proposed. These methods, however, typically start from a network that has\nbeen trained in a standard manner, without considering such a future\ncompression. In this paper, we propose to explicitly account for compression in\nthe training process. To this end, we introduce a regularizer that encourages\nthe parameter matrix of each layer to have low rank during training. We show\nthat accounting for compression during training allows us to learn much more\ncompact, yet at least as effective, models than state-of-the-art compression\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 17:58:34 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 18:10:42 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Alvarez", "Jose M.", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1711.02652", "submitter": "Artur Jordao", "authors": "Artur Jordao, Ricardo Kloss and William Robson Schwartz", "title": "Latent hypernet: Exploring all Layers from Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Convolutional Neural Networks (ConvNets) are able to simultaneously\nlearn features and classifiers to discriminate different categories of\nactivities, recent works have employed ConvNets approaches to perform human\nactivity recognition (HAR) based on wearable sensors, allowing the removal of\nexpensive human work and expert knowledge. However, these approaches have their\npower of discrimination limited mainly by the large number of parameters that\ncompose the network and the reduced number of samples available for training.\nInspired by this, we propose an accurate and robust approach, referred to as\nLatent HyperNet (LHN). The LHN uses feature maps from early layers (hyper) and\nprojects them, individually, onto a low dimensionality space (latent). Then,\nthese latent features are concatenated and presented to a classifier. To\ndemonstrate the robustness and accuracy of the LHN, we evaluate it using four\ndifferent networks architectures in five publicly available HAR datasets based\non wearable sensors, which vary in the sampling rate and number of activities.\nOur experiments demonstrate that the proposed LHN is able to produce rich\ninformation, improving the results regarding the original ConvNets.\nFurthermore, the method outperforms existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 18:32:40 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 14:57:28 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Jordao", "Artur", ""], ["Kloss", "Ricardo", ""], ["Schwartz", "William Robson", ""]]}, {"id": "1711.02718", "submitter": "Yuhang Lu", "authors": "Yuhang Lu, Jun Zhou, Jing Wang, Jun Chen, Karen Smith, Colin Wilder,\n  Song Wang", "title": "Curve-Structure Segmentation from Depth Maps: A CNN-based Approach and\n  Its Application to Exploring Cultural Heritage Objects", "comments": "Accepted by AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the important archaeological application of exploring cultural\nheritage objects, in this paper we study the challenging problem of\nautomatically segmenting curve structures that are very weakly stamped or\ncarved on an object surface in the form of a highly noisy depth map. Different\nfrom most classical low-level image segmentation methods that are known to be\nvery sensitive to the noise and occlusions, we propose a new supervised\nlearning algorithm based on Convolutional Neural Network (CNN) to implicitly\nlearn and utilize more curve geometry and pattern information for addressing\nthis challenging problem. More specifically, we first propose a Fully\nConvolutional Network (FCN) to estimate the skeleton of curve structures and at\neach skeleton pixel, a scale value is estimated to reflect the local curve\nwidth. Then we propose a dense prediction network to refine the estimated curve\nskeletons. Based on the estimated scale values, we finally develop an adaptive\nthresholding algorithm to achieve the final segmentation of curve structures.\nIn the experiment, we validate the performance of the proposed method on a\ndataset of depth images scanned from unearthed pottery sherds dating to the\nWoodland period of Southeastern North America.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 20:45:07 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 05:51:14 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Lu", "Yuhang", ""], ["Zhou", "Jun", ""], ["Wang", "Jing", ""], ["Chen", "Jun", ""], ["Smith", "Karen", ""], ["Wilder", "Colin", ""], ["Wang", "Song", ""]]}, {"id": "1711.02741", "submitter": "Kuan Fang", "authors": "Kuan Fang, Yu Xiang, Xiaocheng Li, Silvio Savarese", "title": "Recurrent Autoregressive Networks for Online Multi-Object Tracking", "comments": "10 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenge of online multi-object tracking is to reliably associate\nobject trajectories with detections in each video frame based on their tracking\nhistory. In this work, we propose the Recurrent Autoregressive Network (RAN), a\ntemporal generative modeling framework to characterize the appearance and\nmotion dynamics of multiple objects over time. The RAN couples an external\nmemory and an internal memory. The external memory explicitly stores previous\ninputs of each trajectory in a time window, while the internal memory learns to\nsummarize long-term tracking history and associate detections by processing the\nexternal memory. We conduct experiments on the MOT 2015 and 2016 datasets to\ndemonstrate the robustness of our tracking method in highly crowded and\noccluded scenes. Our method achieves top-ranked results on the two benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 21:51:22 GMT"}, {"version": "v2", "created": "Sun, 4 Mar 2018 04:21:03 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Fang", "Kuan", ""], ["Xiang", "Yu", ""], ["Li", "Xiaocheng", ""], ["Savarese", "Silvio", ""]]}, {"id": "1711.02809", "submitter": "Haiqing Ren", "authors": "Haiqing Ren and Weiqiang Wang", "title": "A New Hybrid-parameter Recurrent Neural Networks for Online Handwritten\n  Chinese Character Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recurrent neural network (RNN) is appropriate for dealing with temporal\nsequences. In this paper, we present a deep RNN with new features and apply it\nfor online handwritten Chinese character recognition. Compared with the\nexisting RNN models, three innovations are involved in the proposed system.\nFirst, a new hidden layer function for RNN is proposed for learning temporal\ninformation better. we call it Memory Pool Unit (MPU). The proposed MPU has a\nsimple architecture. Second, a new RNN architecture with hybrid parameter is\npresented, in order to increasing the expression capacity of RNN. The proposed\nhybrid-parameter RNN has parameter changes when calculating the iteration at\ntemporal dimension. Third, we make a adaptation that all the outputs of each\nlayer are stacked as the output of network. Stacked hidden layer states combine\nall the hidden layer states for increasing the expression capacity. Experiments\nare carried out on the IAHCC-UCAS2016 dataset and the CASIA-OLHWDB1.1 dataset.\nThe experimental results show that the hybrid-parameter RNN obtain a better\nrecognition performance with higher efficiency (fewer parameters and faster\nspeed). And the proposed Memory Pool Unit is proved to be a simple hidden layer\nfunction and obtains a competitive recognition results.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 03:04:41 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 03:03:44 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Ren", "Haiqing", ""], ["Wang", "Weiqiang", ""]]}, {"id": "1711.02816", "submitter": "Tianshui Chen", "authors": "Zhouxia Wang, Tianshui Chen, Guanbin Li, Ruijia Xu, Liang Lin", "title": "Multi-label Image Recognition by Recurrently Discovering Attentional\n  Regions", "comments": "Accepted at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel deep architecture to address multi-label image\nrecognition, a fundamental and practical task towards general visual\nunderstanding. Current solutions for this task usually rely on an extra step of\nextracting hypothesis regions (i.e., region proposals), resulting in redundant\ncomputation and sub-optimal performance. In this work, we achieve the\ninterpretable and contextualized multi-label image classification by developing\na recurrent memorized-attention module. This module consists of two alternately\nperformed components: i) a spatial transformer layer to locate attentional\nregions from the convolutional feature maps in a region-proposal-free way and\nii) an LSTM (Long-Short Term Memory) sub-network to sequentially predict\nsemantic labeling scores on the located regions while capturing the global\ndependencies of these regions. The LSTM also output the parameters for\ncomputing the spatial transformer. On large-scale benchmarks of multi-label\nimage classification (e.g., MS-COCO and PASCAL VOC 07), our approach\ndemonstrates superior performances over other existing state-of-the-arts in\nboth accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 03:43:51 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Wang", "Zhouxia", ""], ["Chen", "Tianshui", ""], ["Li", "Guanbin", ""], ["Xu", "Ruijia", ""], ["Lin", "Liang", ""]]}, {"id": "1711.02823", "submitter": "Xiao Zhou", "authors": "Xiao Zhou, Peilin Jiang and Fei Wang", "title": "Heuristic Search for Structural Constraints in Data Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research on multi-object tracking (MOT) is essentially to solve for the\ndata association assignment, the core of which is to design the association\ncost as discriminative as possible. Generally speaking, the match ambiguities\ncaused by similar appearances of objects and the moving cameras make the data\nassociation perplexing and challenging. In this paper, we propose a new\nheuristic method to search for structural constraints (HSSC) of multiple\ntargets when solving the problem of online multi-object tracking. We believe\nthat the internal structure among multiple targets in the adjacent frames could\nremain constant and stable even though the video sequences are captured by a\nmoving camera. As a result, the structural constraints are able to cut down the\nmatch ambiguities caused by the moving cameras as well as similar appearances\nof the tracked objects. The proposed heuristic method aims to obtain a maximum\nmatch set under the minimum structural cost for each available match pair,\nwhich can be integrated with the raw association costs and make them more\nelaborate and discriminative compared with other approaches. In addition, this\npaper presents a new method to recover missing targets by minimizing the cost\nfunction generated from both motion and structure cues. Our online multi-object\ntracking (MOT) algorithm based on HSSC has achieved the multi-object tracking\naccuracy (MOTA) of 25.0 on the public dataset 2DMOT2015[1].\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 04:26:35 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Zhou", "Xiao", ""], ["Jiang", "Peilin", ""], ["Wang", "Fei", ""]]}, {"id": "1711.02831", "submitter": "Biswarup Bhattacharya", "authors": "Arna Ghosh, Biswarup Bhattacharya, Somnath Basu Roy Chowdhury", "title": "SIMILARnet: Simultaneous Intelligent Localization and Recognition\n  Network", "comments": "5 pages; 2 figures; 2 tables; All authors have equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global Average Pooling (GAP) [4] has been used previously to generate class\nactivation for image classification tasks. The motivation behind SIMILARnet\ncomes from the fact that the convolutional filters possess position information\nof the essential features and hence, combination of the feature maps could help\nus locate the class instances in an image. We propose a biologically inspired\nmodel that is free of differential connections and doesn't require separate\ntraining thereby reducing computation overhead. Our novel architecture\ngenerates promising results and unlike existing methods, the model is not\nsensitive to the input image size, thus promising wider application. Codes for\nthe experiment and illustrations can be found at:\nhttps://github.com/brcsomnath/Advanced-GAP .\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 05:08:26 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Ghosh", "Arna", ""], ["Bhattacharya", "Biswarup", ""], ["Chowdhury", "Somnath Basu Roy", ""]]}, {"id": "1711.02837", "submitter": "Qi Yan", "authors": "Qi Yan, Zhaofei Yu, Feng Chen and Jian K. Liu", "title": "Revealing structure components of the retina by deep learning networks", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have demonstrated impressive\nperformance on visual object classification tasks. In addition, it is a useful\nmodel for predication of neuronal responses recorded in visual system. However,\nthere is still no clear understanding of what CNNs learn in terms of visual\nneuronal circuits. Visualizing CNN's features to obtain possible connections to\nneuronscience underpinnings is not easy due to highly complex circuits from the\nretina to higher visual cortex. Here we address this issue by focusing on\nsingle retinal ganglion cells with a simple model and electrophysiological\nrecordings from salamanders. By training CNNs with white noise images to\npredicate neural responses, we found that convolutional filters learned in the\nend are resembling to biological components of the retinal circuit. Features\nrepresented by these filters tile the space of conventional receptive field of\nretinal ganglion cells. These results suggest that CNN could be used to reveal\nstructure components of neuronal circuits.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 05:35:27 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Yan", "Qi", ""], ["Yu", "Zhaofei", ""], ["Chen", "Feng", ""], ["Liu", "Jian K.", ""]]}, {"id": "1711.02856", "submitter": "Hanjiang Lai", "authors": "Hanjiang Lai and Yan Pan", "title": "Transductive Zero-Shot Hashing via Coarse-to-Fine Similarity Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot Hashing (ZSH) is to learn hashing models for novel/target classes\nwithout training data, which is an important and challenging problem. Most\nexisting ZSH approaches exploit transfer learning via an intermediate shared\nsemantic representations between the seen/source classes and novel/target\nclasses. However, due to having disjoint, the hash functions learned from the\nsource dataset are biased when applied directly to the target classes. In this\npaper, we study the transductive ZSH, i.e., we have unlabeled data for novel\nclasses. We put forward a simple yet efficient joint learning approach via\ncoarse-to-fine similarity mining which transfers knowledges from source data to\ntarget data. It mainly consists of two building blocks in the proposed deep\narchitecture: 1) a shared two-streams network, which the first stream operates\non the source data and the second stream operates on the unlabeled data, to\nlearn the effective common image representations, and 2) a coarse-to-fine\nmodule, which begins with finding the most representative images from target\nclasses and then further detect similarities among these images, to transfer\nthe similarities of the source data to the target data in a greedy fashion.\nExtensive evaluation results on several benchmark datasets demonstrate that the\nproposed hashing method achieves significant improvement over the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 07:46:47 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Lai", "Hanjiang", ""], ["Pan", "Yan", ""]]}, {"id": "1711.02857", "submitter": "Jianqiao Wangni", "authors": "Jianqiao Wangni, Dahua Lin", "title": "Learning Sparse Visual Representations with Leaky Capped Norm\n  Regularizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity inducing regularization is an important part for learning\nover-complete visual representations. Despite the popularity of $\\ell_1$\nregularization, in this paper, we investigate the usage of non-convex\nregularizations in this problem. Our contribution consists of three parts.\nFirst, we propose the leaky capped norm regularization (LCNR), which allows\nmodel weights below a certain threshold to be regularized more strongly as\nopposed to those above, therefore imposes strong sparsity and only introduces\ncontrollable estimation bias. We propose a majorization-minimization algorithm\nto optimize the joint objective function. Second, our study over monocular 3D\nshape recovery and neural networks with LCNR outperforms $\\ell_1$ and other\nnon-convex regularizations, achieving state-of-the-art performance and faster\nconvergence. Third, we prove a theoretical global convergence speed on the 3D\nrecovery problem. To the best of our knowledge, this is the first convergence\nanalysis of the 3D recovery problem.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 07:54:41 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Wangni", "Jianqiao", ""], ["Lin", "Dahua", ""]]}, {"id": "1711.03082", "submitter": "Jugurta Montalv\\~ao", "authors": "Jugurta Montalv\\~ao, Luiz Miranda, J\\^anio Canuto", "title": "Offline signature authenticity verification through unambiguously\n  connected skeleton segments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for offline signature verification is presented in this paper. It is\nbased on the segmentation of the signature skeleton (through standard image\nskeletonization) into unambiguous sequences of points, or unambiguously\nconnected skeleton segments corresponding to vectorial representations of\nsignature portions. These segments are assumed to be the fundamental carriers\nof useful information for authenticity verification, and are compactly encoded\nas sets of 9 scalars (4 sampled coordinates and 1 length measure). Thus\nsignature authenticity is inferred through Euclidean distance based comparisons\nbetween pairs of such compact representations. The average performance of this\nmethod is evaluated through experiments with offline versions of signatures\nfrom the MCYT-100 database. For comparison purposes, three other approaches are\napplied to the same set of signatures, namely: (1) a straightforward approach\nbased on Dynamic Time Warping distances between segments, (2) a published\nmethod by [shanker2007], also based on DTW, and (3) the average human\nperformance under equivalent experimental protocol. Results suggest that if\nhuman performance is taken as a goal for automatic verification, then we should\ndiscard signature shape details to approach this goal. Moreover, our best\nresult -- close to human performance -- was obtained by the simplest strategy,\nwhere equal weights were given to segment shape and length.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 18:25:31 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Montalv\u00e3o", "Jugurta", ""], ["Miranda", "Luiz", ""], ["Canuto", "J\u00e2nio", ""]]}, {"id": "1711.03129", "submitter": "Jiajun Wu", "authors": "Jiajun Wu, Yifan Wang, Tianfan Xue, Xingyuan Sun, William T Freeman,\n  Joshua B Tenenbaum", "title": "MarrNet: 3D Shape Reconstruction via 2.5D Sketches", "comments": "NIPS 2017. The first two authors contributed equally to this paper.\n  Project page: http://marrnet.csail.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object reconstruction from a single image is a highly under-determined\nproblem, requiring strong prior knowledge of plausible 3D shapes. This\nintroduces challenges for learning-based approaches, as 3D object annotations\nare scarce in real images. Previous work chose to train on synthetic data with\nground truth 3D information, but suffered from domain adaptation when tested on\nreal data. In this work, we propose MarrNet, an end-to-end trainable model that\nsequentially estimates 2.5D sketches and 3D object shape. Our disentangled,\ntwo-step formulation has three advantages. First, compared to full 3D shape,\n2.5D sketches are much easier to be recovered from a 2D image; models that\nrecover 2.5D sketches are also more likely to transfer from synthetic to real\ndata. Second, for 3D reconstruction from 2.5D sketches, systems can learn\npurely from synthetic data. This is because we can easily render realistic 2.5D\nsketches without modeling object appearance variations in real images,\nincluding lighting, texture, etc. This further relieves the domain adaptation\nproblem. Third, we derive differentiable projective functions from 3D shape to\n2.5D sketches; the framework is therefore end-to-end trainable on real images,\nrequiring no human annotations. Our model achieves state-of-the-art performance\non 3D shape reconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 19:29:01 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Wu", "Jiajun", ""], ["Wang", "Yifan", ""], ["Xue", "Tianfan", ""], ["Sun", "Xingyuan", ""], ["Freeman", "William T", ""], ["Tenenbaum", "Joshua B", ""]]}, {"id": "1711.03172", "submitter": "Ehud Barnea", "authors": "Ehud Barnea, Ohad Ben-Shahar", "title": "Curve Reconstruction via the Global Statistics of Natural Curves", "comments": "CVPR version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing the missing parts of a curve has been the subject of much\ncomputational research, with applications in image inpainting, object\nsynthesis, etc. Different approaches for solving that problem are typically\nbased on processes that seek visually pleasing or perceptually plausible\ncompletions. In this work we focus on reconstructing the underlying physically\nlikely shape by utilizing the global statistics of natural curves. More\nspecifically, we develop a reconstruction model that seeks the mean physical\ncurve for a given inducer configuration. This simple model is both\nstraightforward to compute and it is receptive to diverse additional\ninformation, but it requires enough samples for all curve configurations, a\npractical requirement that limits its effective utilization. To address this\npractical issue we explore and exploit statistical geometrical properties of\nnatural curves, and in particular, we show that in many cases the mean curve is\nscale invariant and oftentimes it is extensible. This, in turn, allows to boost\nthe number of examples and thus the robustness of the statistics and its\napplicability. The reconstruction results are not only more physically\nplausible but they also lead to important insights on the reconstruction\nproblem, including an elegant explanation why certain inducer configurations\nare more likely to yield consistent perceptual completions than others.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 21:20:53 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 19:35:24 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 06:22:52 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Barnea", "Ehud", ""], ["Ben-Shahar", "Ohad", ""]]}, {"id": "1711.03179", "submitter": "Yun Gu", "authors": "Yang Hu, Yun Gu, Jie Yang and Guang-Zhong Yang", "title": "Multi-stage Suture Detection for Robot Assisted Anastomosis based on\n  Deep Learning", "comments": "Submitted to ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In robotic surgery, task automation and learning from demonstration combined\nwith human supervision is an emerging trend for many new surgical robot\nplatforms. One such task is automated anastomosis, which requires bimanual\nneedle handling and suture detection. Due to the complexity of the surgical\nenvironment and varying patient anatomies, reliable suture detection is\ndifficult, which is further complicated by occlusion and thread topologies. In\nthis paper, we propose a multi-stage framework for suture thread detection\nbased on deep learning. Fully convolutional neural networks are used to obtain\nthe initial detection and the overlapping status of suture thread, which are\nlater fused with the original image to learn a gradient road map of the thread.\nBased on the gradient road map, multiple segments of the thread are extracted\nand linked to form the whole thread using a curvilinear structure detector.\nExperiments on two different types of sutures demonstrate the accuracy of the\nproposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 21:44:14 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Hu", "Yang", ""], ["Gu", "Yun", ""], ["Yang", "Jie", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1711.03189", "submitter": "Weiyang Liu", "authors": "Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao,\n  Le Song", "title": "Deep Hyperspherical Learning", "comments": "NIPS 2017 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution as inner product has been the founding basis of convolutional\nneural networks (CNNs) and the key to end-to-end visual representation\nlearning. Benefiting from deeper architectures, recent CNNs have demonstrated\nincreasingly strong representation abilities. Despite such improvement, the\nincreased depth and larger parameter space have also led to challenges in\nproperly training a network. In light of such challenges, we propose\nhyperspherical convolution (SphereConv), a novel learning framework that gives\nangular representations on hyperspheres. We introduce SphereNet, deep\nhyperspherical convolution networks that are distinct from conventional inner\nproduct based convolutional networks. In particular, SphereNet adopts\nSphereConv as its basic convolution operator and is supervised by generalized\nangular softmax loss - a natural loss formulation under SphereConv. We show\nthat SphereNet can effectively encode discriminative representation and\nalleviate training difficulty, leading to easier optimization, faster\nconvergence and comparable (even better) classification accuracy over\nconvolutional counterparts. We also provide some theoretical insights for the\nadvantages of learning on hyperspheres. In addition, we introduce the learnable\nSphereConv, i.e., a natural improvement over prefixed SphereConv, and\nSphereNorm, i.e., hyperspherical learning as a normalization method.\nExperiments have verified our conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 22:21:21 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 15:15:19 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 18:18:04 GMT"}, {"version": "v4", "created": "Mon, 27 Nov 2017 20:48:17 GMT"}, {"version": "v5", "created": "Tue, 30 Jan 2018 16:00:14 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Liu", "Weiyang", ""], ["Zhang", "Yan-Ming", ""], ["Li", "Xingguo", ""], ["Yu", "Zhiding", ""], ["Dai", "Bo", ""], ["Zhao", "Tuo", ""], ["Song", "Le", ""]]}, {"id": "1711.03213", "submitter": "Judy Hoffman", "authors": "Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola,\n  Kate Saenko, Alexei A. Efros, Trevor Darrell", "title": "CyCADA: Cycle-Consistent Adversarial Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is critical for success in new, unseen environments.\nAdversarial adaptation models applied in feature spaces discover domain\ninvariant representations, but are difficult to visualize and sometimes fail to\ncapture pixel-level and low-level domain shifts. Recent work has shown that\ngenerative adversarial networks combined with cycle-consistency constraints are\nsurprisingly effective at mapping images between domains, even without the use\nof aligned image pairs. We propose a novel discriminatively-trained\nCycle-Consistent Adversarial Domain Adaptation model. CyCADA adapts\nrepresentations at both the pixel-level and feature-level, enforces\ncycle-consistency while leveraging a task loss, and does not require aligned\npairs. Our model can be applied in a variety of visual recognition and\nprediction settings. We show new state-of-the-art results across multiple\nadaptation tasks, including digit classification and semantic segmentation of\nroad scenes demonstrating transfer from synthetic to real world domains.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 23:54:52 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 02:52:43 GMT"}, {"version": "v3", "created": "Fri, 29 Dec 2017 05:00:37 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Hoffman", "Judy", ""], ["Tzeng", "Eric", ""], ["Park", "Taesung", ""], ["Zhu", "Jun-Yan", ""], ["Isola", "Phillip", ""], ["Saenko", "Kate", ""], ["Efros", "Alexei A.", ""], ["Darrell", "Trevor", ""]]}, {"id": "1711.03214", "submitter": "Filippo Santarelli", "authors": "Pierluigi Maponi, Riccardo Piergallini and Filippo Santarelli", "title": "Fingerprint Orientation Refinement through Iterative Smoothing", "comments": null, "journal-ref": "Signal & Image Processing: An International Journal, 8(5), 29-43,\n  2017", "doi": "10.5121/sipij.2017.8503", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new gradient-based method for the extraction of the orientation\nfield associated to a fingerprint, and a regularisation procedure to improve\nthe orientation field computed from noisy fingerprint images. The\nregularisation algorithm is based on three new integral operators, introduced\nand discussed in this paper. A pre-processing technique is also proposed to\nachieve better performances of the algorithm. The results of a numerical\nexperiment are reported to give an evidence of the efficiency of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 00:06:27 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Maponi", "Pierluigi", ""], ["Piergallini", "Riccardo", ""], ["Santarelli", "Filippo", ""]]}, {"id": "1711.03270", "submitter": "Xiaojie Jin Mr.", "authors": "Xiaojie Jin, Huaxin Xiao, Xiaohui Shen, Jimei Yang, Zhe Lin, Yunpeng\n  Chen, Zequn Jie, Jiashi Feng, Shuicheng Yan", "title": "Predicting Scene Parsing and Motion Dynamics in the Future", "comments": "To appear in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of predicting the future is important for intelligent systems,\ne.g. autonomous vehicles and robots to plan early and make decisions\naccordingly. Future scene parsing and optical flow estimation are two key tasks\nthat help agents better understand their environments as the former provides\ndense semantic information, i.e. what objects will be present and where they\nwill appear, while the latter provides dense motion information, i.e. how the\nobjects will move. In this paper, we propose a novel model to simultaneously\npredict scene parsing and optical flow in unobserved future video frames. To\nour best knowledge, this is the first attempt in jointly predicting scene\nparsing and motion dynamics. In particular, scene parsing enables structured\nmotion prediction by decomposing optical flow into different groups while\noptical flow estimation brings reliable pixel-wise correspondence to scene\nparsing. By exploiting this mutually beneficial relationship, our model shows\nsignificantly better parsing and motion prediction results when compared to\nwell-established baselines and individual prediction models on the large-scale\nCityscapes dataset. In addition, we also demonstrate that our model can be used\nto predict the steering angle of the vehicles, which further verifies the\nability of our model to learn latent representations of scene dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 06:34:07 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Jin", "Xiaojie", ""], ["Xiao", "Huaxin", ""], ["Shen", "Xiaohui", ""], ["Yang", "Jimei", ""], ["Lin", "Zhe", ""], ["Chen", "Yunpeng", ""], ["Jie", "Zequn", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1711.03273", "submitter": "Yuxin Peng", "authors": "Yuxin Peng, Yunzhen Zhao and Junchao Zhang", "title": "Two-stream Collaborative Learning with Spatial-Temporal Attention for\n  Video Classification", "comments": "14 pages, accepted by IEEE Transactions on Circuits and Systems for\n  Video Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video classification is highly important with wide applications, such as\nvideo search and intelligent surveillance. Video naturally consists of static\nand motion information, which can be represented by frame and optical flow.\nRecently, researchers generally adopt the deep networks to capture the static\nand motion information \\textbf{\\emph{separately}}, which mainly has two\nlimitations: (1) Ignoring the coexistence relationship between spatial and\ntemporal attention, while they should be jointly modelled as the spatial and\ntemporal evolutions of video, thus discriminative video features can be\nextracted.(2) Ignoring the strong complementarity between static and motion\ninformation coexisted in video, while they should be collaboratively learned to\nboost each other. For addressing the above two limitations, this paper proposes\nthe approach of two-stream collaborative learning with spatial-temporal\nattention (TCLSTA), which consists of two models: (1) Spatial-temporal\nattention model: The spatial-level attention emphasizes the salient regions in\nframe, and the temporal-level attention exploits the discriminative frames in\nvideo. They are jointly learned and mutually boosted to learn the\ndiscriminative static and motion features for better classification\nperformance. (2) Static-motion collaborative model: It not only achieves mutual\nguidance on static and motion information to boost the feature learning, but\nalso adaptively learns the fusion weights of static and motion streams, so as\nto exploit the strong complementarity between static and motion information to\npromote video classification. Experiments on 4 widely-used datasets show that\nour TCLSTA approach achieves the best performance compared with more than 10\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 06:49:21 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Peng", "Yuxin", ""], ["Zhao", "Yunzhen", ""], ["Zhang", "Junchao", ""]]}, {"id": "1711.03278", "submitter": "Pushparaja Murugan", "authors": "Pushparaja Murugan", "title": "Feed Forward and Backward Run in Deep Convolution Neural Network", "comments": "20 pages, 20th International Conference on Computer Vision and Image\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolution Neural Networks (CNN), known as ConvNets are widely used in many\nvisual imagery application, object classification, speech recognition. After\nthe implementation and demonstration of the deep convolution neural network in\nImagenet classification in 2012 by krizhevsky, the architecture of deep\nConvolution Neural Network is attracted many researchers. This has led to the\nmajor development in Deep learning frameworks such as Tensorflow, caffe, keras,\ntheno. Though the implementation of deep learning is quite possible by\nemploying deep learning frameworks, mathematical theory and concepts are harder\nto understand for new learners and practitioners. This article is intended to\nprovide an overview of ConvNets architecture and to explain the mathematical\ntheory behind it including activation function, loss function, feedforward and\nbackward propagation. In this article, grey scale image is taken as input\ninformation image, ReLU and Sigmoid activation function are considered for\ndeveloping the architecture and cross-entropy loss function are used for\ncomputing the difference between predicted value and actual value. The\narchitecture is developed in such a way that it can contain one convolution\nlayer, one pooling layer, and multiple dense layers\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 07:32:30 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Murugan", "Pushparaja", ""]]}, {"id": "1711.03306", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl, Thiago Santini, Enkelejda Kasneci", "title": "Fast camera focus estimation for gaze-based focus control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many cameras implement auto-focus functionality. However, they typically\nrequire the user to manually identify the location to be focused on. While such\nan approach works for temporally-sparse autofocusing functionality (e.g., photo\nshooting), it presents extreme usability problems when the focus must be\nquickly switched between multiple areas (and depths) of interest - e.g., in a\ngaze-based autofocus approach. This work introduces a novel, real-time\nauto-focus approach based on eye-tracking, which enables the user to shift the\ncamera focus plane swiftly based solely on the gaze information. Moreover, the\nproposed approach builds a graph representation of the image to estimate depth\nplane surfaces and runs in real time (requiring ~20ms on a single i5 core),\nthus allowing for the depth map estimation to be performed dynamically. We\nevaluated our algorithm for gaze-based depth estimation against\nstate-of-the-art approaches based on eight new data sets with flat, skewed, and\nround surfaces, as well as publicly available datasets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 09:47:37 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Santini", "Thiago", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "1711.03345", "submitter": "Weilin Fu", "authors": "Weilin Fu, Katharina Breininger, Tobias W\\\"urfl, Nishant Ravikumar,\n  Roman Schaffert, Andreas Maier", "title": "Frangi-Net: A Neural Network Approach to Vessel Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we reformulate the conventional 2-D Frangi vesselness measure\ninto a pre-weighted neural network (\"Frangi-Net\"), and illustrate that the\nFrangi-Net is equivalent to the original Frangi filter. Furthermore, we show\nthat, as a neural network, Frangi-Net is trainable. We evaluate the proposed\nmethod on a set of 45 high resolution fundus images. After fine-tuning, we\nobserve both qualitative and quantitative improvements in the segmentation\nquality compared to the original Frangi measure, with an increase up to $17\\%$\nin F1 score.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 12:15:33 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Fu", "Weilin", ""], ["Breininger", "Katharina", ""], ["W\u00fcrfl", "Tobias", ""], ["Ravikumar", "Nishant", ""], ["Schaffert", "Roman", ""], ["Maier", "Andreas", ""]]}, {"id": "1711.03357", "submitter": "Andrew Hallam", "authors": "Andrew Hallam, Edward Grant, Vid Stojevic, Simone Severini, Andrew G.\n  Green", "title": "Compact Neural Networks based on the Multiscale Entanglement\n  Renormalization Ansatz", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates a method for tensorizing neural networks based upon\nan efficient way of approximating scale invariant quantum states, the\nMulti-scale Entanglement Renormalization Ansatz (MERA). We employ MERA as a\nreplacement for the fully connected layers in a convolutional neural network\nand test this implementation on the CIFAR-10 and CIFAR-100 datasets. The\nproposed method outperforms factorization using tensor trains, providing\ngreater compression for the same level of accuracy and greater accuracy for the\nsame level of compression. We demonstrate MERA layers with 14000 times fewer\nparameters and a reduction in accuracy of less than 1% compared to the\nequivalent fully connected layers, scaling like O(N).\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 12:55:59 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 17:25:21 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 23:55:50 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Hallam", "Andrew", ""], ["Grant", "Edward", ""], ["Stojevic", "Vid", ""], ["Severini", "Simone", ""], ["Green", "Andrew G.", ""]]}, {"id": "1711.03368", "submitter": "Wei-Hong Li", "authors": "Wei-Hong Li, Zhuowei Zhong and Wei-Shi Zheng", "title": "One-pass Person Re-identification by Sketch Online Discriminant Analysis", "comments": "Online learning, Person re-identification, Discriminant feature\n  extraction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id) is to match people across disjoint camera\nviews in a multi-camera system, and re-id has been an important technology\napplied in smart city in recent years. However, the majority of existing person\nre-id methods are not designed for processing sequential data in an online way.\nThis ignores the real-world scenario that person images detected from\nmulti-cameras system are coming sequentially. While there is a few work on\ndiscussing online re-id, most of them require considerable storage of all\npassed data samples that have been ever observed, and this could be unrealistic\nfor processing data from a large camera network. In this work, we present an\nonepass person re-id model that adapts the re-id model based on each newly\nobserved data and no passed data are directly used for each update. More\nspecifically, we develop an Sketch online Discriminant Analysis (SoDA) by\nembedding sketch processing into Fisher discriminant analysis (FDA). SoDA can\nefficiently keep the main data variations of all passed samples in a low rank\nmatrix when processing sequential data samples, and estimate the approximate\nwithin-class variance (i.e. within-class covariance matrix) from the sketch\ndata information. We provide theoretical analysis on the effect of the\nestimated approximate within-class covariance matrix. In particular, we derive\nupper and lower bounds on the Fisher discriminant score (i.e. the quotient\nbetween between-class variation and within-class variation after feature\ntransformation) in order to investigate how the optimal feature transformation\nlearned by SoDA sequentially approximates the offline FDA that is learned on\nall observed data. Extensive experimental results have shown the effectiveness\nof our SoDA and empirically support our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 13:29:33 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Li", "Wei-Hong", ""], ["Zhong", "Zhuowei", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1711.03473", "submitter": "Michel Melo Silva", "authors": "Michel Melo Silva, Washington Luis Souza Ramos, Felipe Cadar Chamone,\n  Jo\\~ao Pedro Klock Ferreira, Mario Fernando Montenegro Campos, Erickson\n  Rangel Nascimento", "title": "Making a long story short: A Multi-Importance fast-forwarding egocentric\n  videos with the emphasis on relevant objects", "comments": "Accepted to publication in the Journal of Visual Communication and\n  Image Representation (JVCI) 2018. Project website:\n  https://www.verlab.dcc.ufmg.br/semantic-hyperlapse", "journal-ref": null, "doi": "10.1016/j.jvcir.2018.02.013", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of low-cost high-quality personal wearable cameras combined\nwith the increasing storage capacity of video-sharing websites have evoked a\ngrowing interest in first-person videos, since most videos are composed of\nlong-running unedited streams which are usually tedious and unpleasant to\nwatch. State-of-the-art semantic fast-forward methods currently face the\nchallenge of providing an adequate balance between smoothness in visual flow\nand the emphasis on the relevant parts. In this work, we present the\nMulti-Importance Fast-Forward (MIFF), a fully automatic methodology to\nfast-forward egocentric videos facing these challenges. The dilemma of defining\nwhat is the semantic information of a video is addressed by a learning process\nbased on the preferences of the user. Results show that the proposed method\nkeeps over $3$ times more semantic content than the state-of-the-art\nfast-forward. Finally, we discuss the need of a particular video stabilization\ntechnique for fast-forward egocentric videos.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 17:03:29 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 15:56:26 GMT"}, {"version": "v3", "created": "Wed, 7 Mar 2018 17:59:11 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Silva", "Michel Melo", ""], ["Ramos", "Washington Luis Souza", ""], ["Chamone", "Felipe Cadar", ""], ["Ferreira", "Jo\u00e3o Pedro Klock", ""], ["Campos", "Mario Fernando Montenegro", ""], ["Nascimento", "Erickson Rangel", ""]]}, {"id": "1711.03483", "submitter": "Eloi Zablocki", "authors": "\\'Eloi Zablocki, Benjamin Piwowarski, Laure Soulier, Patrick Gallinari", "title": "Learning Multi-Modal Word Representation Grounded in Visual Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing the semantics of words is a long-standing problem for the\nnatural language processing community. Most methods compute word semantics\ngiven their textual context in large corpora. More recently, researchers\nattempted to integrate perceptual and visual features. Most of these works\nconsider the visual appearance of objects to enhance word representations but\nthey ignore the visual environment and context in which objects appear. We\npropose to unify text-based techniques with vision-based techniques by\nsimultaneously leveraging textual and visual context to learn multimodal word\nembeddings. We explore various choices for what can serve as a visual context\nand present an end-to-end method to integrate visual context elements in a\nmultimodal skip-gram model. We provide experiments and extensive analysis of\nthe obtained results.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 17:28:07 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Zablocki", "\u00c9loi", ""], ["Piwowarski", "Benjamin", ""], ["Soulier", "Laure", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1711.03527", "submitter": "M. Salman Asif", "authors": "M. Salman Asif", "title": "Toward Depth Estimation Using Mask-Based Lensless Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, coded masks have been used to demonstrate a thin form-factor\nlensless camera, FlatCam, in which a mask is placed immediately on top of a\nbare image sensor. In this paper, we present an imaging model and algorithm to\njointly estimate depth and intensity information in the scene from a single or\nmultiple FlatCams. We use a light field representation to model the mapping of\n3D scene onto the sensor in which light rays from different depths yield\ndifferent modulation patterns. We present a greedy depth pursuit algorithm to\nsearch the 3D volume and estimate the depth and intensity of each pixel within\nthe camera field-of-view. We present simulation results to analyze the\nperformance of our proposed model and algorithm with different FlatCam\nsettings.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 18:54:43 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Asif", "M. Salman", ""]]}, {"id": "1711.03536", "submitter": "Ahmed Elgammal", "authors": "Ahmed Elgammal, Yan Kang, Milko Den Leeuw", "title": "Picasso, Matisse, or a Fake? Automated Analysis of Drawings at the\n  Stroke Level for Attribution and Authentication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a computational approach for analysis of strokes in line\ndrawings by artists. We aim at developing an AI methodology that facilitates\nattribution of drawings of unknown authors in a way that is not easy to be\ndeceived by forged art. The methodology used is based on quantifying the\ncharacteristics of individual strokes in drawings. We propose a novel algorithm\nfor segmenting individual strokes. We designed and compared different\nhand-crafted and learned features for the task of quantifying stroke\ncharacteristics. We also propose and compare different classification methods\nat the drawing level. We experimented with a dataset of 300 digitized drawings\nwith over 80 thousands strokes. The collection mainly consisted of drawings of\nPablo Picasso, Henry Matisse, and Egon Schiele, besides a small number of\nrepresentative works of other artists. The experiments shows that the proposed\nmethodology can classify individual strokes with accuracy 70%-90%, and\naggregate over drawings with accuracy above 80%, while being robust to be\ndeceived by fakes (with accuracy 100% for detecting fakes in most settings).\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 20:26:40 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Elgammal", "Ahmed", ""], ["Kang", "Yan", ""], ["Leeuw", "Milko Den", ""]]}, {"id": "1711.03564", "submitter": "Keiller Nogueira", "authors": "Keiller Nogueira, Samuel G. Fadel, \\'Icaro C. Dourado, Rafael de O.\n  Werneck, Javier A. V. Mu\\~noz, Ot\\'avio A. B. Penatti, Rodrigo T. Calumby,\n  Lin Tzy Li, Jefersson A. dos Santos, Ricardo da S. Torres", "title": "Exploiting ConvNet Diversity for Flooding Identification", "comments": "Work winner of the Flood-Detection in Satellite Images, a subtask of\n  2017 Multimedia Satellite Task (MediaEval Benchmark) Accepted for publication\n  in the Geoscience and Remote Sensing Letters (GRSL)", "journal-ref": null, "doi": "10.1109/LGRS.2018.2845549", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flooding is the world's most costly type of natural disaster in terms of both\neconomic losses and human causalities. A first and essential procedure towards\nflood monitoring is based on identifying the area most vulnerable to flooding,\nwhich gives authorities relevant regions to focus. In this work, we propose\nseveral methods to perform flooding identification in high-resolution remote\nsensing images using deep learning. Specifically, some proposed techniques are\nbased upon unique networks, such as dilated and deconvolutional ones, while\nother was conceived to exploit diversity of distinct networks in order to\nextract the maximum performance of each classifier. Evaluation of the proposed\nalgorithms were conducted in a high-resolution remote sensing dataset. Results\nshow that the proposed algorithms outperformed several state-of-the-art\nbaselines, providing improvements ranging from 1 to 4% in terms of the Jaccard\nIndex.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 19:19:00 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 14:05:42 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Nogueira", "Keiller", ""], ["Fadel", "Samuel G.", ""], ["Dourado", "\u00cdcaro C.", ""], ["Werneck", "Rafael de O.", ""], ["Mu\u00f1oz", "Javier A. V.", ""], ["Penatti", "Ot\u00e1vio A. B.", ""], ["Calumby", "Rodrigo T.", ""], ["Li", "Lin Tzy", ""], ["Santos", "Jefersson A. dos", ""], ["Torres", "Ricardo da S.", ""]]}, {"id": "1711.03654", "submitter": "Anthony Perez", "authors": "Anthony Perez, Christopher Yeh, George Azzari, Marshall Burke, David\n  Lobell, Stefano Ermon", "title": "Poverty Prediction with Public Landsat 7 Satellite Imagery and Machine\n  Learning", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining detailed and reliable data about local economic livelihoods in\ndeveloping countries is expensive, and data are consequently scarce. Previous\nwork has shown that it is possible to measure local-level economic livelihoods\nusing high-resolution satellite imagery. However, such imagery is relatively\nexpensive to acquire, often not updated frequently, and is mainly available for\nrecent years. We train CNN models on free and publicly available multispectral\ndaytime satellite images of the African continent from the Landsat 7 satellite,\nwhich has collected imagery with global coverage for almost two decades. We\nshow that despite these images' lower resolution, we can achieve accuracies\nthat exceed previous benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 00:21:54 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Perez", "Anthony", ""], ["Yeh", "Christopher", ""], ["Azzari", "George", ""], ["Burke", "Marshall", ""], ["Lobell", "David", ""], ["Ermon", "Stefano", ""]]}, {"id": "1711.03665", "submitter": "Zhenheng Yang", "authors": "Zhenheng Yang, Peng Wang, Wei Xu, Liang Zhao, Ramakant Nevatia", "title": "Unsupervised Learning of Geometry with Edge-aware Depth-Normal\n  Consistency", "comments": "Accepted at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to reconstruct depths in a single image by watching unlabeled videos\nvia deep convolutional network (DCN) is attracting significant attention in\nrecent years. In this paper, we introduce a surface normal representation for\nunsupervised depth estimation framework. Our estimated depths are constrained\nto be compatible with predicted normals, yielding more robust geometry results.\nSpecifically, we formulate an edge-aware depth-normal consistency term, and\nsolve it by constructing a depth-to-normal layer and a normal-to-depth layer\ninside of the DCN. The depth-to-normal layer takes estimated depths as input,\nand computes normal directions using cross production based on neighboring\npixels. Then given the estimated normals, the normal-to-depth layer outputs a\nregularized depth map through local planar smoothness. Both layers are computed\nwith awareness of edges inside the image to help address the issue of\ndepth/normal discontinuity and preserve sharp edges. Finally, to train the\nnetwork, we apply the photometric error and gradient smoothness for both depth\nand normal predictions. We conducted experiments on both outdoor (KITTI) and\nindoor (NYUv2) datasets, and show that our algorithm vastly outperforms state\nof the art, which demonstrates the benefits from our approach.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 01:39:29 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Yang", "Zhenheng", ""], ["Wang", "Peng", ""], ["Xu", "Wei", ""], ["Zhao", "Liang", ""], ["Nevatia", "Ramakant", ""]]}, {"id": "1711.03674", "submitter": "Krzysztof J. Geras", "authors": "Nan Wu, Krzysztof J. Geras, Yiqiu Shen, Jingyi Su, S. Gene Kim, Eric\n  Kim, Stacey Wolfson, Linda Moy, Kyunghyun Cho", "title": "Breast density classification with deep convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast density classification is an essential part of breast cancer\nscreening. Although a lot of prior work considered this problem as a task for\nlearning algorithms, to our knowledge, all of them used small and not\nclinically realistic data both for training and evaluation of their models. In\nthis work, we explore the limits of this task with a data set coming from over\n200,000 breast cancer screening exams. We use this data to train and evaluate a\nstrong convolutional neural network classifier. In a reader study, we find that\nour model can perform this task comparably to a human expert.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 02:50:46 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Wu", "Nan", ""], ["Geras", "Krzysztof J.", ""], ["Shen", "Yiqiu", ""], ["Su", "Jingyi", ""], ["Kim", "S. Gene", ""], ["Kim", "Eric", ""], ["Wolfson", "Stacey", ""], ["Moy", "Linda", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1711.03677", "submitter": "Shao Huang", "authors": "Shao Huang, Weiqiang Wang, Shengfeng He, Rynson W.H. Lau", "title": "Egocentric Hand Detection Via Dynamic Region Growing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric videos, which mainly record the activities carried out by the\nusers of the wearable cameras, have drawn much research attentions in recent\nyears. Due to its lengthy content, a large number of ego-related applications\nhave been developed to abstract the captured videos. As the users are\naccustomed to interacting with the target objects using their own hands while\ntheir hands usually appear within their visual fields during the interaction,\nan egocentric hand detection step is involved in tasks like gesture\nrecognition, action recognition and social interaction understanding. In this\nwork, we propose a dynamic region growing approach for hand region detection in\negocentric videos, by jointly considering hand-related motion and egocentric\ncues. We first determine seed regions that most likely belong to the hand, by\nanalyzing the motion patterns across successive frames. The hand regions can\nthen be located by extending from the seed regions, according to the scores\ncomputed for the adjacent superpixels. These scores are derived from four\negocentric cues: contrast, location, position consistency and appearance\ncontinuity. We discuss how to apply the proposed method in real-life scenarios,\nwhere multiple hands irregularly appear and disappear from the videos.\nExperimental results on public datasets show that the proposed method achieves\nsuperior performance compared with the state-of-the-art methods, especially in\ncomplicated scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 03:23:55 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Huang", "Shao", ""], ["Wang", "Weiqiang", ""], ["He", "Shengfeng", ""], ["Lau", "Rynson W. H.", ""]]}, {"id": "1711.03678", "submitter": "Michael Janner", "authors": "Michael Janner, Jiajun Wu, Tejas D. Kulkarni, Ilker Yildirim, Joshua\n  B. Tenenbaum", "title": "Self-Supervised Intrinsic Image Decomposition", "comments": "NIPS 2017 camera-ready version, project page:\n  http://rin.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsic decomposition from a single image is a highly challenging task, due\nto its inherent ambiguity and the scarcity of training data. In contrast to\ntraditional fully supervised learning approaches, in this paper we propose\nlearning intrinsic image decomposition by explaining the input image. Our\nmodel, the Rendered Intrinsics Network (RIN), joins together an image\ndecomposition pipeline, which predicts reflectance, shape, and lighting\nconditions given a single image, with a recombination function, a learned\nshading model used to recompose the original input based off of intrinsic image\npredictions. Our network can then use unsupervised reconstruction error as an\nadditional signal to improve its intermediate representations. This allows\nlarge-scale unlabeled data to be useful during training, and also enables\ntransferring learned knowledge to images of unseen object categories, lighting\nconditions, and shapes. Extensive experiments demonstrate that our method\nperforms well on both intrinsic image decomposition and knowledge transfer.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 03:31:27 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 22:52:18 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Janner", "Michael", ""], ["Wu", "Jiajun", ""], ["Kulkarni", "Tejas D.", ""], ["Yildirim", "Ilker", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1711.03694", "submitter": "Junting Zhang", "authors": "Junting Zhang, Chen Liang, C.-C. Jay Kuo", "title": "A Fully Convolutional Tri-branch Network (FCTN) for Domain Adaptation", "comments": "Accepted by ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A domain adaptation method for urban scene segmentation is proposed in this\nwork. We develop a fully convolutional tri-branch network, where two branches\nassign pseudo labels to images in the unlabeled target domain while the third\nbranch is trained with supervision based on images in the pseudo-labeled target\ndomain. The re-labeling and re-training processes alternate. With this design,\nthe tri-branch network learns target-specific discriminative representations\nprogressively and, as a result, the cross-domain capability of the segmenter\nimproves. We evaluate the proposed network on large-scale domain adaptation\nexperiments using both synthetic (GTA) and real (Cityscapes) images. It is\nshown that our solution achieves the state-of-the-art performance and it\noutperforms previous methods by a significant margin.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 05:01:28 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 01:43:35 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Zhang", "Junting", ""], ["Liang", "Chen", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1711.03726", "submitter": "Prakhar Gupta", "authors": "Prakhar Gupta, Shubh Gupta, Ajaykrishnan Jayagopal, Sourav Pal, Ritwik\n  Sinha", "title": "Saliency Prediction for Mobile User Interfaces", "comments": "Paper accepted at WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce models for saliency prediction for mobile user interfaces. A\nmobile interface may include elements like buttons, text, etc. in addition to\nnatural images which enable performing a variety of tasks. Saliency in natural\nimages is a well studied area. However, given the difference in what\nconstitutes a mobile interface, and the usage context of these devices, we\npostulate that saliency prediction for mobile interface images requires a fresh\napproach. Mobile interface design involves operating on elements, the building\nblocks of the interface. We first collected eye-gaze data from mobile devices\nfor free viewing task. Using this data, we develop a novel autoencoder based\nmulti-scale deep learning model that provides saliency prediction at the mobile\ninterface element level. Compared to saliency prediction approaches developed\nfor natural images, we show that our approach performs significantly better on\na range of established metrics.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 08:38:44 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 01:58:19 GMT"}, {"version": "v3", "created": "Tue, 28 Nov 2017 05:26:59 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Gupta", "Prakhar", ""], ["Gupta", "Shubh", ""], ["Jayagopal", "Ajaykrishnan", ""], ["Pal", "Sourav", ""], ["Sinha", "Ritwik", ""]]}, {"id": "1711.03800", "submitter": "Arun Balajee Vasudevan", "authors": "Arun Balajee Vasudevan, Dengxin Dai, Luc Van Gool", "title": "Object Referring in Visual Scene with Spoken Language", "comments": "10 pages, Submitted to WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object referring has important applications, especially for human-machine\ninteraction. While having received great attention, the task is mainly attacked\nwith written language (text) as input rather than spoken language (speech),\nwhich is more natural. This paper investigates Object Referring with Spoken\nLanguage (ORSpoken) by presenting two datasets and one novel approach. Objects\nare annotated with their locations in images, text descriptions and speech\ndescriptions. This makes the datasets ideal for multi-modality learning. The\napproach is developed by carefully taking down ORSpoken problem into three\nsub-problems and introducing task-specific vision-language interactions at the\ncorresponding levels. Experiments show that our method outperforms competing\nmethods consistently and significantly. The approach is also evaluated in the\npresence of audio noise, showing the efficacy of the proposed vision-language\ninteraction methods in counteracting background noise.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 13:04:55 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 15:12:24 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Vasudevan", "Arun Balajee", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1711.03810", "submitter": "Shan Luo Dr", "authors": "Shan Luo, Joao Bimbo, Ravinder Dahiya and Hongbin Liu", "title": "Robotic Tactile Perception of Object Properties: A Review", "comments": "17 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Touch sensing can help robots understand their sur- rounding environment, and\nin particular the objects they interact with. To this end, roboticists have, in\nthe last few decades, developed several tactile sensing solutions, extensively\nreported in the literature. Research into interpreting the conveyed tactile\ninformation has also started to attract increasing attention in recent years.\nHowever, a comprehensive study on this topic is yet to be reported. In an\neffort to collect and summarize the major scientific achievements in the area,\nthis survey extensively reviews current trends in robot tactile perception of\nobject properties. Available tactile sensing technologies are briefly presented\nbefore an extensive review on tactile recognition of object properties. The\nobject properties that are targeted by this review are shape, surface material\nand object pose. The role of touch sensing in combination with other sensing\nsources is also discussed. In this review, open issues are identified and\nfuture directions for applying tactile sensing in different tasks are\nsuggested.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 13:26:08 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Luo", "Shan", ""], ["Bimbo", "Joao", ""], ["Dahiya", "Ravinder", ""], ["Liu", "Hongbin", ""]]}, {"id": "1711.03874", "submitter": "Anca Sticlaru", "authors": "Grigorios Kalliatakis, Anca Sticlaru, George Stamatiadis, Shoaib\n  Ehsan, Ales Leonardis, Juergen Gall and Klaus D. McDonald-Maier", "title": "Material Classification in the Wild: Do Synthesized Training Data\n  Generalise Better than Real-World Training Data?", "comments": "accepted for publication in VISAPP 2018. arXiv admin note: text\n  overlap with arXiv:1703.04101", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We question the dominant role of real-world training images in the field of\nmaterial classification by investigating whether synthesized data can\ngeneralise more effectively than real-world data. Experimental results on three\nchallenging real-world material databases show that the best performing\npre-trained convolutional neural network (CNN) architectures can achieve up to\n91.03% mean average precision when classifying materials in cross-dataset\nscenarios. We demonstrate that synthesized data achieve an improvement on mean\naverage precision when used as training data and in conjunction with\npre-trained CNN architectures, which spans from ~ 5% to ~ 19% across three\nwidely used material databases of real-world images.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 15:48:26 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Kalliatakis", "Grigorios", ""], ["Sticlaru", "Anca", ""], ["Stamatiadis", "George", ""], ["Ehsan", "Shoaib", ""], ["Leonardis", "Ales", ""], ["Gall", "Juergen", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1711.03892", "submitter": "Tomas Teijeiro", "authors": "Tom\\'as Teijeiro, Constantino A. Garc\\'ia, Daniel Castro, Paulo\n  F\\'elix", "title": "Arrhythmia Classification from the Abductive Interpretation of Short\n  Single-Lead ECG Records", "comments": "4 pages, 3 figures. Presented in the Computing in Cardiology 2017\n  conference", "journal-ref": null, "doi": "10.22489/CinC.2017.166-054", "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a new method for the rhythm classification of short\nsingle-lead ECG records, using a set of high-level and clinically meaningful\nfeatures provided by the abductive interpretation of the records. These\nfeatures include morphological and rhythm-related features that are used to\nbuild two classifiers: one that evaluates the record globally, using aggregated\nvalues for each feature; and another one that evaluates the record as a\nsequence, using a Recurrent Neural Network fed with the individual features for\neach detected heartbeat. The two classifiers are finally combined using the\nstacking technique, providing an answer by means of four target classes: Normal\nsinus rhythm, Atrial fibrillation, Other anomaly, and Noisy. The approach has\nbeen validated against the 2017 Physionet/CinC Challenge dataset, obtaining a\nfinal score of 0.83 and ranking first in the competition.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 15:47:21 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Teijeiro", "Tom\u00e1s", ""], ["Garc\u00eda", "Constantino A.", ""], ["Castro", "Daniel", ""], ["F\u00e9lix", "Paulo", ""]]}, {"id": "1711.03938", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez,\n  Vladlen Koltun", "title": "CARLA: An Open Urban Driving Simulator", "comments": "Published at the 1st Conference on Robot Learning (CoRL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce CARLA, an open-source simulator for autonomous driving research.\nCARLA has been developed from the ground up to support development, training,\nand validation of autonomous urban driving systems. In addition to open-source\ncode and protocols, CARLA provides open digital assets (urban layouts,\nbuildings, vehicles) that were created for this purpose and can be used freely.\nThe simulation platform supports flexible specification of sensor suites and\nenvironmental conditions. We use CARLA to study the performance of three\napproaches to autonomous driving: a classic modular pipeline, an end-to-end\nmodel trained via imitation learning, and an end-to-end model trained via\nreinforcement learning. The approaches are evaluated in controlled scenarios of\nincreasing difficulty, and their performance is examined via metrics provided\nby CARLA, illustrating the platform's utility for autonomous driving research.\nThe supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 17:54:40 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Ros", "German", ""], ["Codevilla", "Felipe", ""], ["Lopez", "Antonio", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1711.03954", "submitter": "Redouane Lguensat", "authors": "Redouane Lguensat, Miao Sun, Ronan Fablet, Evan Mason, Pierre Tandeo,\n  Ge Chen", "title": "EddyNet: A Deep Neural Network For Pixel-Wise Classification of Oceanic\n  Eddies", "comments": null, "journal-ref": null, "doi": "10.1109/IGARSS.2018.8518411", "report-no": null, "categories": "cs.CV physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents EddyNet, a deep learning based architecture for automated\neddy detection and classification from Sea Surface Height (SSH) maps provided\nby the Copernicus Marine and Environment Monitoring Service (CMEMS). EddyNet is\na U-Net like network that consists of a convolutional encoder-decoder followed\nby a pixel-wise classification layer. The output is a map with the same size of\nthe input where pixels have the following labels \\{'0': Non eddy, '1':\nanticyclonic eddy, '2': cyclonic eddy\\}. We investigate the use of SELU\nactivation function instead of the classical ReLU+BN and we use an overlap\nbased loss function instead of the cross entropy loss. Keras Python code, the\ntraining datasets and EddyNet weights files are open-source and freely\navailable on https://github.com/redouanelg/EddyNet.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 18:30:05 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Lguensat", "Redouane", ""], ["Sun", "Miao", ""], ["Fablet", "Ronan", ""], ["Mason", "Evan", ""], ["Tandeo", "Pierre", ""], ["Chen", "Ge", ""]]}, {"id": "1711.03990", "submitter": "Debayan Deb", "authors": "Debayan Deb, Neeta Nain, Anil K. Jain", "title": "Longitudinal Study of Child Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a longitudinal study of face recognition performance on Children\nLongitudinal Face (CLF) dataset containing 3,682 face images of 919 subjects,\nin the age group [2, 18] years. Each subject has at least four face images\nacquired over a time span of up to six years. Face comparison scores are\nobtained from (i) a state-of-the-art COTS matcher (COTS-A), (ii) an open-source\nmatcher (FaceNet), and (iii) a simple sum fusion of scores obtained from COTS-A\nand FaceNet matchers. To improve the performance of the open-source FaceNet\nmatcher for child face recognition, we were able to fine-tune it on an\nindependent training set of 3,294 face images of 1,119 children in the age\ngroup [3, 18] years. Multilevel statistical models are fit to genuine\ncomparison scores from the CLF dataset to determine the decrease in face\nrecognition accuracy over time. Additionally, we analyze both the verification\nand open-set identification accuracies in order to evaluate state-of-the-art\nface recognition technology for tracing and identifying children lost at a\nyoung age as victims of child trafficking or abduction.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 19:19:55 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Deb", "Debayan", ""], ["Nain", "Neeta", ""], ["Jain", "Anil K.", ""]]}, {"id": "1711.04047", "submitter": "Lei Wang", "authors": "Melih Engin, Lei Wang, Luping Zhou, Xinwang Liu", "title": "DeepKSPD: Learning Kernel-matrix-based SPD Representation for\n  Fine-grained Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being symmetric positive-definite (SPD), covariance matrix has traditionally\nbeen used to represent a set of local descriptors in visual recognition. Recent\nstudy shows that kernel matrix can give considerably better representation by\nmodelling the nonlinearity in the local descriptor set. Nevertheless, neither\nthe descriptors nor the kernel matrix is deeply learned. Worse, they are\nconsidered separately, hindering the pursuit of an optimal SPD representation.\nThis work proposes a deep network that jointly learns local descriptors,\nkernel-matrix-based SPD representation, and the classifier via an end-to-end\ntraining process. We derive the derivatives for the mapping from a local\ndescriptor set to the SPD representation to carry out backpropagation. Also, we\nexploit the Daleckii-Krein formula in operator theory to give a concise and\nunified result on differentiating SPD matrix functions, including the matrix\nlogarithm to handle the Riemannian geometry of kernel matrix. Experiments not\nonly show the superiority of kernel-matrix-based SPD representation with deep\nlocal descriptors, but also verify the advantage of the proposed deep network\nin pursuing better SPD representations for fine-grained image recognition\ntasks.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 00:41:32 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Engin", "Melih", ""], ["Wang", "Lei", ""], ["Zhou", "Luping", ""], ["Liu", "Xinwang", ""]]}, {"id": "1711.04048", "submitter": "Mostafa El-Khamy", "authors": "Haoyu Ren, Mostafa El-Khamy, and Jungwon Lee", "title": "CT-SRCNN: Cascade Trained and Trimmed Deep Convolutional Neural Networks\n  for Image Super Resolution", "comments": "Accepted to IEEE Winter Conf. on Applications of Computer Vision\n  (WACV) 2018, Lake Tahoe, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose methodologies to train highly accurate and efficient deep\nconvolutional neural networks (CNNs) for image super resolution (SR). A cascade\ntraining approach to deep learning is proposed to improve the accuracy of the\nneural networks while gradually increasing the number of network layers. Next,\nwe explore how to improve the SR efficiency by making the network slimmer. Two\nmethodologies, the one-shot trimming and the cascade trimming, are proposed.\nWith the cascade trimming, the network's size is gradually reduced layer by\nlayer, without significant loss on its discriminative ability. Experiments on\nbenchmark image datasets show that our proposed SR network achieves the\nstate-of-the-art super resolution accuracy, while being more than 4 times\nfaster compared to existing deep super resolution networks.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 00:55:57 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Ren", "Haoyu", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "1711.04061", "submitter": "Stefan Hinterstoisser", "authors": "Stefan Hinterstoisser and Vincent Lepetit and Naresh Rajkumar and Kurt\n  Konolige", "title": "Going Further with Point Pair Features", "comments": "Corrected post-print of manuscript accepted to the European\n  Conference on Computer Vision (ECCV) 2016;\n  https://link.springer.com/chapter/10.1007/978-3-319-46487-9_51", "journal-ref": null, "doi": "10.1007/978-3-319-46487-9_51", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point Pair Features is a widely used method to detect 3D objects in point\nclouds, however they are prone to fail in presence of sensor noise and\nbackground clutter. We introduce novel sampling and voting schemes that\nsignificantly reduces the influence of clutter and sensor noise. Our\nexperiments show that with our improvements, PPFs become competitive against\nstate-of-the-art methods as it outperforms them on several objects from\nchallenging benchmarks, at a low computational cost.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 02:24:58 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Hinterstoisser", "Stefan", ""], ["Lepetit", "Vincent", ""], ["Rajkumar", "Naresh", ""], ["Konolige", "Kurt", ""]]}, {"id": "1711.04069", "submitter": "Christian Samuel Perone", "authors": "Christian S. Perone", "title": "Towards ECDSA key derivation from deep embeddings for novel Blockchain\n  applications", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a straightforward method to derive Elliptic Curve\nDigital Signature Algorithm (ECDSA) key pairs from embeddings created using\nDeep Learning and Metric Learning approaches. We also show that these keys\nallows the derivation of cryptocurrencies (such as Bitcoin) addresses that can\nbe used to transfer and receive funds, allowing novel Blockchain-based\napplications that can be used to transfer funds or data directly to domains\nsuch as image, text, sound or any other domain where Deep Learning can extract\nhigh-quality embeddings; providing thus a novel integration between the\nproperties of the Blockchain-based technologies such as trust minimization and\ndecentralization together with the high-quality learned representations from\nDeep Learning techniques.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 03:07:34 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Perone", "Christian S.", ""]]}, {"id": "1711.04147", "submitter": "Xiaobing Wang", "authors": "Xiangyu Zhu, Yingying Jiang, Shuli Yang, Xiaobing Wang, Wei Li, Pei\n  Fu, Hua Wang and Zhenbo Luo", "title": "Deep Residual Text Detection Network for Scene Text", "comments": "IAPR International Conference on Document Analysis and Recognition\n  (ICDAR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text detection is a challenging problem in computer vision. In this\npaper, we propose a novel text detection network based on prevalent object\ndetection frameworks. In order to obtain stronger semantic feature, we adopt\nResNet as feature extraction layers and exploit multi-level feature by\ncombining hierarchical convolutional networks. A vertical proposal mechanism is\nutilized to avoid proposal classification, while regression layer remains\nworking to improve localization accuracy. Our approach evaluated on ICDAR2013\ndataset achieves F-measure of 0.91, which outperforms previous state-of-the-art\nresults in scene text detection.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 15:03:33 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Zhu", "Xiangyu", ""], ["Jiang", "Yingying", ""], ["Yang", "Shuli", ""], ["Wang", "Xiaobing", ""], ["Li", "Wei", ""], ["Fu", "Pei", ""], ["Wang", "Hua", ""], ["Luo", "Zhenbo", ""]]}, {"id": "1711.04161", "submitter": "Jiagang Zhu", "authors": "Jiagang Zhu, Wei Zou, Zheng Zhu", "title": "End-to-end Video-level Representation Learning for Action Recognition", "comments": "10 pages, 6 figures, 6 tables. The explanation for the batch size is\n  added. Accepted by ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From the frame/clip-level feature learning to the video-level representation\nbuilding, deep learning methods in action recognition have developed rapidly in\nrecent years. However, current methods suffer from the confusion caused by\npartial observation training, or without end-to-end learning, or restricted to\nsingle temporal scale modeling and so on. In this paper, we build upon\ntwo-stream ConvNets and propose Deep networks with Temporal Pyramid Pooling\n(DTPP), an end-to-end video-level representation learning approach, to address\nthese problems. Specifically, at first, RGB images and optical flow stacks are\nsparsely sampled across the whole video. Then a temporal pyramid pooling layer\nis used to aggregate the frame-level features which consist of spatial and\ntemporal cues. Lastly, the trained model has compact video-level representation\nwith multiple temporal scales, which is both global and sequence-aware.\nExperimental results show that DTPP achieves the state-of-the-art performance\non two challenging video action datasets: UCF101 and HMDB51, either by ImageNet\npre-training or Kinetics pre-training.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 15:52:19 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 04:52:04 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 03:31:56 GMT"}, {"version": "v4", "created": "Tue, 12 Dec 2017 06:59:34 GMT"}, {"version": "v5", "created": "Sun, 14 Jan 2018 07:43:46 GMT"}, {"version": "v6", "created": "Sat, 14 Apr 2018 07:41:07 GMT"}, {"version": "v7", "created": "Sat, 21 Apr 2018 15:03:17 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Zhu", "Jiagang", ""], ["Zou", "Wei", ""], ["Zhu", "Zheng", ""]]}, {"id": "1711.04170", "submitter": "Siqi Bao", "authors": "Siqi Bao, Pei Wang, Tony C. W. Mok and Albert C. S. Chung", "title": "3D Randomized Connection Network with Graph-based Label Inference", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2829263", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel 3D deep learning network is proposed for brain MR\nimage segmentation with randomized connection, which can decrease the\ndependency between layers and increase the network capacity. The convolutional\nLSTM and 3D convolution are employed as network units to capture the long-term\nand short-term 3D properties respectively. To assemble these two kinds of\nspatial-temporal information and refine the deep learning outcomes, we further\nintroduce an efficient graph-based node selection and label inference method.\nExperiments have been carried out on two publicly available databases and\nresults demonstrate that the proposed method can obtain competitive\nperformances as compared with other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 16:50:42 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Bao", "Siqi", ""], ["Wang", "Pei", ""], ["Mok", "Tony C. W.", ""], ["Chung", "Albert C. S.", ""]]}, {"id": "1711.04178", "submitter": "Keaton Hamm", "authors": "Akram Aldroubi, Keaton Hamm, Ahmet Bugra Koku, and Ali Sekmen", "title": "CUR Decompositions, Similarity Matrices, and Subspace Clustering", "comments": "Approximately 30 pages. Current version contains improved algorithm\n  and numerical experiments from the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general framework for solving the subspace clustering problem using the CUR\ndecomposition is presented. The CUR decomposition provides a natural way to\nconstruct similarity matrices for data that come from a union of unknown\nsubspaces $\\mathscr{U}=\\underset{i=1}{\\overset{M}\\bigcup}S_i$. The similarity\nmatrices thus constructed give the exact clustering in the noise-free case.\nAdditionally, this decomposition gives rise to many distinct similarity\nmatrices from a given set of data, which allow enough flexibility to perform\naccurate clustering of noisy data. We also show that two known methods for\nsubspace clustering can be derived from the CUR decomposition. An algorithm\nbased on the theoretical construction of similarity matrices is presented, and\nexperiments on synthetic and real data are presented to test the method.\n  Additionally, an adaptation of our CUR based similarity matrices is utilized\nto provide a heuristic algorithm for subspace clustering; this algorithm yields\nthe best overall performance to date for clustering the Hopkins155 motion\nsegmentation dataset.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 18:34:34 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 21:14:03 GMT"}, {"version": "v3", "created": "Tue, 11 Dec 2018 20:53:22 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Aldroubi", "Akram", ""], ["Hamm", "Keaton", ""], ["Koku", "Ahmet Bugra", ""], ["Sekmen", "Ali", ""]]}, {"id": "1711.04192", "submitter": "Chen Chen", "authors": "Baochang Zhang and Shangzhen Luan and Chen Chen and Jungong Han and\n  Wei Wang and Alessandro Perina and Ling Shao", "title": "Latent Constrained Correlation Filter", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2775060", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filters are special classifiers designed for shift-invariant\nobject recognition, which are robust to pattern distortions. The recent\nliterature shows that combining a set of sub-filters trained based on a single\nor a small group of images obtains the best performance. The idea is equivalent\nto estimating variable distribution based on the data sampling (bagging), which\ncan be interpreted as finding solutions (variable distribution approximation)\ndirectly from sampled data space. However, this methodology fails to account\nfor the variations existed in the data. In this paper, we introduce an\nintermediate step -- solution sampling -- after the data sampling step to form\na subspace, in which an optimal solution can be estimated. More specifically,\nwe propose a new method, named latent constrained correlation filters (LCCF),\nby mapping the correlation filters to a given latent subspace, and develop a\nnew learning framework in the latent subspace that embeds distribution-related\nconstraints into the original problem. To solve the optimization problem, we\nintroduce a subspace based alternating direction method of multipliers (SADMM),\nwhich is proven to converge at the saddle point. Our approach is successfully\napplied to three different tasks, including eye localization, car detection and\nobject tracking. Extensive experiments demonstrate that LCCF outperforms the\nstate-of-the-art methods. The source code will be publicly available.\nhttps://github.com/bczhangbczhang/.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 20:27:39 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Zhang", "Baochang", ""], ["Luan", "Shangzhen", ""], ["Chen", "Chen", ""], ["Han", "Jungong", ""], ["Wang", "Wei", ""], ["Perina", "Alessandro", ""], ["Shao", "Ling", ""]]}, {"id": "1711.04226", "submitter": "Zhanzhan Cheng", "authors": "Zhanzhan Cheng, Yangliu Xu, Fan Bai, Yi Niu, Shiliang Pu and Shuigeng\n  Zhou", "title": "AON: Towards Arbitrarily-Oriented Text Recognition", "comments": "Accepted by CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recognizing text from natural images is a hot research topic in computer\nvision due to its various applications. Despite the enduring research of\nseveral decades on optical character recognition (OCR), recognizing texts from\nnatural images is still a challenging task. This is because scene texts are\noften in irregular (e.g. curved, arbitrarily-oriented or seriously distorted)\narrangements, which have not yet been well addressed in the literature.\nExisting methods on text recognition mainly work with regular (horizontal and\nfrontal) texts and cannot be trivially generalized to handle irregular texts.\nIn this paper, we develop the arbitrary orientation network (AON) to directly\ncapture the deep features of irregular texts, which are combined into an\nattention-based decoder to generate character sequence. The whole network can\nbe trained end-to-end by using only images and word-level annotations.\nExtensive experiments on various benchmarks, including the CUTE80,\nSVT-Perspective, IIIT5k, SVT and ICDAR datasets, show that the proposed\nAON-based method achieves the-state-of-the-art performance in irregular\ndatasets, and is comparable to major existing methods in regular datasets.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 03:11:25 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 06:15:45 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Cheng", "Zhanzhan", ""], ["Xu", "Yangliu", ""], ["Bai", "Fan", ""], ["Niu", "Yi", ""], ["Pu", "Shiliang", ""], ["Zhou", "Shuigeng", ""]]}, {"id": "1711.04237", "submitter": "Shiqi Yang", "authors": "Shiqi Yang and Gang Peng", "title": "D-PCN: Parallel Convolutional Networks for Image Recognition via a\n  Discriminator", "comments": "20 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a simple but quite effective recognition\nframework dubbed D-PCN, aiming at enhancing feature extracting ability of CNN.\nThe framework consists of two parallel CNNs, a discriminator and an extra\nclassifier which takes integrated features from parallel networks and gives\nfinal prediction. The discriminator is core which drives parallel networks to\nfocus on different regions and learn complementary representations. The\ncorresponding joint training strategy is introduced which ensures the\nutilization of discriminator. We validate D-PCN with several CNN models on two\nbenchmark datasets: CIFAR-100 and ImageNet32x32, D-PCN enhances all models. In\nparticular it yields state of the art performance on CIFAR-100 compared with\nrelated works. We also conduct visualization experiment on fine-grained\nStanford Dogs dataset and verify our motivation. Additionally, we apply D-PCN\nfor segmentation on PASCAL VOC 2012 and also find promotion.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 05:11:42 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 01:29:55 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 13:55:43 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Yang", "Shiqi", ""], ["Peng", "Gang", ""]]}, {"id": "1711.04247", "submitter": "Reza Abbasi-Asl", "authors": "Reza Abbasi-Asl, Aboozar Ghaffari and Emad Fatemizadeh", "title": "Robust registration of medical images in the presence of\n  spatially-varying noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially-varying intensity noise is a common source of distortion in medical\nimages. Bias field noise is one example of such a distortion that is often\npresent in the magnetic resonance (MR) images or other modalities such as\nretina images. In this paper, we first show that the bias field noise can be\nconsiderably reduced using Empirical Mode Decomposition (EMD) technique. EMD is\na multi-resolution tool that decomposes a signal into several principle\npatterns and residual components. We show that the spatially-varying noise is\nhighly expressed in the residual component of the EMD and could be filtered\nout. Then, we propose two hierarchical multi-resolution EMD-based algorithms\nfor robust registration of images in the presence of spatially varying noise.\nOne algorithm (LR-EMD) is based on registration of EMD feature-maps from both\nfloating and reference images in various resolution levels. In the second\nalgorithm (AFR-EMD), we first extract an average feature-map based on EMD from\nboth floating and reference images. Then, we use a simple hierarchical\nmulti-resolution algorithm to register the average feature-maps. For the brain\nMR images, both algorithms achieve lower error rate and higher convergence\npercentage compared to the intensity-based hierarchical registration.\nSpecifically, using mutual information as the similarity measure, AFR-EMD\nachieves 42% lower error rate in intensity and 52% lower error rate in\ntransformation compared to intensity-based hierarchical registration. For\nLR-EMD, the error rate is 32% lower for the intensity and 41% lower for the\ntransformation. Furthermore, we demonstrate that our proposed algorithms\nimprove the registration of retina images in the presence of spatially varying\nnoise.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 07:45:29 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 04:55:41 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Abbasi-Asl", "Reza", ""], ["Ghaffari", "Aboozar", ""], ["Fatemizadeh", "Emad", ""]]}, {"id": "1711.04249", "submitter": "Lianwen Jin", "authors": "Sheng Zhang, Yuliang Liu, Lianwen Jin, Canjie Luo", "title": "Feature Enhancement Network: A Refined Scene Text Detector", "comments": "8 pages, 5 figures, 2 tables. This paper is accepted to appear in\n  AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a refined scene text detector with a \\textit{novel}\nFeature Enhancement Network (FEN) for Region Proposal and Text Detection\nRefinement. Retrospectively, both region proposal with \\textit{only} $3\\times\n3$ sliding-window feature and text detection refinement with \\textit{single\nscale} high level feature are insufficient, especially for smaller scene text.\nTherefore, we design a new FEN network with \\textit{task-specific},\n\\textit{low} and \\textit{high} level semantic features fusion to improve the\nperformance of text detection. Besides, since \\textit{unitary}\nposition-sensitive RoI pooling in general object detection is unreasonable for\nvariable text regions, an \\textit{adaptively weighted} position-sensitive RoI\npooling layer is devised for further enhancing the detecting accuracy. To\ntackle the \\textit{sample-imbalance} problem during the refinement stage, we\nalso propose an effective \\textit{positives mining} strategy for efficiently\ntraining our network. Experiments on ICDAR 2011 and 2013 robust text detection\nbenchmarks demonstrate that our method can achieve state-of-the-art results,\noutperforming all reported methods in terms of F-measure.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 08:12:54 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Zhang", "Sheng", ""], ["Liu", "Yuliang", ""], ["Jin", "Lianwen", ""], ["Luo", "Canjie", ""]]}, {"id": "1711.04258", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng, Zenglin Xu", "title": "Unified Spectral Clustering with Optimal Graph", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering has found extensive use in many areas. Most traditional\nspectral clustering algorithms work in three separate steps: similarity graph\nconstruction; continuous labels learning; discretizing the learned labels by\nk-means clustering. Such common practice has two potential flaws, which may\nlead to severe information loss and performance degradation. First, predefined\nsimilarity graph might not be optimal for subsequent clustering. It is\nwell-accepted that similarity graph highly affects the clustering results. To\nthis end, we propose to automatically learn similarity information from data\nand simultaneously consider the constraint that the similarity matrix has exact\nc connected components if there are c clusters. Second, the discrete solution\nmay deviate from the spectral solution since k-means method is well-known as\nsensitive to the initialization of cluster centers. In this work, we transform\nthe candidate solution into a new one that better approximates the discrete\none. Finally, those three subtasks are integrated into a unified framework,\nwith each subtask iteratively boosted by using the results of the others\ntowards an overall optimal solution. It is known that the performance of a\nkernel method is largely determined by the choice of kernels. To tackle this\npractical problem of how to select the most suitable kernel for a particular\ndata set, we further extend our model to incorporate multiple kernel learning\nability. Extensive experiments demonstrate the superiority of our proposed\nmethod as compared to existing clustering approaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 09:20:25 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""], ["Xu", "Zenglin", ""]]}, {"id": "1711.04260", "submitter": "Yucao Tang", "authors": "Yucao Tang and Guillaume-Alexandre Bilodeau", "title": "Evaluation of trackers for Pan-Tilt-Zoom Scenarios", "comments": "6 pages, 2 figures, International Conference on Pattern Recognition\n  and Artificial Intelligence 2018", "journal-ref": "ICPRAI (2018) 3-9", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking with a Pan-Tilt-Zoom (PTZ) camera has been a research topic in\ncomputer vision for many years. Compared to tracking with a still camera, the\nimages captured with a PTZ camera are highly dynamic in nature because the\ncamera can perform large motion resulting in quickly changing capture\nconditions. Furthermore, tracking with a PTZ camera involves camera control to\nposition the camera on the target. For successful tracking and camera control,\nthe tracker must be fast enough, or has to be able to predict accurately the\nnext position of the target. Therefore, standard benchmarks do not allow to\nassess properly the quality of a tracker for the PTZ scenario. In this work, we\nuse a virtual PTZ framework to evaluate different tracking algorithms and\ncompare their performances. We also extend the framework to add target position\nprediction for the next frame, accounting for camera motion and processing\ndelays. By doing this, we can assess if predicting can make long-term tracking\nmore robust as it may help slower algorithms for keeping the target in the\nfield of view of the camera. Results confirm that both speed and robustness are\nrequired for tracking under the PTZ scenario.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 09:33:53 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Tang", "Yucao", ""], ["Bilodeau", "Guillaume-Alexandre", ""]]}, {"id": "1711.04293", "submitter": "Youchen Du", "authors": "Youchen Du, Shenglan Liu, Lin Feng, Menghui Chen, Jie Wu", "title": "Hand Gesture Recognition with Leap Motion", "comments": "6 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent introduction of depth cameras like Leap Motion Controller allows\nresearchers to exploit the depth information to recognize hand gesture more\nrobustly. This paper proposes a novel hand gesture recognition system with Leap\nMotion Controller. A series of features are extracted from Leap Motion tracking\ndata, we feed these features along with HOG feature extracted from sensor\nimages into a multi-class SVM classifier to recognize performed gesture,\ndimension reduction and feature weighted fusion are also discussed. Our results\nshow that our model is much more accurate than previous work.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 13:27:31 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Du", "Youchen", ""], ["Liu", "Shenglan", ""], ["Feng", "Lin", ""], ["Chen", "Menghui", ""], ["Wu", "Jie", ""]]}, {"id": "1711.04322", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi", "title": "11K Hands: Gender recognition and biometric identification using a large\n  dataset of hand images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human hand possesses distinctive features which can reveal gender\ninformation. In addition, the hand is considered one of the primary biometric\ntraits used to identify a person. In this work, we propose a large dataset of\nhuman hand images (dorsal and palmar sides) with detailed ground-truth\ninformation for gender recognition and biometric identification. Using this\ndataset, a convolutional neural network (CNN) can be trained effectively for\nthe gender recognition task. Based on this, we design a two-stream CNN to\ntackle the gender recognition problem. This trained model is then used as a\nfeature extractor to feed a set of support vector machine classifiers for the\nbiometric identification task. We show that the dorsal side of hand images,\ncaptured by a regular digital camera, convey effective distinctive features\nsimilar to, if not better, those available in the palmar hand images. To\nfacilitate access to the proposed dataset and replication of our experiments,\nthe dataset, trained CNN models, and Matlab source code are available at\n(https://goo.gl/rQJndd).\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 17:22:20 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 17:15:19 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 02:08:35 GMT"}, {"version": "v4", "created": "Sat, 18 Nov 2017 18:21:43 GMT"}, {"version": "v5", "created": "Mon, 27 Nov 2017 15:54:57 GMT"}, {"version": "v6", "created": "Wed, 29 Nov 2017 00:29:04 GMT"}, {"version": "v7", "created": "Sun, 1 Apr 2018 18:43:26 GMT"}, {"version": "v8", "created": "Tue, 29 May 2018 22:11:59 GMT"}, {"version": "v9", "created": "Mon, 17 Sep 2018 02:04:23 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Afifi", "Mahmoud", ""]]}, {"id": "1711.04323", "submitter": "Idan Schwartz", "authors": "Idan Schwartz, Alexander G. Schwing, Tamir Hazan", "title": "High-Order Attention Models for Visual Question Answering", "comments": "9 pages, 8 figures, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quest for algorithms that enable cognitive abilities is an important part\nof machine learning. A common trait in many recently investigated\ncognitive-like tasks is that they take into account different data modalities,\nsuch as visual and textual input. In this paper we propose a novel and\ngenerally applicable form of attention mechanism that learns high-order\ncorrelations between various data modalities. We show that high-order\ncorrelations effectively direct the appropriate attention to the relevant\nelements in the different data modalities that are required to solve the joint\ntask. We demonstrate the effectiveness of our high-order attention mechanism on\nthe task of visual question answering (VQA), where we achieve state-of-the-art\nperformance on the standard VQA dataset.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 17:30:05 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Schwartz", "Idan", ""], ["Schwing", "Alexander G.", ""], ["Hazan", "Tamir", ""]]}, {"id": "1711.04325", "submitter": "Takuya Akiba", "authors": "Takuya Akiba, Shuji Suzuki, Keisuke Fukuda", "title": "Extremely Large Minibatch SGD: Training ResNet-50 on ImageNet in 15\n  Minutes", "comments": "NIPS'17 Workshop: Deep Learning at Supercomputer Scale", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that training ResNet-50 on ImageNet for 90 epochs can be\nachieved in 15 minutes with 1024 Tesla P100 GPUs. This was made possible by\nusing a large minibatch size of 32k. To maintain accuracy with this large\nminibatch size, we employed several techniques such as RMSprop warm-up, batch\nnormalization without moving averages, and a slow-start learning rate schedule.\nThis paper also describes the details of the hardware and software of the\nsystem used to achieve the above performance.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 17:36:46 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Akiba", "Takuya", ""], ["Suzuki", "Shuji", ""], ["Fukuda", "Keisuke", ""]]}, {"id": "1711.04340", "submitter": "Antreas Antoniou Mr", "authors": "Antreas Antoniou, Amos Storkey and Harrison Edwards", "title": "Data Augmentation Generative Adversarial Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective training of neural networks requires much data. In the low-data\nregime, parameters are underdetermined, and learnt networks generalise poorly.\nData Augmentation alleviates this by using existing data more effectively.\nHowever standard data augmentation produces only limited plausible alternative\ndata. Given there is potential to generate a much broader set of augmentations,\nwe design and train a generative model to do data augmentation. The model,\nbased on image conditional Generative Adversarial Networks, takes data from a\nsource domain and learns to take any data item and generalise it to generate\nother within-class data items. As this generative process does not depend on\nthe classes themselves, it can be applied to novel unseen classes of data. We\nshow that a Data Augmentation Generative Adversarial Network (DAGAN) augments\nstandard vanilla classifiers well. We also show a DAGAN can enhance few-shot\nlearning systems such as Matching Networks. We demonstrate these approaches on\nOmniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In\nour experiments we can see over 13% increase in accuracy in the low-data regime\nexperiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face\n(4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5%\n(from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 19:17:57 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 16:46:40 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 23:26:15 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Antoniou", "Antreas", ""], ["Storkey", "Amos", ""], ["Edwards", "Harrison", ""]]}, {"id": "1711.04433", "submitter": "Miaojing Shi", "authors": "Lu Zhang, Miaojing Shi and Qiaobo Chen", "title": "Crowd counting via scale-adaptive convolutional neural network", "comments": "IEEE Winter Conf. on Applications of Computer Vision (WACV'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of crowd counting is to automatically estimate the pedestrian number\nin crowd images. To cope with the scale and perspective changes that commonly\nexist in crowd images, state-of-the-art approaches employ multi-column CNN\narchitectures to regress density maps of crowd images. Multiple columns have\ndifferent receptive fields corresponding to pedestrians (heads) of different\nscales. We instead propose a scale-adaptive CNN (SaCNN) architecture with a\nbackbone of fixed small receptive fields. We extract feature maps from multiple\nlayers and adapt them to have the same output size; we combine them to produce\nthe final density map. The number of people is computed by integrating the\ndensity map. We also introduce a relative count loss along with the density map\nloss to improve the network generalization on crowd scenes with few\npedestrians, where most representative approaches perform poorly on. We conduct\nextensive experiments on the ShanghaiTech, UCF_CC_50 and WorldExpo datasets as\nwell as a new dataset SmartCity that we collect for crowd scenes with few\npeople. The results demonstrate significant improvements of SaCNN over the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 06:25:56 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 06:54:10 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 21:22:53 GMT"}, {"version": "v4", "created": "Tue, 6 Feb 2018 21:32:53 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Zhang", "Lu", ""], ["Shi", "Miaojing", ""], ["Chen", "Qiaobo", ""]]}, {"id": "1711.04450", "submitter": "Yoshihide Sawada PhD", "authors": "Yoshihide Sawada, Yoshikuni Sato, Toru Nakada, Kei Ujimoto, and\n  Nobuhiro Hayashi", "title": "All-Transfer Learning for Deep Neural Networks and its Application to\n  Sepsis Classification", "comments": "Long version of article published at ECAI 2016 (9 pages, 13 figures,\n  8 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a transfer learning method for deep neural\nnetworks (DNNs). Deep learning has been widely used in many applications.\nHowever, applying deep learning is problematic when a large amount of training\ndata are not available. One of the conventional methods for solving this\nproblem is transfer learning for DNNs. In the field of image recognition,\nstate-of-the-art transfer learning methods for DNNs re-use parameters trained\non source domain data except for the output layer. However, this method may\nresult in poor classification performance when the amount of target domain data\nis significantly small. To address this problem, we propose a method called\nAll-Transfer Deep Learning, which enables the transfer of all parameters of a\nDNN. With this method, we can compute the relationship between the source and\ntarget labels by the source domain knowledge. We applied our method to actual\ntwo-dimensional electrophoresis image~(2-DE image) classification for\ndetermining if an individual suffers from sepsis; the first attempt to apply a\nclassification approach to 2-DE images for proteomics, which has attracted\nconsiderable attention as an extension beyond genomics. The results suggest\nthat our proposed method outperforms conventional transfer learning methods for\nDNNs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 07:28:37 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Sawada", "Yoshihide", ""], ["Sato", "Yoshikuni", ""], ["Nakada", "Toru", ""], ["Ujimoto", "Kei", ""], ["Hayashi", "Nobuhiro", ""]]}, {"id": "1711.04451", "submitter": "Jianyu Wang", "authors": "Jianyu Wang, Zhishuai Zhang, Cihang Xie, Yuyin Zhou, Vittal\n  Premachandran, Jun Zhu, Lingxi Xie, Alan Yuille", "title": "Visual Concepts and Compositional Voting", "comments": "It is accepted by Annals of Mathematical Sciences and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is very attractive to formulate vision in terms of pattern theory\n\\cite{Mumford2010pattern}, where patterns are defined hierarchically by\ncompositions of elementary building blocks. But applying pattern theory to real\nworld images is currently less successful than discriminative methods such as\ndeep networks. Deep networks, however, are black-boxes which are hard to\ninterpret and can easily be fooled by adding occluding objects. It is natural\nto wonder whether by better understanding deep networks we can extract building\nblocks which can be used to develop pattern theoretic models. This motivates us\nto study the internal representations of a deep network using vehicle images\nfrom the PASCAL3D+ dataset. We use clustering algorithms to study the\npopulation activities of the features and extract a set of visual concepts\nwhich we show are visually tight and correspond to semantic parts of vehicles.\nTo analyze this we annotate these vehicles by their semantic parts to create a\nnew dataset, VehicleSemanticParts, and evaluate visual concepts as unsupervised\npart detectors. We show that visual concepts perform fairly well but are\noutperformed by supervised discriminative methods such as Support Vector\nMachines (SVM). We next give a more detailed analysis of visual concepts and\nhow they relate to semantic parts. Following this, we use the visual concepts\nas building blocks for a simple pattern theoretical model, which we call\ncompositional voting. In this model several visual concepts combine to detect\nsemantic parts. We show that this approach is significantly better than\ndiscriminative methods like SVM and deep networks trained specifically for\nsemantic part detection. Finally, we return to studying occlusion by creating\nan annotated dataset with occlusion, called VehicleOcclusion, and show that\ncompositional voting outperforms even deep networks when the amount of\nocclusion becomes large.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 07:29:04 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Wang", "Jianyu", ""], ["Zhang", "Zhishuai", ""], ["Xie", "Cihang", ""], ["Zhou", "Yuyin", ""], ["Premachandran", "Vittal", ""], ["Zhu", "Jun", ""], ["Xie", "Lingxi", ""], ["Yuille", "Alan", ""]]}, {"id": "1711.04481", "submitter": "JIachi Zhang", "authors": "Xiaolei Shen, Jiachi Zhang, Chenjun Yan, Hong Zhou", "title": "An Automatic Diagnosis Method of Facial Acne Vulgaris Based on\n  Convolutional Neural Network", "comments": "12 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new automatic diagnosis method of facial acne\nvulgaris based on convolutional neural network. This method is proposed to\novercome the shortcoming of classification types in previous methods. The core\nof our method is to extract features of images based on convolutional neural\nnetwork and achieve classification by classifier. We design a binary classifier\nof skin-and-non-skin to detect skin area and a seven-classifier to achieve the\nclassification of facial acne vulgaris and healthy skin. In the experiment, we\ncompared the effectiveness of our convolutional neural network and the\npre-trained VGG16 neural network on the ImageNet dataset. And we use the ROC\ncurve and normal confusion matrix to evaluate the performance of the binary\nclassifier and the seven-classifier. The results of our experiment show that\nthe pre-trained VGG16 neural network is more effective in extracting image\nfeatures. The classifiers based on the pre-trained VGG16 neural network achieve\nthe skin detection and acne classification and have good robustness.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 09:16:09 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Shen", "Xiaolei", ""], ["Zhang", "Jiachi", ""], ["Yan", "Chenjun", ""], ["Zhou", "Hong", ""]]}, {"id": "1711.04483", "submitter": "Fahim Alam", "authors": "Fahim Irfan Alam, Jun Zhou, Alan Wee-Chung Liew, Xiuping Jia, Jocelyn\n  Chanussot and Yongsheng Gao", "title": "Conditional Random Field and Deep Feature Learning for Hyperspectral\n  Image Segmentation", "comments": "Submitted for Journal (Version 2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is considered to be one of the critical tasks in\nhyperspectral remote sensing image processing. Recently, convolutional neural\nnetwork (CNN) has established itself as a powerful model in segmentation and\nclassification by demonstrating excellent performances. The use of a graphical\nmodel such as a conditional random field (CRF) contributes further in capturing\ncontextual information and thus improving the segmentation performance. In this\npaper, we propose a method to segment hyperspectral images by considering both\nspectral and spatial information via a combined framework consisting of CNN and\nCRF. We use multiple spectral cubes to learn deep features using CNN, and then\nformulate deep CRF with CNN-based unary and pairwise potential functions to\neffectively extract the semantic correlations between patches consisting of\nthree-dimensional data cubes. Effective piecewise training is applied in order\nto avoid the computationally expensive iterative CRF inference. Furthermore, we\nintroduce a deep deconvolution network that improves the segmentation masks. We\nalso introduce a new dataset and experimented our proposed method on it along\nwith several widely adopted benchmark datasets to evaluate the effectiveness of\nour method. By comparing our results with those from several state-of-the-art\nmodels, we show the promising potential of our method.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 09:18:25 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 06:26:01 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Alam", "Fahim Irfan", ""], ["Zhou", "Jun", ""], ["Liew", "Alan Wee-Chung", ""], ["Jia", "Xiuping", ""], ["Chanussot", "Jocelyn", ""], ["Gao", "Yongsheng", ""]]}, {"id": "1711.04592", "submitter": "Jan Egger", "authors": "Jan Egger, Christopher Nimsky, Xiaojun Chen", "title": "Vertebral body segmentation with GrowCut: Initial experience, workflow\n  and practical application", "comments": "10 pages", "journal-ref": "SAGE Open Medicine, Volume 5, pp. 1-10, Nov. 2017", "doi": "10.1177/2050312117740984", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we used the GrowCut segmentation algorithm publicly\navailable in three-dimensional Slicer for three-dimensional segmentation of\nvertebral bodies. To the best of our knowledge, this is the first time that the\nGrowCut method has been studied for the usage of vertebral body segmentation.\nIn brief, we found that the GrowCut segmentation times were consistently less\nthan the manual segmentation times. Hence, GrowCut provides an alternative to a\nmanual slice-by-slice segmentation process.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 14:19:05 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Egger", "Jan", ""], ["Nimsky", "Christopher", ""], ["Chen", "Xiaojun", ""]]}, {"id": "1711.04598", "submitter": "Boris Knyazev", "authors": "Boris Knyazev, Roman Shvetsov, Natalia Efremova, Artem Kuharenko", "title": "Convolutional neural networks pretrained on large face recognition\n  datasets for emotion classification from video", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a solution to our entry for the emotion recognition\nchallenge EmotiW 2017. We propose an ensemble of several models, which capture\nspatial and audio features from videos. Spatial features are captured by\nconvolutional neural networks, pretrained on large face recognition datasets.\nWe show that usage of strong industry-level face recognition networks increases\nthe accuracy of emotion recognition. Using our ensemble we improve on the\nprevious best result on the test set by about 1 %, achieving a 60.03 %\nclassification accuracy without any use of visual temporal information.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 14:38:24 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Knyazev", "Boris", ""], ["Shvetsov", "Roman", ""], ["Efremova", "Natalia", ""], ["Kuharenko", "Artem", ""]]}, {"id": "1711.04623", "submitter": "Zachary Kenton", "authors": "Stanis{\\l}aw Jastrz\\k{e}bski, Zachary Kenton, Devansh Arpit, Nicolas\n  Ballas, Asja Fischer, Yoshua Bengio, Amos Storkey", "title": "Three Factors Influencing Minima in SGD", "comments": "First two authors contributed equally. Short version accepted into\n  ICLR workshop. Accepted to Artificial Neural Networks and Machine Learning,\n  ICANN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the dynamical and convergent properties of stochastic gradient\ndescent (SGD) applied to Deep Neural Networks (DNNs). Characterizing the\nrelation between learning rate, batch size and the properties of the final\nminima, such as width or generalization, remains an open question. In order to\ntackle this problem we investigate the previously proposed approximation of SGD\nby a stochastic differential equation (SDE). We theoretically argue that three\nfactors - learning rate, batch size and gradient covariance - influence the\nminima found by SGD. In particular we find that the ratio of learning rate to\nbatch size is a key determinant of SGD dynamics and of the width of the final\nminima, and that higher values of the ratio lead to wider minima and often\nbetter generalization. We confirm these findings experimentally. Further, we\ninclude experiments which show that learning rate schedules can be replaced\nwith batch size schedules and that the ratio of learning rate to batch size is\nan important factor influencing the memorization process.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 15:11:56 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 16:22:54 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 09:29:55 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Jastrz\u0119bski", "Stanis\u0142aw", ""], ["Kenton", "Zachary", ""], ["Arpit", "Devansh", ""], ["Ballas", "Nicolas", ""], ["Fischer", "Asja", ""], ["Bengio", "Yoshua", ""], ["Storkey", "Amos", ""]]}, {"id": "1711.04661", "submitter": "Zheng Zhu", "authors": "Zheng Zhu, Guan Huang, Wei Zou, Dalong Du, Chang Huang", "title": "UCT: Learning Unified Convolutional Networks for Real-time Visual\n  Tracking", "comments": "ICCV2017 Workshops. arXiv admin note: text overlap with\n  arXiv:1711.01124", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) based tracking approaches have shown\nfavorable performance in recent benchmarks. Nonetheless, the chosen CNN\nfeatures are always pre-trained in different task and individual components in\ntracking systems are learned separately, thus the achieved tracking performance\nmay be suboptimal. Besides, most of these trackers are not designed towards\nreal-time applications because of their time-consuming feature extraction and\ncomplex optimization details.In this paper, we propose an end-to-end framework\nto learn the convolutional features and perform the tracking process\nsimultaneously, namely, a unified convolutional tracker (UCT). Specifically,\nThe UCT treats feature extractor and tracking process both as convolution\noperation and trains them jointly, enabling learned CNN features are tightly\ncoupled to tracking process. In online tracking, an efficient updating method\nis proposed by introducing peak-versus-noise ratio (PNR) criterion, and scale\nchanges are handled efficiently by incorporating a scale branch into network.\nThe proposed approach results in superior tracking performance, while\nmaintaining real-time speed. The standard UCT and UCT-Lite can track generic\nobjects at 41 FPS and 154 FPS without further optimization, respectively.\nExperiments are performed on four challenging benchmark tracking datasets:\nOTB2013, OTB2015, VOT2014 and VOT2015, and our method achieves state-of-the-art\nresults on these benchmarks compared with other real-time trackers.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 09:18:06 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Zhu", "Zheng", ""], ["Huang", "Guan", ""], ["Zou", "Wei", ""], ["Du", "Dalong", ""], ["Huang", "Chang", ""]]}, {"id": "1711.04708", "submitter": "Anuj Karpatne", "authors": "Anuj Karpatne, Imme Ebert-Uphoff, Sai Ravela, Hassan Ali Babaie, and\n  Vipin Kumar", "title": "Machine Learning for the Geosciences: Challenges and Opportunities", "comments": "Under review at IEEE Transactions on Knowledge and Data Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geosciences is a field of great societal relevance that requires solutions to\nseveral urgent problems facing our humanity and the planet. As geosciences\nenters the era of big data, machine learning (ML) -- that has been widely\nsuccessful in commercial domains -- offers immense potential to contribute to\nproblems in geosciences. However, problems in geosciences have several unique\nchallenges that are seldom found in traditional applications, requiring novel\nproblem formulations and methodologies in machine learning. This article\nintroduces researchers in the machine learning (ML) community to these\nchallenges offered by geoscience problems and the opportunities that exist for\nadvancing both machine learning and geosciences. We first highlight typical\nsources of geoscience data and describe their properties that make it\nchallenging to use traditional machine learning techniques. We then describe\nsome of the common categories of geoscience problems where machine learning can\nplay a role, and discuss some of the existing efforts and promising directions\nfor methodological development in machine learning. We conclude by discussing\nsome of the emerging research themes in machine learning that are applicable\nacross all problems in the geosciences, and the importance of a deep\ncollaboration between machine learning and geosciences for synergistic\nadvancements in both disciplines.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 17:16:38 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Karpatne", "Anuj", ""], ["Ebert-Uphoff", "Imme", ""], ["Ravela", "Sai", ""], ["Babaie", "Hassan Ali", ""], ["Kumar", "Vipin", ""]]}, {"id": "1711.04710", "submitter": "Anuj Karpatne", "authors": "Gowtham Atluri, Anuj Karpatne, and Vipin Kumar", "title": "Spatio-Temporal Data Mining: A Survey of Problems and Methods", "comments": "Accepted for publication at ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large volumes of spatio-temporal data are increasingly collected and studied\nin diverse domains including, climate science, social sciences, neuroscience,\nepidemiology, transportation, mobile health, and Earth sciences.\nSpatio-temporal data differs from relational data for which computational\napproaches are developed in the data mining community for multiple decades, in\nthat both spatial and temporal attributes are available in addition to the\nactual measurements/attributes. The presence of these attributes introduces\nadditional challenges that needs to be dealt with. Approaches for mining\nspatio-temporal data have been studied for over a decade in the data mining\ncommunity. In this article we present a broad survey of this relatively young\nfield of spatio-temporal data mining. We discuss different types of\nspatio-temporal data and the relevant data mining questions that arise in the\ncontext of analyzing each of these datasets. Based on the nature of the data\nmining problem studied, we classify literature on spatio-temporal data mining\ninto six major categories: clustering, predictive learning, change detection,\nfrequent pattern mining, anomaly detection, and relationship mining. We discuss\nthe various forms of spatio-temporal data mining problems in each of these\ncategories.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 17:17:29 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 17:31:54 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Atluri", "Gowtham", ""], ["Karpatne", "Anuj", ""], ["Kumar", "Vipin", ""]]}, {"id": "1711.04851", "submitter": "Aditya Balu", "authors": "Sambit Ghadai, Aditya Balu, Adarsh Krishnamurthy, Soumik Sarkar", "title": "Learning and Visualizing Localized Geometric Features Using 3D-CNN: An\n  Application to Manufacturability Analysis of Drilled Holes", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Convolutional Neural Networks (3D-CNN) have been used for object\nrecognition based on the voxelized shape of an object. However, interpreting\nthe decision making process of these 3D-CNNs is still an infeasible task. In\nthis paper, we present a unique 3D-CNN based Gradient-weighted Class Activation\nMapping method (3D-GradCAM) for visual explanations of the distinct local\ngeometric features of interest within an object. To enable efficient learning\nof 3D geometries, we augment the voxel data with surface normals of the object\nboundary. We then train a 3D-CNN with this augmented data and identify the\nlocal features critical for decision-making using 3D GradCAM. An application of\nthis feature identification framework is to recognize difficult-to-manufacture\ndrilled hole features in a complex CAD geometry. The framework can be extended\nto identify difficult-to-manufacture features at multiple spatial scales\nleading to a real-time design for manufacturability decision support system.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 21:05:39 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 18:49:35 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 03:52:41 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Ghadai", "Sambit", ""], ["Balu", "Aditya", ""], ["Krishnamurthy", "Adarsh", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1711.04853", "submitter": "Alexander Tibbs", "authors": "Alexander B. Tibbs (1 and 2), Ilse M. Daly (2), Nicholas W. Roberts\n  (2), David R. Bull (1) ((1) Department of Electrical and Electronic\n  Engineering, University of Bristol, (2) School of Biological Sciences,\n  University of Bristol)", "title": "Denoising Imaging Polarimetry by an Adapted BM3D Method", "comments": null, "journal-ref": null, "doi": "10.1364/JOSAA.35.000690", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging polarimetry allows more information to be extracted from a scene than\nconventional intensity or colour imaging. However, a major challenge of imaging\npolarimetry is image degradation due to noise. This paper investigates the\nmitigation of noise through denoising algorithms and compares existing\ndenoising algorithms with a new method, based on BM3D. This algorithm, PBM3D,\ngives visual quality superior to the state of the art across all images and\nnoise standard deviations tested. We show that denoising polarization images\nusing PBM3D allows the degree of polarization to be more accurately calculated\nby comparing it to spectroscopy methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 21:07:40 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 10:21:33 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Tibbs", "Alexander B.", "", "1 and 2"], ["Daly", "Ilse M.", ""], ["Roberts", "Nicholas W.", ""], ["Bull", "David R.", ""]]}, {"id": "1711.04855", "submitter": "Ruairidh Battleday", "authors": "Ruairidh M. Battleday, Joshua C. Peterson, Thomas L. Griffiths", "title": "Modeling Human Categorization of Natural Images Using Deep Feature\n  Representations", "comments": "13 pages, 7 figures, 6 tables. Preliminary work presented at CogSci\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few decades, psychologists have developed sophisticated formal\nmodels of human categorization using simple artificial stimuli. In this paper,\nwe use modern machine learning methods to extend this work into the realm of\nnaturalistic stimuli, enabling human categorization to be studied over the\ncomplex visual domain in which it evolved and developed. We show that\nrepresentations derived from a convolutional neural network can be used to\nmodel behavior over a database of >300,000 human natural image classifications,\nand find that a group of models based on these representations perform well,\nnear the reliability of human judgments. Interestingly, this group includes\nboth exemplar and prototype models, contrasting with the dominance of exemplar\nmodels in previous work. We are able to improve the performance of the\nremaining models by preprocessing neural network representations to more\nclosely capture human similarity judgments.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 21:18:29 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Battleday", "Ruairidh M.", ""], ["Peterson", "Joshua C.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1711.04875", "submitter": "Foteini Fotopoulou", "authors": "F. Fotopoulou, S. Oikonomou, A. Papathanasiou, G. Economou and S.\n  Fotopoulos", "title": "3D Shape Classification Using Collaborative Representation based\n  Projections", "comments": "16 pages, 6 Figures, 3 Tables Statement including that an updated\n  version of this manuscript is under condiseration at Pattern Recognition\n  Letters, is added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel 3D shape classification scheme, based on collaborative representation\nlearning, is investigated in this work. A data-driven feature-extraction\nprocedure, taking the form of a simple projection operator, is in the core of\nour methodology. Provided a shape database, a graph encapsulating the\nstructural relationships among all the available shapes, is first constructed\nand then employed in defining low-dimensional sparse projections. The recently\nintroduced method of CRPs (collaborative representation based projections),\nwhich is based on L2-Graph, is the first variant that is included towards this\nend. A second algorithm, that particularizes the CRPs to shape descriptors that\nare inherently nonnegative, is also introduced as potential alternative. In\nboth cases, the weights in the graph reflecting the database structure are\ncalculated so as to approximate each shape as a sparse linear combination of\nthe remaining dataset objects. By way of solving a generalized eigenanalysis\nproblem, a linear matrix operator is designed that will act as the feature\nextractor. Two popular, inherently high dimensional descriptors, namely\nShapeDNA and Global Point Signature (GPS), are employed in our experimentations\nwith SHREC10, SHREC11 and SCHREC 15 datasets, where shape recognition is cast\nas a multi-class classification problem that is tackled by means of an SVM\n(support vector machine) acting within the reduced dimensional space of the\ncrafted projections. The results are very promising and outperform state of the\nart methods, providing evidence about the highly discriminative nature of the\nintroduced 3D shape representations.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 22:28:16 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 14:17:38 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Fotopoulou", "F.", ""], ["Oikonomou", "S.", ""], ["Papathanasiou", "A.", ""], ["Economou", "G.", ""], ["Fotopoulos", "S.", ""]]}, {"id": "1711.04901", "submitter": "Carlos Pena-Caballero", "authors": "Carlos Pena-Caballero, Elifaleth Cantu, Jesus Rodriguez, Adolfo\n  Gonzales, Osvaldo Castellanos, Angel Cantu, Megan Strait, Jae Son and\n  Dongchul Kim", "title": "A Multiple Radar Approach for Automatic Target Recognition of Aircraft\n  using Inverse Synthetic Aperture Radar", "comments": "8 pages, 9 figures, International Conference for Data Intelligence\n  and Security (ICDIS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the improvement of radar technologies, Automatic Target\nRecognition (ATR) using Synthetic Aperture Radar (SAR) and Inverse SAR (ISAR)\nhas come to be an active research area. SAR/ISAR are radar techniques to\ngenerate a two-dimensional high-resolution image of a target. Unlike other\nsimilar experiments using Convolutional Neural Networks (CNN) to solve this\nproblem, we utilize an unusual approach that leads to better performance and\nfaster training times. Our CNN uses complex values generated by a simulation to\ntrain the network; additionally, we utilize a multi-radar approach to increase\nthe accuracy of the training and testing processes, thus resulting in higher\naccuracies than the other papers working on SAR/ISAR ATR. We generated our\ndataset with 7 different aircraft models with a radar simulator we developed\ncalled RadarPixel; it is a Windows GUI program implemented using Matlab and\nJava programming, the simulator is capable of accurately replicating a real\nSAR/ISAR configurations. Our objective is to utilize our multi-radar technique\nand determine the optimal number of radars needed to detect and classify\ntargets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 01:31:24 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 17:10:24 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Pena-Caballero", "Carlos", ""], ["Cantu", "Elifaleth", ""], ["Rodriguez", "Jesus", ""], ["Gonzales", "Adolfo", ""], ["Castellanos", "Osvaldo", ""], ["Cantu", "Angel", ""], ["Strait", "Megan", ""], ["Son", "Jae", ""], ["Kim", "Dongchul", ""]]}, {"id": "1711.04945", "submitter": "Parag Chandakkar", "authors": "Parag Shridhar Chandakkar and Baoxin Li", "title": "Capturing Localized Image Artifacts through a CNN-based Hyper-image\n  Representation", "comments": "Our work on No-reference Image Quality Estimation (NR-IQA) using deep\n  neural networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep CNNs to capture localized image artifacts on a relatively small\ndataset is a challenging task. With enough images at hand, one can hope that a\ndeep CNN characterizes localized artifacts over the entire data and their\neffect on the output. However, on smaller datasets, such deep CNNs may overfit\nand shallow ones find it hard to capture local artifacts. Thus some image-based\nsmall-data applications first train their framework on a collection of patches\n(instead of the entire image) to better learn the representation of localized\nartifacts. Then the output is obtained by averaging the patch-level results.\nSuch an approach ignores the spatial correlation among patches and how various\npatch locations affect the output. It also fails in cases where few patches\nmainly contribute to the image label. To combat these scenarios, we develop the\nnotion of hyper-image representations. Our CNN has two stages. The first stage\nis trained on patches. The second stage utilizes the last layer representation\ndeveloped in the first stage to form a hyper-image, which is used to train the\nsecond stage. We show that this approach is able to develop a better mapping\nbetween the image and its output. We analyze additional properties of our\napproach and show its effectiveness on one synthetic and two real-world vision\ntasks - no-reference image quality estimation and image tampering detection -\nby its performance improvement over existing strong baselines.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 04:32:16 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Chandakkar", "Parag Shridhar", ""], ["Li", "Baoxin", ""]]}, {"id": "1711.05104", "submitter": "Odemir Bruno PhD", "authors": "Gisele H. B. Miranda, Jeaneth Machicao, Odemir M. Bruno", "title": "An optimized shape descriptor based on structural properties of networks", "comments": "19 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structural analysis of shape boundaries leads to the characterization of\nobjects as well as to the understanding of shape properties. The literature on\ngraphs and networks have contributed to the structural characterization of\nshapes with different theoretical approaches. We performed a study on the\nrelationship between the shape architecture and the network topology\nconstructed over the shape boundary. For that, we used a method for network\nmodeling proposed in 2009. Firstly, together with curvature analysis, we\nevaluated the proposed approach for regular polygons. This way, it was possible\nto investigate how the network measurements vary according to some specific\nshape properties. Secondly, we evaluated the performance of the proposed shape\ndescriptor in classification tasks for three datasets, accounting for both\nreal-world and synthetic shapes. We demonstrated that not only degree related\nmeasurements are capable of distinguishing classes of objects. Yet, when using\nmeasurements that account for distinct properties of the network structure, the\nconstruction of the shape descriptor becomes more computationally efficient.\nGiven the fact the network is dynamically constructed, the number of iterations\ncan be reduced. The proposed approach accounts for a more robust set of\nstructural measurements, that improved the discriminant power of the shape\ndescriptors.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 14:26:19 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Miranda", "Gisele H. B.", ""], ["Machicao", "Jeaneth", ""], ["Bruno", "Odemir M.", ""]]}, {"id": "1711.05128", "submitter": "Eduardo Aguilar", "authors": "Eduardo Aguilar, Beatriz Remeseiro, Marc Bola\\~nos, and Petia Radeva", "title": "Grab, Pay and Eat: Semantic Food Detection for Smart Restaurants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increase in awareness of people towards their nutritional habits has\ndrawn considerable attention to the field of automatic food analysis. Focusing\non self-service restaurants environment, automatic food analysis is not only\nuseful for extracting nutritional information from foods selected by customers,\nit is also of high interest to speed up the service solving the bottleneck\nproduced at the cashiers in times of high demand. In this paper, we address the\nproblem of automatic food tray analysis in canteens and restaurants\nenvironment, which consists in predicting multiple foods placed on a tray\nimage. We propose a new approach for food analysis based on convolutional\nneural networks, we name Semantic Food Detection, which integrates in the same\nframework food localization, recognition and segmentation. We demonstrate that\nour method improves the state of the art food detection by a considerable\nmargin on the public dataset UNIMIB2016 achieving about 90% in terms of\nF-measure, and thus provides a significant technological advance towards the\nautomatic billing in restaurant environments.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 14:49:13 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Aguilar", "Eduardo", ""], ["Remeseiro", "Beatriz", ""], ["Bola\u00f1os", "Marc", ""], ["Radeva", "Petia", ""]]}, {"id": "1711.05139", "submitter": "Amelie Royer", "authors": "Am\\'elie Royer, Konstantinos Bousmalis, Stephan Gouws, Fred Bertsch,\n  Inbar Mosseri, Forrester Cole, Kevin Murphy", "title": "XGAN: Unsupervised Image-to-Image Translation for Many-to-Many Mappings", "comments": "Domain Adaptation for Visual Understanding at ICML'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer usually refers to the task of applying color and texture\ninformation from a specific style image to a given content image while\npreserving the structure of the latter. Here we tackle the more generic problem\nof semantic style transfer: given two unpaired collections of images, we aim to\nlearn a mapping between the corpus-level style of each collection, while\npreserving semantic content shared across the two domains. We introduce XGAN\n(\"Cross-GAN\"), a dual adversarial autoencoder, which captures a shared\nrepresentation of the common domain semantic content in an unsupervised way,\nwhile jointly learning the domain-to-domain image translations in both\ndirections. We exploit ideas from the domain adaptation literature and define a\nsemantic consistency loss which encourages the model to preserve semantics in\nthe learned embedding space. We report promising qualitative results for the\ntask of face-to-cartoon translation. The cartoon dataset, CartoonSet, we\ncollected for this purpose is publicly available at\ngoogle.github.io/cartoonset/ as a new benchmark for semantic style transfer.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 15:18:38 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 12:46:25 GMT"}, {"version": "v3", "created": "Thu, 30 Nov 2017 16:06:06 GMT"}, {"version": "v4", "created": "Mon, 11 Dec 2017 00:09:05 GMT"}, {"version": "v5", "created": "Wed, 25 Apr 2018 12:57:23 GMT"}, {"version": "v6", "created": "Tue, 10 Jul 2018 17:25:59 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Royer", "Am\u00e9lie", ""], ["Bousmalis", "Konstantinos", ""], ["Gouws", "Stephan", ""], ["Bertsch", "Fred", ""], ["Mosseri", "Inbar", ""], ["Cole", "Forrester", ""], ["Murphy", "Kevin", ""]]}, {"id": "1711.05165", "submitter": "Sean Welleck", "authors": "Sean Welleck, Jialin Mao, Kyunghyun Cho, Zheng Zhang", "title": "Saliency-based Sequential Image Attention with Multiset Prediction", "comments": "To appear in Advances in Neural Information Processing Systems 30\n  (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans process visual scenes selectively and sequentially using attention.\nCentral to models of human visual attention is the saliency map. We propose a\nhierarchical visual architecture that operates on a saliency map and uses a\nnovel attention mechanism to sequentially focus on salient regions and take\nadditional glimpses within those regions. The architecture is motivated by\nhuman visual attention, and is used for multi-label image classification on a\nnovel multiset task, demonstrating that it achieves high precision and recall\nwhile localizing objects with its attention. Unlike conventional multi-label\nimage classification models, the model supports multiset prediction due to a\nreinforcement-learning based training process that allows for arbitrary label\npermutation and multiple instances per label.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 16:16:36 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Welleck", "Sean", ""], ["Mao", "Jialin", ""], ["Cho", "Kyunghyun", ""], ["Zhang", "Zheng", ""]]}, {"id": "1711.05166", "submitter": "Guofeng Zhang", "authors": "Haomin Liu, Chen Li, Guojun Chen, Guofeng Zhang, Michael Kaess, Hujun\n  Bao", "title": "Robust Keyframe-based Dense SLAM with an RGB-D Camera", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present RKD-SLAM, a robust keyframe-based dense SLAM\napproach for an RGB-D camera that can robustly handle fast motion and dense\nloop closure, and run without time limitation in a moderate size scene. It not\nonly can be used to scan high-quality 3D models, but also can satisfy the\ndemand of VR and AR applications. First, we combine color and depth information\nto construct a very fast keyframe-based tracking method on a CPU, which can\nwork robustly in challenging cases (e.g.~fast camera motion and complex loops).\nFor reducing accumulation error, we also introduce a very efficient incremental\nbundle adjustment (BA) algorithm, which can greatly save unnecessary\ncomputation and perform local and global BA in a unified optimization\nframework. An efficient keyframe-based depth representation and fusion method\nis proposed to generate and timely update the dense 3D surface with online\ncorrection according to the refined camera poses of keyframes through BA. The\nexperimental results and comparisons on a variety of challenging datasets and\nTUM RGB-D benchmark demonstrate the effectiveness of the proposed system.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 16:18:04 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Liu", "Haomin", ""], ["Li", "Chen", ""], ["Chen", "Guojun", ""], ["Zhang", "Guofeng", ""], ["Kaess", "Michael", ""], ["Bao", "Hujun", ""]]}, {"id": "1711.05175", "submitter": "Antonia Creswell", "authors": "Antonia Creswell, Yumnah Mohamied, Biswa Sengupta, Anil A Bharath", "title": "Adversarial Information Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel generative model architecture designed to learn\nrepresentations for images that factor out a single attribute from the rest of\nthe representation. A single object may have many attributes which when altered\ndo not change the identity of the object itself. Consider the human face; the\nidentity of a particular person is independent of whether or not they happen to\nbe wearing glasses. The attribute of wearing glasses can be changed without\nchanging the identity of the person. However, the ability to manipulate and\nalter image attributes without altering the object identity is not a trivial\ntask. Here, we are interested in learning a representation of the image that\nseparates the identity of an object (such as a human face) from an attribute\n(such as 'wearing glasses'). We demonstrate the success of our factorization\napproach by using the learned representation to synthesize the same face with\nand without a chosen attribute. We refer to this specific synthesis process as\nimage attribute manipulation. We further demonstrate that our model achieves\ncompetitive scores, with state of the art, on a facial attribute classification\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 16:25:09 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 14:42:04 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Creswell", "Antonia", ""], ["Mohamied", "Yumnah", ""], ["Sengupta", "Biswa", ""], ["Bharath", "Anil A", ""]]}, {"id": "1711.05187", "submitter": "Mingfei Gao", "authors": "Mingfei Gao, Ruichi Yu, Ang Li, Vlad I. Morariu, Larry S. Davis", "title": "Dynamic Zoom-in Network for Fast Object Detection in Large Images", "comments": "CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a generic framework that reduces the computational cost of\nobject detection while retaining accuracy for scenarios where objects with\nvaried sizes appear in high resolution images. Detection progresses in a\ncoarse-to-fine manner, first on a down-sampled version of the image and then on\na sequence of higher resolution regions identified as likely to improve the\ndetection accuracy. Built upon reinforcement learning, our approach consists of\na model (R-net) that uses coarse detection results to predict the potential\naccuracy gain for analyzing a region at a higher resolution and another model\n(Q-net) that sequentially selects regions to zoom in. Experiments on the\nCaltech Pedestrians dataset show that our approach reduces the number of\nprocessed pixels by over 50% without a drop in detection accuracy. The merits\nof our approach become more significant on a high resolution test set collected\nfrom YFCC100M dataset, where our approach maintains high detection performance\nwhile reducing the number of processed pixels by about 70% and the detection\ntime by over 50%.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 16:52:50 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 15:10:11 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Gao", "Mingfei", ""], ["Yu", "Ruichi", ""], ["Li", "Ang", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1711.05225", "submitter": "Pranav Rajpurkar", "authors": "Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel\n  Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya,\n  Matthew P. Lungren, Andrew Y. Ng", "title": "CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an algorithm that can detect pneumonia from chest X-rays at a\nlevel exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer\nconvolutional neural network trained on ChestX-ray14, currently the largest\npublicly available chest X-ray dataset, containing over 100,000 frontal-view\nX-ray images with 14 diseases. Four practicing academic radiologists annotate a\ntest set, on which we compare the performance of CheXNet to that of\nradiologists. We find that CheXNet exceeds average radiologist performance on\nthe F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and\nachieve state of the art results on all 14 diseases.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 17:58:50 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 04:21:27 GMT"}, {"version": "v3", "created": "Mon, 25 Dec 2017 11:09:06 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Rajpurkar", "Pranav", ""], ["Irvin", "Jeremy", ""], ["Zhu", "Kaylie", ""], ["Yang", "Brandon", ""], ["Mehta", "Hershel", ""], ["Duan", "Tony", ""], ["Ding", "Daisy", ""], ["Bagul", "Aarti", ""], ["Langlotz", "Curtis", ""], ["Shpanskaya", "Katie", ""], ["Lungren", "Matthew P.", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1711.05226", "submitter": "Tianfu Wu", "authors": "Tianfu Wu, Wei Sun, Xilai Li, Xi Song and Bo Li", "title": "Towards Interpretable R-CNN by Unfolding Latent Structures", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper first proposes a method of formulating model interpretability in\nvisual understanding tasks based on the idea of unfolding latent structures. It\nthen presents a case study in object detection using popular two-stage\nregion-based convolutional network (i.e., R-CNN) detection systems. We focus on\nweakly-supervised extractive rationale generation, that is learning to unfold\nlatent discriminative part configurations of object instances automatically and\nsimultaneously in detection without using any supervision for part\nconfigurations. We utilize a top-down hierarchical and compositional grammar\nmodel embedded in a directed acyclic AND-OR Graph (AOG) to explore and unfold\nthe space of latent part configurations of regions of interest (RoIs). We\npropose an AOGParsing operator to substitute the RoIPooling operator widely\nused in R-CNN. In detection, a bounding box is interpreted by the best parse\ntree derived from the AOG on-the-fly, which is treated as the qualitatively\nextractive rationale generated for interpreting detection. We propose a\nfolding-unfolding method to train the AOG and convolutional networks\nend-to-end. In experiments, we build on R-FCN and test our method on the PASCAL\nVOC 2007 and 2012 datasets. We show that the method can unfold promising latent\nstructures without hurting the performance.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 17:59:01 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 16:52:13 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Wu", "Tianfu", ""], ["Sun", "Wei", ""], ["Li", "Xilai", ""], ["Song", "Xi", ""], ["Li", "Bo", ""]]}, {"id": "1711.05246", "submitter": "Sean Welleck", "authors": "Sean Welleck, Zixin Yao, Yu Gai, Jialin Mao, Zheng Zhang, Kyunghyun\n  Cho", "title": "Loss Functions for Multiset Prediction", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of multiset prediction. The goal of multiset prediction\nis to train a predictor that maps an input to a multiset consisting of multiple\nitems. Unlike existing problems in supervised learning, such as classification,\nranking and sequence generation, there is no known order among items in a\ntarget multiset, and each item in the multiset may appear more than once,\nmaking this problem extremely challenging. In this paper, we propose a novel\nmultiset loss function by viewing this problem from the perspective of\nsequential decision making. The proposed multiset loss function is empirically\nevaluated on two families of datasets, one synthetic and the other real, with\nvarying levels of difficulty, against various baseline loss functions including\nreinforcement learning, sequence, and aggregated distribution matching loss\nfunctions. The experiments reveal the effectiveness of the proposed loss\nfunction over the others.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 18:43:22 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 18:32:36 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Welleck", "Sean", ""], ["Yao", "Zixin", ""], ["Gai", "Yu", ""], ["Mao", "Jialin", ""], ["Zhang", "Zheng", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1711.05282", "submitter": "Mingfei Gao", "authors": "Mingfei Gao, Ang Li, Ruichi Yu, Vlad I. Morariu, Larry S. Davis", "title": "C-WSL: Count-guided Weakly Supervised Localization", "comments": "ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce count-guided weakly supervised localization (C-WSL), an approach\nthat uses per-class object count as a new form of supervision to improve weakly\nsupervised localization (WSL). C-WSL uses a simple count-based region selection\nalgorithm to select high-quality regions, each of which covers a single object\ninstance during training, and improves existing WSL methods by training with\nthe selected regions. To demonstrate the effectiveness of C-WSL, we integrate\nit into two WSL architectures and conduct extensive experiments on VOC2007 and\nVOC2012. Experimental results show that C-WSL leads to large improvements in\nWSL and that the proposed approach significantly outperforms the\nstate-of-the-art methods. The results of annotation experiments on VOC2007\nsuggest that a modest extra time is needed to obtain per-class object counts\ncompared to labeling only object categories in an image. Furthermore, we reduce\nthe annotation time by more than $2\\times$ and $38\\times$ compared to\ncenter-click and bounding-box annotations.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 19:08:23 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 04:40:46 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Gao", "Mingfei", ""], ["Li", "Ang", ""], ["Yu", "Ruichi", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1711.05366", "submitter": "Douglas Brinkerhoff", "authors": "Douglas Brinkerhoff and Shad O'Neel", "title": "Velocity variations at Columbia Glacier captured by particle filtering\n  of oblique time-lapse images", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a probabilistic method for tracking glacier surface motion based\non time-lapse imagery, which works by sequentially resampling a stochastic\nstate-space model according to a likelihood determined through correlation\nbetween reference and test images. The method is robust due to its natural\nhandling of periodic occlusion and its capacity to follow multiple hypothesis\ndisplacements between images, and can improve estimates of velocity magnitude\nand direction through the inclusion of observations from an arbitrary number of\ncameras. We apply the method to an annual record of images from two cameras\nnear the terminus of Columbia Glacier. While the method produces velocities at\ndaily resolution, we verify our results by comparing eleven-day means to\nTerraSar-X. We find that Columbia Glacier transitions between a winter state\ncharacterized by moderate velocities and little temporal variability, to an\nearly summer speed-up in which velocities are sensitive to increases in melt-\nand rainwater, to a fall slowdown, where velocities drop to below their winter\nmean and become insensitive to external forcing, a pattern consistent with the\ndevelopment and collapse of efficient and inefficient subglacial hydrologic\nnetworks throughout the year.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 00:45:59 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Brinkerhoff", "Douglas", ""], ["O'Neel", "Shad", ""]]}, {"id": "1711.05368", "submitter": "Bao Zhao", "authors": "Bao Zhao, Xinyi Le, Juntong Xi", "title": "A Novel SDASS Descriptor for Fully Encoding the Information of 3D Local\n  Surface", "comments": "21 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local feature description is a fundamental yet challenging task in 3D\ncomputer vision. This paper proposes a novel descriptor, named Statistic of\nDeviation Angles on Subdivided Space (SDASS), of encoding geometrical and\nspatial information of local surface on Local Reference Axis (LRA). In terms of\nencoding geometrical information, considering that surface normals, which are\nusually used for encoding geometrical information of local surface, are\nvulnerable to various nuisances (e.g., noise, varying mesh resolutions etc.),\nwe propose a robust geometrical attribute, called Local Minimum Axis (LMA), to\nreplace the normals for generating the geometrical feature in our SDASS\ndescriptor. For encoding spatial information, we use two spatial features for\nfully encoding the spatial information of a local surface based on LRA which\nusually presents high overall repeatability than Local Reference Axis (LRF).\nBesides, an improved LRA is proposed for increasing the robustness of our SDASS\nto noise and varying mesh resolutions. The performance of the SDASS descriptor\nis rigorously tested on four popular datasets. The results show that our\ndescriptor has a high descriptiveness and strong robustness, and its\nperformance outperform existing algorithms by a large margin. Finally, the\nproposed descriptor is applied to 3D registration. The accurate result further\nconfirms the effectiveness of our SDASS method.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 00:50:16 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 13:39:53 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 13:06:34 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Zhao", "Bao", ""], ["Le", "Xinyi", ""], ["Xi", "Juntong", ""]]}, {"id": "1711.05376", "submitter": "Soheil Kolouri", "authors": "Soheil Kolouri, Gustavo K. Rohde, Heiko Hoffmann", "title": "Sliced Wasserstein Distance for Learning Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian mixture models (GMM) are powerful parametric tools with many\napplications in machine learning and computer vision. Expectation maximization\n(EM) is the most popular algorithm for estimating the GMM parameters. However,\nEM guarantees only convergence to a stationary point of the log-likelihood\nfunction, which could be arbitrarily worse than the optimal solution. Inspired\nby the relationship between the negative log-likelihood function and the\nKullback-Leibler (KL) divergence, we propose an alternative formulation for\nestimating the GMM parameters using the sliced Wasserstein distance, which\ngives rise to a new algorithm. Specifically, we propose minimizing the\nsliced-Wasserstein distance between the mixture model and the data distribution\nwith respect to the GMM parameters. In contrast to the KL-divergence, the\nenergy landscape for the sliced-Wasserstein distance is more well-behaved and\ntherefore more suitable for a stochastic gradient descent scheme to obtain the\noptimal GMM parameters. We show that our formulation results in parameter\nestimates that are more robust to random initializations and demonstrate that\nit can estimate high-dimensional data distributions more faithfully than the EM\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 01:33:01 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 02:05:11 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Kolouri", "Soheil", ""], ["Rohde", "Gustavo K.", ""], ["Hoffmann", "Heiko", ""]]}, {"id": "1711.05397", "submitter": "Lixin Fan", "authors": "Lixin Fan", "title": "Deep Epitome for Unravelling Generalized Hamming Network: A Fuzzy Logic\n  Interpretation of Deep Learning", "comments": "25 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives a rigorous analysis of trained Generalized Hamming\nNetworks(GHN) proposed by Fan (2017) and discloses an interesting finding about\nGHNs, i.e., stacked convolution layers in a GHN is equivalent to a single yet\nwide convolution layer. The revealed equivalence, on the theoretical side, can\nbe regarded as a constructive manifestation of the universal approximation\ntheorem Cybenko(1989); Hornik (1991). In practice, it has profound and\nmulti-fold implications. For network visualization, the constructed deep\nepitomes at each layer provide a visualization of network internal\nrepresentation that does not rely on the input data. Moreover, deep epitomes\nallows the direct extraction of features in just one step, without resorting to\nregularized optimizations used in existing visualization tools.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 03:36:06 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Fan", "Lixin", ""]]}, {"id": "1711.05407", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Jayaraman J. Thiagarajan, Rahul Sridhar, Peer-Timo\n  Bremer", "title": "MARGIN: Uncovering Deep Neural Networks using Graph Signal Analysis", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability has emerged as a crucial aspect of building trust in machine\nlearning systems, aimed at providing insights into the working of complex\nneural networks that are otherwise opaque to a user. There are a plethora of\nexisting solutions addressing various aspects of interpretability ranging from\nidentifying prototypical samples in a dataset to explaining image predictions\nor explaining mis-classifications. While all of these diverse techniques\naddress seemingly different aspects of interpretability, we hypothesize that a\nlarge family of interepretability tasks are variants of the same central\nproblem which is identifying \\emph{relative} change in a model's prediction.\nThis paper introduces MARGIN, a simple yet general approach to address a large\nset of interpretability tasks MARGIN exploits ideas rooted in graph signal\nanalysis to determine influential nodes in a graph, which are defined as those\nnodes that maximally describe a function defined on the graph. By carefully\ndefining task-specific graphs and functions, we demonstrate that MARGIN\noutperforms existing approaches in a number of disparate interpretability\nchallenges.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 04:52:38 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 15:58:09 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 04:36:36 GMT"}, {"version": "v4", "created": "Fri, 15 Jan 2021 20:40:23 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Sridhar", "Rahul", ""], ["Bremer", "Peer-Timo", ""]]}, {"id": "1711.05415", "submitter": "Taihong Xiao", "authors": "Taihong Xiao, Jiapeng Hong, Jinwen Ma", "title": "DNA-GAN: Learning Disentangled Representations from Multi-Attribute\n  Images", "comments": "ICLR 2018 workshop, github: https://github.com/Prinsphield/DNA-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Disentangling factors of variation has become a very challenging problem on\nrepresentation learning. Existing algorithms suffer from many limitations, such\nas unpredictable disentangling factors, poor quality of generated images from\nencodings, lack of identity information, etc. In this paper, we propose a\nsupervised learning model called DNA-GAN which tries to disentangle different\nfactors or attributes of images. The latent representations of images are\nDNA-like, in which each individual piece (of the encoding) represents an\nindependent factor of the variation. By annihilating the recessive piece and\nswapping a certain piece of one latent representation with that of the other\none, we obtain two different representations which could be decoded into two\nkinds of images with the existence of the corresponding attribute being\nchanged. In order to obtain realistic images and also disentangled\nrepresentations, we further introduce the discriminator for adversarial\ntraining. Experiments on Multi-PIE and CelebA datasets finally demonstrate that\nour proposed method is effective for factors disentangling and even overcome\ncertain limitations of the existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 05:49:39 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 02:42:06 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Xiao", "Taihong", ""], ["Hong", "Jiapeng", ""], ["Ma", "Jinwen", ""]]}, {"id": "1711.05431", "submitter": "Tang Yongliang", "authors": "Yongliang Tang, Weiguo Gong, Xi Chen, and Weihong Li", "title": "Deep Inception-Residual Laplacian Pyramid Networks for Accurate Single\n  Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With exploiting contextual information over large image regions in an\nefficient way, the deep convolutional neural network has shown an impressive\nperformance for single image super-resolution (SR). In this paper, we propose a\ndeep convolutional network by cascading the well-designed inception-residual\nblocks within the deep Laplacian pyramid framework to progressively restore the\nmissing high-frequency details of high-resolution (HR) images. By optimizing\nour network structure, the trainable depth of the proposed network gains a\nsignificant improvement, which in turn improves super-resolving accuracy. With\nour network depth increasing, however, the saturation and degradation of\ntraining accuracy continues to be a critical problem. As regard to this, we\npropose an effective two-stage training strategy, in which we firstly use\nimages downsampled from the ground-truth HR images as the optimal objective to\ntrain the inception-residual blocks in each pyramid level with an extremely\nhigh learning rate enabled by gradient clipping, and then the ground-truth HR\nimages are used to fine-tune all the pre-trained inception-residual blocks for\nobtaining the final SR model. Furthermore, we present a new loss function\noperating in both image space and local rank space to optimize our network for\nexploiting the contextual information among different output components.\nExtensive experiments on benchmark datasets validate that the proposed method\noutperforms existing state-of-the-art SR methods in terms of the objective\nevaluation as well as the visual quality.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 07:04:30 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Tang", "Yongliang", ""], ["Gong", "Weiguo", ""], ["Chen", "Xi", ""], ["Li", "Weihong", ""]]}, {"id": "1711.05444", "submitter": "Nuri Murat Arar", "authors": "Nuri Murat Arar and Jean-Philippe Thiran", "title": "Robust Real-Time Multi-View Eye Tracking", "comments": "Organisational changes in the main msp and supplementary info.\n  Results unchanged. Main msp: 14 pages, 15 figures. Supplementary: 2 tables, 1\n  figure. Under review for an IEEE transactions publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant advances in improving the gaze tracking accuracy under\ncontrolled conditions, the tracking robustness under real-world conditions,\nsuch as large head pose and movements, use of eyeglasses, illumination and eye\ntype variations, remains a major challenge in eye tracking. In this paper, we\nrevisit this challenge and introduce a real-time multi-camera eye tracking\nframework to improve the tracking robustness. First, differently from previous\nwork, we design a multi-view tracking setup that allows for acquiring multiple\neye appearances simultaneously. Leveraging multi-view appearances enables to\nmore reliably detect gaze features under challenging conditions, particularly\nwhen they are obstructed in conventional single-view appearance due to large\nhead movements or eyewear effects. The features extracted on various\nappearances are then used for estimating multiple gaze outputs. Second, we\npropose to combine estimated gaze outputs through an adaptive fusion mechanism\nto compute user's overall point of regard. The proposed mechanism firstly\ndetermines the estimation reliability of each gaze output according to user's\nmomentary head pose and predicted gazing behavior, and then performs a\nreliability-based weighted fusion. We demonstrate the efficacy of our framework\nwith extensive simulations and user experiments on a collected dataset\nfeaturing 20 subjects. Our results show that in comparison with\nstate-of-the-art eye trackers, the proposed framework provides not only a\nsignificant enhancement in accuracy but also a notable robustness. Our\nprototype system runs at 30 frames-per-second (fps) and achieves 1 degree\naccuracy under challenging experimental scenarios, which makes it suitable for\napplications demanding high accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 08:23:06 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 16:10:53 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Arar", "Nuri Murat", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "1711.05458", "submitter": "Mads Dyrmann", "authors": "Thomas Mosgaard Giselsson, Rasmus Nyholm J{\\o}rgensen, Peter Kryger\n  Jensen, Mads Dyrmann, Henrik Skov Midtiby", "title": "A Public Image Database for Benchmark of Plant Seedling Classification\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A database of images of approximately 960 unique plants belonging to 12\nspecies at several growth stages is made publicly available. It comprises\nannotated RGB images with a physical resolution of roughly 10 pixels per mm. To\nstandardise the evaluation of classification results obtained with the\ndatabase, a benchmark based on $f_{1}$ scores is proposed. The dataset is\navailable at https://vision.eng.au.dk/plant-seedlings-dataset\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 08:56:25 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Giselsson", "Thomas Mosgaard", ""], ["J\u00f8rgensen", "Rasmus Nyholm", ""], ["Jensen", "Peter Kryger", ""], ["Dyrmann", "Mads", ""], ["Midtiby", "Henrik Skov", ""]]}, {"id": "1711.05471", "submitter": "Ehud Barnea", "authors": "Ehud Barnea, Ohad Ben-Shahar", "title": "Exploring the Bounds of the Utility of Context for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recurring context in which objects appear holds valuable information that\ncan be employed to predict their existence. This intuitive observation indeed\nled many researchers to endow appearance-based detectors with explicit\nreasoning about context. The underlying thesis suggests that stronger\ncontextual relations would facilitate greater improvements in detection\ncapacity. In practice, however, the observed improvement in many cases is\nmodest at best, and often only marginal. In this work we seek to improve our\nunderstanding of this phenomenon, in part by pursuing an opposite approach.\nInstead of attempting to improve detection scores by employing context, we\ntreat the utility of context as an optimization problem: to what extent can\ndetection scores be improved by considering context or any other kind of\nadditional information? With this approach we explore the bounds on improvement\nby using contextual relations between objects and provide a tool for\nidentifying the most helpful ones. We show that simple co-occurrence relations\ncan often provide large gains, while in other cases a significant improvement\nis simply impossible or impractical with either co-occurrence or more precise\nspatial relations. To better understand these results we then analyze the\nability of context to handle different types of false detections, revealing\nthat tested contextual information cannot ameliorate localization errors,\nseverely limiting its gains. These and additional insights further our\nunderstanding on where and why utilization of context for object detection\nsucceeds and fails.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 09:25:49 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 19:17:56 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 17:54:22 GMT"}, {"version": "v4", "created": "Thu, 4 Apr 2019 14:53:28 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Barnea", "Ehud", ""], ["Ben-Shahar", "Ohad", ""]]}, {"id": "1711.05480", "submitter": "Balasubramanyam Appina Mr", "authors": "Appina Balasubramanyam, Jalli Akshith, Battula Shanmukh Srinivas,\n  Channappayya S Sumohana", "title": "No Reference Stereoscopic Video Quality Assessment Using Joint Motion\n  and Depth Statistics", "comments": "13 PAGES, 7 FIGURES, 7 TABLES", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a no reference (NR) quality assessment algorithm for assessing the\nperceptual quality of natural stereoscopic 3D (S3D) videos. This work is\ninspired by our finding that the joint statistics of the subband coefficients\nof motion (optical flow or motion vector magnitude) and depth (disparity map)\nof natural S3D videos possess a unique signature. Specifically, we empirically\nshow that the joint statistics of the motion and depth subband coefficients of\nS3D video frames can be modeled accurately using a Bivariate Generalized\nGaussian Distribution (BGGD). We then demonstrate that the parameters of the\nBGGD model possess the ability to discern quality variations in S3D videos.\nTherefore, the BGGD model parameters are employed as motion and depth quality\nfeatures. In addition to these features, we rely on a frame level spatial\nquality feature that is computed using a robust off the shelf NR image quality\nassessment (IQA) algorithm. These frame level motion, depth and spatial\nfeatures are consolidated and used with the corresponding S3D video's\ndifference mean opinion score (DMOS) labels for supervised learning using\nsupport vector regression (SVR). The overall quality of an S3D video is\ncomputed by averaging the frame level quality predictions of the constituent\nvideo frames. The proposed algorithm, dubbed Video QUality Evaluation using\nMOtion and DEpth Statistics (VQUEMODES) is shown to outperform the state of the\nart methods when evaluated over the IRCCYN and LFOVIA S3D subjective quality\nassessment databases.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 09:52:23 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Balasubramanyam", "Appina", ""], ["Akshith", "Jalli", ""], ["Srinivas", "Battula Shanmukh", ""], ["Sumohana", "Channappayya S", ""]]}, {"id": "1711.05491", "submitter": "Geraldin Nanfack", "authors": "Geraldin Nanfack, Azeddine Elhassouny and Rachid Oulad Haj Thami", "title": "Squeeze-SegNet: A new fast Deep Convolutional Neural Network for\n  Semantic Segmentation", "comments": "The 10th International Conference on Machine Vision (ICMV 2017).\n  arXiv admin note: text overlap with arXiv:1704.06857 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent researches in Deep Convolutional Neural Network have focused their\nattention on improving accuracy that provide significant advances. However, if\nthey were limited to classification tasks, nowadays with contributions from\nScientific Communities who are embarking in this field, they have become very\nuseful in higher level tasks such as object detection and pixel-wise semantic\nsegmentation. Thus, brilliant ideas in the field of semantic segmentation with\ndeep learning have completed the state of the art of accuracy, however this\narchitectures become very difficult to apply in embedded systems as is the case\nfor autonomous driving. We present a new Deep fully Convolutional Neural\nNetwork for pixel-wise semantic segmentation which we call Squeeze-SegNet. The\narchitecture is based on Encoder-Decoder style. We use a SqueezeNet-like\nencoder and a decoder formed by our proposed squeeze-decoder module and\nupsample layer using downsample indices like in SegNet and we add a\ndeconvolution layer to provide final multi-channel feature map. On datasets\nlike Camvid or City-states, our net gets SegNet-level accuracy with less than\n10 times fewer parameters than SegNet.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 10:41:52 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Nanfack", "Geraldin", ""], ["Elhassouny", "Azeddine", ""], ["Thami", "Rachid Oulad Haj", ""]]}, {"id": "1711.05512", "submitter": "Jacopo Acquarelli", "authors": "Jacopo Acquarelli, Elena Marchiori, Lutgarde M.C. Buydens, Thanh Tran,\n  Twan van Laarhoven", "title": "Spectral-spatial classification of hyperspectral images: three tricks\n  and a new supervised learning setting", "comments": "Remote Sensing 2018", "journal-ref": null, "doi": "10.3390/rs10071156", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral-spatial classification of hyperspectral images has been the subject\nof many studies in recent years. In the presence of only very few labeled\npixels, this task becomes challenging. In this paper we address the following\ntwo research questions: 1) Can a simple neural network with just a single\nhidden layer achieve state of the art performance in the presence of few\nlabeled pixels? 2) How is the performance of hyperspectral image classification\nmethods affected when using disjoint train and test sets? We give a positive\nanswer to the first question by using three tricks within a very basic shallow\nConvolutional Neural Network (CNN) architecture: a tailored loss function, and\nsmooth- and label-based data augmentation. The tailored loss function enforces\nthat neighborhood wavelengths have similar contributions to the features\ngenerated during training. A new label-based technique here proposed favors\nselection of pixels in smaller classes, which is beneficial in the presence of\nvery few labeled pixels and skewed class distributions. To address the second\nquestion, we introduce a new sampling procedure to generate disjoint train and\ntest set. Then the train set is used to obtain the CNN model, which is then\napplied to pixels in the test set to estimate their labels. We assess the\nefficacy of the simple neural network method on five publicly available\nhyperspectral images. On these images our method significantly outperforms\nconsidered baselines. Notably, with just 1% of labeled pixels per class, on\nthese datasets our method achieves an accuracy that goes from 86.42%\n(challenging dataset) to 99.52% (easy dataset). Furthermore we show that the\nsimple neural network method improves over other baselines in the new\nchallenging supervised setting. Our analysis substantiates the highly\nbeneficial effect of using the entire image (so train and test data) for\nconstructing a model.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 12:02:57 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 14:36:52 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 08:23:10 GMT"}, {"version": "v4", "created": "Mon, 23 Jul 2018 16:24:52 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Acquarelli", "Jacopo", ""], ["Marchiori", "Elena", ""], ["Buydens", "Lutgarde M. C.", ""], ["Tran", "Thanh", ""], ["van Laarhoven", "Twan", ""]]}, {"id": "1711.05523", "submitter": "Reza Kahani", "authors": "Reza Kahani, Alireza Talebpour, Ahmad Mahmoudi-Aznaveh", "title": "A Correlation Based Feature Representation for First-Person Activity\n  Recognition", "comments": "15 pages, 6 figures, 5 tables", "journal-ref": "Multimedia Tools and Applications, March 30 2019", "doi": "10.1007/s11042-019-7429-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a simple yet efficient activity recognition method for\nfirst-person video is introduced. The proposed method is appropriate for\nrepresentation of high-dimensional features such as those extracted from\nconvolutional neural networks (CNNs). The per-frame (per-segment) extracted\nfeatures are considered as a set of time series, and inter and intra-time\nseries relations are employed to represent the video descriptors. To find the\ninter-time relations, the series are grouped and the linear correlation between\neach pair of groups is calculated. The relations between them can represent the\nscene dynamics and local motions. The introduced grouping strategy helps to\nconsiderably reduce the computational cost. Furthermore, we split the series in\ntemporal direction in order to preserve long term motions and better focus on\neach local time window. In order to extract the cyclic motion patterns, which\ncan be considered as primary components of various activities, intra-time\nseries correlations are exploited. The representation method results in highly\ndiscriminative features which can be linearly classified. The experiments\nconfirm that our method outperforms the state-of-the-art methods on recognizing\nfirst-person activities on the two challenging first-person datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 12:26:42 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 13:30:55 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Kahani", "Reza", ""], ["Talebpour", "Alireza", ""], ["Mahmoudi-Aznaveh", "Ahmad", ""]]}, {"id": "1711.05535", "submitter": "Zhedong Zheng", "authors": "Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang, Mingliang Xu,\n  Yi-Dong Shen", "title": "Dual-Path Convolutional Image-Text Embeddings with Instance Loss", "comments": "15pages, 15 figures, 8 tables", "journal-ref": null, "doi": "10.1145/3383184", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching images and sentences demands a fine understanding of both\nmodalities. In this paper, we propose a new system to discriminatively embed\nthe image and text to a shared visual-textual space. In this field, most\nexisting works apply the ranking loss to pull the positive image / text pairs\nclose and push the negative pairs apart from each other. However, directly\ndeploying the ranking loss is hard for network learning, since it starts from\nthe two heterogeneous features to build inter-modal relationship. To address\nthis problem, we propose the instance loss which explicitly considers the\nintra-modal data distribution. It is based on an unsupervised assumption that\neach image / text group can be viewed as a class. So the network can learn the\nfine granularity from every image/text group. The experiment shows that the\ninstance loss offers better weight initialization for the ranking loss, so that\nmore discriminative embeddings can be learned. Besides, existing works usually\napply the off-the-shelf features, i.e., word2vec and fixed visual feature. So\nin a minor contribution, this paper constructs an end-to-end dual-path\nconvolutional network to learn the image and text representations. End-to-end\nlearning allows the system to directly learn from the data and fully utilize\nthe supervision. On two generic retrieval datasets (Flickr30k and MSCOCO),\nexperiments demonstrate that our method yields competitive accuracy compared to\nstate-of-the-art methods. Moreover, in language based person retrieval, we\nimprove the state of the art by a large margin. The code has been made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 12:40:11 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 02:40:11 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2018 17:38:54 GMT"}, {"version": "v4", "created": "Tue, 27 Jul 2021 07:45:26 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zheng", "Zhedong", ""], ["Zheng", "Liang", ""], ["Garrett", "Michael", ""], ["Yang", "Yi", ""], ["Xu", "Mingliang", ""], ["Shen", "Yi-Dong", ""]]}, {"id": "1711.05557", "submitter": "Chee Seng Chan", "authors": "Ying Hua Tan, Chee Seng Chan", "title": "Phrase-based Image Captioning with Hierarchical LSTM Model", "comments": "17 pages, 12 figures, ACCV2016 extension, phrase-based image\n  captioning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic generation of caption to describe the content of an image has been\ngaining a lot of research interests recently, where most of the existing works\ntreat the image caption as pure sequential data. Natural language, however\npossess a temporal hierarchy structure, with complex dependencies between each\nsubsequence. In this paper, we propose a phrase-based hierarchical Long\nShort-Term Memory (phi-LSTM) model to generate image description. In contrast\nto the conventional solutions that generate caption in a pure sequential\nmanner, our proposed model decodes image caption from phrase to sentence. It\nconsists of a phrase decoder at the bottom hierarchy to decode noun phrases of\nvariable length, and an abbreviated sentence decoder at the upper hierarchy to\ndecode an abbreviated form of the image description. A complete image caption\nis formed by combining the generated phrases with sentence during the inference\nstage. Empirically, our proposed model shows a better or competitive result on\nthe Flickr8k, Flickr30k and MS-COCO datasets in comparison to the state-of-the\nart models. We also show that our proposed model is able to generate more novel\ncaptions (not seen in the training data) which are richer in word contents in\nall these three datasets.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 10:48:59 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Tan", "Ying Hua", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1711.05586", "submitter": "Mark Marsden", "authors": "Mark Marsden, Kevin McGuinness, Suzanne Little, Ciara E. Keogh, Noel\n  E. O'Connor", "title": "People, Penguins and Petri Dishes: Adapting Object Counting Models To\n  New Visual Domains And Object Types Without Forgetting", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a technique to adapt a convolutional neural network\n(CNN) based object counter to additional visual domains and object types while\nstill preserving the original counting function. Domain-specific normalisation\nand scaling operators are trained to allow the model to adjust to the\nstatistical distributions of the various visual domains. The developed\nadaptation technique is used to produce a singular patch-based counting\nregressor capable of counting various object types including people, vehicles,\ncell nuclei and wildlife. As part of this study a challenging new cell counting\ndataset in the context of tissue culture and patient diagnosis is constructed.\nThis new collection, referred to as the Dublin Cell Counting (DCC) dataset, is\nthe first of its kind to be made available to the wider computer vision\ncommunity. State-of-the-art object counting performance is achieved in both the\nShanghaitech (parts A and B) and Penguins datasets while competitive\nperformance is observed on the TRANCOS and Modified Bone Marrow (MBM) datasets,\nall using a shared counting model.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 14:25:20 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Marsden", "Mark", ""], ["McGuinness", "Kevin", ""], ["Little", "Suzanne", ""], ["Keogh", "Ciara E.", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "1711.05611", "submitter": "David Bau iii", "authors": "Bolei Zhou, David Bau, Aude Oliva, and Antonio Torralba", "title": "Interpreting Deep Visual Representations via Network Dissection", "comments": "*B. Zhou and D. Bau contributed equally to this work. 15 pages, 27\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of recent deep convolutional neural networks (CNNs) depends on\nlearning hidden representations that can summarize the important factors of\nvariation behind the data. However, CNNs often criticized as being black boxes\nthat lack interpretability, since they have millions of unexplained model\nparameters. In this work, we describe Network Dissection, a method that\ninterprets networks by providing labels for the units of their deep visual\nrepresentations. The proposed method quantifies the interpretability of CNN\nrepresentations by evaluating the alignment between individual hidden units and\na set of visual semantic concepts. By identifying the best alignments, units\nare given human interpretable labels across a range of objects, parts, scenes,\ntextures, materials, and colors. The method reveals that deep representations\nare more transparent and interpretable than expected: we find that\nrepresentations are significantly more interpretable than they would be under a\nrandom equivalently powerful basis. We apply the method to interpret and\ncompare the latent representations of various network architectures trained to\nsolve different supervised and self-supervised training tasks. We then examine\nfactors affecting the network interpretability such as the number of the\ntraining iterations, regularizations, different initializations, and the\nnetwork depth and width. Finally we show that the interpreted units can be used\nto provide explicit explanations of a prediction given by a CNN for an image.\nOur results highlight that interpretability is an important property of deep\nneural networks that provides new insights into their hierarchical structure.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 15:05:25 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 15:38:31 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Zhou", "Bolei", ""], ["Bau", "David", ""], ["Oliva", "Aude", ""], ["Torralba", "Antonio", ""]]}, {"id": "1711.05702", "submitter": "Xu Han", "authors": "Xu Han, Roland Kwitt, Stephen Aylward, Spyridon Bakas, Bjoern Menze,\n  Alexander Asturias, Paul Vespa, John Van Horn, Marc Niethammer", "title": "Brain Extraction from Normal and Pathological Images: A Joint\n  PCA/Image-Reconstruction Approach", "comments": null, "journal-ref": null, "doi": "10.1016/j.neuroimage.2018.04.073", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain extraction from images is a common pre-processing step. Many approaches\nexist, but they are frequently only designed to perform brain extraction from\nimages without strong pathologies. Extracting the brain from images with strong\npathologies, for example, the presence of a tumor or of a traumatic brain\ninjury, is challenging. In such cases, tissue appearance may deviate from\nnormal tissue and violates algorithmic assumptions for these approaches; hence,\nthe brain may not be correctly extracted. This paper proposes a brain\nextraction approach which can explicitly account for pathologies by jointly\nmodeling normal tissue and pathologies. Specifically, our model uses a\nthree-part image decomposition: (1) normal tissue appearance is captured by\nprincipal component analysis, (2) pathologies are captured via a total\nvariation term, and (3) non-brain tissue is captured by a sparse term.\nDecomposition and image registration steps are alternated to allow statistical\nmodeling in a fixed atlas space. As a beneficial side effect, the model allows\nfor the identification of potential pathologies and the reconstruction of a\nquasi-normal image in atlas space. We demonstrate the effectiveness of our\nmethod on four datasets: the IBSR and LPBA40 datasets which show normal images,\nthe BRATS dataset containing images with brain tumors and a dataset containing\nclinical TBI images. We compare the performance with other popular models:\nROBEX, BEaST, MASS, BET, BSE and a recently proposed deep learning approach.\nOur model performs better than these competing methods on all four datasets.\nSpecifically, our model achieves the best median (97.11) and mean (96.88) Dice\nscores over all datasets. The two best performing competitors, ROBEX and MASS,\nachieve scores of 96.23/95.62 and 96.67/94.25 respectively. Hence, our approach\nis an effective method for high quality brain extraction on a wide variety of\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 17:57:52 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 19:53:45 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Han", "Xu", ""], ["Kwitt", "Roland", ""], ["Aylward", "Stephen", ""], ["Bakas", "Spyridon", ""], ["Menze", "Bjoern", ""], ["Asturias", "Alexander", ""], ["Vespa", "Paul", ""], ["Van Horn", "John", ""], ["Niethammer", "Marc", ""]]}, {"id": "1711.05705", "submitter": "Ehud Barnea", "authors": "Ehud Barnea, Ohad Ben-Shahar", "title": "Contextual Object Detection with a Few Relevant Neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural way to improve the detection of objects is to consider the\ncontextual constraints imposed by the detection of additional objects in a\ngiven scene. In this work, we exploit the spatial relations between objects in\norder to improve detection capacity, as well as analyze various properties of\nthe contextual object detection problem. To precisely calculate context-based\nprobabilities of objects, we developed a model that examines the interactions\nbetween objects in an exact probabilistic setting, in contrast to previous\nmethods that typically utilize approximations based on pairwise interactions.\nSuch a scheme is facilitated by the realistic assumption that the existence of\nan object in any given location is influenced by only few informative locations\nin space. Based on this assumption, we suggest a method for identifying these\nrelevant locations and integrating them into a mostly exact calculation of\nprobability based on their raw detector responses. This scheme is shown to\nimprove detection results and provides unique insights about the process of\ncontextual inference for object detection. We show that it is generally\ndifficult to learn that a particular object reduces the probability of another,\nand that in cases when the context and detector strongly disagree this learning\nbecomes virtually impossible for the purposes of improving the results of an\nobject detector. Finally, we demonstrate improved detection results through use\nof our approach as applied to the PASCAL VOC and COCO datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 17:59:42 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 19:08:40 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 21:51:54 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Barnea", "Ehud", ""], ["Ben-Shahar", "Ohad", ""]]}, {"id": "1711.05766", "submitter": "Zhipeng Ding", "authors": "Zhipeng Ding, Greg Fleishman, Xiao Yang, Paul Thompson, Roland Kwitt,\n  Marc Niethammer", "title": "Fast Predictive Simple Geodesic Regression", "comments": "19 pages, 10 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable image registration and regression are important tasks in medical\nimage analysis. However, they are computationally expensive, especially when\nanalyzing large-scale datasets that contain thousands of images. Hence, cluster\ncomputing is typically used, making the approaches dependent on such\ncomputational infrastructure. Even larger computational resources are required\nas study sizes increase. This limits the use of deformable image registration\nand regression for clinical applications and as component algorithms for other\nimage analysis approaches. We therefore propose using a fast predictive\napproach to perform image registrations. In particular, we employ these fast\nregistration predictions to approximate a simplified geodesic regression model\nto capture longitudinal brain changes. The resulting method is orders of\nmagnitude faster than the standard optimization-based regression model and\nhence facilitates large-scale analysis on a single graphics processing unit\n(GPU). We evaluate our results on 3D brain magnetic resonance images (MRI) from\nthe ADNI datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 19:30:20 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Ding", "Zhipeng", ""], ["Fleishman", "Greg", ""], ["Yang", "Xiao", ""], ["Thompson", "Paul", ""], ["Kwitt", "Roland", ""], ["Niethammer", "Marc", ""]]}, {"id": "1711.05769", "submitter": "Arun Mallya", "authors": "Arun Mallya and Svetlana Lazebnik", "title": "PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for adding multiple tasks to a single deep\nneural network while avoiding catastrophic forgetting. Inspired by network\npruning techniques, we exploit redundancies in large deep networks to free up\nparameters that can then be employed to learn new tasks. By performing\niterative pruning and network re-training, we are able to sequentially \"pack\"\nmultiple tasks into a single network while ensuring minimal drop in performance\nand minimal storage overhead. Unlike prior work that uses proxy losses to\nmaintain accuracy on older tasks, we always optimize for the task at hand. We\nperform extensive experiments on a variety of network architectures and\nlarge-scale datasets, and observe much better robustness against catastrophic\nforgetting than prior work. In particular, we are able to add three\nfine-grained classification tasks to a single ImageNet-trained VGG-16 network\nand achieve accuracies close to those of separately trained networks for each\ntask. Code available at https://github.com/arunmallya/packnet\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 19:36:51 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 16:48:51 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Mallya", "Arun", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1711.05775", "submitter": "Li Shen", "authors": "Li Shen", "title": "End-to-end Training for Whole Image Breast Cancer Diagnosis using An All\n  Convolutional Design", "comments": "Accepted poster at NIPS 2017 Workshop on Machine Learning for Health\n  (https://ml4health.github.io/2017/)", "journal-ref": "Scientific Reports, volume 9, Article number: 12495 (2019)", "doi": "10.1038/s41598-019-48995-4", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We develop an end-to-end training algorithm for whole-image breast cancer\ndiagnosis based on mammograms. It requires lesion annotations only at the first\nstage of training. After that, a whole image classifier can be trained using\nonly image level labels. This greatly reduced the reliance on lesion\nannotations. Our approach is implemented using an all convolutional design that\nis simple yet provides superior performance in comparison with the previous\nmethods. On DDSM, our best single-model achieves a per-image AUC score of 0.88\nand three-model averaging increases the score to 0.91. On INbreast, our best\nsingle-model achieves a per-image AUC score of 0.96. Using DDSM as benchmark,\nour models compare favorably with the current state-of-the-art. We also\ndemonstrate that a whole image model trained on DDSM can be easily transferred\nto INbreast without using its lesion annotations and using only a small amount\nof training data. Code availability: https://github.com/lishen/end2end-all-conv\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 19:52:27 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Shen", "Li", ""]]}, {"id": "1711.05792", "submitter": "Yukun Chen", "authors": "Yukun Chen, Jianbo Ye, and Jia Li", "title": "Aggregated Wasserstein Metric and State Registration for Hidden Markov\n  Models", "comments": "Our manuscript is based on our conference paper [arXiv:1608.01747]\n  published in 14th European Conference on Computer Vision (ECCV 2016,\n  spotlight). It has been significantly extended and is now in journal\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework, named Aggregated Wasserstein, for computing a\ndissimilarity measure or distance between two Hidden Markov Models with state\nconditional distributions being Gaussian. For such HMMs, the marginal\ndistribution at any time position follows a Gaussian mixture distribution, a\nfact exploited to softly match, aka register, the states in two HMMs. We refer\nto such HMMs as Gaussian mixture model-HMM (GMM-HMM). The registration of\nstates is inspired by the intrinsic relationship of optimal transport and the\nWasserstein metric between distributions. Specifically, the components of the\nmarginal GMMs are matched by solving an optimal transport problem where the\ncost between components is the Wasserstein metric for Gaussian distributions.\nThe solution of the optimization problem is a fast approximation to the\nWasserstein metric between two GMMs. The new Aggregated Wasserstein distance is\na semi-metric and can be computed without generating Monte Carlo samples. It is\ninvariant to relabeling or permutation of states. The distance is defined\nmeaningfully even for two HMMs that are estimated from data of different\ndimensionality, a situation that can arise due to missing variables. This\ndistance quantifies the dissimilarity of GMM-HMMs by measuring both the\ndifference between the two marginal GMMs and that between the two transition\nmatrices. Our new distance is tested on tasks of retrieval, classification, and\nt-SNE visualization of time series. Experiments on both synthetic and real data\nhave demonstrated its advantages in terms of accuracy as well as efficiency in\ncomparison with existing distances based on the Kullback-Leibler divergence.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 22:43:22 GMT"}, {"version": "v2", "created": "Sun, 19 Nov 2017 20:19:50 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Chen", "Yukun", ""], ["Ye", "Jianbo", ""], ["Li", "Jia", ""]]}, {"id": "1711.05805", "submitter": "Shiyu Song", "authors": "Guowei Wan, Xiaolong Yang, Renlan Cai, Hao Li, Hao Wang, Shiyu Song", "title": "Robust and Precise Vehicle Localization based on Multi-sensor Fusion in\n  Diverse City Scenes", "comments": "8 pages, 6 figures, 2 tables, Accepted by ICRA 2018", "journal-ref": "IEEE International Conference on Robotics and Automation (ICRA),\n  2018, pp. 4670-4677", "doi": "10.1109/ICRA.2018.8461224", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust and precise localization system that achieves\ncentimeter-level localization accuracy in disparate city scenes. Our system\nadaptively uses information from complementary sensors such as GNSS, LiDAR, and\nIMU to achieve high localization accuracy and resilience in challenging scenes,\nsuch as urban downtown, highways, and tunnels. Rather than relying only on\nLiDAR intensity or 3D geometry, we make innovative use of LiDAR intensity and\naltitude cues to significantly improve localization system accuracy and\nrobustness. Our GNSS RTK module utilizes the help of the multi-sensor fusion\nframework and achieves a better ambiguity resolution success rate. An\nerror-state Kalman filter is applied to fuse the localization measurements from\ndifferent sources with novel uncertainty estimation. We validate, in detail,\nthe effectiveness of our approaches, achieving 5-10cm RMS accuracy and\noutperforming previous state-of-the-art systems. Importantly, our system, while\ndeployed in a large autonomous driving fleet, made our vehicles fully\nautonomous in crowded city streets despite road construction that occurred from\ntime to time. A dataset including more than 60 km real traffic driving in\nvarious urban roads is used to comprehensively test our system.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 21:05:13 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 04:18:16 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Wan", "Guowei", ""], ["Yang", "Xiaolong", ""], ["Cai", "Renlan", ""], ["Li", "Hao", ""], ["Wang", "Hao", ""], ["Song", "Shiyu", ""]]}, {"id": "1711.05820", "submitter": "Wenlin Wang", "authors": "Wenlin Wang, Yunchen Pu, Vinay Kumar Verma, Kai Fan, Yizhe Zhang,\n  Changyou Chen, Piyush Rai, Lawrence Carin", "title": "Zero-Shot Learning via Class-Conditioned Deep Generative Models", "comments": "To appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep generative model for learning to predict classes not seen\nat training time. Unlike most existing methods for this problem, that represent\neach class as a point (via a semantic embedding), we represent each seen/unseen\nclass using a class-specific latent-space distribution, conditioned on class\nattributes. We use these latent-space distributions as a prior for a supervised\nvariational autoencoder (VAE), which also facilitates learning highly\ndiscriminative feature representations for the inputs. The entire framework is\nlearned end-to-end using only the seen-class training data. The model infers\ncorresponding attributes of a test image by maximizing the VAE lower bound; the\ninferred attributes may be linked to labels not seen when training. We further\nextend our model to a (1) semi-supervised/transductive setting by leveraging\nunlabeled unseen-class data via an unsupervised learning module, and (2)\nfew-shot learning where we also have a small number of labeled inputs from the\nunseen classes. We compare our model with several state-of-the-art methods\nthrough a comprehensive set of experiments on a variety of benchmark data sets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 21:53:11 GMT"}, {"version": "v2", "created": "Sun, 19 Nov 2017 23:32:05 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Wang", "Wenlin", ""], ["Pu", "Yunchen", ""], ["Verma", "Vinay Kumar", ""], ["Fan", "Kai", ""], ["Zhang", "Yizhe", ""], ["Chen", "Changyou", ""], ["Rai", "Piyush", ""], ["Carin", "Lawrence", ""]]}, {"id": "1711.05847", "submitter": "Tianfu Wu", "authors": "Xilai Li, Xi Song and Tianfu Wu", "title": "AOGNets: Compositional Grammatical Architectures for Deep Learning", "comments": "accepted to CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architectures are the foundation for improving performance of deep\nneural networks (DNNs). This paper presents deep compositional grammatical\narchitectures which harness the best of two worlds: grammar models and DNNs.\nThe proposed architectures integrate compositionality and reconfigurability of\nthe former and the capability of learning rich features of the latter in a\nprincipled way. We utilize AND-OR Grammar (AOG) as network generator in this\npaper and call the resulting networks AOGNets. An AOGNet consists of a number\nof stages each of which is composed of a number of AOG building blocks. An AOG\nbuilding block splits its input feature map into N groups along feature\nchannels and then treat it as a sentence of N words. It then jointly realizes a\nphrase structure grammar and a dependency grammar in bottom-up parsing the\n\"sentence\" for better feature exploration and reuse. It provides a unified\nframework for the best practices developed in state-of-the-art DNNs. In\nexperiments, AOGNet is tested in the CIFAR-10, CIFAR-100 and ImageNet-1K\nclassification benchmark and the MS-COCO object detection and segmentation\nbenchmark. In CIFAR-10, CIFAR-100 and ImageNet-1K, AOGNet obtains better\nperformance than ResNet and most of its variants, ResNeXt and its attention\nbased variants such as SENet, DenseNet and DualPathNet. AOGNet also obtains the\nbest model interpretability score using network dissection. AOGNet further\nshows better potential in adversarial defense. In MS-COCO, AOGNet obtains\nbetter performance than the ResNet and ResNeXt backbones in Mask R-CNN.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 23:42:45 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 19:21:26 GMT"}, {"version": "v3", "created": "Sat, 6 Apr 2019 14:04:30 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Li", "Xilai", ""], ["Song", "Xi", ""], ["Wu", "Tianfu", ""]]}, {"id": "1711.05852", "submitter": "Asit Mishra", "authors": "Asit Mishra, Debbie Marr", "title": "Apprentice: Using Knowledge Distillation Techniques To Improve\n  Low-Precision Network Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning networks have achieved state-of-the-art accuracies on computer\nvision workloads like image classification and object detection. The performant\nsystems, however, typically involve big models with numerous parameters. Once\ntrained, a challenging aspect for such top performing models is deployment on\nresource constrained inference systems - the models (often deep networks or\nwide networks or both) are compute and memory intensive. Low-precision numerics\nand model compression using knowledge distillation are popular techniques to\nlower both the compute requirements and memory footprint of these deployed\nmodels. In this paper, we study the combination of these two techniques and\nshow that the performance of low-precision networks can be significantly\nimproved by using knowledge distillation techniques. Our approach, Apprentice,\nachieves state-of-the-art accuracies using ternary precision and 4-bit\nprecision for variants of ResNet architecture on ImageNet dataset. We present\nthree schemes using which one can apply knowledge distillation techniques to\nvarious stages of the train-and-deploy pipeline.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 23:45:59 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Mishra", "Asit", ""], ["Marr", "Debbie", ""]]}, {"id": "1711.05858", "submitter": "Shima Kamyab", "authors": "Shima Kamyab, S. Zohreh Azimifar", "title": "End-to-end 3D shape inverse rendering of different classes of objects\n  from a single input image", "comments": "16 pages, 12 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a semi-supervised deep framework is proposed for the problem of\n3D shape inverse rendering from a single 2D input image. The main structure of\nproposed framework consists of unsupervised pre-trained components which\nsignificantly reduce the need to labeled data for training the whole framework.\nusing labeled data has the advantage of achieving to accurate results without\nthe need to predefined assumptions about image formation process. Three main\ncomponents are used in the proposed network: an encoder which maps 2D input\nimage to a representation space, a 3D decoder which decodes a representation to\na 3D structure and a mapping component in order to map 2D to 3D representation.\nThe only part that needs label for training is the mapping part with not too\nmany parameters. The other components in the network can be pre-trained\nunsupervised using only 2D images or 3D data in each case. The way of\nreconstructing 3D shapes in the decoder component, inspired by the model based\nmethods for 3D reconstruction, maps a low dimensional representation to 3D\nshape space with the advantage of extracting the basis vectors of shape space\nfrom training data itself and is not restricted to a small set of examples as\nused in predefined models. Therefore, the proposed framework deals directly\nwith coordinate values of the point cloud representation which leads to achieve\ndense 3D shapes in the output. The experimental results on several benchmark\ndatasets of objects and human faces and comparing with recent similar methods\nshows the power of proposed network in recovering more details from single 2D\nimages.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 19:13:57 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Kamyab", "Shima", ""], ["Azimifar", "S. Zohreh", ""]]}, {"id": "1711.05859", "submitter": "Sungmin Rhee", "authors": "Sungmin Rhee, Seokjun Seo, Sun Kim", "title": "Hybrid Approach of Relation Network and Localized Graph Convolutional\n  Filtering for Breast Cancer Subtype Classification", "comments": "8 pages, To be published in proceeding of IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network biology has been successfully used to help reveal complex mechanisms\nof disease, especially cancer. On the other hand, network biology requires\nin-depth knowledge to construct disease-specific networks, but our current\nknowledge is very limited even with the recent advances in human cancer\nbiology. Deep learning has shown a great potential to address the difficult\nsituation like this. However, deep learning technologies conventionally use\ngrid-like structured data, thus application of deep learning technologies to\nthe classification of human disease subtypes is yet to be explored. Recently,\ngraph based deep learning techniques have emerged, which becomes an opportunity\nto leverage analyses in network biology. In this paper, we proposed a hybrid\nmodel, which integrates two key components 1) graph convolution neural network\n(graph CNN) and 2) relation network (RN). We utilize graph CNN as a component\nto learn expression patterns of cooperative gene community, and RN as a\ncomponent to learn associations between learned patterns. The proposed model is\napplied to the PAM50 breast cancer subtype classification task, the standard\nbreast cancer subtype classification of clinical utility. In experiments of\nboth subtype classification and patient survival analysis, our proposed method\nachieved significantly better performances than existing methods. We believe\nthat this work is an important starting point to realize the upcoming\npersonalized medicine.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 15:15:31 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 09:10:25 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 05:29:23 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Rhee", "Sungmin", ""], ["Seo", "Seokjun", ""], ["Kim", "Sun", ""]]}, {"id": "1711.05860", "submitter": "Yufeng Hao", "authors": "Yufeng Hao", "title": "A General Neural Network Hardware Architecture on FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Field Programmable Gate Arrays (FPGAs) plays an increasingly important role\nin data sampling and processing industries due to its highly parallel\narchitecture, low power consumption, and flexibility in custom algorithms.\nEspecially, in the artificial intelligence field, for training and implement\nthe neural networks and machine learning algorithms, high energy efficiency\nhardware implement and massively parallel computing capacity are heavily\ndemanded. Therefore, many global companies have applied FPGAs into AI and\nMachine learning fields such as autonomous driving and Automatic Spoken\nLanguage Recognition (Baidu) [1] [2] and Bing search (Microsoft) [3].\nConsidering the FPGAs great potential in these fields, we tend to implement a\ngeneral neural network hardware architecture on XILINX ZU9CG System On Chip\n(SOC) platform [4], which contains abundant hardware resource and powerful\nprocessing capacity. The general neural network architecture on the FPGA SOC\nplatform can perform forward and backward algorithms in deep neural networks\n(DNN) with high performance and easily be adjusted according to the type and\nscale of the neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 19:17:58 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Hao", "Yufeng", ""]]}, {"id": "1711.05861", "submitter": "Yulong Wang", "authors": "Yulong Wang, Yuan Yan Tang, Luoqing Li, and Hong Chen", "title": "Modal Regression based Atomic Representation for Robust Face Recognition", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation based classification (RC) methods such as sparse RC (SRC) have\nshown great potential in face recognition in recent years. Most previous RC\nmethods are based on the conventional regression models, such as lasso\nregression, ridge regression or group lasso regression. These regression models\nessentially impose a predefined assumption on the distribution of the noise\nvariable in the query sample, such as the Gaussian or Laplacian distribution.\nHowever, the complicated noises in practice may violate the assumptions and\nimpede the performance of these RC methods. In this paper, we propose a modal\nregression based atomic representation and classification (MRARC) framework to\nalleviate such limitation. Unlike previous RC methods, the MRARC framework does\nnot require the noise variable to follow any specific predefined distributions.\nThis gives rise to the capability of MRARC in handling various complex noises\nin reality. Using MRARC as a general platform, we also develop four novel RC\nmethods for unimodal and multimodal face recognition, respectively. In\naddition, we devise a general optimization algorithm for the unified MRARC\nframework based on the alternating direction method of multipliers (ADMM) and\nhalf-quadratic theory. The experiments on real-world data validate the efficacy\nof MRARC for robust face recognition.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 02:27:43 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Wang", "Yulong", ""], ["Tang", "Yuan Yan", ""], ["Li", "Luoqing", ""], ["Chen", "Hong", ""]]}, {"id": "1711.05862", "submitter": "Andreas K\\\"olsch", "authors": "Andreas K\\\"olsch, Muhammad Zeshan Afzal, Markus Ebbecke, Marcus\n  Liwicki", "title": "Real-Time Document Image Classification using Deep CNN and Extreme\n  Learning Machines", "comments": null, "journal-ref": null, "doi": "10.1109/ICDAR.2017.217", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach for real-time training and testing for\ndocument image classification. In production environments, it is crucial to\nperform accurate and (time-)efficient training. Existing deep learning\napproaches for classifying documents do not meet these requirements, as they\nrequire much time for training and fine-tuning the deep architectures.\nMotivated from Computer Vision, we propose a two-stage approach. The first\nstage trains a deep network that works as feature extractor and in the second\nstage, Extreme Learning Machines (ELMs) are used for classification. The\nproposed approach outperforms all previously reported structural and deep\nlearning based methods with a final accuracy of 83.24% on Tobacco-3482 dataset,\nleading to a relative error reduction of 25% when compared to a previous\nConvolutional Neural Network (CNN) based approach (DeepDocClassifier). More\nimportantly, the training time of the ELM is only 1.176 seconds and the overall\nprediction time for 2,482 images is 3.066 seconds. As such, this novel approach\nmakes deep learning-based document classification suitable for large-scale\nreal-time applications.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 17:02:57 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["K\u00f6lsch", "Andreas", ""], ["Afzal", "Muhammad Zeshan", ""], ["Ebbecke", "Markus", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1711.05864", "submitter": "John Stechschulte", "authors": "John Stechschulte, Christoffer Heckman", "title": "Hidden Markov Random Field Iterative Closest Point", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When registering point clouds resolved from an underlying 2-D pixel\nstructure, such as those resulting from structured light and flash LiDAR\nsensors, or stereo reconstruction, it is expected that some points in one cloud\ndo not have corresponding points in the other cloud, and that these would occur\ntogether, such as along an edge of the depth map. In this work, a hidden Markov\nrandom field model is used to capture this prior within the framework of the\niterative closest point algorithm. The EM algorithm is used to estimate the\ndistribution parameters and the hidden component memberships. Experiments are\npresented demonstrating that this method outperforms several other outlier\nrejection methods when the point clouds have low or moderate overlap.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 23:12:26 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Stechschulte", "John", ""], ["Heckman", "Christoffer", ""]]}, {"id": "1711.05866", "submitter": "You Hao", "authors": "He Zhang, Hanlin Mo, You Hao, Shirui Li and Hua Li", "title": "Fast and Efficient Calculations of Structural Invariants of Chirality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chirality plays an important role in physics, chemistry, biology, and other\nfields. It describes an essential symmetry in structure. However, chirality\ninvariants are usually complicated in expression or difficult to evaluate. In\nthis paper, we present five general three-dimensional chirality invariants\nbased on the generating functions. And the five chiral invariants have four\ncharacteristics:(1) They play an important role in the detection of symmetry,\nespecially in the treatment of 'false zero' problem. (2) Three of the five\nchiral invariants decode an universal chirality index. (3) Three of them are\nproposed for the first time. (4) The five chiral invariants have low order no\nbigger than 4, brief expression, low time complexity O(n) and can act as\ndescriptors of three-dimensional objects in shape analysis. The five chiral\ninvariants give a geometric view to study the chiral invariants. And the\nexperiments show that the five chirality invariants are effective and\nefficient, they can be used as a tool for symmetry detection or features in\nshape analysis.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 06:53:58 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 07:21:47 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Zhang", "He", ""], ["Mo", "Hanlin", ""], ["Hao", "You", ""], ["Li", "Shirui", ""], ["Li", "Hua", ""]]}, {"id": "1711.05890", "submitter": "Yang Wang", "authors": "Yang Wang, Yi Yang, Zhenheng Yang, Liang Zhao, Peng Wang and Wei Xu", "title": "Occlusion Aware Unsupervised Learning of Optical Flow", "comments": "CVPR 2018 Camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been recently shown that a convolutional neural network can learn\noptical flow estimation with unsupervised learning. However, the performance of\nthe unsupervised methods still has a relatively large gap compared to its\nsupervised counterpart. Occlusion and large motion are some of the major\nfactors that limit the current unsupervised learning of optical flow methods.\nIn this work we introduce a new method which models occlusion explicitly and a\nnew warping way that facilitates the learning of large motion. Our method shows\npromising results on Flying Chairs, MPI-Sintel and KITTI benchmark datasets.\nEspecially on KITTI dataset where abundant unlabeled samples exist, our\nunsupervised method outperforms its counterpart trained with supervised\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 02:02:42 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 00:29:12 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Wang", "Yang", ""], ["Yang", "Yi", ""], ["Yang", "Zhenheng", ""], ["Zhao", "Liang", ""], ["Wang", "Peng", ""], ["Xu", "Wei", ""]]}, {"id": "1711.05908", "submitter": "Ruichi Yu", "authors": "Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I. Morariu,\n  Xintong Han, Mingfei Gao, Ching-Yung Lin, Larry S. Davis", "title": "NISP: Pruning Networks using Neuron Importance Score Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce the significant redundancy in deep Convolutional Neural Networks\n(CNNs), most existing methods prune neurons by only considering statistics of\nan individual layer or two consecutive layers (e.g., prune one layer to\nminimize the reconstruction error of the next layer), ignoring the effect of\nerror propagation in deep networks. In contrast, we argue that it is essential\nto prune neurons in the entire neuron network jointly based on a unified goal:\nminimizing the reconstruction error of important responses in the \"final\nresponse layer\" (FRL), which is the second-to-last layer before classification,\nfor a pruned network to retrain its predictive power. Specifically, we apply\nfeature ranking techniques to measure the importance of each neuron in the FRL,\nand formulate network pruning as a binary integer optimization problem and\nderive a closed-form solution to it for pruning neurons in earlier layers.\nBased on our theoretical analysis, we propose the Neuron Importance Score\nPropagation (NISP) algorithm to propagate the importance scores of final\nresponses to every neuron in the network. The CNN is pruned by removing neurons\nwith least importance, and then fine-tuned to retain its predictive power. NISP\nis evaluated on several datasets with multiple CNN models and demonstrated to\nachieve significant acceleration and compression with negligible accuracy loss.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 03:49:02 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 14:47:14 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 19:26:43 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Yu", "Ruichi", ""], ["Li", "Ang", ""], ["Chen", "Chun-Fu", ""], ["Lai", "Jui-Hsin", ""], ["Morariu", "Vlad I.", ""], ["Han", "Xintong", ""], ["Gao", "Mingfei", ""], ["Lin", "Ching-Yung", ""], ["Davis", "Larry S.", ""]]}, {"id": "1711.05918", "submitter": "Amir Rosenfeld", "authors": "Amir Rosenfeld, Mahdi Biparva, John K. Tsotsos", "title": "Priming Neural Networks", "comments": "fixed error in author name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual priming is known to affect the human visual system to allow detection\nof scene elements, even those that may have been near unnoticeable before, such\nas the presence of camouflaged animals. This process has been shown to be an\neffect of top-down signaling in the visual system triggered by the said cue. In\nthis paper, we propose a mechanism to mimic the process of priming in the\ncontext of object detection and segmentation. We view priming as having a\nmodulatory, cue dependent effect on layers of features within a network. Our\nresults show how such a process can be complementary to, and at times more\neffective than simple post-processing applied to the output of the network,\nnotably so in cases where the object is hard to detect such as in severe noise.\nMoreover, we find the effects of priming are sometimes stronger when early\nvisual layers are affected. Overall, our experiments confirm that top-down\nsignals can go a long way in improving object detection and segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 04:21:14 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 01:50:00 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Rosenfeld", "Amir", ""], ["Biparva", "Mahdi", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1711.05919", "submitter": "Ravi Garg", "authors": "Chamara Saroj Weerasekera, Ravi Garg, Yasir Latif, Ian Reid", "title": "Learning Deeply Supervised Good Features to Match for Dense Monocular\n  Reconstruction", "comments": "ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual SLAM (Simultaneous Localization and Mapping) methods typically rely on\nhandcrafted visual features or raw RGB values for establishing correspondences\nbetween images. These features, while suitable for sparse mapping, often lead\nto ambiguous matches in texture-less regions when performing dense\nreconstruction due to the aperture problem. In this work, we explore the use of\nlearned features for the matching task in dense monocular reconstruction. We\npropose a novel convolutional neural network (CNN) architecture along with a\ndeeply supervised feature learning scheme for pixel-wise regression of visual\ndescriptors from an image which are best suited for dense monocular SLAM. In\nparticular, our learning scheme minimizes a multi-view matching cost-volume\nloss with respect to the regressed features at multiple stages within the\nnetwork, for explicitly learning contextual features that are suitable for\ndense matching between images captured by a moving monocular camera along the\nepipolar line. We integrate the learned features from our model for depth\nestimation inside a real-time dense monocular SLAM framework, where photometric\nerror is replaced by our learned descriptor error. Our extensive evaluation on\nseveral challenging indoor datasets demonstrate greatly improved accuracy in\ndense reconstructions of the well celebrated dense SLAM systems like DTAM,\nwithout compromising their real-time performance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 04:22:49 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 06:12:22 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Weerasekera", "Chamara Saroj", ""], ["Garg", "Ravi", ""], ["Latif", "Yasir", ""], ["Reid", "Ian", ""]]}, {"id": "1711.05929", "submitter": "Naveed Akhtar Dr.", "authors": "Naveed Akhtar and Jian Liu and Ajmal Mian", "title": "Defense against Universal Adversarial Perturbations", "comments": "Accepted in IEEE CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Deep Learning show the existence of image-agnostic\nquasi-imperceptible perturbations that when applied to `any' image can fool a\nstate-of-the-art network classifier to change its prediction about the image\nlabel. These `Universal Adversarial Perturbations' pose a serious threat to the\nsuccess of Deep Learning in practice. We present the first dedicated framework\nto effectively defend the networks against such perturbations. Our approach\nlearns a Perturbation Rectifying Network (PRN) as `pre-input' layers to a\ntargeted model, such that the targeted model needs no modification. The PRN is\nlearned from real and synthetic image-agnostic perturbations, where an\nefficient method to compute the latter is also proposed. A perturbation\ndetector is separately trained on the Discrete Cosine Transform of the\ninput-output difference of the PRN. A query image is first passed through the\nPRN and verified by the detector. If a perturbation is detected, the output of\nthe PRN is used for label prediction instead of the actual image. A rigorous\nevaluation shows that our framework can defend the network classifiers against\nunseen adversarial perturbations in the real-world scenarios with up to 97.5%\nsuccess rate. The PRN also generalizes well in the sense that training for one\ntargeted network defends another network with a comparable success rate.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 05:08:49 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 08:46:09 GMT"}, {"version": "v3", "created": "Wed, 28 Feb 2018 05:45:34 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Akhtar", "Naveed", ""], ["Liu", "Jian", ""], ["Mian", "Ajmal", ""]]}, {"id": "1711.05934", "submitter": "Yujia Liu", "authors": "Yujia Liu, Weiming Zhang, Shaohua Li, Nenghai Yu", "title": "Enhanced Attacks on Defensively Distilled Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved tremendous success in many tasks of\nmachine learning, such as the image classification. Unfortunately, researchers\nhave shown that DNNs are easily attacked by adversarial examples, slightly\nperturbed images which can mislead DNNs to give incorrect classification\nresults. Such attack has seriously hampered the deployment of DNN systems in\nareas where security or safety requirements are strict, such as autonomous\ncars, face recognition, malware detection. Defensive distillation is a\nmechanism aimed at training a robust DNN which significantly reduces the\neffectiveness of adversarial examples generation. However, the state-of-the-art\nattack can be successful on distilled networks with 100% probability. But it is\na white-box attack which needs to know the inner information of DNN. Whereas,\nthe black-box scenario is more general. In this paper, we first propose the\nepsilon-neighborhood attack, which can fool the defensively distilled networks\nwith 100% success rate in the white-box setting, and it is fast to generate\nadversarial examples with good visual quality. On the basis of this attack, we\nfurther propose the region-based attack against defensively distilled DNNs in\nthe black-box setting. And we also perform the bypass attack to indirectly\nbreak the distillation defense as a complementary method. The experimental\nresults show that our black-box attacks have a considerable success rate on\ndefensively distilled networks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 05:37:14 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Liu", "Yujia", ""], ["Zhang", "Weiming", ""], ["Li", "Shaohua", ""], ["Yu", "Nenghai", ""]]}, {"id": "1711.05941", "submitter": "Naveed Akhtar Dr.", "authors": "Jian Liu, Naveed Akhtar, Ajmal Mian", "title": "Skepxels: Spatio-temporal Image Representation of Human Skeleton Joints\n  for Action Recognition", "comments": "Submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human skeleton joints are popular for action analysis since they can be\neasily extracted from videos to discard background noises. However, current\nskeleton representations do not fully benefit from machine learning with CNNs.\nWe propose \"Skepxels\" a spatio-temporal representation for skeleton sequences\nto fully exploit the \"local\" correlations between joints using the 2D\nconvolution kernels of CNN. We transform skeleton videos into images of\nflexible dimensions using Skepxels and develop a CNN-based framework for\neffective human action recognition using the resulting images. Skepxels encode\nrich spatio-temporal information about the skeleton joints in the frames by\nmaximizing a unique distance metric, defined collaboratively over the distinct\njoint arrangements used in the skeletal image. Moreover, they are flexible in\nencoding compound semantic notions such as location and speed of the joints.\nThe proposed action recognition exploits the representation in a hierarchical\nmanner by first capturing the micro-temporal relations between the skeleton\njoints with the Skepxels and then exploiting their macro-temporal relations by\ncomputing the Fourier Temporal Pyramids over the CNN features of the skeletal\nimages. We extend the Inception-ResNet CNN architecture with the proposed\nmethod and improve the state-of-the-art accuracy by 4.4% on the large scale NTU\nhuman activity dataset. On the medium-sized N-UCLA and UTH-MHAD datasets, our\nmethod outperforms the existing results by 5.7% and 9.3% respectively.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 06:03:52 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 11:44:46 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 06:37:21 GMT"}, {"version": "v4", "created": "Fri, 3 Aug 2018 06:57:39 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Liu", "Jian", ""], ["Akhtar", "Naveed", ""], ["Mian", "Ajmal", ""]]}, {"id": "1711.05942", "submitter": "Syed Zulqarnain Gilani", "authors": "Syed Zulqarnain Gilani and Ajmal Mian", "title": "Learning from Millions of 3D Scans for Large-scale 3D Face Recognition", "comments": "11 pages", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition, 2018", "doi": "10.1109/CVPR.2018.00203", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks trained on millions of facial images are believed to be closely\napproaching human-level performance in face recognition. However, open world\nface recognition still remains a challenge. Although, 3D face recognition has\nan inherent edge over its 2D counterpart, it has not benefited from the recent\ndevelopments in deep learning due to the unavailability of large training as\nwell as large test datasets. Recognition accuracies have already saturated on\nexisting 3D face datasets due to their small gallery sizes. Unlike 2D\nphotographs, 3D facial scans cannot be sourced from the web causing a\nbottleneck in the development of deep 3D face recognition networks and\ndatasets. In this backdrop, we propose a method for generating a large corpus\nof labeled 3D face identities and their multiple instances for training and a\nprotocol for merging the most challenging existing 3D datasets for testing. We\nalso propose the first deep CNN model designed specifically for 3D face\nrecognition and trained on 3.1 Million 3D facial scans of 100K identities. Our\ntest dataset comprises 1,853 identities with a single 3D scan in the gallery\nand another 31K scans as probes, which is several orders of magnitude larger\nthan existing ones. Without fine tuning on this dataset, our network already\noutperforms state of the art face recognition by over 10%. We fine tune our\nnetwork on the gallery set to perform end-to-end large scale 3D face\nrecognition which further improves accuracy. Finally, we show the efficacy of\nour method for the open world face recognition problem.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 06:07:10 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 09:02:11 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 07:24:59 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Gilani", "Syed Zulqarnain", ""], ["Mian", "Ajmal", ""]]}, {"id": "1711.05944", "submitter": "Abhishake Kumar Bojja", "authors": "Abhishake Kumar Bojja, Franziska Mueller, Sri Raghu Malireddi, Markus\n  Oberweger, Vincent Lepetit, Christian Theobalt, Kwang Moo Yi, Andrea\n  Tagliasacchi", "title": "HandSeg: An Automatically Labeled Dataset for Hand Segmentation from\n  Depth Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automatic method for generating high-quality annotations for\ndepth-based hand segmentation, and introduce a large-scale hand segmentation\ndataset. Existing datasets are typically limited to a single hand. By\nexploiting the visual cues given by an RGBD sensor and a pair of colored\ngloves, we automatically generate dense annotations for two hand segmentation.\nThis lowers the cost/complexity of creating high quality datasets, and makes it\neasy to expand the dataset in the future. We further show that existing\ndatasets, even with data augmentation, are not sufficient to train a hand\nsegmentation algorithm that can distinguish two hands. Source and datasets will\nbe made publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 06:14:11 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 01:46:33 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 21:41:23 GMT"}, {"version": "v4", "created": "Thu, 2 Aug 2018 20:17:08 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Bojja", "Abhishake Kumar", ""], ["Mueller", "Franziska", ""], ["Malireddi", "Sri Raghu", ""], ["Oberweger", "Markus", ""], ["Lepetit", "Vincent", ""], ["Theobalt", "Christian", ""], ["Yi", "Kwang Moo", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "1711.05953", "submitter": "Syed Zulqarnain Gilani", "authors": "Mingtao Feng, Syed Zulqarnain Gilani, Yaonan Wang and Ajmal Mian", "title": "3D Face Reconstruction from Light Field Images: A Model-free Approach", "comments": null, "journal-ref": "European Conference on Computer Vision (ECCV), 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing 3D facial geometry from a single RGB image has recently\ninstigated wide research interest. However, it is still an ill-posed problem\nand most methods rely on prior models hence undermining the accuracy of the\nrecovered 3D faces. In this paper, we exploit the Epipolar Plane Images (EPI)\nobtained from light field cameras and learn CNN models that recover horizontal\nand vertical 3D facial curves from the respective horizontal and vertical EPIs.\nOur 3D face reconstruction network (FaceLFnet) comprises a densely connected\narchitecture to learn accurate 3D facial curves from low resolution EPIs. To\ntrain the proposed FaceLFnets from scratch, we synthesize photo-realistic light\nfield images from 3D facial scans. The curve by curve 3D face estimation\napproach allows the networks to learn from only 14K images of 80 identities,\nwhich still comprises over 11 Million EPIs/curves. The estimated facial curves\nare merged into a single pointcloud to which a surface is fitted to get the\nfinal 3D face. Our method is model-free, requires only a few training samples\nto learn FaceLFnet and can reconstruct 3D faces with high accuracy from single\nlight field images under varying poses, expressions and lighting conditions.\nComparison on the BU-3DFE and BU-4DFE datasets show that our method reduces\nreconstruction errors by over 20% compared to recent state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 06:50:19 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 01:35:09 GMT"}, {"version": "v3", "created": "Fri, 12 Jan 2018 03:44:32 GMT"}, {"version": "v4", "created": "Thu, 5 Jul 2018 08:29:30 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Feng", "Mingtao", ""], ["Gilani", "Syed Zulqarnain", ""], ["Wang", "Yaonan", ""], ["Mian", "Ajmal", ""]]}, {"id": "1711.05954", "submitter": "Qingyi Tao", "authors": "Qingyi Tao, Hao Yang, Jianfei Cai", "title": "Zero-Annotation Object Detection with Web Knowledge Transfer", "comments": "Accepted in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is one of the major problems in computer vision, and has\nbeen extensively studied. Most of the existing detection works rely on\nlabor-intensive supervision, such as ground truth bounding boxes of objects or\nat least image-level annotations. On the contrary, we propose an object\ndetection method that does not require any form of human annotation on target\ntasks, by exploiting freely available web images. In order to facilitate\neffective knowledge transfer from web images, we introduce a multi-instance\nmulti-label domain adaption learning framework with two key innovations. First\nof all, we propose an instance-level adversarial domain adaptation network with\nattention on foreground objects to transfer the object appearances from web\ndomain to target domain. Second, to preserve the class-specific semantic\nstructure of transferred object features, we propose a simultaneous transfer\nmechanism to transfer the supervision across domains through pseudo strong\nlabel generation. With our end-to-end framework that simultaneously learns a\nweakly supervised detector and transfers knowledge across domains, we achieved\nsignificant improvements over baseline methods on the benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 06:51:40 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 07:44:52 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Tao", "Qingyi", ""], ["Yang", "Hao", ""], ["Cai", "Jianfei", ""]]}, {"id": "1711.05959", "submitter": "Heechul Jung", "authors": "Heechul Jung, Jeongwoo Ju, Minju Jung, Junmo Kim", "title": "Less-forgetful Learning for Domain Expansion in Deep Neural Networks", "comments": "8 pages, accepted to AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expanding the domain that deep neural network has already learned without\naccessing old domain data is a challenging task because deep neural networks\nforget previously learned information when learning new data from a new domain.\nIn this paper, we propose a less-forgetful learning method for the domain\nexpansion scenario. While existing domain adaptation techniques solely focused\non adapting to new domains, the proposed technique focuses on working well with\nboth old and new domains without needing to know whether the input is from the\nold or new domain. First, we present two naive approaches which will be\nproblematic, then we provide a new method using two proposed properties for\nless-forgetful learning. Finally, we prove the effectiveness of our method\nthrough experiments on image classification tasks. All datasets used in the\npaper, will be released on our website for someone's follow-up study.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 07:04:51 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Jung", "Heechul", ""], ["Ju", "Jeongwoo", ""], ["Jung", "Minju", ""], ["Kim", "Junmo", ""]]}, {"id": "1711.05971", "submitter": "Kwang Moo Yi", "authors": "Kwang Moo Yi, Eduard Trulls, Yuki Ono, Vincent Lepetit, Mathieu\n  Salzmann, Pascal Fua", "title": "Learning to Find Good Correspondences", "comments": "CVPR 2018 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a deep architecture to learn to find good correspondences for\nwide-baseline stereo. Given a set of putative sparse matches and the camera\nintrinsics, we train our network in an end-to-end fashion to label the\ncorrespondences as inliers or outliers, while simultaneously using them to\nrecover the relative pose, as encoded by the essential matrix. Our architecture\nis based on a multi-layer perceptron operating on pixel coordinates rather than\ndirectly on the image, and is thus simple and small. We introduce a novel\nnormalization technique, called Context Normalization, which allows us to\nprocess each data point separately while imbuing it with global information,\nand also makes the network invariant to the order of the correspondences. Our\nexperiments on multiple challenging datasets demonstrate that our method is\nable to drastically improve the state of the art with little training data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 07:54:57 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 14:26:17 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Yi", "Kwang Moo", ""], ["Trulls", "Eduard", ""], ["Ono", "Yuki", ""], ["Lepetit", "Vincent", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1711.05998", "submitter": "Satoshi Tsutsui", "authors": "Satoshi Tsutsui, Tommi Kerola, Shunta Saito, David J. Crandall", "title": "Minimizing Supervision for Free-space Segmentation", "comments": "Link to source code added; Typo fixed from the version published in\n  CVPR 2018 Workshop on Autonomous Driving (WAD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying \"free-space,\" or safely driveable regions in the scene ahead, is\na fundamental task for autonomous navigation. While this task can be addressed\nusing semantic segmentation, the manual labor involved in creating pixelwise\nannotations to train the segmentation model is very costly. Although weakly\nsupervised segmentation addresses this issue, most methods are not designed for\nfree-space. In this paper, we observe that homogeneous texture and location are\ntwo key characteristics of free-space, and develop a novel, practical framework\nfor free-space segmentation with minimal human supervision. Our experiments\nshow that our framework performs better than other weakly supervised methods\nwhile using less supervision. Our work demonstrates the potential for\nperforming free-space segmentation without tedious and costly manual\nannotation, which will be important for adapting autonomous driving systems to\ndifferent types of vehicles and environments\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 09:24:15 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 10:15:53 GMT"}, {"version": "v3", "created": "Sat, 8 Dec 2018 20:17:34 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Tsutsui", "Satoshi", ""], ["Kerola", "Tommi", ""], ["Saito", "Shunta", ""], ["Crandall", "David J.", ""]]}, {"id": "1711.06011", "submitter": "Gautam Pai", "authors": "Gautam Pai, Ronen Talmon, Alex Bronstein, Ron Kimmel", "title": "DIMAL: Deep Isometric Manifold Learning Using Sparse Geodesic Sampling", "comments": "10 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores a fully unsupervised deep learning approach for computing\ndistance-preserving maps that generate low-dimensional embeddings for a certain\nclass of manifolds. We use the Siamese configuration to train a neural network\nto solve the problem of least squares multidimensional scaling for generating\nmaps that approximately preserve geodesic distances. By training with only a\nfew landmarks, we show a significantly improved local and nonlocal\ngeneralization of the isometric mapping as compared to analogous non-parametric\ncounterparts. Importantly, the combination of a deep-learning framework with a\nmultidimensional scaling objective enables a numerical analysis of network\narchitectures to aid in understanding their representation power. This provides\na geometric perspective to the generalizability of deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 10:28:43 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 15:52:33 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Pai", "Gautam", ""], ["Talmon", "Ronen", ""], ["Bronstein", "Alex", ""], ["Kimmel", "Ron", ""]]}, {"id": "1711.06016", "submitter": "Deng Cai", "authors": "Deng Cai and Xiuye Gu and Chaoqi Wang", "title": "A Revisit on Deep Hashings for Large-scale Content Based Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing trend in studying deep hashing methods for content-based\nimage retrieval (CBIR), where hash functions and binary codes are learnt using\ndeep convolutional neural networks and then the binary codes can be used to do\napproximate nearest neighbor (ANN) search. All the existing deep hashing papers\nreport their methods' superior performance over the traditional hashing methods\naccording to their experimental results. However, there are serious flaws in\nthe evaluations of existing deep hashing papers: (1) The datasets they used are\ntoo small and simple to simulate the real CBIR situation. (2) They did not\ncorrectly include the search time in their evaluation criteria, while the\nsearch time is crucial in real CBIR systems. (3) The performance of some\nunsupervised hashing algorithms (e.g., LSH) can easily be boosted if one uses\nmultiple hash tables, which is an important factor should be considered in the\nevaluation while most of the deep hashing papers failed to do so.\n  We re-evaluate several state-of-the-art deep hashing methods with a carefully\ndesigned experimental setting. Empirical results reveal that the performance of\nthese deep hashing methods are inferior to multi-table IsoH, a very simple\nunsupervised hashing method. Thus, the conclusions in all the deep hashing\npapers should be carefully re-examined.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 10:45:39 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Cai", "Deng", ""], ["Gu", "Xiuye", ""], ["Wang", "Chaoqi", ""]]}, {"id": "1711.06020", "submitter": "Guo-Jun Qi", "authors": "Guo-Jun Qi, Liheng Zhang, Hao Hu, Marzieh Edraki, Jingdong Wang and\n  Xian-Sheng Hua", "title": "Global versus Localized Generative Adversarial Nets", "comments": null, "journal-ref": "Proceedings of of IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2018), Salt Lake City, Utah, June 18th - June 22nd, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel localized Generative Adversarial Net (GAN)\nto learn on the manifold of real data. Compared with the classic GAN that {\\em\nglobally} parameterizes a manifold, the Localized GAN (LGAN) uses local\ncoordinate charts to parameterize distinct local geometry of how data points\ncan transform at different locations on the manifold. Specifically, around each\npoint there exists a {\\em local} generator that can produce data following\ndiverse patterns of transformations on the manifold. The locality nature of\nLGAN enables local generators to adapt to and directly access the local\ngeometry without need to invert the generator in a global GAN. Furthermore, it\ncan prevent the manifold from being locally collapsed to a dimensionally\ndeficient tangent subspace by imposing an orthonormality prior between\ntangents. This provides a geometric approach to alleviating mode collapse at\nleast locally on the manifold by imposing independence between data\ntransformations in different tangent directions. We will also demonstrate the\nLGAN can be applied to train a robust classifier that prefers locally\nconsistent classification decisions on the manifold, and the resultant\nregularizer is closely related with the Laplace-Beltrami operator. Our\nexperiments show that the proposed LGANs can not only produce diverse image\ntransformations, but also deliver superior classification performances.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 10:53:53 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 20:47:34 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Qi", "Guo-Jun", ""], ["Zhang", "Liheng", ""], ["Hu", "Hao", ""], ["Edraki", "Marzieh", ""], ["Wang", "Jingdong", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "1711.06025", "submitter": "Li Zhang", "authors": "Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr,\n  Timothy M. Hospedales", "title": "Learning to Compare: Relation Network for Few-Shot Learning", "comments": "To appear in CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a conceptually simple, flexible, and general framework for\nfew-shot learning, where a classifier must learn to recognise new classes given\nonly few examples from each. Our method, called the Relation Network (RN), is\ntrained end-to-end from scratch. During meta-learning, it learns to learn a\ndeep distance metric to compare a small number of images within episodes, each\nof which is designed to simulate the few-shot setting. Once trained, a RN is\nable to classify images of new classes by computing relation scores between\nquery images and the few examples of each new class without further updating\nthe network. Besides providing improved performance on few-shot learning, our\nframework is easily extended to zero-shot learning. Extensive experiments on\nfive benchmarks demonstrate that our simple approach provides a unified and\neffective approach for both of these two tasks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 11:01:51 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 10:55:50 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Sung", "Flood", ""], ["Yang", "Yongxin", ""], ["Zhang", "Li", ""], ["Xiang", "Tao", ""], ["Torr", "Philip H. S.", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1711.06032", "submitter": "Michael Ying Yang", "authors": "Wentong Liao, Lin Shuai, Bodo Rosenhahn, Michael Ying Yang", "title": "Natural Language Guided Visual Relationship Detection", "comments": "added supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning about the relationships between object pairs in images is a crucial\ntask for holistic scene understanding. Most of the existing works treat this\ntask as a pure visual classification task: each type of relationship or phrase\nis classified as a relation category based on the extracted visual features.\nHowever, each kind of relationships has a wide variety of object combination\nand each pair of objects has diverse interactions. Obtaining sufficient\ntraining samples for all possible relationship categories is difficult and\nexpensive. In this work, we propose a natural language guided framework to\ntackle this problem. We propose to use a generic bi-directional recurrent\nneural network to predict the semantic connection between the participating\nobjects in the relationship from the aspect of natural language. The proposed\nsimple method achieves the state-of-the-art on the Visual Relationship\nDetection (VRD) and Visual Genome datasets, especially when predicting unseen\nrelationships (e.g. recall improved from 76.42% to 89.79% on VRD zero-shot\ntesting set).\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 11:26:19 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 10:51:31 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Liao", "Wentong", ""], ["Shuai", "Lin", ""], ["Rosenhahn", "Bodo", ""], ["Yang", "Michael Ying", ""]]}, {"id": "1711.06045", "submitter": "Jose Caballero", "authors": "Joost van Amersfoort, Wenzhe Shi, Alejandro Acosta, Francisco Massa,\n  Johannes Totz, Zehan Wang, Jose Caballero", "title": "Frame Interpolation with Multi-Scale Deep Loss Functions and Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frame interpolation attempts to synthesise frames given one or more\nconsecutive video frames. In recent years, deep learning approaches, and\nnotably convolutional neural networks, have succeeded at tackling low- and\nhigh-level computer vision problems including frame interpolation. These\ntechniques often tackle two problems, namely algorithm efficiency and\nreconstruction quality. In this paper, we present a multi-scale generative\nadversarial network for frame interpolation (\\mbox{FIGAN}). To maximise the\nefficiency of our network, we propose a novel multi-scale residual estimation\nmodule where the predicted flow and synthesised frame are constructed in a\ncoarse-to-fine fashion. To improve the quality of synthesised intermediate\nvideo frames, our network is jointly supervised at different levels with a\nperceptual loss function that consists of an adversarial and two content\nlosses. We evaluate the proposed approach using a collection of 60fps videos\nfrom YouTube-8m. Our results improve the state-of-the-art accuracy and provide\nsubjective visual quality comparable to the best performing interpolation\nmethod at x47 faster runtime.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 11:46:16 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 14:58:05 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["van Amersfoort", "Joost", ""], ["Shi", "Wenzhe", ""], ["Acosta", "Alejandro", ""], ["Massa", "Francisco", ""], ["Totz", "Johannes", ""], ["Wang", "Zehan", ""], ["Caballero", "Jose", ""]]}, {"id": "1711.06047", "submitter": "Makoto Yamada", "authors": "Tanmoy Mukherjee, Makoto Yamada, Timothy M. Hospedales", "title": "Deep Matching Autoencoders", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly many real world tasks involve data in multiple modalities or\nviews. This has motivated the development of many effective algorithms for\nlearning a common latent space to relate multiple domains. However, most\nexisting cross-view learning algorithms assume access to paired data for\ntraining. Their applicability is thus limited as the paired data assumption is\noften violated in practice: many tasks have only a small subset of data\navailable with pairing annotation, or even no paired data at all. In this paper\nwe introduce Deep Matching Autoencoders (DMAE), which learn a common latent\nspace and pairing from unpaired multi-modal data. Specifically we formulate\nthis as a cross-domain representation learning and object matching problem. We\nsimultaneously optimise parameters of representation learning auto-encoders and\nthe pairing of unpaired multi-modal data. This framework elegantly spans the\nfull regime from fully supervised, semi-supervised, and unsupervised (no paired\ndata) multi-modal learning. We show promising results in image captioning, and\non a new task that is uniquely enabled by our methodology: unsupervised\nclassifier learning.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 11:50:41 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Mukherjee", "Tanmoy", ""], ["Yamada", "Makoto", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1711.06055", "submitter": "Jianshu Li", "authors": "Jianshu Li, Shengtao Xiao, Fang Zhao, Jian Zhao, Jianan Li, Jiashi\n  Feng, Shuicheng Yan, Terence Sim", "title": "Integrated Face Analytics Networks through Cross-Dataset Hybrid Training", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face analytics benefits many multimedia applications. It consists of a number\nof tasks, such as facial emotion recognition and face parsing, and most\nexisting approaches generally treat these tasks independently, which limits\ntheir deployment in real scenarios. In this paper we propose an integrated Face\nAnalytics Network (iFAN), which is able to perform multiple tasks jointly for\nface analytics with a novel carefully designed network architecture to fully\nfacilitate the informative interaction among different tasks. The proposed\nintegrated network explicitly models the interactions between tasks so that the\ncorrelations between tasks can be fully exploited for performance boost. In\naddition, to solve the bottleneck of the absence of datasets with comprehensive\ntraining data for various tasks, we propose a novel cross-dataset hybrid\ntraining strategy. It allows \"plug-in and play\" of multiple datasets annotated\nfor different tasks without the requirement of a fully labeled common dataset\nfor all the tasks. We experimentally show that the proposed iFAN achieves\nstate-of-the-art performance on multiple face analytics tasks using a single\nintegrated model. Specifically, iFAN achieves an overall F-score of 91.15% on\nthe Helen dataset for face parsing, a normalized mean error of 5.81% on the\nMTFL dataset for facial landmark localization and an accuracy of 45.73% on the\nBNU dataset for emotion recognition with a single model.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 12:09:40 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Li", "Jianshu", ""], ["Xiao", "Shengtao", ""], ["Zhao", "Fang", ""], ["Zhao", "Jian", ""], ["Li", "Jianan", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""], ["Sim", "Terence", ""]]}, {"id": "1711.06077", "submitter": "Yochai Blau", "authors": "Yochai Blau and Tomer Michaeli", "title": "The Perception-Distortion Tradeoff", "comments": "CVPR 2018 (long oral presentation), see talk at:\n  https://youtu.be/_aXbGqdEkjk?t=39m43s", "journal-ref": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition, pp. 6228-6237, 2018", "doi": "10.1109/CVPR.2018.00652", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image restoration algorithms are typically evaluated by some distortion\nmeasure (e.g. PSNR, SSIM, IFC, VIF) or by human opinion scores that quantify\nperceived perceptual quality. In this paper, we prove mathematically that\ndistortion and perceptual quality are at odds with each other. Specifically, we\nstudy the optimal probability for correctly discriminating the outputs of an\nimage restoration algorithm from real images. We show that as the mean\ndistortion decreases, this probability must increase (indicating worse\nperceptual quality). As opposed to the common belief, this result holds true\nfor any distortion measure, and is not only a problem of the PSNR or SSIM\ncriteria. We also show that generative-adversarial-nets (GANs) provide a\nprincipled way to approach the perception-distortion bound. This constitutes\ntheoretical support to their observed success in low-level vision tasks. Based\non our analysis, we propose a new methodology for evaluating image restoration\nmethods, and use it to perform an extensive comparison between recent\nsuper-resolution algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 13:22:30 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2018 09:14:05 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2019 16:16:13 GMT"}, {"version": "v4", "created": "Sun, 25 Oct 2020 07:29:01 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Blau", "Yochai", ""], ["Michaeli", "Tomer", ""]]}, {"id": "1711.06078", "submitter": "Bin Liu", "authors": "Dan Ma, Bin Liu, Zhao Kang, Jiayu Zhou, Jianke Zhu, Zenglin Xu", "title": "Two Birds with One Stone: Transforming and Generating Facial Images with\n  Iterative GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating high fidelity identity-preserving faces with different facial\nattributes has a wide range of applications. Although a number of generative\nmodels have been developed to tackle this problem, there is still much room for\nfurther improvement.In paticular, the current solutions usually ignore the\nperceptual information of images, which we argue that it benefits the output of\na high-quality image while preserving the identity information, especially in\nfacial attributes learning area.To this end, we propose to train GAN\niteratively via regularizing the min-max process with an integrated loss, which\nincludes not only the per-pixel loss but also the perceptual loss. In contrast\nto the existing methods only deal with either image generation or\ntransformation, our proposed iterative architecture can achieve both of them.\nExperiments on the multi-label facial dataset CelebA demonstrate that the\nproposed model has excellent performance on recognizing multiple attributes,\ngenerating a high-quality image, and transforming image with controllable\nattributes.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 13:23:20 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 02:03:26 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Ma", "Dan", ""], ["Liu", "Bin", ""], ["Kang", "Zhao", ""], ["Zhou", "Jiayu", ""], ["Zhu", "Jianke", ""], ["Xu", "Zenglin", ""]]}, {"id": "1711.06095", "submitter": "Radu-Laurentiu Vieriu", "authors": "Evgeny Stepanov, Stephane Lathuiliere, Shammur Absar Chowdhury,\n  Arindam Ghosh, Radu-Laurentiu Vieriu, Nicu Sebe, Giuseppe Riccardi", "title": "Depression Severity Estimation from Multiple Modalities", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depression is a major debilitating disorder which can affect people from all\nages. With a continuous increase in the number of annual cases of depression,\nthere is a need to develop automatic techniques for the detection of the\npresence and extent of depression. In this AVEC challenge we explore different\nmodalities (speech, language and visual features extracted from face) to design\nand develop automatic methods for the detection of depression. In psychology\nliterature, the PHQ-8 questionnaire is well established as a tool for measuring\nthe severity of depression. In this paper we aim to automatically predict the\nPHQ-8 scores from features extracted from the different modalities. We show\nthat visual features extracted from facial landmarks obtain the best\nperformance in terms of estimating the PHQ-8 results with a mean absolute error\n(MAE) of 4.66 on the development set. Behavioral characteristics from speech\nprovide an MAE of 4.73. Language features yield a slightly higher MAE of 5.17.\nWhen switching to the test set, our Turn Features derived from audio\ntranscriptions achieve the best performance, scoring an MAE of 4.11\n(corresponding to an RMSE of 4.94), which makes our system the winner of the\nAVEC 2017 depression sub-challenge.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 12:47:52 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Stepanov", "Evgeny", ""], ["Lathuiliere", "Stephane", ""], ["Chowdhury", "Shammur Absar", ""], ["Ghosh", "Arindam", ""], ["Vieriu", "Radu-Laurentiu", ""], ["Sebe", "Nicu", ""], ["Riccardi", "Giuseppe", ""]]}, {"id": "1711.06106", "submitter": "Avisek Lahiri", "authors": "Avisek Lahiri, Arnav Jain, Prabir Kumar Biswas, Pabitra Mitra", "title": "Improving Consistency and Correctness of Sequence Inpainting using\n  Semantically Guided Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary benchmark methods for image inpainting are based on deep\ngenerative models and specifically leverage adversarial loss for yielding\nrealistic reconstructions. However, these models cannot be directly applied on\nimage/video sequences because of an intrinsic drawback- the reconstructions\nmight be independently realistic, but, when visualized as a sequence, often\nlacks fidelity to the original uncorrupted sequence. The fundamental reason is\nthat these methods try to find the best matching latent space representation\nnear to natural image manifold without any explicit distance based loss. In\nthis paper, we present a semantically conditioned Generative Adversarial\nNetwork (GAN) for sequence inpainting. The conditional information constrains\nthe GAN to map a latent representation to a point in image manifold respecting\nthe underlying pose and semantics of the scene. To the best of our knowledge,\nthis is the first work which simultaneously addresses consistency and\ncorrectness of generative model based inpainting. We show that our generative\nmodel learns to disentangle pose and appearance information; this independence\nis exploited by our model to generate highly consistent reconstructions. The\nconditional information also aids the generator network in GAN to produce\nsharper images compared to the original GAN formulation. This helps in\nachieving more appealing inpainting performance. Though generic, our algorithm\nwas targeted for inpainting on faces. When applied on CelebA and Youtube Faces\ndatasets, the proposed method results in a significant improvement over the\ncurrent benchmark, both in terms of quantitative evaluation (Peak Signal to\nNoise Ratio) and human visual scoring over diversified combinations of\nresolutions and deformations.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 14:27:57 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 08:10:08 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Lahiri", "Avisek", ""], ["Jain", "Arnav", ""], ["Biswas", "Prabir Kumar", ""], ["Mitra", "Pabitra", ""]]}, {"id": "1711.06127", "submitter": "R\\\"udiger G\\\"obl", "authors": "R\\\"udiger G\\\"obl, Nassir Navab, Christoph Hennersperger", "title": "SUPRA: Open Source Software Defined Ultrasound Processing for Real-Time\n  Applications", "comments": "This is a pre-print of an article published in the International\n  Journal of Computer Assisted Radiology and Surgery. The final authenticated\n  version is available online at: https://doi.org/10.1007/s11548-018-1750-6", "journal-ref": "G\\\"obl, R., Navab, N. & Hennersperger, C. , \"SUPRA: Open Source\n  Software Defined Ultrasound Processing for Real-Time Applications\" Int J CARS\n  (2018)", "doi": "10.1007/s11548-018-1750-6", "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in ultrasound imaging is limited in reproducibility by two factors:\nFirst, many existing ultrasound pipelines are protected by intellectual\nproperty, rendering exchange of code difficult. Second, most pipelines are\nimplemented in special hardware, resulting in limited flexibility of\nimplemented processing steps on such platforms.\n  Methods: With SUPRA we propose an open-source pipeline for fully Software\nDefined Ultrasound Processing for Real-time Applications to alleviate these\nproblems. Covering all steps from beamforming to output of B-mode images, SUPRA\ncan help improve the reproducibility of results and make modifications to the\nimage acquisition mode accessible to the research community. We evaluate the\npipeline qualitatively, quantitatively, and regarding its run-time.\n  Results: The pipeline shows image quality comparable to a clinical system and\nbacked by point-spread function measurements a comparable resolution. Including\nall processing stages of a usual ultrasound pipeline, the run-time analysis\nshows that it can be executed in 2D and 3D on consumer GPUs in real-time.\n  Conclusions: Our software ultrasound pipeline opens up the research in image\nacquisition. Given access to ultrasound data from early stages (raw channel\ndata, radiofrequency data) it simplifies the development in imaging.\nFurthermore, it tackles the reproducibility of research results, as code can be\nshared easily and even be executed without dedicated ultrasound hardware.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 15:12:35 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 18:04:11 GMT"}, {"version": "v3", "created": "Thu, 10 May 2018 15:45:57 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["G\u00f6bl", "R\u00fcdiger", ""], ["Navab", "Nassir", ""], ["Hennersperger", "Christoph", ""]]}, {"id": "1711.06136", "submitter": "Sebastian Bullinger", "authors": "Sebastian Bullinger, Christoph Bodensteiner, Michael Arens, Rainer\n  Stiefelhagen", "title": "3D Trajectory Reconstruction of Dynamic Objects Using Planarity\n  Constraints", "comments": "9 Pages, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to reconstruct the three-dimensional trajectory of a\nmoving instance of a known object category in monocular video data. We track\nthe two-dimensional shape of objects on pixel level exploiting instance-aware\nsemantic segmentation techniques and optical flow cues. We apply Structure from\nMotion techniques to object and background images to determine for each frame\ncamera poses relative to object instances and background structures. By\ncombining object and background camera pose information, we restrict the object\ntrajectory to a one-parameter family of possible solutions. We compute a ground\nrepresentation by fusing background structures and corresponding semantic\nsegmentations. This allows us to determine an object trajectory consistent to\nimage observations and reconstructed environment model. Our method is robust to\nocclusion and handles temporarily stationary objects. We show qualitative\nresults using drone imagery. Due to the lack of suitable benchmark datasets we\npresent a new dataset to evaluate the quality of reconstructed\nthree-dimensional object trajectories. The video sequences contain vehicles in\nurban areas and are rendered using the path-tracing render engine Cycles to\nachieve realistic results. We perform a quantitative evaluation of the\npresented approach using this dataset. Our algorithm achieves an average\nreconstruction-to-ground-truth distance of 0.31 meter.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 15:21:36 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Bullinger", "Sebastian", ""], ["Bodensteiner", "Christoph", ""], ["Arens", "Michael", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1711.06148", "submitter": "Srikrishna Karanam", "authors": "Yunye Gong and Srikrishna Karanam and Ziyan Wu and Kuan-Chuan Peng and\n  Jan Ernst and Peter C. Doerschuk", "title": "Learning Compositional Visual Concepts with Mutual Consistency", "comments": "10 pages, 8 figures, 4 tables, CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositionality of semantic concepts in image synthesis and analysis is\nappealing as it can help in decomposing known and generatively recomposing\nunknown data. For instance, we may learn concepts of changing illumination,\ngeometry or albedo of a scene, and try to recombine them to generate physically\nmeaningful, but unseen data for training and testing. In practice however we\noften do not have samples from the joint concept space available: We may have\ndata on illumination change in one data set and on geometric change in another\none without complete overlap. We pose the following question: How can we learn\ntwo or more concepts jointly from different data sets with mutual consistency\nwhere we do not have samples from the full joint space? We present a novel\nanswer in this paper based on cyclic consistency over multiple concepts,\nrepresented individually by generative adversarial networks (GANs). Our method,\nConceptGAN, can be understood as a drop in for data augmentation to improve\nresilience for real world applications. Qualitative and quantitative\nevaluations demonstrate its efficacy in generating semantically meaningful\nimages, as well as one shot face verification as an example application.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 15:41:34 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 15:22:53 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Gong", "Yunye", ""], ["Karanam", "Srikrishna", ""], ["Wu", "Ziyan", ""], ["Peng", "Kuan-Chuan", ""], ["Ernst", "Jan", ""], ["Doerschuk", "Peter C.", ""]]}, {"id": "1711.06167", "submitter": "Li Niu", "authors": "Li Niu, Jianfei Cai, Ashok Veeraraghavan", "title": "Zero-Shot Learning via Category-Specific Visual-Semantic Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Learning (ZSL) aims to classify a test instance from an unseen\ncategory based on the training instances from seen categories, in which the gap\nbetween seen categories and unseen categories is generally bridged via\nvisual-semantic mapping between the low-level visual feature space and the\nintermediate semantic space. However, the visual-semantic mapping learnt based\non seen categories may not generalize well to unseen categories because the\ndata distributions between seen categories and unseen categories are\nconsiderably different, which is known as the projection domain shift problem\nin ZSL. To address this domain shift issue, we propose a method named Adaptive\nEmbedding ZSL (AEZSL) to learn an adaptive visual-semantic mapping for each\nunseen category based on the similarities between each unseen category and all\nthe seen categories. Then, we further make two extensions based on our AEZSL\nmethod. Firstly, in order to utilize the unlabeled test instances from unseen\ncategories, we extend our AEZSL to a semi-supervised approach named AEZSL with\nLabel Refinement (AEZSL_LR), in which a progressive approach is developed to\nupdate the visual classifiers and refine the predicted test labels\nalternatively based on the similarities among test instances and among unseen\ncategories. Secondly, to avoid learning visual-semantic mapping for each unseen\ncategory in the large-scale classification task, we extend our AEZSL to a deep\nadaptive embedding model named Deep AEZSL (DAEZSL) sharing the similar idea\n(i.e., visual-semantic mapping should be category-specific and related to the\nsemantic space) with AEZSL, which only needs to be trained once, but can be\napplied to arbitrary number of unseen categories. Extensive experiments\ndemonstrate that our proposed methods achieve the state-of-the-art results for\nimage classification on four benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 16:03:15 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 03:46:07 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Niu", "Li", ""], ["Cai", "Jianfei", ""], ["Veeraraghavan", "Ashok", ""]]}, {"id": "1711.06221", "submitter": "Aditya Balu", "authors": "Aditya Balu, Thanh V. Nguyen, Apurva Kokate, Chinmay Hegde and Soumik\n  Sarkar", "title": "A Forward-Backward Approach for Visualizing Information Flow in Deep\n  Networks", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new, systematic framework for visualizing information flow in\ndeep networks. Specifically, given any trained deep convolutional network model\nand a given test image, our method produces a compact support in the image\ndomain that corresponds to a (high-resolution) feature that contributes to the\ngiven explanation. Our method is both computationally efficient as well as\nnumerically robust. We present several preliminary numerical results that\nsupport the benefits of our framework over existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 18:00:24 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Balu", "Aditya", ""], ["Nguyen", "Thanh V.", ""], ["Kokate", "Apurva", ""], ["Hegde", "Chinmay", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1711.06232", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Cuong Duc Dao, Modar Alfadly, Bernard Ghanem", "title": "A Novel Framework for Robustness Analysis of Visual QA Models", "comments": "Accepted by the Thirty-Third AAAI Conference on Artificial\n  Intelligence, (AAAI-19), as an oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been playing an essential role in many computer\nvision tasks including Visual Question Answering (VQA). Until recently, the\nstudy of their accuracy was the main focus of research but now there is a trend\ntoward assessing the robustness of these models against adversarial attacks by\nevaluating their tolerance to varying noise levels. In VQA, adversarial attacks\ncan target the image and/or the proposed main question and yet there is a lack\nof proper analysis of the later. In this work, we propose a flexible framework\nthat focuses on the language part of VQA that uses semantically relevant\nquestions, dubbed basic questions, acting as controllable noise to evaluate the\nrobustness of VQA models. We hypothesize that the level of noise is positively\ncorrelated to the similarity of a basic question to the main question. Hence,\nto apply noise on any given main question, we rank a pool of basic questions\nbased on their similarity by casting this ranking task as a LASSO optimization\nproblem. Then, we propose a novel robustness measure, R_score, and two\nlarge-scale basic question datasets (BQDs) in order to standardize robustness\nanalysis for VQA models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 18:27:49 GMT"}, {"version": "v2", "created": "Sun, 19 Nov 2017 05:47:07 GMT"}, {"version": "v3", "created": "Tue, 25 Dec 2018 04:08:27 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Dao", "Cuong Duc", ""], ["Alfadly", "Modar", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1711.06246", "submitter": "Wei Zhu", "authors": "Wei Zhu, Qiang Qiu, Jiaji Huang, Robert Calderbank, Guillermo Sapiro,\n  Ingrid Daubechies", "title": "LDMNet: Low Dimensional Manifold Regularized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have proved very successful on archetypal tasks for\nwhich large training sets are available, but when the training data are scarce,\ntheir performance suffers from overfitting. Many existing methods of reducing\noverfitting are data-independent, and their efficacy is often limited when the\ntraining set is very small. Data-dependent regularizations are mostly motivated\nby the observation that data of interest lie close to a manifold, which is\ntypically hard to parametrize explicitly and often requires human input of\ntangent vectors. These methods typically only focus on the geometry of the\ninput data, and do not necessarily encourage the networks to produce\ngeometrically meaningful features. To resolve this, we propose a new framework,\nthe Low-Dimensional-Manifold-regularized neural Network (LDMNet), which\nincorporates a feature regularization method that focuses on the geometry of\nboth the input data and the output features. In LDMNet, we regularize the\nnetwork by encouraging the combination of the input data and the output\nfeatures to sample a collection of low dimensional manifolds, which are\nsearched efficiently without explicit parametrization. To achieve this, we\ndirectly use the manifold dimension as a regularization term in a variational\nfunctional. The resulting Euler-Lagrange equation is a Laplace-Beltrami\nequation over a point cloud, which is solved by the point integral method\nwithout increasing the computational complexity. We demonstrate two benefits of\nLDMNet in the experiments. First, we show that LDMNet significantly outperforms\nwidely-used network regularizers such as weight decay and DropOut. Second, we\nshow that LDMNet can be designed to extract common features of an object imaged\nvia different modalities, which proves to be very useful in real-world\napplications such as cross-spectral face recognition.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 18:48:01 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Zhu", "Wei", ""], ["Qiu", "Qiang", ""], ["Huang", "Jiaji", ""], ["Calderbank", "Robert", ""], ["Sapiro", "Guillermo", ""], ["Daubechies", "Ingrid", ""]]}, {"id": "1711.06288", "submitter": "Jianbo Chen", "authors": "Jianbo Chen, Yelong Shen, Jianfeng Gao, Jingjing Liu, Xiaodong Liu", "title": "Language-Based Image Editing with Recurrent Attentive Models", "comments": "Accepted to CVPR 2018 as a Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of Language-Based Image Editing (LBIE). Given a\nsource image and a natural language description, we want to generate a target\nimage by editing the source image based on the description. We propose a\ngeneric modeling framework for two sub-tasks of LBIE: language-based image\nsegmentation and image colorization. The framework uses recurrent attentive\nmodels to fuse image and language features. Instead of using a fixed step size,\nwe introduce for each region of the image a termination gate to dynamically\ndetermine after each inference step whether to continue extrapolating\nadditional information from the textual description. The effectiveness of the\nframework is validated on three datasets. First, we introduce a synthetic\ndataset, called CoSaL, to evaluate the end-to-end performance of our LBIE\nsystem. Second, we show that the framework leads to state-of-the-art\nperformance on image segmentation on the ReferIt dataset. Third, we present the\nfirst language-based colorization result on the Oxford-102 Flowers dataset.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 19:10:21 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 04:04:30 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Chen", "Jianbo", ""], ["Shen", "Yelong", ""], ["Gao", "Jianfeng", ""], ["Liu", "Jingjing", ""], ["Liu", "Xiaodong", ""]]}, {"id": "1711.06303", "submitter": "Devesh Walawalkar", "authors": "Devesh Walawalkar", "title": "Grammatical facial expression recognition using customized deep neural\n  network architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to expand the visual understanding capacity of computers\nby helping it recognize human sign language more efficiently. This is carried\nout through recognition of facial expressions, which accompany the hand signs\nused in this language. This paper specially focuses on the popular Brazilian\nsign language (LIBRAS). While classifying different hand signs into their\nrespective word meanings has already seen much literature dedicated to it, the\nemotions or intention with which the words are expressed haven't primarily been\ntaken into consideration. As from our normal human experience, words expressed\nwith different emotions or mood can have completely different meanings attached\nto it. Lending computers the ability of classifying these facial expressions,\ncan help add another level of deep understanding of what the deaf person\nexactly wants to communicate. The proposed idea is implemented through a deep\nneural network having a customized architecture. This helps learning specific\npatterns in individual expressions much better as compared to a generic\napproach. With an overall accuracy of 98.04%, the implemented deep network\nperforms excellently well and thus is fit to be used in any given practical\nscenario.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 19:50:46 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Walawalkar", "Devesh", ""]]}, {"id": "1711.06315", "submitter": "Sanchari Sen", "authors": "Sanchari Sen, Shubham Jain, Swagath Venkataramani, Anand Raghunathan", "title": "SparCE: Sparsity aware General Purpose Core Extensions to Accelerate\n  Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have emerged as the method of choice for solving\na wide range of machine learning tasks. The enormous computational demands\nposed by DNNs have most commonly been addressed through the design of custom\naccelerators. However, these accelerators are prohibitive in many design\nscenarios (e.g., wearable devices and IoT sensors), due to stringent area/cost\nconstraints. Accelerating DNNs on these low-power systems, comprising of mainly\nthe general-purpose processor (GPP) cores, requires new approaches. We improve\nthe performance of DNNs on GPPs by exploiting a key attribute of DNNs, i.e.,\nsparsity. We propose Sparsity aware Core Extensions (SparCE)- a set of\nmicro-architectural and ISA extensions that leverage sparsity and are minimally\nintrusive and low-overhead. We dynamically detect zero operands and skip a set\nof future instructions that use it. Our design ensures that the instructions to\nbe skipped are prevented from even being fetched, as squashing instructions\ncomes with a penalty. SparCE consists of 2 key micro-architectural\nenhancements- a Sparsity Register File (SpRF) that tracks zero registers and a\nSparsity aware Skip Address (SASA) table that indicates instructions to be\nskipped. When an instruction is fetched, SparCE dynamically pre-identifies\nwhether the following instruction(s) can be skipped and appropriately modifies\nthe program counter, thereby skipping the redundant instructions and improving\nperformance. We model SparCE using the gem5 architectural simulator, and\nevaluate our approach on 6 image-recognition DNNs in the context of both\ntraining and inference using the Caffe framework. On a scalar microprocessor,\nSparCE achieves 19%-31% reduction in application-level. We also evaluate SparCE\non a 4-way SIMD ARMv8 processor using the OpenBLAS library, and demonstrate\nthat SparCE achieves 8%-15% reduction in the application-level execution time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 01:20:19 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 16:42:03 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Sen", "Sanchari", ""], ["Jain", "Shubham", ""], ["Venkataramani", "Swagath", ""], ["Raghunathan", "Anand", ""]]}, {"id": "1711.06330", "submitter": "Chih-Yao Ma", "authors": "Chih-Yao Ma, Asim Kadav, Iain Melvin, Zsolt Kira, Ghassan AlRegib,\n  Hans Peter Graf", "title": "Attend and Interact: Higher-Order Object Interactions for Video\n  Understanding", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human actions often involve complex interactions across several inter-related\nobjects in the scene. However, existing approaches to fine-grained video\nunderstanding or visual relationship detection often rely on single object\nrepresentation or pairwise object relationships. Furthermore, learning\ninteractions across multiple objects in hundreds of frames for video is\ncomputationally infeasible and performance may suffer since a large\ncombinatorial space has to be modeled. In this paper, we propose to efficiently\nlearn higher-order interactions between arbitrary subgroups of objects for\nfine-grained video understanding. We demonstrate that modeling object\ninteractions significantly improves accuracy for both action recognition and\nvideo captioning, while saving more than 3-times the computation over\ntraditional pairwise relationships. The proposed method is validated on two\nlarge-scale datasets: Kinetics and ActivityNet Captions. Our SINet and\nSINet-Caption achieve state-of-the-art performances on both datasets even\nthough the videos are sampled at a maximum of 1 FPS. To the best of our\nknowledge, this is the first work modeling object interactions on open domain\nlarge-scale video datasets, and we additionally model higher-order object\ninteractions which improves the performance with low computational costs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 22:14:52 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 21:22:42 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Ma", "Chih-Yao", ""], ["Kadav", "Asim", ""], ["Melvin", "Iain", ""], ["Kira", "Zsolt", ""], ["AlRegib", "Ghassan", ""], ["Graf", "Hans Peter", ""]]}, {"id": "1711.06354", "submitter": "Chih-Yao Ma", "authors": "Chih-Yao Ma, Asim Kadav, Iain Melvin, Zsolt Kira, Ghassan AlRegib,\n  Hans Peter Graf", "title": "Grounded Objects and Interactions for Video Captioning", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.06330", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of video captioning by grounding language generation\non object interactions in the video. Existing work mostly focuses on overall\nscene understanding with often limited or no emphasis on object interactions to\naddress the problem of video understanding. In this paper, we propose\nSINet-Caption that learns to generate captions grounded over higher-order\ninteractions between arbitrary groups of objects for fine-grained video\nunderstanding. We discuss the challenges and benefits of such an approach. We\nfurther demonstrate state-of-the-art results on the ActivityNet Captions\ndataset using our model, SINet-Caption based on this approach.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 23:39:08 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Ma", "Chih-Yao", ""], ["Kadav", "Asim", ""], ["Melvin", "Iain", ""], ["Kira", "Zsolt", ""], ["AlRegib", "Ghassan", ""], ["Graf", "Hans Peter", ""]]}, {"id": "1711.06363", "submitter": "Renato Hermoza Aragon\\'es", "authors": "Renato Hermoza and Ivan Sipiran", "title": "3D Reconstruction of Incomplete Archaeological Objects Using a\n  Generative Adversarial Network", "comments": "6 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a data-driven approach to aid the repairing and conservation of\narchaeological objects: ORGAN, an object reconstruction generative adversarial\nnetwork (GAN). By using an encoder-decoder 3D deep neural network on a GAN\narchitecture, and combining two loss objectives: a completion loss and an\nImproved Wasserstein GAN loss, we can train a network to effectively predict\nthe missing geometry of damaged objects. As archaeological objects can greatly\ndiffer between them, the network is conditioned on a variable, which can be a\nculture, a region or any metadata of the object. In our results, we show that\nour method can recover most of the information from damaged objects, even in\ncases where more than half of the voxels are missing, without producing many\nerrors.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 00:58:53 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 18:12:27 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Hermoza", "Renato", ""], ["Sipiran", "Ivan", ""]]}, {"id": "1711.06368", "submitter": "Mason Liu", "authors": "Mason Liu, Menglong Zhu", "title": "Mobile Video Object Detection with Temporally-Aware Feature Maps", "comments": "In CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an online model for object detection in videos designed\nto run in real-time on low-powered mobile and embedded devices. Our approach\ncombines fast single-image object detection with convolutional long short term\nmemory (LSTM) layers to create an interweaved recurrent-convolutional\narchitecture. Additionally, we propose an efficient Bottleneck-LSTM layer that\nsignificantly reduces computational cost compared to regular LSTMs. Our network\nachieves temporal awareness by using Bottleneck-LSTMs to refine and propagate\nfeature maps across frames. This approach is substantially faster than existing\ndetection methods in video, outperforming the fastest single-frame models in\nmodel size and computational cost while attaining accuracy comparable to much\nmore expensive single-frame models on the Imagenet VID 2015 dataset. Our model\nreaches a real-time inference speed of up to 15 FPS on a mobile CPU.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 01:40:12 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 20:05:29 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Liu", "Mason", ""], ["Zhu", "Menglong", ""]]}, {"id": "1711.06370", "submitter": "Chunhua Shen", "authors": "Bohan Zhuang, Qi Wu, Chunhua Shen, Ian Reid, Anton van den Hengel", "title": "Parallel Attention: A Unified Framework for Visual Object Discovery\n  through Dialogs and Queries", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognising objects according to a pre-defined fixed set of class labels has\nbeen well studied in the Computer Vision. There are a great many practical\napplications where the subjects that may be of interest are not known\nbeforehand, or so easily delineated, however. In many of these cases natural\nlanguage dialog is a natural way to specify the subject of interest, and the\ntask achieving this capability (a.k.a, Referring Expression Comprehension) has\nrecently attracted attention. To this end we propose a unified framework, the\nParalleL AttentioN (PLAN) network, to discover the object in an image that is\nbeing referred to in variable length natural expression descriptions, from\nshort phrases query to long multi-round dialogs. The PLAN network has two\nattention mechanisms that relate parts of the expressions to both the global\nvisual content and also directly to object candidates. Furthermore, the\nattention mechanisms are recurrent, making the referring process visualizable\nand explainable. The attended information from these dual sources are combined\nto reason about the referred object. These two attention mechanisms can be\ntrained in parallel and we find the combined system outperforms the\nstate-of-art on several benchmarked datasets with different length language\ninput, such as RefCOCO, RefCOCO+ and GuessWhat?!.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 01:46:48 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Zhuang", "Bohan", ""], ["Wu", "Qi", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1711.06373", "submitter": "Zhe Li", "authors": "Zhe Li, Chong Wang, Mei Han, Yuan Xue, Wei Wei, Li-Jia Li, Li Fei-Fei", "title": "Thoracic Disease Identification and Localization with Limited\n  Supervision", "comments": "Conference on Computer Vision and Pattern Recognition 2018 (CVPR\n  2018). V1: CVPR submission; V2: +supplementary; V3: CVPR camera-ready; V4:\n  correction, update reference baseline results according to their latest post;\n  V5: minor correction; V6: Identification results using NIH data splits and\n  various image models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate identification and localization of abnormalities from radiology\nimages play an integral part in clinical diagnosis and treatment planning.\nBuilding a highly accurate prediction model for these tasks usually requires a\nlarge number of images manually annotated with labels and finding sites of\nabnormalities. In reality, however, such annotated data are expensive to\nacquire, especially the ones with location annotations. We need methods that\ncan work well with only a small amount of location annotations. To address this\nchallenge, we present a unified approach that simultaneously performs disease\nidentification and localization through the same underlying model for all\nimages. We demonstrate that our approach can effectively leverage both class\ninformation as well as limited location annotation, and significantly\noutperforms the comparative reference baseline in both classification and\nlocalization tasks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 01:52:56 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 03:31:10 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 06:11:39 GMT"}, {"version": "v4", "created": "Wed, 28 Mar 2018 19:28:30 GMT"}, {"version": "v5", "created": "Tue, 24 Apr 2018 16:17:19 GMT"}, {"version": "v6", "created": "Wed, 20 Jun 2018 23:24:24 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Li", "Zhe", ""], ["Wang", "Chong", ""], ["Han", "Mei", ""], ["Xue", "Yuan", ""], ["Wei", "Wei", ""], ["Li", "Li-Jia", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1711.06375", "submitter": "Qiangui Huang", "authors": "Weiyue Wang, Qiangui Huang, Suya You, Chao Yang, Ulrich Neumann", "title": "Shape Inpainting using 3D Generative Adversarial Network and Recurrent\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in convolutional neural networks have shown promising results\nin 3D shape completion. But due to GPU memory limitations, these methods can\nonly produce low-resolution outputs. To inpaint 3D models with semantic\nplausibility and contextual details, we introduce a hybrid framework that\ncombines a 3D Encoder-Decoder Generative Adversarial Network (3D-ED-GAN) and a\nLong-term Recurrent Convolutional Network (LRCN). The 3D-ED-GAN is a 3D\nconvolutional neural network trained with a generative adversarial paradigm to\nfill missing 3D data in low-resolution. LRCN adopts a recurrent neural network\narchitecture to minimize GPU memory usage and incorporates an Encoder-Decoder\npair into a Long Short-term Memory Network. By handling the 3D model as a\nsequence of 2D slices, LRCN transforms a coarse 3D shape into a more complete\nand higher resolution volume. While 3D-ED-GAN captures global contextual\nstructure of the 3D shape, LRCN localizes the fine-grained details.\nExperimental results on both real-world and synthetic data show reconstructions\nfrom corrupted models result in complete and high-resolution 3D objects.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 02:01:11 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Wang", "Weiyue", ""], ["Huang", "Qiangui", ""], ["You", "Suya", ""], ["Yang", "Chao", ""], ["Neumann", "Ulrich", ""]]}, {"id": "1711.06379", "submitter": "Terrell Mundhenk", "authors": "T. Nathan Mundhenk, Daniel Ho and Barry Y. Chen", "title": "Improvements to context based self-supervised learning", "comments": "Accepted paper at CVPR 2018", "journal-ref": null, "doi": "10.1109/CVPR.2018.00973", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a set of methods to improve on the results of self-supervised\nlearning using context. We start with a baseline of patch based arrangement\ncontext learning and go from there. Our methods address some overt problems\nsuch as chromatic aberration as well as other potential problems such as\nspatial skew and mid-level feature neglect. We prevent problems with testing\ngeneralization on common self-supervised benchmark tests by using different\ndatasets during our development. The results of our methods combined yield top\nscores on all standard self-supervised benchmarks, including classification and\ndetection on PASCAL VOC 2007, segmentation on PASCAL VOC 2012, and \"linear\ntests\" on the ImageNet and CSAIL Places datasets. We obtain an improvement over\nour baseline method of between 4.0 to 7.1 percentage points on transfer\nlearning classification tests. We also show results on different standard\nnetwork architectures to demonstrate generalization as well as portability. All\ndata, models and programs are available at:\nhttps://gdo-datasci.llnl.gov/selfsupervised/.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 02:22:21 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 23:00:35 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 22:14:26 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Mundhenk", "T. Nathan", ""], ["Ho", "Daniel", ""], ["Chen", "Barry Y.", ""]]}, {"id": "1711.06382", "submitter": "Tianci Liu", "authors": "Tianci Liu, Zelin Shi, Yunpeng Liu", "title": "Dimensionality Reduction on Grassmannian via Riemannian Optimization: A\n  Generalized Perspective", "comments": "12 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a generalized framework with joint normalization which\nlearns lower-dimensional subspaces with maximum discriminative power by making\nuse of the Riemannian geometry. In particular, we model the\nsimilarity/dissimilarity between subspaces using various metrics defined on\nGrassmannian and formulate dimen-sionality reduction as a non-linear constraint\noptimization problem considering the orthogonalization. To obtain the linear\nmapping, we derive the components required to per-form Riemannian optimization\n(e.g., Riemannian conju-gate gradient) from the original Grassmannian through\nan orthonormal projection. We respect the Riemannian ge-ometry of the Grassmann\nmanifold and search for this projection directly from one Grassmann manifold to\nan-other face-to-face without any additional transformations. In this natural\ngeometry-aware way, any metric on the Grassmann manifold can be resided in our\nmodel theoreti-cally. We have combined five metrics with our model and the\nlearning process can be treated as an unconstrained optimization problem on a\nGrassmann manifold. Exper-iments on several datasets demonstrate that our\napproach leads to a significant accuracy gain over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 02:50:04 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Liu", "Tianci", ""], ["Shi", "Zelin", ""], ["Liu", "Yunpeng", ""]]}, {"id": "1711.06396", "submitter": "Yin Zhou", "authors": "Yin Zhou and Oncel Tuzel", "title": "VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection of objects in 3D point clouds is a central problem in many\napplications, such as autonomous navigation, housekeeping robots, and\naugmented/virtual reality. To interface a highly sparse LiDAR point cloud with\na region proposal network (RPN), most existing efforts have focused on\nhand-crafted feature representations, for example, a bird's eye view\nprojection. In this work, we remove the need of manual feature engineering for\n3D point clouds and propose VoxelNet, a generic 3D detection network that\nunifies feature extraction and bounding box prediction into a single stage,\nend-to-end trainable deep network. Specifically, VoxelNet divides a point cloud\ninto equally spaced 3D voxels and transforms a group of points within each\nvoxel into a unified feature representation through the newly introduced voxel\nfeature encoding (VFE) layer. In this way, the point cloud is encoded as a\ndescriptive volumetric representation, which is then connected to a RPN to\ngenerate detections. Experiments on the KITTI car detection benchmark show that\nVoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a\nlarge margin. Furthermore, our network learns an effective discriminative\nrepresentation of objects with various geometries, leading to encouraging\nresults in 3D detection of pedestrians and cyclists, based on only LiDAR.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 04:25:24 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Zhou", "Yin", ""], ["Tuzel", "Oncel", ""]]}, {"id": "1711.06406", "submitter": "Ye Xia", "authors": "Ye Xia, Danqing Zhang, Jinkyu Kim, Ken Nakayama, Karl Zipser, David\n  Whitney", "title": "Predicting Driver Attention in Critical Situations", "comments": "ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust driver attention prediction for critical situations is a challenging\ncomputer vision problem, yet essential for autonomous driving. Because critical\ndriving moments are so rare, collecting enough data for these situations is\ndifficult with the conventional in-car data collection protocol---tracking eye\nmovements during driving. Here, we first propose a new in-lab driver attention\ncollection protocol and introduce a new driver attention dataset, Berkeley\nDeepDrive Attention (BDD-A) dataset, which is built upon braking event videos\nselected from a large-scale, crowd-sourced driving video dataset. We further\npropose Human Weighted Sampling (HWS) method, which uses human gaze behavior to\nidentify crucial frames of a driving dataset and weights them heavily during\nmodel training. With our dataset and HWS, we built a driver attention\nprediction model that outperforms the state-of-the-art and demonstrates\nsophisticated behaviors, like attending to crossing pedestrians but not giving\nfalse alarms to pedestrians safely walking on the sidewalk. Its prediction\nresults are nearly indistinguishable from ground-truth to humans. Although only\nbeing trained with our in-lab attention data, the model also predicts in-car\ndriver attention data of routine driving with state-of-the-art accuracy. This\nresult not only demonstrates the performance of our model but also proves the\nvalidity and usefulness of our dataset and data collection protocol.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 04:53:51 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 05:41:26 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 06:40:01 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Xia", "Ye", ""], ["Zhang", "Danqing", ""], ["Kim", "Jinkyu", ""], ["Nakayama", "Ken", ""], ["Zipser", "Karl", ""], ["Whitney", "David", ""]]}, {"id": "1711.06420", "submitter": "Jiuxiang Gu Mr", "authors": "Jiuxiang Gu, Jianfei Cai, Shafiq Joty, Li Niu, Gang Wang", "title": "Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval\n  with Generative Models", "comments": "10 pages, 6 figures, Accepted as spotlight at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textual-visual cross-modal retrieval has been a hot research topic in both\ncomputer vision and natural language processing communities. Learning\nappropriate representations for multi-modal data is crucial for the cross-modal\nretrieval performance. Unlike existing image-text retrieval approaches that\nembed image-text pairs as single feature vectors in a common representational\nspace, we propose to incorporate generative processes into the cross-modal\nfeature embedding, through which we are able to learn not only the global\nabstract features but also the local grounded features. Extensive experiments\nshow that our framework can well match images and sentences with complex\ncontent, and achieve the state-of-the-art cross-modal retrieval results on\nMSCOCO dataset.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 06:10:03 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 08:56:56 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Gu", "Jiuxiang", ""], ["Cai", "Jianfei", ""], ["Joty", "Shafiq", ""], ["Niu", "Li", ""], ["Wang", "Gang", ""]]}, {"id": "1711.06423", "submitter": "Shruti Mittal", "authors": "Shruti Mittal, Dattaraj Rao", "title": "Vision Based Railway Track Monitoring using Deep Learning", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": "2017TSD0004", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision based methods have been explored in the past for detection of\nrailway track defects, but full automation has always been a challenge because\nboth traditional image processing methods and deep learning classifiers trained\nfrom scratch fail to generalize that well to infinite novel scenarios seen in\nthe real world, given limited amount of labeled data. Advancements have been\nmade recently to make machine learning models utilize knowledge from a\ndifferent but related domain. In this paper, we show that even though similar\ndomain data is not available, transfer learning provides the model\nunderstanding of other real world objects and enables training production scale\ndeep learning classifiers for uncontrolled real world data. Our models\nefficiently detect both track defects like sunkinks, loose ballast and railway\nassets like switches and signals. Models were validated with hours of track\nvideos recorded in different continents resulting in different weather\nconditions, different ambience and surroundings. A track health index concept\nhas also been proposed to monitor complete rail network.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 06:16:41 GMT"}, {"version": "v2", "created": "Sun, 25 Nov 2018 14:25:54 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Mittal", "Shruti", ""], ["Rao", "Dattaraj", ""]]}, {"id": "1711.06427", "submitter": "Chaolong Li", "authors": "Chaolong Li, Zhen Cui, Wenming Zheng, Chunyan Xu, Rongrong Ji, Jian\n  Yang", "title": "Action-Attending Graphic Neural Network", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2815744", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motion analysis of human skeletons is crucial for human action\nrecognition, which is one of the most active topics in computer vision. In this\npaper, we propose a fully end-to-end action-attending graphic neural network\n(A$^2$GNN) for skeleton-based action recognition, in which each irregular\nskeleton is structured as an undirected attribute graph. To extract high-level\nsemantic representation from skeletons, we perform the local spectral graph\nfiltering on the constructed attribute graphs like the standard image\nconvolution operation. Considering not all joints are informative for action\nanalysis, we design an action-attending layer to detect those salient action\nunits (AUs) by adaptively weighting skeletal joints. Herein the filtering\nresponses are parameterized into a weighting function irrelevant to the order\nof input nodes. To further encode continuous motion variations, the deep\nfeatures learnt from skeletal graphs are gathered along consecutive temporal\nslices and then fed into a recurrent gated network. Finally, the spectral graph\nfiltering, action-attending and recurrent temporal encoding are integrated\ntogether to jointly train for the sake of robust action recognition as well as\nthe intelligibility of human actions. To evaluate our A$^2$GNN, we conduct\nextensive experiments on four benchmark skeleton-based action datasets,\nincluding the large-scale challenging NTU RGB+D dataset. The experimental\nresults demonstrate that our network achieves the state-of-the-art\nperformances.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 06:32:09 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Li", "Chaolong", ""], ["Cui", "Zhen", ""], ["Zheng", "Wenming", ""], ["Xu", "Chunyan", ""], ["Ji", "Rongrong", ""], ["Yang", "Jian", ""]]}, {"id": "1711.06431", "submitter": "Housam Khalifa Bashier Babiker", "authors": "Housam Khalifa Bashier Babiker and Randy Goebel", "title": "Using KL-divergence to focus Deep Visual Explanation", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for explaining the image classification predictions of\ndeep convolution neural networks, by highlighting the pixels in the image which\ninfluence the final class prediction. Our method requires the identification of\na heuristic method to select parameters hypothesized to be most relevant in\nthis prediction, and here we use Kullback-Leibler divergence to provide this\nfocus. Overall, our approach helps in understanding and interpreting deep\nnetwork predictions and we hope contributes to a foundation for such\nunderstanding of deep learning networks. In this brief paper, our experiments\nevaluate the performance of two popular networks in this context of\ninterpretability.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 06:53:17 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 06:18:18 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Babiker", "Housam Khalifa Bashier", ""], ["Goebel", "Randy", ""]]}, {"id": "1711.06439", "submitter": "Holger Roth", "authors": "Holger Roth, Masahiro Oda, Natsuki Shimizu, Hirohisa Oda, Yuichiro\n  Hayashi, Takayuki Kitasaka, Michitaka Fujiwara, Kazunari Misawa, Kensaku Mori", "title": "Towards dense volumetric pancreas segmentation in CT using 3D fully\n  convolutional networks", "comments": "Accepted for oral presentation at SPIE Medical Imaging 2018, Houston,\n  TX, USA Updated experiment in Fig. 4", "journal-ref": "Medical Imaging 2018: Image Processing", "doi": "10.1117/12.2293499", "report-no": "Published in SPIE Proceedings Vol. 10574", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pancreas segmentation in computed tomography imaging has been historically\ndifficult for automated methods because of the large shape and size variations\nbetween patients. In this work, we describe a custom-build 3D fully\nconvolutional network (FCN) that can process a 3D image including the whole\npancreas and produce an automatic segmentation. We investigate two variations\nof the 3D FCN architecture; one with concatenation and one with summation skip\nconnections to the decoder part of the network. We evaluate our methods on a\ndataset from a clinical trial with gastric cancer patients, including 147\ncontrast enhanced abdominal CT scans acquired in the portal venous phase. Using\nthe summation architecture, we achieve an average Dice score of 89.7 $\\pm$ 3.8\n(range [79.8, 94.8]) % in testing, achieving the new state-of-the-art\nperformance in pancreas segmentation on this dataset.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 07:26:39 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 02:14:48 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Roth", "Holger", ""], ["Oda", "Masahiro", ""], ["Shimizu", "Natsuki", ""], ["Oda", "Hirohisa", ""], ["Hayashi", "Yuichiro", ""], ["Kitasaka", "Takayuki", ""], ["Fujiwara", "Michitaka", ""], ["Misawa", "Kazunari", ""], ["Mori", "Kensaku", ""]]}, {"id": "1711.06445", "submitter": "Idan Kligvasser", "authors": "Idan Kligvasser, Tamar Rott Shaham and Tomer Michaeli", "title": "xUnit: Learning a Spatial Activation Function for Efficient Image\n  Restoration", "comments": "Conference on Computer Vision and Pattern Recognition (CVPR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks (DNNs) achieved unprecedented\nperformance in many low-level vision tasks. However, state-of-the-art results\nare typically achieved by very deep networks, which can reach tens of layers\nwith tens of millions of parameters. To make DNNs implementable on platforms\nwith limited resources, it is necessary to weaken the tradeoff between\nperformance and efficiency. In this paper, we propose a new activation unit,\nwhich is particularly suitable for image restoration problems. In contrast to\nthe widespread per-pixel activation units, like ReLUs and sigmoids, our unit\nimplements a learnable nonlinear function with spatial connections. This\nenables the net to capture much more complex features, thus requiring a\nsignificantly smaller number of layers in order to reach the same performance.\nWe illustrate the effectiveness of our units through experiments with\nstate-of-the-art nets for denoising, de-raining, and super resolution, which\nare already considered to be very small. With our approach, we are able to\nfurther reduce these models by nearly 50% without incurring any degradation in\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 08:00:44 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 14:05:33 GMT"}, {"version": "v3", "created": "Sun, 25 Mar 2018 08:49:43 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Kligvasser", "Idan", ""], ["Shaham", "Tamar Rott", ""], ["Michaeli", "Tomer", ""]]}, {"id": "1711.06448", "submitter": "Jie Chang", "authors": "Jie Chang, Yujun Gu, Ya Zhang", "title": "Chinese Typeface Transformation with Hierarchical Adversarial Network", "comments": "8 pages(exclude reference), 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore automated typeface generation through image style\ntransfer which has shown great promise in natural image generation. Existing\nstyle transfer methods for natural images generally assume that the source and\ntarget images share similar high-frequency features. However, this assumption\nis no longer true in typeface transformation. Inspired by the recent\nadvancement in Generative Adversarial Networks (GANs), we propose a\nHierarchical Adversarial Network (HAN) for typeface transformation. The\nproposed HAN consists of two sub-networks: a transfer network and a\nhierarchical adversarial discriminator. The transfer network maps characters\nfrom one typeface to another. A unique characteristic of typefaces is that the\nsame radicals may have quite different appearances in different characters even\nunder the same typeface. Hence, a stage-decoder is employed by the transfer\nnetwork to leverage multiple feature layers, aiming to capture both the global\nand local features. The hierarchical adversarial discriminator implicitly\nmeasures data discrepancy between the generated domain and the target domain.\nTo leverage the complementary discriminating capability of different feature\nlayers, a hierarchical structure is proposed for the discriminator. We have\nexperimentally demonstrated that HAN is an effective framework for typeface\ntransfer and characters restoration.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 08:05:49 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Chang", "Jie", ""], ["Gu", "Yujun", ""], ["Zhang", "Ya", ""]]}, {"id": "1711.06451", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Saeed Bagheri Shouraki, Hoda Mohammadzade, Ensieh\n  Iranmehr", "title": "A Fusion-based Gender Recognition Method Using Facial Images", "comments": "6 pages, 4 figures, 2 tables, key words: gender recognition, Gabor\n  filter, local binary pattern, lower face, LDA, SVM, back propagation neural\n  network, PCA", "journal-ref": null, "doi": "10.1109/ICEE.2018.8472550", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a fusion-based gender recognition method which uses\nfacial images as input. Firstly, this paper utilizes pre-processing and a\nlandmark detection method in order to find the important landmarks of faces.\nThereafter, four different frameworks are proposed which are inspired by\nstate-of-the-art gender recognition systems. The first framework extracts\nfeatures using Local Binary Pattern (LBP) and Principal Component Analysis\n(PCA) and uses back propagation neural network. The second framework uses Gabor\nfilters, PCA, and kernel Support Vector Machine (SVM). The third framework uses\nlower part of faces as input and classifies them using kernel SVM. The fourth\nframework uses Linear Discriminant Analysis (LDA) in order to classify the side\noutline landmarks of faces. Finally, the four decisions of frameworks are fused\nusing weighted voting. This paper takes advantage of both texture and\ngeometrical information, the two dominant types of information in facial gender\nrecognition. Experimental results show the power and effectiveness of the\nproposed method. This method obtains recognition rate of 94% for neutral faces\nof FEI face dataset, which is equal to state-of-the-art rate for this dataset.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 08:16:55 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Shouraki", "Saeed Bagheri", ""], ["Mohammadzade", "Hoda", ""], ["Iranmehr", "Ensieh", ""]]}, {"id": "1711.06454", "submitter": "Yexun Zhang", "authors": "Yexun Zhang, Ya Zhang, Wenbin Cai, Jie Chang", "title": "Separating Style and Content for Generalized Style Transfer", "comments": "Accepted by CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural style transfer has drawn broad attention in recent years. However,\nmost existing methods aim to explicitly model the transformation between\ndifferent styles, and the learned model is thus not generalizable to new\nstyles. We here attempt to separate the representations for styles and\ncontents, and propose a generalized style transfer network consisting of style\nencoder, content encoder, mixer and decoder. The style encoder and content\nencoder are used to extract the style and content factors from the style\nreference images and content reference images, respectively. The mixer employs\na bilinear model to integrate the above two factors and finally feeds it into a\ndecoder to generate images with target style and content. To separate the style\nfeatures and content features, we leverage the conditional dependence of styles\nand contents given an image. During training, the encoder network learns to\nextract styles and contents from two sets of reference images in limited size,\none with shared style and the other with shared content. This learning\nframework allows simultaneous style transfer among multiple styles and can be\ndeemed as a special `multi-task' learning scenario. The encoders are expected\nto capture the underlying features for different styles and contents which is\ngeneralizable to new styles and contents. For validation, we applied the\nproposed algorithm to the Chinese Typeface transfer problem. Extensive\nexperiment results on character generation have demonstrated the effectiveness\nand robustness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 08:26:12 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 12:21:57 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 03:21:37 GMT"}, {"version": "v4", "created": "Tue, 27 Mar 2018 10:40:45 GMT"}, {"version": "v5", "created": "Fri, 30 Mar 2018 02:09:03 GMT"}, {"version": "v6", "created": "Sun, 23 Sep 2018 10:36:52 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Zhang", "Yexun", ""], ["Zhang", "Ya", ""], ["Cai", "Wenbin", ""], ["Chang", "Jie", ""]]}, {"id": "1711.06459", "submitter": "Yiqi Hou", "authors": "Yiqi Hou, Sascha Hornauer, Karl Zipser", "title": "Fast Recurrent Fully Convolutional Networks for Direct Perception in\n  Autonomous Driving", "comments": "CVPR 2018 Submission, 9 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have been shown to perform\nextremely well at a variety of tasks including subtasks of autonomous driving\nsuch as image segmentation and object classification. However, networks\ndesigned for these tasks typically require vast quantities of training data and\nlong training periods to converge. We investigate the design rationale behind\nend-to-end driving network designs by proposing and comparing three small and\ncomputationally inexpensive deep end-to-end neural network models that generate\ndriving control signals directly from input images. In contrast to prior work\nthat segments the autonomous driving task, our models take on a novel approach\nto the autonomous driving problem by utilizing deep and thin Fully\nConvolutional Nets (FCNs) with recurrent neural nets and low parameter counts\nto tackle a complex end-to-end regression task predicting both steering and\nacceleration commands. In addition, we include layers optimized for\nclassification to allow the networks to implicitly learn image semantics. We\nshow that the resulting networks use 3x fewer parameters than the most recent\ncomparable end-to-end driving network and 500x fewer parameters than the\nAlexNet variations and converge both faster and to lower losses while\nmaintaining robustness against overfitting.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 08:49:35 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 03:06:42 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Hou", "Yiqi", ""], ["Hornauer", "Sascha", ""], ["Zipser", "Karl", ""]]}, {"id": "1711.06465", "submitter": "Lisa Anne Hendricks", "authors": "Lisa Anne Hendricks, Ronghang Hu, Trevor Darrell, Zeynep Akata", "title": "Grounding Visual Explanations (Extended Abstract)", "comments": "Presented at NIPS 2017 Symposium on Interpretable Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing models which generate textual explanations enforce task relevance\nthrough a discriminative term loss function, but such mechanisms only weakly\nconstrain mentioned object parts to actually be present in the image. In this\npaper, a new model is proposed for generating explanations by utilizing\nlocalized grounding of constituent phrases in generated explanations to ensure\nimage relevance. Specifically, we introduce a phrase-critic model to refine\n(re-score/re-rank) generated candidate explanations and employ a\nrelative-attribute inspired ranking loss using \"flipped\" phrases as negative\nexamples for training. At test time, our phrase-critic model takes an image and\na candidate explanation as input and outputs a score indicating how well the\ncandidate explanation is grounded in the image.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 09:33:58 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Hendricks", "Lisa Anne", ""], ["Hu", "Ronghang", ""], ["Darrell", "Trevor", ""], ["Akata", "Zeynep", ""]]}, {"id": "1711.06475", "submitter": "Bo Zhao", "authors": "Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan, Rui Liang,\n  Wenjia Wang, Shipei Zhou, Guosen Lin, Yanwei Fu, Yizhou Wang, Yonggang Wang", "title": "AI Challenger : A Large-scale Dataset for Going Deeper in Image\n  Understanding", "comments": null, "journal-ref": "2019 IEEE International Conference on Multimedia and Expo (ICME)", "doi": "10.1109/ICME.2019.00256", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant progress has been achieved in Computer Vision by leveraging\nlarge-scale image datasets. However, large-scale datasets for complex Computer\nVision tasks beyond classification are still limited. This paper proposed a\nlarge-scale dataset named AIC (AI Challenger) with three sub-datasets, human\nkeypoint detection (HKD), large-scale attribute dataset (LAD) and image Chinese\ncaptioning (ICC). In this dataset, we annotate class labels (LAD), keypoint\ncoordinate (HKD), bounding box (HKD and LAD), attribute (LAD) and caption\n(ICC). These rich annotations bridge the semantic gap between low-level images\nand high-level concepts. The proposed dataset is an effective benchmark to\nevaluate and improve different computational methods. In addition, for related\ntasks, others can also use our dataset as a new resource to pre-train their\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 09:58:20 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wu", "Jiahong", ""], ["Zheng", "He", ""], ["Zhao", "Bo", ""], ["Li", "Yixin", ""], ["Yan", "Baoming", ""], ["Liang", "Rui", ""], ["Wang", "Wenjia", ""], ["Zhou", "Shipei", ""], ["Lin", "Guosen", ""], ["Fu", "Yanwei", ""], ["Wang", "Yizhou", ""], ["Wang", "Yonggang", ""]]}, {"id": "1711.06491", "submitter": "J. D. Curt\\'o", "authors": "J. D. Curt\\'o and I. C. Zarza and Fernando de la Torre and Irwin King\n  and Michael R. Lyu", "title": "High-resolution Deep Convolutional Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) [Goodfellow et al. 2014] convergence\nin a high-resolution setting with a computational constrain of GPU memory\ncapacity has been beset with difficulty due to the known lack of convergence\nrate stability. In order to boost network convergence of DCGAN (Deep\nConvolutional Generative Adversarial Networks) [Radford et al. 2016] and\nachieve good-looking high-resolution results we propose a new layered network,\nHDCGAN, that incorporates current state-of-the-art techniques for this effect.\nGlasses, a mechanism to arbitrarily improve the final GAN generated results by\nenlarging the input size by a telescope {\\zeta} is also presented. A novel\nbias-free dataset, Curt\\'o & Zarza, containing human faces from different\nethnical groups in a wide variety of illumination conditions and image\nresolutions is introduced. Curt\\'o is enhanced with HDCGAN synthetic images,\nthus being the first GAN augmented dataset of faces. We conduct extensive\nexperiments on CelebA [Liu et al. 2015], CelebA-hq [Karras et al. 2018] and\nCurt\\'o. HDCGAN is the current state-of-the-art in synthetic image generation\non CelebA achieving a MS-SSIM of 0.1978 and a FR\\'ECHET Inception Distance of\n8.44.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 10:47:08 GMT"}, {"version": "v10", "created": "Sat, 26 May 2018 15:26:47 GMT"}, {"version": "v11", "created": "Thu, 31 May 2018 06:25:31 GMT"}, {"version": "v12", "created": "Sun, 10 Jun 2018 12:23:26 GMT"}, {"version": "v13", "created": "Sun, 6 Jan 2019 16:43:48 GMT"}, {"version": "v14", "created": "Thu, 24 Jan 2019 20:03:44 GMT"}, {"version": "v15", "created": "Wed, 20 Feb 2019 16:47:49 GMT"}, {"version": "v16", "created": "Sun, 24 Mar 2019 17:15:00 GMT"}, {"version": "v17", "created": "Tue, 31 Dec 2019 18:47:06 GMT"}, {"version": "v18", "created": "Fri, 17 Apr 2020 16:59:30 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 19:25:17 GMT"}, {"version": "v3", "created": "Sat, 27 Jan 2018 15:03:14 GMT"}, {"version": "v4", "created": "Sat, 3 Feb 2018 10:54:57 GMT"}, {"version": "v5", "created": "Fri, 16 Mar 2018 17:03:28 GMT"}, {"version": "v6", "created": "Tue, 20 Mar 2018 16:53:06 GMT"}, {"version": "v7", "created": "Tue, 27 Mar 2018 18:17:12 GMT"}, {"version": "v8", "created": "Thu, 19 Apr 2018 12:30:42 GMT"}, {"version": "v9", "created": "Thu, 10 May 2018 12:13:59 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Curt\u00f3", "J. D.", ""], ["Zarza", "I. C.", ""], ["de la Torre", "Fernando", ""], ["King", "Irwin", ""], ["Lyu", "Michael R.", ""]]}, {"id": "1711.06500", "submitter": "Fuqing Zhu", "authors": "Fuqing Zhu, Xiangwei Kong, Haiyan Fu, Qi Tian", "title": "Pseudo-positive regularization for deep person re-identification", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intrinsic challenge of person re-identification (re-ID) is the annotation\ndifficulty. This typically means 1) few training samples per identity, and 2)\nthus the lack of diversity among the training samples. Consequently, we face\nhigh risk of over-fitting when training the convolutional neural network (CNN),\na state-of-the-art method in person re-ID. To reduce the risk of over-fitting,\nthis paper proposes a Pseudo Positive Regularization (PPR) method to enrich the\ndiversity of the training data. Specifically, unlabeled data from an\nindependent pedestrian database is retrieved using the target training data as\nquery. A small proportion of these retrieved samples are randomly selected as\nthe Pseudo Positive samples and added to the target training set for the\nsupervised CNN training. The addition of Pseudo Positive samples is therefore a\ndata augmentation method to reduce the risk of over-fitting during CNN\ntraining. We implement our idea in the identification CNN models (i.e.,\nCaffeNet, VGGNet-16 and ResNet-50). On CUHK03 and Market-1501 datasets,\nexperimental results demonstrate that the proposed method consistently improves\nthe baseline and yields competitive performance to the state-of-the-art person\nre-ID methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 11:33:14 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Zhu", "Fuqing", ""], ["Kong", "Xiangwei", ""], ["Fu", "Haiyan", ""], ["Tian", "Qi", ""]]}, {"id": "1711.06504", "submitter": "Luke Oakden-Rayner", "authors": "William Gale, Luke Oakden-Rayner, Gustavo Carneiro, Andrew P. Bradley,\n  Lyle J. Palmer", "title": "Detecting hip fractures with radiologist-level performance using deep\n  neural networks", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed an automated deep learning system to detect hip fractures from\nfrontal pelvic x-rays, an important and common radiological task. Our system\nwas trained on a decade of clinical x-rays (~53,000 studies) and can be applied\nto clinical data, automatically excluding inappropriate and technically\nunsatisfactory studies. We demonstrate diagnostic performance equivalent to a\nhuman radiologist and an area under the ROC curve of 0.994. Translated to\nclinical practice, such a system has the potential to increase the efficiency\nof diagnosis, reduce the need for expensive additional testing, expand access\nto expert level medical image interpretation, and improve overall patient\noutcomes.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 11:56:07 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Gale", "William", ""], ["Oakden-Rayner", "Luke", ""], ["Carneiro", "Gustavo", ""], ["Bradley", "Andrew P.", ""], ["Palmer", "Lyle J.", ""]]}, {"id": "1711.06505", "submitter": "Tiezheng Ge", "authors": "Tiezheng Ge, Liqin Zhao, Guorui Zhou, Keyu Chen, Shuying Liu, Huimin\n  Yi, Zelin Hu, Bochao Liu, Peng Sun, Haoyu Liu, Pengtao Yi, Sui Huang,\n  Zhiqiang Zhang, Xiaoqiang Zhu, Yu Zhang, Kun Gai", "title": "Image Matters: Visually modeling user behaviors using Advanced Model\n  Server", "comments": "CIKM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Taobao, the largest e-commerce platform in China, billions of items are\nprovided and typically displayed with their images. For better user experience\nand business effectiveness, Click Through Rate (CTR) prediction in online\nadvertising system exploits abundant user historical behaviors to identify\nwhether a user is interested in a candidate ad. Enhancing behavior\nrepresentations with user behavior images will help understand user's visual\npreference and improve the accuracy of CTR prediction greatly. So we propose to\nmodel user preference jointly with user behavior ID features and behavior\nimages. However, training with user behavior images brings tens to hundreds of\nimages in one sample, giving rise to a great challenge in both communication\nand computation. To handle these challenges, we propose a novel and efficient\ndistributed machine learning paradigm called Advanced Model Server (AMS). With\nthe well known Parameter Server (PS) framework, each server node handles a\nseparate part of parameters and updates them independently. AMS goes beyond\nthis and is designed to be capable of learning a unified image descriptor model\nshared by all server nodes which embeds large images into low dimensional high\nlevel features before transmitting images to worker nodes. AMS thus\ndramatically reduces the communication load and enables the arduous joint\ntraining process. Based on AMS, the methods of effectively combining the images\nand ID features are carefully studied, and then we propose a Deep Image CTR\nModel. Our approach is shown to achieve significant improvements in both online\nand offline evaluations, and has been deployed in Taobao display advertising\nsystem serving the main traffic.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 11:57:13 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 12:08:39 GMT"}, {"version": "v3", "created": "Tue, 4 Sep 2018 09:11:57 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Ge", "Tiezheng", ""], ["Zhao", "Liqin", ""], ["Zhou", "Guorui", ""], ["Chen", "Keyu", ""], ["Liu", "Shuying", ""], ["Yi", "Huimin", ""], ["Hu", "Zelin", ""], ["Liu", "Bochao", ""], ["Sun", "Peng", ""], ["Liu", "Haoyu", ""], ["Yi", "Pengtao", ""], ["Huang", "Sui", ""], ["Zhang", "Zhiqiang", ""], ["Zhu", "Xiaoqiang", ""], ["Zhang", "Yu", ""], ["Gai", "Kun", ""]]}, {"id": "1711.06526", "submitter": "Wei Fang", "authors": "Chung-Wei Lee, Wei Fang, Chih-Kuan Yeh, Yu-Chiang Frank Wang", "title": "Multi-Label Zero-Shot Learning with Structured Knowledge Graphs", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel deep learning architecture for multi-label\nzero-shot learning (ML-ZSL), which is able to predict multiple unseen class\nlabels for each input instance. Inspired by the way humans utilize semantic\nknowledge between objects of interests, we propose a framework that\nincorporates knowledge graphs for describing the relationships between multiple\nlabels. Our model learns an information propagation mechanism from the semantic\nlabel space, which can be applied to model the interdependencies between seen\nand unseen class labels. With such investigation of structured knowledge graphs\nfor visual reasoning, we show that our model can be applied for solving\nmulti-label classification and ML-ZSL tasks. Compared to state-of-the-art\napproaches, comparable or improved performances can be achieved by our method.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 13:31:57 GMT"}, {"version": "v2", "created": "Sat, 26 May 2018 12:48:10 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Lee", "Chung-Wei", ""], ["Fang", "Wei", ""], ["Yeh", "Chih-Kuan", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "1711.06540", "submitter": "Zhi Gao", "authors": "Zhi Gao, Yuwei Wu, Xingyuan Bu, and Yunde Jia", "title": "Learning a Robust Representation via a Deep Network on Symmetric\n  Positive Definite Manifolds", "comments": "11 pages, 8figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that aggregating convolutional features of a\npre-trained Convolutional Neural Network (CNN) can obtain impressive\nperformance for a variety of visual tasks. The symmetric Positive Definite\n(SPD) matrix becomes a powerful tool due to its remarkable ability to learn an\nappropriate statistic representation to characterize the underlying structure\nof visual features. In this paper, we propose to aggregate deep convolutional\nfeatures into an SPD matrix representation through the SPD generation and the\nSPD transformation under an end-to-end deep network. To this end, several new\nlayers are introduced in our network, including a nonlinear kernel aggregation\nlayer, an SPD matrix transformation layer, and a vectorization layer. The\nnonlinear kernel aggregation layer is employed to aggregate the convolutional\nfeatures into a real SPD matrix directly. The SPD matrix transformation layer\nis designed to construct a more compact and discriminative SPD representation.\nThe vectorization and normalization operations are performed in the\nvectorization layer for reducing the redundancy and accelerating the\nconvergence. The SPD matrix in our network can be considered as a mid-level\nrepresentation bridging convolutional features and high-level semantic\nfeatures. To demonstrate the effectiveness of our method, we conduct extensive\nexperiments on visual classification. Experiment results show that our method\nnotably outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 14:04:12 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 11:19:21 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Gao", "Zhi", ""], ["Wu", "Yuwei", ""], ["Bu", "Xingyuan", ""], ["Jia", "Yunde", ""]]}, {"id": "1711.06564", "submitter": "Kourosh Meshgi", "authors": "Kourosh Meshgi, Shigeyuki Oba, Shin Ishii", "title": "Efficient Diverse Ensemble for Discriminative Co-Tracking", "comments": "CVPR 2018 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble discriminative tracking utilizes a committee of classifiers, to\nlabel data samples, which are in turn, used for retraining the tracker to\nlocalize the target using the collective knowledge of the committee. Committee\nmembers could vary in their features, memory update schemes, or training data,\nhowever, it is inevitable to have committee members that excessively agree\nbecause of large overlaps in their version space. To remove this redundancy and\nhave an effective ensemble learning, it is critical for the committee to\ninclude consistent hypotheses that differ from one-another, covering the\nversion space with minimum overlaps. In this study, we propose an online\nensemble tracker that directly generates a diverse committee by generating an\nefficient set of artificial training. The artificial data is sampled from the\nempirical distribution of the samples taken from both target and background,\nwhereas the process is governed by query-by-committee to shrink the overlap\nbetween classifiers. The experimental results demonstrate that the proposed\nscheme outperforms conventional ensemble trackers on public benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 08:34:42 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 05:46:02 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Meshgi", "Kourosh", ""], ["Oba", "Shigeyuki", ""], ["Ishii", "Shin", ""]]}, {"id": "1711.06583", "submitter": "Pawe{\\l} Liskowski", "authors": "Pawe{\\l} Liskowski, Wojciech Ja\\'skowski, Krzysztof Krawiec", "title": "Learning to Play Othello with Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TG.2018.2799997", "report-no": null, "categories": "cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving superhuman playing level by AlphaGo corroborated the capabilities\nof convolutional neural architectures (CNNs) for capturing complex spatial\npatterns. This result was to a great extent due to several analogies between Go\nboard states and 2D images CNNs have been designed for, in particular\ntranslational invariance and a relatively large board. In this paper, we verify\nwhether CNN-based move predictors prove effective for Othello, a game with\nsignificantly different characteristics, including a much smaller board size\nand complete lack of translational invariance. We compare several CNN\narchitectures and board encodings, augment them with state-of-the-art\nextensions, train on an extensive database of experts' moves, and examine them\nwith respect to move prediction accuracy and playing strength. The empirical\nevaluation confirms high capabilities of neural move predictors and suggests a\nstrong correlation between prediction accuracy and playing strength. The best\nCNNs not only surpass all other 1-ply Othello players proposed to date but\ndefeat (2-ply) Edax, the best open-source Othello player.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 15:14:20 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Liskowski", "Pawe\u0142", ""], ["Ja\u015bkowski", "Wojciech", ""], ["Krawiec", "Krzysztof", ""]]}, {"id": "1711.06588", "submitter": "Osamu Hirose", "authors": "Osamu Hirose", "title": "Dependent landmark drift: robust point set registration with a Gaussian\n  mixture model and a statistical shape model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of point set registration is to find point-by-point correspondences\nbetween point sets, each of which characterizes the shape of an object. Because\nlocal preservation of object geometry is assumed, prevalent algorithms in the\narea can often elegantly solve the problems without using geometric information\nspecific to the objects. This means that registration performance can be\nfurther improved by using prior knowledge of object geometry. In this paper, we\npropose a novel point set registration method using the Gaussian mixture model\nwith prior shape information encoded as a statistical shape model. Our\ntransformation model is defined as a combination of the similar transformation,\nmotion coherence, and the statistical shape model. Therefore, the proposed\nmethod works effectively if the target point set includes outliers and missing\nregions, or if it is rotated. The computational cost can be reduced to linear,\nand therefore the method is scalable to large point sets. The effectiveness of\nthe method will be verified through comparisons with existing algorithms using\ndatasets concerning human body shapes, hands, and faces.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 15:24:17 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 01:11:50 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 01:00:59 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Hirose", "Osamu", ""]]}, {"id": "1711.06597", "submitter": "Kelwin Fernandes", "authors": "Kelwin Fernandes and Jaime S. Cardoso", "title": "Deep Local Binary Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Binary Pattern (LBP) is a traditional descriptor for texture analysis\nthat gained attention in the last decade. Being robust to several properties\nsuch as invariance to illumination translation and scaling, LBPs achieved\nstate-of-the-art results in several applications. However, LBPs are not able to\ncapture high-level features from the image, merely encoding features with low\nabstraction levels. In this work, we propose Deep LBP, which borrow ideas from\nthe deep learning community to improve LBP expressiveness. By using\nparametrized data-driven LBP, we enable successive applications of the LBP\noperators with increasing abstraction levels. We validate the relevance of the\nproposed idea in several datasets from a wide range of applications. Deep LBP\nimproved the performance of traditional and multiscale LBP in all cases.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 15:40:27 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Fernandes", "Kelwin", ""], ["Cardoso", "Jaime S.", ""]]}, {"id": "1711.06606", "submitter": "Faisal Mahmood", "authors": "Faisal Mahmood, Richard Chen, Nicholas J. Durr", "title": "Unsupervised Reverse Domain Adaptation for Synthetic Medical Images via\n  Adversarial Training", "comments": "10 pages, 8 figure", "journal-ref": null, "doi": "10.1109/TMI.2018.2842767", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To realize the full potential of deep learning for medical imaging, large\nannotated datasets are required for training. Such datasets are difficult to\nacquire because labeled medical images are not usually available due to privacy\nissues, lack of experts available for annotation, underrepresentation of rare\nconditions and poor standardization. Lack of annotated data has been addressed\nin conventional vision applications using synthetic images refined via\nunsupervised adversarial training to look like real images. However, this\napproach is difficult to extend to general medical imaging because of the\ncomplex and diverse set of features found in real human tissues. We propose an\nalternative framework that uses a reverse flow, where adversarial training is\nused to make real medical images more like synthetic images, and hypothesize\nthat clinically-relevant features can be preserved via self-regularization.\nThese domain-adapted images can then be accurately interpreted by networks\ntrained on large datasets of synthetic medical images. We test this approach\nfor the notoriously difficult task of depth-estimation from endoscopy. We train\na depth estimator on a large dataset of synthetic images generated using an\naccurate forward model of an endoscope and an anatomically-realistic colon.\nThis network predicts significantly better depths when using synthetic-like\ndomain-adapted images compared to the real images, confirming that the\nclinically-relevant features of depth are preserved.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 16:02:37 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 01:32:42 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Mahmood", "Faisal", ""], ["Chen", "Richard", ""], ["Durr", "Nicholas J.", ""]]}, {"id": "1711.06616", "submitter": "Omid Haji Maghsoudi", "authors": "Omid Haji Maghsoudi", "title": "Superpixels Based Segmentation and SVM Based Classification Method to\n  Distinguish Five Diseases from Normal Regions in Wireless Capsule Endoscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless Capsule Endoscopy (WCE) is relatively a new technology to examine\nthe entire GI trace. During an examination, it captures more than 55,000\nframes. Reviewing all these images is time-consuming and prone to human error.\nIt has been a challenge to develop intelligent methods assisting physicians to\nreview the frames. The WCE frames are captured in 8-bit color depths which\nprovides enough a color range to detect abnormalities. Here, superpixel based\nmethods are proposed to segment five diseases including: bleeding, Crohn's\ndisease, Lymphangiectasia, Xanthoma, and Lymphoid hyperplasia. Two superpixels\nmethods are compared to provide semantic segmentation of these prolific\ndiseases: simple linear iterative clustering (SLIC) and quick shift (QS). The\nsegmented superpixels were classified into two classes (normal and abnormal) by\nsupport vector machine (SVM) using texture and color features. For both\nsuperpixel methods, the accuracy, specificity, sensitivity, and precision\n(SLIC, QS) were around 92%, 93%, 93%, and 88%, respectively. However, SLIC was\ndramatically faster than QS.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 16:25:34 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Maghsoudi", "Omid Haji", ""]]}, {"id": "1711.06620", "submitter": "Xiaodong Cun", "authors": "Xiaodong Cun, Feng Xu, Chi-Man Pun, Hao Gao", "title": "Depth Assisted Full Resolution Network for Single Image-based View\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researches in novel viewpoint synthesis majorly focus on interpolation from\nmulti-view input images. In this paper, we focus on a more challenging and\nill-posed problem that is to synthesize novel viewpoints from one single input\nimage. To achieve this goal, we propose a novel deep learning-based technique.\nWe design a full resolution network that extracts local image features with the\nsame resolution of the input, which contributes to derive high resolution and\nprevent blurry artifacts in the final synthesized images. We also involve a\npre-trained depth estimation network into our system, and thus 3D information\nis able to be utilized to infer the flow field between the input and the target\nimage. Since the depth network is trained by depth order information between\narbitrary pairs of points in the scene, global image features are also involved\ninto our system. Finally, a synthesis layer is used to not only warp the\nobserved pixels to the desired positions but also hallucinate the missing\npixels with recorded pixels. Experiments show that our technique performs well\non images of various scenes, and outperforms the state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 16:50:13 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Cun", "Xiaodong", ""], ["Xu", "Feng", ""], ["Pun", "Chi-Man", ""], ["Gao", "Hao", ""]]}, {"id": "1711.06623", "submitter": "Dan Barnes", "authors": "Dan Barnes, Will Maddern, Geoffrey Pascoe and Ingmar Posner", "title": "Driven to Distraction: Self-Supervised Distractor Learning for Robust\n  Monocular Visual Odometry in Urban Environments", "comments": "International Conference on Robotics and Automation (ICRA), 2018.\n  Video summary: http://youtu.be/ebIrBn_nc-k", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-supervised approach to ignoring \"distractors\" in camera\nimages for the purposes of robustly estimating vehicle motion in cluttered\nurban environments. We leverage offline multi-session mapping approaches to\nautomatically generate a per-pixel ephemerality mask and depth map for each\ninput image, which we use to train a deep convolutional network. At run-time we\nuse the predicted ephemerality and depth as an input to a monocular visual\nodometry (VO) pipeline, using either sparse features or dense photometric\nmatching. Our approach yields metric-scale VO using only a single camera and\ncan recover the correct egomotion even when 90% of the image is obscured by\ndynamic, independently moving objects. We evaluate our robust VO methods on\nmore than 400km of driving from the Oxford RobotCar Dataset and demonstrate\nreduced odometry drift and significantly improved egomotion estimation in the\npresence of large moving vehicles in urban traffic.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 16:54:40 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 14:29:23 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Barnes", "Dan", ""], ["Maddern", "Will", ""], ["Pascoe", "Geoffrey", ""], ["Posner", "Ingmar", ""]]}, {"id": "1711.06636", "submitter": "Hejia Zhang", "authors": "Hejia Zhang, Xia Zhu, Theodore L. Willke", "title": "Segmenting Brain Tumors with Symmetry", "comments": "NIPS ML4H Workshop 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore encoding brain symmetry into a neural network for a brain tumor\nsegmentation task. A healthy human brain is symmetric at a high level of\nabstraction, and the high-level asymmetric parts are more likely to be tumor\nregions. Paying more attention to asymmetries has the potential to boost the\nperformance in brain tumor segmentation. We propose a method to encode brain\nsymmetry into existing neural networks and apply the method to a\nstate-of-the-art neural network for medical imaging segmentation. We evaluate\nour symmetry-encoded network on the dataset from a brain tumor segmentation\nchallenge and verify that the new model extracts information in the training\nimages more efficiently than the original model.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 17:22:30 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Zhang", "Hejia", ""], ["Zhu", "Xia", ""], ["Willke", "Theodore L.", ""]]}, {"id": "1711.06640", "submitter": "Rowan Zellers", "authors": "Rowan Zellers, Mark Yatskar, Sam Thomson, Yejin Choi", "title": "Neural Motifs: Scene Graph Parsing with Global Context", "comments": "CVPR 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of producing structured graph representations of\nvisual scenes. Our work analyzes the role of motifs: regularly appearing\nsubstructures in scene graphs. We present new quantitative insights on such\nrepeated structures in the Visual Genome dataset. Our analysis shows that\nobject labels are highly predictive of relation labels but not vice-versa. We\nalso find that there are recurring patterns even in larger subgraphs: more than\n50% of graphs contain motifs involving at least two relations. Our analysis\nmotivates a new baseline: given object detections, predict the most frequent\nrelation between object pairs with the given labels, as seen in the training\nset. This baseline improves on the previous state-of-the-art by an average of\n3.6% relative improvement across evaluation settings. We then introduce Stacked\nMotif Networks, a new architecture designed to capture higher order motifs in\nscene graphs that further improves over our strong baseline by an average 7.1%\nrelative gain. Our code is available at github.com/rowanz/neural-motifs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 17:33:01 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 16:17:53 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Zellers", "Rowan", ""], ["Yatskar", "Mark", ""], ["Thomson", "Sam", ""], ["Choi", "Yejin", ""]]}, {"id": "1711.06663", "submitter": "Mart\\'in Villanueva", "authors": "Mart\\'in Villanueva and Mauricio Araya", "title": "Multiresolution and Hierarchical Analysis of Astronomical Spectroscopic\n  Cubes using 3D Discrete Wavelet Transform", "comments": "Presented at the 8th International Conference on Pattern Recognition\n  Systems, Madrid-Spain, 2017", "journal-ref": null, "doi": "10.1049/cp.2017.0145", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The intrinsically hierarchical and blended structure of interstellar\nmolecular clouds, plus the always increasing resolution of astronomical\ninstruments, demand advanced and automated pattern recognition techniques for\nidentifying and connecting source components in spectroscopic cubes. We extend\nthe work done in multiresolution analysis using Wavelets for astronomical 2D\nimages to 3D spectroscopic cubes, combining the results with the Dendrograms\napproach to offer a hierarchical representation of connections between sources\nat different scale levels. We test our approach in real data from the ALMA\nobservatory, exploring different Wavelet families and assessing the main\nparameter for source identification (i.e., RMS) at each level. Our approach\nshows that is feasible to perform multiresolution analysis for the spatial and\nfrequency domains simultaneously rather than analyzing each spectral channel\nindependently.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 18:37:00 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 20:51:45 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Villanueva", "Mart\u00edn", ""], ["Araya", "Mauricio", ""]]}, {"id": "1711.06666", "submitter": "Keren Ye", "authors": "Keren Ye, Adriana Kovashka", "title": "ADVISE: Symbolism and External Knowledge for Decoding Advertisements", "comments": "To appear, Proceedings of the European Conference on Computer Vision\n  (ECCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to convey the most content in their limited space, advertisements\nembed references to outside knowledge via symbolism. For example, a motorcycle\nstands for adventure (a positive property the ad wants associated with the\nproduct being sold), and a gun stands for danger (a negative property to\ndissuade viewers from undesirable behaviors). We show how to use symbolic\nreferences to better understand the meaning of an ad. We further show how\nanchoring ad understanding in general-purpose object recognition and image\ncaptioning improves results. We formulate the ad understanding task as matching\nthe ad image to human-generated statements that describe the action that the ad\nprompts, and the rationale it provides for taking this action. Our proposed\nmethod outperforms the state of the art on this task, and on an alternative\nformulation of question-answering on ads. We show additional applications of\nour learned representations for matching ads to slogans, and clustering ads\naccording to their topic, without extra training.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 18:47:25 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 23:41:33 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Ye", "Keren", ""], ["Kovashka", "Adriana", ""]]}, {"id": "1711.06703", "submitter": "Zining Wang", "authors": "Zining Wang, Wei Zhan, Masayoshi Tomizuka", "title": "Fusing Bird View LIDAR Point Cloud and Front View Camera Image for Deep\n  Object Detection", "comments": "10 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for fusing a LIDAR point cloud and camera-captured\nimages in the deep convolutional neural network (CNN). The proposed method\nconstructs a new layer called non-homogeneous pooling layer to transform\nfeatures between bird view map and front view map. The sparse LIDAR point cloud\nis used to construct the mapping between the two maps. The pooling layer allows\nefficient fusion of the bird view and front view features at any stage of the\nnetwork. This is favorable for the 3D-object detection using camera-LIDAR\nfusion in autonomous driving scenarios. A corresponding deep CNN is designed\nand tested on the KITTI bird view object detection dataset, which produces 3D\nbounding boxes from the bird view map. The fusion method shows particular\nbenefit for detection of pedestrians in the bird view compared to other\nfusion-based object detection networks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 19:36:49 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 01:22:13 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 03:52:03 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Wang", "Zining", ""], ["Zhan", "Wei", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "1711.06704", "submitter": "Dmytro Mishkin", "authors": "Dmytro Mishkin, Filip Radenovic, Jiri Matas", "title": "Repeatability Is Not Enough: Learning Affine Regions via\n  Discriminability", "comments": "ECCV 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for learning local affine-covariant regions is presented. We show\nthat maximizing geometric repeatability does not lead to local regions, a.k.a\nfeatures,that are reliably matched and this necessitates descriptor-based\nlearning. We explore factors that influence such learning and registration: the\nloss function, descriptor type, geometric parametrization and the trade-off\nbetween matchability and geometric accuracy and propose a novel hard\nnegative-constant loss function for learning of affine regions. The affine\nshape estimator -- AffNet -- trained with the hard negative-constant loss\noutperforms the state-of-the-art in bag-of-words image retrieval and wide\nbaseline stereo. The proposed training process does not require precisely\ngeometrically aligned patches.The source codes and trained weights are\navailable at https://github.com/ducha-aiki/affnet\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 19:37:28 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 09:51:54 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2018 15:16:50 GMT"}, {"version": "v4", "created": "Tue, 28 Aug 2018 10:01:42 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Mishkin", "Dmytro", ""], ["Radenovic", "Filip", ""], ["Matas", "Jiri", ""]]}, {"id": "1711.06712", "submitter": "Joon Hee Choi", "authors": "Joon Hee Choi, Omar Elgendy, Stanley H. Chan", "title": "Optimal Combination of Image Denoisers", "comments": "IEEE Transaction on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of image denoisers, each having a different denoising capability,\nis there a provably optimal way of combining these denoisers to produce an\noverall better result? An answer to this question is fundamental to designing\nan ensemble of weak estimators for complex scenes. In this paper, we present an\noptimal combination scheme by leveraging deep neural networks and convex\noptimization. The proposed framework, called the Consensus Neural Network\n(CsNet), introduces three new concepts in image denoising: (1) A provably\noptimal procedure to combine the denoised outputs via convex optimization; (2)\nA deep neural network to estimate the mean squared error (MSE) of denoised\nimages without needing the ground truths; (3) An image boosting procedure using\na deep neural network to improve contrast and to recover lost details of the\ncombined images. Experimental results show that CsNet can consistently improve\ndenoising performance for both deterministic and neural network denoisers.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 20:00:26 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 20:53:40 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 17:36:53 GMT"}, {"version": "v4", "created": "Thu, 28 Feb 2019 18:41:53 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Choi", "Joon Hee", ""], ["Elgendy", "Omar", ""], ["Chan", "Stanley H.", ""]]}, {"id": "1711.06721", "submitter": "Carlos Esteves", "authors": "Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, Kostas\n  Daniilidis", "title": "Learning SO(3) Equivariant Representations with Spherical CNNs", "comments": "Camera-ready. Accepted to ECCV'18 as oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of 3D rotation equivariance in convolutional neural\nnetworks. 3D rotations have been a challenging nuisance in 3D classification\ntasks requiring higher capacity and extended data augmentation in order to\ntackle it. We model 3D data with multi-valued spherical functions and we\npropose a novel spherical convolutional network that implements exact\nconvolutions on the sphere by realizing them in the spherical harmonic domain.\nResulting filters have local symmetry and are localized by enforcing smooth\nspectra. We apply a novel pooling on the spectral domain and our operations are\nindependent of the underlying spherical resolution throughout the network. We\nshow that networks with much lower capacity and without requiring data\naugmentation can exhibit performance comparable to the state of the art in\nstandard retrieval and classification benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 20:49:28 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 17:51:41 GMT"}, {"version": "v3", "created": "Fri, 28 Sep 2018 03:19:48 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Esteves", "Carlos", ""], ["Allen-Blanchette", "Christine", ""], ["Makadia", "Ameesh", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1711.06753", "submitter": "Zhenhua Feng", "authors": "Zhen-Hua Feng, Josef Kittler, Muhammad Awais, Patrik Huber, Xiao-Jun\n  Wu", "title": "Wing Loss for Robust Facial Landmark Localisation with Convolutional\n  Neural Networks", "comments": "11 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new loss function, namely Wing loss, for robust facial landmark\nlocalisation with Convolutional Neural Networks (CNNs). We first compare and\nanalyse different loss functions including L2, L1 and smooth L1. The analysis\nof these loss functions suggests that, for the training of a CNN-based\nlocalisation model, more attention should be paid to small and medium range\nerrors. To this end, we design a piece-wise loss function. The new loss\namplifies the impact of errors from the interval (-w, w) by switching from L1\nloss to a modified logarithm function.\n  To address the problem of under-representation of samples with large\nout-of-plane head rotations in the training set, we propose a simple but\neffective boosting strategy, referred to as pose-based data balancing. In\nparticular, we deal with the data imbalance problem by duplicating the minority\ntraining samples and perturbing them by injecting random image rotation,\nbounding box translation and other data augmentation approaches. Last, the\nproposed approach is extended to create a two-stage framework for robust facial\nlandmark localisation. The experimental results obtained on AFLW and 300W\ndemonstrate the merits of the Wing loss function, and prove the superiority of\nthe proposed method over the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 22:26:55 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 21:40:58 GMT"}, {"version": "v3", "created": "Thu, 7 Dec 2017 17:27:54 GMT"}, {"version": "v4", "created": "Sat, 24 Mar 2018 15:37:59 GMT"}, {"version": "v5", "created": "Tue, 23 Oct 2018 20:09:07 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Feng", "Zhen-Hua", ""], ["Kittler", "Josef", ""], ["Awais", "Muhammad", ""], ["Huber", "Patrik", ""], ["Wu", "Xiao-Jun", ""]]}, {"id": "1711.06764", "submitter": "Eli (Omid) David", "authors": "Sarit Chicotay, Eli David, Nathan S. Netanyahu", "title": "Image Registration of Very Large Images via Genetic Programming", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  Workshop on Registration of Very Large Images, pp. 323-328, Columbus, OH,\n  June 2014", "doi": "10.1109/CVPRW.2014.56", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration (IR) is a fundamental task in image processing for\nmatching two or more images of the same scene taken at different times, from\ndifferent viewpoints and/or by different sensors. Due to the enormous diversity\nof IR applications, automatic IR remains a challenging problem to this day. A\nwide range of techniques has been developed for various data types and\nproblems. However, they might not handle effectively very large images, which\ngive rise usually to more complex transformations, e.g., deformations and\nvarious other distortions.\n  In this paper we present a genetic programming (GP)-based approach for IR,\nwhich could offer a significant advantage in dealing with very large images, as\nit does not make any prior assumptions about the transformation model. Thus, by\nincorporating certain generic building blocks into the proposed GP framework,\nwe hope to realize a large set of specialized transformations that should yield\naccurate registration of very large images.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:12:26 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 07:21:37 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Chicotay", "Sarit", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06765", "submitter": "Eli (Omid) David", "authors": "Sarit Chicotay, Eli David, Nathan S. Netanyahu", "title": "A Two-Phase Genetic Algorithm for Image Registration", "comments": null, "journal-ref": "ACM Genetic and Evolutionary Computation Conference (GECCO), pp.\n  189-190, Berlin, Germany, July 2017", "doi": "10.1145/3067695.3076017", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Registration (IR) is the process of aligning two (or more) images of\nthe same scene taken at different times, different viewpoints and/or by\ndifferent sensors. It is an important, crucial step in various image analysis\ntasks where multiple data sources are integrated/fused, in order to extract\nhigh-level information.\n  Registration methods usually assume a relevant transformation model for a\ngiven problem domain. The goal is to search for the \"optimal\" instance of the\ntransformation model assumed with respect to a similarity measure in question.\n  In this paper we present a novel genetic algorithm (GA)-based approach for\nIR. Since GA performs effective search in various optimization problems, it\ncould prove useful also for IR. Indeed, various GAs have been proposed for IR.\nHowever, most of them assume certain constraints, which simplify the\ntransformation model, restrict the search space or make additional\npreprocessing requirements. In contrast, we present a generalized GA-based\nsolution for an almost fully affine transformation model, which achieves\ncompetitive results without such limitations using a two-phase method and a\nmulti-objective optimization (MOO) approach.\n  We present good results for multiple dataset and demonstrate the robustness\nof our method in the presence of noisy data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:15:19 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Chicotay", "Sarit", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06766", "submitter": "Eli (Omid) David", "authors": "Dror Sholomon, Eli David, Nathan S. Netanyahu", "title": "Genetic Algorithm-Based Solver for Very Large Multiple Jigsaw Puzzles of\n  Unknown Dimensions and Piece Orientation", "comments": null, "journal-ref": "ACM Genetic and Evolutionary Computation Conference (GECCO), pages\n  1191-1198, Vancouver, Canada, July 2014", "doi": "10.1145/2576768.2598289", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the first genetic algorithm (GA)-based solver for\njigsaw puzzles of unknown puzzle dimensions and unknown piece location and\norientation. Our solver uses a novel crossover technique, and sets a new\nstate-of-the-art in terms of the puzzle sizes solved and the accuracy obtained.\nThe results are significantly improved, even when compared to previous solvers\nassuming known puzzle dimensions. Moreover, the solver successfully contends\nwith a mixed bag of multiple puzzle pieces, assembling simultaneously all\npuzzles.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:17:20 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Sholomon", "Dror", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06767", "submitter": "Eli (Omid) David", "authors": "Dror Sholomon, Eli David, Nathan S. Netanyahu", "title": "An Automatic Solver for Very Large Jigsaw Puzzles Using Genetic\n  Algorithms", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.06769", "journal-ref": "Genetic Programming and Evolvable Machines, Vol. 17, No. 3, pp.\n  291-313, September 2016", "doi": "10.1007/s10710-015-9258-0", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the first effective genetic algorithm (GA)-based\njigsaw puzzle solver. We introduce a novel crossover procedure that merges two\n\"parent\" solutions to an improved \"child\" configuration by detecting,\nextracting, and combining correctly assembled puzzle segments. The solver\nproposed exhibits state-of-the-art performance, as far as handling previously\nattempted puzzles more accurately and efficiently, as well puzzle sizes that\nhave not been attempted before. The extended experimental results provided in\nthis paper include, among others, a thorough inspection of up to 30,745-piece\npuzzles (compared to previous attempts on 22,755-piece puzzles), using a\nconsiderably faster concurrent implementation of the algorithm. Furthermore, we\nexplore the impact of different phases of the novel crossover operator by\nexperimenting with several variants of the GA. Finally, we compare different\nfitness functions and their effect on the overall results of the GA-based\nsolver.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:17:23 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Sholomon", "Dror", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06768", "submitter": "Eli (Omid) David", "authors": "Dror Sholomon, Eli David, Nathan S. Netanyahu", "title": "A Generalized Genetic Algorithm-Based Solver for Very Large Jigsaw\n  Puzzles of Complex Types", "comments": null, "journal-ref": "AAAI Conference on Artificial Intelligence, pages 2839-2845,\n  Quebec City, Canada, July 2014", "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce new types of square-piece jigsaw puzzles, where in\naddition to the unknown location and orientation of each piece, a piece might\nalso need to be flipped. These puzzles, which are associated with a number of\nreal world problems, are considerably harder, from a computational standpoint.\nSpecifically, we present a novel generalized genetic algorithm (GA)-based\nsolver that can handle puzzle pieces of unknown location and orientation (Type\n2 puzzles) and (two-sided) puzzle pieces of unknown location, orientation, and\nface (Type 4 puzzles). To the best of our knowledge, our solver provides a new\nstate-of-the-art, solving previously attempted puzzles faster and far more\naccurately, handling puzzle sizes that have never been attempted before, and\nassembling the newly introduced two-sided puzzles automatically and\neffectively. This paper also presents, among other results, the most extensive\nset of experimental results, compiled as of yet, on Type 2 puzzles.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:17:29 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Sholomon", "Dror", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06769", "submitter": "Eli (Omid) David", "authors": "Dror Sholomon, Eli David, Nathan S. Netanyahu", "title": "A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.06767", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  pages 1767-1774, Portland, OR, June 2013", "doi": "10.1109/CVPR.2013.231", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the first effective automated, genetic algorithm\n(GA)-based jigsaw puzzle solver. We introduce a novel procedure of merging two\n\"parent\" solutions to an improved \"child\" solution by detecting, extracting,\nand combining correctly assembled puzzle segments. The solver proposed exhibits\nstate-of-the-art performance solving previously attempted puzzles faster and\nfar more accurately, and also puzzles of size never before attempted. Other\ncontributions include the creation of a benchmark of large images, previously\nunavailable. We share the data sets and all of our results for future testing\nand comparative evaluation of jigsaw puzzle solvers.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:17:33 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Sholomon", "Dror", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.06778", "submitter": "Andrea Zunino", "authors": "Sarah Adel Bargal, Andrea Zunino, Donghyun Kim, Jianming Zhang,\n  Vittorio Murino, Stan Sclaroff", "title": "Excitation Backprop for RNNs", "comments": "CVPR 2018 Camera Ready Version", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep models are state-of-the-art for many vision tasks including video action\nrecognition and video captioning. Models are trained to caption or classify\nactivity in videos, but little is known about the evidence used to make such\ndecisions. Grounding decisions made by deep networks has been studied in\nspatial visual content, giving more insight into model predictions for images.\nHowever, such studies are relatively lacking for models of spatiotemporal\nvisual content - videos. In this work, we devise a formulation that\nsimultaneously grounds evidence in space and time, in a single pass, using\ntop-down saliency. We visualize the spatiotemporal cues that contribute to a\ndeep model's classification/captioning output using the model's internal\nrepresentation. Based on these spatiotemporal cues, we are able to localize\nsegments within a video that correspond with a specific action, or phrase from\na caption, without explicitly optimizing/training for these tasks.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 00:22:17 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 19:23:46 GMT"}, {"version": "v3", "created": "Thu, 8 Mar 2018 16:49:13 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Bargal", "Sarah Adel", ""], ["Zunino", "Andrea", ""], ["Kim", "Donghyun", ""], ["Zhang", "Jianming", ""], ["Murino", "Vittorio", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1711.06787", "submitter": "Risheng Liu", "authors": "Risheng Liu, Xin Fan, Minjun Hou, Zhiying Jiang, Zhongxuan Luo and Lei\n  Zhang", "title": "Learning Aggregated Transmission Propagation Networks for Haze Removal\n  and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image dehazing is an important low-level vision task with many\napplications. Early researches have investigated different kinds of visual\npriors to address this problem. However, they may fail when their assumptions\nare not valid on specific images. Recent deep networks also achieve relatively\ngood performance in this task. But unfortunately, due to the disappreciation of\nrich physical rules in hazes, large amounts of data are required for their\ntraining. More importantly, they may still fail when there exist completely\ndifferent haze distributions in testing images. By considering the\ncollaborations of these two perspectives, this paper designs a novel residual\narchitecture to aggregate both prior (i.e., domain knowledge) and data (i.e.,\nhaze distribution) information to propagate transmissions for scene radiance\nestimation. We further present a variational energy based perspective to\ninvestigate the intrinsic propagation behavior of our aggregated deep model. In\nthis way, we actually bridge the gap between prior driven models and data\ndriven networks and leverage advantages but avoid limitations of previous\ndehazing approaches. A lightweight learning framework is proposed to train our\npropagation network. Finally, by introducing a taskaware image separation\nformulation with a flexible optimization scheme, we extend the proposed model\nfor more challenging vision tasks, such as underwater image enhancement and\nsingle image rain removal. Experiments on both synthetic and realworld images\ndemonstrate the effectiveness and efficiency of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 01:37:04 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 14:43:16 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Liu", "Risheng", ""], ["Fan", "Xin", ""], ["Hou", "Minjun", ""], ["Jiang", "Zhiying", ""], ["Luo", "Zhongxuan", ""], ["Zhang", "Lei", ""]]}, {"id": "1711.06794", "submitter": "Pan Lu", "authors": "Pan Lu, Hongsheng Li, Wei Zhang, Jianyong Wang, Xiaogang Wang", "title": "Co-attending Free-form Regions and Detections with Multi-modal\n  Multiplicative Feature Embedding for Visual Question Answering", "comments": "To appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Visual Question Answering (VQA) task has gained increasing\nattention in artificial intelligence. Existing VQA methods mainly adopt the\nvisual attention mechanism to associate the input question with corresponding\nimage regions for effective question answering. The free-form region based and\nthe detection-based visual attention mechanisms are mostly investigated, with\nthe former ones attending free-form image regions and the latter ones attending\npre-specified detection-box regions. We argue that the two attention mechanisms\nare able to provide complementary information and should be effectively\nintegrated to better solve the VQA problem. In this paper, we propose a novel\ndeep neural network for VQA that integrates both attention mechanisms. Our\nproposed framework effectively fuses features from free-form image regions,\ndetection boxes, and question representations via a multi-modal multiplicative\nfeature embedding scheme to jointly attend question-related free-form image\nregions and detection boxes for more accurate question answering. The proposed\nmethod is extensively evaluated on two publicly available datasets, COCO-QA and\nVQA, and outperforms state-of-the-art approaches. Source code is available at\nhttps://github.com/lupantech/dual-mfa-vqa.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 02:07:34 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 08:34:43 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Lu", "Pan", ""], ["Li", "Hongsheng", ""], ["Zhang", "Wei", ""], ["Wang", "Jianyong", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1711.06809", "submitter": "\\'Erico Marco Pereira", "authors": "\\'Erico M. Pereira, Ricardo da S. Torres, Jefersson A. dos Santos", "title": "A Genetic Algorithm Approach for ImageRepresentation Learning through\n  Color Quantization", "comments": "Submitted to Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": "MTAP-D-19-02724R2", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decades, hand-crafted feature extractors have been used to\nencode image visual properties into feature vectors. Recently, data-driven\nfeature learning approaches have been successfully explored as alternatives for\nproducing more representative visual features. In this work, we combine both\nresearch venues, focusing on the color quantization problem. We propose two\ndata-driven approaches to learn image representations through the search for\noptimized quantization schemes, which lead to more effective feature extraction\nalgorithms and compact representations. Our strategy employs Genetic Algorithm,\na soft-computing apparatus successfully utilized in\nInformation-retrieval-related optimization problems. We hypothesize that\nchanging the quantization affects the quality of image description approaches,\nleading to effective and efficient representations. We evaluate our approaches\nin content-based image retrieval tasks, considering eight well-known datasets\nwith different visual properties. Results indicate that the approach focused on\nrepresentation effectiveness outperformed baselines in all tested scenarios.\nThe other approach, which also considers the size of created representations,\nproduced competitive results keeping or even reducing the dimensionality of\nfeature vectors up to 25%.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 04:08:34 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 17:49:16 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 14:03:43 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Pereira", "\u00c9rico M.", ""], ["Torres", "Ricardo da S.", ""], ["Santos", "Jefersson A. dos", ""]]}, {"id": "1711.06821", "submitter": "Guillem Collell", "authors": "Guillem Collell, Luc Van Gool, Marie-Francine Moens", "title": "Acquiring Common Sense Spatial Knowledge through Implicit Spatial\n  Templates", "comments": "To appear at AAAI 2018 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial understanding is a fundamental problem with wide-reaching real-world\napplications. The representation of spatial knowledge is often modeled with\nspatial templates, i.e., regions of acceptability of two objects under an\nexplicit spatial relationship (e.g., \"on\", \"below\", etc.). In contrast with\nprior work that restricts spatial templates to explicit spatial prepositions\n(e.g., \"glass on table\"), here we extend this concept to implicit spatial\nlanguage, i.e., those relationships (generally actions) for which the spatial\narrangement of the objects is only implicitly implied (e.g., \"man riding\nhorse\"). In contrast with explicit relationships, predicting spatial\narrangements from implicit spatial language requires significant common sense\nspatial understanding. Here, we introduce the task of predicting spatial\ntemplates for two objects under a relationship, which can be seen as a spatial\nquestion-answering task with a (2D) continuous output (\"where is the man w.r.t.\na horse when the man is walking the horse?\"). We present two simple\nneural-based models that leverage annotated images and structured text to learn\nthis task. The good performance of these models reveals that spatial locations\nare to a large extent predictable from implicit spatial language. Crucially,\nthe models attain similar performance in a challenging generalized setting,\nwhere the object-relation-object combinations (e.g.,\"man walking dog\") have\nnever been seen before. Next, we go one step further by presenting the models\nwith unseen objects (e.g., \"dog\"). In this scenario, we show that leveraging\nword embeddings enables the models to output accurate spatial predictions,\nproving that the models acquire solid common sense spatial knowledge allowing\nfor such generalization.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 07:00:44 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 02:41:36 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 15:23:13 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Collell", "Guillem", ""], ["Van Gool", "Luc", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1711.06828", "submitter": "Huaxin Xiao", "authors": "Huaxin Xiao, Yunchao Wei, Yu Liu, Maojun Zhang, Jiashi Feng", "title": "Transferable Semi-supervised Semantic Segmentation", "comments": "Minor update of arXiv:1711.06828", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of deep learning based semantic segmentation models heavily\ndepends on sufficient data with careful annotations. However, even the largest\npublic datasets only provide samples with pixel-level annotations for rather\nlimited semantic categories. Such data scarcity critically limits scalability\nand applicability of semantic segmentation models in real applications. In this\npaper, we propose a novel transferable semi-supervised semantic segmentation\nmodel that can transfer the learned segmentation knowledge from a few strong\ncategories with pixel-level annotations to unseen weak categories with only\nimage-level annotations, significantly broadening the applicable territory of\ndeep segmentation models. In particular, the proposed model consists of two\ncomplementary and learnable components: a Label transfer Network (L-Net) and a\nPrediction transfer Network (P-Net). The L-Net learns to transfer the\nsegmentation knowledge from strong categories to the images in the weak\ncategories and produces coarse pixel-level semantic maps, by effectively\nexploiting the similar appearance shared across categories. Meanwhile, the\nP-Net tailors the transferred knowledge through a carefully designed\nadversarial learning strategy and produces refined segmentation results with\nbetter details. Integrating the L-Net and P-Net achieves 96.5% and 89.4%\nperformance of the fully-supervised baseline using 50% and 0% categories with\npixel-level annotations respectively on PASCAL VOC 2012. With such a novel\ntransfer mechanism, our proposed model is easily generalizable to a variety of\nnew categories, only requiring image-level annotations, and offers appealing\nscalability in real applications.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 08:56:06 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 01:53:52 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Xiao", "Huaxin", ""], ["Wei", "Yunchao", ""], ["Liu", "Yu", ""], ["Zhang", "Maojun", ""], ["Feng", "Jiashi", ""]]}, {"id": "1711.06834", "submitter": "Radu Horaud P", "authors": "St\\'ephane Lathuili\\`ere, Benoit Mass\\'e, Pablo Mesejo and Radu Horaud", "title": "Neural Network Based Reinforcement Learning for Audio-Visual Gaze\n  Control in Human-Robot Interaction", "comments": "Paper submitted to Pattern Recognition Letters", "journal-ref": "Pattern Recognition Letters, vol. 118, 2019, 61-71", "doi": "10.1016/j.patrec.2018.05.023", "report-no": null, "categories": "cs.RO cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel neural network-based reinforcement learning\napproach for robot gaze control. Our approach enables a robot to learn and to\nadapt its gaze control strategy for human-robot interaction neither with the\nuse of external sensors nor with human supervision. The robot learns to focus\nits attention onto groups of people from its own audio-visual experiences,\nindependently of the number of people, of their positions and of their physical\nappearances. In particular, we use a recurrent neural network architecture in\ncombination with Q-learning to find an optimal action-selection policy; we\npre-train the network using a simulated environment that mimics realistic\nscenarios that involve speaking/silent participants, thus avoiding the need of\ntedious sessions of a robot interacting with people. Our experimental\nevaluation suggests that the proposed method is robust against parameter\nestimation, i.e. the parameter values yielded by the method do not have a\ndecisive impact on the performance. The best results are obtained when both\naudio and visual information is jointly used. Experiments with the Nao robot\nindicate that our framework is a step forward towards the autonomous learning\nof socially acceptable gaze behavior.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 09:19:47 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 15:07:23 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Lathuili\u00e8re", "St\u00e9phane", ""], ["Mass\u00e9", "Benoit", ""], ["Mesejo", "Pablo", ""], ["Horaud", "Radu", ""]]}, {"id": "1711.06853", "submitter": "Nick Pawlowski", "authors": "Nick Pawlowski, Sofia Ira Ktena, Matthew C.H. Lee, Bernhard Kainz,\n  Daniel Rueckert, Ben Glocker, Martin Rajchl", "title": "DLTK: State of the Art Reference Implementations for Deep Learning on\n  Medical Images", "comments": "Submitted to Medical Imaging Meets NIPS 2017, Code at\n  https://github.com/DLTK/DLTK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DLTK, a toolkit providing baseline implementations for efficient\nexperimentation with deep learning methods on biomedical images. It builds on\ntop of TensorFlow and its high modularity and easy-to-use examples allow for a\nlow-threshold access to state-of-the-art implementations for typical medical\nimaging problems. A comparison of DLTK's reference implementations of popular\nnetwork architectures for image segmentation demonstrates new top performance\non the publicly available challenge data \"Multi-Atlas Labeling Beyond the\nCranial Vault\". The average test Dice similarity coefficient of $81.5$ exceeds\nthe previously best performing CNN ($75.7$) and the accuracy of the challenge\nwinning method ($79.0$).\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 12:31:10 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Pawlowski", "Nick", ""], ["Ktena", "Sofia Ira", ""], ["Lee", "Matthew C. H.", ""], ["Kainz", "Bernhard", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""], ["Rajchl", "Martin", ""]]}, {"id": "1711.06897", "submitter": "Shifeng Zhang", "authors": "Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, Stan Z. Li", "title": "Single-Shot Refinement Neural Network for Object Detection", "comments": "14 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For object detection, the two-stage approach (e.g., Faster R-CNN) has been\nachieving the highest accuracy, whereas the one-stage approach (e.g., SSD) has\nthe advantage of high efficiency. To inherit the merits of both while\novercoming their disadvantages, in this paper, we propose a novel single-shot\nbased detector, called RefineDet, that achieves better accuracy than two-stage\nmethods and maintains comparable efficiency of one-stage methods. RefineDet\nconsists of two inter-connected modules, namely, the anchor refinement module\nand the object detection module. Specifically, the former aims to (1) filter\nout negative anchors to reduce search space for the classifier, and (2)\ncoarsely adjust the locations and sizes of anchors to provide better\ninitialization for the subsequent regressor. The latter module takes the\nrefined anchors as the input from the former to further improve the regression\nand predict multi-class label. Meanwhile, we design a transfer connection block\nto transfer the features in the anchor refinement module to predict locations,\nsizes and class labels of objects in the object detection module. The\nmulti-task loss function enables us to train the whole network in an end-to-end\nway. Extensive experiments on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO\ndemonstrate that RefineDet achieves state-of-the-art detection accuracy with\nhigh efficiency. Code is available at https://github.com/sfzhang15/RefineDet\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 17:05:34 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 02:54:00 GMT"}, {"version": "v3", "created": "Wed, 3 Jan 2018 13:01:54 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Zhang", "Shifeng", ""], ["Wen", "Longyin", ""], ["Bian", "Xiao", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "1711.06918", "submitter": "George He Z", "authors": "George He, Sami Oueida, Tucker Ward", "title": "Gazing into the Abyss: Real-time Gaze Estimation", "comments": "9 pages, computer vision, source code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze and face tracking algorithms have traditionally battled a compromise\nbetween computational complexity and accuracy; the most accurate neural net\nalgorithms cannot be implemented in real time, but less complex real-time\nalgorithms suffer from higher error. This project seeks to better bridge that\ngap by improving on real-time eye and facial recognition algorithms in order to\ndevelop accurate, real-time gaze estimation with an emphasis on minimizing\ntraining data and computational complexity. Our goal is to use eye and facial\nrecognition techniques to enable users to perform limited tasks based on gaze\nand facial input using only a standard, low-quality web cam found in most\nmodern laptops and smart phones and the limited computational power and\ntraining data typical of those scenarios. We therefore identified seven\npromising, fundamentally different algorithms based on different user features\nand developed one outstanding, one workable, and one honorable mention gaze\ntracking pipelines that match the performance of modern gaze trackers while\nusing no training data.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 18:53:40 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["He", "George", ""], ["Oueida", "Sami", ""], ["Ward", "Tucker", ""]]}, {"id": "1711.06948", "submitter": "Zhizhen Liang Dr.", "authors": "Zhizheng Liang, Lei Zhang, Jin Liu, Yong Zhou", "title": "A novel total variation model based on kernel functions and its\n  application", "comments": "22 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The total variation (TV) model and its related variants have already been\nproposed for image processing in previous literature. In this paper a novel\ntotal variation model based on kernel functions is proposed. In this novel\nmodel, we first map each pixel value of an image into a Hilbert space by using\na nonlinear map, and then define a coupled image of an original image in order\nto construct a kernel function. Finally, the proposed model is solved in a\nkernel function space instead of in the projecting space from a nonlinear map.\nFor the proposed model, we theoretically show under what conditions the mapping\nimage is in the space of bounded variation when the original image is in the\nspace of bounded variation. It is also found that the proposed model further\nextends the generalized TV model and the information from three different\nchannels of color images can be fused by adopting various kernel functions. A\nseries of experiments on some gray and color images are carried out to\ndemonstrate the effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 01:30:44 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Liang", "Zhizheng", ""], ["Zhang", "Lei", ""], ["Liu", "Jin", ""], ["Zhou", "Yong", ""]]}, {"id": "1711.06959", "submitter": "Ziming Zhang", "authors": "Ziming Zhang, Yuanwei Wu, Guanghui Wang", "title": "BPGrad: Towards Global Optimality in Deep Learning via Branch and\n  Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the global optimality in deep learning (DL) has been attracting\nmore and more attention recently. Conventional DL solvers, however, have not\nbeen developed intentionally to seek for such global optimality. In this paper\nwe propose a novel approximation algorithm, BPGrad, towards optimizing deep\nmodels globally via branch and pruning. Our BPGrad algorithm is based on the\nassumption of Lipschitz continuity in DL, and as a result it can adaptively\ndetermine the step size for current gradient given the history of previous\nupdates, wherein theoretically no smaller steps can achieve the global\noptimality. We prove that, by repeating such branch-and-pruning procedure, we\ncan locate the global optimality within finite iterations. Empirically an\nefficient solver based on BPGrad for DL is proposed as well, and it outperforms\nconventional DL solvers such as Adagrad, Adadelta, RMSProp, and Adam in the\ntasks of object recognition, detection, and segmentation.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 02:44:31 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Zhang", "Ziming", ""], ["Wu", "Yuanwei", ""], ["Wang", "Guanghui", ""]]}, {"id": "1711.06969", "submitter": "Swami Sankaranarayanan", "authors": "Swami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Ser Nam Lim, Rama\n  Chellappa", "title": "Learning from Synthetic Data: Addressing Domain Shift for Semantic\n  Segmentation", "comments": "Accepted as spotlight talk at CVPR 2018. Code available here:\n  https://github.com/swamiviv/LSD-seg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Domain Adaptation is a problem of immense importance in computer\nvision. Previous approaches showcase the inability of even deep neural networks\nto learn informative representations across domain shift. This problem is more\nsevere for tasks where acquiring hand labeled data is extremely hard and\ntedious. In this work, we focus on adapting the representations learned by\nsegmentation networks across synthetic and real domains. Contrary to previous\napproaches that use a simple adversarial objective or superpixel information to\naid the process, we propose an approach based on Generative Adversarial\nNetworks (GANs) that brings the embeddings closer in the learned feature space.\nTo showcase the generality and scalability of our approach, we show that we can\nachieve state of the art results on two challenging scenarios of synthetic to\nreal domain adaptation. Additional exploratory experiments show that our\napproach: (1) generalizes to unseen domains and (2) results in improved\nalignment of source and target distributions.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 05:25:24 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 21:48:18 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Sankaranarayanan", "Swami", ""], ["Balaji", "Yogesh", ""], ["Jain", "Arpit", ""], ["Lim", "Ser Nam", ""], ["Chellappa", "Rama", ""]]}, {"id": "1711.06976", "submitter": "Lex Fridman", "authors": "Lex Fridman, Daniel E. Brown, Michael Glazer, William Angell, Spencer\n  Dodd, Benedikt Jenik, Jack Terwilliger, Aleksandr Patsekin, Julia\n  Kindelsberger, Li Ding, Sean Seaman, Alea Mehler, Andrew Sipperley, Anthony\n  Pettinato, Bobbie Seppelt, Linda Angell, Bruce Mehler, Bryan Reimer", "title": "MIT Advanced Vehicle Technology Study: Large-Scale Naturalistic Driving\n  Study of Driver Behavior and Interaction with Automation", "comments": null, "journal-ref": "IEEE Access, vol. 7, pp. 102021-102038, 2019", "doi": "10.1109/ACCESS.2019.2926040", "report-no": null, "categories": "cs.CY cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the foreseeble future, human beings will likely remain an integral part\nof the driving task, monitoring the AI system as it performs anywhere from just\nover 0% to just under 100% of the driving. The governing objectives of the MIT\nAutonomous Vehicle Technology (MIT-AVT) study are to (1) undertake large-scale\nreal-world driving data collection that includes high-definition video to fuel\nthe development of deep learning based internal and external perception\nsystems, (2) gain a holistic understanding of how human beings interact with\nvehicle automation technology by integrating video data with vehicle state\ndata, driver characteristics, mental models, and self-reported experiences with\ntechnology, and (3) identify how technology and other factors related to\nautomation adoption and use can be improved in ways that save lives. In\npursuing these objectives, we have instrumented 23 Tesla Model S and Model X\nvehicles, 2 Volvo S90 vehicles, 2 Range Rover Evoque, and 2 Cadillac CT6\nvehicles for both long-term (over a year per driver) and medium term (one month\nper driver) naturalistic driving data collection. Furthermore, we are\ncontinually developing new methods for analysis of the massive-scale dataset\ncollected from the instrumented vehicle fleet. The recorded data streams\ninclude IMU, GPS, CAN messages, and high-definition video streams of the driver\nface, the driver cabin, the forward roadway, and the instrument cluster (on\nselect vehicles). The study is on-going and growing. To date, we have 122\nparticipants, 15,610 days of participation, 511,638 miles, and 7.1 billion\nvideo frames. This paper presents the design of the study, the data collection\nhardware, the processing of the data, and the computer vision algorithms\ncurrently being used to extract actionable knowledge from the data.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 06:46:21 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 04:02:20 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 01:13:57 GMT"}, {"version": "v4", "created": "Wed, 14 Aug 2019 11:17:00 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Fridman", "Lex", ""], ["Brown", "Daniel E.", ""], ["Glazer", "Michael", ""], ["Angell", "William", ""], ["Dodd", "Spencer", ""], ["Jenik", "Benedikt", ""], ["Terwilliger", "Jack", ""], ["Patsekin", "Aleksandr", ""], ["Kindelsberger", "Julia", ""], ["Ding", "Li", ""], ["Seaman", "Sean", ""], ["Mehler", "Alea", ""], ["Sipperley", "Andrew", ""], ["Pettinato", "Anthony", ""], ["Seppelt", "Bobbie", ""], ["Angell", "Linda", ""], ["Mehler", "Bruce", ""], ["Reimer", "Bryan", ""]]}, {"id": "1711.06998", "submitter": "Junjie Zhang", "authors": "Junjie Zhang, Qi Wu, Jian Zhang, Chunhua Shen, Jianfeng Lu", "title": "Kill Two Birds with One Stone: Weakly-Supervised Neural Network for\n  Image Annotation and Tag Refinement", "comments": "AAAI-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of social images has exploded by the wide adoption of social\nnetworks, and people like to share their comments about them. These comments\ncan be a description of the image, or some objects, attributes, scenes in it,\nwhich are normally used as the user-provided tags. However, it is well-known\nthat user-provided tags are incomplete and imprecise to some extent. Directly\nusing them can damage the performance of related applications, such as the\nimage annotation and retrieval. In this paper, we propose to learn an image\nannotation model and refine the user-provided tags simultaneously in a\nweakly-supervised manner. The deep neural network is utilized as the image\nfeature learning and backbone annotation model, while visual consistency,\nsemantic dependency, and user-error sparsity are introduced as the constraints\nat the batch level to alleviate the tag noise. Therefore, our model is highly\nflexible and stable to handle large-scale image sets. Experimental results on\ntwo benchmark datasets indicate that our proposed model achieves the best\nperformance compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 10:47:39 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Zhang", "Junjie", ""], ["Wu", "Qi", ""], ["Zhang", "Jian", ""], ["Shen", "Chunhua", ""], ["Lu", "Jianfeng", ""]]}, {"id": "1711.07011", "submitter": "Ilke Cugu", "authors": "\\.Ilke \\c{C}u\\u{g}u, Eren \\c{S}ener, Emre Akba\\c{s}", "title": "MicroExpNet: An Extremely Small and Fast Model For Expression\n  Recognition From Face Images", "comments": "International Conference on Image Processing Theory, Tools and\n  Applications (IPTA) 2019 camera ready version. Codes are available at:\n  https://github.com/cuguilke/microexpnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is aimed at creating extremely small and fast convolutional neural\nnetworks (CNN) for the problem of facial expression recognition (FER) from\nfrontal face images. To this end, we employed the popular knowledge\ndistillation (KD) method and identified two major shortcomings with its use: 1)\na fine-grained grid search is needed for tuning the temperature hyperparameter\nand 2) to find the optimal size-accuracy balance, one needs to search for the\nfinal network size (or the compression rate). On the other hand, KD is proved\nto be useful for model compression for the FER problem, and we discovered that\nits effects gets more and more significant with the decreasing model size. In\naddition, we hypothesized that translation invariance achieved using\nmax-pooling layers would not be useful for the FER problem as the expressions\nare sensitive to small, pixel-wise changes around the eye and the mouth.\nHowever, we have found an intriguing improvement on generalization when\nmax-pooling is used. We conducted experiments on two widely-used FER datasets,\nCK+ and Oulu-CASIA. Our smallest model (MicroExpNet), obtained using knowledge\ndistillation, is less than 1MB in size and works at 1851 frames per second on\nan Intel i7 CPU. Despite being less accurate than the state-of-the-art,\nMicroExpNet still provides significant insights for designing a\nmicroarchitecture for the FER problem.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 12:31:09 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 08:40:17 GMT"}, {"version": "v3", "created": "Thu, 17 Jan 2019 07:38:19 GMT"}, {"version": "v4", "created": "Tue, 24 Dec 2019 10:44:15 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["\u00c7u\u011fu", "\u0130lke", ""], ["\u015eener", "Eren", ""], ["Akba\u015f", "Emre", ""]]}, {"id": "1711.07027", "submitter": "Deng Weijian", "authors": "Weijian Deng, Liang Zheng, Qixiang Ye, Guoliang Kang, Yi Yang, Jianbin\n  Jiao", "title": "Image-Image Domain Adaptation with Preserved Self-Similarity and\n  Domain-Dissimilarity for Person Re-identification", "comments": "Accepted in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Person re-identification (re-ID) models trained on one domain often fail to\ngeneralize well to another. In our attempt, we present a \"learning via\ntranslation\" framework. In the baseline, we translate the labeled images from\nsource to target domain in an unsupervised manner. We then train re-ID models\nwith the translated images by supervised methods. Yet, being an essential part\nof this framework, unsupervised image-image translation suffers from the\ninformation loss of source-domain labels during translation.\n  Our motivation is two-fold. First, for each image, the discriminative cues\ncontained in its ID label should be maintained after translation. Second, given\nthe fact that two domains have entirely different persons, a translated image\nshould be dissimilar to any of the target IDs. To this end, we propose to\npreserve two types of unsupervised similarities, 1) self-similarity of an image\nbefore and after translation, and 2) domain-dissimilarity of a translated\nsource image and a target image. Both constraints are implemented in the\nsimilarity preserving generative adversarial network (SPGAN) which consists of\nan Siamese network and a CycleGAN. Through domain adaptation experiment, we\nshow that images generated by SPGAN are more suitable for domain adaptation and\nyield consistent and competitive re-ID accuracy on two large-scale datasets.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 15:05:17 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 14:39:37 GMT"}, {"version": "v3", "created": "Tue, 15 May 2018 13:13:37 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Deng", "Weijian", ""], ["Zheng", "Liang", ""], ["Ye", "Qixiang", ""], ["Kang", "Guoliang", ""], ["Yang", "Yi", ""], ["Jiao", "Jianbin", ""]]}, {"id": "1711.07064", "submitter": "Dmytro Mishkin", "authors": "Orest Kupyn, Volodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, Jiri\n  Matas", "title": "DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial\n  Networks", "comments": "CVPR 2018 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeblurGAN, an end-to-end learned method for motion deblurring. The\nlearning is based on a conditional GAN and the content loss . DeblurGAN\nachieves state-of-the art performance both in the structural similarity measure\nand visual appearance. The quality of the deblurring model is also evaluated in\na novel way on a real-world problem -- object detection on (de-)blurred images.\nThe method is 5 times faster than the closest competitor -- DeepDeblur. We also\nintroduce a novel method for generating synthetic motion blurred images from\nsharp ones, allowing realistic dataset augmentation.\n  The model, code and the dataset are available at\nhttps://github.com/KupynOrest/DeblurGAN\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 19:46:18 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 14:17:01 GMT"}, {"version": "v3", "created": "Tue, 16 Jan 2018 15:51:18 GMT"}, {"version": "v4", "created": "Tue, 3 Apr 2018 10:28:19 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Kupyn", "Orest", ""], ["Budzan", "Volodymyr", ""], ["Mykhailych", "Mykola", ""], ["Mishkin", "Dmytro", ""], ["Matas", "Jiri", ""]]}, {"id": "1711.07068", "submitter": "Liwei Wang", "authors": "Liwei Wang, Alexander G. Schwing, Svetlana Lazebnik", "title": "Diverse and Accurate Image Description Using a Variational Auto-Encoder\n  with an Additive Gaussian Encoding Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores image caption generation using conditional variational\nauto-encoders (CVAEs). Standard CVAEs with a fixed Gaussian prior yield\ndescriptions with too little variability. Instead, we propose two models that\nexplicitly structure the latent space around $K$ components corresponding to\ndifferent types of image content, and combine components to create priors for\nimages that contain multiple types of content simultaneously (e.g., several\nkinds of objects). Our first model uses a Gaussian Mixture model (GMM) prior,\nwhile the second one defines a novel Additive Gaussian (AG) prior that linearly\ncombines component means. We show that both models produce captions that are\nmore diverse and more accurate than a strong LSTM baseline or a \"vanilla\" CVAE\nwith a fixed Gaussian prior, with AG-CVAE showing particular promise.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 20:12:43 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Wang", "Liwei", ""], ["Schwing", "Alexander G.", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1711.07131", "submitter": "Kuang-Huei Lee", "authors": "Kuang-Huei Lee, Xiaodong He, Lei Zhang, Linjun Yang", "title": "CleanNet: Transfer Learning for Scalable Image Classifier Training with\n  Label Noise", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning image classification models\nwith label noise. Existing approaches depending on human supervision are\ngenerally not scalable as manually identifying correct or incorrect labels is\ntime-consuming, whereas approaches not relying on human supervision are\nscalable but less effective. To reduce the amount of human supervision for\nlabel noise cleaning, we introduce CleanNet, a joint neural embedding network,\nwhich only requires a fraction of the classes being manually verified to\nprovide the knowledge of label noise that can be transferred to other classes.\nWe further integrate CleanNet and conventional convolutional neural network\nclassifier into one framework for image classification learning. We demonstrate\nthe effectiveness of the proposed algorithm on both of the label noise\ndetection task and the image classification on noisy data task on several\nlarge-scale datasets. Experimental results show that CleanNet can reduce label\nnoise detection error rate on held-out classes where no human supervision\navailable by 41.5% compared to current weakly supervised methods. It also\nachieves 47% of the performance gain of verifying all images with only 3.2%\nimages verified on an image classification task. Source code and dataset will\nbe available at kuanghuei.github.io/CleanNetProject.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 03:50:53 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 23:07:58 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Lee", "Kuang-Huei", ""], ["He", "Xiaodong", ""], ["Zhang", "Lei", ""], ["Yang", "Linjun", ""]]}, {"id": "1711.07134", "submitter": "Felix Heide", "authors": "Felix Heide, Matthew O'Toole, Kai Zang, David Lindell, Steven Diamond,\n  Gordon Wetzstein", "title": "Non-line-of-sight Imaging with Partial Occluders and Surface Normals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging objects obscured by occluders is a significant challenge for many\napplications. A camera that could \"see around corners\" could help improve\nnavigation and mapping capabilities of autonomous vehicles or make search and\nrescue missions more effective. Time-resolved single-photon imaging systems\nhave recently been demonstrated to record optical information of a scene that\ncan lead to an estimation of the shape and reflectance of objects hidden from\nthe line of sight of a camera. However, existing non-line-of-sight (NLOS)\nreconstruction algorithms have been constrained in the types of light transport\neffects they model for the hidden scene parts. We introduce a factored NLOS\nlight transport representation that accounts for partial occlusions and surface\nnormals. Based on this model, we develop a factorization approach for inverse\ntime-resolved light transport and demonstrate high-fidelity NLOS\nreconstructions for challenging scenes both in simulation and with an\nexperimental NLOS imaging system.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 04:12:30 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 06:50:44 GMT"}, {"version": "v3", "created": "Wed, 4 Apr 2018 23:17:26 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Heide", "Felix", ""], ["O'Toole", "Matthew", ""], ["Zang", "Kai", ""], ["Lindell", "David", ""], ["Diamond", "Steven", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "1711.07141", "submitter": "Alan JiaXiang Guo", "authors": "Alan J.X. Guo and Fei Zhu", "title": "Spectral-Spatial Feature Extraction and Classification by ANN Supervised\n  with Center Loss in Hyperspectral Imagery", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a spectral-spatial feature extraction and\nclassification framework based on artificial neuron network (ANN) in the\ncontext of hyperspectral imagery. With limited labeled samples, only spectral\ninformation is exploited for training and spatial context is integrated\nposteriorly at the testing stage. Taking advantage of recent advances in face\nrecognition, a joint supervision symbol that combines softmax loss and center\nloss is adopted to train the proposed network, by which intra-class features\nare gathered while inter-class variations are enlarged. Based on the learned\narchitecture, the extracted spectrum-based features are classified by a center\nclassifier. Moreover, to fuse the spectral and spatial information, an adaptive\nspectral-spatial center classifier is developed, where multiscale neighborhoods\nare considered simultaneously, and the final label is determined using an\nadaptive voting strategy. Finally, experimental results on three well-known\ndatasets validate the effectiveness of the proposed methods compared with the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 04:46:45 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Guo", "Alan J. X.", ""], ["Zhu", "Fei", ""]]}, {"id": "1711.07155", "submitter": "Guodong Ding", "authors": "Guodong Ding, Salman Khan, Zhenmin Tang, Fatih Porikli", "title": "Let Features Decide for Themselves: Feature Mask Network for Person\n  Re-identification", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.patrec.2019.02.015", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification aims at establishing the identity of a pedestrian\nfrom a gallery that contains images of multiple people obtained from a\nmulti-camera system. Many challenges such as occlusions, drastic lighting and\npose variations across the camera views, indiscriminate visual appearances,\ncluttered backgrounds, imperfect detections, motion blur, and noise make this\ntask highly challenging. While most approaches focus on learning features and\nmetrics to derive better representations, we hypothesize that both local and\nglobal contextual cues are crucial for an accurate identity matching. To this\nend, we propose a Feature Mask Network (FMN) that takes advantage of ResNet\nhigh-level features to predict a feature map mask and then imposes it on the\nlow-level features to dynamically reweight different object parts for a locally\naware feature representation. This serves as an effective attention mechanism\nby allowing the network to focus on local details selectively. Given the\nresemblance of person re-identification with classification and retrieval\ntasks, we frame the network training as a multi-task objective optimization,\nwhich further improves the learned feature descriptions. We conduct experiments\non Market-1501, DukeMTMC-reID and CUHK03 datasets, where the proposed approach\nrespectively achieves significant improvements of $5.3\\%$, $9.1\\%$ and $10.7\\%$\nin mAP measure relative to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 05:44:29 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Ding", "Guodong", ""], ["Khan", "Salman", ""], ["Tang", "Zhenmin", ""], ["Porikli", "Fatih", ""]]}, {"id": "1711.07170", "submitter": "Jiren Jin", "authors": "Jiren Jin, Richard G. Calland, Takeru Miyato, Brian K. Vogel, Hideki\n  Nakayama", "title": "Parameter Reference Loss for Unsupervised Domain Adaptation", "comments": "Add experiments that compare parameter reference loss with existing\n  methods using the same architecture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning in computer vision is mainly attributed to an\nabundance of data. However, collecting large-scale data is not always possible,\nespecially for the supervised labels. Unsupervised domain adaptation (UDA) aims\nto utilize labeled data from a source domain to learn a model that generalizes\nto a target domain of unlabeled data. A large amount of existing work uses\nSiamese network-based models, where two streams of neural networks process the\nsource and the target domain data respectively. Nevertheless, most of these\napproaches focus on minimizing the domain discrepancy, overlooking the\nimportance of preserving the discriminative ability for target domain features.\nAnother important problem in UDA research is how to evaluate the methods\nproperly. Common evaluation procedures require target domain labels for\nhyper-parameter tuning and model selection, contradicting the definition of the\nUDA task. Hence we propose a more reasonable evaluation principle that avoids\nthis contradiction by simply adopting the latest snapshot of a model for\nevaluation. This adds an extra requirement for UDA methods besides the main\nperformance criteria: the stability during training. We design a novel method\nthat connects the target domain stream to the source domain stream with a\nParameter Reference Loss (PRL) to solve these problems simultaneously.\nExperiments on various datasets show that the proposed PRL not only improves\nthe performance on the target domain, but also stabilizes the training\nprocedure. As a result, PRL based models do not need the contradictory model\nselection, and thus are more suitable for practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 06:40:43 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 10:19:34 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Jin", "Jiren", ""], ["Calland", "Richard G.", ""], ["Miyato", "Takeru", ""], ["Vogel", "Brian K.", ""], ["Nakayama", "Hideki", ""]]}, {"id": "1711.07183", "submitter": "Chenxi Liu", "authors": "Xiaohui Zeng, Chenxi Liu, Yu-Siang Wang, Weichao Qiu, Lingxi Xie,\n  Yu-Wing Tai, Chi Keung Tang, Alan L. Yuille", "title": "Adversarial Attacks Beyond the Image Space", "comments": "To appear in CVPR 2019 as oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating adversarial examples is an intriguing problem and an important way\nof understanding the working mechanism of deep neural networks. Most existing\napproaches generated perturbations in the image space, i.e., each pixel can be\nmodified independently. However, in this paper we pay special attention to the\nsubset of adversarial examples that correspond to meaningful changes in 3D\nphysical properties (like rotation and translation, illumination condition,\netc.). These adversaries arguably pose a more serious concern, as they\ndemonstrate the possibility of causing neural network failure by easy\nperturbations of real-world 3D objects and scenes.\n  In the contexts of object classification and visual question answering, we\naugment state-of-the-art deep neural networks that receive 2D input images with\na rendering module (either differentiable or not) in front, so that a 3D scene\n(in the physical space) is rendered into a 2D image (in the image space), and\nthen mapped to a prediction (in the output space). The adversarial\nperturbations can now go beyond the image space, and have clear meanings in the\n3D physical world. Though image-space adversaries can be interpreted as\nper-pixel albedo change, we verify that they cannot be well explained along\nthese physically meaningful dimensions, which often have a non-local effect.\nBut it is still possible to successfully attack beyond the image space on the\nphysical space, though this is more difficult than image-space attacks,\nreflected in lower success rates and heavier perturbations required.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 08:05:24 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 01:56:50 GMT"}, {"version": "v3", "created": "Fri, 7 Sep 2018 03:20:11 GMT"}, {"version": "v4", "created": "Mon, 10 Sep 2018 14:46:21 GMT"}, {"version": "v5", "created": "Sat, 24 Nov 2018 18:39:59 GMT"}, {"version": "v6", "created": "Sat, 6 Apr 2019 19:46:43 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zeng", "Xiaohui", ""], ["Liu", "Chenxi", ""], ["Wang", "Yu-Siang", ""], ["Qiu", "Weichao", ""], ["Xie", "Lingxi", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi Keung", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1711.07190", "submitter": "Kensuke Nakamura", "authors": "Kensuke Nakamura, Stefano Soatto, and Byung-Woo Hong", "title": "Block-Cyclic Stochastic Coordinate Descent for Deep Neural Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a stochastic first-order optimization algorithm, named BCSC, that\nadds a cyclic constraint to stochastic block-coordinate descent. It uses\ndifferent subsets of the data to update different subsets of the parameters,\nthus limiting the detrimental effect of outliers in the training set. Empirical\ntests in benchmark datasets show that our algorithm outperforms\nstate-of-the-art optimization methods in both accuracy as well as convergence\nspeed. The improvements are consistent across different architectures, and can\nbe combined with other training techniques and regularization methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 08:20:35 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Nakamura", "Kensuke", ""], ["Soatto", "Stefano", ""], ["Hong", "Byung-Woo", ""]]}, {"id": "1711.07201", "submitter": "Atique Rehman", "authors": "Atique ur Rehman, Rafia Rahim, M Shahroz Nadeem, Sibt ul Hussain", "title": "End-to-end Trained CNN Encode-Decoder Networks for Image Steganography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All the existing image steganography methods use manually crafted features to\nhide binary payloads into cover images. This leads to small payload capacity\nand image distortion. Here we propose a convolutional neural network based\nencoder-decoder architecture for embedding of images as payload. To this end,\nwe make following three major contributions: (i) we propose a deep learning\nbased generic encoder-decoder architecture for image steganography; (ii) we\nintroduce a new loss function that ensures joint end-to-end training of\nencoder-decoder networks; (iii) we perform extensive empirical evaluation of\nproposed architecture on a range of challenging publicly available datasets\n(MNIST, CIFAR10, PASCAL-VOC12, ImageNet, LFW) and report state-of-the-art\npayload capacity at high PSNR and SSIM values.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 08:49:32 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Rehman", "Atique ur", ""], ["Rahim", "Rafia", ""], ["Nadeem", "M Shahroz", ""], ["Hussain", "Sibt ul", ""]]}, {"id": "1711.07231", "submitter": "Alexis Arnaudon Mr", "authors": "Alexis Arnaudon, Darryl Holm, Stefan Sommer", "title": "Stochastic metamorphosis with template uncertainties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate two stochastic perturbations of the\nmetamorphosis equations of image analysis, in the geometrical context of the\nEuler-Poincar\\'e theory. In the metamorphosis of images, the Lie group of\ndiffeomorphisms deforms a template image that is undergoing its own internal\ndynamics as it deforms. This type of deformation allows more freedom for image\nmatching and has analogies with complex fluids when the template properties are\nregarded as order parameters (coset spaces of broken symmetries). The first\nstochastic perturbation we consider corresponds to uncertainty due to random\nerrors in the reconstruction of the deformation map from its vector field. We\nalso consider a second stochastic perturbation, which compounds the uncertainty\nin of the deformation map with the uncertainty in the reconstruction of the\ntemplate position from its velocity field. We apply this general geometric\ntheory to several classical examples, including landmarks, images, and closed\ncurves, and we discuss its use for functional data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 09:55:15 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Arnaudon", "Alexis", ""], ["Holm", "Darryl", ""], ["Sommer", "Stefan", ""]]}, {"id": "1711.07235", "submitter": "Burak Uzkent", "authors": "Burak Uzkent, Aneesh Rangnekar, and Matthew J. Hoffman", "title": "Tracking in Aerial Hyperspectral Videos using Deep Kernelized\n  Correlation Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging holds enormous potential to improve the\nstate-of-the-art in aerial vehicle tracking with low spatial and temporal\nresolutions. Recently, adaptive multi-modal hyperspectral sensors have\nattracted growing interest due to their ability to record extended data quickly\nfrom aerial platforms. In this study, we apply popular concepts from\ntraditional object tracking, namely (1) Kernelized Correlation Filters (KCF)\nand (2) Deep Convolutional Neural Network (CNN) features to aerial tracking in\nhyperspectral domain. We propose the Deep Hyperspectral Kernelized Correlation\nFilter based tracker (DeepHKCF) to efficiently track aerial vehicles using an\nadaptive multi-modal hyperspectral sensor. We address low temporal resolution\nby designing a single KCF-in-multiple Regions-of-Interest (ROIs) approach to\ncover a reasonably large area. To increase the speed of deep convolutional\nfeatures extraction from multiple ROIs, we design an effective ROI mapping\nstrategy. The proposed tracker also provides flexibility to couple with the\nmore advanced correlation filter trackers. The DeepHKCF tracker performs\nexceptionally well with deep features set up in a synthetic hyperspectral video\ngenerated by the Digital Imaging and Remote Sensing Image Generation (DIRSIG)\nsoftware. Additionally, we generate a large, synthetic, single-channel dataset\nusing DIRSIG to perform vehicle classification in the Wide Area Motion Imagery\n(WAMI) platform. This way, the high-fidelity of the DIRSIG software is proved\nand a large scale aerial vehicle classification dataset is released to support\nstudies on vehicle detection and tracking in the WAMI platform.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 10:06:07 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 10:15:22 GMT"}, {"version": "v3", "created": "Sun, 6 May 2018 06:04:27 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Uzkent", "Burak", ""], ["Rangnekar", "Aneesh", ""], ["Hoffman", "Matthew J.", ""]]}, {"id": "1711.07240", "submitter": "Chao Peng", "authors": "Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Kai Jia,\n  Gang Yu, Jian Sun", "title": "MegDet: A Large Mini-Batch Object Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The improvements in recent CNN-based object detection works, from R-CNN [11],\nFast/Faster R-CNN [10, 31] to recent Mask R-CNN [14] and RetinaNet [24], mainly\ncome from new network, new framework, or novel loss design. But mini-batch\nsize, a key factor in the training, has not been well studied. In this paper,\nwe propose a Large MiniBatch Object Detector (MegDet) to enable the training\nwith much larger mini-batch size than before (e.g. from 16 to 256), so that we\ncan effectively utilize multiple GPUs (up to 128 in our experiments) to\nsignificantly shorten the training time. Technically, we suggest a learning\nrate policy and Cross-GPU Batch Normalization, which together allow us to\nsuccessfully train a large mini-batch detector in much less time (e.g., from 33\nhours to 4 hours), and achieve even better accuracy. The MegDet is the backbone\nof our submission (mmAP 52.5%) to COCO 2017 Challenge, where we won the 1st\nplace of Detection task.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 10:16:33 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 03:11:30 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 10:48:44 GMT"}, {"version": "v4", "created": "Wed, 11 Apr 2018 05:59:52 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Peng", "Chao", ""], ["Xiao", "Tete", ""], ["Li", "Zeming", ""], ["Jiang", "Yuning", ""], ["Zhang", "Xiangyu", ""], ["Jia", "Kai", ""], ["Yu", "Gang", ""], ["Sun", "Jian", ""]]}, {"id": "1711.07245", "submitter": "Chandra Prakash Konkimalla", "authors": "Chandra Prakash Konkimalla, Manikanta Srikar Yellapragada, Trishal\n  Gayam, Souraj Mandal, Sumohana S. Channappayya", "title": "Optical Character Recognition (OCR) for Telugu: Database, Algorithm and\n  Application", "comments": "Accepted to IEEE International Conference on Image Processing 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telugu is a Dravidian language spoken by more than 80 million people\nworldwide. The optical character recognition (OCR) of the Telugu script has\nwide ranging applications including education, health-care, administration etc.\nThe beautiful Telugu script however is very different from Germanic scripts\nlike English and German. This makes the use of transfer learning of Germanic\nOCR solutions to Telugu a non-trivial task. To address the challenge of OCR for\nTelugu, we make three contributions in this work: (i) a database of Telugu\ncharacters, (ii) a deep learning based OCR algorithm, and (iii) a client server\nsolution for the online deployment of the algorithm. For the benefit of the\nTelugu people and the research community, we will make our code freely\navailable at https://gayamtrishal.github.io/OCR_Telugu.github.io/\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 10:33:29 GMT"}, {"version": "v2", "created": "Tue, 25 Dec 2018 18:33:15 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Konkimalla", "Chandra Prakash", ""], ["Yellapragada", "Manikanta Srikar", ""], ["Gayam", "Trishal", ""], ["Mandal", "Souraj", ""], ["Channappayya", "Sumohana S.", ""]]}, {"id": "1711.07246", "submitter": "Jianfeng Wang", "authors": "Jianfeng Wang, Ye Yuan, Gang Yu", "title": "Face Attention Network: An Effective Face Detector for the Occluded\n  Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of face detection has been largely improved with the\ndevelopment of convolutional neural network. However, the occlusion issue due\nto mask and sunglasses, is still a challenging problem. The improvement on the\nrecall of these occluded cases usually brings the risk of high false positives.\nIn this paper, we present a novel face detector called Face Attention Network\n(FAN), which can significantly improve the recall of the face detection problem\nin the occluded case without compromising the speed. More specifically, we\npropose a new anchor-level attention, which will highlight the features from\nthe face region. Integrated with our anchor assign strategy and data\naugmentation techniques, we obtain state-of-art results on public face\ndetection benchmarks like WiderFace and MAFA. The code will be released for\nreproduction.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 10:48:12 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 07:21:17 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Wang", "Jianfeng", ""], ["Yuan", "Ye", ""], ["Yu", "Gang", ""]]}, {"id": "1711.07264", "submitter": "Zeming Li", "authors": "Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, Jian Sun", "title": "Light-Head R-CNN: In Defense of Two-Stage Object Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first investigate why typical two-stage methods are not as\nfast as single-stage, fast detectors like YOLO and SSD. We find that Faster\nR-CNN and R-FCN perform an intensive computation after or before RoI warping.\nFaster R-CNN involves two fully connected layers for RoI recognition, while\nR-FCN produces a large score maps. Thus, the speed of these networks is slow\ndue to the heavy-head design in the architecture. Even if we significantly\nreduce the base model, the computation cost cannot be largely decreased\naccordingly.\n  We propose a new two-stage detector, Light-Head R-CNN, to address the\nshortcoming in current two-stage approaches. In our design, we make the head of\nnetwork as light as possible, by using a thin feature map and a cheap R-CNN\nsubnet (pooling and single fully-connected layer). Our ResNet-101 based\nlight-head R-CNN outperforms state-of-art object detectors on COCO while\nkeeping time efficiency. More importantly, simply replacing the backbone with a\ntiny network (e.g, Xception), our Light-Head R-CNN gets 30.7 mmAP at 102 FPS on\nCOCO, significantly outperforming the single-stage, fast detectors like YOLO\nand SSD on both speed and accuracy. Code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 11:50:19 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 02:53:09 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Li", "Zeming", ""], ["Peng", "Chao", ""], ["Yu", "Gang", ""], ["Zhang", "Xiangyu", ""], ["Deng", "Yangdong", ""], ["Sun", "Jian", ""]]}, {"id": "1711.07280", "submitter": "Peter Anderson", "authors": "Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko\n  S\\\"underhauf, Ian Reid, Stephen Gould, Anton van den Hengel", "title": "Vision-and-Language Navigation: Interpreting visually-grounded\n  navigation instructions in real environments", "comments": "CVPR 2018 Spotlight presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robot that can carry out a natural-language instruction has been a dream\nsince before the Jetsons cartoon series imagined a life of leisure mediated by\na fleet of attentive robot helpers. It is a dream that remains stubbornly\ndistant. However, recent advances in vision and language methods have made\nincredible progress in closely related areas. This is significant because a\nrobot interpreting a natural-language navigation instruction on the basis of\nwhat it sees is carrying out a vision and language process that is similar to\nVisual Question Answering. Both tasks can be interpreted as visually grounded\nsequence-to-sequence translation problems, and many of the same methods are\napplicable. To enable and encourage the application of vision and language\nmethods to the problem of interpreting visually-grounded navigation\ninstructions, we present the Matterport3D Simulator -- a large-scale\nreinforcement learning environment based on real imagery. Using this simulator,\nwhich can in future support a range of embodied vision and language tasks, we\nprovide the first benchmark dataset for visually-grounded natural language\nnavigation in real buildings -- the Room-to-Room (R2R) dataset.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 12:17:47 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 01:48:34 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2018 22:57:44 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Anderson", "Peter", ""], ["Wu", "Qi", ""], ["Teney", "Damien", ""], ["Bruce", "Jake", ""], ["Johnson", "Mark", ""], ["S\u00fcnderhauf", "Niko", ""], ["Reid", "Ian", ""], ["Gould", "Stephen", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1711.07289", "submitter": "Maurice Weiler", "authors": "Maurice Weiler, Fred A. Hamprecht, Martin Storath", "title": "Learning Steerable Filters for Rotation Equivariant CNNs", "comments": "Camera ready version, accepted for CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many machine learning tasks it is desirable that a model's prediction\ntransforms in an equivariant way under transformations of its input.\nConvolutional neural networks (CNNs) implement translational equivariance by\nconstruction; for other transformations, however, they are compelled to learn\nthe proper mapping. In this work, we develop Steerable Filter CNNs (SFCNNs)\nwhich achieve joint equivariance under translations and rotations by design.\nThe proposed architecture employs steerable filters to efficiently compute\norientation dependent responses for many orientations without suffering\ninterpolation artifacts from filter rotation. We utilize group convolutions\nwhich guarantee an equivariant mapping. In addition, we generalize He's weight\ninitialization scheme to filters which are defined as a linear combination of a\nsystem of atomic filters. Numerical experiments show a substantial enhancement\nof the sample complexity with a growing number of sampled filter orientations\nand confirm that the network generalizes learned patterns over orientations.\nThe proposed approach achieves state-of-the-art on the rotated MNIST benchmark\nand on the ISBI 2012 2D EM segmentation challenge.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 12:49:02 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 22:59:29 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2018 17:10:52 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Weiler", "Maurice", ""], ["Hamprecht", "Fred A.", ""], ["Storath", "Martin", ""]]}, {"id": "1711.07302", "submitter": "Bo Zhao", "authors": "Bo Zhao, Xinwei Sun, Yuan Yao, Yizhou Wang", "title": "Zero-shot Learning via Shared-Reconstruction-Graph Pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to recognize objects from novel unseen classes\nwithout any training data. Recently, structure-transfer based methods are\nproposed to implement ZSL by transferring structural knowledge from the\nsemantic embedding space to image feature space to classify testing images.\nHowever, we observe that such a knowledge transfer framework may suffer from\nthe problem of the geometric inconsistency between the data in the training and\ntesting spaces. We call this problem as the space shift problem. In this paper,\nwe propose a novel graph based method to alleviate this space shift problem.\nSpecifically, a Shared Reconstruction Graph (SRG) is pursued to capture the\ncommon structure of data in the two spaces. With the learned SRG, each unseen\nclass prototype (cluster center) in the image feature space can be synthesized\nby the linear combination of other class prototypes, so that testing instances\ncan be classified based on the distance to these synthesized prototypes. The\nSRG bridges the image feature space and semantic embedding space. By applying\nspectral clustering on the learned SRG, many meaningful clusters can be\ndiscovered, which interprets ZSL performance on the datasets. Our method can be\neasily extended to the generalized zero-shot learning setting. Experiments on\nthree popular datasets show that our method outperforms other methods on all\ndatasets. Even with a small number of training samples, our method can achieve\nthe state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 13:31:16 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Zhao", "Bo", ""], ["Sun", "Xinwei", ""], ["Yao", "Yuan", ""], ["Wang", "Yizhou", ""]]}, {"id": "1711.07312", "submitter": "Muktabh Mayank Srivastava", "authors": "Muktabh Mayank Srivastava, Pratyush Kumar, Lalit Pradhan, Srikrishna\n  Varadarajan", "title": "Detection of Tooth caries in Bitewing Radiographs using Deep Learning", "comments": "Accepted at NIPS 2017 workshop on Machine Learning for Health (NIPS\n  2017 ML4H)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We develop a Computer Aided Diagnosis (CAD) system, which enhances the\nperformance of dentists in detecting wide range of dental caries. The CAD\nSystem achieves this by acting as a second opinion for the dentists with way\nhigher sensitivity on the task of detecting cavities than the dentists\nthemselves. We develop annotated dataset of more than 3000 bitewing radiographs\nand utilize it for developing a system for automated diagnosis of dental\ncaries. Our system consists of a deep fully convolutional neural network (FCNN)\nconsisting 100+ layers, which is trained to mark caries on bitewing\nradiographs. We have compared the performance of our proposed system with three\ncertified dentists for marking dental caries. We exceed the average performance\nof the dentists in both recall (sensitivity) and F1-Score (agreement with\ntruth) by a very large margin. Working example of our system is shown in Figure\n1.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 14:12:32 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 16:08:27 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Srivastava", "Muktabh Mayank", ""], ["Kumar", "Pratyush", ""], ["Pradhan", "Lalit", ""], ["Varadarajan", "Srikrishna", ""]]}, {"id": "1711.07319", "submitter": "Zhicheng Wang", "authors": "Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, Jian\n  Sun", "title": "Cascaded Pyramid Network for Multi-Person Pose Estimation", "comments": "10 pages, accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topic of multi-person pose estimation has been largely improved recently,\nespecially with the development of convolutional neural network. However, there\nstill exist a lot of challenging cases, such as occluded keypoints, invisible\nkeypoints and complex background, which cannot be well addressed. In this\npaper, we present a novel network structure called Cascaded Pyramid Network\n(CPN) which targets to relieve the problem from these \"hard\" keypoints. More\nspecifically, our algorithm includes two stages: GlobalNet and RefineNet.\nGlobalNet is a feature pyramid network which can successfully localize the\n\"simple\" keypoints like eyes and hands but may fail to precisely recognize the\noccluded or invisible keypoints. Our RefineNet tries explicitly handling the\n\"hard\" keypoints by integrating all levels of feature representations from the\nGlobalNet together with an online hard keypoint mining loss. In general, to\naddress the multi-person pose estimation problem, a top-down pipeline is\nadopted to first generate a set of human bounding boxes based on a detector,\nfollowed by our CPN for keypoint localization in each human bounding box. Based\non the proposed algorithm, we achieve state-of-art results on the COCO keypoint\nbenchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1\non the COCO test-challenge dataset, which is a 19% relative improvement\ncompared with 60.5 from the COCO 2016 keypoint challenge.Code\n(https://github.com/chenyilun95/tf-cpn.git) and the detection results are\npublicly available for further research.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 14:36:31 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 12:00:58 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Chen", "Yilun", ""], ["Wang", "Zhicheng", ""], ["Peng", "Yuxiang", ""], ["Zhang", "Zhiqiang", ""], ["Yu", "Gang", ""], ["Sun", "Jian", ""]]}, {"id": "1711.07328", "submitter": "Ivan Pires", "authors": "Ivan Miguel Pires, Nuno M. Garcia, Nuno Pombo, Francisco\n  Fl\\'orez-Revuelta and Susanna Spinsante", "title": "Data Fusion on Motion and Magnetic Sensors embedded on Mobile Devices\n  for the Identification of Activities of Daily Living", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.00124,\n  arXiv:1711.00104; text overlap with arXiv:1711.00096", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several types of sensors have been available in off-the-shelf mobile devices,\nincluding motion, magnetic, vision, acoustic, and location sensors. This paper\nfocuses on the fusion of the data acquired from motion and magnetic sensors,\ni.e., accelerometer, gyroscope and magnetometer sensors, for the recognition of\nActivities of Daily Living (ADL) using pattern recognition techniques. The\nsystem developed in this study includes data acquisition, data processing, data\nfusion, and artificial intelligence methods. Artificial Neural Networks (ANN)\nare included in artificial intelligence methods, which are used in this study\nfor the recognition of ADL. The purpose of this study is the creation of a new\nmethod using ANN for the identification of ADL, comparing three types of ANN,\nin order to achieve results with a reliable accuracy. The best accuracy was\nobtained with Deep Learning, which, after the application of the L2\nregularization and normalization techniques on the sensors data, reports an\naccuracy of 89.51%.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 20:29:22 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Pires", "Ivan Miguel", ""], ["Garcia", "Nuno M.", ""], ["Pombo", "Nuno", ""], ["Fl\u00f3rez-Revuelta", "Francisco", ""], ["Spinsante", "Susanna", ""]]}, {"id": "1711.07354", "submitter": "Ziming Zhang", "authors": "Ziming Zhang and Matthew Brand", "title": "Convergent Block Coordinate Descent for Training Tikhonov Regularized\n  Deep Neural Networks", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By lifting the ReLU function into a higher dimensional space, we develop a\nsmooth multi-convex formulation for training feed-forward deep neural networks\n(DNNs). This allows us to develop a block coordinate descent (BCD) training\nalgorithm consisting of a sequence of numerically well-behaved convex\noptimizations. Using ideas from proximal point methods in convex analysis, we\nprove that this BCD algorithm will converge globally to a stationary point with\nR-linear convergence rate of order one. In experiments with the MNIST database,\nDNNs trained with this BCD algorithm consistently yielded better test-set error\nrates than identical DNN architectures trained via all the stochastic gradient\ndescent (SGD) variants in the Caffe toolbox.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 15:04:45 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Zhang", "Ziming", ""], ["Brand", "Matthew", ""]]}, {"id": "1711.07356", "submitter": "Vincent B Tjeng", "authors": "Vincent Tjeng, Kai Xiao, Russ Tedrake", "title": "Evaluating Robustness of Neural Networks with Mixed Integer Programming", "comments": "Accepted as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have demonstrated considerable success on a wide variety of\nreal-world problems. However, networks trained only to optimize for training\naccuracy can often be fooled by adversarial examples - slightly perturbed\ninputs that are misclassified with high confidence. Verification of networks\nenables us to gauge their vulnerability to such adversarial examples. We\nformulate verification of piecewise-linear neural networks as a mixed integer\nprogram. On a representative task of finding minimum adversarial distortions,\nour verifier is two to three orders of magnitude quicker than the\nstate-of-the-art. We achieve this computational speedup via tight formulations\nfor non-linearities, as well as a novel presolve algorithm that makes full use\nof all information available. The computational speedup allows us to verify\nproperties on convolutional networks with an order of magnitude more ReLUs than\nnetworks previously verified by any complete verifier. In particular, we\ndetermine for the first time the exact adversarial accuracy of an MNIST\nclassifier to perturbations with bounded $l_\\infty$ norm $\\epsilon=0.1$: for\nthis classifier, we find an adversarial example for 4.38% of samples, and a\ncertificate of robustness (to perturbations with bounded norm) for the\nremainder. Across all robust training procedures and network architectures\nconsidered, we are able to certify more samples than the state-of-the-art and\nfind more adversarial examples than a strong first-order attack.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 15:05:33 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 17:41:34 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 04:39:10 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Tjeng", "Vincent", ""], ["Xiao", "Kai", ""], ["Tedrake", "Russ", ""]]}, {"id": "1711.07368", "submitter": "Federico Pernici", "authors": "Federico Pernici, Federico Bartoli, Matteo Bruni and Alberto Del Bimbo", "title": "Memory Based Online Learning of Deep Representations from Video Streams", "comments": "arXiv admin note: text overlap with arXiv:1708.03615", "journal-ref": null, "doi": "10.1109/CVPR.2018.00247", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel online unsupervised method for face identity learning from\nvideo streams. The method exploits deep face descriptors together with a memory\nbased learning mechanism that takes advantage of the temporal coherence of\nvisual data. Specifically, we introduce a discriminative feature matching\nsolution based on Reverse Nearest Neighbour and a feature forgetting strategy\nthat detect redundant features and discard them appropriately while time\nprogresses. It is shown that the proposed learning procedure is asymptotically\nstable and can be effectively used in relevant applications like multiple face\nidentification and tracking from unconstrained video streams. Experimental\nresults show that the proposed method achieves comparable results in the task\nof multiple face tracking and better performance in face identification with\noffline approaches exploiting future information. Code will be publicly\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 14:51:58 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Pernici", "Federico", ""], ["Bartoli", "Federico", ""], ["Bruni", "Matteo", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1711.07373", "submitter": "Dong Huk Park", "authors": "Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt\n  Schiele, Trevor Darrell, Marcus Rohrbach", "title": "Attentive Explanations: Justifying Decisions and Pointing to the\n  Evidence (Extended Abstract)", "comments": "arXiv admin note: text overlap with arXiv:1612.04757", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep models are the defacto standard in visual decision problems due to their\nimpressive performance on a wide array of visual tasks. On the other hand,\ntheir opaqueness has led to a surge of interest in explainable systems. In this\nwork, we emphasize the importance of model explanation in various forms such as\nvisual pointing and textual justification. The lack of data with justification\nannotations is one of the bottlenecks of generating multimodal explanations.\nThus, we propose two large-scale datasets with annotations that visually and\ntextually justify a classification decision for various activities, i.e. ACT-X,\nand for question answering, i.e. VQA-X. We also introduce a multimodal\nmethodology for generating visual and textual explanations simultaneously. We\nquantitatively show that training with the textual explanations not only yields\nbetter textual justification models, but also models that better localize the\nevidence that support their decision.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 18:36:14 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Park", "Dong Huk", ""], ["Hendricks", "Lisa Anne", ""], ["Akata", "Zeynep", ""], ["Rohrbach", "Anna", ""], ["Schiele", "Bernt", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "1711.07377", "submitter": "Yilin Song", "authors": "Yilin Song, Chenge Li, Yao Wang", "title": "Pixel-wise object tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel pixel-wise visual object tracking framework\nthat can track any anonymous object in a noisy background. The framework\nconsists of two submodels, a global attention model and a local segmentation\nmodel. The global model generates a region of interests (ROI) that the object\nmay lie in the new frame based on the past object segmentation maps, while the\nlocal model segments the new image in the ROI. Each model uses a LSTM structure\nto model the temporal dynamics of the motion and appearance, respectively. To\ncircumvent the dependency of the training data between the two models, we use\nan iterative update strategy. Once the models are trained, there is no need to\nrefine them to track specific objects, making our method efficient compared to\nonline learning approaches. We demonstrate our real time pixel-wise object\ntracking framework on a challenging VOT dataset\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 15:30:41 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 03:19:52 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Song", "Yilin", ""], ["Li", "Chenge", ""], ["Wang", "Yao", ""]]}, {"id": "1711.07399", "submitter": "Gyeongsik Moon", "authors": "Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee", "title": "V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and\n  Human Pose Estimation from a Single Depth Map", "comments": "HANDS 2017 Challenge Frame-based 3D Hand Pose Estimation Winner (ICCV\n  2017), Published at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing deep learning-based methods for 3D hand and human pose\nestimation from a single depth map are based on a common framework that takes a\n2D depth map and directly regresses the 3D coordinates of keypoints, such as\nhand or human body joints, via 2D convolutional neural networks (CNNs). The\nfirst weakness of this approach is the presence of perspective distortion in\nthe 2D depth map. While the depth map is intrinsically 3D data, many previous\nmethods treat depth maps as 2D images that can distort the shape of the actual\nobject through projection from 3D to 2D space. This compels the network to\nperform perspective distortion-invariant estimation. The second weakness of the\nconventional approach is that directly regressing 3D coordinates from a 2D\nimage is a highly non-linear mapping, which causes difficulty in the learning\nprocedure. To overcome these weaknesses, we firstly cast the 3D hand and human\npose estimation problem from a single depth map into a voxel-to-voxel\nprediction that uses a 3D voxelized grid and estimates the per-voxel likelihood\nfor each keypoint. We design our model as a 3D CNN that provides accurate\nestimates while running in real-time. Our system outperforms previous methods\nin almost all publicly available 3D hand and human pose estimation datasets and\nplaced first in the HANDS 2017 frame-based 3D hand pose estimation challenge.\nThe code is available in https://github.com/mks0601/V2V-PoseNet_RELEASE.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 16:41:13 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 07:55:00 GMT"}, {"version": "v3", "created": "Thu, 16 Aug 2018 14:55:40 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Moon", "Gyeongsik", ""], ["Chang", "Ju Yong", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1711.07410", "submitter": "Attila Szab\\'o", "authors": "Qiyang Hu, Attila Szab\\'o, Tiziano Portenier, Matthias Zwicker, Paolo\n  Favaro", "title": "Disentangling Factors of Variation by Mixing Them", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to learn image representations that consist of\ndisentangled factors of variation without exploiting any manual labeling or\ndata domain knowledge. A factor of variation corresponds to an image attribute\nthat can be discerned consistently across a set of images, such as the pose or\ncolor of objects. Our disentangled representation consists of a concatenation\nof feature chunks, each chunk representing a factor of variation. It supports\napplications such as transferring attributes from one image to another, by\nsimply mixing and unmixing feature chunks, and classification or retrieval\nbased on one or several attributes, by considering a user-specified subset of\nfeature chunks. We learn our representation without any labeling or knowledge\nof the data domain, using an autoencoder architecture with two novel training\nobjectives: first, we propose an invariance objective to encourage that\nencoding of each attribute, and decoding of each chunk, are invariant to\nchanges in other attributes and chunks, respectively; second, we include a\nclassification objective, which ensures that each chunk corresponds to a\nconsistently discernible attribute in the represented image, hence avoiding\ndegenerate feature mappings where some chunks are completely ignored. We\ndemonstrate the effectiveness of our approach on the MNIST, Sprites, and CelebA\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 17:00:08 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 10:27:00 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Hu", "Qiyang", ""], ["Szab\u00f3", "Attila", ""], ["Portenier", "Tiziano", ""], ["Zwicker", "Matthias", ""], ["Favaro", "Paolo", ""]]}, {"id": "1711.07419", "submitter": "Mario Amrehn", "authors": "Mario Amrehn, Stefan Steidl, Markus Kowarschik, Andreas Maier", "title": "Robust Seed Mask Generation for Interactive Image Segmentation", "comments": "Medical Imaging Conference (MIC) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In interactive medical image segmentation, anatomical structures are\nextracted from reconstructed volumetric images. The first iterations of user\ninteraction traditionally consist of drawing pictorial hints as an initial\nestimate of the object to extract. Only after this time consuming first phase,\nthe efficient selective refinement of current segmentation results begins.\nErroneously labeled seeds, especially near the border of the object, are\nchallenging to detect and replace for a human and may substantially impact the\noverall segmentation quality. We propose an automatic seeding pipeline as well\nas a configuration based on saliency recognition, in order to skip the\ntime-consuming initial interaction phase during segmentation. A median Dice\nscore of 68.22% is reached before the first user interaction on the test data\nset with an error rate in seeding of only 0.088%.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 17:19:24 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Amrehn", "Mario", ""], ["Steidl", "Stefan", ""], ["Kowarschik", "Markus", ""], ["Maier", "Andreas", ""]]}, {"id": "1711.07426", "submitter": "Siddharth Mahendran", "authors": "Siddharth Mahendran, Haider Ali and Rene Vidal", "title": "Convolutional Networks for Object Category and 3D Pose Estimation from\n  2D Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current CNN-based algorithms for recovering the 3D pose of an object in an\nimage assume knowledge about both the object category and its 2D localization\nin the image. In this paper, we relax one of these constraints and propose to\nsolve the task of joint object category and 3D pose estimation from an image\nassuming known 2D localization. We design a new architecture for this task\ncomposed of a feature network that is shared between subtasks, an object\ncategorization network built on top of the feature network, and a collection of\ncategory dependent pose regression networks. We also introduce suitable loss\nfunctions and a training method for the new architecture. Experiments on the\nchallenging PASCAL3D+ dataset show state-of-the-art performance in the joint\ncategorization and pose estimation task. Moreover, our performance on the joint\ntask is comparable to the performance of state-of-the-art methods on the\nsimpler 3D pose estimation with known object category task.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 17:31:27 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 19:15:52 GMT"}, {"version": "v3", "created": "Fri, 20 Jul 2018 19:21:36 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Mahendran", "Siddharth", ""], ["Ali", "Haider", ""], ["Vidal", "Rene", ""]]}, {"id": "1711.07430", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Yang Mi, Jianxin Wu, Ke Lu, Hongkai Xiong", "title": "Action Recognition with Coarse-to-Fine Deep Feature Integration and\n  Asynchronous Fusion", "comments": "accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition is an important yet challenging task in computer vision.\nIn this paper, we propose a novel deep-based framework for action recognition,\nwhich improves the recognition accuracy by: 1) deriving more precise features\nfor representing actions, and 2) reducing the asynchrony between different\ninformation streams. We first introduce a coarse-to-fine network which extracts\nshared deep features at different action class granularities and progressively\nintegrates them to obtain a more accurate feature representation for input\nactions. We further introduce an asynchronous fusion network. It fuses\ninformation from different streams by asynchronously integrating stream-wise\nfeatures at different time points, hence better leveraging the complementary\ninformation in different streams. Experimental results on action recognition\nbenchmarks demonstrate that our approach achieves the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 17:35:46 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Lin", "Weiyao", ""], ["Mi", "Yang", ""], ["Wu", "Jianxin", ""], ["Lu", "Ke", ""], ["Xiong", "Hongkai", ""]]}, {"id": "1711.07476", "submitter": "Saki Shinoda", "authors": "Saki Shinoda, Daniel E. Worrall, Gabriel J. Brostow", "title": "Virtual Adversarial Ladder Networks For Semi-supervised Learning", "comments": "Camera-ready version for NIPS 2017 workshop Learning with Limited\n  Labeled Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning (SSL) partially circumvents the high cost of\nlabeling data by augmenting a small labeled dataset with a large and relatively\ncheap unlabeled dataset drawn from the same distribution. This paper offers a\nnovel interpretation of two deep learning-based SSL approaches, ladder networks\nand virtual adversarial training (VAT), as applying distributional smoothing to\ntheir respective latent spaces. We propose a class of models that fuse these\napproaches. We achieve near-supervised accuracy with high consistency on the\nMNIST dataset using just 5 labels per class: our best model, ladder with\nlayer-wise virtual adversarial noise (LVAN-LW), achieves 1.42% +/- 0.12 average\nerror rate on the MNIST test set, in comparison with 1.62% +/- 0.65 reported\nfor the ladder network. On adversarial examples generated with L2-normalized\nfast gradient method, LVAN-LW trained with 5 examples per class achieves\naverage error rate 2.4% +/- 0.3 compared to 68.6% +/- 6.5 for the ladder\nnetwork and 9.9% +/- 7.5 for VAT.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 11:10:40 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 11:23:01 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Shinoda", "Saki", ""], ["Worrall", "Daniel E.", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1711.07513", "submitter": "Christopher Tralie", "authors": "Christopher J. Tralie", "title": "Self-Similarity Based Time Warping", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore the problem of aligning two time-ordered point\nclouds which are spatially transformed and re-parameterized versions of each\nother. This has a diverse array of applications such as cross modal time series\nsynchronization (e.g. MOCAP to video) and alignment of discretized curves in\nimages. Most other works that address this problem attempt to jointly uncover a\nspatial alignment and correspondences between the two point clouds, or to\nderive local invariants to spatial transformations such as curvature before\ncomputing correspondences. By contrast, we sidestep spatial alignment\ncompletely by using self-similarity matrices (SSMs) as a proxy to the\ntime-ordered point clouds, since self-similarity matrices are blind to\nisometries and respect global geometry. Our algorithm, dubbed \"Isometry Blind\nDynamic Time Warping\" (IBDTW), is simple and general, and we show that its\nassociated dissimilarity measure lower bounds the L1 Gromov-Hausdorff distance\nbetween the two point sets when restricted to warping paths. We also present a\nlocal, partial alignment extension of IBDTW based on the Smith Waterman\nalgorithm. This eliminates the need for tedious manual cropping of time series,\nwhich is ordinarily necessary for global alignment algorithms to function\nproperly.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 19:43:28 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Tralie", "Christopher J.", ""]]}, {"id": "1711.07520", "submitter": "Hao Dong", "authors": "Hao Dong, Chao Wu, Zhen Wei, Yike Guo", "title": "Dropping Activation Outputs with Localized First-layer Deep Network for\n  Enhancing User Privacy and Data Security", "comments": "IEEE Trans. Information Forensics and Security (TIFS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods can play a crucial role in anomaly detection,\nprediction, and supporting decision making for applications like personal\nhealth-care, pervasive body sensing, etc. However, current architecture of deep\nnetworks suffers the privacy issue that users need to give out their data to\nthe model (typically hosted in a server or a cluster on Cloud) for training or\nprediction. This problem is getting more severe for those sensitive health-care\nor medical data (e.g fMRI or body sensors measures like EEG signals). In\naddition to this, there is also a security risk of leaking these data during\nthe data transmission from user to the model (especially when it's through\nInternet). Targeting at these issues, in this paper we proposed a new\narchitecture for deep network in which users don't reveal their original data\nto the model. In our method, feed-forward propagation and data encryption are\ncombined into one process: we migrate the first layer of deep network to users'\nlocal devices, and apply the activation functions locally, and then use\n\"dropping activation output\" method to make the output non-invertible. The\nresulting approach is able to make model prediction without accessing users'\nsensitive raw data. Experiment conducted in this paper showed that our approach\nachieves the desirable privacy protection requirement, and demonstrated several\nadvantages over the traditional approach with encryption / decryption\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 19:57:57 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Dong", "Hao", ""], ["Wu", "Chao", ""], ["Wei", "Zhen", ""], ["Guo", "Yike", ""]]}, {"id": "1711.07566", "submitter": "Hiroharu Kato", "authors": "Hiroharu Kato, Yoshitaka Ushiku, Tatsuya Harada", "title": "Neural 3D Mesh Renderer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For modeling the 3D world behind 2D images, which 3D representation is most\nappropriate? A polygon mesh is a promising candidate for its compactness and\ngeometric properties. However, it is not straightforward to model a polygon\nmesh from 2D images using neural networks because the conversion from a mesh to\nan image, or rendering, involves a discrete operation called rasterization,\nwhich prevents back-propagation. Therefore, in this work, we propose an\napproximate gradient for rasterization that enables the integration of\nrendering into neural networks. Using this renderer, we perform single-image 3D\nmesh reconstruction with silhouette image supervision and our system\noutperforms the existing voxel-based approach. Additionally, we perform\ngradient-based 3D mesh editing operations, such as 2D-to-3D style transfer and\n3D DeepDream, with 2D supervision for the first time. These applications\ndemonstrate the potential of the integration of a mesh renderer into neural\nnetworks and the effectiveness of our proposed renderer.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 22:12:23 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Kato", "Hiroharu", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1711.07568", "submitter": "Iuri Frosio", "authors": "Iuri Frosio and Jan Kautz", "title": "On Nearest Neighbors in Non Local Means Denoising", "comments": "This paper is accepted at the 2017 NIPS workshop \"Nearest Neighbors\n  for Modern Applications with Massive Data\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To denoise a reference patch, the Non-Local-Means denoising filter processes\na set of neighbor patches. Few Nearest Neighbors (NN) are used to limit the\ncomputational burden of the algorithm. Here here we show analytically that the\nNN approach introduces a bias in the denoised patch, and we propose a different\nneighbors' collection criterion, named Statistical NN (SNN), to alleviate this\nissue. Our approach outperforms the traditional one in case of both white and\ncolored noise: fewer SNNs generate images of higher quality, at a lower\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 22:31:27 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Frosio", "Iuri", ""], ["Kautz", "Jan", ""]]}, {"id": "1711.07607", "submitter": "Jiyang Gao", "authors": "Jiyang Gao, Zijian (James) Guo, Zhen Li, Ram Nevatia", "title": "Knowledge Concentration: Learning 100K Object Classifiers in a Single\n  CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image labels are desirable for many computer vision\napplications, such as visual search or mobile AI assistant. These applications\nrely on image classification models that can produce hundreds of thousands\n(e.g. 100K) of diversified fine-grained image labels on input images. However,\ntraining a network at this vocabulary scale is challenging, and suffers from\nintolerable large model size and slow training speed, which leads to\nunsatisfying classification performance. A straightforward solution would be\ntraining separate expert networks (specialists), with each specialist focusing\non learning one specific vertical (e.g. cars, birds...). However, deploying\ndozens of expert networks in a practical system would significantly increase\nsystem complexity and inference latency, and consumes large amounts of\ncomputational resources. To address these challenges, we propose a Knowledge\nConcentration method, which effectively transfers the knowledge from dozens of\nspecialists (multiple teacher networks) into one single model (one student\nnetwork) to classify 100K object categories. There are three salient aspects in\nour method: (1) a multi-teacher single-student knowledge distillation\nframework; (2) a self-paced learning mechanism to allow the student to learn\nfrom different teachers at various paces; (3) structurally connected layers to\nexpand the student network capacity with limited extra parameters. We validate\nour method on OpenImage and a newly collected dataset, Entity-Foto-Tree (EFT),\nwith 100K categories, and show that the proposed model performs significantly\nbetter than the baseline generalist model.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 02:38:34 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 05:30:30 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Gao", "Jiyang", "", "James"], ["Zijian", "", "", "James"], ["Guo", "", ""], ["Li", "Zhen", ""], ["Nevatia", "Ram", ""]]}, {"id": "1711.07613", "submitter": "Qi Wu", "authors": "Qi Wu, Peng Wang, Chunhua Shen, Ian Reid, and Anton van den Hengel", "title": "Are You Talking to Me? Reasoned Visual Dialog Generation through\n  Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Visual Dialogue task requires an agent to engage in a conversation about\nan image with a human. It represents an extension of the Visual Question\nAnswering task in that the agent needs to answer a question about an image, but\nit needs to do so in light of the previous dialogue that has taken place. The\nkey challenge in Visual Dialogue is thus maintaining a consistent, and natural\ndialogue while continuing to answer questions correctly. We present a novel\napproach that combines Reinforcement Learning and Generative Adversarial\nNetworks (GANs) to generate more human-like responses to questions. The GAN\nhelps overcome the relative paucity of training data, and the tendency of the\ntypical MLE-based approach to generate overly terse answers. Critically, the\nGAN is tightly integrated into the attention mechanism that generates\nhuman-interpretable reasons for each answer. This means that the discriminative\nmodel of the GAN has the task of assessing whether a candidate answer is\ngenerated by a human or not, given the provided reason. This is significant\nbecause it drives the generative model to produce high quality answers that are\nwell supported by the associated reasoning. The method also generates the\nstate-of-the-art results on the primary benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 03:11:49 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Wu", "Qi", ""], ["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1711.07614", "submitter": "Qi Wu", "authors": "Junjie Zhang, Qi Wu, Chunhua Shen, Jian Zhang, Jianfeng Lu, and Anton\n  van den Hengel", "title": "Asking the Difficult Questions: Goal-Oriented Visual Question Generation\n  via Intermediate Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress in a variety of vision-and-language problems,\ndeveloping a method capable of asking intelligent, goal-oriented questions\nabout images is proven to be an inscrutable challenge. Towards this end, we\npropose a Deep Reinforcement Learning framework based on three new intermediate\nrewards, namely goal-achieved, progressive and informativeness that encourage\nthe generation of succinct questions, which in turn uncover valuable\ninformation towards the overall goal. By directly optimizing for questions that\nwork quickly towards fulfilling the overall goal, we avoid the tendency of\nexisting methods to generate long series of insane queries that add little\nvalue. We evaluate our model on the GuessWhat?! dataset and show that the\nresulting questions can help a standard Guesser identify a specific object in\nan image at a much higher success rate.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 03:15:30 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Zhang", "Junjie", ""], ["Wu", "Qi", ""], ["Shen", "Chunhua", ""], ["Zhang", "Jian", ""], ["Lu", "Jianfeng", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1711.07618", "submitter": "Ming-Ming Cheng Prof.", "authors": "Ruochen Fan, Ming-Ming Cheng, Qibin Hou, Tai-Jiang Mu, Jingdong Wang,\n  Shi-Min Hu", "title": "S4Net: Single Stage Salient-Instance Segmentation", "comments": null, "journal-ref": "IEEE CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an interesting problem-salient instance segmentation in this\npaper. Other than producing bounding boxes, our network also outputs\nhigh-quality instance-level segments. Taking into account the\ncategory-independent property of each target, we design a single stage salient\ninstance segmentation framework, with a novel segmentation branch. Our new\nbranch regards not only local context inside each detection window but also its\nsurrounding context, enabling us to distinguish the instances in the same scope\neven with obstruction. Our network is end-to-end trainable and runs at a fast\nspeed (40 fps when processing an image with resolution 320x320). We evaluate\nour approach on a publicly available benchmark and show that it outperforms\nother alternative solutions. We also provide a thorough analysis of the design\nchoices to help readers better understand the functions of each part of our\nnetwork. The source code can be found at\n\\url{https://github.com/RuochenFan/S4Net}.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 03:47:36 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 10:40:28 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Fan", "Ruochen", ""], ["Cheng", "Ming-Ming", ""], ["Hou", "Qibin", ""], ["Mu", "Tai-Jiang", ""], ["Wang", "Jingdong", ""], ["Hu", "Shi-Min", ""]]}, {"id": "1711.07624", "submitter": "Jiajia Guo", "authors": "Jiajia Guo, Hongwei Du, Bensheng Qiu and Xiao Liang", "title": "A deep learning-based method for relative location prediction in CT scan\n  images", "comments": "Accepted poster at NIPS 2017 Workshop on Machine Learning for Health\n  (https://ml4health.github.io/2017/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relative location prediction in computed tomography (CT) scan images is a\nchallenging problem. In this paper, a regression model based on one-dimensional\nconvolutional neural networks is proposed to determine the relative location of\na CT scan image both robustly and precisely. A public dataset is employed to\nvalidate the performance of the study's proposed method using a 5-fold cross\nvalidation. Experimental results demonstrate an excellent performance of the\nproposed model when compared with the state-of-the-art techniques, achieving a\nmedian absolute error of 1.04 cm and mean absolute error of 1.69 cm.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 04:11:37 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Guo", "Jiajia", ""], ["Du", "Hongwei", ""], ["Qiu", "Bensheng", ""], ["Liang", "Xiao", ""]]}, {"id": "1711.07641", "submitter": "Qianqian Wang", "authors": "Qianqian Wang, Xiaowei Zhou, Kostas Daniilidis", "title": "Multi-Image Semantic Matching by Mining Consistent Features", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a multi-image matching method to estimate semantic\ncorrespondences across multiple images. In contrast to the previous methods\nthat optimize all pairwise correspondences, the proposed method identifies and\nmatches only a sparse set of reliable features in the image collection. In this\nway, the proposed method is able to prune nonrepeatable features and also\nhighly scalable to handle thousands of images. We additionally propose a\nlow-rank constraint to ensure the geometric consistency of feature\ncorrespondences over the whole image collection. Besides the competitive\nperformance on multi-graph matching and semantic flow benchmarks, we also\ndemonstrate the applicability of the proposed method for reconstructing\nobject-class models and discovering object-class landmarks from images without\nusing any annotation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 06:17:39 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 08:06:25 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Wang", "Qianqian", ""], ["Zhou", "Xiaowei", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1711.07653", "submitter": "Risheng Liu", "authors": "Risheng Liu, Xin Fan, Shichao Cheng, Xiangyu Wang, Zhongxuan Luo", "title": "Proximal Alternating Direction Network: A Globally Converged Deep\n  Unrolling Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have gained great success in many real-world\napplications. However, most existing networks are typically designed in\nheuristic manners, thus lack of rigorous mathematical principles and\nderivations. Several recent studies build deep structures by unrolling a\nparticular optimization model that involves task information. Unfortunately,\ndue to the dynamic nature of network parameters, their resultant deep\npropagation networks do \\emph{not} possess the nice convergence property as the\noriginal optimization scheme does. This paper provides a novel proximal\nunrolling framework to establish deep models by integrating experimentally\nverified network architectures and rich cues of the tasks. More importantly, we\n\\emph{prove in theory} that 1) the propagation generated by our unrolled deep\nmodel globally converges to a critical-point of a given variational energy, and\n2) the proposed framework is still able to learn priors from training data to\ngenerate a convergent propagation even when task information is only partially\navailable. Indeed, these theoretical results are the best we can ask for,\nunless stronger assumptions are enforced. Extensive experiments on various\nreal-world applications verify the theoretical convergence and demonstrate the\neffectiveness of designed deep models.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 07:21:53 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 11:32:21 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Liu", "Risheng", ""], ["Fan", "Xin", ""], ["Cheng", "Shichao", ""], ["Wang", "Xiangyu", ""], ["Luo", "Zhongxuan", ""]]}, {"id": "1711.07695", "submitter": "Christoph Wick", "authors": "Christoph Wick, Frank Puppe", "title": "Fully Convolutional Neural Networks for Page Segmentation of Historical\n  Document Images", "comments": "6 pages, 7 figures, conference", "journal-ref": null, "doi": "10.1109/DAS.2018.39", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a high-performance fully convolutional neural network (FCN) for\nhistorical document segmentation that is designed to process a single page in\none step. The advantage of this model beside its speed is its ability to\ndirectly learn from raw pixels instead of using preprocessing steps e. g.\nfeature computation or superpixel generation. We show that this network yields\nbetter results than existing methods on different public data sets. For\nevaluation of this model we introduce a novel metric that is independent of\nambiguous ground truth called Foreground Pixel Accuracy (FgPA). This pixel\nbased measure only counts foreground pixels in the binarized page, any\nbackground pixel is omitted. The major advantage of this metric is, that it\nenables researchers to compare different segmentation methods on their ability\nto successfully segment text or pictures and not on their ability to learn and\npossibly overfit the peculiarities of an ambiguous hand-made ground truth\nsegmentation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 10:02:34 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 12:52:33 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Wick", "Christoph", ""], ["Puppe", "Frank", ""]]}, {"id": "1711.07714", "submitter": "Artem Rozantsev Dr.", "authors": "Artem Rozantsev, Mathieu Salzmann, Pascal Fua", "title": "Residual Parameter Transfer for Deep Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of Deep Domain Adaptation is to make it possible to use Deep Nets\ntrained in one domain where there is enough annotated training data in another\nwhere there is little or none. Most current approaches have focused on learning\nfeature representations that are invariant to the changes that occur when going\nfrom one domain to the other, which means using the same network parameters in\nboth domains. While some recent algorithms explicitly model the changes by\nadapting the network parameters, they either severely restrict the possible\ndomain changes, or significantly increase the number of model parameters.\n  By contrast, we introduce a network architecture that includes auxiliary\nresidual networks, which we train to predict the parameters in the domain with\nlittle annotated data from those in the other one. This architecture enables us\nto flexibly preserve the similarities between domains where they exist and\nmodel the differences when necessary. We demonstrate that our approach yields\nhigher accuracy than state-of-the-art methods without undue complexity.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 11:03:55 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Rozantsev", "Artem", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1711.07719", "submitter": "Hossein Javidnia", "authors": "Hossein Javidnia, Peter Corcoran", "title": "Total Variation-Based Dense Depth from Multi-Camera Array", "comments": "21 pages, 13 figures", "journal-ref": "Optical Engineering, 57(6), 063105 (2018)", "doi": "10.1117/1.OE.57.6.063105", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Camera arrays are increasingly employed in both consumer and industrial\napplications, and various passive techniques are documented to estimate depth\nfrom such camera arrays. Current depth estimation methods provide useful\nestimations of depth in an imaged scene but are often impractical due to\nsignificant computational requirements. This paper presents a novel framework\nthat generates a high-quality continuous depth map from multi-camera\narray/light field cameras. The proposed framework utilizes analysis of the\nlocal Epipolar Plane Image (EPI) to initiate the depth estimation process. The\nestimated depth map is then processed using Total Variation (TV) minimization\nbased on the Fenchel-Rockafellar duality. Evaluation of this method based on a\nwell-known benchmark indicates that the proposed framework performs well in\nterms of accuracy when compared to the top-ranked depth estimation methods and\na baseline algorithm. The test dataset includes both photorealistic and\nnon-photorealistic scenes. Notably, the computational requirements required to\nachieve an equivalent accuracy are significantly reduced when compared to the\ntop algorithms. As a consequence, the proposed framework is suitable for\ndeployment in consumer and industrial applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 11:16:21 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Javidnia", "Hossein", ""], ["Corcoran", "Peter", ""]]}, {"id": "1711.07721", "submitter": "Hossein Javidnia", "authors": "Hossein Javidnia, Peter Corcoran", "title": "The Application of Preconditioned Alternating Direction Method of\n  Multipliers in Depth from Focal Stack", "comments": "15 pages, 8 figures", "journal-ref": "J. of Electronic Imaging, 27(2), 023019 (2018)", "doi": "10.1117/1.JEI.27.2.023019", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post capture refocusing effect in smartphone cameras is achievable by using\nfocal stacks. However, the accuracy of this effect is totally dependent on the\ncombination of the depth layers in the stack. The accuracy of the extended\ndepth of field effect in this application can be improved significantly by\ncomputing an accurate depth map which has been an open issue for decades. To\ntackle this issue, in this paper, a framework is proposed based on\nPreconditioned Alternating Direction Method of Multipliers (PADMM) for depth\nfrom the focal stack and synthetic defocus application. In addition to its\nability to provide high structural accuracy and occlusion handling, the\noptimization function of the proposed method can, in fact, converge faster and\nbetter than state of the art methods. The evaluation has been done on 21 sets\nof focal stacks and the optimization function has been compared against 5 other\nmethods. Preliminary results indicate that the proposed method has a better\nperformance in terms of structural accuracy and optimization in comparison to\nthe current state of the art methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 11:22:55 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Javidnia", "Hossein", ""], ["Corcoran", "Peter", ""]]}, {"id": "1711.07752", "submitter": "Tete Xiao", "authors": "Xinlong Wang, Tete Xiao, Yuning Jiang, Shuai Shao, Jian Sun, Chunhua\n  Shen", "title": "Repulsion Loss: Detecting Pedestrians in a Crowd", "comments": "Accepted to IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting individual pedestrians in a crowd remains a challenging problem\nsince the pedestrians often gather together and occlude each other in\nreal-world scenarios. In this paper, we first explore how a state-of-the-art\npedestrian detector is harmed by crowd occlusion via experimentation, providing\ninsights into the crowd occlusion problem. Then, we propose a novel bounding\nbox regression loss specifically designed for crowd scenes, termed repulsion\nloss. This loss is driven by two motivations: the attraction by target, and the\nrepulsion by other surrounding objects. The repulsion term prevents the\nproposal from shifting to surrounding objects thus leading to more crowd-robust\nlocalization. Our detector trained by repulsion loss outperforms all the\nstate-of-the-art methods with a significant improvement in occlusion cases.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 12:38:41 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 06:31:48 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Wang", "Xinlong", ""], ["Xiao", "Tete", ""], ["Jiang", "Yuning", ""], ["Shao", "Shuai", ""], ["Sun", "Jian", ""], ["Shen", "Chunhua", ""]]}, {"id": "1711.07767", "submitter": "Songtao Liu", "authors": "Songtao Liu, Di Huang, Yunhong Wang", "title": "Receptive Field Block Net for Accurate and Fast Object Detection", "comments": "Accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current top-performing object detectors depend on deep CNN backbones, such as\nResNet-101 and Inception, benefiting from their powerful feature\nrepresentations but suffering from high computational costs. Conversely, some\nlightweight model based detectors fulfil real time processing, while their\naccuracies are often criticized. In this paper, we explore an alternative to\nbuild a fast and accurate detector by strengthening lightweight features using\na hand-crafted mechanism. Inspired by the structure of Receptive Fields (RFs)\nin human visual systems, we propose a novel RF Block (RFB) module, which takes\nthe relationship between the size and eccentricity of RFs into account, to\nenhance the feature discriminability and robustness. We further assemble RFB to\nthe top of SSD, constructing the RFB Net detector. To evaluate its\neffectiveness, experiments are conducted on two major benchmarks and the\nresults show that RFB Net is able to reach the performance of advanced very\ndeep detectors while keeping the real-time speed. Code is available at\nhttps://github.com/ruinmessi/RFBNet.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 13:18:26 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 05:37:57 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 08:37:02 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Liu", "Songtao", ""], ["Huang", "Di", ""], ["Wang", "Yunhong", ""]]}, {"id": "1711.07794", "submitter": "Julian Yarkony", "authors": "Shaofei Wang, Konrad Paul Kording, Julian Yarkony", "title": "Efficient Multi-Person Pose Estimation with Provable Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person pose estimation (MPPE) in natural images is key to the\nmeaningful use of visual data in many fields including movement science,\nsecurity, and rehabilitation. In this paper we tackle MPPE with a bottom-up\napproach, starting with candidate detections of body parts from a convolutional\nneural network (CNN) and grouping them into people. We formulate the grouping\nof body part detections into people as a minimum-weight set packing (MWSP)\nproblem where the set of potential people is the power set of body part\ndetections. We model the quality of a hypothesis of a person which is a set in\nthe MWSP by an augmented tree-structured Markov random field where variables\ncorrespond to body-parts and their state-spaces correspond to the power set of\nthe detections for that part.\n  We describe a novel algorithm that combines efficiency with provable bounds\non this MWSP problem. We employ an implicit column generation strategy where\nthe pricing problem is formulated as a dynamic program. To efficiently solve\nthis dynamic program we exploit the problem structure utilizing a nested\nBender's decomposition (NBD) exact inference strategy which we speed up by\nrecycling Bender's rows between calls to the pricing problem.\n  We test our approach on the MPII-Multiperson dataset, showing that our\napproach obtains comparable results with the state-of-the-art algorithm for\njoint node labeling and grouping problems, and that NBD achieves considerable\nspeed-ups relative to a naive dynamic programming approach. Typical algorithms\nthat solve joint node labeling and grouping problems use heuristics and thus\ncan not obtain proofs of optimality. Our approach, in contrast, proves that for\nover 99 percent of problem instances we find the globally optimal solution and\notherwise provide upper/lower bounds.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 14:13:41 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Wang", "Shaofei", ""], ["Kording", "Konrad Paul", ""], ["Yarkony", "Julian", ""]]}, {"id": "1711.07798", "submitter": "Qingjie Liu", "authors": "Xingyue Chen, Yunhong Wang, Qingjie Liu", "title": "Visual and Textual Sentiment Analysis Using Deep Fusion Convolutional\n  Neural Networks", "comments": "Accepted as oral presentation by ICIP2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is attracting more and more attentions and has become a\nvery hot research topic due to its potential applications in personalized\nrecommendation, opinion mining, etc. Most of the existing methods are based on\neither textual or visual data and can not achieve satisfactory results, as it\nis very hard to extract sufficient information from only one single modality\ndata. Inspired by the observation that there exists strong semantic correlation\nbetween visual and textual data in social medias, we propose an end-to-end deep\nfusion convolutional neural network to jointly learn textual and visual\nsentiment representations from training examples. The two modality information\nare fused together in a pooling layer and fed into fully-connected layers to\npredict the sentiment polarity. We evaluate the proposed approach on two widely\nused data sets. Results show that our method achieves promising result compared\nwith the state-of-the-art methods which clearly demonstrate its competency.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 14:19:48 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Chen", "Xingyue", ""], ["Wang", "Yunhong", ""], ["Liu", "Qingjie", ""]]}, {"id": "1711.07807", "submitter": "Stamatios Lefkimmiatis", "authors": "Stamatios Lefkimmiatis", "title": "Universal Denoising Networks : A Novel CNN Architecture for Image\n  Denoising", "comments": "Camera ready paper to appear in the Proceedings of CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a novel network architecture for learning discriminative image\nmodels that are employed to efficiently tackle the problem of grayscale and\ncolor image denoising. Based on the proposed architecture, we introduce two\ndifferent variants. The first network involves convolutional layers as a core\ncomponent, while the second one relies instead on non-local filtering layers\nand thus it is able to exploit the inherent non-local self-similarity property\nof natural images. As opposed to most of the existing deep network approaches,\nwhich require the training of a specific model for each considered noise level,\nthe proposed models are able to handle a wide range of noise levels using a\nsingle set of learned parameters, while they are very robust when the noise\ndegrading the latent image does not match the statistics of the noise used\nduring training. The latter argument is supported by results that we report on\npublicly available images corrupted by unknown noise and which we compare\nagainst solutions obtained by competing methods. At the same time the\nintroduced networks achieve excellent results under additive white Gaussian\nnoise (AWGN), which are comparable to those of the current state-of-the-art\nnetwork, while they depend on a more shallow architecture with the number of\ntrained parameters being one order of magnitude smaller. These properties make\nthe proposed networks ideal candidates to serve as sub-solvers on restoration\nmethods that deal with general inverse imaging problems such as deblurring,\ndemosaicking, superresolution, etc.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 14:36:14 GMT"}, {"version": "v2", "created": "Sat, 24 Mar 2018 18:24:08 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Lefkimmiatis", "Stamatios", ""]]}, {"id": "1711.07827", "submitter": "Ahmad W. Bitar (Ahmad Wasfi Bitar)", "authors": "Ahmad W. Bitar (Ahmad Wasfi Bitar), Mohammad M. Mansour, Ali Chehab", "title": "Efficient Implementation of a Recognition System Using the Cortex\n  Ventral Stream Model", "comments": "10 pages", "journal-ref": "In Proceedings of the 10th International Conference on Computer\n  Vision Theory and Applications (VISIGRAPP 2015)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, an efficient implementation for a recognition system based on\nthe original HMAX model of the visual cortex is proposed. Various optimizations\ntargeted to increase accuracy at the so-called layers S1, C1, and S2 of the\nHMAX model are proposed. At layer S1, all unimportant information such as\nillumination and expression variations are eliminated from the images. Each\nimage is then convolved with 64 separable Gabor filters in the spatial domain.\nAt layer C1, the minimum scales values are exploited to be embedded into the\nmaximum ones using the additive embedding space. At layer S2, the prototypes\nare generated in a more efficient way using Partitioning Around Medoid (PAM)\nclustering algorithm. The impact of these optimizations in terms of accuracy\nand computational complexity was evaluated on the Caltech101 database, and\ncompared with the baseline performance using support vector machine (SVM) and\nnearest neighbor (NN) classifiers. The results show that our model provides\nsignificant improvement in accuracy at the S1 layer by more than 10% where the\ncomputational complexity is also reduced. The accuracy is slightly increased\nfor both approximations at the C1 and S2 layers.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 15:03:04 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Bitar", "Ahmad W.", "", "Ahmad Wasfi Bitar"], ["Mansour", "Mohammad M.", ""], ["Chehab", "Ali", ""]]}, {"id": "1711.07828", "submitter": "Gabriel Spadon", "authors": "Bruno B. Machado, Gabriel Spadon, Mauro S. Arruda, Wesley N.\n  Goncalves, Andre C. P. L. F. Carvalho, Jose F. Rodrigues-Jr", "title": "A smartphone application to measure the quality of pest control spraying\n  machines via image analysis", "comments": "Paper to be published on the 33rd ACM/SIGAPP Symposium On Applied\n  Computing (SAC), 2018", "journal-ref": null, "doi": "10.1145/3167132.3167237", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for higher agricultural productivity has demanded the intensive use\nof pesticides. However, their correct use depends on assessment methods that\ncan accurately predict how well the pesticides' spraying covered the intended\ncrop region. Some methods have been proposed in the literature, but their high\ncost and low portability harm their widespread use. This paper proposes and\nexperimentally evaluates a new methodology based on the use of a\nsmartphone-based mobile application, named DropLeaf. Experiments performed\nusing DropLeaf showed that, in addition to its versatility, it can predict with\nhigh accuracy the pesticide spraying. DropLeaf is a five-fold image-processing\nmethodology based on: (i) color space conversion, (ii) threshold noise removal,\n(iii) convolutional operations of dilation and erosion, (iv) detection of\ncontour markers in the water-sensitive card, and, (v) identification of\ndroplets via the marker-controlled watershed transformation. The authors\nperformed successful experiments over two case studies, the first using a set\nof synthetic cards and the second using a real-world crop. The proposed tool\ncan be broadly used by farmers equipped with conventional mobile phones,\nimproving the use of pesticides with health, environmental and financial\nbenefits.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 15:04:07 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 10:03:16 GMT"}, {"version": "v3", "created": "Sat, 16 Dec 2017 20:50:15 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Machado", "Bruno B.", ""], ["Spadon", "Gabriel", ""], ["Arruda", "Mauro S.", ""], ["Goncalves", "Wesley N.", ""], ["Carvalho", "Andre C. P. L. F.", ""], ["Rodrigues-Jr", "Jose F.", ""]]}, {"id": "1711.07829", "submitter": "Taihang Dong", "authors": "Taihang Dong, Sheng Zhong", "title": "Discussion among Different Methods of Updating Model Filter in Object\n  Tracking", "comments": "8 pages, 3 figures, SPIE 10th International Symposium on\n  Multispectral Image Processing and Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative correlation filters (DCF) have recently shown excellent\nperformance in visual object tracking area. In this paper, we summarize the\nmethods of updating model filter from discriminative correlation filter (DCF)\nbased tracking algorithms and analyzes similarities and differences among these\nmethods. We deduce the relationship between updating coefficient in high\ndimension (kernel trick), updating filter in frequency domain and updating\nfilter in spatial domain, and analyze the difference among these different\nways. We also analyze the difference between the updating filter directly and\nupdating filter's numerator (object response power) with updating filter's\ndenominator (filter's power). The experiments about comparing different\nupdating methods and visualizing the template filters are used to prove our\nderivation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 15:09:29 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 08:20:29 GMT"}, {"version": "v3", "created": "Sun, 15 Nov 2020 13:26:12 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Dong", "Taihang", ""], ["Zhong", "Sheng", ""]]}, {"id": "1711.07835", "submitter": "Taihang Dong", "authors": "Taihang Dong, Sheng Zhong", "title": "Robust Object Tracking Based on Self-adaptive Search Area", "comments": "10 pages, 4 figures, 3 tables, SPIE 10th International Symposium on\n  Multispectral Image Processing and Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative correlation filter (DCF) based trackers have recently achieved\nexcellent performance with great computational efficiency. However, DCF based\ntrackers suffer boundary effects, which result in unstable performance in\nchallenging situations exhibiting fast motion. In this paper, we propose a\nnovel method to mitigate this side-effect in DCF based trackers. We change the\nsearch area according to the prediction of target motion. When the object moves\nfast, broad search area could alleviate boundary effects and reserve the\nprobability of locating the object. When the object moves slowly, narrow search\narea could prevent effect of useless background information and improve\ncomputational efficiency to attain real-time performance. This strategy can\nimpressively soothe boundary effects in situations exhibiting fast motion and\nmotion blur, and it can be used in almost all DCF based trackers. The\nexperiments on OTB benchmark show that the proposed framework improves the\nperformance compared with the baseline trackers.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 15:15:11 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 08:27:40 GMT"}, {"version": "v3", "created": "Sun, 15 Nov 2020 13:06:43 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Dong", "Taihang", ""], ["Zhong", "Sheng", ""]]}, {"id": "1711.07837", "submitter": "Simon Meister", "authors": "Simon Meister, Junhwa Hur, Stefan Roth", "title": "UnFlow: Unsupervised Learning of Optical Flow with a Bidirectional\n  Census Loss", "comments": "9 pages, To appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of end-to-end deep learning, many advances in computer vision are\ndriven by large amounts of labeled data. In the optical flow setting, however,\nobtaining dense per-pixel ground truth for real scenes is difficult and thus\nsuch data is rare. Therefore, recent end-to-end convolutional networks for\noptical flow rely on synthetic datasets for supervision, but the domain\nmismatch between training and test scenarios continues to be a challenge.\nInspired by classical energy-based optical flow methods, we design an\nunsupervised loss based on occlusion-aware bidirectional flow estimation and\nthe robust census transform to circumvent the need for ground truth flow. On\nthe KITTI benchmarks, our unsupervised approach outperforms previous\nunsupervised deep networks by a large margin, and is even more accurate than\nsimilar supervised methods trained on synthetic datasets alone. By optionally\nfine-tuning on the KITTI training data, our method achieves competitive optical\nflow accuracy on the KITTI 2012 and 2015 benchmarks, thus in addition enabling\ngeneric pre-training of supervised networks for datasets with limited amounts\nof ground truth.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 15:19:26 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Meister", "Simon", ""], ["Hur", "Junhwa", ""], ["Roth", "Stefan", ""]]}, {"id": "1711.07846", "submitter": "Gordon Christie", "authors": "Gordon Christie, Neil Fendley, James Wilson, Ryan Mukherjee", "title": "Functional Map of the World", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new dataset, Functional Map of the World (fMoW), which aims to\ninspire the development of machine learning models capable of predicting the\nfunctional purpose of buildings and land use from temporal sequences of\nsatellite images and a rich set of metadata features. The metadata provided\nwith each image enables reasoning about location, time, sun angles, physical\nsizes, and other features when making predictions about objects in the image.\nOur dataset consists of over 1 million images from over 200 countries. For each\nimage, we provide at least one bounding box annotation containing one of 63\ncategories, including a \"false detection\" category. We present an analysis of\nthe dataset along with baseline approaches that reason about metadata and\ntemporal views. Our data, code, and pretrained models have been made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 15:28:00 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 04:55:20 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 19:03:50 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Christie", "Gordon", ""], ["Fendley", "Neil", ""], ["Wilson", "James", ""], ["Mukherjee", "Ryan", ""]]}, {"id": "1711.07871", "submitter": "Ya Ju Fan", "authors": "Ya Ju Fan", "title": "Autoencoder Node Saliency: Selecting Relevant Latent Representations", "comments": null, "journal-ref": "Pattern Recognition, Volume 88, 2019, Pages 643-653", "doi": "10.1016/j.patcog.2018.12.015", "report-no": "ISSN 0031-3203", "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The autoencoder is an artificial neural network model that learns hidden\nrepresentations of unlabeled data. With a linear transfer function it is\nsimilar to the principal component analysis (PCA). While both methods use\nweight vectors for linear transformations, the autoencoder does not come with\nany indication similar to the eigenvalues in PCA that are paired with the\neigenvectors. We propose a novel supervised node saliency (SNS) method that\nranks the hidden nodes by comparing class distributions of latent\nrepresentations against a fixed reference distribution. The latent\nrepresentations of a hidden node can be described using a one-dimensional\nhistogram. We apply normalized entropy difference (NED) to measure the\n\"interestingness\" of the histograms, and conclude a property for NED values to\nidentify a good classifying node. By applying our methods to real data sets, we\ndemonstrate the ability of SNS to explain what the trained autoencoders have\nlearned.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 16:17:14 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 02:09:31 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Fan", "Ya Ju", ""]]}, {"id": "1711.07888", "submitter": "Olivia Wiles", "authors": "Olivia Wiles and Andrew Zisserman", "title": "SilNet : Single- and Multi-View Reconstruction by Learning from\n  Silhouettes", "comments": "BMVC 2017; Best Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is 3D shape understanding from single and\nmultiple images. To this end, we introduce a new deep-learning architecture and\nloss function, SilNet, that can handle multiple views in an order-agnostic\nmanner. The architecture is fully convolutional, and for training we use a\nproxy task of silhouette prediction, rather than directly learning a mapping\nfrom 2D images to 3D shape as has been the target in most recent work.\n  We demonstrate that with the SilNet architecture there is generalisation over\nthe number of views -- for example, SilNet trained on 2 views can be used with\n3 or 4 views at test-time; and performance improves with more views.\n  We introduce two new synthetics datasets: a blobby object dataset useful for\npre-training, and a challenging and realistic sculpture dataset; and\ndemonstrate on these datasets that SilNet has indeed learnt 3D shape. Finally,\nwe show that SilNet exceeds the state of the art on the ShapeNet benchmark\ndataset, and use SilNet to generate novel views of the sculpture dataset.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 16:33:18 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Wiles", "Olivia", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1711.07933", "submitter": "Pratul Srinivasan", "authors": "Pratul P. Srinivasan, Rahul Garg, Neal Wadhwa, Ren Ng, Jonathan T.\n  Barron", "title": "Aperture Supervision for Monocular Depth Estimation", "comments": "To appear at CVPR 2018 (updated to camera ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to train machine learning algorithms to estimate\nscene depths from a single image, by using the information provided by a\ncamera's aperture as supervision. Prior works use a depth sensor's outputs or\nimages of the same scene from alternate viewpoints as supervision, while our\nmethod instead uses images from the same viewpoint taken with a varying camera\naperture. To enable learning algorithms to use aperture effects as supervision,\nwe introduce two differentiable aperture rendering functions that use the input\nimage and predicted depths to simulate the depth-of-field effects caused by\nreal camera apertures. We train a monocular depth estimation network end-to-end\nto predict the scene depths that best explain these finite aperture images as\ndefocus-blurred renderings of the input all-in-focus image.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 17:39:51 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 17:26:00 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Srinivasan", "Pratul P.", ""], ["Garg", "Rahul", ""], ["Wadhwa", "Neal", ""], ["Ng", "Ren", ""], ["Barron", "Jonathan T.", ""]]}, {"id": "1711.07971", "submitter": "Xiaolong Wang", "authors": "Xiaolong Wang, Ross Girshick, Abhinav Gupta, Kaiming He", "title": "Non-local Neural Networks", "comments": "CVPR 2018, code is available at:\n  https://github.com/facebookresearch/video-nonlocal-net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both convolutional and recurrent operations are building blocks that process\none local neighborhood at a time. In this paper, we present non-local\noperations as a generic family of building blocks for capturing long-range\ndependencies. Inspired by the classical non-local means method in computer\nvision, our non-local operation computes the response at a position as a\nweighted sum of the features at all positions. This building block can be\nplugged into many computer vision architectures. On the task of video\nclassification, even without any bells and whistles, our non-local models can\ncompete or outperform current competition winners on both Kinetics and Charades\ndatasets. In static image recognition, our non-local models improve object\ndetection/segmentation and pose estimation on the COCO suite of tasks. Code is\navailable at https://github.com/facebookresearch/video-nonlocal-net .\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 18:51:16 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 18:59:43 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 06:40:44 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Wang", "Xiaolong", ""], ["Girshick", "Ross", ""], ["Gupta", "Abhinav", ""], ["He", "Kaiming", ""]]}, {"id": "1711.07974", "submitter": "Bingqing Yu", "authors": "Bingqing Yu, James J. Clark", "title": "WAYLA - Generating Images from Eye Movements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for reconstructing images viewed by observers based only\non their eye movements. By exploring the relationships between gaze patterns\nand image stimuli, the \"What Are You Looking At?\" (WAYLA) system learns to\nsynthesize photo-realistic images that are similar to the original pictures\nbeing viewed. The WAYLA approach is based on the Conditional Generative\nAdversarial Network (Conditional GAN) image-to-image translation technique of\nIsola et al. We consider two specific applications - the first, of\nreconstructing newspaper images from gaze heat maps, and the second, of\ndetailed reconstruction of images containing only text. The newspaper image\nreconstruction process is divided into two image-to-image translation\noperations, the first mapping gaze heat maps into image segmentations, and the\nsecond mapping the generated segmentation into a newspaper image. We validate\nthe performance of our approach using various evaluation metrics, along with\nhuman visual inspection. All results confirm the ability of our network to\nperform image generation tasks using eye tracking data.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 18:53:31 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Yu", "Bingqing", ""], ["Clark", "James J.", ""]]}, {"id": "1711.07992", "submitter": "Namit Juneja", "authors": "Namit Juneja, Rajesh Kumar Muthu", "title": "Generating Analytic Insights on Human Behaviour using Image Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a method to track human figures in physical spaces and\nthen utilizes this data to generate several data points such as footfall\ndistribution, demographic analysis,heat maps as well as gender distribution.\nThe proposed framework aims to establish this while utilizing minimum\ncomputational resources while remaining real time. It is often useful to have\ninformation such as what kind of people visit a certain place or what hour of\nthe day experiences maximum activity, Such analysis can be used improve sales,\nmanage huge number of people as well as predict future behaviour. The proposed\nframework is designed in a way such that it can take input streams from IP\ncameras and use that to generate relevant data points using open source tools\nsuch as OpenCV and raspberryPi.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 19:00:32 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Juneja", "Namit", ""], ["Muthu", "Rajesh Kumar", ""]]}, {"id": "1711.07998", "submitter": "Edward Kim", "authors": "Edward Kim, Darryl Hannan, Garrett Kenyon", "title": "Deep Sparse Coding for Invariant Multimodal Halle Berry Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep feed-forward convolutional neural networks (CNNs) have become ubiquitous\nin virtually all machine learning and computer vision challenges; however,\nadvancements in CNNs have arguably reached an engineering saturation point\nwhere incremental novelty results in minor performance gains. Although there is\nevidence that object classification has reached human levels on narrowly\ndefined tasks, for general applications, the biological visual system is far\nsuperior to that of any computer. Research reveals there are numerous missing\ncomponents in feed-forward deep neural networks that are critical in mammalian\nvision. The brain does not work solely in a feed-forward fashion, but rather\nall of the neurons are in competition with each other; neurons are integrating\ninformation in a bottom up and top down fashion and incorporating expectation\nand feedback in the modeling process. Furthermore, our visual cortex is working\nin tandem with our parietal lobe, integrating sensory information from various\nmodalities.\n  In our work, we sought to improve upon the standard feed-forward deep\nlearning model by augmenting them with biologically inspired concepts of\nsparsity, top-down feedback, and lateral inhibition. We define our model as a\nsparse coding problem using hierarchical layers. We solve the sparse coding\nproblem with an additional top-down feedback error driving the dynamics of the\nneural network. While building and observing the behavior of our model, we were\nfascinated that multimodal, invariant neurons naturally emerged that mimicked,\n\"Halle Berry neurons\" found in the human brain. Furthermore, our sparse\nrepresentation of multimodal signals demonstrates qualitative and quantitative\nsuperiority to the standard feed-forward joint embedding in common vision and\nmachine learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 19:06:04 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 21:30:32 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Kim", "Edward", ""], ["Hannan", "Darryl", ""], ["Kenyon", "Garrett", ""]]}, {"id": "1711.07999", "submitter": "Aaron Walsman", "authors": "Aaron Walsman, Weilin Wan, Tanner Schmidt, Dieter Fox", "title": "Dynamic High Resolution Deformable Articulated Tracking", "comments": "10 pages, 8 figures, Presented at 3DV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last several years have seen significant progress in using depth cameras\nfor tracking articulated objects such as human bodies, hands, and robotic\nmanipulators. Most approaches focus on tracking skeletal parameters of a fixed\nshape model, which makes them insufficient for applications that require\naccurate estimates of deformable object surfaces. To overcome this limitation,\nwe present a 3D model-based tracking system for articulated deformable objects.\nOur system is able to track human body pose and high resolution surface\ncontours in real time using a commodity depth sensor and GPU hardware. We\nimplement this as a joint optimization over a skeleton to account for changes\nin pose, and over the vertices of a high resolution mesh to track the subject's\nshape. Through experimental results we show that we are able to capture dynamic\nsub-centimeter surface detail such as folds and wrinkles in clothing. We also\nshow that this shape estimation aids kinematic pose estimation by providing a\nmore accurate target to match against the point cloud. The end result is highly\naccurate spatiotemporal and semantic information which is well suited for\nphysical human robot interaction as well as virtual and augmented reality\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 19:07:30 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Walsman", "Aaron", ""], ["Wan", "Weilin", ""], ["Schmidt", "Tanner", ""], ["Fox", "Dieter", ""]]}, {"id": "1711.08000", "submitter": "Bingqing Yu", "authors": "Bingqing Yu, James J. Clark", "title": "Personalization of Saliency Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing saliency models use low-level features or task descriptions\nwhen generating attention predictions. However, the link between observer\ncharacteristics and gaze patterns is rarely investigated. We present a novel\nsaliency prediction technique which takes viewers' identities and personal\ntraits into consideration when modeling human attention. Instead of only\ncomputing image salience for average observers, we consider the interpersonal\nvariation in the viewing behaviors of observers with different personal traits\nand backgrounds. We present an enriched derivative of the GAN network, which is\nable to generate personalized saliency predictions when fed with image stimuli\nand specific information about the observer. Our model contains a generator\nwhich generates grayscale saliency heat maps based on the image and an observer\nlabel. The generator is paired with an adversarial discriminator which learns\nto distinguish generated salience from ground truth salience. The discriminator\nalso has the observer label as an input, which contributes to the\npersonalization ability of our approach. We evaluate the performance of our\npersonalized salience model by comparison with a benchmark model along with\nother un-personalized predictions, and illustrate improvements in prediction\naccuracy for all tested observer groups.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 19:10:44 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Yu", "Bingqing", ""], ["Clark", "James J.", ""]]}, {"id": "1711.08006", "submitter": "Ning Xie", "authors": "Ning Xie, Md Kamruzzaman Sarker, Derek Doran, Pascal Hitzler, Michael\n  Raymer", "title": "Relating Input Concepts to Convolutional Neural Network Decisions", "comments": "10 pages (including references), 9 figures, paper accepted by NIPS\n  IEVDL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many current methods to interpret convolutional neural networks (CNNs) use\nvisualization techniques and words to highlight concepts of the input seemingly\nrelevant to a CNN's decision. The methods hypothesize that the recognition of\nthese concepts are instrumental in the decision a CNN reaches, but the nature\nof this relationship has not been well explored. To address this gap, this\npaper examines the quality of a concept's recognition by a CNN and the degree\nto which the recognitions are associated with CNN decisions. The study\nconsiders a CNN trained for scene recognition over the ADE20k dataset. It uses\na novel approach to find and score the strength of minimally distributed\nrepresentations of input concepts (defined by objects in scene images) across\nlate stage feature maps. Subsequent analysis finds evidence that concept\nrecognition impacts decision making. Strong recognition of concepts\nfrequently-occurring in few scenes are indicative of correct decisions, but\nrecognizing concepts common to many scenes may mislead the network.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 19:37:13 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Xie", "Ning", ""], ["Sarker", "Md Kamruzzaman", ""], ["Doran", "Derek", ""], ["Hitzler", "Pascal", ""], ["Raymer", "Michael", ""]]}, {"id": "1711.08014", "submitter": "Abhishek Kumar", "authors": "Hang Shao, Abhishek Kumar, P. Thomas Fletcher", "title": "The Riemannian Geometry of Deep Generative Models", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models learn a mapping from a low dimensional latent space to\na high-dimensional data space. Under certain regularity conditions, these\nmodels parameterize nonlinear manifolds in the data space. In this paper, we\ninvestigate the Riemannian geometry of these generated manifolds. First, we\ndevelop efficient algorithms for computing geodesic curves, which provide an\nintrinsic notion of distance between points on the manifold. Second, we develop\nan algorithm for parallel translation of a tangent vector along a path on the\nmanifold. We show how parallel translation can be used to generate analogies,\ni.e., to transport a change in one data point into a semantically similar\nchange of another data point. Our experiments on real image data show that the\nmanifolds learned by deep generative models, while nonlinear, are surprisingly\nclose to zero curvature. The practical implication is that linear paths in the\nlatent space closely approximate geodesics on the generated manifold. However,\nfurther investigation into this phenomenon is warranted, to identify if there\nare other architectures or datasets where curvature plays a more prominent\nrole. We believe that exploring the Riemannian geometry of deep generative\nmodels, using the tools developed in this paper, will be an important step in\nunderstanding the high-dimensional, nonlinear spaces these models learn.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 19:59:24 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Shao", "Hang", ""], ["Kumar", "Abhishek", ""], ["Fletcher", "P. Thomas", ""]]}, {"id": "1711.08040", "submitter": "Sachin Mehta", "authors": "Sachin Mehta, Hannaneh Hajishirzi, and Linda Shapiro", "title": "Identifying Most Walkable Direction for Navigation in an Outdoor\n  Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for identifying the most walkable direction for\nnavigation using a hand-held camera. Our approach extracts semantically rich\ncontextual information from the scene using a custom encoder-decoder\narchitecture for semantic segmentation and models the spatial and temporal\nbehavior of objects in the scene using a spatio-temporal graph. The system\nlearns to minimize a cost function over the spatial and temporal object\nattributes to identify the most walkable direction. We construct a new\nannotated navigation dataset collected using a hand-held mobile camera in an\nunconstrained outdoor environment, which includes challenging settings such as\nhighly dynamic scenes, occlusion between objects, and distortions. Our system\nachieves an accuracy of 84% on predicting a safe direction. We also show that\nour custom segmentation network is both fast and accurate, achieving mIOU (mean\nintersection over union) scores of 81 and 44.7 on the PASCAL VOC and the PASCAL\nContext datasets, respectively, while running at about 21 frames per second.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 21:15:33 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 03:18:52 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Mehta", "Sachin", ""], ["Hajishirzi", "Hannaneh", ""], ["Shapiro", "Linda", ""]]}, {"id": "1711.08097", "submitter": "Wangli Hao", "authors": "Wangli Hao, Zhaoxiang Zhang, He Guan, Guibo Zhu", "title": "Integrating both Visual and Audio Cues for Enhanced Video Caption", "comments": "Have some problems need to be handled", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video caption refers to generating a descriptive sentence for a specific\nshort video clip automatically, which has achieved remarkable success recently.\nHowever, most of the existing methods focus more on visual information while\nignoring the synchronized audio cues. We propose three multimodal deep fusion\nstrategies to maximize the benefits of visual-audio resonance information. The\nfirst one explores the impact on cross-modalities feature fusion from low to\nhigh order. The second establishes the visual-audio short-term dependency by\nsharing weights of corresponding front-end networks. The third extends the\ntemporal dependency to long-term through sharing multimodal memory across\nvisual and audio modalities. Extensive experiments have validated the\neffectiveness of our three cross-modalities fusion strategies on two benchmark\ndatasets, including Microsoft Research Video to Text (MSRVTT) and Microsoft\nVideo Description (MSVD). It is worth mentioning that sharing weight can\ncoordinate visual-audio feature fusion effectively and achieve the state-of-art\nperformance on both BELU and METEOR metrics. Furthermore, we first propose a\ndynamic multimodal feature fusion framework to deal with the part modalities\nmissing case. Experimental results demonstrate that even in the audio absence\nmode, we can still obtain comparable results with the aid of the additional\naudio modality inference module.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 01:12:00 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 04:03:21 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Hao", "Wangli", ""], ["Zhang", "Zhaoxiang", ""], ["Guan", "He", ""], ["Zhu", "Guibo", ""]]}, {"id": "1711.08102", "submitter": "Wangli Hao", "authors": "Wangli Hao, Zhaoxiang Zhang, He Guan", "title": "CMCGAN: A Uniform Framework for Cross-Modal Visual-Audio Mutual\n  Generation", "comments": "Have some problems need to be handled", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual and audio modalities are two symbiotic modalities underlying videos,\nwhich contain both common and complementary information. If they can be mined\nand fused sufficiently, performances of related video tasks can be\nsignificantly enhanced. However, due to the environmental interference or\nsensor fault, sometimes, only one modality exists while the other is abandoned\nor missing. By recovering the missing modality from the existing one based on\nthe common information shared between them and the prior information of the\nspecific modality, great bonus will be gained for various vision tasks. In this\npaper, we propose a Cross-Modal Cycle Generative Adversarial Network (CMCGAN)\nto handle cross-modal visual-audio mutual generation. Specifically, CMCGAN is\ncomposed of four kinds of subnetworks: audio-to-visual, visual-to-audio,\naudio-to-audio and visual-to-visual subnetworks respectively, which are\norganized in a cycle architecture. CMCGAN has several remarkable advantages.\nFirstly, CMCGAN unifies visual-audio mutual generation into a common framework\nby a joint corresponding adversarial loss. Secondly, through introducing a\nlatent vector with Gaussian distribution, CMCGAN can handle dimension and\nstructure asymmetry over visual and audio modalities effectively. Thirdly,\nCMCGAN can be trained end-to-end to achieve better convenience. Benefiting from\nCMCGAN, we develop a dynamic multimodal classification network to handle the\nmodality missing problem. Abundant experiments have been conducted and validate\nthat CMCGAN obtains the state-of-the-art cross-modal visual-audio generation\nresults. Furthermore, it is shown that the generated modality achieves\ncomparable effects with those of original modality, which demonstrates the\neffectiveness and advantages of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 01:36:20 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 04:01:40 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Hao", "Wangli", ""], ["Zhang", "Zhaoxiang", ""], ["Guan", "He", ""]]}, {"id": "1711.08105", "submitter": "Damien Teney", "authors": "Damien Teney, Anton van den Hengel", "title": "Visual Question Answering as a Meta Learning Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predominant approach to Visual Question Answering (VQA) demands that the\nmodel represents within its weights all of the information required to answer\nany question about any image. Learning this information from any real training\nset seems unlikely, and representing it in a reasonable number of weights\ndoubly so. We propose instead to approach VQA as a meta learning task, thus\nseparating the question answering method from the information required. At test\ntime, the method is provided with a support set of example questions/answers,\nover which it reasons to resolve the given question. The support set is not\nfixed and can be extended without retraining, thereby expanding the\ncapabilities of the model. To exploit this dynamically provided information, we\nadapt a state-of-the-art VQA model with two techniques from the recent meta\nlearning literature, namely prototypical networks and meta networks.\nExperiments demonstrate the capability of the system to learn to produce\ncompletely novel answers (i.e. never seen during training) from examples\nprovided at test time. In comparison to the existing state of the art, the\nproposed method produces qualitatively distinct results with higher recall of\nrare answers, and a better sample efficiency that allows training with little\ninitial data. More importantly, it represents an important step towards\nvision-and-language methods that can learn and reason on-the-fly.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 02:04:31 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Teney", "Damien", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1711.08106", "submitter": "Qian Yu", "authors": "Qian Yu, Xiaobin Chang, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales", "title": "The Devil is in the Middle: Exploiting Mid-level Representations for\n  Cross-Domain Instance Matching", "comments": "Reference updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many vision problems require matching images of object instances across\ndifferent domains. These include fine-grained sketch-based image retrieval\n(FG-SBIR) and Person Re-identification (person ReID). Existing approaches\nattempt to learn a joint embedding space where images from different domains\ncan be directly compared. In most cases, this space is defined by the output of\nthe final layer of a deep neural network (DNN), which primarily contains\nfeatures of a high semantic level. In this paper, we argue that both high and\nmid-level features are relevant for cross-domain instance matching (CDIM).\nImportantly, mid-level features already exist in earlier layers of the DNN.\nThey just need to be extracted, represented, and fused properly with the final\nlayer. Based on this simple but powerful idea, we propose a unified framework\nfor CDIM. Instantiating our framework for FG-SBIR and ReID, we show that our\nsimple models can easily beat the state-of-the-art models, which are often\nequipped with much more elaborate architectures.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 02:10:03 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 20:12:42 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Yu", "Qian", ""], ["Chang", "Xiaobin", ""], ["Song", "Yi-Zhe", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1711.08141", "submitter": "Bichen Wu", "authors": "Bichen Wu, Alvin Wan, Xiangyu Yue, Peter Jin, Sicheng Zhao, Noah\n  Golmant, Amir Gholaminejad, Joseph Gonzalez, Kurt Keutzer", "title": "Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions", "comments": "Source code will be released afterwards", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks rely on convolutions to aggregate spatial information.\nHowever, spatial convolutions are expensive in terms of model size and\ncomputation, both of which grow quadratically with respect to kernel size. In\nthis paper, we present a parameter-free, FLOP-free \"shift\" operation as an\nalternative to spatial convolutions. We fuse shifts and point-wise convolutions\nto construct end-to-end trainable shift-based modules, with a hyperparameter\ncharacterizing the tradeoff between accuracy and efficiency. To demonstrate the\noperation's efficacy, we replace ResNet's 3x3 convolutions with shift-based\nmodules for improved CIFAR10 and CIFAR100 accuracy using 60% fewer parameters;\nwe additionally demonstrate the operation's resilience to parameter reduction\non ImageNet, outperforming ResNet family members. We finally show the shift\noperation's applicability across domains, achieving strong performance with\nfewer parameters on classification, face verification and style transfer.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 05:52:19 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 07:07:21 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Wu", "Bichen", ""], ["Wan", "Alvin", ""], ["Yue", "Xiangyu", ""], ["Jin", "Peter", ""], ["Zhao", "Sicheng", ""], ["Golmant", "Noah", ""], ["Gholaminejad", "Amir", ""], ["Gonzalez", "Joseph", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1711.08155", "submitter": "Sk. Mohammadul Haque", "authors": "Sk. Mohammadul Haque and Venu Madhav Govindu", "title": "A Face Fairness Framework for 3D Meshes", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a face fairness framework for 3D meshes that\npreserves the regular shape of faces and is applicable to a variety of 3D mesh\nrestoration tasks. Specifically, we present a number of desirable properties\nfor any mesh restoration method and show that our framework satisfies them. We\nthen apply our framework to two different tasks --- mesh-denoising and\nmesh-refinement, and present comparative results for these two tasks showing\nimprovement over other relevant methods in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 07:22:08 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Haque", "Sk. Mohammadul", ""], ["Govindu", "Venu Madhav", ""]]}, {"id": "1711.08174", "submitter": "Ali Diba", "authors": "Ali Diba, Vivek Sharma, Rainer Stiefelhagen, Luc Van Gool", "title": "Weakly Supervised Object Discovery by Generative Adversarial & Ranking\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep generative adversarial networks (GAN) recently have been shown to be\npromising for different computer vision applications, like image edit- ing,\nsynthesizing high resolution images, generating videos, etc. These networks and\nthe corresponding learning scheme can handle various visual space map- pings.\nWe approach GANs with a novel training method and learning objective, to\ndiscover multiple object instances for three cases: 1) synthesizing a picture\nof a specific object within a cluttered scene; 2) localizing different\ncategories in images for weakly supervised object detection; and 3) improving\nobject discov- ery in object detection pipelines. A crucial advantage of our\nmethod is that it learns a new deep similarity metric, to distinguish multiple\nobjects in one im- age. We demonstrate that the network can act as an\nencoder-decoder generating parts of an image which contain an object, or as a\nmodified deep CNN to rep- resent images for object detection in supervised and\nweakly supervised scheme. Our ranking GAN offers a novel way to search through\nimages for object specific patterns. We have conducted experiments for\ndifferent scenarios and demonstrate the method performance for object\nsynthesizing and weakly supervised object detection and classification using\nthe MS-COCO and PASCAL VOC datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 08:36:39 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 15:06:04 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Diba", "Ali", ""], ["Sharma", "Vivek", ""], ["Stiefelhagen", "Rainer", ""], ["Van Gool", "Luc", ""]]}, {"id": "1711.08180", "submitter": "Seong-Jin Park", "authors": "Seong-Jin Park and Ki-Sang Hong", "title": "Video Semantic Object Segmentation by Self-Adaptation of DCNN", "comments": "Submitted to Pattern Recognition Letters on August 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new framework for semantic segmentation of objects in\nvideos. We address the label inconsistency problem of deep convolutional neural\nnetworks (DCNNs) by exploiting the fact that videos have multiple frames; in a\nfew frames the object is confidently-estimated (CE) and we use the information\nin them to improve labels of the other frames. Given the semantic segmentation\nresults of each frame obtained from DCNN, we sample several CE frames to adapt\nthe DCNN model to the input video by focusing on specific instances in the\nvideo rather than general objects in various circumstances. We propose offline\nand online approaches under different supervision levels. In experiments our\nmethod achieved great improvement over the original model and previous\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 09:08:03 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Park", "Seong-Jin", ""], ["Hong", "Ki-Sang", ""]]}, {"id": "1711.08184", "submitter": "Chi Zhang", "authors": "Xuan Zhang, Hao Luo, Xing Fan, Weilai Xiang, Yixiao Sun, Qiqi Xiao,\n  Wei Jiang, Chi Zhang, Jian Sun", "title": "AlignedReID: Surpassing Human-Level Performance in Person\n  Re-Identification", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method called AlignedReID that extracts a\nglobal feature which is jointly learned with local features. Global feature\nlearning benefits greatly from local feature learning, which performs an\nalignment/matching by calculating the shortest path between two sets of local\nfeatures, without requiring extra supervision. After the joint learning, we\nonly keep the global feature to compute the similarities between images. Our\nmethod achieves rank-1 accuracy of 94.4% on Market1501 and 97.8% on CUHK03,\noutperforming state-of-the-art methods by a large margin. We also evaluate\nhuman-level performance and demonstrate that our method is the first to surpass\nhuman-level performance on Market1501 and CUHK03, two widely used Person ReID\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 09:15:22 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 11:08:29 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Zhang", "Xuan", ""], ["Luo", "Hao", ""], ["Fan", "Xing", ""], ["Xiang", "Weilai", ""], ["Sun", "Yixiao", ""], ["Xiao", "Qiqi", ""], ["Jiang", "Wei", ""], ["Zhang", "Chi", ""], ["Sun", "Jian", ""]]}, {"id": "1711.08189", "submitter": "Bharat Singh", "authors": "Bharat Singh, Larry S. Davis", "title": "An Analysis of Scale Invariance in Object Detection - SNIP", "comments": "CVPR 2018, camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An analysis of different techniques for recognizing and detecting objects\nunder extreme scale variation is presented. Scale specific and scale invariant\ndesign of detectors are compared by training them with different configurations\nof input data. By evaluating the performance of different network architectures\nfor classifying small objects on ImageNet, we show that CNNs are not robust to\nchanges in scale. Based on this analysis, we propose to train and test\ndetectors on the same scales of an image-pyramid. Since small and large objects\nare difficult to recognize at smaller and larger scales respectively, we\npresent a novel training scheme called Scale Normalization for Image Pyramids\n(SNIP) which selectively back-propagates the gradients of object instances of\ndifferent sizes as a function of the image scale. On the COCO dataset, our\nsingle model performance is 45.7% and an ensemble of 3 networks obtains an mAP\nof 48.3%. We use off-the-shelf ImageNet-1000 pre-trained models and only train\nwith bounding box supervision. Our submission won the Best Student Entry in the\nCOCO 2017 challenge. Code will be made available at\n\\url{http://bit.ly/2yXVg4c}.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 09:30:06 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 12:47:23 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Singh", "Bharat", ""], ["Davis", "Larry S.", ""]]}, {"id": "1711.08195", "submitter": "Baoyu Jing", "authors": "Baoyu Jing, Pengtao Xie, Eric Xing", "title": "On the Automatic Generation of Medical Imaging Reports", "comments": "ACL 2018", "journal-ref": null, "doi": "10.18653/v1/P18-1240", "report-no": "P18-1240", "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical imaging is widely used in clinical practice for diagnosis and\ntreatment. Report-writing can be error-prone for unexperienced physicians, and\ntime- consuming and tedious for experienced physicians. To address these\nissues, we study the automatic generation of medical imaging reports. This task\npresents several challenges. First, a complete report contains multiple\nheterogeneous forms of information, including findings and tags. Second,\nabnormal regions in medical images are difficult to identify. Third, the re-\nports are typically long, containing multiple sentences. To cope with these\nchallenges, we (1) build a multi-task learning framework which jointly performs\nthe pre- diction of tags and the generation of para- graphs, (2) propose a\nco-attention mechanism to localize regions containing abnormalities and\ngenerate narrations for them, (3) develop a hierarchical LSTM model to generate\nlong paragraphs. We demonstrate the effectiveness of the proposed methods on\ntwo publicly available datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 09:45:51 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 21:49:03 GMT"}, {"version": "v3", "created": "Fri, 20 Jul 2018 17:45:14 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Jing", "Baoyu", ""], ["Xie", "Pengtao", ""], ["Xing", "Eric", ""]]}, {"id": "1711.08200", "submitter": "Ali Diba", "authors": "Ali Diba, Mohsen Fayyaz, Vivek Sharma, Amir Hossein Karami, Mohammad\n  Mahdi Arzani, Rahman Yousefzadeh, Luc Van Gool", "title": "Temporal 3D ConvNets: New Architecture and Transfer Learning for Video\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work in this paper is driven by the question how to exploit the temporal\ncues available in videos for their accurate classification, and for human\naction recognition in particular? Thus far, the vision community has focused on\nspatio-temporal approaches with fixed temporal convolution kernel depths. We\nintroduce a new temporal layer that models variable temporal convolution kernel\ndepths. We embed this new temporal layer in our proposed 3D CNN. We extend the\nDenseNet architecture - which normally is 2D - with 3D filters and pooling\nkernels. We name our proposed video convolutional network `Temporal 3D\nConvNet'~(T3D) and its new temporal layer `Temporal Transition Layer'~(TTL).\nOur experiments show that T3D outperforms the current state-of-the-art methods\non the HMDB51, UCF101 and Kinetics datasets.\n  The other issue in training 3D ConvNets is about training them from scratch\nwith a huge labeled dataset to get a reasonable performance. So the knowledge\nlearned in 2D ConvNets is completely ignored. Another contribution in this work\nis a simple and effective technique to transfer knowledge from a pre-trained 2D\nCNN to a randomly initialized 3D CNN for a stable weight initialization. This\nallows us to significantly reduce the number of training samples for 3D CNNs.\nThus, by finetuning this network, we beat the performance of generic and recent\nmethods in 3D CNNs, which were trained on large video datasets, e.g. Sports-1M,\nand finetuned on the target datasets, e.g. HMDB51/UCF101. The T3D codes will be\nreleased\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 10:01:37 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Diba", "Ali", ""], ["Fayyaz", "Mohsen", ""], ["Sharma", "Vivek", ""], ["Karami", "Amir Hossein", ""], ["Arzani", "Mohammad Mahdi", ""], ["Yousefzadeh", "Rahman", ""], ["Van Gool", "Luc", ""]]}, {"id": "1711.08229", "submitter": "Xiao Sun", "authors": "Xiao Sun, Bin Xiao, Fangyin Wei, Shuang Liang, Yichen Wei", "title": "Integral Human Pose Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art human pose estimation methods are based on heat map\nrepresentation. In spite of the good performance, the representation has a few\nissues in nature, such as not differentiable and quantization error. This work\nshows that a simple integral operation relates and unifies the heat map\nrepresentation and joint regression, thus avoiding the above issues. It is\ndifferentiable, efficient, and compatible with any heat map based methods. Its\neffectiveness is convincingly validated via comprehensive ablation experiments\nunder various settings, specifically on 3D pose estimation, for the first time.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 11:15:06 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 17:04:37 GMT"}, {"version": "v3", "created": "Tue, 20 Mar 2018 07:41:52 GMT"}, {"version": "v4", "created": "Tue, 18 Sep 2018 08:29:46 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Sun", "Xiao", ""], ["Xiao", "Bin", ""], ["Wei", "Fangyin", ""], ["Liang", "Shuang", ""], ["Wei", "Yichen", ""]]}, {"id": "1711.08238", "submitter": "Zhenxing Zheng", "authors": "Zhenxing Zheng, Gaoyun An, Qiuqi Ruan", "title": "Multi-Level Recurrent Residual Networks for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing Convolutional Neural Networks(CNNs) used for action recognition\nare either difficult to optimize or underuse crucial temporal information.\nInspired by the fact that the recurrent model consistently makes breakthroughs\nin the task related to sequence, we propose a novel Multi-Level Recurrent\nResidual Networks(MRRN) which incorporates three recognition streams. Each\nstream consists of a Residual Networks(ResNets) and a recurrent model. The\nproposed model captures spatiotemporal information by employing both\nalternative ResNets to learn spatial representations from static frames and\nstacked Simple Recurrent Units(SRUs) to model temporal dynamics. Three\ndistinct-level streams learned low-, mid-, high-level representations\nindependently are fused by computing a weighted average of their softmax scores\nto obtain the complementary representations of the video. Unlike previous\nmodels which boost performance at the cost of time complexity and space\ncomplexity, our models have a lower complexity by employing shortcut connection\nand are trained end-to-end with greater efficiency. MRRN displays significant\nperformance improvements compared to CNN-RNN framework baselines and obtains\ncomparable performance with the state-of-the-art, achieving 51.3% on HMDB-51\ndataset and 81.9% on UCF-101 dataset although no additional data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 11:40:29 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 11:31:34 GMT"}, {"version": "v3", "created": "Tue, 28 Nov 2017 14:58:12 GMT"}, {"version": "v4", "created": "Tue, 5 Dec 2017 14:37:23 GMT"}, {"version": "v5", "created": "Sat, 16 Dec 2017 08:27:39 GMT"}, {"version": "v6", "created": "Wed, 3 Jan 2018 09:20:09 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Zheng", "Zhenxing", ""], ["An", "Gaoyun", ""], ["Ruan", "Qiuqi", ""]]}, {"id": "1711.08241", "submitter": "Yizhak Ben-Shabat", "authors": "Yizhak Ben-Shabat, Michael Lindenbaum, Anath Fischer", "title": "3D Point Cloud Classification and Segmentation using 3D Modified Fisher\n  Vector Representation for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The point cloud is gaining prominence as a method for representing 3D shapes,\nbut its irregular format poses a challenge for deep learning methods. The\ncommon solution of transforming the data into a 3D voxel grid introduces its\nown challenges, mainly large memory size. In this paper we propose a novel 3D\npoint cloud representation called 3D Modified Fisher Vectors (3DmFV). Our\nrepresentation is hybrid as it combines the discrete structure of a grid with\ncontinuous generalization of Fisher vectors, in a compact and computationally\nefficient way. Using the grid enables us to design a new CNN architecture for\npoint cloud classification and part segmentation. In a series of experiments we\ndemonstrate competitive performance or even better than state-of-the-art on\nchallenging benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 11:47:42 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Ben-Shabat", "Yizhak", ""], ["Lindenbaum", "Michael", ""], ["Fischer", "Anath", ""]]}, {"id": "1711.08277", "submitter": "Boyang Deng", "authors": "Boyang Deng, Qing Liu, Siyuan Qiao, Alan Yuille", "title": "Few-shot Learning by Exploiting Visual Concepts within CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are one of the driving forces for the\nadvancement of computer vision. Despite their promising performances on many\ntasks, CNNs still face major obstacles on the road to achieving ideal machine\nintelligence. One is that CNNs are complex and hard to interpret. Another is\nthat standard CNNs require large amounts of annotated data, which is sometimes\nhard to obtain, and it is desirable to learn to recognize objects from few\nexamples. In this work, we address these limitations of CNNs by developing\nnovel, flexible, and interpretable models for few-shot learning. Our models are\nbased on the idea of encoding objects in terms of visual concepts (VCs), which\nare interpretable visual cues represented by the feature vectors within CNNs.\nWe first adapt the learning of VCs to the few-shot setting, and then uncover\ntwo key properties of feature encoding using VCs, which we call category\nsensitivity and spatial pattern. Motivated by these properties, we present two\nintuitive models for the problem of few-shot learning. Experiments show that\nour models achieve competitive performances, while being more flexible and\ninterpretable than alternative state-of-the-art few-shot learning methods. We\nconclude that using VCs helps expose the natural capability of CNNs for\nfew-shot learning.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 13:44:44 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 13:09:51 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 12:30:09 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Deng", "Boyang", ""], ["Liu", "Qing", ""], ["Qiao", "Siyuan", ""], ["Yuille", "Alan", ""]]}, {"id": "1711.08278", "submitter": "Fanglin Gu", "authors": "Zhenhua Wang, Fanglin Gu, Dani Lischinski, Daniel Cohen-Or, Changhe\n  Tu, Baoquan Chen", "title": "Neuron-level Selective Context Aggregation for Scene Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual information provides important cues for disambiguating visually\nsimilar pixels in scene segmentation. In this paper, we introduce a\nneuron-level Selective Context Aggregation (SCA) module for scene segmentation,\ncomprised of a contextual dependency predictor and a context aggregation\noperator. The dependency predictor is implicitly trained to infer contextual\ndependencies between different image regions. The context aggregation operator\naugments local representations with global context, which is aggregated\nselectively at each neuron according to its on-the-fly predicted dependencies.\nThe proposed mechanism enables data-driven inference of contextual\ndependencies, and facilitates context-aware feature learning. The proposed\nmethod improves strong baselines built upon VGG16 on challenging scene\nsegmentation datasets, which demonstrates its effectiveness in modeling context\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 13:45:06 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Wang", "Zhenhua", ""], ["Gu", "Fanglin", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Daniel", ""], ["Tu", "Changhe", ""], ["Chen", "Baoquan", ""]]}, {"id": "1711.08324", "submitter": "Fangzhou Liao", "authors": "Fangzhou Liao, Ming Liang, Zhe Li, Xiaolin Hu, Sen Song", "title": "Evaluate the Malignancy of Pulmonary Nodules Using the 3D Deep Leaky\n  Noisy-or Network", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TNNLS.2019.2892409", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic diagnosing lung cancer from Computed Tomography (CT) scans involves\ntwo steps: detect all suspicious lesions (pulmonary nodules) and evaluate the\nwhole-lung/pulmonary malignancy. Currently, there are many studies about the\nfirst step, but few about the second step. Since the existence of nodule does\nnot definitely indicate cancer, and the morphology of nodule has a complicated\nrelationship with cancer, the diagnosis of lung cancer demands careful\ninvestigations on every suspicious nodule and integration of information of all\nnodules. We propose a 3D deep neural network to solve this problem. The model\nconsists of two modules. The first one is a 3D region proposal network for\nnodule detection, which outputs all suspicious nodules for a subject. The\nsecond one selects the top five nodules based on the detection confidence,\nevaluates their cancer probabilities and combines them with a leaky noisy-or\ngate to obtain the probability of lung cancer for the subject. The two modules\nshare the same backbone network, a modified U-net. The over-fitting caused by\nthe shortage of training data is alleviated by training the two modules\nalternately. The proposed model won the first place in the Data Science Bowl\n2017 competition. The code has been made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 15:14:10 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Liao", "Fangzhou", ""], ["Liang", "Ming", ""], ["Li", "Zhe", ""], ["Hu", "Xiaolin", ""], ["Song", "Sen", ""]]}, {"id": "1711.08362", "submitter": "Pichao Wang", "authors": "Pichao Wang and Wanqing Li and Philip Ogunbona and Jun Wan and Sergio\n  Escalera", "title": "RGB-D-based Human Motion Recognition with Deep Learning: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion recognition is one of the most important branches of\nhuman-centered research activities. In recent years, motion recognition based\non RGB-D data has attracted much attention. Along with the development in\nartificial intelligence, deep learning techniques have gained remarkable\nsuccess in computer vision. In particular, convolutional neural networks (CNN)\nhave achieved great success for image-based tasks, and recurrent neural\nnetworks (RNN) are renowned for sequence-based problems. Specifically, deep\nlearning methods based on the CNN and RNN architectures have been adopted for\nmotion recognition using RGB-D data. In this paper, a detailed overview of\nrecent advances in RGB-D-based motion recognition is presented. The reviewed\nmethods are broadly categorized into four groups, depending on the modality\nadopted for recognition: RGB-based, depth-based, skeleton-based and\nRGB+D-based. As a survey focused on the application of deep learning to\nRGB-D-based motion recognition, we explicitly discuss the advantages and\nlimitations of existing techniques. Particularly, we highlighted the methods of\nencoding spatial-temporal-structural information inherent in video sequence,\nand discuss potential directions for future research.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 09:21:23 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 23:58:12 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Wang", "Pichao", ""], ["Li", "Wanqing", ""], ["Ogunbona", "Philip", ""], ["Wan", "Jun", ""], ["Escalera", "Sergio", ""]]}, {"id": "1711.08364", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Jose Lezama, Alex Bronstein, Guillermo Sapiro", "title": "ForestHash: Semantic Hashing With Shallow Random Forests and Tiny\n  Convolutional Networks", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash codes are efficient data representations for coping with the ever\ngrowing amounts of data. In this paper, we introduce a random forest semantic\nhashing scheme that embeds tiny convolutional neural networks (CNN) into\nshallow random forests, with near-optimal information-theoretic code\naggregation among trees. We start with a simple hashing scheme, where random\ntrees in a forest act as hashing functions by setting `1' for the visited tree\nleaf, and `0' for the rest. We show that traditional random forests fail to\ngenerate hashes that preserve the underlying similarity between the trees,\nrendering the random forests approach to hashing challenging. To address this,\nwe propose to first randomly group arriving classes at each tree split node\ninto two groups, obtaining a significantly simplified two-class classification\nproblem, which can be handled using a light-weight CNN weak learner. Such\nrandom class grouping scheme enables code uniqueness by enforcing each class to\nshare its code with different classes in different trees. A non-conventional\nlow-rank loss is further adopted for the CNN weak learners to encourage code\nconsistency by minimizing intra-class variations and maximizing inter-class\ndistance for the two random class groups. Finally, we introduce an\ninformation-theoretic approach for aggregating codes of individual trees into a\nsingle hash code, producing a near-optimal unique hash for each class. The\nproposed approach significantly outperforms state-of-the-art hashing methods\nfor image retrieval tasks on large-scale public datasets, while performing at\nthe level of other state-of-the-art image classification techniques while\nutilizing a more compact and efficient scalable representation. This work\nproposes a principled and robust procedure to train and deploy in parallel an\nensemble of light-weight CNNs, instead of simply going deeper.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 16:16:42 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 00:06:37 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Qiu", "Qiang", ""], ["Lezama", "Jose", ""], ["Bronstein", "Alex", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1711.08389", "submitter": "Bryan Plummer", "authors": "Bryan A. Plummer, Paige Kordas, M. Hadi Kiapour, Shuai Zheng, Robinson\n  Piramuthu, Svetlana Lazebnik", "title": "Conditional Image-Text Embedding Networks", "comments": "ECCV 2018 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach for grounding phrases in images which jointly\nlearns multiple text-conditioned embeddings in a single end-to-end model. In\norder to differentiate text phrases into semantically distinct subspaces, we\npropose a concept weight branch that automatically assigns phrases to\nembeddings, whereas prior works predefine such assignments. Our proposed\nsolution simplifies the representation requirements for individual embeddings\nand allows the underrepresented concepts to take advantage of the shared\nrepresentations before feeding them into concept-specific layers. Comprehensive\nexperiments verify the effectiveness of our approach across three phrase\ngrounding datasets, Flickr30K Entities, ReferIt Game, and Visual Genome, where\nwe obtain a (resp.) 4%, 3%, and 4% improvement in grounding performance over a\nstrong region-phrase embedding baseline.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 16:58:31 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 16:57:50 GMT"}, {"version": "v3", "created": "Fri, 23 Mar 2018 02:09:34 GMT"}, {"version": "v4", "created": "Sat, 28 Jul 2018 16:15:53 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Plummer", "Bryan A.", ""], ["Kordas", "Paige", ""], ["Kiapour", "M. Hadi", ""], ["Zheng", "Shuai", ""], ["Piramuthu", "Robinson", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1711.08393", "submitter": "Zuxuan Wu", "authors": "Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry S.\n  Davis, Kristen Grauman, Rogerio Feris", "title": "BlockDrop: Dynamic Inference Paths in Residual Networks", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very deep convolutional neural networks offer excellent recognition results,\nyet their computational expense limits their impact for many real-world\napplications. We introduce BlockDrop, an approach that learns to dynamically\nchoose which layers of a deep network to execute during inference so as to best\nreduce total computation without degrading prediction accuracy. Exploiting the\nrobustness of Residual Networks (ResNets) to layer dropping, our framework\nselects on-the-fly which residual blocks to evaluate for a given novel image.\nIn particular, given a pretrained ResNet, we train a policy network in an\nassociative reinforcement learning setting for the dual reward of utilizing a\nminimal number of blocks while preserving recognition accuracy. We conduct\nextensive experiments on CIFAR and ImageNet. The results provide strong\nquantitative and qualitative evidence that these learned policies not only\naccelerate inference but also encode meaningful visual information. Built upon\na ResNet-101 model, our method achieves a speedup of 20\\% on average, going as\nhigh as 36\\% for some images, while maintaining the same 76.4\\% top-1 accuracy\non ImageNet.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 17:01:59 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 15:18:01 GMT"}, {"version": "v3", "created": "Thu, 12 Apr 2018 20:46:15 GMT"}, {"version": "v4", "created": "Mon, 28 Jan 2019 16:36:44 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Wu", "Zuxuan", ""], ["Nagarajan", "Tushar", ""], ["Kumar", "Abhishek", ""], ["Rennie", "Steven", ""], ["Davis", "Larry S.", ""], ["Grauman", "Kristen", ""], ["Feris", "Rogerio", ""]]}, {"id": "1711.08413", "submitter": "Subhadip Dey", "authors": "Subhadip Dey, Sawon Pratiher, Saon Banerjee, Chanchal Kumar Mukherjee", "title": "SolarisNet: A Deep Regression Network for Solar Radiation Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective utilization of photovoltaic (PV) plants requires weather\nvariability robust global solar radiation (GSR) forecasting models. Random\nweather turbulence phenomena coupled with assumptions of clear sky model as\nsuggested by Hottel pose significant challenges to parametric & non-parametric\nmodels in GSR conversion rate estimation. Also, a decent GSR estimate requires\ncostly high-tech radiometer and expert dependent instrument handling and\nmeasurements, which are subjective. As such, a computer aided monitoring (CAM)\nsystem to evaluate PV plant operation feasibility by employing smart grid past\ndata analytics and deep learning is developed. Our algorithm, SolarisNet is a\n6-layer deep neural network trained on data collected at two weather stations\nlocated near Kalyani metrological site, West Bengal, India. The daily GSR\nprediction performance using SolarisNet outperforms the existing state of art\nand its efficacy in inferring past GSR data insights to comprehend daily and\nseasonal GSR variability along with its competence for short term forecasting\nis discussed.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 17:41:40 GMT"}, {"version": "v2", "created": "Sun, 10 Dec 2017 18:56:29 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Dey", "Subhadip", ""], ["Pratiher", "Sawon", ""], ["Banerjee", "Saon", ""], ["Mukherjee", "Chanchal Kumar", ""]]}, {"id": "1711.08447", "submitter": "Zuxuan Wu", "authors": "Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, Larry S. Davis", "title": "VITON: An Image-based Virtual Try-on Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an image-based VIirtual Try-On Network (VITON) without using 3D\ninformation in any form, which seamlessly transfers a desired clothing item\nonto the corresponding region of a person using a coarse-to-fine strategy.\nConditioned upon a new clothing-agnostic yet descriptive person representation,\nour framework first generates a coarse synthesized image with the target\nclothing item overlaid on that same person in the same pose. We further enhance\nthe initial blurry clothing area with a refinement network. The network is\ntrained to learn how much detail to utilize from the target clothing item, and\nwhere to apply to the person in order to synthesize a photo-realistic image in\nwhich the target item deforms naturally with clear visual patterns. Experiments\non our newly collected Zalando dataset demonstrate its promise in the\nimage-based virtual try-on task over state-of-the-art generative models.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 18:48:54 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 18:51:28 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2018 18:21:41 GMT"}, {"version": "v4", "created": "Tue, 12 Jun 2018 21:57:44 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Han", "Xintong", ""], ["Wu", "Zuxuan", ""], ["Wu", "Zhe", ""], ["Yu", "Ruichi", ""], ["Davis", "Larry S.", ""]]}, {"id": "1711.08456", "submitter": "Ivan Rodriguez-Montoya", "authors": "Iv\\'an Rodr\\'iguez-Montoya, David S\\'anchez-Arg\\\"uelles, Itziar\n  Aretxaga, Emanuele Bertone, Miguel Ch\\'avez-Dagostino, David H. Hughes,\n  Alfredo Monta\\~na, Grant W. Wilson, Milagros Zeballos", "title": "Multiple component decomposition from millimeter single-channel data", "comments": "Accepted in ApJS", "journal-ref": null, "doi": "10.3847/1538-4365/aaa83c", "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an implementation of a blind source separation algorithm to remove\nforegrounds off millimeter surveys made by single-channel instruments. In order\nto make possible such a decomposition over single-wavelength data: we generate\nlevels of artificial redundancy, then perform a blind decomposition, calibrate\nthe resulting maps, and lastly measure physical information. We simulate the\nreduction pipeline using mock data: atmospheric fluctuations, extended\nastrophysical foregrounds, and point-like sources, but we apply the same\nmethodology to the AzTEC/ASTE survey of the Great Observatories Origins Deep\nSurvey-South (GOODS-S). In both applications, our technique robustly decomposes\nredundant maps into their underlying components, reducing flux bias, improving\nsignal-to-noise, and minimizing information loss. In particular, the GOODS-S\nsurvey is decomposed into four independent physical components, one of them is\nthe already known map of point sources, two are atmospheric and systematic\nforegrounds, and the fourth component is an extended emission that can be\ninterpreted as the confusion background of faint sources.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 02:52:20 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Rodr\u00edguez-Montoya", "Iv\u00e1n", ""], ["S\u00e1nchez-Arg\u00fcelles", "David", ""], ["Aretxaga", "Itziar", ""], ["Bertone", "Emanuele", ""], ["Ch\u00e1vez-Dagostino", "Miguel", ""], ["Hughes", "David H.", ""], ["Monta\u00f1a", "Alfredo", ""], ["Wilson", "Grant W.", ""], ["Zeballos", "Milagros", ""]]}, {"id": "1711.08488", "submitter": "Charles Ruizhongtai Qi", "authors": "Charles R. Qi, Wei Liu, Chenxia Wu, Hao Su, Leonidas J. Guibas", "title": "Frustum PointNets for 3D Object Detection from RGB-D Data", "comments": "15 pages, 12 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study 3D object detection from RGB-D data in both indoor and\noutdoor scenes. While previous methods focus on images or 3D voxels, often\nobscuring natural 3D patterns and invariances of 3D data, we directly operate\non raw point clouds by popping up RGB-D scans. However, a key challenge of this\napproach is how to efficiently localize objects in point clouds of large-scale\nscenes (region proposal). Instead of solely relying on 3D proposals, our method\nleverages both mature 2D object detectors and advanced 3D deep learning for\nobject localization, achieving efficiency as well as high recall for even small\nobjects. Benefited from learning directly in raw point clouds, our method is\nalso able to precisely estimate 3D bounding boxes even under strong occlusion\nor with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection\nbenchmarks, our method outperforms the state of the art by remarkable margins\nwhile having real-time capability.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 19:52:18 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 00:30:24 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Qi", "Charles R.", ""], ["Liu", "Wei", ""], ["Wu", "Chenxia", ""], ["Su", "Hao", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1711.08490", "submitter": "Yu-An Chung", "authors": "Yu-An Chung and Wei-Hung Weng", "title": "Learning Deep Representations of Medical Images using Siamese CNNs with\n  Application to Content-Based Image Retrieval", "comments": "Presented in NIPS 2017 Workshop on Machine Learning for Health\n  (ML4H); add retrieval results; fix typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been investigated in learning latent\nrepresentations of medical images, yet most of the studies limit their approach\nin a single supervised convolutional neural network (CNN), which usually rely\nheavily on a large scale annotated dataset for training. To learn image\nrepresentations with less supervision involved, we propose a deep Siamese CNN\n(SCNN) architecture that can be trained with only binary image pair\ninformation. We evaluated the learned image representations on a task of\ncontent-based medical image retrieval using a publicly available multiclass\ndiabetic retinopathy fundus image dataset. The experimental results show that\nour proposed deep SCNN is comparable to the state-of-the-art single supervised\nCNN, and requires much less supervision for training.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 20:04:01 GMT"}, {"version": "v2", "created": "Wed, 27 Dec 2017 20:19:26 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Chung", "Yu-An", ""], ["Weng", "Wei-Hung", ""]]}, {"id": "1711.08496", "submitter": "Bolei Zhou", "authors": "Bolei Zhou, Alex Andonian, Aude Oliva, Antonio Torralba", "title": "Temporal Relational Reasoning in Videos", "comments": "camera-ready version for ECCV'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal relational reasoning, the ability to link meaningful transformations\nof objects or entities over time, is a fundamental property of intelligent\nspecies. In this paper, we introduce an effective and interpretable network\nmodule, the Temporal Relation Network (TRN), designed to learn and reason about\ntemporal dependencies between video frames at multiple time scales. We evaluate\nTRN-equipped networks on activity recognition tasks using three recent video\ndatasets - Something-Something, Jester, and Charades - which fundamentally\ndepend on temporal relational reasoning. Our results demonstrate that the\nproposed TRN gives convolutional neural networks a remarkable capacity to\ndiscover temporal relations in videos. Through only sparsely sampled video\nframes, TRN-equipped networks can accurately predict human-object interactions\nin the Something-Something dataset and identify various human gestures on the\nJester dataset with very competitive performance. TRN-equipped networks also\noutperform two-stream networks and 3D convolution networks in recognizing daily\nactivities in the Charades dataset. Further analyses show that the models learn\nintuitive and interpretable visual common sense knowledge in videos.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 20:31:19 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 03:03:32 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Zhou", "Bolei", ""], ["Andonian", "Alex", ""], ["Oliva", "Aude", ""], ["Torralba", "Antonio", ""]]}, {"id": "1711.08502", "submitter": "Tae Soo Kim", "authors": "Jingxuan Hou, Tae Soo Kim, Austin Reiter", "title": "Train, Diagnose and Fix: Interpretable Approach for Fine-grained Action\n  Recognition", "comments": "8 pages, 8 figures, CVPR18 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing discriminative capabilities of modern deep learning\nmethods for recognition tasks, the inner workings of the state-of-art models\nstill remain mostly black-boxes. In this paper, we propose a systematic\ninterpretation of model parameters and hidden representations of Residual\nTemporal Convolutional Networks (Res-TCN) for action recognition in time-series\ndata. We also propose a Feature Map Decoder as part of the interpretation\nanalysis, which outputs a representation of model's hidden variables in the\nsame domain as the input. Such analysis empowers us to expose model's\ncharacteristic learning patterns in an interpretable way. For example, through\nthe diagnosis analysis, we discovered that our model has learned to achieve\nview-point invariance by implicitly learning to perform rotational\nnormalization of the input to a more discriminative view. Based on the findings\nfrom the model interpretation analysis, we propose a targeted refinement\ntechnique, which can generalize to various other recognition models. The\nproposed work introduces a three-stage paradigm for model learning: training,\ninterpretable diagnosis and targeted refinement. We validate our approach on\nskeleton based 3D human action recognition benchmark of NTU RGB+D. We show that\nthe proposed workflow is an effective model learning strategy and the resulting\nMulti-stream Residual Temporal Convolutional Network (MS-Res-TCN) achieves the\nstate-of-the-art performance on NTU RGB+D.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 20:51:32 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Hou", "Jingxuan", ""], ["Kim", "Tae Soo", ""], ["Reiter", "Austin", ""]]}, {"id": "1711.08506", "submitter": "Xide Xia", "authors": "Xide Xia, Brian Kulis", "title": "W-Net: A Deep Model for Fully Unsupervised Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While significant attention has been recently focused on designing supervised\ndeep semantic segmentation algorithms for vision tasks, there are many domains\nin which sufficient supervised pixel-level labels are difficult to obtain. In\nthis paper, we revisit the problem of purely unsupervised image segmentation\nand propose a novel deep architecture for this problem. We borrow recent ideas\nfrom supervised semantic segmentation methods, in particular by concatenating\ntwo fully convolutional networks together into an autoencoder--one for encoding\nand one for decoding. The encoding layer produces a k-way pixelwise prediction,\nand both the reconstruction error of the autoencoder as well as the normalized\ncut produced by the encoder are jointly minimized during training. When\ncombined with suitable postprocessing involving conditional random field\nsmoothing and hierarchical segmentation, our resulting algorithm achieves\nimpressive results on the benchmark Berkeley Segmentation Data Set,\noutperforming a number of competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 21:06:13 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Xia", "Xide", ""], ["Kulis", "Brian", ""]]}, {"id": "1711.08561", "submitter": "Riccardo Volpi", "authors": "Riccardo Volpi, Pietro Morerio, Silvio Savarese, Vittorio Murino", "title": "Adversarial Feature Augmentation for Unsupervised Domain Adaptation", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works showed that Generative Adversarial Networks (GANs) can be\nsuccessfully applied in unsupervised domain adaptation, where, given a labeled\nsource dataset and an unlabeled target dataset, the goal is to train powerful\nclassifiers for the target samples. In particular, it was shown that a GAN\nobjective function can be used to learn target features indistinguishable from\nthe source ones. In this work, we extend this framework by (i) forcing the\nlearned feature extractor to be domain-invariant, and (ii) training it through\ndata augmentation in the feature space, namely performing feature augmentation.\nWhile data augmentation in the image space is a well established technique in\ndeep learning, feature augmentation has not yet received the same level of\nattention. We accomplish it by means of a feature generator trained by playing\nthe GAN minimax game against source features. Results show that both enforcing\ndomain-invariance and performing feature augmentation lead to superior or\ncomparable performance to state-of-the-art results in several unsupervised\ndomain adaptation benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 03:17:11 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 11:03:09 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Volpi", "Riccardo", ""], ["Morerio", "Pietro", ""], ["Savarese", "Silvio", ""], ["Murino", "Vittorio", ""]]}, {"id": "1711.08565", "submitter": "Longhui Wei", "authors": "Longhui Wei, Shiliang Zhang, Wen Gao, Qi Tian", "title": "Person Transfer GAN to Bridge Domain Gap for Person Re-Identification", "comments": "10 pages, 9 figures; accepted in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the performance of person Re-Identification (ReID) has been\nsignificantly boosted, many challenging issues in real scenarios have not been\nfully investigated, e.g., the complex scenes and lighting variations, viewpoint\nand pose changes, and the large number of identities in a camera network. To\nfacilitate the research towards conquering those issues, this paper contributes\na new dataset called MSMT17 with many important features, e.g., 1) the raw\nvideos are taken by an 15-camera network deployed in both indoor and outdoor\nscenes, 2) the videos cover a long period of time and present complex lighting\nvariations, and 3) it contains currently the largest number of annotated\nidentities, i.e., 4,101 identities and 126,441 bounding boxes. We also observe\nthat, domain gap commonly exists between datasets, which essentially causes\nsevere performance drop when training and testing on different datasets. This\nresults in that available training data cannot be effectively leveraged for new\ntesting domains. To relieve the expensive costs of annotating new training\nsamples, we propose a Person Transfer Generative Adversarial Network (PTGAN) to\nbridge the domain gap. Comprehensive experiments show that the domain gap could\nbe substantially narrowed-down by the PTGAN.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 03:23:21 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 12:50:58 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Wei", "Longhui", ""], ["Zhang", "Shiliang", ""], ["Gao", "Wen", ""], ["Tian", "Qi", ""]]}, {"id": "1711.08569", "submitter": "Christopher Tralie", "authors": "Christopher J. Tralie, Abraham Smith, Nathan Borggren, Jay Hineman,\n  Paul Bendich, Peter Zulch, John Harer", "title": "Geometric Cross-Modal Comparison of Heterogeneous Sensor Data", "comments": "10 pages, 13 figures, Proceedings of IEEE Aeroconf 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of cross-modal comparison of aerial data\nstreams. A variety of simulated automobile trajectories are sensed using two\ndifferent modalities: full-motion video, and radio-frequency (RF) signals\nreceived by detectors at various locations. The information represented by the\ntwo modalities is compared using self-similarity matrices (SSMs) corresponding\nto time-ordered point clouds in feature spaces of each of these data sources;\nwe note that these feature spaces can be of entirely different scale and\ndimensionality. Several metrics for comparing SSMs are explored, including a\ncutting-edge time-warping technique that can simultaneously handle local time\nwarping and partial matches, while also controlling for the change in geometry\nbetween feature spaces of the two modalities. We note that this technique is\nquite general, and does not depend on the choice of modalities. In this\nparticular setting, we demonstrate that the cross-modal distance between SSMs\ncorresponding to the same trajectory type is smaller than the cross-modal\ndistance between SSMs corresponding to distinct trajectory types, and we\nformalize this observation via precision-recall metrics in experiments.\nFinally, we comment on promising implications of these ideas for future\nintegration into multiple-hypothesis tracking systems.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 03:55:41 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Tralie", "Christopher J.", ""], ["Smith", "Abraham", ""], ["Borggren", "Nathan", ""], ["Hineman", "Jay", ""], ["Bendich", "Paul", ""], ["Zulch", "Peter", ""], ["Harer", "John", ""]]}, {"id": "1711.08580", "submitter": "Siqi Liu", "authors": "Siqi Liu, Daguang Xu, S. Kevin Zhou, Thomas Mertelmeier, Julia\n  Wicklein, Anna Jerebko, Sasa Grbic, Olivier Pauly, Weidong Cai, Dorin\n  Comaniciu", "title": "3D Anisotropic Hybrid Network: Transferring Convolutional Features from\n  2D Images to 3D Anisotropic Volumes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep convolutional neural networks (CNN) have been successfully applied\nfor 2D image analysis, it is still challenging to apply them to 3D anisotropic\nvolumes, especially when the within-slice resolution is much higher than the\nbetween-slice resolution and when the amount of 3D volumes is relatively small.\nOn one hand, direct learning of CNN with 3D convolution kernels suffers from\nthe lack of data and likely ends up with poor generalization; insufficient GPU\nmemory limits the model size or representational power. On the other hand,\napplying 2D CNN with generalizable features to 2D slices ignores between-slice\ninformation. Coupling 2D network with LSTM to further handle the between-slice\ninformation is not optimal due to the difficulty in LSTM learning. To overcome\nthe above challenges, we propose a 3D Anisotropic Hybrid Network (AH-Net) that\ntransfers convolutional features learned from 2D images to 3D anisotropic\nvolumes. Such a transfer inherits the desired strong generalization capability\nfor within-slice information while naturally exploiting between-slice\ninformation for more effective modelling. The focal loss is further utilized\nfor more effective end-to-end learning. We experiment with the proposed 3D\nAH-Net on two different medical image analysis tasks, namely lesion detection\nfrom a Digital Breast Tomosynthesis volume, and liver and liver tumor\nsegmentation from a Computed Tomography volume and obtain the state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 05:30:08 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 01:54:49 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Liu", "Siqi", ""], ["Xu", "Daguang", ""], ["Zhou", "S. Kevin", ""], ["Mertelmeier", "Thomas", ""], ["Wicklein", "Julia", ""], ["Jerebko", "Anna", ""], ["Grbic", "Sasa", ""], ["Pauly", "Olivier", ""], ["Cai", "Weidong", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "1711.08585", "submitter": "Mir Rayat Imtiaz Hossain", "authors": "Mir Rayat Imtiaz Hossain, James J. Little", "title": "Exploiting temporal information for 3D pose estimation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-01249-6_5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of 3D human pose estimation from a\nsequence of 2D human poses. Although the recent success of deep networks has\nled many state-of-the-art methods for 3D pose estimation to train deep networks\nend-to-end to predict from images directly, the top-performing approaches have\nshown the effectiveness of dividing the task of 3D pose estimation into two\nsteps: using a state-of-the-art 2D pose estimator to estimate the 2D pose from\nimages and then mapping them into 3D space. They also showed that a\nlow-dimensional representation like 2D locations of a set of joints can be\ndiscriminative enough to estimate 3D pose with high accuracy. However,\nestimation of 3D pose for individual frames leads to temporally incoherent\nestimates due to independent error in each frame causing jitter. Therefore, in\nthis work we utilize the temporal information across a sequence of 2D joint\nlocations to estimate a sequence of 3D poses. We designed a\nsequence-to-sequence network composed of layer-normalized LSTM units with\nshortcut connections connecting the input to the output on the decoder side and\nimposed temporal smoothness constraint during training. We found that the\nknowledge of temporal consistency improves the best reported result on\nHuman3.6M dataset by approximately $12.2\\%$ and helps our network to recover\ntemporally consistent 3D poses over a sequence of images even when the 2D pose\ndetector fails.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 06:20:51 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 04:49:22 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2018 13:54:34 GMT"}, {"version": "v4", "created": "Wed, 12 Sep 2018 05:15:11 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Hossain", "Mir Rayat Imtiaz", ""], ["Little", "James J.", ""]]}, {"id": "1711.08588", "submitter": "Weiyue Wang", "authors": "Weiyue Wang, Ronald Yu, Qiangui Huang, Ulrich Neumann", "title": "SGPN: Similarity Group Proposal Network for 3D Point Cloud Instance\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Similarity Group Proposal Network (SGPN), a simple and intuitive\ndeep learning framework for 3D object instance segmentation on point clouds.\nSGPN uses a single network to predict point grouping proposals and a\ncorresponding semantic class for each proposal, from which we can directly\nextract instance segmentation results. Important to the effectiveness of SGPN\nis its novel representation of 3D instance segmentation results in the form of\na similarity matrix that indicates the similarity between each pair of points\nin embedded feature space, thus producing an accurate grouping proposal for\neach point. To the best of our knowledge, SGPN is the first framework to learn\n3D instance-aware semantic segmentation on point clouds. Experimental results\non various 3D scenes show the effectiveness of our method on 3D instance\nsegmentation, and we also evaluate the capability of SGPN to improve 3D object\ndetection and semantic segmentation results. We also demonstrate its\nflexibility by seamlessly incorporating 2D CNN features into the framework to\nboost performance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 06:37:45 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 23:10:56 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Wang", "Weiyue", ""], ["Yu", "Ronald", ""], ["Huang", "Qiangui", ""], ["Neumann", "Ulrich", ""]]}, {"id": "1711.08589", "submitter": "Benjamin Klein", "authors": "Benjamin Klein and Lior Wolf", "title": "End-to-End Supervised Product Quantization for Image Search and\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product Quantization, a dictionary based hashing method, is one of the\nleading unsupervised hashing techniques. While it ignores the labels, it\nharnesses the features to construct look up tables that can approximate the\nfeature space. In recent years, several works have achieved state of the art\nresults on hashing benchmarks by learning binary representations in a\nsupervised manner. This work presents Deep Product Quantization (DPQ), a\ntechnique that leads to more accurate retrieval and classification than the\nlatest state of the art methods, while having similar computational complexity\nand memory footprint as the Product Quantization method. To our knowledge, this\nis the first work to introduce a dictionary-based representation that is\ninspired by Product Quantization and which is learned end-to-end, and thus\nbenefits from the supervised signal. DPQ explicitly learns soft and hard\nrepresentations to enable an efficient and accurate asymmetric search, by using\na straight-through estimator. Our method obtains state of the art results on an\nextensive array of retrieval and classification experiments.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 06:40:28 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 22:56:50 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Klein", "Benjamin", ""], ["Wolf", "Lior", ""]]}, {"id": "1711.08590", "submitter": "Yuhang Song", "authors": "Yuhang Song, Chao Yang, Zhe Lin, Xiaofeng Liu, Qin Huang, Hao Li,\n  C.-C. Jay Kuo", "title": "Contextual-based Image Inpainting: Infer, Match, and Translate", "comments": "ECCV 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of image inpainting, which is to fill in the missing region\nof an incomplete image with plausible contents. To this end, we propose a\nlearning-based approach to generate visually coherent completion given a\nhigh-resolution image with missing components. In order to overcome the\ndifficulty to directly learn the distribution of high-dimensional image data,\nwe divide the task into inference and translation as two separate steps and\nmodel each step with a deep neural network. We also use simple heuristics to\nguide the propagation of local textures from the boundary to the hole. We show\nthat, by using such techniques, inpainting reduces to the problem of learning\ntwo image-feature translation functions in much smaller space and hence easier\nto train. We evaluate our method on several public datasets and show that we\ngenerate results of better visual quality than previous state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 06:44:43 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 22:31:14 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2018 23:57:20 GMT"}, {"version": "v4", "created": "Sun, 8 Jul 2018 03:57:51 GMT"}, {"version": "v5", "created": "Wed, 25 Jul 2018 19:00:08 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Song", "Yuhang", ""], ["Yang", "Chao", ""], ["Lin", "Zhe", ""], ["Liu", "Xiaofeng", ""], ["Huang", "Qin", ""], ["Li", "Hao", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1711.08591", "submitter": "Salman Khan Dr.", "authors": "Salman Khan, Munawar Hayat, Fatih Porikli", "title": "Regularization of Deep Neural Networks with Spectral Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The big breakthrough on the ImageNet challenge in 2012 was partially due to\nthe `dropout' technique used to avoid overfitting. Here, we introduce a new\napproach called `Spectral Dropout' to improve the generalization ability of\ndeep neural networks. We cast the proposed approach in the form of regular\nConvolutional Neural Network (CNN) weight layers using a decorrelation\ntransform with fixed basis functions. Our spectral dropout method prevents\noverfitting by eliminating weak and `noisy' Fourier domain coefficients of the\nneural network activations, leading to remarkably better results than the\ncurrent regularization methods. Furthermore, the proposed is very efficient due\nto the fixed basis functions used for spectral transformation. In particular,\ncompared to Dropout and Drop-Connect, our method significantly speeds up the\nnetwork convergence rate during the training process (roughly x2), with\nconsiderably higher neuron pruning rates (an increase of ~ 30%). We demonstrate\nthat the spectral dropout can also be used in conjunction with other\nregularization approaches resulting in additional performance gains.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 06:47:37 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Khan", "Salman", ""], ["Hayat", "Munawar", ""], ["Porikli", "Fatih", ""]]}, {"id": "1711.08608", "submitter": "Siyuan Shan", "authors": "Siyuan Shan, Wen Yan, Xiaoqing Guo, Eric I-Chao Chang, Yubo Fan and\n  Yan Xu", "title": "Unsupervised End-to-end Learning for Deformable Medical Image\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a registration algorithm for 2D CT/MRI medical images with a new\nunsupervised end-to-end strategy using convolutional neural networks. The\ncontributions of our algorithm are threefold: (1) We transplant traditional\nimage registration algorithms to an end-to-end convolutional neural network\nframework, while maintaining the unsupervised nature of image registration\nproblems. The image-to-image integrated framework can simultaneously learn both\nimage features and transformation matrix for registration. (2) Training with\nadditional data without any label can further improve the registration\nperformance by approximately 10 %. (3) The registration speed is 100x faster\nthan traditional methods. The proposed network is easy to implement and can be\ntrained efficiently. Experiments demonstrate that our system achieves\nstate-of-the-art results on 2D brain registration and achieves comparable\nresults on 2D liver registration. It can be extended to register other organs\nbeyond liver and brain such as kidney, lung, and heart.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 08:20:49 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 02:48:31 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Shan", "Siyuan", ""], ["Yan", "Wen", ""], ["Guo", "Xiaoqing", ""], ["Chang", "Eric I-Chao", ""], ["Fan", "Yubo", ""], ["Xu", "Yan", ""]]}, {"id": "1711.08624", "submitter": "Xin Fan", "authors": "Xin Fan, Risheng Liu, Kang Huyan, Yuyao Feng, Zhongxuan Luo", "title": "Self-Reinforced Cascaded Regression for Face Alignment", "comments": "8 pages, 11 figures, AAAI 18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascaded regression is prevailing in face alignment thanks to its accuracy\nand robustness, but typically demands manually annotated examples having low\ndiscrepancy between shape-indexed features and shape updates. In this paper, we\npropose a self-reinforced strategy that iteratively expands the quantity and\nimproves the quality of training examples, thus upgrading the performance of\ncascaded regression itself. The reinforced term evaluates the example quality\nupon the consistence on both local appearance and global geometry of human\nfaces, and constitutes the example evolution by the philosophy of \"survival of\nthe fittest\". We train a set of discriminative classifiers, each associated\nwith one landmark label, to prune those examples with inconsistent local\nappearance, and further validate the geometric relationship among groups of\nlabeled landmarks against the common global geometry derived from a projective\ninvariant. We embed this generic strategy into typical cascaded regressions,\nand the alignment results on several benchmark data sets demonstrate its\neffectiveness to predict good examples starting from a small subset.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 09:15:22 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Fan", "Xin", ""], ["Liu", "Risheng", ""], ["Huyan", "Kang", ""], ["Feng", "Yuyao", ""], ["Luo", "Zhongxuan", ""]]}, {"id": "1711.08654", "submitter": "Xingxing Zuo", "authors": "Xingxing Zuo, Xiaojia Xie, Yong Liu, Guoquan Huang", "title": "Robust Visual SLAM with Point and Line Features", "comments": "8 pages, Conference paper, IROS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a robust efficient visual SLAM system that utilizes\nheterogeneous point and line features. By leveraging ORB-SLAM [1], the proposed\nsystem consists of stereo matching, frame tracking, local mapping, loop\ndetection, and bundle adjustment of both point and line features. In\nparticular, as the main theoretical contributions of this paper, we, for the\nfirst time, employ the orthonormal representation as the minimal\nparameterization to model line features along with point features in visual\nSLAM and analytically derive the Jacobians of the re-projection errors with\nrespect to the line parameters, which significantly improves the SLAM solution.\nThe proposed SLAM has been extensively tested in both synthetic and real-world\nexperiments whose results demonstrate that the proposed system outperforms the\nstate-of-the-art methods in various scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 11:23:22 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Zuo", "Xingxing", ""], ["Xie", "Xiaojia", ""], ["Liu", "Yong", ""], ["Huang", "Guoquan", ""]]}, {"id": "1711.08664", "submitter": "Shih-Han Chou", "authors": "Shih-Han Chou, Yi-Chun Chen, Kuo-Hao Zeng, Hou-Ning Hu, Jianlong Fu,\n  Min Sun", "title": "Self-view Grounding Given a Narrated 360{\\deg} Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Narrated 360{\\deg} videos are typically provided in many touring scenarios to\nmimic real-world experience. However, previous work has shown that smart\nassistance (i.e., providing visual guidance) can significantly help users to\nfollow the Normal Field of View (NFoV) corresponding to the narrative. In this\nproject, we aim at automatically grounding the NFoVs of a 360{\\deg} video given\nsubtitles of the narrative (referred to as \"NFoV-grounding\"). We propose a\nnovel Visual Grounding Model (VGM) to implicitly and efficiently predict the\nNFoVs given the video content and subtitles. Specifically, at each frame, we\nefficiently encode the panorama into feature map of candidate NFoVs using a\nConvolutional Neural Network (CNN) and the subtitles to the same hidden space\nusing an RNN with Gated Recurrent Units (GRU). Then, we apply soft-attention on\ncandidate NFoVs to trigger sentence decoder aiming to minimize the reconstruct\nloss between the generated and given sentence. Finally, we obtain the NFoV as\nthe candidate NFoV with the maximum attention without any human supervision. To\ntrain VGM more robustly, we also generate a reverse sentence conditioning on\none minus the soft-attention such that the attention focuses on candidate NFoVs\nless relevant to the given sentence. The negative log reconstruction loss of\nthe reverse sentence (referred to as \"irrelevant loss\") is jointly minimized to\nencourage the reverse sentence to be different from the given sentence. To\nevaluate our method, we collect the first narrated 360{\\deg} videos dataset and\nachieve state-of-the-art NFoV-grounding performance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 12:06:20 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Chou", "Shih-Han", ""], ["Chen", "Yi-Chun", ""], ["Zeng", "Kuo-Hao", ""], ["Hu", "Hou-Ning", ""], ["Fu", "Jianlong", ""], ["Sun", "Min", ""]]}, {"id": "1711.08681", "submitter": "Nicolas Audebert", "authors": "Nicolas Audebert (OBELIX, Palaiseau), Bertrand Le Saux (Palaiseau),\n  S\\'ebastien Lef\\`evre (OBELIX)", "title": "Beyond RGB: Very High Resolution Urban Remote Sensing With Multimodal\n  Deep Networks", "comments": "ISPRS Journal of Photogrammetry and Remote Sensing, Elsevier, A\n  Para{\\^i}tre", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate various methods to deal with semantic labeling\nof very high resolution multi-modal remote sensing data. Especially, we study\nhow deep fully convolutional networks can be adapted to deal with multi-modal\nand multi-scale remote sensing data for semantic labeling. Our contributions\nare threefold: a) we present an efficient multi-scale approach to leverage both\na large spatial context and the high resolution data, b) we investigate early\nand late fusion of Lidar and multispectral data, c) we validate our methods on\ntwo public datasets with state-of-the-art results. Our results indicate that\nlate fusion make it possible to recover errors steaming from ambiguous data,\nwhile early fusion allows for better joint-feature learning but at the cost of\nhigher sensitivity to missing data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 13:10:24 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Audebert", "Nicolas", "", "OBELIX, Palaiseau"], ["Saux", "Bertrand Le", "", "Palaiseau"], ["Lef\u00e8vre", "S\u00e9bastien", "", "OBELIX"]]}, {"id": "1711.08682", "submitter": "Chunyan Bai", "authors": "Haoye Cai, Chunyan Bai, Yu-Wing Tai, Chi-Keung Tang", "title": "Deep Video Generation, Prediction and Completion of Human Action\n  Sequences", "comments": "Under review for CVPR 2018. Haoye and Chunyan have equal contribution", "journal-ref": null, "doi": "10.1007/978-3-030-01216-8_23", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning results on video generation are limited while there are\nonly a few first results on video prediction and no relevant significant\nresults on video completion. This is due to the severe ill-posedness inherent\nin these three problems. In this paper, we focus on human action videos, and\npropose a general, two-stage deep framework to generate human action videos\nwith no constraints or arbitrary number of constraints, which uniformly address\nthe three problems: video generation given no input frames, video prediction\ngiven the first few frames, and video completion given the first and last\nframes. To make the problem tractable, in the first stage we train a deep\ngenerative model that generates a human pose sequence from random noise. In the\nsecond stage, a skeleton-to-image network is trained, which is used to generate\na human action video given the complete human pose sequence generated in the\nfirst stage. By introducing the two-stage strategy, we sidestep the original\nill-posed problems while producing for the first time high-quality video\ngeneration/prediction/completion results of much longer duration. We present\nquantitative and qualitative evaluation to show that our two-stage approach\noutperforms state-of-the-art methods in video generation, prediction and video\ncompletion. Our video result demonstration can be viewed at\nhttps://iamacewhite.github.io/supp/index.html\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 13:10:34 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 04:48:27 GMT"}, {"version": "v3", "created": "Fri, 8 Dec 2017 12:17:15 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Cai", "Haoye", ""], ["Bai", "Chunyan", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1711.08690", "submitter": "Wenjie Pei", "authors": "Wenjie Pei and Hamdi Dibeklio\\u{g}lu and Tadas Baltru\\v{s}aitis and\n  David M.J. Tax", "title": "Attended End-to-end Architecture for Age Estimation from Facial\n  Expression Videos", "comments": "Accepted by Transactions on Image Processing (TIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenges of age estimation from facial expression videos lie not\nonly in the modeling of the static facial appearance, but also in the capturing\nof the temporal facial dynamics. Traditional techniques to this problem focus\non constructing handcrafted features to explore the discriminative information\ncontained in facial appearance and dynamics separately. This relies on\nsophisticated feature-refinement and framework-design. In this paper, we\npresent an end-to-end architecture for age estimation, called Spatially-Indexed\nAttention Model (SIAM), which is able to simultaneously learn both the\nappearance and dynamics of age from raw videos of facial expressions.\nSpecifically, we employ convolutional neural networks to extract effective\nlatent appearance representations and feed them into recurrent networks to\nmodel the temporal dynamics. More importantly, we propose to leverage attention\nmodels for salience detection in both the spatial domain for each single image\nand the temporal domain for the whole video as well. We design a specific\nspatially-indexed attention mechanism among the convolutional layers to extract\nthe salient facial regions in each individual image, and a temporal attention\nlayer to assign attention weights to each frame. This two-pronged approach not\nonly improves the performance by allowing the model to focus on informative\nframes and facial areas, but it also offers an interpretable correspondence\nbetween the spatial facial regions as well as temporal frames, and the task of\nage estimation. We demonstrate the strong performance of our model in\nexperiments on a large, gender-balanced database with 400 subjects with ages\nspanning from 8 to 76 years. Experiments reveal that our model exhibits\nsignificant superiority over the state-of-the-art methods given sufficient\ntraining data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 13:43:49 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 15:46:37 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Pei", "Wenjie", ""], ["Dibeklio\u011flu", "Hamdi", ""], ["Baltru\u0161aitis", "Tadas", ""], ["Tax", "David M. J.", ""]]}, {"id": "1711.08716", "submitter": "Alexandre B\\^one", "authors": "Alexandre B\\^one, Maxime Louis, Alexandre Routier, Jorge Samper,\n  Michael Bacci, Benjamin Charlier, Olivier Colliot, Stanley Durrleman", "title": "Prediction of the progression of subcortical brain structures in\n  Alzheimer's disease from baseline", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-67675-3_10", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to predict the subject-specific longitudinal progression\nof brain structures extracted from baseline MRI, and evaluate its performance\non Alzheimer's disease data. The disease progression is modeled as a trajectory\non a group of diffeomorphisms in the context of large deformation diffeomorphic\nmetric mapping (LDDMM). We first exhibit the limited predictive abilities of\ngeodesic regression extrapolation on this group. Building on the recent concept\nof parallel curves in shape manifolds, we then introduce a second predictive\nprotocol which personalizes previously learned trajectories to new subjects,\nand investigate the relative performances of two parallel shifting paradigms.\nThis design only requires the baseline imaging data. Finally, coefficients\nencoding the disease dynamics are obtained from longitudinal cognitive\nmeasurements for each subject, and exploited to refine our methodology which is\ndemonstrated to successfully predict the follow-up visits.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 14:47:42 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["B\u00f4ne", "Alexandre", ""], ["Louis", "Maxime", ""], ["Routier", "Alexandre", ""], ["Samper", "Jorge", ""], ["Bacci", "Michael", ""], ["Charlier", "Benjamin", ""], ["Colliot", "Olivier", ""], ["Durrleman", "Stanley", ""]]}, {"id": "1711.08725", "submitter": "Alexandre B\\^one", "authors": "Maxime Louis, Alexandre B\\^one, Benjamin Charlier, Stanley Durrleman", "title": "Parallel transport in shape analysis: a scalable numerical scheme", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-68445-1_4", "report-no": null, "categories": "cs.CV math.DG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of manifold-valued data requires efficient tools from Riemannian\ngeometry to cope with the computational complexity at stake. This complexity\narises from the always-increasing dimension of the data, and the absence of\nclosed-form expressions to basic operations such as the Riemannian logarithm.\nIn this paper, we adapt a generic numerical scheme recently introduced for\ncomputing parallel transport along geodesics in a Riemannian manifold to\nfinite-dimensional manifolds of diffeomorphisms. We provide a qualitative and\nquantitative analysis of its behavior on high-dimensional manifolds, and\ninvestigate an application with the prediction of brain structures progression.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 14:57:32 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Louis", "Maxime", ""], ["B\u00f4ne", "Alexandre", ""], ["Charlier", "Benjamin", ""], ["Durrleman", "Stanley", ""]]}, {"id": "1711.08740", "submitter": "Stylianos Venieris", "authors": "Stylianos I. Venieris and Christos-Savvas Bouganis", "title": "fpgaConvNet: A Toolflow for Mapping Diverse Convolutional Neural\n  Networks on Embedded FPGAs", "comments": "Accepted at NIPS 2017 Workshop on Machine Learning on the Phone and\n  other Consumer Devices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Convolutional Neural Networks (ConvNets) have become an\nenabling technology for a wide range of novel embedded Artificial Intelligence\nsystems. Across the range of applications, the performance needs vary\nsignificantly, from high-throughput video surveillance to the very low-latency\nrequirements of autonomous cars. In this context, FPGAs can provide a potential\nplatform that can be optimally configured based on the different performance\nneeds. However, the complexity of ConvNet models keeps increasing making their\nmapping to an FPGA device a challenging task. This work presents fpgaConvNet,\nan end-to-end framework for mapping ConvNets on FPGAs. The proposed framework\nemploys an automated design methodology based on the Synchronous Dataflow (SDF)\nparadigm and defines a set of SDF transformations in order to efficiently\nexplore the architectural design space. By selectively optimising for\nthroughput, latency or multiobjective criteria, the presented tool is able to\nefficiently explore the design space and generate hardware designs from\nhigh-level ConvNet specifications, explicitly optimised for the performance\nmetric of interest. Overall, our framework yields designs that improve the\nperformance by up to 6.65x over highly optimised embedded GPU designs for the\nsame power constraints in embedded environments.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 15:37:21 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Venieris", "Stylianos I.", ""], ["Bouganis", "Christos-Savvas", ""]]}, {"id": "1711.08757", "submitter": "Girish Varma", "authors": "Ameya Prabhu, Girish Varma, Anoop Namboodiri", "title": "Deep Expander Networks: Efficient Deep Networks from Graph Theory", "comments": "ECCV'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient CNN designs like ResNets and DenseNet were proposed to improve\naccuracy vs efficiency trade-offs. They essentially increased the connectivity,\nallowing efficient information flow across layers. Inspired by these\ntechniques, we propose to model connections between filters of a CNN using\ngraphs which are simultaneously sparse and well connected. Sparsity results in\nefficiency while well connectedness can preserve the expressive power of the\nCNNs. We use a well-studied class of graphs from theoretical computer science\nthat satisfies these properties known as Expander graphs. Expander graphs are\nused to model connections between filters in CNNs to design networks called\nX-Nets. We present two guarantees on the connectivity of X-Nets: Each node\ninfluences every node in a layer in logarithmic steps, and the number of paths\nbetween two sets of nodes is proportional to the product of their sizes. We\nalso propose efficient training and inference algorithms, making it possible to\ntrain deeper and wider X-Nets effectively.\n  Expander based models give a 4% improvement in accuracy on MobileNet over\ngrouped convolutions, a popular technique, which has the same sparsity but\nworse connectivity. X-Nets give better performance trade-offs than the original\nResNet and DenseNet-BC architectures. We achieve model sizes comparable to\nstate-of-the-art pruning techniques using our simple architecture design,\nwithout any pruning. We hope that this work motivates other approaches to\nutilize results from graph theory to develop efficient network architectures.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 16:16:04 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 08:41:56 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 07:31:24 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Prabhu", "Ameya", ""], ["Varma", "Girish", ""], ["Namboodiri", "Anoop", ""]]}, {"id": "1711.08760", "submitter": "Muktabh Mayank Srivastava", "authors": "Pulkit Kumar, Monika Grewal, Muktabh Mayank Srivastava", "title": "Boosted Cascaded Convnets for Multilabel Classification of Thoracic\n  Diseases in Chest Radiographs", "comments": "Submitted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Chest X-ray is one of the most accessible medical imaging technique for\ndiagnosis of multiple diseases. With the availability of ChestX-ray14, which is\na massive dataset of chest X-ray images and provides annotations for 14\nthoracic diseases; it is possible to train Deep Convolutional Neural Networks\n(DCNN) to build Computer Aided Diagnosis (CAD) systems. In this work, we\nexperiment a set of deep learning models and present a cascaded deep neural\nnetwork that can diagnose all 14 pathologies better than the baseline and is\ncompetitive with other published methods. Our work provides the quantitative\nresults to answer following research questions for the dataset: 1) What loss\nfunctions to use for training DCNN from scratch on ChestX-ray14 dataset that\ndemonstrates high class imbalance and label co occurrence? 2) How to use\ncascading to model label dependency and to improve accuracy of the deep\nlearning model?\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 16:25:29 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Kumar", "Pulkit", ""], ["Grewal", "Monika", ""], ["Srivastava", "Muktabh Mayank", ""]]}, {"id": "1711.08762", "submitter": "Eli (Omid) David", "authors": "Dror Sholomon, Eli David, Nathan S. Netanyahu", "title": "DNN-Buddies: A Deep Neural Network-Based Estimation Metric for the\n  Jigsaw Puzzle Problem", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 9887, pp. 170-178, Barcelona, Spain, September 2016", "doi": "10.1007/978-3-319-44781-0_21", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the first deep neural network-based estimation metric\nfor the jigsaw puzzle problem. Given two puzzle piece edges, the neural network\npredicts whether or not they should be adjacent in the correct assembly of the\npuzzle, using nothing but the pixels of each piece. The proposed metric\nexhibits an extremely high precision even though no manual feature extraction\nis performed. When incorporated into an existing puzzle solver, the solution's\naccuracy increases significantly, achieving thereby a new state-of-the-art\nstandard.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 16:32:57 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Sholomon", "Dror", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.08763", "submitter": "Eli (Omid) David", "authors": "Eli David, Nathan S. Netanyahu", "title": "DeepPainter: Painter Classification Using Deep Convolutional\n  Autoencoders", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 9887, pp. 20-28, Barcelona, Spain, September 2016", "doi": "10.1007/978-3-319-44781-0_3", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the problem of painter classification, and propose\na novel approach based on deep convolutional autoencoder neural networks. While\nprevious approaches relied on image processing and manual feature extraction\nfrom paintings, our approach operates on the raw pixel level, without any\npreprocessing or manual feature extraction. We first train a deep convolutional\nautoencoder on a dataset of paintings, and subsequently use it to initialize a\nsupervised convolutional neural network for the classification phase.\n  The proposed approach substantially outperforms previous methods, improving\nthe previous state-of-the-art for the 3-painter classification problem from\n90.44% accuracy (previous state-of-the-art) to 96.52% accuracy, i.e., a 63%\nreduction in error rate.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 16:36:28 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1711.08766", "submitter": "Yu Liu", "authors": "Guanglu Song, Biao Leng, Yu Liu, Congrui Hetang, Shaofan Cai", "title": "Region-based Quality Estimation Network for Large-scale Person\n  Re-identification", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major restrictions on the performance of video-based person re-id\nis partial noise caused by occlusion, blur and illumination. Since different\nspatial regions of a single frame have various quality, and the quality of the\nsame region also varies across frames in a tracklet, a good way to address the\nproblem is to effectively aggregate complementary information from all frames\nin a sequence, using better regions from other frames to compensate the\ninfluence of an image region with poor quality. To achieve this, we propose a\nnovel Region-based Quality Estimation Network (RQEN), in which an ingenious\ntraining mechanism enables the effective learning to extract the complementary\nregion-based information between different frames. Compared with other feature\nextraction methods, we achieved comparable results of 92.4%, 76.1% and 77.83%\non the PRID 2011, iLIDS-VID and MARS, respectively. In addition, to alleviate\nthe lack of clean large-scale person re-id datasets for the community, this\npaper also contributes a new high-quality dataset, named \"Labeled Pedestrian in\nthe Wild (LPW)\" which contains 7,694 tracklets with over 590,000 images.\nDespite its relatively large scale, the annotations also possess high\ncleanliness. Moreover, it's more challenging in the following aspects: the age\nof characters varies from childhood to elderhood; the postures of people are\ndiverse, including running and cycling in addition to the normal walking state.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 16:40:51 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 12:25:26 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Song", "Guanglu", ""], ["Leng", "Biao", ""], ["Liu", "Yu", ""], ["Hetang", "Congrui", ""], ["Cai", "Shaofan", ""]]}, {"id": "1711.08785", "submitter": "Omid Haji Maghsoudi", "authors": "Omid Haji Maghsoudi, Andrew Spence", "title": "3D Based Landmark Tracker Using Superpixels Based Segmentation for\n  Neuroscience and Biomechanics Studies", "comments": "Submitted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Examining locomotion has improved our basic understanding of motor control\nand aided in treating motor impairment. Mice and rats are premier models of\nhuman disease and increasingly the model systems of choice for basic\nneuroscience. High frame rates (250 Hz) are needed to quantify the kinematics\nof these running rodents. Manual tracking, especially for multiple markers,\nbecomes time-consuming and impossible for large sample sizes. Therefore, the\nneed for automatic segmentation of these markers has grown in recent years.\nHere, we address this need by presenting a method to segment the markers using\nthe SLIC superpixel method. The 2D coordinates on the image plane are projected\nto a 3D domain using direct linear transform (DLT) and a 3D Kalman filter has\nbeen used to predict the position of markers based on the speed and position of\nmarkers from the previous frames. Finally, a probabilistic function is used to\nfind the best match among superpixels. The method is evaluated for different\ndifficulties for tracking of the markers and it achieves 95% correct labeling\nof markers.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 17:33:16 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Maghsoudi", "Omid Haji", ""], ["Spence", "Andrew", ""]]}, {"id": "1711.08789", "submitter": "Aviv Gabbay", "authors": "Aviv Gabbay, Asaph Shamir, Shmuel Peleg", "title": "Visual Speech Enhancement", "comments": "Accepted to Interspeech 2018. Supplementary video:\n  https://www.youtube.com/watch?v=nyYarDGpcYA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When video is shot in noisy environment, the voice of a speaker seen in the\nvideo can be enhanced using the visible mouth movements, reducing background\nnoise. While most existing methods use audio-only inputs, improved performance\nis obtained with our visual speech enhancement, based on an audio-visual neural\nnetwork. We include in the training data videos to which we added the voice of\nthe target speaker as background noise. Since the audio input is not sufficient\nto separate the voice of a speaker from his own voice, the trained model better\nexploits the visual input and generalizes well to different noise types. The\nproposed model outperforms prior audio visual methods on two public lipreading\ndatasets. It is also the first to be demonstrated on a dataset not designed for\nlipreading, such as the weekly addresses of Barack Obama.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 17:51:46 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 13:44:10 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 07:25:47 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Gabbay", "Aviv", ""], ["Shamir", "Asaph", ""], ["Peleg", "Shmuel", ""]]}, {"id": "1711.08823", "submitter": "Daniel Czech", "authors": "Daniel Czech, Amit Mishra and Michael Inggs", "title": "A Dictionary Approach to Identifying Transient RFI", "comments": null, "journal-ref": null, "doi": "10.1029/2018RS006538", "report-no": null, "categories": "astro-ph.IM cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As radio telescopes become more sensitive, the damaging effects of radio\nfrequency interference (RFI) become more apparent. Near radio telescope arrays,\nRFI sources are often easily removed or replaced; the challenge lies in\nidentifying them. Transient (impulsive) RFI is particularly difficult to\nidentify. We propose a novel dictionary-based approach to transient RFI\nidentification. RFI events are treated as sequences of sub-events, drawn from\nparticular labelled classes. We demonstrate an automated method of extracting\nand labelling sub-events using a dataset of transient RFI. A dictionary of\nlabels may be used in conjunction with hidden Markov models to identify the\nsources of RFI events reliably. We attain improved classification accuracy over\ntraditional approaches such as SVMs or a na\\\"ive kNN classifier. Finally, we\ninvestigate why transient RFI is difficult to classify. We show that cluster\nseparation in the principal components domain is influenced by the mains supply\nphase for certain sources.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 20:19:26 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Czech", "Daniel", ""], ["Mishra", "Amit", ""], ["Inggs", "Michael", ""]]}, {"id": "1711.08848", "submitter": "Bugra Tekin", "authors": "Bugra Tekin, Sudipta N. Sinha, Pascal Fua", "title": "Real-Time Seamless Single Shot 6D Object Pose Prediction", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a single-shot approach for simultaneously detecting an object in\nan RGB image and predicting its 6D pose without requiring multiple stages or\nhaving to examine multiple hypotheses. Unlike a recently proposed single-shot\ntechnique for this task (Kehl et al., ICCV'17) that only predicts an\napproximate 6D pose that must then be refined, ours is accurate enough not to\nrequire additional post-processing. As a result, it is much faster - 50 fps on\na Titan X (Pascal) GPU - and more suitable for real-time processing. The key\ncomponent of our method is a new CNN architecture inspired by the YOLO network\ndesign that directly predicts the 2D image locations of the projected vertices\nof the object's 3D bounding box. The object's 6D pose is then estimated using a\nPnP algorithm.\n  For single object and multiple object pose estimation on the LINEMOD and\nOCCLUSION datasets, our approach substantially outperforms other recent\nCNN-based approaches when they are all used without post-processing. During\npost-processing, a pose refinement step can be used to boost the accuracy of\nthe existing methods, but at 10 fps or less, they are much slower than our\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 00:56:37 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 16:23:49 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 12:53:49 GMT"}, {"version": "v4", "created": "Wed, 18 Apr 2018 08:09:22 GMT"}, {"version": "v5", "created": "Fri, 7 Dec 2018 09:38:58 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Tekin", "Bugra", ""], ["Sinha", "Sudipta N.", ""], ["Fua", "Pascal", ""]]}, {"id": "1711.08875", "submitter": "Kwonjoon Lee", "authors": "Kwonjoon Lee, Weijian Xu, Fan Fan, Zhuowen Tu", "title": "Wasserstein Introspective Neural Networks", "comments": "Accepted to CVPR 2018 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Wasserstein introspective neural networks (WINN) that are both a\ngenerator and a discriminator within a single model. WINN provides a\nsignificant improvement over the recent introspective neural networks (INN)\nmethod by enhancing INN's generative modeling capability. WINN has three\ninteresting properties: (1) A mathematical connection between the formulation\nof the INN algorithm and that of Wasserstein generative adversarial networks\n(WGAN) is made. (2) The explicit adoption of the Wasserstein distance into INN\nresults in a large enhancement to INN, achieving compelling results even with a\nsingle classifier --- e.g., providing nearly a 20 times reduction in model size\nover INN for unsupervised generative modeling. (3) When applied to supervised\nclassification, WINN also gives rise to improved robustness against adversarial\nexamples in terms of the error reduction. In the experiments, we report\nencouraging results on unsupervised learning problems including texture, face,\nand object modeling, as well as a supervised classification task against\nadversarial attacks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 06:04:02 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 23:18:09 GMT"}, {"version": "v3", "created": "Sun, 3 Dec 2017 08:32:47 GMT"}, {"version": "v4", "created": "Fri, 8 Dec 2017 03:42:16 GMT"}, {"version": "v5", "created": "Sat, 7 Apr 2018 17:05:25 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Lee", "Kwonjoon", ""], ["Xu", "Weijian", ""], ["Fan", "Fan", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1711.08879", "submitter": "Yao Zhai", "authors": "Yao Zhai, Jingjing Fu, Yan Lu, Houqiang Li", "title": "Feature Selective Networks for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects for detection usually have distinct characteristics in different\nsub-regions and different aspect ratios. However, in prevalent two-stage object\ndetection methods, Region-of-Interest (RoI) features are extracted by RoI\npooling with little emphasis on these translation-variant feature components.\nWe present feature selective networks to reform the feature representations of\nRoIs by exploiting their disparities among sub-regions and aspect ratios. Our\nnetwork produces the sub-region attention bank and aspect ratio attention bank\nfor the whole image. The RoI-based sub-region attention map and aspect ratio\nattention map are selectively pooled from the banks, and then used to refine\nthe original RoI features for RoI classification. Equipped with a light-weight\ndetection subnetwork, our network gets a consistent boost in detection\nperformance based on general ConvNet backbones (ResNet-101, GoogLeNet and\nVGG-16). Without bells and whistles, our detectors equipped with ResNet-101\nachieve more than 3% mAP improvement compared to counterparts on PASCAL VOC\n2007, PASCAL VOC 2012 and MS COCO datasets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 06:39:49 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Zhai", "Yao", ""], ["Fu", "Jingjing", ""], ["Lu", "Yan", ""], ["Li", "Houqiang", ""]]}, {"id": "1711.08901", "submitter": "Dang-Khoa Le Tan", "authors": "Dang-Khoa Le Tan, Thanh-Toan Do, Ngai-Man Cheung", "title": "Supervised Hashing with End-to-End Binary Deep Neural Network", "comments": "Accepted to IEEE ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image hashing is a popular technique applied to large scale content-based\nvisual retrieval due to its compact and efficient binary codes. Our work\nproposes a new end-to-end deep network architecture for supervised hashing\nwhich directly learns binary codes from input images and maintains good\nproperties over binary codes such as similarity preservation, independence, and\nbalancing. Furthermore, we also propose a new learning scheme that can cope\nwith the binary constrained loss function. The proposed algorithm not only is\nscalable for learning over large-scale datasets but also outperforms\nstate-of-the-art supervised hashing methods, which are illustrated throughout\nextensive experiments from various image retrieval benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 09:10:32 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 11:16:43 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Tan", "Dang-Khoa Le", ""], ["Do", "Thanh-Toan", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1711.08904", "submitter": "Shanshan Wang", "authors": "Shanshan Wang and Lei Zhang and JingRu Fu", "title": "Adversarial Transfer Learning for Cross-domain Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical visual recognition scenarios, feature distribution in the\nsource domain is generally different from that of the target domain, which\nresults in the emergence of general cross-domain visual recognition problems.\nTo address the problems of visual domain mismatch, we propose a novel\nsemi-supervised adversarial transfer learning approach, which is called Coupled\nadversarial transfer Domain Adaptation (CatDA), for distribution alignment\nbetween two domains. The proposed CatDA approach is inspired by cycleGAN, but\nleveraging multiple shallow multilayer perceptrons (MLPs) instead of deep\nnetworks. Specifically, our CatDA comprises of two symmetric and slim\nsub-networks, such that the coupled adversarial learning framework is\nformulated. With such symmetry of two generators, the input data from\nsource/target domain can be fed into the MLP network for target/source domain\ngeneration, supervised by two confrontation oriented coupled discriminators.\nNotably, in order to avoid the critical flaw of high-capacity of the feature\nextraction function during domain adversarial training, domain specific loss\nand domain knowledge fidelity loss are proposed in each generator, such that\nthe effectiveness of the proposed transfer network is guaranteed. Additionally,\nthe essential difference from cycleGAN is that our method aims to generate\ndomain-agnostic and aligned features for domain adaptation and transfer\nlearning rather than synthesize realistic images. We show experimentally on a\nnumber of benchmark datasets and the proposed approach achieves competitive\nperformance over state-of-the-art domain adaptation and transfer learning\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 09:34:53 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 08:47:10 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Wang", "Shanshan", ""], ["Zhang", "Lei", ""], ["Fu", "JingRu", ""]]}, {"id": "1711.08917", "submitter": "Majd Zreik", "authors": "Majd Zreik, Nikolas Lessmann, Robbert W. van Hamersvelt, Jelmer M.\n  Wolterink, Michiel Voskuil, Max A. Viergever, Tim Leiner, Ivana I\\v{s}gum", "title": "Deep learning analysis of the myocardium in coronary CT angiography for\n  identification of patients with functionally significant coronary artery\n  stenosis", "comments": "This paper was submitted in April 2017 and accepted in November 2017\n  for publication in Medical Image Analysis. Please cite as: Zreik et al.,\n  Medical Image Analysis, 2018, vol. 44, pp. 72-85", "journal-ref": null, "doi": "10.1016/j.media.2017.11.008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In patients with coronary artery stenoses of intermediate severity, the\nfunctional significance needs to be determined. Fractional flow reserve (FFR)\nmeasurement, performed during invasive coronary angiography (ICA), is most\noften used in clinical practice. To reduce the number of ICA procedures, we\npresent a method for automatic identification of patients with functionally\nsignificant coronary artery stenoses, employing deep learning analysis of the\nleft ventricle (LV) myocardium in rest coronary CT angiography (CCTA). The\nstudy includes consecutively acquired CCTA scans of 166 patients with FFR\nmeasurements. To identify patients with a functionally significant coronary\nartery stenosis, analysis is performed in several stages. First, the LV\nmyocardium is segmented using a multiscale convolutional neural network (CNN).\nTo characterize the segmented LV myocardium, it is subsequently encoded using\nunsupervised convolutional autoencoder (CAE). Thereafter, patients are\nclassified according to the presence of functionally significant stenosis using\nan SVM classifier based on the extracted and clustered encodings. Quantitative\nevaluation of LV myocardium segmentation in 20 images resulted in an average\nDice coefficient of 0.91 and an average mean absolute distance between the\nsegmented and reference LV boundaries of 0.7 mm. Classification of patients was\nevaluated in the remaining 126 CCTA scans in 50 10-fold cross-validation\nexperiments and resulted in an area under the receiver operating characteristic\ncurve of 0.74 +- 0.02. At sensitivity levels 0.60, 0.70 and 0.80, the\ncorresponding specificity was 0.77, 0.71 and 0.59, respectively. The results\ndemonstrate that automatic analysis of the LV myocardium in a single CCTA scan\nacquired at rest, without assessment of the anatomy of the coronary arteries,\ncan be used to identify patients with functionally significant coronary artery\nstenosis.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 10:27:36 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 09:22:49 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Zreik", "Majd", ""], ["Lessmann", "Nikolas", ""], ["van Hamersvelt", "Robbert W.", ""], ["Wolterink", "Jelmer M.", ""], ["Voskuil", "Michiel", ""], ["Viergever", "Max A.", ""], ["Leiner", "Tim", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1711.08920", "submitter": "Jan Eric Lenssen", "authors": "Matthias Fey, Jan Eric Lenssen, Frank Weichert, Heinrich M\\\"uller", "title": "SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels", "comments": "Presented at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant\nof deep neural networks for irregular structured and geometric input, e.g.,\ngraphs or meshes. Our main contribution is a novel convolution operator based\non B-splines, that makes the computation time independent from the kernel size\ndue to the local support property of the B-spline basis functions. As a result,\nwe obtain a generalization of the traditional CNN convolution operator by using\ncontinuous kernel functions parametrized by a fixed number of trainable\nweights. In contrast to related approaches that filter in the spectral domain,\nthe proposed method aggregates features purely in the spatial domain. In\naddition, SplineCNN allows entire end-to-end training of deep architectures,\nusing only the geometric structure as input, instead of handcrafted feature\ndescriptors. For validation, we apply our method on tasks from the fields of\nimage graph classification, shape correspondence and graph node classification,\nand show that it outperforms or pars state-of-the-art approaches while being\nsignificantly faster and having favorable properties like domain-independence.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 10:33:05 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 08:57:29 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Fey", "Matthias", ""], ["Lenssen", "Jan Eric", ""], ["Weichert", "Frank", ""], ["M\u00fcller", "Heinrich", ""]]}, {"id": "1711.08922", "submitter": "Hsuan-I Ho", "authors": "Hsuan-I Ho, Wei-Chen Chiu, Yu-Chiang Frank Wang", "title": "Summarizing First-Person Videos from Third Persons' Points of Views", "comments": "16+10 pages, ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video highlight or summarization is among interesting topics in computer\nvision, which benefits a variety of applications like viewing, searching, or\nstorage. However, most existing studies rely on training data of third-person\nvideos, which cannot easily generalize to highlight the first-person ones. With\nthe goal of deriving an effective model to summarize first-person videos, we\npropose a novel deep neural network architecture for describing and\ndiscriminating vital spatiotemporal information across videos with different\npoints of view. Our proposed model is realized in a semi-supervised setting, in\nwhich fully annotated third-person videos, unlabeled first-person videos, and a\nsmall number of annotated first-person ones are presented during training. In\nour experiments, qualitative and quantitative evaluations on both benchmarks\nand our collected first-person video datasets are presented.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 10:42:42 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 13:28:51 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Ho", "Hsuan-I", ""], ["Chiu", "Wei-Chen", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "1711.08925", "submitter": "Alessandro Artusi PhD", "authors": "E. Sikudova, T. Pouli, A. Artusi, A. O. Akyuz, F. Banterle, Z. M.\n  Mazlumoglu and E. Reinhard", "title": "A Gamut-Mapping Framework for Color-Accurate Reproduction of HDR Images", "comments": null, "journal-ref": "IEEE Computer Graphics and Applications, Volume 36, Issue 4,\n  p.78-90, ISSN 0272-1716, July-August 2016", "doi": "10.1109/MCG.2015.116", "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few tone mapping operators (TMOs) take color management into consideration,\nlimiting compression to luminance values only. This may lead to changes in\nimage chroma and hues which are typically managed with a post-processing step.\nHowever, current post-processing techniques for tone reproduction do not\nexplicitly consider the target display gamut. Gamut mapping on the other hand,\ndeals with mapping images from one color gamut to another, usually smaller,\ngamut but has traditionally focused on smaller scale, chromatic changes. In\nthis context, we present a novel gamut and tone management framework for\ncolor-accurate reproduction of high dynamic range (HDR) images, which is\nconceptually and computationally simple, parameter-free, and compatible with\nexisting TMOs. In the CIE LCh color space, we compress chroma to fit the gamut\nof the output color space. This prevents hue and luminance shifts while taking\ngamut boundaries into consideration. We also propose a compatible lightness\ncompression scheme that minimizes the number of color space conversions. Our\nresults show that our gamut management method effectively compresses the chroma\nof tone mapped images, respecting the target gamut and without reducing image\nquality.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 11:06:07 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Sikudova", "E.", ""], ["Pouli", "T.", ""], ["Artusi", "A.", ""], ["Akyuz", "A. O.", ""], ["Banterle", "F.", ""], ["Mazlumoglu", "Z. M.", ""], ["Reinhard", "E.", ""]]}, {"id": "1711.08937", "submitter": "Shangzhe Wu", "authors": "Shangzhe Wu, Jiarui Xu, Yu-Wing Tai, Chi-Keung Tang", "title": "Deep High Dynamic Range Imaging with Large Foreground Motions", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the first non-flow-based deep framework for high dynamic\nrange (HDR) imaging of dynamic scenes with large-scale foreground motions. In\nstate-of-the-art deep HDR imaging, input images are first aligned using optical\nflows before merging, which are still error-prone due to occlusion and large\nmotions. In stark contrast to flow-based methods, we formulate HDR imaging as\nan image translation problem without optical flows. Moreover, our simple\ntranslation network can automatically hallucinate plausible HDR details in the\npresence of total occlusion, saturation and under-exposure, which are otherwise\nalmost impossible to recover by conventional optimization approaches. Our\nframework can also be extended for different reference images. We performed\nextensive qualitative and quantitative comparisons to show that our approach\nproduces excellent results where color artifacts and geometric distortions are\nsignificantly reduced compared to existing state-of-the-art methods, and is\nrobust across various inputs, including images without radiometric calibration.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 12:12:01 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 07:23:49 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 11:07:30 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Wu", "Shangzhe", ""], ["Xu", "Jiarui", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1711.08972", "submitter": "Shangzhe Wu", "authors": "Yongyi Lu, Shangzhe Wu, Yu-Wing Tai, Chi-Keung Tang", "title": "Image Generation from Sketch Constraint Using Contextual GAN", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate image generation guided by hand sketch. When the\ninput sketch is badly drawn, the output of common image-to-image translation\nfollows the input edges due to the hard condition imposed by the translation\nprocess. Instead, we propose to use sketch as weak constraint, where the output\nedges do not necessarily follow the input edges. We address this problem using\na novel joint image completion approach, where the sketch provides the image\ncontext for completing, or generating the output image. We train a generated\nadversarial network, i.e, contextual GAN to learn the joint distribution of\nsketch and the corresponding image by using joint images. Our contextual GAN\nhas several advantages. First, the simple joint image representation allows for\nsimple and effective learning of joint distribution in the same image-sketch\nspace, which avoids complicated issues in cross-domain learning. Second, while\nthe output is related to its input overall, the generated features exhibit more\nfreedom in appearance and do not strictly align with the input features as\nprevious conditional GANs do. Third, from the joint image's point of view,\nimage and sketch are of no difference, thus exactly the same deep joint image\ncompletion network can be used for image-to-sketch generation. Experiments\nevaluated on three different datasets show that our contextual GAN can generate\nmore realistic images than state-of-the-art conditional GANs on challenging\ninputs and generalize well on common categories.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 14:19:25 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 03:01:01 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Lu", "Yongyi", ""], ["Wu", "Shangzhe", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1711.08992", "submitter": "Kalin Stefanov", "authors": "Kalin Stefanov, Jonas Beskow and Giampiero Salvi", "title": "Self-Supervised Vision-Based Detection of the Active Speaker as Support\n  for Socially-Aware Language Acquisition", "comments": "10 pages, IEEE Transactions on Cognitive and Developmental Systems", "journal-ref": null, "doi": "10.1109/TCDS.2019.2927941", "report-no": null, "categories": "cs.CV cs.CL cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a self-supervised method for visual detection of the\nactive speaker in a multi-person spoken interaction scenario. Active speaker\ndetection is a fundamental prerequisite for any artificial cognitive system\nattempting to acquire language in social settings. The proposed method is\nintended to complement the acoustic detection of the active speaker, thus\nimproving the system robustness in noisy conditions. The method can detect an\narbitrary number of possibly overlapping active speakers based exclusively on\nvisual information about their face. Furthermore, the method does not rely on\nexternal annotations, thus complying with cognitive development. Instead, the\nmethod uses information from the auditory modality to support learning in the\nvisual domain. This paper reports an extensive evaluation of the proposed\nmethod using a large multi-person face-to-face interaction dataset. The results\nshow good performance in a speaker dependent setting. However, in a speaker\nindependent setting the proposed method yields a significantly lower\nperformance. We believe that the proposed method represents an essential\ncomponent of any artificial cognitive system or robotic platform engaging in\nsocial interactions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 14:45:06 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 17:55:38 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Stefanov", "Kalin", ""], ["Beskow", "Jonas", ""], ["Salvi", "Giampiero", ""]]}, {"id": "1711.08995", "submitter": "Pedro O. Pinheiro", "authors": "Pedro O. Pinheiro", "title": "Unsupervised Domain Adaptation with Similarity Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of unsupervised domain adaptation is to leverage features from\na labeled source domain and learn a classifier for an unlabeled target domain,\nwith a similar but different data distribution. Most deep learning approaches\nto domain adaptation consist of two steps: (i) learn features that preserve a\nlow risk on labeled samples (source domain) and (ii) make the features from\nboth domains to be as indistinguishable as possible, so that a classifier\ntrained on the source can also be applied on the target domain. In general, the\nclassifiers in step (i) consist of fully-connected layers applied directly on\nthe indistinguishable features learned in (ii). In this paper, we propose a\ndifferent way to do the classification, using similarity learning. The proposed\nmethod learns a pairwise similarity function in which classification can be\nperformed by computing similarity between prototype representations of each\ncategory. The domain-invariant features and the categorical prototype\nrepresentations are learned jointly and in an end-to-end fashion. At inference\ntime, images from the target domain are compared to the prototypes and the\nlabel associated with the one that best matches the image is outputed. The\napproach is simple, scalable and effective. We show that our model achieves\nstate-of-the-art performance in different unsupervised domain adaptation\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 14:49:25 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 01:06:21 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Pinheiro", "Pedro O.", ""]]}, {"id": "1711.08996", "submitter": "Chengde Wan Mr", "authors": "Chengde Wan, Thomas Probst, Luc Van Gool, Angela Yao", "title": "Dense 3D Regression for Hand Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and effective method for 3D hand pose estimation from a\nsingle depth frame. As opposed to previous state-of-the-art methods based on\nholistic 3D regression, our method works on dense pixel-wise estimation. This\nis achieved by careful design choices in pose parameterization, which leverages\nboth 2D and 3D properties of depth map. Specifically, we decompose the pose\nparameters into a set of per-pixel estimations, i.e., 2D heat maps, 3D heat\nmaps and unit 3D directional vector fields. The 2D/3D joint heat maps and 3D\njoint offsets are estimated via multi-task network cascades, which is trained\nend-to-end. The pixel-wise estimations can be directly translated into a vote\ncasting scheme. A variant of mean shift is then used to aggregate local votes\nwhile enforcing consensus between the the estimated 3D pose and the pixel-wise\n2D and 3D estimations by design. Our method is efficient and highly accurate.\nOn MSRA and NYU hand dataset, our method outperforms all previous\nstate-of-the-art approaches by a large margin. On the ICVL hand dataset, our\nmethod achieves similar accuracy compared to the currently proposed nearly\nsaturated result and outperforms various other proposed methods. Code is\navailable $\\href{\"https://github.com/melonwan/denseReg\"}{\\text{online}}$.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 14:50:23 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Wan", "Chengde", ""], ["Probst", "Thomas", ""], ["Van Gool", "Luc", ""], ["Yao", "Angela", ""]]}, {"id": "1711.08998", "submitter": "Christian Baumgartner", "authors": "Christian F. Baumgartner, Lisa M. Koch, Kerem Can Tezcan, Jia Xi Ang,\n  Ender Konukoglu", "title": "Visual Feature Attribution using Wasserstein GANs", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attributing the pixels of an input image to a certain category is an\nimportant and well-studied problem in computer vision, with applications\nranging from weakly supervised localisation to understanding hidden effects in\nthe data. In recent years, approaches based on interpreting a previously\ntrained neural network classifier have become the de facto state-of-the-art and\nare commonly used on medical as well as natural image datasets. In this paper,\nwe discuss a limitation of these approaches which may lead to only a subset of\nthe category specific features being detected. To address this problem we\ndevelop a novel feature attribution technique based on Wasserstein Generative\nAdversarial Networks (WGAN), which does not suffer from this limitation. We\nshow that our proposed method performs substantially better than the\nstate-of-the-art for visual attribution on a synthetic dataset and on real 3D\nneuroimaging data from patients with mild cognitive impairment (MCI) and\nAlzheimer's disease (AD). For AD patients the method produces compellingly\nrealistic disease effect maps which are very close to the observed effects.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 14:52:08 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 09:40:54 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 10:38:13 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Baumgartner", "Christian F.", ""], ["Koch", "Lisa M.", ""], ["Tezcan", "Kerem Can", ""], ["Ang", "Jia Xi", ""], ["Konukoglu", "Ender", ""]]}, {"id": "1711.09001", "submitter": "Qianru Sun", "authors": "Qianru Sun, Liqian Ma, Seong Joon Oh, Luc Van Gool, Bernt Schiele,\n  Mario Fritz", "title": "Natural and Effective Obfuscation by Head Inpainting", "comments": "To appear in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As more and more personal photos are shared online, being able to obfuscate\nidentities in such photos is becoming a necessity for privacy protection.\nPeople have largely resorted to blacking out or blurring head regions, but they\nresult in poor user experience while being surprisingly ineffective against\nstate of the art person recognizers. In this work, we propose a novel head\ninpainting obfuscation technique. Generating a realistic head inpainting in\nsocial media photos is challenging because subjects appear in diverse\nactivities and head orientations. We thus split the task into two sub-tasks:\n(1) facial landmark generation from image context (e.g. body pose) for seamless\nhypothesis of sensible head pose, and (2) facial landmark conditioned head\ninpainting. We verify that our inpainting method generates realistic person\nimages, while achieving superior obfuscation performance against automatic\nperson recognizers.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 15:00:53 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 10:50:54 GMT"}, {"version": "v3", "created": "Thu, 7 Dec 2017 13:30:45 GMT"}, {"version": "v4", "created": "Sun, 21 Jan 2018 20:36:14 GMT"}, {"version": "v5", "created": "Fri, 16 Mar 2018 10:56:10 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Sun", "Qianru", ""], ["Ma", "Liqian", ""], ["Oh", "Seong Joon", ""], ["Van Gool", "Luc", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1711.09017", "submitter": "Xucong Zhang", "authors": "Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling", "title": "MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based methods are believed to work well for unconstrained gaze\nestimation, i.e. gaze estimation from a monocular RGB camera without\nassumptions regarding user, environment, or camera. However, current gaze\ndatasets were collected under laboratory conditions and methods were not\nevaluated across multiple datasets. Our work makes three contributions towards\naddressing these limitations. First, we present the MPIIGaze that contains\n213,659 full face images and corresponding ground-truth gaze positions\ncollected from 15 users during everyday laptop use over several months. An\nexperience sampling approach ensured continuous gaze and head poses and\nrealistic variation in eye appearance and illumination. To facilitate\ncross-dataset evaluations, 37,667 images were manually annotated with eye\ncorners, mouth corners, and pupil centres. Second, we present an extensive\nevaluation of state-of-the-art gaze estimation methods on three current\ndatasets, including MPIIGaze. We study key challenges including target gaze\nrange, illumination conditions, and facial appearance variation. We show that\nimage resolution and the use of both eyes affect gaze estimation performance\nwhile head pose and pupil centre information are less informative. Finally, we\npropose GazeNet, the first deep appearance-based gaze estimation method.\nGazeNet improves the state of the art by 22% percent (from a mean error of 13.9\ndegrees to 10.8 degrees) for the most challenging cross-dataset evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 15:20:22 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Zhang", "Xucong", ""], ["Sugano", "Yusuke", ""], ["Fritz", "Mario", ""], ["Bulling", "Andreas", ""]]}, {"id": "1711.09020", "submitter": "Yunjey Choi", "authors": "Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim,\n  Jaegul Choo", "title": "StarGAN: Unified Generative Adversarial Networks for Multi-Domain\n  Image-to-Image Translation", "comments": "Accepted to CVPR 2018 (Oral)", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2018, pp. 8789-8797", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown remarkable success in image-to-image translation\nfor two domains. However, existing approaches have limited scalability and\nrobustness in handling more than two domains, since different models should be\nbuilt independently for every pair of image domains. To address this\nlimitation, we propose StarGAN, a novel and scalable approach that can perform\nimage-to-image translations for multiple domains using only a single model.\nSuch a unified model architecture of StarGAN allows simultaneous training of\nmultiple datasets with different domains within a single network. This leads to\nStarGAN's superior quality of translated images compared to existing models as\nwell as the novel capability of flexibly translating an input image to any\ndesired target domain. We empirically demonstrate the effectiveness of our\napproach on a facial attribute transfer and a facial expression synthesis\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 15:37:30 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 06:09:36 GMT"}, {"version": "v3", "created": "Fri, 21 Sep 2018 08:17:49 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Choi", "Yunjey", ""], ["Choi", "Minje", ""], ["Kim", "Munyoung", ""], ["Ha", "Jung-Woo", ""], ["Kim", "Sunghun", ""], ["Choo", "Jaegul", ""]]}, {"id": "1711.09026", "submitter": "Apratim Bhattacharyya", "authors": "Apratim Bhattacharyya, Mario Fritz, Bernt Schiele", "title": "Long-Term On-Board Prediction of People in Traffic Scenes under\n  Uncertainty", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress towards advanced systems for assisted and autonomous driving is\nleveraging recent advances in recognition and segmentation methods. Yet, we are\nstill facing challenges in bringing reliable driving to inner cities, as those\nare composed of highly dynamic scenes observed from a moving platform at\nconsiderable speeds. Anticipation becomes a key element in order to react\ntimely and prevent accidents. In this paper we argue that it is necessary to\npredict at least 1 second and we thus propose a new model that jointly predicts\nego motion and people trajectories over such large time horizons. We pay\nparticular attention to modeling the uncertainty of our estimates arising from\nthe non-deterministic nature of natural traffic scenes. Our experimental\nresults show that it is indeed possible to predict people trajectories at the\ndesired time horizons and that our uncertainty estimates are informative of the\nprediction error. We also show that both sequence modeling of trajectories as\nwell as our novel method of long term odometry prediction are essential for\nbest performance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 15:54:49 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 15:04:58 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Bhattacharyya", "Apratim", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1711.09055", "submitter": "Giovanni Saponaro", "authors": "Giovanni Saponaro, Lorenzo Jamone, Alexandre Bernardino and Giampiero\n  Salvi", "title": "Interactive Robot Learning of Gestures, Language and Affordances", "comments": "code available at https://github.com/gsaponaro/glu-gestures", "journal-ref": "International Workshop on Grounding Language Understanding (GLU),\n  Satellite of Interspeech 2017", "doi": "10.21437/GLU.2017-17", "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing field in robotics and Artificial Intelligence (AI) research is\nhuman-robot collaboration, whose target is to enable effective teamwork between\nhumans and robots. However, in many situations human teams are still superior\nto human-robot teams, primarily because human teams can easily agree on a\ncommon goal with language, and the individual members observe each other\neffectively, leveraging their shared motor repertoire and sensorimotor\nresources. This paper shows that for cognitive robots it is possible, and\nindeed fruitful, to combine knowledge acquired from interacting with elements\nof the environment (affordance exploration) with the probabilistic observation\nof another agent's actions.\n  We propose a model that unites (i) learning robot affordances and word\ndescriptions with (ii) statistical recognition of human gestures with vision\nsensors. We discuss theoretical motivations, possible implementations, and we\nshow initial results which highlight that, after having acquired knowledge of\nits surrounding environment, a humanoid robot can generalize this knowledge to\nthe case when it observes another agent (human partner) performing the same\nmotor actions previously executed during training.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 17:34:32 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Saponaro", "Giovanni", ""], ["Jamone", "Lorenzo", ""], ["Bernardino", "Alexandre", ""], ["Salvi", "Giampiero", ""]]}, {"id": "1711.09060", "submitter": "Thomio Watanabe", "authors": "Thomio Watanabe and Denis Wolf", "title": "Distance to Center of Mass Encoding for Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The instance segmentation can be considered an extension of the object\ndetection problem where bounding boxes are replaced by object contours.\nStrictly speaking the problem requires to identify each pixel instance and\nclass independently of the artifice used for this mean. The advantage of\ninstance segmentation over the usual object detection lies in the precise\ndelineation of objects improving object localization. Additionally, object\ncontours allow the evaluation of partial occlusion with basic image processing\nalgorithms. This work approaches the instance segmentation problem as an\nannotation problem and presents a novel technique to encode and decode ground\ntruth annotations. We propose a mathematical representation of instances that\nany deep semantic segmentation model can learn and generalize. Each individual\ninstance is represented by a center of mass and a field of vectors pointing to\nit. This encoding technique has been denominated Distance to Center of Mass\nEncoding (DCME).\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 17:53:15 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Watanabe", "Thomio", ""], ["Wolf", "Denis", ""]]}, {"id": "1711.09064", "submitter": "Hongyang Gao", "authors": "Hongyang Gao, Shuiwang Ji", "title": "Efficient and Invariant Convolutional Neural Networks for Dense\n  Prediction", "comments": "6 pages, ICDM2017", "journal-ref": "Gao, Hongyang, and Shuiwang Ji. \"Efficient and Invariant\n  Convolutional Neural Networks for Dense Prediction.\" In Data Mining (ICDM),\n  2017 IEEE International Conference on, pp. 871-876. IEEE, 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have shown great success on feature extraction\nfrom raw input data such as images. Although convolutional neural networks are\ninvariant to translations on the inputs, they are not invariant to other\ntransformations, including rotation and flip. Recent attempts have been made to\nincorporate more invariance in image recognition applications, but they are not\napplicable to dense prediction tasks, such as image segmentation. In this\npaper, we propose a set of methods based on kernel rotation and flip to enable\nrotation and flip invariance in convolutional neural networks. The kernel\nrotation can be achieved on kernels of 3 $\\times$ 3, while kernel flip can be\napplied on kernels of any size. By rotating in eight or four angles, the\nconvolutional layers could produce the corresponding number of feature maps\nbased on eight or four different kernels. By using flip, the convolution layer\ncan produce three feature maps. By combining produced feature maps using\nmaxout, the resource requirement could be significantly reduced while still\nretain the invariance properties. Experimental results demonstrate that the\nproposed methods can achieve various invariance at reasonable resource\nrequirements in terms of both memory and time.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 18:19:08 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Gao", "Hongyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1711.09078", "submitter": "Jiajun Wu", "authors": "Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, William T. Freeman", "title": "Video Enhancement with Task-Oriented Flow", "comments": "IJCV 2019. Project page: http://toflow.csail.mit.edu", "journal-ref": "International Journal of Computer Vision (IJCV), 127(8):1106-1125,\n  2019", "doi": "10.1007/s11263-018-01144-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many video enhancement algorithms rely on optical flow to register frames in\na video sequence. Precise flow estimation is however intractable; and optical\nflow itself is often a sub-optimal representation for particular video\nprocessing tasks. In this paper, we propose task-oriented flow (TOFlow), a\nmotion representation learned in a self-supervised, task-specific manner. We\ndesign a neural network with a trainable motion estimation component and a\nvideo processing component, and train them jointly to learn the task-oriented\nflow. For evaluation, we build Vimeo-90K, a large-scale, high-quality video\ndataset for low-level video processing. TOFlow outperforms traditional optical\nflow on standard benchmarks as well as our Vimeo-90K dataset in three video\nprocessing tasks: frame interpolation, video denoising/deblocking, and video\nsuper-resolution.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 18:48:36 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 21:05:04 GMT"}, {"version": "v3", "created": "Sun, 10 Nov 2019 20:06:00 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Xue", "Tianfan", ""], ["Chen", "Baian", ""], ["Wu", "Jiajun", ""], ["Wei", "Donglai", ""], ["Freeman", "William T.", ""]]}, {"id": "1711.09081", "submitter": "Kevis-Kokitsi Maninis", "authors": "Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, Luc Van Gool", "title": "Deep Extreme Cut: From Extreme Points to Object Segmentation", "comments": "CVPR 2018 camera ready. Project webpage and code:\n  http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of extreme points in an object (left-most,\nright-most, top, bottom pixels) as input to obtain precise object segmentation\nfor images and videos. We do so by adding an extra channel to the image in the\ninput of a convolutional neural network (CNN), which contains a Gaussian\ncentered in each of the extreme points. The CNN learns to transform this\ninformation into a segmentation of an object that matches those extreme points.\nWe demonstrate the usefulness of this approach for guided segmentation\n(grabcut-style), interactive segmentation, video object segmentation, and dense\nsegmentation annotation. We show that we obtain the most precise results to\ndate, also with less user input, in an extensive and varied selection of\nbenchmarks and datasets. All our models and code are publicly available on\nhttp://www.vision.ee.ethz.ch/~cvlsegmentation/dextr/.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 18:54:35 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 11:47:16 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Maninis", "Kevis-Kokitsi", ""], ["Caelles", "Sergi", ""], ["Pont-Tuset", "Jordi", ""], ["Van Gool", "Luc", ""]]}, {"id": "1711.09082", "submitter": "Zhongzheng Ren", "authors": "Zhongzheng Ren and Yong Jae Lee", "title": "Cross-Domain Self-supervised Multi-task Feature Learning using Synthetic\n  Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In human learning, it is common to use multiple sources of information\njointly. However, most existing feature learning approaches learn from only a\nsingle task. In this paper, we propose a novel multi-task deep network to learn\ngeneralizable high-level visual representations. Since multi-task learning\nrequires annotations for multiple properties of the same training instance, we\nlook to synthetic images to train our network. To overcome the domain\ndifference between real and synthetic data, we employ an unsupervised feature\nspace domain adaptation method based on adversarial learning. Given an input\nsynthetic RGB image, our network simultaneously predicts its surface normal,\ndepth, and instance contour, while also minimizing the feature space domain\ndifferences between real and synthetic data. Through extensive experiments, we\ndemonstrate that our network learns more transferable representations compared\nto single-task baselines. Our learned representation produces state-of-the-art\ntransfer learning results on PASCAL VOC 2007 classification and 2012 detection.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 18:55:01 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Ren", "Zhongzheng", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1711.09115", "submitter": "Can Kanbak", "authors": "Can Kanbak, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard", "title": "Geometric robustness of deep networks: analysis and improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have been shown to be vulnerable to\narbitrary geometric transformations. However, there is no systematic method to\nmeasure the invariance properties of deep networks to such transformations. We\npropose ManiFool as a simple yet scalable algorithm to measure the invariance\nof deep networks. In particular, our algorithm measures the robustness of deep\nnetworks to geometric transformations in a worst-case regime as they can be\nproblematic for sensitive applications. Our extensive experimental results show\nthat ManiFool can be used to measure the invariance of fairly complex networks\non high dimensional datasets and these values can be used for analyzing the\nreasons for it. Furthermore, we build on Manifool to propose a new adversarial\ntraining scheme and we show its effectiveness on improving the invariance\nproperties of deep neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 19:32:57 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Kanbak", "Can", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""]]}, {"id": "1711.09125", "submitter": "Limin Wang", "authors": "Limin Wang, Wei Li, Wen Li, Luc Van Gool", "title": "Appearance-and-Relation Networks for Video Classification", "comments": "CVPR18 camera-ready version. Code & models available at\n  https://github.com/wanglimin/ARTNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatiotemporal feature learning in videos is a fundamental problem in\ncomputer vision. This paper presents a new architecture, termed as\nAppearance-and-Relation Network (ARTNet), to learn video representation in an\nend-to-end manner. ARTNets are constructed by stacking multiple generic\nbuilding blocks, called as SMART, whose goal is to simultaneously model\nappearance and relation from RGB input in a separate and explicit manner.\nSpecifically, SMART blocks decouple the spatiotemporal learning module into an\nappearance branch for spatial modeling and a relation branch for temporal\nmodeling. The appearance branch is implemented based on the linear combination\nof pixels or filter responses in each frame, while the relation branch is\ndesigned based on the multiplicative interactions between pixels or filter\nresponses across multiple frames. We perform experiments on three action\nrecognition benchmarks: Kinetics, UCF101, and HMDB51, demonstrating that SMART\nblocks obtain an evident improvement over 3D convolutions for spatiotemporal\nfeature learning. Under the same training setting, ARTNets achieve superior\nperformance on these three datasets to the existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 20:10:26 GMT"}, {"version": "v2", "created": "Sun, 6 May 2018 13:44:34 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Wang", "Limin", ""], ["Li", "Wei", ""], ["Li", "Wen", ""], ["Van Gool", "Luc", ""]]}, {"id": "1711.09151", "submitter": "Aditya Deshpande", "authors": "Jyoti Aneja, Aditya Deshpande, Alexander Schwing", "title": "Convolutional Image Captioning", "comments": "11 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning is an important but challenging task, applicable to virtual\nassistants, editing tools, image indexing, and support of the disabled. Its\nchallenges are due to the variability and ambiguity of possible image\ndescriptions. In recent years significant progress has been made in image\ncaptioning, using Recurrent Neural Networks powered by long-short-term-memory\n(LSTM) units. Despite mitigating the vanishing gradient problem, and despite\ntheir compelling ability to memorize dependencies, LSTM units are complex and\ninherently sequential across time. To address this issue, recent work has shown\nbenefits of convolutional networks for machine translation and conditional\nimage generation. Inspired by their success, in this paper, we develop a\nconvolutional image captioning technique. We demonstrate its efficacy on the\nchallenging MSCOCO dataset and demonstrate performance on par with the\nbaseline, while having a faster training time per number of parameters. We also\nperform a detailed analysis, providing compelling reasons in favor of\nconvolutional language generation approaches.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 22:04:14 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Aneja", "Jyoti", ""], ["Deshpande", "Aditya", ""], ["Schwing", "Alexander", ""]]}, {"id": "1711.09168", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Marc Gorriz, Axel Carlier, Emmanuel Faure and Xavier Giro-i-Nieto", "title": "Cost-Effective Active Learning for Melanoma Segmentation", "comments": "NIPS ML4H 2017 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Active Learning framework capable to train effectively a\nconvolutional neural network for semantic segmentation of medical imaging, with\na limited amount of training labeled data. Our contribution is a practical\nCost-Effective Active Learning approach using dropout at test time as Monte\nCarlo sampling to model the pixel-wise uncertainty and to analyze the image\ninformation to improve the training performance. The source code of this\nproject is available at\nhttps://marc-gorriz.github.io/CEAL-Medical-Image-Segmentation/ .\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 23:42:50 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 09:17:35 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Gorriz", "Marc", ""], ["Carlier", "Axel", ""], ["Faure", "Emmanuel", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1711.09175", "submitter": "Sherif Abdulatif", "authors": "Sherif Abdulatif, Fady Aziz, Bernhard Kleiner, Urs Schneider", "title": "Real-Time Capable Micro-Doppler Signature Decomposition of Walking Human\n  Limbs", "comments": "6 pages, IEEE RadarConf 17", "journal-ref": "IEEE Radar Conference 2017 1093 1098", "doi": "10.1109/RADAR.2017.7944367", "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unique micro-Doppler signature ($\\boldsymbol{\\mu}$-D) of a human body motion\ncan be analyzed as the superposition of different body parts\n$\\boldsymbol{\\mu}$-D signatures. Extraction of human limbs $\\boldsymbol{\\mu}$-D\nsignatures in real-time can be used to detect, classify and track human motion\nespecially for safety application. In this paper, two methods are combined to\nsimulate $\\boldsymbol{\\mu}$-D signatures of a walking human. Furthermore, a\nnovel limbs $\\mu$-D signature time independent decomposition feasibility study\nis presented based on features as $\\mu$-D signatures and range profiles also\nknown as micro-Range ($\\mu$-R). Walking human body parts can be divided into\nfour classes (base, arms, legs, feet) and a decision tree classifier is used.\nValidation is done and the classifier is able to decompose $\\mu$-D signatures\nof limbs from a walking human signature on real-time basis.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 01:28:00 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Abdulatif", "Sherif", ""], ["Aziz", "Fady", ""], ["Kleiner", "Bernhard", ""], ["Schneider", "Urs", ""]]}, {"id": "1711.09177", "submitter": "Sherif Abdulatif", "authors": "Sherif Abdulatif, Qian Wei, Fady Aziz, Bernhard Kleiner, Urs Schneider", "title": "Micro-Doppler Based Human-Robot Classification Using Ensemble and Deep\n  Learning Approaches", "comments": "6 pages, accepted in IEEE Radar Conference 2018", "journal-ref": "IEEE Radar Conference 2017 1043 1048", "doi": "10.1109/RADAR.2018.8378705", "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radar sensors can be used for analyzing the induced frequency shifts due to\nmicro-motions in both range and velocity dimensions identified as micro-Doppler\n($\\boldsymbol{\\mu}$-D) and micro-Range ($\\boldsymbol{\\mu}$-R), respectively.\nDifferent moving targets will have unique $\\boldsymbol{\\mu}$-D and\n$\\boldsymbol{\\mu}$-R signatures that can be used for target classification.\nSuch classification can be used in numerous fields, such as gait recognition,\nsafety and surveillance. In this paper, a 25 GHz FMCW Single-Input\nSingle-Output (SISO) radar is used in industrial safety for real-time\nhuman-robot identification. Due to the real-time constraint, joint\nRange-Doppler (R-D) maps are directly analyzed for our classification problem.\nFurthermore, a comparison between the conventional classical learning\napproaches with handcrafted extracted features, ensemble classifiers and deep\nlearning approaches is presented. For ensemble classifiers, restructured range\nand velocity profiles are passed directly to ensemble trees, such as gradient\nboosting and random forest without feature extraction. Finally, a Deep\nConvolutional Neural Network (DCNN) is used and raw R-D images are directly fed\ninto the constructed network. DCNN shows a superior performance of 99\\%\naccuracy in identifying humans from robots on a single R-D map.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 01:38:03 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 01:15:48 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 09:13:32 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Abdulatif", "Sherif", ""], ["Wei", "Qian", ""], ["Aziz", "Fady", ""], ["Kleiner", "Bernhard", ""], ["Schneider", "Urs", ""]]}, {"id": "1711.09191", "submitter": "Siyang Li", "authors": "Siyang Li, Xiangxin Zhu, Qin Huang, Hao Xu and C.-C. Jay Kuo", "title": "Multiple Instance Curriculum Learning for Weakly Supervised Object\n  Detection", "comments": "Published in BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When supervising an object detector with weakly labeled data, most existing\napproaches are prone to trapping in the discriminative object parts, e.g.,\nfinding the face of a cat instead of the full body, due to lacking the\nsupervision on the extent of full objects. To address this challenge, we\nincorporate object segmentation into the detector training, which guides the\nmodel to correctly localize the full objects. We propose the multiple instance\ncurriculum learning (MICL) method, which injects curriculum learning (CL) into\nthe multiple instance learning (MIL) framework. The MICL method starts by\nautomatically picking the easy training examples, where the extent of the\nsegmentation masks agree with detection bounding boxes. The training set is\ngradually expanded to include harder examples to train strong detectors that\nhandle complex images. The proposed MICL method with segmentation in the loop\noutperforms the state-of-the-art weakly supervised object detectors by a\nsubstantial margin on the PASCAL VOC datasets.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 05:08:09 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Li", "Siyang", ""], ["Zhu", "Xiangxin", ""], ["Huang", "Qin", ""], ["Xu", "Hao", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1711.09219", "submitter": "Shuai Zhang", "authors": "Shuai Zhang, Jianxin Li, Pengtao Xie, Yingchun Zhang, Minglai Shao,\n  Haoyi Zhou, Mengyi Yan", "title": "Stacked Kernel Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are powerful tools to capture nonlinear patterns behind data.\nThey implicitly learn high (even infinite) dimensional nonlinear features in\nthe Reproducing Kernel Hilbert Space (RKHS) while making the computation\ntractable by leveraging the kernel trick. Classic kernel methods learn a single\nlayer of nonlinear features, whose representational power may be limited.\nMotivated by recent success of deep neural networks (DNNs) that learn\nmulti-layer hierarchical representations, we propose a Stacked Kernel Network\n(SKN) that learns a hierarchy of RKHS-based nonlinear features. SKN interleaves\nseveral layers of nonlinear transformations (from a linear space to a RKHS) and\nlinear transformations (from a RKHS to a linear space). Similar to DNNs, a SKN\nis composed of multiple layers of hidden units, but each parameterized by a\nRKHS function rather than a finite-dimensional vector. We propose three ways to\nrepresent the RKHS functions in SKN: (1)nonparametric representation,\n(2)parametric representation and (3)random Fourier feature representation.\nFurthermore, we expand SKN into CNN architecture called Stacked Kernel\nConvolutional Network (SKCN). SKCN learning a hierarchy of RKHS-based nonlinear\nfeatures by convolutional operation with each filter also parameterized by a\nRKHS function rather than a finite-dimensional matrix in CNN, which is suitable\nfor image inputs. Experiments on various datasets demonstrate the effectiveness\nof SKN and SKCN, which outperform the competitive methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 09:01:40 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Zhang", "Shuai", ""], ["Li", "Jianxin", ""], ["Xie", "Pengtao", ""], ["Zhang", "Yingchun", ""], ["Shao", "Minglai", ""], ["Zhou", "Haoyi", ""], ["Yan", "Mengyi", ""]]}, {"id": "1711.09224", "submitter": "Gao Huang", "authors": "Gao Huang and Shichen Liu and Laurens van der Maaten and Kilian Q.\n  Weinberger", "title": "CondenseNet: An Efficient DenseNet using Learned Group Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are increasingly used on mobile devices, where\ncomputational resources are limited. In this paper we develop CondenseNet, a\nnovel network architecture with unprecedented efficiency. It combines dense\nconnectivity with a novel module called learned group convolution. The dense\nconnectivity facilitates feature re-use in the network, whereas learned group\nconvolutions remove connections between layers for which this feature re-use is\nsuperfluous. At test time, our model can be implemented using standard group\nconvolutions, allowing for efficient computation in practice. Our experiments\nshow that CondenseNets are far more efficient than state-of-the-art compact\nconvolutional networks such as MobileNets and ShuffleNets.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 09:34:12 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 14:59:11 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Huang", "Gao", ""], ["Liu", "Shichen", ""], ["van der Maaten", "Laurens", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1711.09243", "submitter": "Zheng Linyu", "authors": "Jinqiao Wang and Ming Tang and Linyu Zheng and Jiayi Feng", "title": "On the Relations of Correlation Filter Based Trackers and Struck", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, two types of trackers, namely correlation filter based\ntracker (CF tracker) and structured output tracker (Struck), have exhibited the\nstate-of-the-art performance. However, there seems to be lack of analytic work\non their relations in the computer vision community. In this paper, we\ninvestigate two state-of-the-art CF trackers, i.e., spatial regularization\ndiscriminative correlation filter (SRDCF) and correlation filter with limited\nboundaries (CFLB), and Struck, and reveal their relations. Specifically, after\nextending the CFLB to its multiple channel version we prove the relation\nbetween SRDCF and CFLB on the condition that the spatial regularization factor\nof SRDCF is replaced by the masking matrix of CFLB. We also prove the\nasymptotical approximate relation between SRDCF and Struck on the conditions\nthat the spatial regularization factor of SRDCF is replaced by an indicator\nfunction of object bounding box, the weights of SRDCF in its loss item are\nreplaced by those of Struck, the linear kernel is employed by Struck, and the\nsearch region tends to infinity. Extensive experiments on public benchmarks\nOTB50 and OTB100 are conducted to verify our theoretical results. Moreover, we\nexplain how detailed differences among SRDCF, CFLB, and Struck would give rise\nto slightly different performances on visual sequences\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 14:44:29 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Wang", "Jinqiao", ""], ["Tang", "Ming", ""], ["Zheng", "Linyu", ""], ["Feng", "Jiayi", ""]]}, {"id": "1711.09250", "submitter": "Rishabh Dabral", "authors": "Rishabh Dabral, Anurag Mundhada, Uday Kusupati, Safeer Afaque,\n  Abhishek Sharma and Arjun Jain", "title": "Learning 3D Human Pose from Structure and Motion", "comments": "ECCV 2018. Project page: https://www.cse.iitb.ac.in/~rdabral/3DPose/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D human pose estimation from a single image is a challenging problem,\nespecially for in-the-wild settings due to the lack of 3D annotated data. We\npropose two anatomically inspired loss functions and use them with a\nweakly-supervised learning framework to jointly learn from large-scale\nin-the-wild 2D and indoor/synthetic 3D data. We also present a simple temporal\nnetwork that exploits temporal and structural cues present in predicted pose\nsequences to temporally harmonize the pose estimations. We carefully analyze\nthe proposed contributions through loss surface visualizations and sensitivity\nanalysis to facilitate deeper understanding of their working mechanism. Our\ncomplete pipeline improves the state-of-the-art by 11.8% and 12% on Human3.6M\nand MPI-INF-3DHP, respectively, and runs at 30 FPS on a commodity graphics\ncard.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 15:25:16 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 20:52:54 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Dabral", "Rishabh", ""], ["Mundhada", "Anurag", ""], ["Kusupati", "Uday", ""], ["Afaque", "Safeer", ""], ["Sharma", "Abhishek", ""], ["Jain", "Arjun", ""]]}, {"id": "1711.09265", "submitter": "Shi Zhenyu", "authors": "Yu Runsheng, Shi Zhenyu, Ma Qiongxiong, Qing Laiyun", "title": "Predictive Learning: Using Future Representation Learning Variantial\n  Autoencoder for Human Action Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unsupervised Pretraining method has been widely used in aiding human\naction recognition. However, existing methods focus on reconstructing the\nalready present frames rather than generating frames which happen in future.In\nthis paper, We propose an improved Variantial Autoencoder model to extract the\nfeatures with a high connection to the coming scenarios, also known as\nPredictive Learning. Our framework lists as following: two steam 3D-convolution\nneural networks are used to extract both spatial and temporal information as\nlatent variables. Then a resample method is introduced to create new normal\ndistribution probabilistic latent variables and finally, the deconvolution\nneural network will use these latent variables generate next frames. Through\nthis possess, we train the model to focus more on how to generate the future\nand thus it will extract the future high connected features. In the experiment\nstage, A large number of experiments on UT and UCF101 datasets reveal that\nfuture generation aids Prediction does improve the performance. Moreover, the\nFuture Representation Learning Network reach a higher score than other methods\nwhen in half observation. This means that Future Representation Learning is\nbetter than the traditional Representation Learning and other state- of-the-art\nmethods in solving the human action prediction problems to some extends.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 17:43:12 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 11:22:18 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Runsheng", "Yu", ""], ["Zhenyu", "Shi", ""], ["Qiongxiong", "Ma", ""], ["Laiyun", "Qing", ""]]}, {"id": "1711.09280", "submitter": "Siyuan Qiao", "authors": "Siyuan Qiao, Zhishuai Zhang, Wei Shen, Bo Wang, Alan Yuille", "title": "Gradually Updated Neural Networks for Large-Scale Image Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth is one of the keys that make neural networks succeed in the task of\nlarge-scale image recognition. The state-of-the-art network architectures\nusually increase the depths by cascading convolutional layers or building\nblocks. In this paper, we present an alternative method to increase the depth.\nOur method is by introducing computation orderings to the channels within\nconvolutional layers or blocks, based on which we gradually compute the outputs\nin a channel-wise manner. The added orderings not only increase the depths and\nthe learning capacities of the networks without any additional computation\ncosts, but also eliminate the overlap singularities so that the networks are\nable to converge faster and perform better. Experiments show that the networks\nbased on our method achieve the state-of-the-art performances on CIFAR and\nImageNet datasets.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 20:17:54 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 21:00:21 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Qiao", "Siyuan", ""], ["Zhang", "Zhishuai", ""], ["Shen", "Wei", ""], ["Wang", "Bo", ""], ["Yuille", "Alan", ""]]}, {"id": "1711.09312", "submitter": "Yi Fang", "authors": "Lingjing Wang and Yi Fang", "title": "Unsupervised 3D Reconstruction from a Single Image via Adversarial\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in deep learning opened new opportunities for learning a\nhigh-quality 3D model from a single 2D image given sufficient training on\nlarge-scale data sets. However, the significant imbalance between available\namount of images and 3D models, and the limited availability of labeled 2D\nimage data (i.e. manually annotated pairs between images and their\ncorresponding 3D models), severely impacts the training of most supervised deep\nlearning methods in practice. In this paper, driven by a novel design of\nadversarial networks, we have developed an unsupervised learning paradigm to\nreconstruct 3D models from a single 2D image, which is free of manually\nannotated pairwise input image and its associated 3D model. Particularly, the\nparadigm begins with training an adaption network via autoencoder with\nadversarial loss, which embeds unpaired 2D synthesized image domain with real\nworld image domain to a shared latent vector space. Then, we jointly train a 3D\ndeconvolutional network to transform the latent vector space to the 3D object\nspace together with the embedding process. Our experiments verify our network's\nrobust and superior performance in handling 3D volumetric object generation\nfrom a single 2D image.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 00:12:43 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Wang", "Lingjing", ""], ["Fang", "Yi", ""]]}, {"id": "1711.09313", "submitter": "Jameson Merkow", "authors": "Jameson Merkow and Robert Lufkin and Kim Nguyen and Stefano Soatto and\n  Zhuowen Tu and Andrea Vedaldi", "title": "DeepRadiologyNet: Radiologist Level Pathology Detection in CT Head\n  Images", "comments": "22 pages with references, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a system to automatically filter clinically significant findings\nfrom computerized tomography (CT) head scans, operating at performance levels\nexceeding that of practicing radiologists. Our system, named DeepRadiologyNet,\nbuilds on top of deep convolutional neural networks (CNNs) trained using\napproximately 3.5 million CT head images gathered from over 24,000 studies\ntaken from January 1, 2015 to August 31, 2015 and January 1, 2016 to April 30\n2016 in over 80 clinical sites. For our initial system, we identified 30\nphenomenological traits to be recognized in the CT scans. To test the system,\nwe designed a clinical trial using over 4.8 million CT head images (29,925\nstudies), completely disjoint from the training and validation set, interpreted\nby 35 US Board Certified radiologists with specialized CT head experience. We\nmeasured clinically significant error rates to ascertain whether the\nperformance of DeepRadiologyNet was comparable to or better than that of US\nBoard Certified radiologists. DeepRadiologyNet achieved a clinically\nsignificant miss rate of 0.0367% on automatically selected high-confidence\nstudies. Thus, DeepRadiologyNet enables significant reduction in the workload\nof human radiologists by automatically filtering studies and reporting on the\nhigh-confidence ones at an operating point well below the literal error rate\nfor US Board Certified radiologists, estimated at 0.82%.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 00:30:45 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 18:17:29 GMT"}, {"version": "v3", "created": "Sat, 2 Dec 2017 19:14:49 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Merkow", "Jameson", ""], ["Lufkin", "Robert", ""], ["Nguyen", "Kim", ""], ["Soatto", "Stefano", ""], ["Tu", "Zhuowen", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1711.09334", "submitter": "Pramuditha Perera", "authors": "Pramuditha Perera, Mahdi Abavisani, Vishal M. Patel", "title": "In2I : Unsupervised Multi-Image-to-Image Translation Using Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unsupervised image-to-image translation, the goal is to learn the mapping\nbetween an input image and an output image using a set of unpaired training\nimages. In this paper, we propose an extension of the unsupervised\nimage-to-image translation problem to multiple input setting. Given a set of\npaired images from multiple modalities, a transformation is learned to\ntranslate the input into a specified domain. For this purpose, we introduce a\nGenerative Adversarial Network (GAN) based framework along with a multi-modal\ngenerator structure and a new loss term, latent consistency loss. Through\nvarious experiments we show that leveraging multiple inputs generally improves\nthe visual quality of the translated images. Moreover, we show that the\nproposed method outperforms current state-of-the-art unsupervised\nimage-to-image translation methods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 04:57:24 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Perera", "Pramuditha", ""], ["Abavisani", "Mahdi", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1711.09345", "submitter": "Pengpeng Liu", "authors": "Pengpeng Liu, Xiaojuan Qi, Pinjia He, Yikang Li, Michael R. Lyu, Irwin\n  King", "title": "Semantically Consistent Image Completion with Fine-grained Details", "comments": "9 pages plus 2-page supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image completion has achieved significant progress due to advances in\ngenerative adversarial networks (GANs). Albeit natural-looking, the synthesized\ncontents still lack details, especially for scenes with complex structures or\nimages with large holes. This is because there exists a gap between low-level\nreconstruction loss and high-level adversarial loss. To address this issue, we\nintroduce a perceptual network to provide mid-level guidance, which measures\nthe semantical similarity between the synthesized and original contents in a\nsimilarity-enhanced space. We conduct a detailed analysis on the effects of\ndifferent losses and different levels of perceptual features in image\ncompletion, showing that there exist complementarity between adversarial\ntraining and perceptual features. By combining them together, our model can\nachieve nearly seamless fusion results in an end-to-end manner. Moreover, we\ndesign an effective lightweight generator architecture, which can achieve\neffective image inpainting with far less parameters. Evaluated on CelebA Face\nand Paris StreetView dataset, our proposed method significantly outperforms\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 07:42:17 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Liu", "Pengpeng", ""], ["Qi", "Xiaojuan", ""], ["He", "Pinjia", ""], ["Li", "Yikang", ""], ["Lyu", "Michael R.", ""], ["King", "Irwin", ""]]}, {"id": "1711.09347", "submitter": "Xi Zhang", "authors": "Xi Zhang, Siyu Zhou, Jiashi Feng, Hanjiang Lai, Bo Li, Yan Pan, Jian\n  Yin, Shuicheng Yan", "title": "HashGAN:Attention-aware Deep Adversarial Hashing for Cross Modal\n  Retrieval", "comments": "10 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the rapid growth of multi-modal data, hashing methods for cross-modal\nretrieval have received considerable attention. Deep-networks-based cross-modal\nhashing methods are appealing as they can integrate feature learning and hash\ncoding into end-to-end trainable frameworks. However, it is still challenging\nto find content similarities between different modalities of data due to the\nheterogeneity gap. To further address this problem, we propose an adversarial\nhashing network with attention mechanism to enhance the measurement of content\nsimilarities by selectively focusing on informative parts of multi-modal data.\nThe proposed new adversarial network, HashGAN, consists of three building\nblocks: 1) the feature learning module to obtain feature representations, 2)\nthe generative attention module to generate an attention mask, which is used to\nobtain the attended (foreground) and the unattended (background) feature\nrepresentations, 3) the discriminative hash coding module to learn hash\nfunctions that preserve the similarities between different modalities. In our\nframework, the generative module and the discriminative module are trained in\nan adversarial way: the generator is learned to make the discriminator cannot\npreserve the similarities of multi-modal data w.r.t. the background feature\nrepresentations, while the discriminator aims to preserve the similarities of\nmulti-modal data w.r.t. both the foreground and the background feature\nrepresentations. Extensive evaluations on several benchmark datasets\ndemonstrate that the proposed HashGAN brings substantial improvements over\nother state-of-the-art cross-modal hashing methods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 08:14:13 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Zhang", "Xi", ""], ["Zhou", "Siyu", ""], ["Feng", "Jiashi", ""], ["Lai", "Hanjiang", ""], ["Li", "Bo", ""], ["Pan", "Yan", ""], ["Yin", "Jian", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1711.09349", "submitter": "Sun Yifan", "authors": "Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, Shengjin Wang", "title": "Beyond Part Models: Person Retrieval with Refined Part Pooling (and a\n  Strong Convolutional Baseline)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Employing part-level features for pedestrian image description offers\nfine-grained information and has been verified as beneficial for person\nretrieval in very recent literature. A prerequisite of part discovery is that\neach part should be well located. Instead of using external cues, e.g., pose\nestimation, to directly locate parts, this paper lays emphasis on the content\nconsistency within each part.\n  Specifically, we target at learning discriminative part-informed features for\nperson retrieval and make two contributions. (i) A network named Part-based\nConvolutional Baseline (PCB). Given an image input, it outputs a convolutional\ndescriptor consisting of several part-level features. With a uniform partition\nstrategy, PCB achieves competitive results with the state-of-the-art methods,\nproving itself as a strong convolutional baseline for person retrieval.\n  (ii) A refined part pooling (RPP) method. Uniform partition inevitably incurs\noutliers in each part, which are in fact more similar to other parts. RPP\nre-assigns these outliers to the parts they are closest to, resulting in\nrefined parts with enhanced within-part consistency. Experiment confirms that\nRPP allows PCB to gain another round of performance boost. For instance, on the\nMarket-1501 dataset, we achieve (77.4+4.2)% mAP and (92.3+1.5)% rank-1\naccuracy, surpassing the state of the art by a large margin.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 08:44:53 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 04:34:28 GMT"}, {"version": "v3", "created": "Tue, 9 Jan 2018 07:11:53 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Sun", "Yifan", ""], ["Zheng", "Liang", ""], ["Yang", "Yi", ""], ["Tian", "Qi", ""], ["Wang", "Shengjin", ""]]}, {"id": "1711.09352", "submitter": "Hisashi Shimodaira", "authors": "Hisashi Shimodaira", "title": "Automatic Color Image Segmentation Using a Square Elemental Region-Based\n  Seeded Region Growing and Merging Method", "comments": "14 pages with 9 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient automatic color image segmentation method\nusing a seeded region growing and merging method based on square elemental\nregions. Our segmentation method consists of the three steps: generating seed\nregions, merging the regions, and applying a pixel-wise boundary determination\nalgorithm to the resultant polygonal regions. The major features of our method\nare as follows: the use of square elemental regions instead of pixels as the\nprocessing unit, a seed generation method based on enhanced gradient values, a\nseed region growing method exploiting local gradient values, a region merging\nmethod using a similarity measure including a homogeneity distance based on\nTsallis entropy, and a termination condition of region merging using an\nestimated desired number of regions. Using square regions as the processing\nunit substantially reduces the time complexity of the algorithm and makes the\nperformance stable. The experimental results show that our method exhibits\nstable performance for a variety of natural images, including heavily textured\nareas, and produces good segmentation results using the same parameter values.\nThe results of our method are fairly comparable to, and in some respects better\nthan, those of existing algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 09:19:05 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Shimodaira", "Hisashi", ""]]}, {"id": "1711.09358", "submitter": "Qingjie Liu", "authors": "Qiang Chen, Yunhong Wang, Zheng Liu, Qingjie Liu and Di Huang", "title": "Feature Map Pooling for Cross-View Gait Recognition Based on Silhouette\n  Sequence Images", "comments": "Accepted to IJCB2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a novel convolutional neural network based approach\nto extract and aggregate useful information from gait silhouette sequence\nimages instead of simply representing the gait process by averaging silhouette\nimages. The network takes a pair of arbitrary length sequence images as inputs\nand extracts features for each silhouette independently. Then a feature map\npooling strategy is adopted to aggregate sequence features. Subsequently, a\nnetwork which is similar to Siamese network is designed to perform recognition.\nThe proposed network is simple and easy to implement and can be trained in an\nend-to-end manner. Cross-view gait recognition experiments are conducted on\nOU-ISIR large population dataset. The results demonstrate that our network can\nextract and aggregate features from silhouette sequence effectively. It also\nachieves significant equal error rates and comparable identification rates when\ncompared with the state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 09:48:39 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Chen", "Qiang", ""], ["Wang", "Yunhong", ""], ["Liu", "Zheng", ""], ["Liu", "Qingjie", ""], ["Huang", "Di", ""]]}, {"id": "1711.09368", "submitter": "Siyu Zhou", "authors": "Siyu Zhou, Weiqiang Zhao, Jiashi Feng, Hanjiang Lai, Yan Pan, Jian\n  Yin, Shuicheng Yan", "title": "Personalized and Occupational-aware Age Progression by Generative\n  Adversarial Networks", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face age progression, which aims to predict the future looks, is important\nfor various applications and has been received considerable attentions.\nExisting methods and datasets are limited in exploring the effects of\noccupations which may influence the personal appearances. In this paper, we\nfirstly introduce an occupational face aging dataset for studying the\ninfluences of occupations on the appearances. It includes five occupations,\nwhich enables the development of new algorithms for age progression and\nfacilitate future researches. Second, we propose a new occupational-aware\nadversarial face aging network, which learns human aging process under\ndifferent occupations. Two factors are taken into consideration in our aging\nprocess: personality-preserving and visually plausible texture change for\ndifferent occupations. We propose personalized network with personalized loss\nin deep autoencoder network for keeping personalized facial characteristics,\nand occupational-aware adversarial network with occupational-aware adversarial\nloss for obtaining more realistic texture changes. Experimental results well\ndemonstrate the advantages of the proposed method by comparing with other\nstate-of-the-arts age progression methods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 10:50:56 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 06:58:03 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Zhou", "Siyu", ""], ["Zhao", "Weiqiang", ""], ["Feng", "Jiashi", ""], ["Lai", "Hanjiang", ""], ["Pan", "Yan", ""], ["Yin", "Jian", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1711.09375", "submitter": "Byeungwoo Jeon", "authors": "Khanh Quoc Dinh, Thuong Nguyen Canh, and Byeungwoo Jeon", "title": "Compressive Sensing of Color Images Using Nonlocal Higher Order\n  Dictionary", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses an ill-posed problem of recovering a color image from\nits compressively sensed measurement data. Differently from the typical 1D\nvector-based approach of the state-of-the-art methods, we exploit the nonlocal\nsimilarities inherently existing in images by treating each patch of a color\nimage as a 3D tensor consisting of not only horizontal and vertical but also\nspectral dimensions. A group of nonlocal similar patches form a 4D tensor for\nwhich a nonlocal higher order dictionary is learned via higher order singular\nvalue decomposition. The multiple sub-dictionaries contained in the higher\norder dictionary decorrelate the group in each corresponding dimension, thus\nhelp the detail of color images to be reconstructed better. Furthermore, we\npromote sparsity of the final solution using a sparsity regularization based on\na weight tensor. It can distinguish those coefficients of the sparse\nrepresentation generated by the higher order dictionary which are expected to\nhave large magnitude from the others in the optimization. Accordingly, in the\niterative solution, it acts like a weighting process which is designed by\napproximating the minimum mean squared error filter for more faithful recovery.\nExperimental results confirm improvement by the proposed method over the\nstate-of-the-art ones.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 12:31:24 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Dinh", "Khanh Quoc", ""], ["Canh", "Thuong Nguyen", ""], ["Jeon", "Byeungwoo", ""]]}, {"id": "1711.09404", "submitter": "Andrew Ross", "authors": "Andrew Slavin Ross, Finale Doshi-Velez", "title": "Improving the Adversarial Robustness and Interpretability of Deep Neural\n  Networks by Regularizing their Input Gradients", "comments": "To appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have proven remarkably effective at solving many\nclassification problems, but have been criticized recently for two major\nweaknesses: the reasons behind their predictions are uninterpretable, and the\npredictions themselves can often be fooled by small adversarial perturbations.\nThese problems pose major obstacles for the adoption of neural networks in\ndomains that require security or transparency. In this work, we evaluate the\neffectiveness of defenses that differentiably penalize the degree to which\nsmall changes in inputs can alter model predictions. Across multiple attacks,\narchitectures, defenses, and datasets, we find that neural networks trained\nwith this input gradient regularization exhibit robustness to transferred\nadversarial examples generated to fool all of the other models. We also find\nthat adversarial examples generated to fool gradient-regularized models fool\nall other models equally well, and actually lead to more \"legitimate,\"\ninterpretable misclassifications as rated by people (which we confirm in a\nhuman subject experiment). Finally, we demonstrate that regularizing input\ngradients makes them more naturally interpretable as rationales for model\npredictions. We conclude by discussing this relationship between\ninterpretability and robustness in deep neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 15:20:46 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Ross", "Andrew Slavin", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1711.09405", "submitter": "Lei Liu", "authors": "Lei Liu, Zongxu Pan and Bin Lei", "title": "Learning a Rotation Invariant Detector with Rotatable Bounding Box", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of arbitrarily rotated objects is a challenging task due to the\ndifficulties of locating the multi-angle objects and separating them\neffectively from the background. The existing methods are not robust to angle\nvaries of the objects because of the use of traditional bounding box, which is\na rotation variant structure for locating rotated objects. In this article, a\nnew detection method is proposed which applies the newly defined rotatable\nbounding box (RBox). The proposed detector (DRBox) can effectively handle the\nsituation where the orientation angles of the objects are arbitrary. The\ntraining of DRBox forces the detection networks to learn the correct\norientation angle of the objects, so that the rotation invariant property can\nbe achieved. DRBox is tested to detect vehicles, ships and airplanes on\nsatellite images, compared with Faster R-CNN and SSD, which are chosen as the\nbenchmark of the traditional bounding box based methods. The results shows that\nDRBox performs much better than traditional bounding box based methods do on\nthe given tasks, and is more robust against rotation of input image and target\nobjects. Besides, results show that DRBox correctly outputs the orientation\nangles of the objects, which is very useful for locating multi-angle objects\nefficiently. The code and models are available at\nhttps://github.com/liulei01/DRBox.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 15:20:52 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Liu", "Lei", ""], ["Pan", "Zongxu", ""], ["Lei", "Bin", ""]]}, {"id": "1711.09414", "submitter": "Boyu Liu", "authors": "Boyu Liu, Yanzhao Wang, Yu-Wing Tai, Chi-Keung Tang", "title": "MAVOT: Memory-Augmented Video Object Tracking", "comments": "Submitted to CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a one-shot learning approach for video object tracking. The\nproposed algorithm requires seeing the object to be tracked only once, and\nemploys an external memory to store and remember the evolving features of the\nforeground object as well as backgrounds over time during tracking. With the\nrelevant memory retrieved and updated in each tracking, our tracking model is\ncapable of maintaining long-term memory of the object, and thus can naturally\ndeal with hard tracking scenarios including partial and total occlusion, motion\nchanges and large scale and shape variations. In our experiments we use the\nImageNet ILSVRC2015 video detection dataset to train and use the VOT-2016\nbenchmark to test and compare our Memory-Augmented Video Object Tracking\n(MAVOT) model. From the results, we conclude that given its oneshot property\nand simplicity in design, MAVOT is an attractive approach in visual tracking\nbecause it shows good performance on VOT-2016 benchmark and is among the top 5\nperformers in accuracy and robustness in occlusion, motion changes and empty\ntarget.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 16:20:45 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Liu", "Boyu", ""], ["Wang", "Yanzhao", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1711.09432", "submitter": "James Pritts", "authors": "James Pritts, Denys Rozumnyi, M. Pawan Kumar, Ondrej Chum", "title": "Coplanar Repeats by Energy Minimization", "comments": "14 pages with supplemental materials attached", "journal-ref": "Proceedings of the British Machine Vision Conference (BMVC) 2016", "doi": "10.5244/C.30.107", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an automated method to detect, group and rectify\narbitrarily-arranged coplanar repeated elements via energy minimization. The\nproposed energy functional combines several features that model how planes with\ncoplanar repeats are projected into images and captures global interactions\nbetween different coplanar repeat groups and scene planes. An inference\nframework based on a recent variant of $\\alpha$-expansion is described and fast\nconvergence is demonstrated. We compare the proposed method to two widely-used\ngeometric multi-model fitting methods using a new dataset of annotated images\ncontaining multiple scene planes with coplanar repeats in varied arrangements.\nThe evaluation shows a significant improvement in the accuracy of\nrectifications computed from coplanar repeats detected with the proposed method\nversus those detected with the baseline methods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 17:56:08 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Pritts", "James", ""], ["Rozumnyi", "Denys", ""], ["Kumar", "M. Pawan", ""], ["Chum", "Ondrej", ""]]}, {"id": "1711.09464", "submitter": "Iuliia Kotseruba", "authors": "Iuliia Kotseruba, John K. Tsotsos", "title": "STAR-RT: Visual attention for real-time video game playing", "comments": "21 page, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present STAR-RT - the first working prototype of Selective\nTuning Attention Reference (STAR) model and Cognitive Programs (CPs). The\nSelective Tuning (ST) model received substantial support through psychological\nand neurophysiological experiments. The STAR framework expands ST and applies\nit to practical visual tasks. In order to do so, similarly to many cognitive\narchitectures, STAR combines the visual hierarchy (based on ST) with the\nexecutive controller, working and short-term memory components and fixation\ncontroller. CPs in turn enable the communication among all these elements for\nvisual task execution. To test the relevance of the system in a realistic\ncontext, we implemented the necessary components of STAR and designed CPs for\nplaying two closed-source video games - Canabaltand Robot Unicorn Attack. Since\nboth games run in a browser window, our algorithm has the same amount of\ninformation and the same amount of time to react to the events on the screen as\na human player would. STAR-RT plays both games in real time using only visual\ninput and achieves scores comparable to human expert players. It thus provides\nan existence proof for the utility of the particular CP structure and\nprimitives used and the potential for continued experimentation and\nverification of their utility in broader scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 21:24:52 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Kotseruba", "Iuliia", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1711.09482", "submitter": "Housam Khalifa Bashier Babiker", "authors": "Housam Khalifa Bashier Babiker and Randy Goebel", "title": "An Introduction to Deep Visual Explanation", "comments": "Accepted at NIPS 2017 - Workshop Interpreting, Explaining and\n  Visualizing Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practical impact of deep learning on complex supervised learning problems\nhas been significant, so much so that almost every Artificial Intelligence\nproblem, or at least a portion thereof, has been somehow recast as a deep\nlearning problem. The applications appeal is significant, but this appeal is\nincreasingly challenged by what some call the challenge of explainability, or\nmore generally the more traditional challenge of debuggability: if the outcomes\nof a deep learning process produce unexpected results (e.g., less than expected\nperformance of a classifier), then there is little available in the way of\ntheories or tools to help investigate the potential causes of such unexpected\nbehavior, especially when this behavior could impact people's lives. We\ndescribe a preliminary framework to help address this issue, which we call\n\"deep visual explanation\" (DVE). \"Deep,\" because it is the development and\nperformance of deep neural network models that we want to understand. \"Visual,\"\nbecause we believe that the most rapid insight into a complex multi-dimensional\nmodel is provided by appropriate visualization techniques, and \"Explanation,\"\nbecause in the spectrum from instrumentation by inserting print statements to\nthe abductive inference of explanatory hypotheses, we believe that the key to\nunderstanding deep learning relies on the identification and exposure of\nhypotheses about the performance behavior of a learned deep model. In the\nexposition of our preliminary framework, we use relatively straightforward\nimage classification examples and a variety of choices on initial configuration\nof a deep model building scenario. By careful but not complicated\ninstrumentation, we expose classification outcomes of deep models using\nvisualization, and also show initial results for one potential application of\ninterpretability.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 22:54:18 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 19:18:33 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Babiker", "Housam Khalifa Bashier", ""], ["Goebel", "Randy", ""]]}, {"id": "1711.09485", "submitter": "Xin Wang", "authors": "Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, Joseph E. Gonzalez", "title": "SkipNet: Learning Dynamic Routing in Convolutional Networks", "comments": "ECCV 2018 Camera ready version. Code is available at\n  https://github.com/ucbdrive/skipnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deeper convolutional networks are needed to achieve maximum accuracy in\nvisual perception tasks, for many inputs shallower networks are sufficient. We\nexploit this observation by learning to skip convolutional layers on a\nper-input basis. We introduce SkipNet, a modified residual network, that uses a\ngating network to selectively skip convolutional blocks based on the\nactivations of the previous layer. We formulate the dynamic skipping problem in\nthe context of sequential decision making and propose a hybrid learning\nalgorithm that combines supervised learning and reinforcement learning to\naddress the challenges of non-differentiable skipping decisions. We show\nSkipNet reduces computation by 30-90% while preserving the accuracy of the\noriginal model on four benchmark datasets and outperforms the state-of-the-art\ndynamic networks and static compression methods. We also qualitatively evaluate\nthe gating policy to reveal a relationship between image scale and saliency and\nthe number of layers skipped.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 23:03:37 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 06:13:24 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Wang", "Xin", ""], ["Yu", "Fisher", ""], ["Dou", "Zi-Yi", ""], ["Darrell", "Trevor", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1711.09492", "submitter": "Praneeth Narayanamurthy", "authors": "Namrata Vaswani, Thierry Bouwmans, Sajid Javed, and Praneeth\n  Narayanamurthy", "title": "Robust Subspace Learning: Robust PCA, Robust Subspace Tracking, and\n  Robust Subspace Recovery", "comments": "To appear, IEEE Signal Processing Magazine, July 2018", "journal-ref": "IEEE Signal Processing Magazine (Volume: 35, Issue: 4, July 2018)", "doi": "10.1109/MSP.2018.2826566", "report-no": null, "categories": "cs.IT cs.CV math.IT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PCA is one of the most widely used dimension reduction techniques. A related\neasier problem is \"subspace learning\" or \"subspace estimation\". Given\nrelatively clean data, both are easily solved via singular value decomposition\n(SVD). The problem of subspace learning or PCA in the presence of outliers is\ncalled robust subspace learning or robust PCA (RPCA). For long data sequences,\nif one tries to use a single lower dimensional subspace to represent the data,\nthe required subspace dimension may end up being quite large. For such data, a\nbetter model is to assume that it lies in a low-dimensional subspace that can\nchange over time, albeit gradually. The problem of tracking such data (and the\nsubspaces) while being robust to outliers is called robust subspace tracking\n(RST). This article provides a magazine-style overview of the entire field of\nrobust subspace learning and tracking. In particular solutions for three\nproblems are discussed in detail: RPCA via sparse+low-rank matrix decomposition\n(S+LR), RST via S+LR, and \"robust subspace recovery (RSR)\". RSR assumes that an\nentire data vector is either an outlier or an inlier. The S+LR formulation\ninstead assumes that outliers occur on only a few data vector indices and hence\nare well modeled as sparse corruptions.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 23:52:53 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 23:33:54 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 21:49:47 GMT"}, {"version": "v4", "created": "Thu, 5 Jul 2018 22:46:31 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Vaswani", "Namrata", ""], ["Bouwmans", "Thierry", ""], ["Javed", "Sajid", ""], ["Narayanamurthy", "Praneeth", ""]]}, {"id": "1711.09501", "submitter": "Yuchao Dai Dr.", "authors": "Liyuan Pan, Yuchao Dai, Miaomiao Liu, Fatih Porikli", "title": "Depth Map Completion by Jointly Exploiting Blurry Color Images and\n  Sparse Depth Maps", "comments": "Accepted by WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim at predicting a complete and high-resolution depth map from\nincomplete, sparse and noisy depth measurements. Existing methods handle this\nproblem either by exploiting various regularizations on the depth maps directly\nor resorting to learning based methods. When the corresponding color images are\navailable, the correlation between the depth maps and the color images are used\nto improve the completion performance, assuming the color images are clean and\nsharp. However, in real world dynamic scenes, color images are often blurry due\nto the camera motion and the moving objects in the scene. In this paper, we\npropose to tackle the problem of depth map completion by jointly exploiting the\nblurry color image sequences and the sparse depth map measurements, and present\nan energy minimization based formulation to simultaneously complete the depth\nmaps, estimate the scene flow and deblur the color images. Our experimental\nevaluations on both outdoor and indoor scenarios demonstrate the\nstate-of-the-art performance of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 01:25:43 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Pan", "Liyuan", ""], ["Dai", "Yuchao", ""], ["Liu", "Miaomiao", ""], ["Porikli", "Fatih", ""]]}, {"id": "1711.09509", "submitter": "Ryota Hinami", "authors": "Ryota Hinami, Shin'ichi Satoh", "title": "Discriminative Learning of Open-Vocabulary Object Retrieval and\n  Localization by Negative Phrase Augmentation", "comments": "Accepted to EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the success of object detection technology, we can retrieve objects\nof the specified classes even from huge image collections. However, the current\nstate-of-the-art object detectors (such as Faster R-CNN) can only handle\npre-specified classes. In addition, large amounts of positive and negative\nvisual samples are required for training. In this paper, we address the problem\nof open-vocabulary object retrieval and localization, where the target object\nis specified by a textual query (e.g., a word or phrase). We first propose\nQuery-Adaptive R-CNN, a simple extension of Faster R-CNN adapted to\nopen-vocabulary queries, by transforming the text embedding vector into an\nobject classifier and localization regressor. Then, for discriminative\ntraining, we then propose negative phrase augmentation (NPA) to mine hard\nnegative samples which are visually similar to the query and at the same time\nsemantically mutually exclusive of the query. The proposed method can retrieve\nand localize objects specified by a textual query from one million images in\nonly 0.5 seconds with high precision.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 02:25:34 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 06:50:38 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Hinami", "Ryota", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "1711.09513", "submitter": "Guangfeng Lin", "authors": "Guangfeng Lin, Yajun Chen, Fan Zhao", "title": "Structure propagation for zero-shot learning", "comments": null, "journal-ref": "Chinese Conference on Pattern Recognition and Computer Vision\n  (PRCV) 2018", "doi": "10.1007/978-3-030-03338-5_39", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key of zero-shot learning (ZSL) is how to find the information transfer\nmodel for bridging the gap between images and semantic information (texts or\nattributes). Existing ZSL methods usually construct the compatibility function\nbetween images and class labels with the consideration of the relevance on the\nsemantic classes (the manifold structure of semantic classes). However, the\nrelationship of image classes (the manifold structure of image classes) is also\nvery important for the compatibility model construction. It is difficult to\ncapture the relationship among image classes due to unseen classes, so that the\nmanifold structure of image classes often is ignored in ZSL. To complement each\nother between the manifold structure of image classes and that of semantic\nclasses information, we propose structure propagation (SP) for improving the\nperformance of ZSL for classification. SP can jointly consider the manifold\nstructure of image classes and that of semantic classes for approximating to\nthe intrinsic structure of object classes. Moreover, the SP can describe the\nconstrain condition between the compatibility function and these manifold\nstructures for balancing the influence of the structure propagation iteration.\nThe SP solution provides not only unseen class labels but also the relationship\nof two manifold structures that encode the positive transfer in structure\npropagation. Experimental results demonstrate that SP can attain the promising\nresults on the AwA, CUB, Dogs and SUN databases.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 02:51:58 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Lin", "Guangfeng", ""], ["Chen", "Yajun", ""], ["Zhao", "Fan", ""]]}, {"id": "1711.09515", "submitter": "Lingxiao Wang", "authors": "Lingxiao Wang, Yali Li, Shengjin Wang", "title": "DeepDeblur: Fast one-step blurry face images restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a very fast and effective one-step restoring method for blurry\nface images. In the last decades, many blind deblurring algorithms have been\nproposed to restore latent sharp images. However, these algorithms run slowly\nbecause of involving two steps: kernel estimation and following non-blind\ndeconvolution or latent image estimation. Also they cannot handle face images\nin small size. Our proposed method restores sharp face images directly in one\nstep using Convolutional Neural Network. Unlike previous deep learning involved\nmethods that can only handle a single blur kernel at one time, our network is\ntrained on totally random and numerous training sample pairs to deal with the\nvariances due to different blur kernels in practice. A smoothness\nregularization as well as a facial regularization are added to keep facial\nidentity information which is the key to face image applications. Comprehensive\nexperiments demonstrate that our proposed method can handle various blur kenels\nand achieve state-of-the-art results for small size blurry face images\nrestoration. Moreover, the proposed method shows significant improvement in\nface recognition accuracy along with increasing running speed by more than 100\ntimes.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 02:52:38 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Wang", "Lingxiao", ""], ["Li", "Yali", ""], ["Wang", "Shengjin", ""]]}, {"id": "1711.09528", "submitter": "Daesik Kim", "authors": "Daesik Kim, Youngjoon Yoo, Jeesoo Kim, Sangkuk Lee, Nojun Kwak", "title": "Dynamic Graph Generation Network: Generating Relational Knowledge from\n  Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a new algorithm for analyzing a diagram, which\ncontains visual and textual information in an abstract and integrated way.\nWhereas diagrams contain richer information compared with individual\nimage-based or language-based data, proper solutions for automatically\nunderstanding them have not been proposed due to their innate characteristics\nof multi-modality and arbitrariness of layouts. To tackle this problem, we\npropose a unified diagram-parsing network for generating knowledge from\ndiagrams based on an object detector and a recurrent neural network designed\nfor a graphical structure. Specifically, we propose a dynamic graph-generation\nnetwork that is based on dynamic memory and graph theory. We explore the\ndynamics of information in a diagram with activation of gates in gated\nrecurrent unit (GRU) cells. On publicly available diagram datasets, our model\ndemonstrates a state-of-the-art result that outperforms other baselines.\nMoreover, further experiments on question answering shows potentials of the\nproposed method for various applications.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 04:32:21 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Kim", "Daesik", ""], ["Yoo", "Youngjoon", ""], ["Kim", "Jeesoo", ""], ["Lee", "Sangkuk", ""], ["Kwak", "Nojun", ""]]}, {"id": "1711.09539", "submitter": "Zhenyu He", "authors": "Xin Li, Qiao Liu, Nana Fan, Zhenyu He, Hongzhi Wang", "title": "Hierarchical Spatial-aware Siamese Network for Thermal Infrared Object\n  Tracking", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most thermal infrared (TIR) tracking methods are discriminative, treating the\ntracking problem as a classification task. However, the objective of the\nclassifier (label prediction) is not coupled to the objective of the tracker\n(location estimation). The classification task focuses on the between-class\ndifference of the arbitrary objects, while the tracking task mainly deals with\nthe within-class difference of the same objects. In this paper, we cast the TIR\ntracking problem as a similarity verification task, which is coupled well to\nthe objective of the tracking task. We propose a TIR tracker via a Hierarchical\nSpatial-aware Siamese Convolutional Neural Network (CNN), named HSSNet. To\nobtain both spatial and semantic features of the TIR object, we design a\nSiamese CNN that coalesces the multiple hierarchical convolutional layers.\nThen, we propose a spatial-aware network to enhance the discriminative ability\nof the coalesced hierarchical feature. Subsequently, we train this network end\nto end on a large visible video detection dataset to learn the similarity\nbetween paired objects before we transfer the network into the TIR domain.\nNext, this pre-trained Siamese network is used to evaluate the similarity\nbetween the target template and target candidates. Finally, we locate the\ncandidate that is most similar to the tracked target. Extensive experimental\nresults on the benchmarks VOT-TIR 2015 and VOT-TIR 2016 show that our proposed\nmethod achieves favourable performance compared to the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 05:15:59 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 09:44:40 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Li", "Xin", ""], ["Liu", "Qiao", ""], ["Fan", "Nana", ""], ["He", "Zhenyu", ""], ["Wang", "Hongzhi", ""]]}, {"id": "1711.09550", "submitter": "Chuang Gan", "authors": "Xiang Long, Chuang Gan, Gerard de Melo, Jiajun Wu, Xiao Liu, Shilei\n  Wen", "title": "Attention Clusters: Purely Attention Based Local Feature Integration for\n  Video Classification", "comments": "The backbone of the winner solution at ActivityNet Kinetics Challenge\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, substantial research effort has focused on how to apply CNNs or\nRNNs to better extract temporal patterns from videos, so as to improve the\naccuracy of video classification. In this paper, however, we show that temporal\ninformation, especially longer-term patterns, may not be necessary to achieve\ncompetitive results on common video classification datasets. We investigate the\npotential of a purely attention based local feature integration. Accounting for\nthe characteristics of such features in video classification, we propose a\nlocal feature integration framework based on attention clusters, and introduce\na shifting operation to capture more diverse signals. We carefully analyze and\ncompare the effect of different attention mechanisms, cluster sizes, and the\nuse of the shifting operation, and also investigate the combination of\nattention clusters for multimodal integration. We demonstrate the effectiveness\nof our framework on three real-world video classification datasets. Our model\nachieves competitive results across all of these. In particular, on the\nlarge-scale Kinetics dataset, our framework obtains an excellent single model\naccuracy of 79.4% in terms of the top-1 and 94.0% in terms of the top-5\naccuracy on the validation set. The attention clusters are the backbone of our\nwinner solution at ActivityNet Kinetics Challenge 2017. Code and models will be\nreleased soon.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 06:16:14 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Long", "Xiang", ""], ["Gan", "Chuang", ""], ["de Melo", "Gerard", ""], ["Wu", "Jiajun", ""], ["Liu", "Xiao", ""], ["Wen", "Shilei", ""]]}, {"id": "1711.09553", "submitter": "Tuan N.A. Hoang", "authors": "T.-T. Do and T. Hoang and V. Pomponiu and Y. Zhou and Z. Chen and\n  N.-M. Cheung and D. Koh and A. Tan and S.-H. Tan", "title": "Accessible Melanoma Detection using Smartphones and Mobile Image\n  Analysis", "comments": "Accepted to IEEE Trans. on Multimedia, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the design of an entire mobile imaging system for early\ndetection of melanoma. Different from previous work, we focus on\nsmartphone-captured visible light images. Our design addresses two major\nchallenges. First, images acquired using a smartphone under loosely-controlled\nenvironmental conditions may be subject to various distortions, and this makes\nmelanoma detection more difficult. Second, processing performed on a smartphone\nis subject to stringent computation and memory constraints. In our work, we\npropose a detection system that is optimized to run entirely on the\nresource-constrained smartphone. Our system intends to localize the skin lesion\nby combining a lightweight method for skin detection with a hierarchical\nsegmentation approach using two fast segmentation methods. Moreover, we study\nan extensive set of image features and propose new numerical features to\ncharacterize a skin lesion. Furthermore, we propose an improved feature\nselection algorithm to determine a small set of discriminative features used by\nthe final lightweight system. In addition, we study the human-computer\ninterface (HCI) design to understand the usability and acceptance issues of the\nproposed system.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 06:31:23 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 11:29:58 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Do", "T. -T.", ""], ["Hoang", "T.", ""], ["Pomponiu", "V.", ""], ["Zhou", "Y.", ""], ["Chen", "Z.", ""], ["Cheung", "N. -M.", ""], ["Koh", "D.", ""], ["Tan", "A.", ""], ["Tan", "S. -H.", ""]]}, {"id": "1711.09554", "submitter": "Chao Wang", "authors": "Chao Wang and Haiyong Zheng and Zhibin Yu and Ziqiang Zheng and\n  Zhaorui Gu and Bing Zheng", "title": "Discriminative Region Proposal Adversarial Networks for High-Quality\n  Image-to-Image Translation", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-to-image translation has been made much progress with embracing\nGenerative Adversarial Networks (GANs). However, it's still very challenging\nfor translation tasks that require high quality, especially at high-resolution\nand photorealism. In this paper, we present Discriminative Region Proposal\nAdversarial Networks (DRPAN) for high-quality image-to-image translation. We\ndecompose the procedure of image-to-image translation task into three iterated\nsteps, first is to generate an image with global structure but some local\nartifacts (via GAN), second is using our DRPnet to propose the most fake region\nfrom the generated image, and third is to implement \"image inpainting\" on the\nmost fake region for more realistic result through a reviser, so that the\nsystem (DRPAN) can be gradually optimized to synthesize images with more\nattention on the most artifact local part. Experiments on a variety of\nimage-to-image translation tasks and datasets validate that our method\noutperforms state-of-the-arts for producing high-quality translation results in\nterms of both human perceptual studies and automatic quantitative measures.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 06:31:24 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 13:12:55 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2018 15:26:44 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Wang", "Chao", ""], ["Zheng", "Haiyong", ""], ["Yu", "Zhibin", ""], ["Zheng", "Ziqiang", ""], ["Gu", "Zhaorui", ""], ["Zheng", "Bing", ""]]}, {"id": "1711.09561", "submitter": "Emad Barsoum", "authors": "Emad Barsoum, John Kender and Zicheng Liu", "title": "HP-GAN: Probabilistic 3D human motion prediction via GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting and understanding human motion dynamics has many applications,\nsuch as motion synthesis, augmented reality, security, and autonomous vehicles.\nDue to the recent success of generative adversarial networks (GAN), there has\nbeen much interest in probabilistic estimation and synthetic data generation\nusing deep neural network architectures and learning algorithms.\n  We propose a novel sequence-to-sequence model for probabilistic human motion\nprediction, trained with a modified version of improved Wasserstein generative\nadversarial networks (WGAN-GP), in which we use a custom loss function designed\nfor human motion prediction. Our model, which we call HP-GAN, learns a\nprobability density function of future human poses conditioned on previous\nposes. It predicts multiple sequences of possible future human poses, each from\nthe same input sequence but a different vector z drawn from a random\ndistribution. Furthermore, to quantify the quality of the non-deterministic\npredictions, we simultaneously train a motion-quality-assessment model that\nlearns the probability that a given skeleton sequence is a real human motion.\n  We test our algorithm on two of the largest skeleton datasets: NTURGB-D and\nHuman3.6M. We train our model on both single and multiple action types. Its\npredictive power for long-term motion estimation is demonstrated by generating\nmultiple plausible futures of more than 30 frames from just 10 frames of input.\nWe show that most sequences generated from the same input have more than 50\\%\nprobabilities of being judged as a real human sequence. We will release all the\ncode used in this paper to Github.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 07:07:11 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Barsoum", "Emad", ""], ["Kender", "John", ""], ["Liu", "Zicheng", ""]]}, {"id": "1711.09577", "submitter": "Kensho Hara", "authors": "Kensho Hara, Hirokatsu Kataoka, Yutaka Satoh", "title": "Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to determine whether current video datasets have\nsufficient data for training very deep convolutional neural networks (CNNs)\nwith spatio-temporal three-dimensional (3D) kernels. Recently, the performance\nlevels of 3D CNNs in the field of action recognition have improved\nsignificantly. However, to date, conventional research has only explored\nrelatively shallow 3D architectures. We examine the architectures of various 3D\nCNNs from relatively shallow to very deep ones on current video datasets. Based\non the results of those experiments, the following conclusions could be\nobtained: (i) ResNet-18 training resulted in significant overfitting for\nUCF-101, HMDB-51, and ActivityNet but not for Kinetics. (ii) The Kinetics\ndataset has sufficient data for training of deep 3D CNNs, and enables training\nof up to 152 ResNets layers, interestingly similar to 2D ResNets on ImageNet.\nResNeXt-101 achieved 78.4% average accuracy on the Kinetics test set. (iii)\nKinetics pretrained simple 3D architectures outperforms complex 2D\narchitectures, and the pretrained ResNeXt-101 achieved 94.5% and 70.2% on\nUCF-101 and HMDB-51, respectively. The use of 2D CNNs trained on ImageNet has\nproduced significant progress in various tasks in image. We believe that using\ndeep 3D CNNs together with Kinetics will retrace the successful history of 2D\nCNNs and ImageNet, and stimulate advances in computer vision for videos. The\ncodes and pretrained models used in this study are publicly available.\nhttps://github.com/kenshohara/3D-ResNets-PyTorch\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 08:29:06 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 01:49:37 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Hara", "Kensho", ""], ["Kataoka", "Hirokatsu", ""], ["Satoh", "Yutaka", ""]]}, {"id": "1711.09584", "submitter": "Junchi Yan", "authors": "Tianshu Yu, Junchi Yan, Jieyi Zhao, Baoxin Li", "title": "Joint Cuts and Matching of Partitions in One Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As two fundamental problems, graph cuts and graph matching have been\ninvestigated over decades, resulting in vast literature in these two topics\nrespectively. However the way of jointly applying and solving graph cuts and\nmatching receives few attention. In this paper, we first formalize the problem\nof simultaneously cutting a graph into two partitions i.e. graph cuts and\nestablishing their correspondence i.e. graph matching. Then we develop an\noptimization algorithm by updating matching and cutting alternatively, provided\nwith theoretical analysis. The efficacy of our algorithm is verified on both\nsynthetic dataset and real-world images containing similar regions or\nstructures.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 08:52:19 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Yu", "Tianshu", ""], ["Yan", "Junchi", ""], ["Zhao", "Jieyi", ""], ["Li", "Baoxin", ""]]}, {"id": "1711.09594", "submitter": "Alan Lukezic", "authors": "Alan Luke\\v{z}i\\v{c}, Luka \\v{C}ehovin Zajc, Tom\\'a\\v{s} Voj\\'i\\v{r},\n  Ji\\v{r}\\'i Matas, Matej Kristan", "title": "FuCoLoT -- A Fully-Correlational Long-Term Tracker", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose FuCoLoT -- a Fully Correlational Long-term Tracker. It exploits\nthe novel DCF constrained filter learning method to design a detector that is\nable to re-detect the target in the whole image efficiently. FuCoLoT maintains\nseveral correlation filters trained on different time scales that act as the\ndetector components. A novel mechanism based on the correlation response is\nused for tracking failure estimation. FuCoLoT achieves state-of-the-art results\non standard short-term benchmarks and it outperforms the current\nbest-performing tracker on the long-term UAV20L benchmark by over 19%. It has\nan order of magnitude smaller memory footprint than its best-performing\ncompetitors and runs at 15fps in a single CPU thread.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 09:31:05 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 09:32:02 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Luke\u017ei\u010d", "Alan", ""], ["Zajc", "Luka \u010cehovin", ""], ["Voj\u00ed\u0159", "Tom\u00e1\u0161", ""], ["Matas", "Ji\u0159\u00ed", ""], ["Kristan", "Matej", ""]]}, {"id": "1711.09601", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach,\n  Tinne Tuytelaars", "title": "Memory Aware Synapses: Learning what (not) to forget", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can learn in a continuous manner. Old rarely utilized knowledge can be\noverwritten by new incoming information while important, frequently used\nknowledge is prevented from being erased. In artificial learning systems,\nlifelong learning so far has focused mainly on accumulating knowledge over\ntasks and overcoming catastrophic forgetting. In this paper, we argue that,\ngiven the limited model capacity and the unlimited new information to be\nlearned, knowledge has to be preserved or erased selectively. Inspired by\nneuroplasticity, we propose a novel approach for lifelong learning, coined\nMemory Aware Synapses (MAS). It computes the importance of the parameters of a\nneural network in an unsupervised and online manner. Given a new sample which\nis fed to the network, MAS accumulates an importance measure for each parameter\nof the network, based on how sensitive the predicted output function is to a\nchange in this parameter. When learning a new task, changes to important\nparameters can then be penalized, effectively preventing important knowledge\nrelated to previous tasks from being overwritten. Further, we show an\ninteresting connection between a local version of our method and Hebb's\nrule,which is a model for the learning process in the brain. We test our method\non a sequence of object recognition tasks and on the challenging problem of\nlearning an embedding for predicting $<$subject, predicate, object$>$ triplets.\nWe show state-of-the-art performance and, for the first time, the ability to\nadapt the importance of the parameters based on unlabeled data towards what the\nnetwork needs (not) to forget, which may vary depending on test conditions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 09:48:44 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 10:46:56 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 16:41:42 GMT"}, {"version": "v4", "created": "Fri, 5 Oct 2018 08:40:30 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Aljundi", "Rahaf", ""], ["Babiloni", "Francesca", ""], ["Elhoseiny", "Mohamed", ""], ["Rohrbach", "Marcus", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1711.09618", "submitter": "Shohei Yamamoto", "authors": "Katsunori Ohnishi, Shohei Yamamoto, Yoshitaka Ushiku, Tatsuya Harada", "title": "Hierarchical Video Generation from Orthogonal Information: Optical Flow\n  and Texture", "comments": "Our supplemental material is available on\n  http://www.mi.t.u-tokyo.ac.jp/assets/publication/hierarchical_video_generation_sup/\n  Accepted to AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to represent and generate videos from unlabeled data is a very\nchallenging problem. To generate realistic videos, it is important not only to\nensure that the appearance of each frame is real, but also to ensure the\nplausibility of a video motion and consistency of a video appearance in the\ntime direction. The process of video generation should be divided according to\nthese intrinsic difficulties. In this study, we focus on the motion and\nappearance information as two important orthogonal components of a video, and\npropose Flow-and-Texture-Generative Adversarial Networks (FTGAN) consisting of\nFlowGAN and TextureGAN. In order to avoid a huge annotation cost, we have to\nexplore a way to learn from unlabeled data. Thus, we employ optical flow as\nmotion information to generate videos. FlowGAN generates optical flow, which\ncontains only the edge and motion of the videos to be begerated. On the other\nhand, TextureGAN specializes in giving a texture to optical flow generated by\nFlowGAN. This hierarchical approach brings more realistic videos with plausible\nmotion and appearance consistency. Our experiments show that our model\ngenerates more plausible motion videos and also achieves significantly improved\nperformance for unsupervised action classification in comparison to previous\nGAN works. In addition, because our model generates videos from two independent\ninformation, our model can generate new combinations of motion and attribute\nthat are not seen in training data, such as a video in which a person is doing\nsit-up in a baseball ground.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 11:03:51 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 08:23:09 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Ohnishi", "Katsunori", ""], ["Yamamoto", "Shohei", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1711.09648", "submitter": "Suresh Kirthi Kumaraswamy", "authors": "Suresh Kirthi Kumaraswamy, PS Sastry and KR Ramakrishnan", "title": "Transfer Learning in CNNs Using Filter-Trees", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are very effective for many pattern\nrecognition tasks. However, training deep CNNs needs extensive computation and\nlarge training data. In this paper we propose Bank of Filter-Trees (BFT) as a\ntrans- fer learning mechanism for improving efficiency of learning CNNs. A\nfilter-tree corresponding to a filter in k^{th} convolu- tional layer of a CNN\nis a subnetwork consisting of the filter along with all its connections to\nfilters in all preceding layers. An ensemble of such filter-trees created from\nthe k^{th} layers of many CNNs learnt on different but related tasks, forms the\nBFT. To learn a new CNN, we sample from the BFT to select a set of filter\ntrees. This fixes the target net up to the k th layer and only the remaining\nnetwork would be learnt using train- ing data of new task. Through simulations\nwe demonstrate the effectiveness of this idea of BFT. This method constitutes a\nnovel transfer learning technique where transfer is at a sub- network level;\ntransfer can be effected from multiple source networks; and, with no finetuning\nof the transferred weights, the performance achieved is on par with networks\nthat are trained from scratch.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 12:29:44 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Kumaraswamy", "Suresh Kirthi", ""], ["Sastry", "PS", ""], ["Ramakrishnan", "KR", ""]]}, {"id": "1711.09663", "submitter": "Eli (Omid) David", "authors": "Ido Cohen, Eli David, Nathan S. Netanyahu, Noa Liscovitch, Gal Chechik", "title": "DeepBrain: Functional Representation of Neural In-Situ Hybridization\n  Images for Gene Ontology Classification Using Deep Convolutional Autoencoders", "comments": null, "journal-ref": "International Conference on Artificial Neural Networks (ICANN),\n  Springer LNCS, Vol. 10614, pp. 287-296, Alghero, Italy, September, 2017", "doi": "10.1007/978-3-319-68612-7_33", "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel deep learning-based method for learning a\nfunctional representation of mammalian neural images. The method uses a deep\nconvolutional denoising autoencoder (CDAE) for generating an invariant, compact\nrepresentation of in situ hybridization (ISH) images. While most existing\nmethods for bio-imaging analysis were not developed to handle images with\nhighly complex anatomical structures, the results presented in this paper show\nthat functional representation extracted by CDAE can help learn features of\nfunctional gene ontology categories for their classification in a highly\naccurate manner. Using this CDAE representation, our method outperforms the\nprevious state-of-the-art classification rate, by improving the average AUC\nfrom 0.92 to 0.98, i.e., achieving 75% reduction in error. The method operates\non input images that were downsampled significantly with respect to the\noriginal ones to make it computationally feasible.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 13:00:03 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Cohen", "Ido", ""], ["David", "Eli", ""], ["Netanyahu", "Nathan S.", ""], ["Liscovitch", "Noa", ""], ["Chechik", "Gal", ""]]}, {"id": "1711.09670", "submitter": "Christian Reul", "authors": "Christian Reul, Uwe Springmann, Christoph Wick, and Frank Puppe", "title": "Improving OCR Accuracy on Early Printed Books by utilizing Cross Fold\n  Training and Voting", "comments": null, "journal-ref": null, "doi": "10.1109/DAS.2018.30", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a method that significantly reduces the character\nerror rates for OCR text obtained from OCRopus models trained on early printed\nbooks. The method uses a combination of cross fold training and confidence\nbased voting. After allocating the available ground truth in different subsets\nseveral training processes are performed, each resulting in a specific OCR\nmodel. The OCR text generated by these models then gets voted to determine the\nfinal output by taking the recognized characters, their alternatives, and the\nconfidence values assigned to each character into consideration. Experiments on\nseven early printed books show that the proposed method outperforms the\nstandard approach considerably by reducing the amount of errors by up to 50%\nand more.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 13:15:52 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Reul", "Christian", ""], ["Springmann", "Uwe", ""], ["Wick", "Christoph", ""], ["Puppe", "Frank", ""]]}, {"id": "1711.09726", "submitter": "Tobias Ross", "authors": "Tobias Ross, David Zimmerer, Anant Vemuri, Fabian Isensee, Manuel\n  Wiesenfarth, Sebastian Bodenstedt, Fabian Both, Philip Kessler, Martin\n  Wagner, Beat M\\\"uller, Hannes Kenngott, Stefanie Speidel, Annette\n  Kopp-Schneider, Klaus Maier-Hein, Lena Maier-Hein", "title": "Exploiting the potential of unlabeled endoscopic video data with\n  self-supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical data science is a new research field that aims to observe all\naspects of the patient treatment process in order to provide the right\nassistance at the right time. Due to the breakthrough successes of deep\nlearning-based solutions for automatic image annotation, the availability of\nreference annotations for algorithm training is becoming a major bottleneck in\nthe field. The purpose of this paper was to investigate the concept of\nself-supervised learning to address this issue.\n  Our approach is guided by the hypothesis that unlabeled video data can be\nused to learn a representation of the target domain that boosts the performance\nof state-of-the-art machine learning algorithms when used for pre-training.\nCore of the method is an auxiliary task based on raw endoscopic video data of\nthe target domain that is used to initialize the convolutional neural network\n(CNN) for the target task. In this paper, we propose the re-colorization of\nmedical images with a generative adversarial network (GAN)-based architecture\nas auxiliary task. A variant of the method involves a second pre-training step\nbased on labeled data for the target task from a related domain. We validate\nboth variants using medical instrument segmentation as target task.\n  The proposed approach can be used to radically reduce the manual annotation\neffort involved in training CNNs. Compared to the baseline approach of\ngenerating annotated data from scratch, our method decreases exploratively the\nnumber of labeled images by up to 75% without sacrificing performance. Our\nmethod also outperforms alternative methods for CNN pre-training, such as\npre-training on publicly available non-medical or medical data using the target\ntask (in this instance: segmentation).\n  As it makes efficient use of available (non-)public and (un-)labeled data,\nthe approach has the potential to become a valuable tool for CNN\n(pre-)training.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 14:56:38 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 23:26:01 GMT"}, {"version": "v3", "created": "Wed, 31 Jan 2018 13:59:45 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Ross", "Tobias", ""], ["Zimmerer", "David", ""], ["Vemuri", "Anant", ""], ["Isensee", "Fabian", ""], ["Wiesenfarth", "Manuel", ""], ["Bodenstedt", "Sebastian", ""], ["Both", "Fabian", ""], ["Kessler", "Philip", ""], ["Wagner", "Martin", ""], ["M\u00fcller", "Beat", ""], ["Kenngott", "Hannes", ""], ["Speidel", "Stefanie", ""], ["Kopp-Schneider", "Annette", ""], ["Maier-Hein", "Klaus", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "1711.09767", "submitter": "Matan Sela", "authors": "Matan Sela, Pingmei Xu, Junfeng He, Vidhya Navalpakkam and Dmitry\n  Lagun", "title": "GazeGAN - Unpaired Adversarial Image Generation for Gaze Estimation", "comments": "Project was done when the first author was at Google Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has demonstrated the ability to estimate gaze on mobile\ndevices by performing inference on the image from the phone's front-facing\ncamera, and without requiring specialized hardware. While this offers wide\npotential applications such as in human-computer interaction, medical diagnosis\nand accessibility (e.g., hands free gaze as input for patients with motor\ndisorders), current methods are limited as they rely on collecting data from\nreal users, which is a tedious and expensive process that is hard to scale\nacross devices. There have been some attempts to synthesize eye region data\nusing 3D models that can simulate various head poses and camera settings,\nhowever these lack in realism.\n  In this paper, we improve upon a recently suggested method, and propose a\ngenerative adversarial framework to generate a large dataset of high resolution\ncolorful images with high diversity (e.g., in subjects, head pose, camera\nsettings) and realism, while simultaneously preserving the accuracy of gaze\nlabels. The proposed approach operates on extended regions of the eye, and even\ncompletes missing parts of the image. Using this rich synthesized dataset, and\nwithout using any additional training data from real users, we demonstrate\nimprovements over state-of-the-art for estimating 2D gaze position on mobile\ndevices. We further demonstrate cross-device generalization of model\nperformance, as well as improved robustness to diverse head pose, blur and\ndistance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 15:32:36 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Sela", "Matan", ""], ["Xu", "Pingmei", ""], ["He", "Junfeng", ""], ["Navalpakkam", "Vidhya", ""], ["Lagun", "Dmitry", ""]]}, {"id": "1711.09822", "submitter": "Clemens Marschner", "authors": "Aayush Garg, Thilo Will, William Darling, Willi Richert, Clemens\n  Marschner", "title": "Scalable Object Detection for Stylized Objects", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following recent breakthroughs in convolutional neural networks and\nmonolithic model architectures, state-of-the-art object detection models can\nreliably and accurately scale into the realm of up to thousands of classes.\nThings quickly break down, however, when scaling into the tens of thousands,\nor, eventually, to millions or billions of unique objects. Further, bounding\nbox-trained end-to-end models require extensive training data. Even though -\nwith some tricks using hierarchies - one can sometimes scale up to thousands of\nclasses, the labor requirements for clean image annotations quickly get out of\ncontrol. In this paper, we present a two-layer object detection method for\nbrand logos and other stylized objects for which prototypical images exist. It\ncan scale to large numbers of unique classes. Our first layer is a CNN from the\nSingle Shot Multibox Detector family of models that learns to propose regions\nwhere some stylized object is likely to appear. The contents of a proposed\nbounding box is then run against an image index that is targeted for the\nretrieval task at hand. The proposed architecture scales to a large number of\nobject classes, allows to continously add new classes without retraining, and\nexhibits state-of-the-art quality on a stylized object detection task such as\nlogo recognition.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 16:46:09 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 10:04:56 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Garg", "Aayush", ""], ["Will", "Thilo", ""], ["Darling", "William", ""], ["Richert", "Willi", ""], ["Marschner", "Clemens", ""]]}, {"id": "1711.09825", "submitter": "Andreas Veit", "authors": "Andreas Veit, Maximilian Nickel, Serge Belongie, Laurens van der\n  Maaten", "title": "Separating Self-Expression and Visual Content in Hashtag Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variety, abundance, and structured nature of hashtags make them an\ninteresting data source for training vision models. For instance, hashtags have\nthe potential to significantly reduce the problem of manual supervision and\nannotation when learning vision models for a large number of concepts. However,\na key challenge when learning from hashtags is that they are inherently\nsubjective because they are provided by users as a form of self-expression. As\na consequence, hashtags may have synonyms (different hashtags referring to the\nsame visual content) and may be ambiguous (the same hashtag referring to\ndifferent visual content). These challenges limit the effectiveness of\napproaches that simply treat hashtags as image-label pairs. This paper presents\nan approach that extends upon modeling simple image-label pairs by modeling the\njoint distribution of images, hashtags, and users. We demonstrate the efficacy\nof such approaches in image tagging and retrieval experiments, and show how the\njoint model can be used to perform user-conditional retrieval and tagging.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 16:50:52 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Veit", "Andreas", ""], ["Nickel", "Maximilian", ""], ["Belongie", "Serge", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1711.09856", "submitter": "Anurag Arnab", "authors": "Anurag Arnab, Ondrej Miksik, Philip H.S. Torr", "title": "On the Robustness of Semantic Segmentation Models to Adversarial Attacks", "comments": "CVPR 2018 extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have demonstrated exceptional performance on most\nrecognition tasks such as image classification and segmentation. However, they\nhave also been shown to be vulnerable to adversarial examples. This phenomenon\nhas recently attracted a lot of attention but it has not been extensively\nstudied on multiple, large-scale datasets and structured prediction tasks such\nas semantic segmentation which often require more specialised networks with\nadditional components such as CRFs, dilated convolutions, skip-connections and\nmultiscale processing. In this paper, we present what to our knowledge is the\nfirst rigorous evaluation of adversarial attacks on modern semantic\nsegmentation models, using two large-scale datasets. We analyse the effect of\ndifferent network architectures, model capacity and multiscale processing, and\nshow that many observations made on the task of classification do not always\ntransfer to this more complex task. Furthermore, we show how mean-field\ninference in deep structured models, multiscale processing (and more generally,\ninput transformations) naturally implement recently proposed adversarial\ndefenses. Our observations will aid future efforts in understanding and\ndefending against adversarial examples. Moreover, in the shorter term, we show\nhow to effectively benchmark robustness and show which segmentation models\nshould currently be preferred in safety-critical applications due to their\ninherent robustness.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 17:59:50 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 13:29:56 GMT"}, {"version": "v3", "created": "Sun, 8 Jul 2018 12:37:09 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Arnab", "Anurag", ""], ["Miksik", "Ondrej", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1711.09867", "submitter": "Anthony Yezzi", "authors": "Anthony Yezzi and Ganesh Sundaramoorthi", "title": "Accelerated Optimization in the PDE Framework: Formulations for the\n  Active Contour Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the seminal work of Nesterov, accelerated optimization methods have\nbeen used to powerfully boost the performance of first-order, gradient-based\nparameter estimation in scenarios where second-order optimization strategies\nare either inapplicable or impractical. Not only does accelerated gradient\ndescent converge considerably faster than traditional gradient descent, but it\nalso performs a more robust local search of the parameter space by initially\novershooting and then oscillating back as it settles into a final\nconfiguration, thereby selecting only local minimizers with a basis of\nattraction large enough to contain the initial overshoot. This behavior has\nmade accelerated and stochastic gradient search methods particularly popular\nwithin the machine learning community. In their recent PNAS 2016 paper,\nWibisono, Wilson, and Jordan demonstrate how a broad class of accelerated\nschemes can be cast in a variational framework formulated around the Bregman\ndivergence, leading to continuum limit ODE's. We show how their formulation may\nbe further extended to infinite dimension manifolds (starting here with the\ngeometric space of curves and surfaces) by substituting the Bregman divergence\nwith inner products on the tangent space and explicitly introducing a\ndistributed mass model which evolves in conjunction with the object of interest\nduring the optimization process. The co-evolving mass model, which is\nintroduced purely for the sake of endowing the optimization with helpful\ndynamics, also links the resulting class of accelerated PDE based optimization\nschemes to fluid dynamical formulations of optimal mass transport.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 18:27:24 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Yezzi", "Anthony", ""], ["Sundaramoorthi", "Ganesh", ""]]}, {"id": "1711.09869", "submitter": "Martin Simonovsky", "authors": "Loic Landrieu, Martin Simonovsky", "title": "Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs", "comments": "Accepted to CVPR 2018; camera ready version. Major updates to [v1]:\n  Improved performance on S3DIS (from +5.8 to +12.4 mIoU) and extended ablation\n  study in Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep learning-based framework to tackle the challenge of\nsemantic segmentation of large-scale point clouds of millions of points. We\nargue that the organization of 3D point clouds can be efficiently captured by a\nstructure called superpoint graph (SPG), derived from a partition of the\nscanned scene into geometrically homogeneous elements. SPGs offer a compact yet\nrich representation of contextual relationships between object parts, which is\nthen exploited by a graph convolutional network. Our framework sets a new state\nof the art for segmenting outdoor LiDAR scans (+11.9 and +8.8 mIoU points for\nboth Semantic3D test sets), as well as indoor scans (+12.4 mIoU points for the\nS3DIS dataset).\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 18:37:50 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 09:01:33 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Landrieu", "Loic", ""], ["Simonovsky", "Martin", ""]]}, {"id": "1711.09952", "submitter": "\\v{Z}iga Emer\\v{s}i\\v{c}", "authors": "\\v{Z}iga Emer\\v{s}i\\v{c} and Dejan \\v{S}tepec and Vitomir \\v{S}truc\n  and Peter Peer", "title": "Training Convolutional Neural Networks with Limited Training Data for\n  Ear Recognition in the Wild", "comments": null, "journal-ref": null, "doi": "10.1109/FG.2017.123", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identity recognition from ear images is an active field of research within\nthe biometric community. The ability to capture ear images from a distance and\nin a covert manner makes ear recognition technology an appealing choice for\nsurveillance and security applications as well as related application domains.\nIn contrast to other biometric modalities, where large datasets captured in\nuncontrolled settings are readily available, datasets of ear images are still\nlimited in size and mostly of laboratory-like quality. As a consequence, ear\nrecognition technology has not benefited yet from advances in deep learning and\nconvolutional neural networks (CNNs) and is still lacking behind other\nmodalities that experienced significant performance gains owing to deep\nrecognition technology. In this paper we address this problem and aim at\nbuilding a CNNbased ear recognition model. We explore different strategies\ntowards model training with limited amounts of training data and show that by\nselecting an appropriate model architecture, using aggressive data augmentation\nand selective learning on existing (pre-trained) models, we are able to learn\nan effective CNN-based model using a little more than 1300 training images. The\nresult of our work is the first CNN-based approach to ear recognition that is\nalso made publicly available to the research community. With our model we are\nable to improve on the rank one recognition rate of the previous\nstate-of-the-art by more than 25% on a challenging dataset of ear images\ncaptured from the web (a.k.a. in the wild).\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 19:51:06 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 08:19:35 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Emer\u0161i\u010d", "\u017diga", ""], ["\u0160tepec", "Dejan", ""], ["\u0160truc", "Vitomir", ""], ["Peer", "Peter", ""]]}, {"id": "1711.10006", "submitter": "Fabian Manhardt", "authors": "Wadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan Ilic, Nassir\n  Navab", "title": "SSD-6D: Making RGB-based 3D detection and 6D pose estimation great again", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for detecting 3D model instances and estimating\ntheir 6D poses from RGB data in a single shot. To this end, we extend the\npopular SSD paradigm to cover the full 6D pose space and train on synthetic\nmodel data only. Our approach competes or surpasses current state-of-the-art\nmethods that leverage RGB-D data on multiple challenging datasets. Furthermore,\nour method produces these results at around 10Hz, which is many times faster\nthan the related methods. For the sake of reproducibility, we make our trained\nnetworks and detection code publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 21:17:51 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Kehl", "Wadim", ""], ["Manhardt", "Fabian", ""], ["Tombari", "Federico", ""], ["Ilic", "Slobodan", ""], ["Navab", "Nassir", ""]]}, {"id": "1711.10061", "submitter": "Amir Sadeghian", "authors": "Amir Sadeghian, Ferdinand Legros, Maxime Voisin, Ricky Vesel,\n  Alexandre Alahi, Silvio Savarese", "title": "CAR-Net: Clairvoyant Attentive Recurrent Network", "comments": "The 2nd and 3rd authors contributed equally", "journal-ref": "ECCV 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an interpretable framework for path prediction that leverages\ndependencies between agents' behaviors and their spatial navigation\nenvironment. We exploit two sources of information: the past motion trajectory\nof the agent of interest and a wide top-view image of the navigation scene. We\npropose a Clairvoyant Attentive Recurrent Network (CAR-Net) that learns where\nto look in a large image of the scene when solving the path prediction task.\nOur method can attend to any area, or combination of areas, within the raw\nimage (e.g., road intersections) when predicting the trajectory of the agent.\nThis allows us to visualize fine-grained semantic elements of navigation scenes\nthat influence the prediction of trajectories. To study the impact of space on\nagents' trajectories, we build a new dataset made of top-view images of\nhundreds of scenes (Formula One racing tracks) where agents' behaviors are\nheavily influenced by known areas in the images (e.g., upcoming turns). CAR-Net\nsuccessfully attends to these salient regions. Additionally, CAR-Net reaches\nstate-of-the-art accuracy on the standard trajectory forecasting benchmark,\nStanford Drone Dataset (SDD). Finally, we show CAR-Net's ability to generalize\nto unseen scenes.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 00:22:09 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 18:25:13 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 06:25:03 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Sadeghian", "Amir", ""], ["Legros", "Ferdinand", ""], ["Voisin", "Maxime", ""], ["Vesel", "Ricky", ""], ["Alahi", "Alexandre", ""], ["Savarese", "Silvio", ""]]}, {"id": "1711.10067", "submitter": "Xiaojie Jin Mr.", "authors": "Xiaojie Jin, Yingzhen Yang, Ning Xu, Jianchao Yang, Nebojsa Jojic,\n  Jiashi Feng, Shuicheng Yan", "title": "WSNet: Compact and Efficient Networks Through Weight Sampling", "comments": "To appear at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach and a novel architecture, termed WSNet, for\nlearning compact and efficient deep neural networks. Existing approaches\nconventionally learn full model parameters independently and then compress them\nvia ad hoc processing such as model pruning or filter factorization.\nAlternatively, WSNet proposes learning model parameters by sampling from a\ncompact set of learnable parameters, which naturally enforces {parameter\nsharing} throughout the learning process. We demonstrate that such a novel\nweight sampling approach (and induced WSNet) promotes both weights and\ncomputation sharing favorably. By employing this method, we can more\nefficiently learn much smaller networks with competitive performance compared\nto baseline networks with equal numbers of convolution filters. Specifically,\nwe consider learning compact and efficient 1D convolutional neural networks for\naudio classification. Extensive experiments on multiple audio classification\ndatasets verify the effectiveness of WSNet. Combined with weight quantization,\nthe resulted models are up to 180 times smaller and theoretically up to 16\ntimes faster than the well-established baselines, without noticeable\nperformance drop.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 00:43:20 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 05:11:35 GMT"}, {"version": "v3", "created": "Tue, 22 May 2018 13:41:19 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Jin", "Xiaojie", ""], ["Yang", "Yingzhen", ""], ["Xu", "Ning", ""], ["Yang", "Jianchao", ""], ["Jojic", "Nebojsa", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1711.10069", "submitter": "Di Yuan", "authors": "Di Yuan, Xiaohuan Lu, Donghao Li, Yingyi Liang and Xinming Zhang", "title": "Particle filter re-detection for visual tracking via correlation filters", "comments": "Multimedia Tools and Applications", "journal-ref": "[J]. Multimedia Tools and Applications, 2019, 78(11): 14277-14301", "doi": "10.1007/s11042-018-6800-0", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the correlation filter based tracking algorithms can achieve good\nperformance and maintain fast computational speed. However, in some complicated\ntracking scenes, there is a fatal defect that causes the object to be located\ninaccurately. In order to address this problem, we propose a particle filter\nredetection based tracking approach for accurate object localization. During\nthe tracking process, the kernelized correlation filter (KCF) based tracker\nlocates the object by relying on the maximum response value of the response\nmap; when the response map becomes ambiguous, the KCF tracking result becomes\nunreliable. Our method can provide more candidates by particle resampling to\ndetect the object accordingly. Additionally, we give a new object scale\nevaluation mechanism, which merely considers the differences between the\nmaximum response values in consecutive frames. Extensive experiments on OTB2013\nand OTB2015 datasets demonstrate that the proposed tracker performs favorably\nin relation to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 00:44:40 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 09:09:00 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 07:05:45 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Yuan", "Di", ""], ["Lu", "Xiaohuan", ""], ["Li", "Donghao", ""], ["Liang", "Yingyi", ""], ["Zhang", "Xinming", ""]]}, {"id": "1711.10098", "submitter": "Rui Qian", "authors": "Rui Qian, Robby T. Tan, Wenhan Yang, Jiajun Su, Jiaying Liu", "title": "Attentive Generative Adversarial Network for Raindrop Removal from a\n  Single Image", "comments": "CVPR2018 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raindrops adhered to a glass window or camera lens can severely hamper the\nvisibility of a background scene and degrade an image considerably. In this\npaper, we address the problem by visually removing raindrops, and thus\ntransforming a raindrop degraded image into a clean one. The problem is\nintractable, since first the regions occluded by raindrops are not given.\nSecond, the information about the background scene of the occluded regions is\ncompletely lost for most part. To resolve the problem, we apply an attentive\ngenerative network using adversarial training. Our main idea is to inject\nvisual attention into both the generative and discriminative networks. During\nthe training, our visual attention learns about raindrop regions and their\nsurroundings. Hence, by injecting this information, the generative network will\npay more attention to the raindrop regions and the surrounding structures, and\nthe discriminative network will be able to assess the local consistency of the\nrestored regions. This injection of visual attention to both generative and\ndiscriminative networks is the main contribution of this paper. Our experiments\nshow the effectiveness of our approach, which outperforms the state of the art\nmethods quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 03:15:55 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 05:23:11 GMT"}, {"version": "v3", "created": "Sun, 1 Apr 2018 14:27:00 GMT"}, {"version": "v4", "created": "Sun, 6 May 2018 06:41:10 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Qian", "Rui", ""], ["Tan", "Robby T.", ""], ["Yang", "Wenhan", ""], ["Su", "Jiajun", ""], ["Liu", "Jiaying", ""]]}, {"id": "1711.10103", "submitter": "Qiangchang Wang", "authors": "Qiangchang Wang, Guodong Guo and Mohammad Iqbal Nouyed", "title": "Learning Channel Inter-dependencies at Multiple Scales on Dense Networks\n  for Face Recognition", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new deep network structure for unconstrained face recognition.\nThe proposed network integrates several key components together in order to\ncharacterize complex data distributions, such as in unconstrained face images.\nInspired by recent progress in deep networks, we consider some important\nconcepts, including multi-scale feature learning, dense connections of network\nlayers, and weighting different network flows, for building our deep network\nstructure. The developed network is evaluated in unconstrained face matching,\nshowing the capability of learning complex data distributions caused by face\nimages with various qualities.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 03:32:21 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 19:35:40 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Wang", "Qiangchang", ""], ["Guo", "Guodong", ""], ["Nouyed", "Mohammad Iqbal", ""]]}, {"id": "1711.10108", "submitter": "Yi Fang", "authors": "Mengwei Ren, Liang Niu, Yi Fang", "title": "3D-A-Nets: 3D Deep Dense Descriptor for Volumetric Shapes with\n  Adversarial Networks", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently researchers have been shifting their focus towards learned 3D shape\ndescriptors from hand-craft ones to better address challenging issues of the\ndeformation and structural variation inherently present in 3D objects. 3D\ngeometric data are often transformed to 3D Voxel grids with regular format in\norder to be better fed to a deep neural net architecture. However, the\ncomputational intractability of direct application of 3D convolutional nets to\n3D volumetric data severely limits the efficiency (i.e. slow processing) and\neffectiveness (i.e. unsatisfied accuracy) in processing 3D geometric data. In\nthis paper, powered with a novel design of adversarial networks (3D-A-Nets), we\nhave developed a novel 3D deep dense shape descriptor (3D-DDSD) to address the\nchallenging issues of efficient and effective 3D volumetric data processing. We\ndeveloped new definition of 2D multilayer dense representation (MDR) of 3D\nvolumetric data to extract concise but geometrically informative shape\ndescription and a novel design of adversarial networks that jointly train a set\nof convolution neural network (CNN), recurrent neural network (RNN) and an\nadversarial discriminator. More specifically, the generator network produces 3D\nshape features that encourages the clustering of samples from the same category\nwith correct class label, whereas the discriminator network discourages the\nclustering by assigning them misleading adversarial class labels. By addressing\nthe challenges posed by the computational inefficiency of direct application of\nCNN to 3D volumetric data, 3D-A-Nets can learn high-quality 3D-DSDD which\ndemonstrates superior performance on 3D shape classification and retrieval over\nother state-of-the-art techniques by a great margin.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 04:01:20 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Ren", "Mengwei", ""], ["Niu", "Liang", ""], ["Fang", "Yi", ""]]}, {"id": "1711.10125", "submitter": "Yen-Chang Hsu", "authors": "Yen-Chang Hsu, Zhaoyang Lv, Zsolt Kira", "title": "Learning to cluster in order to transfer across domains and tasks", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel method to perform transfer learning across\ndomains and tasks, formulating it as a problem of learning to cluster. The key\ninsight is that, in addition to features, we can transfer similarity\ninformation and this is sufficient to learn a similarity function and\nclustering network to perform both domain adaptation and cross-task transfer\nlearning. We begin by reducing categorical information to pairwise constraints,\nwhich only considers whether two instances belong to the same class or not.\nThis similarity is category-agnostic and can be learned from data in the source\ndomain using a similarity network. We then present two novel approaches for\nperforming transfer learning using this similarity function. First, for\nunsupervised domain adaptation, we design a new loss function to regularize\nclassification with a constrained clustering loss, hence learning a clustering\nnetwork with the transferred similarity metric generating the training inputs.\nSecond, for cross-task learning (i.e., unsupervised clustering with unseen\ncategories), we propose a framework to reconstruct and estimate the number of\nsemantic clusters, again using the clustering network. Since the similarity\nnetwork is noisy, the key is to use a robust clustering algorithm, and we show\nthat our formulation is more robust than the alternative constrained and\nunconstrained clustering approaches. Using this method, we first show state of\nthe art results for the challenging cross-task problem, applied on Omniglot and\nImageNet. Our results show that we can reconstruct semantic clusters with high\naccuracy. We then evaluate the performance of cross-domain transfer using\nimages from the Office-31 and SVHN-MNIST tasks and present top accuracy on both\ndatasets. Our approach doesn't explicitly deal with domain discrepancy. If we\ncombine with a domain adaptation loss, it shows further improvement.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 04:59:58 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 15:54:59 GMT"}, {"version": "v3", "created": "Sat, 17 Mar 2018 04:42:49 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Hsu", "Yen-Chang", ""], ["Lv", "Zhaoyang", ""], ["Kira", "Zsolt", ""]]}, {"id": "1711.10131", "submitter": "Shan Suthaharan", "authors": "Shan Suthaharan", "title": "A fatal point concept and a low-sensitivity quantitative measure for\n  traffic safety analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variability of the clusters generated by clustering techniques in the\ndomain of latitude and longitude variables of fatal crash data are\nsignificantly unpredictable. This unpredictability, caused by the randomness of\nfatal crash incidents, reduces the accuracy of crash frequency (i.e., counts of\nfatal crashes per cluster) which is used to measure traffic safety in practice.\nIn this paper, a quantitative measure of traffic safety that is not\nsignificantly affected by the aforementioned variability is proposed. It\nintroduces a fatal point -- a segment with the highest frequency of fatality --\nconcept based on cluster characteristics and detects them by imposing rounding\nerrors to the hundredth decimal place of the longitude. The frequencies of the\ncluster and the cluster's fatal point are combined to construct a low-sensitive\nquantitative measure of traffic safety for the cluster. The performance of the\nproposed measure of traffic safety is then studied by varying the parameter k\nof k-means clustering with the expectation that other clustering techniques can\nbe adopted in a similar fashion. The 2015 North Carolina fatal crash dataset of\nFatality Analysis Reporting System (FARS) is used to evaluate the proposed\nfatal point concept and perform experimental analysis to determine the\neffectiveness of the proposed measure. The empirical study shows that the\naverage traffic safety, measured by the proposed quantitative measure over\nseveral clusters, is not significantly affected by the variability, compared to\nthat of the standard crash frequency.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 05:37:37 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Suthaharan", "Shan", ""]]}, {"id": "1711.10143", "submitter": "Toru Tamaki", "authors": "Kenji Matsui, Toru Tamaki, Gwladys Auffret, Bisser Raytchev, Kazufumi\n  Kaneda", "title": "Revisiting hand-crafted feature for action recognition: a set of\n  improved dense trajectories", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a feature for action recognition called Trajectory-Set (TS), on\ntop of the improved Dense Trajectory (iDT). The TS feature encodes only\ntrajectories around densely sampled interest points, without any appearance\nfeatures. Experimental results on the UCF50, UCF101, and HMDB51 action datasets\ndemonstrate that TS is comparable to state-of-the-arts, and outperforms many\nother methods; for HMDB the accuracy of 85.4%, compared to the best accuracy of\n80.2% obtained by a deep method. Our code is available on-line at\nhttps://github.com/Gauffret/TrajectorySet .\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 06:38:02 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Matsui", "Kenji", ""], ["Tamaki", "Toru", ""], ["Auffret", "Gwladys", ""], ["Raytchev", "Bisser", ""], ["Kaneda", "Kazufumi", ""]]}, {"id": "1711.10151", "submitter": "Lane McIntosh", "authors": "Lane McIntosh, Niru Maheswaranathan, David Sussillo, Jonathon Shlens", "title": "Recurrent Segmentation for Variable Computational Budgets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art systems for semantic image segmentation use feed-forward\npipelines with fixed computational costs. Building an image segmentation system\nthat works across a range of computational budgets is challenging and\ntime-intensive as new architectures must be designed and trained for every\ncomputational setting. To address this problem we develop a recurrent neural\nnetwork that successively improves prediction quality with each iteration.\nImportantly, the RNN may be deployed across a range of computational budgets by\nmerely running the model for a variable number of iterations. We find that this\narchitecture is uniquely suited for efficiently segmenting videos. By\nexploiting the segmentation of past frames, the RNN can perform video\nsegmentation at similar quality but reduced computational cost compared to\nstate-of-the-art image segmentation methods. When applied to static images in\nthe PASCAL VOC 2012 and Cityscapes segmentation datasets, the RNN traces out a\nspeed-accuracy curve that saturates near the performance of state-of-the-art\nsegmentation methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 06:55:19 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 00:30:26 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["McIntosh", "Lane", ""], ["Maheswaranathan", "Niru", ""], ["Sussillo", "David", ""], ["Shlens", "Jonathon", ""]]}, {"id": "1711.10152", "submitter": "Zhicheng Jiao", "authors": "Haoxuan You, Zhicheng Jiao, Haojun Xu, Jie Li, Ying Wang, Xinbo Gao", "title": "Restricting Greed in Training of Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial network (GAN) has gotten wide re-search interest in\nthe field of deep learning. Variations of GAN have achieved competitive results\non specific tasks. However, the stability of training and diversity of\ngenerated instances are still worth studying further. Training of GAN can be\nthought of as a greedy procedure, in which the generative net tries to make the\nlocally optimal choice (minimizing loss function of discriminator) in each\niteration. Unfortunately, this often makes generated data resemble only a few\nmodes of real data and rotate between modes. To alleviate these problems, we\npropose a novel training strategy to restrict greed in training of GAN. With\nhelp of our method, the generated samples can cover more instance modes with\nmore stable training process. Evaluating our method on several representative\ndatasets, we demonstrate superiority of improved training strategy on typical\nGAN models with different distance metrics.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 06:55:59 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 12:37:56 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["You", "Haoxuan", ""], ["Jiao", "Zhicheng", ""], ["Xu", "Haojun", ""], ["Li", "Jie", ""], ["Wang", "Ying", ""], ["Gao", "Xinbo", ""]]}, {"id": "1711.10157", "submitter": "Utako Yamamoto", "authors": "Utako Yamamoto, Megumi Nakao, Masayuki Ohzeki and Tetsuya Matsuda", "title": "Deformation estimation of an elastic object by partial observation using\n  a neural network", "comments": "12 pages, 12 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformation estimation of elastic object assuming an internal organ is\nimportant for the computer navigation of surgery. The aim of this study is to\nestimate the deformation of an entire three-dimensional elastic object using\ndisplacement information of very few observation points. A learning approach\nwith a neural network was introduced to estimate the entire deformation of an\nobject. We applied our method to two elastic objects; a rectangular\nparallelepiped model, and a human liver model reconstructed from computed\ntomography data. The average estimation error for the human liver model was\n0.041 mm when the object was deformed up to 66.4 mm, from only around 3 %\nobservations. These results indicate that the deformation of an entire elastic\nobject can be estimated with an acceptable level of error from limited\nobservations by applying a trained neural network to a new deformation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 07:28:48 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Yamamoto", "Utako", ""], ["Nakao", "Megumi", ""], ["Ohzeki", "Masayuki", ""], ["Matsuda", "Tetsuya", ""]]}, {"id": "1711.10209", "submitter": "\\'Alvaro Parra Bustos", "authors": "\\'Alvaro Parra Bustos and Tat-Jun Chin", "title": "Guaranteed Outlier Removal for Point Cloud Registration with\n  Correspondences", "comments": "TPAMI accepted version (Nov, 2017) 14 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2773482", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An established approach for 3D point cloud registration is to estimate the\nregistration function from 3D keypoint correspondences. Typically, a robust\ntechnique is required to conduct the estimation, since there are false\ncorrespondences or outliers. Current 3D keypoint techniques are much less\naccurate than their 2D counterparts, thus they tend to produce extremely high\noutlier rates. A large number of putative correspondences must thus be\nextracted to ensure that sufficient good correspondences are available. Both\nfactors (high outlier rates, large data sizes) however cause existing robust\ntechniques to require very high computational cost. In this paper, we present a\nnovel preprocessing method called \\emph{guaranteed outlier removal} for point\ncloud registration. Our method reduces the input to a smaller set, in a way\nthat any rejected correspondence is guaranteed to not exist in the globally\noptimal solution. The reduction is performed using purely geometric operations\nwhich are deterministic and fast. Our method significantly reduces the\npopulation of outliers, such that further optimization can be performed\nquickly. Further, since only true outliers are removed, the globally optimal\nsolution is preserved. On various synthetic and real data experiments, we\ndemonstrate the effectiveness of our preprocessing method. Demo code is\navailable as supplementary material.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 10:02:18 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Bustos", "\u00c1lvaro Parra", ""], ["Chin", "Tat-Jun", ""]]}, {"id": "1711.10212", "submitter": "Guodong Zeng", "authors": "Guodong Zeng and Guoyan Zheng", "title": "Multi-stream 3D FCN with Multi-scale Deep Supervision for Multi-modality\n  Isointense Infant Brain MR Image Segmentation", "comments": "5 pages, 3 figures, submitted to ISBI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to address the challenging problem of segmentation of\nmulti-modality isointense infant brain MR images into white matter (WM), gray\nmatter (GM), and cerebrospinal fluid (CSF). Our method is based on\ncontext-guided, multi-stream fully convolutional networks (FCN), which after\ntraining, can directly map a whole volumetric data to its volume-wise labels.\nIn order to alleviate the poten-tial gradient vanishing problem during\ntraining, we designed multi-scale deep supervision. Furthermore, context\ninfor-mation was used to further improve the performance of our method.\nValidated on the test data of the MICCAI 2017 Grand Challenge on 6-month infant\nbrain MRI segmentation (iSeg-2017), our method achieved an average Dice Overlap\nCoefficient of 95.4%, 91.6% and 89.6% for CSF, GM and WM, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 10:11:41 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2019 12:51:37 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Zeng", "Guodong", ""], ["Zheng", "Guoyan", ""]]}, {"id": "1711.10217", "submitter": "Ran Tao", "authors": "Ran Tao, Efstratios Gavves, Arnold W.M. Smeulders", "title": "Tracking for Half an Hour", "comments": "tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term tracking requires extreme stability to the multitude of model\nupdates and robustness to the disappearance and loss of the target as such will\ninevitably happen. For motivation, we have taken 10 randomly selected\nOTB-sequences, doubled each by attaching a reversed version and repeated each\ndouble sequence 20 times. On most of these repetitive videos, the best current\ntracker performs worse on each loop. This illustrates the difference between\noptimization for short-term versus long-term tracking. In a long-term tracker a\ncombined global and local search strategy is beneficial, allowing for recovery\nfrom failures and disappearance. Most importantly, the proposed tracker also\nemploys cautious updating, guided by self-quality assessment. The proposed\ntracker is still among the best on the 20-sec OTB-videos while achieving\nstate-of-the-art on the 100-sec UAV20L benchmark. On 10 new half-an-hour videos\nwith city bicycling, sport games etc, the proposed tracker outperforms others\nby a large margin where the 2010 TLD tracker comes second.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 10:34:37 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Tao", "Ran", ""], ["Gavves", "Efstratios", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1711.10228", "submitter": "Eric Brachmann", "authors": "Eric Brachmann, Carsten Rother", "title": "Learning Less is More - 6D Camera Localization via 3D Surface Regression", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular research areas like autonomous driving and augmented reality have\nrenewed the interest in image-based camera localization. In this work, we\naddress the task of predicting the 6D camera pose from a single RGB image in a\ngiven 3D environment. With the advent of neural networks, previous works have\neither learned the entire camera localization process, or multiple components\nof a camera localization pipeline. Our key contribution is to demonstrate and\nexplain that learning a single component of this pipeline is sufficient. This\ncomponent is a fully convolutional neural network for densely regressing\nso-called scene coordinates, defining the correspondence between the input\nimage and the 3D scene space. The neural network is prepended to a new\nend-to-end trainable pipeline. Our system is efficient, highly accurate, robust\nin training, and exhibits outstanding generalization capabilities. It exceeds\nstate-of-the-art consistently on indoor and outdoor datasets. Interestingly,\nour approach surpasses existing techniques even without utilizing a 3D model of\nthe scene during training, since the network is able to discover 3D scene\ngeometry automatically, solely from single-view constraints.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 11:11:03 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 13:32:47 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Brachmann", "Eric", ""], ["Rother", "Carsten", ""]]}, {"id": "1711.10267", "submitter": "Yong Man Ro", "authors": "Geonmo Gu, Seong Tae Kim, Kihyun Kim, Wissam J. Baddar, Yong Man Ro", "title": "Differential Generative Adversarial Networks: Synthesizing Non-linear\n  Facial Variations with Limited Number of Training Data", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In face-related applications with a public available dataset, synthesizing\nnon-linear facial variations (e.g., facial expression, head-pose, illumination,\netc.) through a generative model is helpful in addressing the lack of training\ndata. In reality, however, there is insufficient data to even train the\ngenerative model for face synthesis. In this paper, we propose Differential\nGenerative Adversarial Networks (D-GAN) that can perform photo-realistic face\nsynthesis even when training data is small. Two discriminators are devised to\nensure the generator to approximate a face manifold, which can express face\nchanges as it wants. Experimental results demonstrate that the proposed method\nis robust to the amount of training data and synthesized images are useful to\nimprove the performance of a face expression classifier.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 13:02:51 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 16:47:07 GMT"}, {"version": "v3", "created": "Thu, 21 Dec 2017 13:10:02 GMT"}, {"version": "v4", "created": "Fri, 29 Dec 2017 00:53:45 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Gu", "Geonmo", ""], ["Kim", "Seong Tae", ""], ["Kim", "Kihyun", ""], ["Baddar", "Wissam J.", ""], ["Ro", "Yong Man", ""]]}, {"id": "1711.10275", "submitter": "Benjamin Graham", "authors": "Benjamin Graham, Martin Engelcke, Laurens van der Maaten", "title": "3D Semantic Segmentation with Submanifold Sparse Convolutional Networks", "comments": "arXiv admin note: text overlap with arXiv:1706.01307", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks are the de-facto standard for analyzing\nspatio-temporal data such as images, videos, and 3D shapes. Whilst some of this\ndata is naturally dense (e.g., photos), many other data sources are inherently\nsparse. Examples include 3D point clouds that were obtained using a LiDAR\nscanner or RGB-D camera. Standard \"dense\" implementations of convolutional\nnetworks are very inefficient when applied on such sparse data. We introduce\nnew sparse convolutional operations that are designed to process\nspatially-sparse data more efficiently, and use them to develop\nspatially-sparse convolutional networks. We demonstrate the strong performance\nof the resulting models, called submanifold sparse convolutional networks\n(SSCNs), on two tasks involving semantic segmentation of 3D point clouds. In\nparticular, our models outperform all prior state-of-the-art on the test set of\na recent semantic segmentation competition.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 13:21:58 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Graham", "Benjamin", ""], ["Engelcke", "Martin", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1711.10284", "submitter": "Yuji Tokozume", "authors": "Yuji Tokozume, Yoshitaka Ushiku, Tatsuya Harada", "title": "Between-class Learning for Image Classification", "comments": "11 pages, 8 figures, published as a conference paper at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel learning method for image classification\ncalled Between-Class learning (BC learning). We generate between-class images\nby mixing two images belonging to different classes with a random ratio. We\nthen input the mixed image to the model and train the model to output the\nmixing ratio. BC learning has the ability to impose constraints on the shape of\nthe feature distributions, and thus the generalization ability is improved. BC\nlearning is originally a method developed for sounds, which can be digitally\nmixed. Mixing two image data does not appear to make sense; however, we argue\nthat because convolutional neural networks have an aspect of treating input\ndata as waveforms, what works on sounds must also work on images. First, we\npropose a simple mixing method using internal divisions, which surprisingly\nproves to significantly improve performance. Second, we propose a mixing method\nthat treats the images as waveforms, which leads to a further improvement in\nperformance. As a result, we achieved 19.4% and 2.26% top-1 errors on\nImageNet-1K and CIFAR-10, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 13:31:14 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 08:50:09 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Tokozume", "Yuji", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1711.10288", "submitter": "Jacopo Cavazza", "authors": "Pietro Morerio and Jacopo Cavazza and Vittorio Murino", "title": "Minimal-Entropy Correlation Alignment for Unsupervised Deep Domain\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we face the problem of unsupervised domain adaptation with a\nnovel deep learning approach which leverages on our finding that entropy\nminimization is induced by the optimal alignment of second order statistics\nbetween source and target domains. We formally demonstrate this hypothesis and,\naiming at achieving an optimal alignment in practical cases, we adopt a more\nprincipled strategy which, differently from the current Euclidean approaches,\ndeploys alignment along geodesics. Our pipeline can be implemented by adding to\nthe standard classification loss (on the labeled source domain), a\nsource-to-target regularizer that is weighted in an unsupervised and\ndata-driven fashion. We provide extensive experiments to assess the superiority\nof our framework on standard domain and modality adaptation benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 13:39:10 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Morerio", "Pietro", ""], ["Cavazza", "Jacopo", ""], ["Murino", "Vittorio", ""]]}, {"id": "1711.10290", "submitter": "Jacopo Cavazza", "authors": "Jacopo Cavazza and Pietro Morerio and Vittorio Murino", "title": "Scalable and Compact 3D Action Recognition with Approximated RBF Kernel\n  Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent deep learning (DL) revolution, kernel machines still\nremain powerful methods for action recognition. DL has brought the use of large\ndatasets and this is typically a problem for kernel approaches, which are not\nscaling up efficiently due to kernel Gram matrices. Nevertheless, kernel\nmethods are still attractive and more generally applicable since they can\nequally manage different sizes of the datasets, also in cases where DL\ntechniques show some limitations. This work investigates these issues by\nproposing an explicit approximated representation that, together with a linear\nmodel, is an equivalent, yet scalable, implementation of a kernel machine. Our\napproximation is directly inspired by the exact feature map that is induced by\nan RBF Gaussian kernel but, unlike the latter, it is finite dimensional and\nvery compact. We justify the soundness of our idea with a theoretical analysis\nwhich proves the unbiasedness of the approximation, and provides a vanishing\nbound for its variance, which is shown to decrease much rapidly than in\nalternative methods in the literature. In a broad experimental validation, we\nassess the superiority of our approximation in terms of 1) ease and speed of\ntraining, 2) compactness of the model, and 3) improvements with respect to the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 13:46:05 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Cavazza", "Jacopo", ""], ["Morerio", "Pietro", ""], ["Murino", "Vittorio", ""]]}, {"id": "1711.10295", "submitter": "Zhun Zhong", "authors": "Zhun Zhong, Liang Zheng, Zhedong Zheng, Shaozi Li, Yi Yang", "title": "Camera Style Adaptation for Person Re-identification", "comments": "To appear in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being a cross-camera retrieval task, person re-identification suffers from\nimage style variations caused by different cameras. The art implicitly\naddresses this problem by learning a camera-invariant descriptor subspace. In\nthis paper, we explicitly consider this challenge by introducing camera style\n(CamStyle) adaptation. CamStyle can serve as a data augmentation approach that\nsmooths the camera style disparities. Specifically, with CycleGAN, labeled\ntraining images can be style-transferred to each camera, and, along with the\noriginal training samples, form the augmented training set. This method, while\nincreasing data diversity against over-fitting, also incurs a considerable\nlevel of noise. In the effort to alleviate the impact of noise, the label\nsmooth regularization (LSR) is adopted. The vanilla version of our method\n(without LSR) performs reasonably well on few-camera systems in which\nover-fitting often occurs. With LSR, we demonstrate consistent improvement in\nall systems regardless of the extent of over-fitting. We also report\ncompetitive accuracy compared with the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 14:01:30 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 12:03:43 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Zhong", "Zhun", ""], ["Zheng", "Liang", ""], ["Zheng", "Zhedong", ""], ["Li", "Shaozi", ""], ["Yang", "Yi", ""]]}, {"id": "1711.10305", "submitter": "Ting Yao", "authors": "Zhaofan Qiu and Ting Yao and Tao Mei", "title": "Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks", "comments": "ICCV 2017; The codes and model of our P3D ResNet are publicly\n  available at: https://github.com/ZhaofanQiu/pseudo-3d-residual-networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have been regarded as a powerful class of\nmodels for image recognition problems. Nevertheless, it is not trivial when\nutilizing a CNN for learning spatio-temporal video representation. A few\nstudies have shown that performing 3D convolutions is a rewarding approach to\ncapture both spatial and temporal dimensions in videos. However, the\ndevelopment of a very deep 3D CNN from scratch results in expensive\ncomputational cost and memory demand. A valid question is why not recycle\noff-the-shelf 2D networks for a 3D CNN. In this paper, we devise multiple\nvariants of bottleneck building blocks in a residual learning framework by\nsimulating $3\\times3\\times3$ convolutions with $1\\times3\\times3$ convolutional\nfilters on spatial domain (equivalent to 2D CNN) plus $3\\times1\\times1$\nconvolutions to construct temporal connections on adjacent feature maps in\ntime. Furthermore, we propose a new architecture, named Pseudo-3D Residual Net\n(P3D ResNet), that exploits all the variants of blocks but composes each in\ndifferent placement of ResNet, following the philosophy that enhancing\nstructural diversity with going deep could improve the power of neural\nnetworks. Our P3D ResNet achieves clear improvements on Sports-1M video\nclassification dataset against 3D CNN and frame-based 2D CNN by 5.3% and 1.8%,\nrespectively. We further examine the generalization performance of video\nrepresentation produced by our pre-trained P3D ResNet on five different\nbenchmarks and three different tasks, demonstrating superior performances over\nseveral state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 14:24:18 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Qiu", "Zhaofan", ""], ["Yao", "Ting", ""], ["Mei", "Tao", ""]]}, {"id": "1711.10312", "submitter": "Marc Bosch", "authors": "Marc Bosch and Christopher M. Gifford and Pedro A. Rodriguez", "title": "Super-Resolution for Overhead Imagery Using DenseNets and Adversarial\n  Learning", "comments": "9 pages, 9 figures, WACV 2018 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Generative Adversarial Learning allow for new modalities\nof image super-resolution by learning low to high resolution mappings. In this\npaper we present our work using Generative Adversarial Networks (GANs) with\napplications to overhead and satellite imagery. We have experimented with\nseveral state-of-the-art architectures. We propose a GAN-based architecture\nusing densely connected convolutional neural networks (DenseNets) to be able to\nsuper-resolve overhead imagery with a factor of up to 8x. We have also\ninvestigated resolution limits of these networks. We report results on several\npublicly available datasets, including SpaceNet data and IARPA Multi-View\nStereo Challenge, and compare performance with other state-of-the-art\narchitectures.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 14:37:48 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Bosch", "Marc", ""], ["Gifford", "Christopher M.", ""], ["Rodriguez", "Pedro A.", ""]]}, {"id": "1711.10339", "submitter": "Ping Guo", "authors": "Ping Guo (1), Fuqing Duan (2), Pei Wang (3), Yao Yao (2), Qian Yin\n  (1), and Xin Xin (2)", "title": "Pulsar Candidate Identification with Artificial Intelligence Techniques", "comments": "The title may be changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering pulsars is a significant and meaningful research topic in the\nfield of radio astronomy. With the advent of astronomical instruments such as\nhe Five-hundred-meter Aperture Spherical Telescope (FAST) in China, data\nvolumes and data rates are exponentially growing. This fact necessitates a\nfocus on artificial intelligence (AI) technologies that can perform the\nautomatic pulsar candidate identification to mine large astronomical data sets.\nAutomatic pulsar candidate identification can be considered as a task of\ndetermining potential candidates for further investigation and eliminating\nnoises of radio frequency interferences or other non-pulsar signals. It is very\nhard to raise the performance of DCNN-based pulsar identification because the\nlimited training samples restrict network structure to be designed deep enough\nfor learning good features as well as the crucial class imbalance problem due\nto very limited number of real pulsar samples. To address these problems, we\nproposed a framework which combines deep convolution generative adversarial\nnetwork (DCGAN) with support vector machine (SVM) to deal with imbalance class\nproblem and to improve pulsar identification accuracy. DCGAN is used as sample\ngeneration and feature learning model, and SVM is adopted as the classifier for\npredicting candidate's labels in the inference stage. The proposed framework is\na novel technique which not only can solve imbalance class problem but also can\nlearn discriminative feature representations of pulsar candidates instead of\ncomputing hand-crafted features in preprocessing steps too, which makes it more\naccurate for automatic pulsar candidate selection. Experiments on two pulsar\ndatasets verify the effectiveness and efficiency of our proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 05:14:19 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 05:07:55 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Guo", "Ping", ""], ["Duan", "Fuqing", ""], ["Wang", "Pei", ""], ["Yao", "Yao", ""], ["Yin", "Qian", ""], ["Xin", "Xin", ""]]}, {"id": "1711.10352", "submitter": "Hongyu Yang", "authors": "Hongyu Yang, Di Huang, Yunhong Wang, Anil K. Jain", "title": "Learning Face Age Progression: A Pyramid Architecture of GANs", "comments": "CVPR 2018. V4 and V2 are the same, i.e. the conference version; V3 is\n  a related but different work, which is mistakenly submitted and will be\n  submitted as a new arXiv paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two underlying requirements of face age progression, i.e. aging accuracy\nand identity permanence, are not well studied in the literature. In this paper,\nwe present a novel generative adversarial network based approach. It separately\nmodels the constraints for the intrinsic subject-specific characteristics and\nthe age-specific facial changes with respect to the elapsed time, ensuring that\nthe generated faces present desired aging effects while simultaneously keeping\npersonalized properties stable. Further, to generate more lifelike facial\ndetails, high-level age-specific features conveyed by the synthesized face are\nestimated by a pyramidal adversarial discriminator at multiple scales, which\nsimulates the aging effects in a finer manner. The proposed method is\napplicable to diverse face samples in the presence of variations in pose,\nexpression, makeup, etc., and remarkably vivid aging effects are achieved. Both\nvisual fidelity and quantitative evaluations show that the approach advances\nthe state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 15:42:46 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 02:41:36 GMT"}, {"version": "v3", "created": "Sat, 29 Dec 2018 05:41:36 GMT"}, {"version": "v4", "created": "Thu, 10 Jan 2019 05:33:29 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Yang", "Hongyu", ""], ["Huang", "Di", ""], ["Wang", "Yunhong", ""], ["Jain", "Anil K.", ""]]}, {"id": "1711.10370", "submitter": "Ronghang Hu", "authors": "Ronghang Hu, Piotr Doll\\'ar, Kaiming He, Trevor Darrell, Ross Girshick", "title": "Learning to Segment Every Thing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most methods for object instance segmentation require all training examples\nto be labeled with segmentation masks. This requirement makes it expensive to\nannotate new categories and has restricted instance segmentation models to ~100\nwell-annotated classes. The goal of this paper is to propose a new partially\nsupervised training paradigm, together with a novel weight transfer function,\nthat enables training instance segmentation models on a large set of categories\nall of which have box annotations, but only a small fraction of which have mask\nannotations. These contributions allow us to train Mask R-CNN to detect and\nsegment 3000 visual concepts using box annotations from the Visual Genome\ndataset and mask annotations from the 80 classes in the COCO dataset. We\nevaluate our approach in a controlled study on the COCO dataset. This work is a\nfirst step towards instance segmentation models that have broad comprehension\nof the visual world.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 16:05:24 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 13:45:52 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Hu", "Ronghang", ""], ["Doll\u00e1r", "Piotr", ""], ["He", "Kaiming", ""], ["Darrell", "Trevor", ""], ["Girshick", "Ross", ""]]}, {"id": "1711.10378", "submitter": "M. Saquib Sarfraz", "authors": "M. Saquib Sarfraz, Arne Schumann, Andreas Eberle, Rainer Stiefelhagen", "title": "A Pose-Sensitive Embedding for Person Re-Identification with Expanded\n  Cross Neighborhood Re-Ranking", "comments": "CVPR 2018: v2 (fixes, added new results on PRW dataset)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re identification is a challenging retrieval task that requires\nmatching a person's acquired image across non overlapping camera views. In this\npaper we propose an effective approach that incorporates both the fine and\ncoarse pose information of the person to learn a discriminative embedding. In\ncontrast to the recent direction of explicitly modeling body parts or\ncorrecting for misalignment based on these, we show that a rather\nstraightforward inclusion of acquired camera view and/or the detected joint\nlocations into a convolutional neural network helps to learn a very effective\nrepresentation. To increase retrieval performance, re-ranking techniques based\non computed distances have recently gained much attention. We propose a new\nunsupervised and automatic re-ranking framework that achieves state-of-the-art\nre-ranking performance. We show that in contrast to the current\nstate-of-the-art re-ranking methods our approach does not require to compute\nnew rank lists for each image pair (e.g., based on reciprocal neighbors) and\nperforms well by using simple direct rank list based comparison or even by just\nusing the already computed euclidean distances between the images. We show that\nboth our learned representation and our re-ranking method achieve\nstate-of-the-art performance on a number of challenging surveillance image and\nvideo datasets.\n  The code is available online at:\nhttps://github.com/pse-ecn/pose-sensitive-embedding\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 16:25:26 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 11:38:23 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Sarfraz", "M. Saquib", ""], ["Schumann", "Arne", ""], ["Eberle", "Andreas", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1711.10388", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Hyojin Kim, Jayaraman J. Thiagarajan, K. Aditya Mohan,\n  Kyle Champley, Timo Bremer", "title": "Lose The Views: Limited Angle CT Reconstruction via Implicit Sinogram\n  Completion", "comments": "Spotlight presentation at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed Tomography (CT) reconstruction is a fundamental component to a wide\nvariety of applications ranging from security, to healthcare. The classical\ntechniques require measuring projections, called sinograms, from a full\n180$^\\circ$ view of the object. This is impractical in a limited angle\nscenario, when the viewing angle is less than 180$^\\circ$, which can occur due\nto different factors including restrictions on scanning time, limited\nflexibility of scanner rotation, etc. The sinograms obtained as a result, cause\nexisting techniques to produce highly artifact-laden reconstructions. In this\npaper, we propose to address this problem through implicit sinogram completion,\non a challenging real world dataset containing scans of common checked-in\nluggage. We propose a system, consisting of 1D and 2D convolutional neural\nnetworks, that operates on a limited angle sinogram to directly produce the\nbest estimate of a reconstruction. Next, we use the x-ray transform on this\nreconstruction to obtain a \"completed\" sinogram, as if it came from a full\n180$^\\circ$ measurement. We feed this to standard analytical and iterative\nreconstruction techniques to obtain the final reconstruction. We show with\nextensive experimentation that this combined strategy outperforms many\ncompetitive baselines. We also propose a measure of confidence for the\nreconstruction that enables a practitioner to gauge the reliability of a\nprediction made by our network. We show that this measure is a strong indicator\nof quality as measured by the PSNR, while not requiring ground truth at test\ntime. Finally, using a segmentation experiment, we show that our reconstruction\npreserves the 3D structure of objects effectively.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 16:37:14 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 16:23:30 GMT"}, {"version": "v3", "created": "Wed, 11 Jul 2018 17:20:08 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Anirudh", "Rushil", ""], ["Kim", "Hyojin", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Mohan", "K. Aditya", ""], ["Champley", "Kyle", ""], ["Bremer", "Timo", ""]]}, {"id": "1711.10394", "submitter": "Tiago Carvalho Dr.", "authors": "Edmar R. S. de Rezende, Guilherme C. S. Ruppert, Antonio Theophilo,\n  Tiago Carvalho", "title": "Exposing Computer Generated Images by Using Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent computer graphics developments have upraised the quality of the\ngenerated digital content, astonishing the most skeptical viewer. Games and\nmovies have taken advantage of this fact but, at the same time, these advances\nhave brought serious negative impacts like the ones yielded by fakeimages\nproduced with malicious intents. Digital artists can compose artificial images\ncapable of deceiving the great majority of people, turning this into a very\ndangerous weapon in a timespan currently know as Fake News/Post-Truth\" Era. In\nthis work, we propose a new approach for dealing with the problem of detecting\ncomputer generated images, through the application of deep convolutional\nnetworks and transfer learning techniques. We start from Residual Networks and\ndevelop different models adapted to the binary problem of identifying if an\nimage was or not computer generated. Differently from the current\nstate-of-the-art approaches, we don't rely on hand-crafted features, but\nprovide to the model the raw pixel information, achieving the same 0.97 of\nstate-of-the-art methods with two main advantages: our methods show more stable\nresults (depicted by lower variance) and eliminate the laborious and manual\nstep of specialized features extraction and selection.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 16:51:22 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["de Rezende", "Edmar R. S.", ""], ["Ruppert", "Guilherme C. S.", ""], ["Theophilo", "Antonio", ""], ["Carvalho", "Tiago", ""]]}, {"id": "1711.10398", "submitter": "Gui-Song Xia", "authors": "Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo\n  Luo, Mihai Datcu, Marcello Pelillo, Liangpei Zhang", "title": "DOTA: A Large-scale Dataset for Object Detection in Aerial Images", "comments": "Accepted to CVPR 2018", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is an important and challenging problem in computer vision.\nAlthough the past decade has witnessed major advances in object detection in\nnatural scenes, such successes have been slow to aerial imagery, not only\nbecause of the huge variation in the scale, orientation and shape of the object\ninstances on the earth's surface, but also due to the scarcity of\nwell-annotated datasets of objects in aerial scenes. To advance object\ndetection research in Earth Vision, also known as Earth Observation and Remote\nSensing, we introduce a large-scale Dataset for Object deTection in Aerial\nimages (DOTA). To this end, we collect $2806$ aerial images from different\nsensors and platforms. Each image is of the size about 4000-by-4000 pixels and\ncontains objects exhibiting a wide variety of scales, orientations, and shapes.\nThese DOTA images are then annotated by experts in aerial image interpretation\nusing $15$ common object categories. The fully annotated DOTA images contains\n$188,282$ instances, each of which is labeled by an arbitrary (8 d.o.f.)\nquadrilateral To build a baseline for object detection in Earth Vision, we\nevaluate state-of-the-art object detection algorithms on DOTA. Experiments\ndemonstrate that DOTA well represents real Earth Vision applications and are\nquite challenging.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 16:52:44 GMT"}, {"version": "v2", "created": "Sat, 27 Jan 2018 12:46:29 GMT"}, {"version": "v3", "created": "Sun, 19 May 2019 08:07:35 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Xia", "Gui-Song", ""], ["Bai", "Xiang", ""], ["Ding", "Jian", ""], ["Zhu", "Zhen", ""], ["Belongie", "Serge", ""], ["Luo", "Jiebo", ""], ["Datcu", "Mihai", ""], ["Pelillo", "Marcello", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1711.10402", "submitter": "Mengjiao Wang", "authors": "Mengjiao Wang, Zhixin Shu, Shiyang Cheng, Yannis Panagakis, Dimitris\n  Samaras, Stefanos Zafeiriou", "title": "An Adversarial Neuro-Tensorial Approach For Learning Disentangled\n  Representations", "comments": null, "journal-ref": "International Journal of Computer Vision, 2019", "doi": "10.1007/s11263-019-01163-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several factors contribute to the appearance of an object in a visual scene,\nincluding pose, illumination, and deformation, among others. Each factor\naccounts for a source of variability in the data, while the multiplicative\ninteractions of these factors emulate the entangled variability, giving rise to\nthe rich structure of visual object appearance. Disentangling such unobserved\nfactors from visual data is a challenging task, especially when the data have\nbeen captured in uncontrolled recording conditions (also referred to as\n\"in-the-wild\") and label information is not available.\n  In this paper, we propose the first unsupervised deep learning method (with\npseudo-supervision) for disentangling multiple latent factors of variation in\nface images captured in-the-wild. To this end, we propose a deep latent\nvariable model, where the multiplicative interactions of multiple latent\nfactors of variation are explicitly modelled by means of multilinear (tensor)\nstructure. We demonstrate that the proposed approach indeed learns disentangled\nrepresentations of facial expressions and pose, which can be used in various\napplications, including face editing, as well as 3D face reconstruction and\nclassification of facial expression, identity and pose.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 16:57:21 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 22:46:45 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Wang", "Mengjiao", ""], ["Shu", "Zhixin", ""], ["Cheng", "Shiyang", ""], ["Panagakis", "Yannis", ""], ["Samaras", "Dimitris", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1711.10412", "submitter": "Subhayan Mukherjee", "authors": "Subhayan Mukherjee, Irene Cheng, Ram Mohana Reddy Guddeti, Anup Basu", "title": "Entropy-difference based stereo error detection", "comments": null, "journal-ref": null, "doi": "10.1109/IVMSPW.2016.7528177", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo depth estimation is error-prone; hence, effective error detection\nmethods are desirable. Most such existing methods depend on characteristics of\nthe stereo matching cost curve, making them unduly dependent on functional\ndetails of the matching algorithm. As a remedy, we propose a novel error\ndetection approach based solely on the input image and its depth map. Our\nassumption is that, entropy of any point on an image will be significantly\nhigher than the entropy of its corresponding point on the image's depth map. In\nthis paper, we propose a confidence measure, Entropy-Difference (ED) for stereo\ndepth estimates and a binary classification method to identify incorrect\ndepths. Experiments on the Middlebury dataset show the effectiveness of our\nmethod. Our proposed stereo confidence measure outperforms 17 existing measures\nin all aspects except occlusion detection. Established metrics such as\nprecision, accuracy, recall, and area-under-curve are used to demonstrate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 17:11:57 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Mukherjee", "Subhayan", ""], ["Cheng", "Irene", ""], ["Guddeti", "Ram Mohana Reddy", ""], ["Basu", "Anup", ""]]}, {"id": "1711.10448", "submitter": "Manu Goyal", "authors": "Manu Goyal, Neil D. Reeves, Adrian K. Davison, Satyan Rajbhandari,\n  Jennifer Spragg and Moi Hoon Yap", "title": "DFUNet: Convolutional Neural Networks for Diabetic Foot Ulcer\n  Classification", "comments": "Submitted to IEEE Access Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Globally, in 2016, one out of eleven adults suffered from Diabetes Mellitus.\nDiabetic Foot Ulcers (DFU) are a major complication of this disease, which if\nnot managed properly can lead to amputation. Current clinical approaches to DFU\ntreatment rely on patient and clinician vigilance, which has significant\nlimitations such as the high cost involved in the diagnosis, treatment and\nlengthy care of the DFU. We collected an extensive dataset of foot images,\nwhich contain DFU from different patients. In this paper, we have proposed the\nuse of traditional computer vision features for detecting foot ulcers among\ndiabetic patients, which represent a cost-effective, remote and convenient\nhealthcare solution. Furthermore, we used Convolutional Neural Networks (CNNs)\nfor the first time in DFU classification. We have proposed a novel\nconvolutional neural network architecture, DFUNet, with better feature\nextraction to identify the feature differences between healthy skin and the\nDFU. Using 10-fold cross-validation, DFUNet achieved an AUC score of 0.962.\nThis outperformed both the machine learning and deep learning classifiers we\nhave tested. Here we present the development of a novel and highly sensitive\nDFUNet for objectively detecting the presence of DFUs. This novel approach has\nthe potential to deliver a paradigm shift in diabetic foot care.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 18:21:22 GMT"}, {"version": "v2", "created": "Sun, 10 Dec 2017 15:14:49 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Goyal", "Manu", ""], ["Reeves", "Neil D.", ""], ["Davison", "Adrian K.", ""], ["Rajbhandari", "Satyan", ""], ["Spragg", "Jennifer", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "1711.10449", "submitter": "Manu Goyal", "authors": "Manu Goyal, Moi Hoon Yap and Saeed Hassanpour", "title": "Multi-class Semantic Segmentation of Skin Lesions via Fully\n  Convolutional Networks", "comments": "Comp2clinic workshop at Biostec 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Melanoma is clinically difficult to distinguish from common benign skin\nlesions, particularly melanocytic naevus and seborrhoeic keratosis. The\ndermoscopic appearance of these lesions has huge intra-class variations and\nhigh inter-class visual similarities. Most current research is focusing on\nsingle-class segmentation irrespective of classes of skin lesions. In this\nwork, we evaluate the performance of deep learning on multi-class segmentation\nof ISIC-2017 challenge dataset, which consists of 2,750 dermoscopic images. We\npropose an end-to-end solution using fully convolutional networks (FCNs) for\nmulti-class semantic segmentation to automatically segment the melanoma,\nseborrhoeic keratosis and naevus. To improve the performance of FCNs, transfer\nlearning and a hybrid loss function are used. We evaluate the performance of\nthe deep learning segmentation methods for multi-class segmentation and lesion\ndiagnosis (with post-processing method) on the testing set of the ISIC-2017\nchallenge dataset. The results showed that the two-tier level transfer learning\nFCN-8s achieved the overall best result with \\textit{Dice} score of 78.5% in a\nnaevus category, 65.3% in melanoma, and 55.7% in seborrhoeic keratosis in\nmulti-class segmentation and Accuracy of 84.62% for recognition of melanoma in\nlesion diagnosis.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 18:24:15 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 20:34:31 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Goyal", "Manu", ""], ["Yap", "Moi Hoon", ""], ["Hassanpour", "Saeed", ""]]}, {"id": "1711.10464", "submitter": "Yasser El-Sonbaty", "authors": "Ibrahim Abdelkader, Yasser El-Sonbaty and Mohamed El-Habrouk", "title": "Openmv: A Python powered, extensible machine vision camera", "comments": null, "journal-ref": "International Conferences Computer Graphics, Visualization,\n  Computer Vision and Image Processing 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in semiconductor manufacturing processes and large scale integration\nkeep pushing demanding applications further away from centralized processing,\nand closer to the edges of the network (i.e. Edge Computing). It has become\npossible to perform complex in-network image processing using low-power\nembedded smart cameras, enabling a multitude of new collaborative image\nprocessing applications. This paper introduces OpenMV, a new low-power smart\ncamera that lends itself naturally to wireless sensor networks and machine\nvision applications. The uniqueness of this platform lies in running an\nembedded Python3 interpreter, allowing its peripherals and machine vision\nlibrary to be scripted in Python. In addition, its hardware is extensible via\nmodules that augment the platform with new capabilities, such as thermal\nimaging and networking modules.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 08:52:12 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Abdelkader", "Ibrahim", ""], ["El-Sonbaty", "Yasser", ""], ["El-Habrouk", "Mohamed", ""]]}, {"id": "1711.10485", "submitter": "Xiaodong He", "authors": "Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei\n  Huang, and Xiaodong He", "title": "AttnGAN: Fine-Grained Text to Image Generation with Attentional\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an Attentional Generative Adversarial Network\n(AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained\ntext-to-image generation. With a novel attentional generative network, the\nAttnGAN can synthesize fine-grained details at different subregions of the\nimage by paying attentions to the relevant words in the natural language\ndescription. In addition, a deep attentional multimodal similarity model is\nproposed to compute a fine-grained image-text matching loss for training the\ngenerator. The proposed AttnGAN significantly outperforms the previous state of\nthe art, boosting the best reported inception score by 14.14% on the CUB\ndataset and 170.25% on the more challenging COCO dataset. A detailed analysis\nis also performed by visualizing the attention layers of the AttnGAN. It for\nthe first time shows that the layered attentional GAN is able to automatically\nselect the condition at the word level for generating different parts of the\nimage.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 18:59:50 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Xu", "Tao", ""], ["Zhang", "Pengchuan", ""], ["Huang", "Qiuyuan", ""], ["Zhang", "Han", ""], ["Gan", "Zhe", ""], ["Huang", "Xiaolei", ""], ["He", "Xiaodong", ""]]}, {"id": "1711.10515", "submitter": "Subhayan Mukherjee", "authors": "Subhayan Mukherjee, Irene Cheng, Anup Basu", "title": "Highlighting objects of interest in an image by integrating saliency and\n  depth", "comments": null, "journal-ref": null, "doi": "10.1109/ICIP.2016.7532308", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo images have been captured primarily for 3D reconstruction in the past.\nHowever, the depth information acquired from stereo can also be used along with\nsaliency to highlight certain objects in a scene. This approach can be used to\nmake still images more interesting to look at, and highlight objects of\ninterest in the scene. We introduce this novel direction in this paper, and\ndiscuss the theoretical framework behind the approach. Even though we use depth\nfrom stereo in this work, our approach is applicable to depth data acquired\nfrom any sensor modality. Experimental results on both indoor and outdoor\nscenes demonstrate the benefits of our algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 19:12:49 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Mukherjee", "Subhayan", ""], ["Cheng", "Irene", ""], ["Basu", "Anup", ""]]}, {"id": "1711.10520", "submitter": "Chi Nhan Duong", "authors": "Chi Nhan Duong, Kha Gia Quach, Khoa Luu, T. Hoang Ngan Le, Marios\n  Savvides, and Tien D. Bui", "title": "Learning from Longitudinal Face Demonstration - Where Tractable Deep\n  Modeling Meets Inverse Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel Subject-dependent Deep Aging Path (SDAP), which\ninherits the merits of both Generative Probabilistic Modeling and Inverse\nReinforcement Learning to model the facial structures and the longitudinal face\naging process of a given subject. The proposed SDAP is optimized using\ntractable log-likelihood objective functions with Convolutional Neural Networks\n(CNNs) based deep feature extraction. Instead of applying a fixed aging\ndevelopment path for all input faces and subjects, SDAP is able to provide the\nmost appropriate aging development path for individual subject that optimizes\nthe reward aging formulation. Unlike previous methods that can take only one\nimage as the input, SDAP further allows multiple images as inputs, i.e. all\ninformation of a subject at either the same or different ages, to produce the\noptimal aging path for the given subject. Finally, SDAP allows efficiently\nsynthesizing in-the-wild aging faces. The proposed model is experimented in\nboth tasks of face aging synthesis and cross-age face verification. The\nexperimental results consistently show SDAP achieves the state-of-the-art\nperformance on numerous face aging databases, i.e. FG-NET, MORPH, AginG Faces\nin the Wild (AGFW), and Cross-Age Celebrity Dataset (CACD). Furthermore, we\nalso evaluate the performance of SDAP on large-scale Megaface challenge to\ndemonstrate the advantages of the proposed solution.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 19:24:20 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 04:59:11 GMT"}, {"version": "v3", "created": "Tue, 4 Sep 2018 17:11:20 GMT"}, {"version": "v4", "created": "Sat, 2 Feb 2019 17:22:16 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Duong", "Chi Nhan", ""], ["Quach", "Kha Gia", ""], ["Luu", "Khoa", ""], ["Le", "T. Hoang Ngan", ""], ["Savvides", "Marios", ""], ["Bui", "Tien D.", ""]]}, {"id": "1711.10521", "submitter": "Fatmatulzehra Uslu Ms.", "authors": "Fatmatulzehra Uslu and Anil Anthony Bharath", "title": "A Recursive Bayesian Approach To Describe Retinal Vasculature Geometry", "comments": "26 pages,13 figures, journal paper", "journal-ref": null, "doi": "10.1016/j.patcog.2018.10.017", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demographic studies suggest that changes in the retinal vasculature geometry,\nespecially in vessel width, are associated with the incidence or progression of\neye-related or systemic diseases. To date, the main information source for\nwidth estimation from fundus images has been the intensity profile between\nvessel edges. However, there are many factors affecting the intensity profile:\npathologies, the central light reflex and local illumination levels, to name a\nfew. In this study, we introduce three information sources for width\nestimation. These are the probability profiles of vessel interior, centreline\nand edge locations generated by a deep network. The probability profiles\nprovide direct access to vessel geometry and are used in the likelihood\ncalculation for a Bayesian method, particle filtering. We also introduce a\ngeometric model which can handle non-ideal conditions of the probability\nprofiles. Our experiments conducted on the REVIEW dataset yielded consistent\nestimates of vessel width, even in cases when one of the vessel edges is\ndifficult to identify. Moreover, our results suggest that the method is better\nthan human observers at locating edges of low contrast vessels.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 19:28:27 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Uslu", "Fatmatulzehra", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1711.10535", "submitter": "Ke Yan", "authors": "Ke Yan, Xiaosong Wang, Le Lu, Ling Zhang, Adam Harrison, Mohammadhad\n  Bagheri, Ronald Summers", "title": "Deep Lesion Graphs in the Wild: Relationship Learning and Organization\n  of Significant Radiology Image Findings in a Diverse Large-scale Lesion\n  Database", "comments": "Accepted by CVPR2018. DeepLesion url added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiologists in their daily work routinely find and annotate significant\nabnormalities on a large number of radiology images. Such abnormalities, or\nlesions, have collected over years and stored in hospitals' picture archiving\nand communication systems. However, they are basically unsorted and lack\nsemantic annotations like type and location. In this paper, we aim to organize\nand explore them by learning a deep feature representation for each lesion. A\nlarge-scale and comprehensive dataset, DeepLesion, is introduced for this task.\nDeepLesion contains bounding boxes and size measurements of over 32K lesions.\nTo model their similarity relationship, we leverage multiple supervision\ninformation including types, self-supervised location coordinates and sizes.\nThey require little manual annotation effort but describe useful attributes of\nthe lesions. Then, a triplet network is utilized to learn lesion embeddings\nwith a sequential sampling strategy to depict their hierarchical similarity\nstructure. Experiments show promising qualitative and quantitative results on\nlesion retrieval, clustering, and classification. The learned embeddings can be\nfurther employed to build a lesion graph for various clinically useful\napplications. We propose algorithms for intra-patient lesion matching and\nmissing annotation mining. Experimental results validate their effectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 20:06:10 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 16:43:21 GMT"}, {"version": "v3", "created": "Sat, 28 Jul 2018 20:51:09 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Yan", "Ke", ""], ["Wang", "Xiaosong", ""], ["Lu", "Le", ""], ["Zhang", "Ling", ""], ["Harrison", "Adam", ""], ["Bagheri", "Mohammadhad", ""], ["Summers", "Ronald", ""]]}, {"id": "1711.10563", "submitter": "Ronald Kemker", "authors": "Ronald Kemker and Christopher Kanan", "title": "FearNet: Brain-Inspired Model for Incremental Learning", "comments": "To appear in ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental class learning involves sequentially learning classes in bursts\nof examples from the same class. This violates the assumptions that underlie\nmethods for training standard deep neural networks, and will cause them to\nsuffer from catastrophic forgetting. Arguably, the best method for incremental\nclass learning is iCaRL, but it requires storing training examples for each\nclass, making it challenging to scale. Here, we propose FearNet for incremental\nclass learning. FearNet is a generative model that does not store previous\nexamples, making it memory efficient. FearNet uses a brain-inspired dual-memory\nsystem in which new memories are consolidated from a network for recent\nmemories inspired by the mammalian hippocampal complex to a network for\nlong-term storage inspired by medial prefrontal cortex. Memory consolidation is\ninspired by mechanisms that occur during sleep. FearNet also uses a module\ninspired by the basolateral amygdala for determining which memory system to use\nfor recall. FearNet achieves state-of-the-art performance at incremental class\nlearning on image (CIFAR-100, CUB-200) and audio classification (AudioSet)\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:26:15 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 20:32:27 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Kemker", "Ronald", ""], ["Kanan", "Christopher", ""]]}, {"id": "1711.10577", "submitter": "Zhe Zhu", "authors": "Zhe Zhu, Michael Harowicz, Jun Zhang, Ashirbani Saha, Lars J. Grimm,\n  E.Shelley Hwang and Maciej A. Mazurowski", "title": "Deep learning analysis of breast MRIs for prediction of occult invasive\n  disease in ductal carcinoma in situ", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To determine whether deep learning-based algorithms applied to\nbreast MR images can aid in the prediction of occult invasive disease following\nthe di- agnosis of ductal carcinoma in situ (DCIS) by core needle biopsy.\nMaterial and Methods: In this institutional review board-approved study, we\nanalyzed dynamic contrast-enhanced fat-saturated T1-weighted MRI sequences of\n131 patients at our institution with a core needle biopsy-confirmed diagnosis\nof DCIS. The patients had no preoperative therapy before breast MRI and no\nprior history of breast cancer. We explored two different deep learning\napproaches to predict whether there was a hidden (occult) invasive component in\nthe analyzed tumors that was ultimately detected at surgical excision. In the\nfirst approach, we adopted the transfer learning strategy, in which a network\npre-trained on a large dataset of natural images is fine-tuned with our DCIS\nimages. Specifically, we used the GoogleNet model pre-trained on the ImageNet\ndataset. In the second approach, we used a pre-trained network to extract deep\nfeatures, and a support vector machine (SVM) that utilizes these features to\npredict the upstaging of the DCIS. We used 10-fold cross validation and the\narea under the ROC curve (AUC) to estimate the performance of the predictive\nmodels. Results: The best classification performance was obtained using the\ndeep features approach with GoogleNet model pre-trained on ImageNet as the\nfeature extractor and a polynomial kernel SVM used as the classifier (AUC =\n0.70, 95% CI: 0.58- 0.79). For the transfer learning based approach, the\nhighest AUC obtained was 0.53 (95% CI: 0.41-0.62). Conclusion: Convolutional\nneural networks could potentially be used to identify occult invasive disease\nin patients diagnosed with DCIS at the initial core needle biopsy.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:52:49 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Zhu", "Zhe", ""], ["Harowicz", "Michael", ""], ["Zhang", "Jun", ""], ["Saha", "Ashirbani", ""], ["Grimm", "Lars J.", ""], ["Hwang", "E. Shelley", ""], ["Mazurowski", "Maciej A.", ""]]}, {"id": "1711.10644", "submitter": "Seungkyun Hong", "authors": "Seungkyun Hong, Seongchan Kim, Minsu Joh, Sa-kwang Song", "title": "PSIque: Next Sequence Prediction of Satellite Images using a\n  Convolutional Sequence-to-Sequence Network", "comments": "Workshop on Deep Learning for Physical Sciences (DLPS 2017), NIPS\n  2017, Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting unseen weather phenomena is an important issue for disaster\nmanagement. In this paper, we suggest a model for a convolutional\nsequence-to-sequence autoencoder for predicting undiscovered weather situations\nfrom previous satellite images. We also propose a symmetric skip connection\nbetween encoder and decoder modules to produce more comprehensive image\npredictions. To examine our model performance, we conducted experiments for\neach suggested model to predict future satellite images from historical\nsatellite images. A specific combination of skip connection and\nsequence-to-sequence autoencoder was able to generate closest prediction from\nthe ground truth image.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 02:02:13 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 21:25:33 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Hong", "Seungkyun", ""], ["Kim", "Seongchan", ""], ["Joh", "Minsu", ""], ["Song", "Sa-kwang", ""]]}, {"id": "1711.10658", "submitter": "Mingkun Yang", "authors": "Xiang Bai, Mingkun Yang, Tengteng Huang, Zhiyong Dou, Rui Yu, Yongchao\n  Xu", "title": "Deep-Person: Learning Discriminative Deep Features for Person\n  Re-Identification", "comments": "Accepted to Pattern Recognition. The code is released:\n  https://github.com/zydou/Deep-Person", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many methods of person re-identification (Re-ID) rely on part-based\nfeature representation to learn a discriminative pedestrian descriptor.\nHowever, the spatial context between these parts is ignored for the independent\nextractor to each separate part. In this paper, we propose to apply Long\nShort-Term Memory (LSTM) in an end-to-end way to model the pedestrian, seen as\na sequence of body parts from head to foot. Integrating the contextual\ninformation strengthens the discriminative ability of local representation. We\nalso leverage the complementary information between local and global feature.\nFurthermore, we integrate both identification task and ranking task in one\nnetwork, where a discriminative embedding and a similarity measurement are\nlearned concurrently. This results in a novel three-branch framework named\nDeep-Person, which learns highly discriminative features for person Re-ID.\nExperimental results demonstrate that Deep-Person outperforms the\nstate-of-the-art methods by a large margin on three challenging datasets\nincluding Market-1501, CUHK03, and DukeMTMC-reID. Specifically, combining with\na re-ranking approach, we achieve a 90.84% mAP on Market-1501 under single\nquery setting.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 03:15:07 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 02:29:03 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 02:30:38 GMT"}, {"version": "v4", "created": "Mon, 9 Sep 2019 03:34:10 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Bai", "Xiang", ""], ["Yang", "Mingkun", ""], ["Huang", "Tengteng", ""], ["Dou", "Zhiyong", ""], ["Yu", "Rui", ""], ["Xu", "Yongchao", ""]]}, {"id": "1711.10662", "submitter": "Wellington Pinheiro Dos Santos", "authors": "Jinmi Lee and Wellington Pinheiro dos Santos", "title": "An Adaptive Fuzzy-Based System to Simulate, Quantify and Compensate\n  Color Blindness", "comments": null, "journal-ref": "Integrated Computer-Aided Engineering, v. 18, p. 29-40, 2011", "doi": "10.3233/ICA-2011-0356", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  About 8% of the male population of the world are affected by a determined\ntype of color vision disturbance, which varies from the partial to complete\nreduction of the ability to distinguish certain colors. A considerable amount\nof color blind people are able to live all life long without knowing they have\ncolor vision disabilities and abnormalities. Nowadays the evolution of\ninformation technology and computer science, specifically image processing\ntechniques and computer graphics, can be fundamental to aid at the development\nof adaptive color blindness correction tools. This paper presents a software\ntool based on Fuzzy Logic to evaluate the type and the degree of color\nblindness a person suffer from. In order to model several degrees of color\nblindness, herein this work we modified the classical linear transform-based\nsimulation method by the use of fuzzy parameters. We also proposed four new\nmethods to correct color blindness based on a fuzzy approach: Methods A and B,\nwith and without histogram equalization. All the methods are based on\ncombinations of linear transforms and histogram operations. In order to\nevaluate the results we implemented a web-based survey to get the best results\naccording to optimize to distinguish different elements in an image. Results\nobtained from 40 volunteers proved that the Method B with histogram\nequalization got the best results for about 47% of volunteers.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 03:31:14 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Lee", "Jinmi", ""], ["Santos", "Wellington Pinheiro dos", ""]]}, {"id": "1711.10669", "submitter": "Jhony Kaesemodel Pontes", "authors": "Jhony K. Pontes, Chen Kong, Sridha Sridharan, Simon Lucey, Anders\n  Eriksson, Clinton Fookes", "title": "Image2Mesh: A Learning Framework for Single Image 3D Reconstruction", "comments": "9 pages, 4 figures", "journal-ref": "Asian Conference on Computer Vision (ACCV) 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One challenge that remains open in 3D deep learning is how to efficiently\nrepresent 3D data to feed deep networks. Recent works have relied on volumetric\nor point cloud representations, but such approaches suffer from a number of\nissues such as computational complexity, unordered data, and lack of finer\ngeometry. This paper demonstrates that a mesh representation (i.e. vertices and\nfaces to form polygonal surfaces) is able to capture fine-grained geometry for\n3D reconstruction tasks. A mesh however is also unstructured data similar to\npoint clouds. We address this problem by proposing a learning framework to\ninfer the parameters of a compact mesh representation rather than learning from\nthe mesh itself. This compact representation encodes a mesh using free-form\ndeformation and a sparse linear combination of models allowing us to\nreconstruct 3D meshes from single images. In contrast to prior work, we do not\nrely on silhouettes and landmarks to perform 3D reconstruction. We evaluate our\nmethod on synthetic and real-world datasets with very promising results. Our\nframework efficiently reconstructs 3D objects in a low-dimensional way while\npreserving its important geometrical aspects.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 03:57:32 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Pontes", "Jhony K.", ""], ["Kong", "Chen", ""], ["Sridharan", "Sridha", ""], ["Lucey", "Simon", ""], ["Eriksson", "Anders", ""], ["Fookes", "Clinton", ""]]}, {"id": "1711.10678", "submitter": "Zhenliang He", "authors": "Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan and Xilin Chen", "title": "AttGAN: Facial Attribute Editing by Only Changing What You Want", "comments": "Submitted to IEEE Transactions on Image Processing, Code:\n  https://github.com/LynnHo/AttGAN-Tensorflow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attribute editing aims to manipulate single or multiple attributes of\na face image, i.e., to generate a new face with desired attributes while\npreserving other details. Recently, generative adversarial net (GAN) and\nencoder-decoder architecture are usually incorporated to handle this task with\npromising results. Based on the encoder-decoder architecture, facial attribute\nediting is achieved by decoding the latent representation of the given face\nconditioned on the desired attributes. Some existing methods attempt to\nestablish an attribute-independent latent representation for further attribute\nediting. However, such attribute-independent constraint on the latent\nrepresentation is excessive because it restricts the capacity of the latent\nrepresentation and may result in information loss, leading to over-smooth and\ndistorted generation. Instead of imposing constraints on the latent\nrepresentation, in this work we apply an attribute classification constraint to\nthe generated image to just guarantee the correct change of desired attributes,\ni.e., to \"change what you want\". Meanwhile, the reconstruction learning is\nintroduced to preserve attribute-excluding details, in other words, to \"only\nchange what you want\". Besides, the adversarial learning is employed for\nvisually realistic editing. These three components cooperate with each other\nforming an effective framework for high quality facial attribute editing,\nreferred as AttGAN. Furthermore, our method is also directly applicable for\nattribute intensity control and can be naturally extended for attribute style\nmanipulation. Experiments on CelebA dataset show that our method outperforms\nthe state-of-the-arts on realistic attribute editing with facial details well\npreserved.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 04:50:31 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 04:28:30 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2018 10:08:00 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["He", "Zhenliang", ""], ["Zuo", "Wangmeng", ""], ["Kan", "Meina", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1711.10683", "submitter": "Aayush Bansal", "authors": "Victor Fragoso, Chunhui Liu, Aayush Bansal, Deva Ramanan", "title": "Patch Correspondences for Interpreting Pixel-level CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present compositional nearest neighbors (CompNN), a simple approach to\nvisually interpreting distributed representations learned by a convolutional\nneural network (CNN) for pixel-level tasks (e.g., image synthesis and\nsegmentation). It does so by reconstructing both a CNN's input and output image\nby copy-pasting corresponding patches from the training set with similar\nfeature embeddings. To do so efficiently, it makes of a patch-match-based\nalgorithm that exploits the fact that the patch representations learned by a\nCNN for pixel level tasks vary smoothly. Finally, we show that CompNN can be\nused to establish semantic correspondences between two images and control\nproperties of the output image by modifying the images contained in the\ntraining set. We present qualitative and quantitative experiments for semantic\nsegmentation and image-to-image translation that demonstrate that CompNN is a\ngood tool for interpreting the embeddings learned by pixel-level CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 05:13:32 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 09:06:08 GMT"}, {"version": "v3", "created": "Tue, 27 Feb 2018 03:08:14 GMT"}, {"version": "v4", "created": "Tue, 4 Sep 2018 01:35:17 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Fragoso", "Victor", ""], ["Liu", "Chunhui", ""], ["Bansal", "Aayush", ""], ["Ramanan", "Deva", ""]]}, {"id": "1711.10684", "submitter": "Qingjie Liu", "authors": "Zhengxin Zhang, Qingjie Liu and Yunhong Wang", "title": "Road Extraction by Deep Residual U-Net", "comments": "Submitted to IEEE Geoscience and Remote Sensing Letters", "journal-ref": null, "doi": "10.1109/LGRS.2018.2802944", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Road extraction from aerial images has been a hot research topic in the field\nof remote sensing image analysis. In this letter, a semantic segmentation\nneural network which combines the strengths of residual learning and U-Net is\nproposed for road area extraction. The network is built with residual units and\nhas similar architecture to that of U-Net. The benefits of this model is\ntwo-fold: first, residual units ease training of deep networks. Second, the\nrich skip connections within the network could facilitate information\npropagation, allowing us to design networks with fewer parameters however\nbetter performance. We test our network on a public road dataset and compare it\nwith U-Net and other two state of the art deep learning based road extraction\nmethods. The proposed approach outperforms all the comparing methods, which\ndemonstrates its superiority over recently developed state of the arts.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 05:16:14 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Zhang", "Zhengxin", ""], ["Liu", "Qingjie", ""], ["Wang", "Yunhong", ""]]}, {"id": "1711.10688", "submitter": "Yong Man Ro", "authors": "Seong Tae Kim and Yong Man Ro", "title": "Facial Dynamics Interpreter Network: What are the Important Relations\n  between Local Dynamics for Facial Trait Estimation?", "comments": "Accepted by ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human face analysis is an important task in computer vision. According to\ncognitive-psychological studies, facial dynamics could provide crucial cues for\nface analysis. The motion of a facial local region in facial expression is\nrelated to the motion of other facial local regions. In this paper, a novel\ndeep learning approach, named facial dynamics interpreter network, has been\nproposed to interpret the important relations between local dynamics for\nestimating facial traits from expression sequence. The facial dynamics\ninterpreter network is designed to be able to encode a relational importance,\nwhich is used for interpreting the relation between facial local dynamics and\nestimating facial traits. By comparative experiments, the effectiveness of the\nproposed method has been verified. The important relations between facial local\ndynamics are investigated by the proposed facial dynamics interpreter network\nin gender classification and age estimation. Moreover, experimental results\nshow that the proposed method outperforms the state-of-the-art methods in\ngender classification and age estimation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 05:44:18 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 05:25:40 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Kim", "Seong Tae", ""], ["Ro", "Yong Man", ""]]}, {"id": "1711.10693", "submitter": "Anthony Ortiz", "authors": "Dalton Rosario, Christoph Borel, Damon Conover, Ryan McAlinden,\n  Anthony Ortiz, Sarah Shiver, Blair Simon", "title": "Small Drone Field Experiment: Data Collection & Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following an initiative formalized in April 2016 formally known as ARL West\nbetween the U.S. Army Research Laboratory (ARL) and University of Southern\nCalifornia's Institute for Creative Technologies (USC ICT), a field experiment\nwas coordinated and executed in the summer of 2016 by ARL, USC ICT, and\nHeadwall Photonics. The purpose was to image part of the USC main campus in Los\nAngeles, USA, using two portable COTS (commercial off the shelf) aerial drone\nsolutions for data acquisition, for photogrammetry (3D reconstruction from\nimages), and fusion of hyperspectral data with the recovered set of 3D point\nclouds representing the target area. The research aims for determining the\nviability of having a machine capable of segmenting the target area into key\nmaterial classes (e.g., manmade structures, live vegetation, water) for use in\nmultiple purposes, to include providing the user with a more accurate scene\nunderstanding and enabling the unsupervised automatic sampling of meaningful\nmaterial classes from the target area for adaptive semi-supervised machine\nlearning. In the latter, a target set library may be used for automatic machine\ntraining with data of local material classes, as an example, to increase the\nprediction chances of machines recognizing targets. The field experiment and\nassociated data post processing approach to correct for reflectance,\ngeo-rectify, recover the area's dense point clouds from images, register\nspectral with elevation properties of scene surfaces from the independently\ncollected datasets, and generate the desired scene segmented maps are\ndiscussed. Lessons learned from the experience are also highlighted throughout\nthe paper.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 06:08:16 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Rosario", "Dalton", ""], ["Borel", "Christoph", ""], ["Conover", "Damon", ""], ["McAlinden", "Ryan", ""], ["Ortiz", "Anthony", ""], ["Shiver", "Sarah", ""], ["Simon", "Blair", ""]]}, {"id": "1711.10700", "submitter": "Pascal Getreuer", "authors": "Pascal Getreuer, Ignacio Garcia-Dorado, John Isidoro, Sungjoon Choi,\n  Frank Ong, Peyman Milanfar", "title": "BLADE: Filter Learning for General Purpose Computational Photography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Rapid and Accurate Image Super Resolution (RAISR) method of Romano,\nIsidoro, and Milanfar is a computationally efficient image upscaling method\nusing a trained set of filters. We describe a generalization of RAISR, which we\nname Best Linear Adaptive Enhancement (BLADE). This approach is a trainable\nedge-adaptive filtering framework that is general, simple, computationally\nefficient, and useful for a wide range of problems in computational\nphotography. We show applications to operations which may appear in a camera\npipeline including denoising, demosaicing, and stylization.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 06:38:41 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 23:26:05 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Getreuer", "Pascal", ""], ["Garcia-Dorado", "Ignacio", ""], ["Isidoro", "John", ""], ["Choi", "Sungjoon", ""], ["Ong", "Frank", ""], ["Milanfar", "Peyman", ""]]}, {"id": "1711.10703", "submitter": "Ying Tai", "authors": "Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, Jian Yang", "title": "FSRNet: End-to-End Learning Face Super-Resolution with Facial Priors", "comments": "Chen and Tai contributed equally to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face Super-Resolution (SR) is a domain-specific super-resolution problem. The\nspecific facial prior knowledge could be leveraged for better super-resolving\nface images. We present a novel deep end-to-end trainable Face Super-Resolution\nNetwork (FSRNet), which makes full use of the geometry prior, i.e., facial\nlandmark heatmaps and parsing maps, to super-resolve very low-resolution (LR)\nface images without well-aligned requirement. Specifically, we first construct\na coarse SR network to recover a coarse high-resolution (HR) image. Then, the\ncoarse HR image is sent to two branches: a fine SR encoder and a prior\ninformation estimation network, which extracts the image features, and\nestimates landmark heatmaps/parsing maps respectively. Both image features and\nprior information are sent to a fine SR decoder to recover the HR image. To\nfurther generate realistic faces, we propose the Face Super-Resolution\nGenerative Adversarial Network (FSRGAN) to incorporate the adversarial loss\ninto FSRNet. Moreover, we introduce two related tasks, face alignment and\nparsing, as the new evaluation metrics for face SR, which address the\ninconsistency of classic metrics w.r.t. visual perception. Extensive benchmark\nexperiments show that FSRNet and FSRGAN significantly outperforms state of the\narts for very LR face SR, both quantitatively and qualitatively. Code will be\nmade available upon publication.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 06:47:04 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Chen", "Yu", ""], ["Tai", "Ying", ""], ["Liu", "Xiaoming", ""], ["Shen", "Chunhua", ""], ["Yang", "Jian", ""]]}, {"id": "1711.10729", "submitter": "Zhang Chen", "authors": "Xinqing Guo, Zhang Chen, Siyuan Li, Yang Yang, Jingyi Yu", "title": "Deep Eyes: Binocular Depth-from-Focus on Focal Stack Pairs", "comments": null, "journal-ref": "The Chinese Conference on Pattern Recognition and Computer Vision\n  (PRCV), 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human visual system relies on both binocular stereo cues and monocular\nfocusness cues to gain effective 3D perception. In computer vision, the two\nproblems are traditionally solved in separate tracks. In this paper, we present\na unified learning-based technique that simultaneously uses both types of cues\nfor depth inference. Specifically, we use a pair of focal stacks as input to\nemulate human perception. We first construct a comprehensive focal stack\ntraining dataset synthesized by depth-guided light field rendering. We then\nconstruct three individual networks: a Focus-Net to extract depth from a single\nfocal stack, a EDoF-Net to obtain the extended depth of field (EDoF) image from\nthe focal stack, and a Stereo-Net to conduct stereo matching. We show how to\nintegrate them into a unified BDfF-Net to obtain high-quality depth maps.\nComprehensive experiments show that our approach outperforms the\nstate-of-the-art in both accuracy and speed and effectively emulates human\nvision systems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 08:52:17 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 09:39:21 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2018 17:30:24 GMT"}, {"version": "v4", "created": "Mon, 10 Aug 2020 16:03:37 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Guo", "Xinqing", ""], ["Chen", "Zhang", ""], ["Li", "Siyuan", ""], ["Yang", "Yang", ""], ["Yu", "Jingyi", ""]]}, {"id": "1711.10733", "submitter": "Florian Bernard", "authors": "Florian Bernard, Christian Theobalt, Michael Moeller", "title": "DS*: Tighter Lifting-Free Convex Relaxations for Quadratic Matching\n  Problems", "comments": "Published at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study convex relaxations of quadratic optimisation problems\nover permutation matrices. While existing semidefinite programming approaches\ncan achieve remarkably tight relaxations, they have the strong disadvantage\nthat they lift the original $n {\\times} n$-dimensional variable to an $n^2\n{\\times} n^2$-dimensional variable, which limits their practical applicability.\nIn contrast, here we present a lifting-free convex relaxation that is provably\nat least as tight as existing (lifting-free) convex relaxations. We demonstrate\nexperimentally that our approach is superior to existing convex and non-convex\nmethods for various problems, including image arrangement and multi-graph\nmatching.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 09:04:27 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 07:22:55 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Bernard", "Florian", ""], ["Theobalt", "Christian", ""], ["Moeller", "Michael", ""]]}, {"id": "1711.10735", "submitter": "Ziqiang Zheng", "authors": "Ziqiang Zheng and Wang Chao and Zhibin Yu and Nan Wang and Haiyong\n  Zheng and Bing Zheng", "title": "Unpaired Photo-to-Caricature Translation on Faces in the Wild", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, image-to-image translation has been made much progress owing to the\nsuccess of conditional Generative Adversarial Networks (cGANs). And some\nunpaired methods based on cycle consistency loss such as DualGAN, CycleGAN and\nDiscoGAN are really popular. However, it's still very challenging for\ntranslation tasks with the requirement of high-level visual information\nconversion, such as photo-to-caricature translation that requires satire,\nexaggeration, lifelikeness and artistry. We present an approach for learning to\ntranslate faces in the wild from the source photo domain to the target\ncaricature domain with different styles, which can also be used for other\nhigh-level image-to-image translation tasks. In order to capture global\nstructure with local statistics while translation, we design a dual pathway\nmodel with one coarse discriminator and one fine discriminator. For generator,\nwe provide one extra perceptual loss in association with adversarial loss and\ncycle consistency loss to achieve representation learning for two different\ndomains. Also the style can be learned by the auxiliary noise input.\nExperiments on photo-to-caricature translation of faces in the wild show\nconsiderable performance gain of our proposed method over state-of-the-art\ntranslation methods as well as its potential real applications.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 09:12:50 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 01:01:15 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Zheng", "Ziqiang", ""], ["Chao", "Wang", ""], ["Yu", "Zhibin", ""], ["Wang", "Nan", ""], ["Zheng", "Haiyong", ""], ["Zheng", "Bing", ""]]}, {"id": "1711.10742", "submitter": "Ziqiang Zheng", "authors": "Ziqiang Zheng and Zhibin Yu and Haiyong Zheng and Chao Wang and Nan\n  Wang", "title": "Pipeline Generative Adversarial Networks for Facial Images Generation\n  with Multiple Attributes", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks are proved to be efficient on various kinds\nof image generation tasks. However, it is still a challenge if we want to\ngenerate images precisely. Many researchers focus on how to generate images\nwith one attribute. But image generation under multiple attributes is still a\ntough work. In this paper, we try to generate a variety of face images under\nmultiple constraints using a pipeline process. The Pip-GAN (Pipeline Generative\nAdversarial Network) we present employs a pipeline network structure which can\ngenerate a complex facial image step by step using a neutral face image. We\napplied our method on two face image databases and demonstrate its ability to\ngenerate convincing novel images of unseen identities under multiple conditions\npreviously.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 09:25:36 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Zheng", "Ziqiang", ""], ["Yu", "Zhibin", ""], ["Zheng", "Haiyong", ""], ["Wang", "Chao", ""], ["Wang", "Nan", ""]]}, {"id": "1711.10752", "submitter": "Hiba Chougrad", "authors": "Hiba Chougrad, Hamid Zouaki, Omar Alheyane", "title": "Convolutional Neural Networks for Breast Cancer Screening: Transfer\n  Learning with Exponential Decay", "comments": "6 pages, 2 figures, NIPS ML4H 2017: Machine Learning for Health\n  Workshop at NIPS 2017, Long Beach, CA, United States, December 8, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a Computer Assisted Diagnosis (CAD) system based on\na deep Convolutional Neural Network (CNN) model, to build an end-to-end\nlearning process that classifies breast mass lesions. We investigate the impact\nthat has transfer learning when large data is scarce, and explore the proper\nway to fine-tune the layers to learn features that are more specific to the new\ndata. The proposed approach showed better performance compared to other\nproposals that classified the same dataset.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 10:08:46 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Chougrad", "Hiba", ""], ["Zouaki", "Hamid", ""], ["Alheyane", "Omar", ""]]}, {"id": "1711.10761", "submitter": "Sam Leroux", "authors": "Sam Leroux, Steven Bohez, Tim Verbelen, Bert Vankeirsbilck, Pieter\n  Simoens, Bart Dhoedt", "title": "Transfer Learning with Binary Neural Networks", "comments": "Machine Learning on the Phone and other Consumer Devices, NIPS2017\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has shown that it is possible to train deep neural networks\nwith low precision weights and activations. In the extreme case it is even\npossible to constrain the network to binary values. The costly floating point\nmultiplications are then reduced to fast logical operations. High end smart\nphones such as Google's Pixel 2 and Apple's iPhone X are already equipped with\nspecialised hardware for image processing and it is very likely that other\nfuture consumer hardware will also have dedicated accelerators for deep neural\nnetworks. Binary neural networks are attractive in this case because the\nlogical operations are very fast and efficient when implemented in hardware. We\npropose a transfer learning based architecture where we first train a binary\nnetwork on Imagenet and then retrain part of the network for different tasks\nwhile keeping most of the network fixed. The fixed binary part could be\nimplemented in a hardware accelerator while the last layers of the network are\nevaluated in software. We show that a single binary neural network trained on\nthe Imagenet dataset can indeed be used as a feature extractor for other\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 10:28:02 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Leroux", "Sam", ""], ["Bohez", "Steven", ""], ["Verbelen", "Tim", ""], ["Vankeirsbilck", "Bert", ""], ["Simoens", "Pieter", ""], ["Dhoedt", "Bart", ""]]}, {"id": "1711.10775", "submitter": "Donna Xu", "authors": "Donna Xu, Ivor W. Tsang, Ying Zhang", "title": "Online Product Quantization", "comments": "To appear in IEEE Transactions on Knowledge and Data Engineering\n  (DOI: 10.1109/TKDE.2018.2817526)", "journal-ref": null, "doi": "10.1109/TKDE.2018.2817526", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate nearest neighbor (ANN) search has achieved great success in many\ntasks. However, existing popular methods for ANN search, such as hashing and\nquantization methods, are designed for static databases only. They cannot\nhandle well the database with data distribution evolving dynamically, due to\nthe high computational effort for retraining the model based on the new\ndatabase. In this paper, we address the problem by developing an online product\nquantization (online PQ) model and incrementally updating the quantization\ncodebook that accommodates to the incoming streaming data. Moreover, to further\nalleviate the issue of large scale computation for the online PQ update, we\ndesign two budget constraints for the model to update partial PQ codebook\ninstead of all. We derive a loss bound which guarantees the performance of our\nonline PQ model. Furthermore, we develop an online PQ model over a sliding\nwindow with both data insertion and deletion supported, to reflect the\nreal-time behaviour of the data. The experiments demonstrate that our online PQ\nmodel is both time-efficient and effective for ANN search in dynamic large\nscale databases compared with baseline methods and the idea of partial PQ\ncodebook update further reduces the update cost.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 11:17:04 GMT"}, {"version": "v2", "created": "Sat, 24 Mar 2018 12:03:46 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Xu", "Donna", ""], ["Tsang", "Ivor W.", ""], ["Zhang", "Ying", ""]]}, {"id": "1711.10792", "submitter": "Mykola Ponomarenko", "authors": "Mykola Ponomarenko, Nikolay Gapon, Viacheslav Voronin, Karen\n  Egiazarian", "title": "Blind estimation of white Gaussian noise variance in highly textured\n  images", "comments": "IS&T International Symposium on Electronic Imaging (EI 2018), Image\n  Processing: Algorithms and Systems XVI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, a new method of blind estimation of noise variance in a single\nhighly textured image is proposed. An input image is divided into 8x8 blocks\nand discrete cosine transform (DCT) is performed for each block. A part of 64\nDCT coefficients with lowest energy calculated through all blocks is selected\nfor further analysis. For the DCT coefficients, a robust estimate of noise\nvariance is calculated. Corresponding to the obtained estimate, a part of\nblocks having very large values of local variance calculated only for the\nselected DCT coefficients are excluded from the further analysis. These two\nsteps (estimation of noise variance and exclusion of blocks) are iteratively\nrepeated three times. For the verification of the proposed method, a new\nnoise-free test image database TAMPERE17 consisting of many highly textured\nimages is designed. It is shown for this database and different values of noise\nvariance from the set {25, 49, 100, 225}, that the proposed method provides\napproximately two times lower estimation root mean square error than other\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 11:43:26 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Ponomarenko", "Mykola", ""], ["Gapon", "Nikolay", ""], ["Voronin", "Viacheslav", ""], ["Egiazarian", "Karen", ""]]}, {"id": "1711.10795", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Eva Mohedano, Kevin McGuinness, Xavier Giro-i-Nieto and Noel E.\n  O'Connor", "title": "Saliency Weighted Convolutional Features for Instance Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores attention models to weight the contribution of local\nconvolutional representations for the instance search task. We present a\nretrieval framework based on bags of local convolutional features (BLCF) that\nbenefits from saliency weighting to build an efficient image representation.\nThe use of human visual attention models (saliency) allows significant\nimprovements in retrieval performance without the need to conduct region\nanalysis or spatial verification, and without requiring any feature fine\ntuning. We investigate the impact of different saliency models, finding that\nhigher performance on saliency benchmarks does not necessarily equate to\nimproved performance when used in instance search tasks. The proposed approach\noutperforms the state-of-the-art on the challenging INSTRE benchmark by a large\nmargin, and provides similar performance on the Oxford and Paris benchmarks\ncompared to more complex methods that use off-the-shelf representations. The\nsource code used in this project is available at\nhttps://imatge-upc.github.io/salbow/\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 11:46:56 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Mohedano", "Eva", ""], ["McGuinness", "Kevin", ""], ["Giro-i-Nieto", "Xavier", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "1711.10796", "submitter": "Qingfu Wan", "authors": "Qingfu Wan, Wei Zhang, Xiangyang Xue", "title": "DeepSkeleton: Skeleton Map for 3D Human Pose Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent success on 2D human pose estimation, 3D human pose estimation\nstill remains an open problem. A key challenge is the ill-posed depth ambiguity\nnature. This paper presents a novel intermediate feature representation named\nskeleton map for regression. It distills structural context from irrelavant\nproperties of RGB image e.g. illumination and texture. It is simple, clean and\ncan be easily generated via deconvolution network. For the first time, we show\nthat training regression network from skeleton map alone is capable of meeting\nthe performance of state-of-theart 3D human pose estimation works. We further\nexploit the power of multiple 3D hypothesis generation to obtain reasonbale 3D\npose in consistent with 2D pose detection. The effectiveness of our approach is\nvalidated on challenging in-the-wild dataset MPII and indoor dataset Human3.6M.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 11:50:01 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Wan", "Qingfu", ""], ["Zhang", "Wei", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1711.10824", "submitter": "Virginia Estellers", "authors": "V. Estellers, F. R. Schmidt, D. Cremers", "title": "Compression for Smooth Shape Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most 3D shape analysis methods use triangular meshes to discretize both the\nshape and functions on it as piecewise linear functions. With this\nrepresentation, shape analysis requires fine meshes to represent smooth shapes\nand geometric operators like normals, curvatures, or Laplace-Beltrami\neigenfunctions at large computational and memory costs.\n  We avoid this bottleneck with a compression technique that represents a\nsmooth shape as subdivision surfaces and exploits the subdivision scheme to\nparametrize smooth functions on that shape with a few control parameters. This\ncompression does not affect the accuracy of the Laplace-Beltrami operator and\nits eigenfunctions and allow us to compute shape descriptors and shape\nmatchings at an accuracy comparable to triangular meshes but a fraction of the\ncomputational cost.\n  Our framework can also compress surfaces represented by point clouds to do\nshape analysis of 3D scanning data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 12:46:08 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Estellers", "V.", ""], ["Schmidt", "F. R.", ""], ["Cremers", "D.", ""]]}, {"id": "1711.10870", "submitter": "Zhang Chen", "authors": "Xuan Cao, Zhang Chen, Anpei Chen, Xin Chen, Cen Wang, Jingyi Yu", "title": "Sparse Photometric 3D Face Reconstruction Guided by Morphable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel 3D face reconstruction technique that leverages sparse\nphotometric stereo (PS) and latest advances on face registration/modeling from\na single image. We observe that 3D morphable faces approach provides a\nreasonable geometry proxy for light position calibration. Specifically, we\ndevelop a robust optimization technique that can calibrate per-pixel lighting\ndirection and illumination at a very high precision without assuming uniform\nsurface albedos. Next, we apply semantic segmentation on input images and the\ngeometry proxy to refine hairy vs. bare skin regions using tailored filters.\nExperiments on synthetic and real data show that by using a very small set of\nimages, our technique is able to reconstruct fine geometric details such as\nwrinkles, eyebrows, whelks, pores, etc, comparable to and sometimes surpassing\nmovie quality productions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 14:24:58 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Cao", "Xuan", ""], ["Chen", "Zhang", ""], ["Chen", "Anpei", ""], ["Chen", "Xin", ""], ["Wang", "Cen", ""], ["Yu", "Jingyi", ""]]}, {"id": "1711.10871", "submitter": "Danfei Xu", "authors": "Danfei Xu, Dragomir Anguelov, Ashesh Jain", "title": "PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PointFusion, a generic 3D object detection method that leverages\nboth image and 3D point cloud information. Unlike existing methods that either\nuse multi-stage pipelines or hold sensor and dataset-specific assumptions,\nPointFusion is conceptually simple and application-agnostic. The image data and\nthe raw point cloud data are independently processed by a CNN and a PointNet\narchitecture, respectively. The resulting outputs are then combined by a novel\nfusion network, which predicts multiple 3D box hypotheses and their\nconfidences, using the input 3D points as spatial anchors. We evaluate\nPointFusion on two distinctive datasets: the KITTI dataset that features\ndriving scenes captured with a lidar-camera setup, and the SUN-RGBD dataset\nthat captures indoor environments with RGB-D cameras. Our model is the first\none that is able to perform better or on-par with the state-of-the-art on these\ndiverse datasets without any dataset-specific model tuning.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 14:25:13 GMT"}, {"version": "v2", "created": "Sat, 25 Aug 2018 22:22:44 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Xu", "Danfei", ""], ["Anguelov", "Dragomir", ""], ["Jain", "Ashesh", ""]]}, {"id": "1711.10872", "submitter": "Qi Ye", "authors": "Qi Ye, Tae-Kyun Kim", "title": "Occlusion-aware Hand Pose Estimation Using Hierarchical Mixture Density\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning and predicting the pose parameters of a 3D hand model given an\nimage, such as locations of hand joints, is challenging due to large viewpoint\nchanges and articulations, and severe self-occlusions exhibited particularly in\negocentric views. Both feature learning and prediction modeling have been\ninvestigated to tackle the problem. Though effective, most existing\ndiscriminative methods yield a single deterministic estimation of target poses.\nDue to their single-value mapping intrinsic, they fail to adequately handle\nself-occlusion problems, where occluded joints present multiple modes. In this\npaper, we tackle the self-occlusion issue and provide a complete description of\nobserved poses given an input depth image by a novel method called hierarchical\nmixture density networks (HMDN). The proposed method leverages the\nstate-of-the-art hand pose estimators based on Convolutional Neural Networks to\nfacilitate feature learning, while it models the multiple modes in a two-level\nhierarchy to reconcile single-valued and multi-valued mapping in its output.\nThe whole framework with a mixture of two differentiable density functions is\nnaturally end-to-end trainable. In the experiments, HMDN produces interpretable\nand diverse candidate samples, and significantly outperforms the\nstate-of-the-art methods on two benchmarks with occlusions, and performs\ncomparably on another benchmark free of occlusions.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 14:25:18 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 18:01:43 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Ye", "Qi", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1711.10914", "submitter": "Yong Man Ro", "authors": "Wissam J. Baddar and Yong Man Ro", "title": "Learning Spatio-temporal Features with Partial Expression Sequences for\n  on-the-Fly Prediction", "comments": "Accepted at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal feature encoding is essential for encoding facial expression\ndynamics in video sequences. At test time, most spatio-temporal encoding\nmethods assume that a temporally segmented sequence is fed to a learned model,\nwhich could require the prediction to wait until the full sequence is available\nto an auxiliary task that performs the temporal segmentation. This causes a\ndelay in predicting the expression. In an interactive setting, such as\naffective interactive agents, such delay in the prediction could not be\ntolerated. Therefore, training a model that can accurately predict the facial\nexpression \"on-the-fly\" (as they are fed to the system) is essential. In this\npaper, we propose a new spatio-temporal feature learning method, which would\nallow prediction with partial sequences. As such, the prediction could be\nperformed on-the-fly. The proposed method utilizes an estimated expression\nintensity to generate dense labels, which are used to regulate the prediction\nmodel training with a novel objective function. As results, the learned\nspatio-temporal features can robustly predict the expression with partial\n(incomplete) expression sequences, on-the-fly. Experimental results showed that\nthe proposed method achieved higher recognition rates compared to the\nstate-of-the-art methods on both datasets. More importantly, the results\nverified that the proposed method improved the prediction frames with partial\nexpression sequence inputs.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 15:27:22 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Baddar", "Wissam J.", ""], ["Ro", "Yong Man", ""]]}, {"id": "1711.10918", "submitter": "Dongwoo Lee", "authors": "Dongwoo Lee, Haesol Park, In Kyu Park and Kyoung Mu Lee", "title": "Joint Blind Motion Deblurring and Depth Estimation of Light Field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing camera motion blur from a single light field is a challenging task\nsince it is highly ill-posed inverse problem. The problem becomes even worse\nwhen blur kernel varies spatially due to scene depth variation and high-order\ncamera motion. In this paper, we propose a novel algorithm to estimate all blur\nmodel variables jointly, including latent sub-aperture image, camera motion,\nand scene depth from the blurred 4D light field. Exploiting multi-view nature\nof a light field relieves the inverse property of the optimization by utilizing\nstrong depth cues and multi-view blur observation. The proposed joint\nestimation achieves high quality light field deblurring and depth estimation\nsimultaneously under arbitrary 6-DOF camera motion and unconstrained scene\ndepth. Intensive experiment on real and synthetic blurred light field confirms\nthat the proposed algorithm outperforms the state-of-the-art light field\ndeblurring and depth estimation methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 15:31:55 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 06:40:34 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Lee", "Dongwoo", ""], ["Park", "Haesol", ""], ["Park", "In Kyu", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1711.10921", "submitter": "Shiv Ram Dubey", "authors": "Swalpa Kumar Roy, Bhabatosh Chanda, Bidyut B. Chaudhuri, Dipak Kumar\n  Ghosh, Shiv Ram Dubey", "title": "Local Jet Pattern: A Robust Descriptor for Texture Classification", "comments": "Accepted in Multimedia Tools and Applications, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods based on local image features have recently shown promise for texture\nclassification tasks, especially in the presence of large intra-class variation\ndue to illumination, scale, and viewpoint changes. Inspired by the theories of\nimage structure analysis, this paper presents a simple, efficient, yet robust\ndescriptor namely local jet pattern (LJP) for texture classification. In this\napproach, a jet space representation of a texture image is derived from a set\nof derivatives of Gaussian (DtGs) filter responses up to second order, so\ncalled local jet vectors (LJV), which also satisfy the Scale Space properties.\nThe LJP is obtained by utilizing the relationship of center pixel with the\nlocal neighborhood information in jet space. Finally, the feature vector of a\ntexture region is formed by concatenating the histogram of LJP for all elements\nof LJV. All DtGs responses up to second order together preserves the intrinsic\nlocal image structure, and achieves invariance to scale, rotation, and\nreflection. This allows us to develop a texture classification framework which\nis discriminative and robust. Extensive experiments on five standard texture\nimage databases, employing nearest subspace classifier (NSC), the proposed\ndescriptor achieves 100%, 99.92%, 99.75%, 99.16%, and 99.65% accuracy for\nOutex_TC-00010 (Outex_TC10), and Outex_TC-00012 (Outex_TC12), KTH-TIPS,\nBrodatz, CUReT, respectively, which are outperforms the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 17:18:46 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 12:18:38 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 02:01:15 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Roy", "Swalpa Kumar", ""], ["Chanda", "Bhabatosh", ""], ["Chaudhuri", "Bidyut B.", ""], ["Ghosh", "Dipak Kumar", ""], ["Dubey", "Shiv Ram", ""]]}, {"id": "1711.10925", "submitter": "Dmitry Ulyanov", "authors": "Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky", "title": "Deep Image Prior", "comments": null, "journal-ref": null, "doi": "10.1007/s11263-020-01303-4", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks have become a popular tool for image generation\nand restoration. Generally, their excellent performance is imputed to their\nability to learn realistic image priors from a large number of example images.\nIn this paper, we show that, on the contrary, the structure of a generator\nnetwork is sufficient to capture a great deal of low-level image statistics\nprior to any learning. In order to do so, we show that a randomly-initialized\nneural network can be used as a handcrafted prior with excellent results in\nstandard inverse problems such as denoising, super-resolution, and inpainting.\nFurthermore, the same prior can be used to invert deep neural representations\nto diagnose them, and to restore images based on flash-no flash input pairs.\n  Apart from its diverse applications, our approach highlights the inductive\nbias captured by standard generator network architectures. It also bridges the\ngap between two very popular families of image restoration methods:\nlearning-based methods using deep convolutional networks and learning-free\nmethods based on handcrafted image priors such as self-similarity. Code and\nsupplementary material are available at\nhttps://dmitryulyanov.github.io/deep_image_prior .\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 15:50:05 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 10:14:39 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2018 14:37:57 GMT"}, {"version": "v4", "created": "Sun, 17 May 2020 10:57:04 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Ulyanov", "Dmitry", ""], ["Vedaldi", "Andrea", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1711.10939", "submitter": "Paul Henderson", "authors": "Paul Henderson, Kartic Subr, Vittorio Ferrari", "title": "Automatic Generation of Constrained Furniture Layouts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient authoring of vast virtual environments hinges on algorithms that\nare able to automatically generate content while also being controllable. We\npropose a method to automatically generate furniture layouts for indoor\nenvironments. Our method is simple, efficient, human-interpretable and amenable\nto a wide variety of constraints. We model the composition of rooms into\nclasses of objects and learn joint (co-occurrence) statistics from a database\nof training layouts. We generate new layouts by performing a sequence of\nconditional sampling steps, exploiting the statistics learned from the\ndatabase. The generated layouts are specified as 3D object models, along with\ntheir positions and orientations. We show they are of equivalent perceived\nquality to the training layouts, and compare favorably to a state-of-the-art\nmethod. We incorporate constraints using a general mechanism -- rejection\nsampling -- which provides great flexibility at the cost of extra computation.\nWe demonstrate the versatility of our method by applying a wide variety of\nconstraints relevant to real-world applications.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 16:21:32 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 17:04:42 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 20:58:23 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Henderson", "Paul", ""], ["Subr", "Kartic", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1711.10959", "submitter": "Calden Wloka", "authors": "Calden Wloka, Iuliia Kotseruba, John K. Tsotsos", "title": "Saccade Sequence Prediction: Beyond Static Saliency Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention is a field with a considerable history, with eye movement\ncontrol and prediction forming an important subfield. Fixation modeling in the\npast decades has been largely dominated computationally by a number of highly\ninfluential bottom-up saliency models, such as the Itti-Koch-Niebur model. The\naccuracy of such models has dramatically increased recently due to deep\nlearning. However, on static images the emphasis of these models has largely\nbeen based on non-ordered prediction of fixations through a saliency map. Very\nfew implemented models can generate temporally ordered human-like sequences of\nsaccades beyond an initial fixation point. Towards addressing these\nshortcomings we present STAR-FC, a novel multi-saccade generator based on a\ncentral/peripheral integration of deep learning-based saliency and lower-level\nfeature-based saliency. We have evaluated our model using the CAT2000 database,\nsuccessfully predicting human patterns of fixation with equivalent accuracy and\nquality compared to what can be achieved by using one human sequence to predict\nanother. This is a significant improvement over fixation sequences predicted by\nstate-of-the-art saliency algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 16:48:28 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Wloka", "Calden", ""], ["Kotseruba", "Iuliia", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1711.10968", "submitter": "Arash Akbarinia", "authors": "Arash Akbarinia, Raquel Gil Rodr\\'iguez, C. Alejandro Parraga", "title": "Colour Constancy: Biologically-inspired Contrast Variant Pooling\n  Mechanism", "comments": null, "journal-ref": "Proceedings of the British machine Vision Conference (BMVC) 2017", "doi": "10.5244/C.31.77", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooling is a ubiquitous operation in image processing algorithms that allows\nfor higher-level processes to collect relevant low-level features from a region\nof interest. Currently, max-pooling is one of the most commonly used operators\nin the computational literature. However, it can lack robustness to outliers\ndue to the fact that it relies merely on the peak of a function. Pooling\nmechanisms are also present in the primate visual cortex where neurons of\nhigher cortical areas pool signals from lower ones. The receptive fields of\nthese neurons have been shown to vary according to the contrast by aggregating\nsignals over a larger region in the presence of low contrast stimuli. We\nhypothesise that this contrast-variant-pooling mechanism can address some of\nthe shortcomings of max-pooling. We modelled this contrast variation through a\nhistogram clipping in which the percentage of pooled signal is inversely\nproportional to the local contrast of an image. We tested our hypothesis by\napplying it to the phenomenon of colour constancy where a number of popular\nalgorithms utilise a max-pooling step (e.g. White-Patch, Grey-Edge and\nDouble-Opponency). For each of these methods, we investigated the consequences\nof replacing their original max-pooling by the proposed\ncontrast-variant-pooling. Our experiments on three colour constancy benchmark\ndatasets suggest that previous results can significantly improve by adopting a\ncontrast-variant-pooling mechanism.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 17:14:50 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Akbarinia", "Arash", ""], ["Rodr\u00edguez", "Raquel Gil", ""], ["Parraga", "C. Alejandro", ""]]}, {"id": "1711.11017", "submitter": "Ethan Perez", "authors": "Simon Brodeur, Ethan Perez, Ankesh Anand, Florian Golemo, Luca\n  Celotti, Florian Strub, Jean Rouat, Hugo Larochelle, Aaron Courville", "title": "HoME: a Household Multimodal Environment", "comments": "Presented at NIPS 2017's Visually-Grounded Interaction and Language\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.RO cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce HoME: a Household Multimodal Environment for artificial agents\nto learn from vision, audio, semantics, physics, and interaction with objects\nand other agents, all within a realistic context. HoME integrates over 45,000\ndiverse 3D house layouts based on the SUNCG dataset, a scale which may\nfacilitate learning, generalization, and transfer. HoME is an open-source,\nOpenAI Gym-compatible platform extensible to tasks in reinforcement learning,\nlanguage grounding, sound-based navigation, robotics, multi-agent learning, and\nmore. We hope HoME better enables artificial agents to learn as humans do: in\nan interactive, multimodal, and richly contextualized setting.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 18:45:59 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Brodeur", "Simon", ""], ["Perez", "Ethan", ""], ["Anand", "Ankesh", ""], ["Golemo", "Florian", ""], ["Celotti", "Luca", ""], ["Strub", "Florian", ""], ["Rouat", "Jean", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""]]}, {"id": "1711.11069", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Miriam Bellver, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Xavier\n  Giro-i-Nieto, Jordi Torres and Luc Van Gool", "title": "Detection-aided liver lesion segmentation using deep learning", "comments": "NIPS 2017 Workshop on Machine Learning for Health (ML4H)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fully automatic technique for segmenting the liver and localizing its\nunhealthy tissues is a convenient tool in order to diagnose hepatic diseases\nand assess the response to the according treatments. In this work we propose a\nmethod to segment the liver and its lesions from Computed Tomography (CT) scans\nusing Convolutional Neural Networks (CNNs), that have proven good results in a\nvariety of computer vision tasks, including medical imaging. The network that\nsegments the lesions consists of a cascaded architecture, which first focuses\non the region of the liver in order to segment the lesions on it. Moreover, we\ntrain a detector to localize the lesions, and mask the results of the\nsegmentation network with the positive detections. The segmentation\narchitecture is based on DRIU, a Fully Convolutional Network (FCN) with side\noutputs that work on feature maps of different resolutions, to finally benefit\nfrom the multi-scale information learned by different stages of the network.\nThe main contribution of this work is the use of a detector to localize the\nlesions, which we show to be beneficial to remove false positives triggered by\nthe segmentation network. Source code and models are available at\nhttps://imatge-upc.github.io/liverseg-2017-nipsws/ .\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 19:27:40 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Bellver", "Miriam", ""], ["Maninis", "Kevis-Kokitsi", ""], ["Pont-Tuset", "Jordi", ""], ["Giro-i-Nieto", "Xavier", ""], ["Torres", "Jordi", ""], ["Van Gool", "Luc", ""]]}, {"id": "1711.11075", "submitter": "Fabiana Zama", "authors": "Damiana Lazzaro and Elena Loli Piccolomini and Fabiana Zama", "title": "A fast nonconvex Compressed Sensing algorithm for highly low-sampled MR\n  images reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a fast and efficient method for the reconstruction\nof Magnetic Resonance Images (MRI) from severely under-sampled data. From the\nCompressed Sensing theory we have mathematically modeled the problem as a\nconstrained minimization problem with a family of non-convex regularizing\nobjective functions depending on a parameter and a least squares data fit\nconstraint. We propose a fast and efficient algorithm, named Fast NonConvex\nReweighting (FNCR) algorithm, based on an iterative scheme where the non-convex\nproblem is approximated by its convex linearization and the penalization\nparameter is automatically updated. The convex problem is solved by a\nForward-Backward procedure, where the Backward step is performed by a Split\nBregman strategy. Moreover, we propose a new efficient iterative solver for the\narising linear systems. We prove the convergence of the proposed FNCR method.\nThe results on synthetic phantoms and real images show that the algorithm is\nvery well performing and computationally efficient, even when compared to the\nbest performing methods proposed in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 19:39:08 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Lazzaro", "Damiana", ""], ["Piccolomini", "Elena Loli", ""], ["Zama", "Fabiana", ""]]}, {"id": "1711.11097", "submitter": "Zhe Zhu", "authors": "Zhe Zhu, Ehab Albadawy, Ashirbani Saha, Jun Zhang, Michael R.\n  Harowicz, Maciej A. Mazurowski", "title": "Deep Learning for identifying radiogenomic associations in breast cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To determine whether deep learning models can distinguish between\nbreast cancer molecular subtypes based on dynamic contrast-enhanced magnetic\nresonance imaging (DCE-MRI). Materials and methods: In this institutional\nreview board-approved single-center study, we analyzed DCE-MR images of 270\npatients at our institution. Lesions of interest were identified by\nradiologists. The task was to automatically determine whether the tumor is of\nthe Luminal A subtype or of another subtype based on the MR image patches\nrepresenting the tumor. Three different deep learning approaches were used to\nclassify the tumor according to their molecular subtypes: learning from scratch\nwhere only tumor patches were used for training, transfer learning where\nnetworks pre-trained on natural images were fine-tuned using tumor patches, and\noff-the-shelf deep features where the features extracted by neural networks\ntrained on natural images were used for classification with a support vector\nmachine. Network architectures utilized in our experiments were GoogleNet, VGG,\nand CIFAR. We used 10-fold crossvalidation method for validation and area under\nthe receiver operating characteristic (AUC) as the measure of performance.\nResults: The best AUC performance for distinguishing molecular subtypes was\n0.65 (95% CI:[0.57,0.71]) and was achieved by the off-the-shelf deep features\napproach. The highest AUC performance for training from scratch was 0.58 (95%\nCI:[0.51,0.64]) and the best AUC performance for transfer learning was 0.60\n(95% CI:[0.52,0.65]) respectively. For the off-the-shelf approach, the features\nextracted from the fully connected layer performed the best. Conclusion: Deep\nlearning may play a role in discovering radiogenomic associations in breast\ncancer.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 20:41:05 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Zhu", "Zhe", ""], ["Albadawy", "Ehab", ""], ["Saha", "Ashirbani", ""], ["Zhang", "Jun", ""], ["Harowicz", "Michael R.", ""], ["Mazurowski", "Maciej A.", ""]]}, {"id": "1711.11117", "submitter": "Naimul Khan", "authors": "Marcia Hon, Naimul Khan", "title": "Towards Alzheimer's Disease Classification through Transfer Learning", "comments": "Presented at IEEE BIBM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of Alzheimer's Disease (AD) from neuroimaging data such as MRI\nthrough machine learning have been a subject of intense research in recent\nyears. Recent success of deep learning in computer vision have progressed such\nresearch further. However, common limitations with such algorithms are reliance\non a large number of training images, and requirement of careful optimization\nof the architecture of deep networks. In this paper, we attempt solving these\nissues with transfer learning, where state-of-the-art architectures such as VGG\nand Inception are initialized with pre-trained weights from large benchmark\ndatasets consisting of natural images, and the fully-connected layer is\nre-trained with only a small number of MRI images. We employ image entropy to\nselect the most informative slices for training. Through experimentation on the\nOASIS MRI dataset, we show that with training size almost 10 times smaller than\nthe state-of-the-art, we reach comparable or even better performance than\ncurrent deep-learning based methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 21:40:36 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Hon", "Marcia", ""], ["Khan", "Naimul", ""]]}, {"id": "1711.11135", "submitter": "Xin Wang", "authors": "Xin Wang, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang, William Yang Wang", "title": "Video Captioning via Hierarchical Reinforcement Learning", "comments": "CVPR 2018, with supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video captioning is the task of automatically generating a textual\ndescription of the actions in a video. Although previous work (e.g.\nsequence-to-sequence model) has shown promising results in abstracting a coarse\ndescription of a short video, it is still very challenging to caption a video\ncontaining multiple fine-grained actions with a detailed description. This\npaper aims to address the challenge by proposing a novel hierarchical\nreinforcement learning framework for video captioning, where a high-level\nManager module learns to design sub-goals and a low-level Worker module\nrecognizes the primitive actions to fulfill the sub-goal. With this\ncompositional framework to reinforce video captioning at different levels, our\napproach significantly outperforms all the baseline methods on a newly\nintroduced large-scale dataset for fine-grained video captioning. Furthermore,\nour non-ensemble model has already achieved the state-of-the-art results on the\nwidely-used MSR-VTT dataset.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 22:23:59 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 08:38:19 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 07:06:47 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Wang", "Xin", ""], ["Chen", "Wenhu", ""], ["Wu", "Jiawei", ""], ["Wang", "Yuan-Fang", ""], ["Wang", "William Yang", ""]]}, {"id": "1711.11151", "submitter": "Guy Ben-Yosef", "authors": "Guy Ben-Yosef, Liav Assif and Shimon Ullman", "title": "Structured learning and detailed interpretation of minimal object images", "comments": "Accepted to Workshop on Mutual Benefits of Cognitive and Computer\n  Vision, at the International Conference on Computer Vision. Venice, Italy,\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model the process of human full interpretation of object images, namely\nthe ability to identify and localize all semantic features and parts that are\nrecognized by human observers. The task is approached by dividing the\ninterpretation of the complete object to the interpretation of multiple reduced\nbut interpretable local regions. We model interpretation by a structured\nlearning framework, in which there are primitive components and relations that\nplay a useful role in local interpretation by humans. To identify useful\ncomponents and relations used in the interpretation process, we consider the\ninterpretation of minimal configurations, namely reduced local regions that are\nminimal in the sense that further reduction will turn them unrecognizable and\nuninterpretable. We show experimental results of our model, and results of\npredicting and testing relations that were useful to the model via transformed\nminimal images.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 23:26:52 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Ben-Yosef", "Guy", ""], ["Assif", "Liav", ""], ["Ullman", "Shimon", ""]]}, {"id": "1711.11152", "submitter": "Shuyang Sun", "authors": "Shuyang Sun, Zhanghui Kuang, Wanli Ouyang, Lu Sheng, Wei Zhang", "title": "Optical Flow Guided Feature: A Fast and Robust Motion Representation for\n  Video Action Recognition", "comments": "CVPR 2018. code available at\n  https://github.com/kevin-ssy/Optical-Flow-Guided-Feature", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion representation plays a vital role in human action recognition in\nvideos. In this study, we introduce a novel compact motion representation for\nvideo action recognition, named Optical Flow guided Feature (OFF), which\nenables the network to distill temporal information through a fast and robust\napproach. The OFF is derived from the definition of optical flow and is\northogonal to the optical flow. The derivation also provides theoretical\nsupport for using the difference between two frames. By directly calculating\npixel-wise spatiotemporal gradients of the deep feature maps, the OFF could be\nembedded in any existing CNN based video action recognition framework with only\na slight additional cost. It enables the CNN to extract spatiotemporal\ninformation, especially the temporal information between frames simultaneously.\nThis simple but powerful idea is validated by experimental results. The network\nwith OFF fed only by RGB inputs achieves a competitive accuracy of 93.3% on\nUCF-101, which is comparable with the result obtained by two streams (RGB and\noptical flow), but is 15 times faster in speed. Experimental results also show\nthat OFF is complementary to other motion modalities such as optical flow. When\nthe proposed method is plugged into the state-of-the-art video action\nrecognition framework, it has 96:0% and 74:2% accuracy on UCF-101 and HMDB-51\nrespectively. The code for this project is available at\nhttps://github.com/kevin-ssy/Optical-Flow-Guided-Feature.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 23:29:02 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 11:18:56 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Sun", "Shuyang", ""], ["Kuang", "Zhanghui", ""], ["Ouyang", "Wanli", ""], ["Sheng", "Lu", ""], ["Zhang", "Wei", ""]]}, {"id": "1711.11155", "submitter": "Aven Samareh", "authors": "Aven Samareh, Yan Jin, Zhangyang Wang, Xiangyu Chang, Shuai Huang", "title": "Predicting Depression Severity by Multi-Modal Feature Engineering and\n  Fusion", "comments": "Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our preliminary work to determine if patient's vocal acoustic,\nlinguistic, and facial patterns could predict clinical ratings of depression\nseverity, namely Patient Health Questionnaire depression scale (PHQ-8). We\nproposed a multi modal fusion model that combines three different modalities:\naudio, video , and text features. By training over AVEC 2017 data set, our\nproposed model outperforms each single modality prediction model, and surpasses\nthe data set baseline with ice margin.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 23:39:43 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Samareh", "Aven", ""], ["Jin", "Yan", ""], ["Wang", "Zhangyang", ""], ["Chang", "Xiangyu", ""], ["Huang", "Shuai", ""]]}, {"id": "1711.11200", "submitter": "Hyunwoo Lee", "authors": "Hyunwoo Lee, Jooyoung Kim, Dojun Yang and Joon-Ho Kim", "title": "Embedded Real-Time Fall Detection Using Deep Learning For Elderly Care", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a real-time embedded fall detection system using a\nDVS(Dynamic Vision Sensor) that has never been used for traditional fall\ndetection, a dataset for fall detection using that, and a DVS-TN(DVS-Temporal\nNetwork). The first contribution is building a DVS Falls Dataset, which made\nour network to recognize a much greater variety of falls than the existing\ndatasets that existed before and solved privacy issues using the DVS. Secondly,\nwe introduce the DVS-TN : optimized deep learning network to detect falls using\nDVS. Finally, we implemented a fall detection system which can run on\nlow-computing H/W with real-time, and tested on DVS Falls Dataset that takes\ninto account various falls situations. Our approach achieved 95.5% on the\nF1-score and operates at 31.25 FPS on NVIDIA Jetson TX1 board.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 03:07:14 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Lee", "Hyunwoo", ""], ["Kim", "Jooyoung", ""], ["Yang", "Dojun", ""], ["Kim", "Joon-Ho", ""]]}, {"id": "1711.11217", "submitter": "Takuma Yagi", "authors": "Takuma Yagi, Karttikeya Mangalam, Ryo Yonetani, Yoichi Sato", "title": "Future Person Localization in First-Person Videos", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new task that predicts future locations of people observed in\nfirst-person videos. Consider a first-person video stream continuously recorded\nby a wearable camera. Given a short clip of a person that is extracted from the\ncomplete stream, we aim to predict that person's location in future frames. To\nfacilitate this future person localization ability, we make the following three\nkey observations: a) First-person videos typically involve significant\nego-motion which greatly affects the location of the target person in future\nframes; b) Scales of the target person act as a salient cue to estimate a\nperspective effect in first-person videos; c) First-person videos often capture\npeople up-close, making it easier to leverage target poses (e.g., where they\nlook) for predicting their future locations. We incorporate these three\nobservations into a prediction framework with a multi-stream\nconvolution-deconvolution architecture. Experimental results reveal our method\nto be effective on our new dataset as well as on a public social interaction\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 04:16:03 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 01:29:15 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Yagi", "Takuma", ""], ["Mangalam", "Karttikeya", ""], ["Yonetani", "Ryo", ""], ["Sato", "Yoichi", ""]]}, {"id": "1711.11224", "submitter": "Tingwei Quan", "authors": "Song Yizhi, Xu Cheng, Ding Daoxin, Zhou Hang, Quan Tingwei, Li Shiwei", "title": "Properties on n-dimensional convolution for image deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution system is linear and time invariant, and can describe the optical\nimaging process. Based on convolution system, many deconvolution techniques\nhave been developed for optical image analysis, such as boosting the space\nresolution of optical images, image denoising, image enhancement and so on.\nHere, we gave properties on N-dimensional convolution. By using these\nproperties, we proposed image deconvolution method. This method uses a series\nof convolution operations to deconvolute image. We demonstrated that the method\nhas the similar deconvolution results to the state-of-art method. The core\ncalculation of the proposed method is image convolution, and thus our method\ncan easily be integrated into GPU mode for large-scale image deconvolution.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 04:45:41 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Yizhi", "Song", ""], ["Cheng", "Xu", ""], ["Daoxin", "Ding", ""], ["Hang", "Zhou", ""], ["Tingwei", "Quan", ""], ["Shiwei", "Li", ""]]}, {"id": "1711.11248", "submitter": "Du Tran", "authors": "Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, Manohar\n  Paluri", "title": "A Closer Look at Spatiotemporal Convolutions for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss several forms of spatiotemporal convolutions for\nvideo analysis and study their effects on action recognition. Our motivation\nstems from the observation that 2D CNNs applied to individual frames of the\nvideo have remained solid performers in action recognition. In this work we\nempirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within\nthe framework of residual learning. Furthermore, we show that factorizing the\n3D convolutional filters into separate spatial and temporal components yields\nsignificantly advantages in accuracy. Our empirical study leads to the design\nof a new spatiotemporal convolutional block \"R(2+1)D\" which gives rise to CNNs\nthat achieve results comparable or superior to the state-of-the-art on\nSports-1M, Kinetics, UCF101 and HMDB51.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 06:28:20 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 23:50:54 GMT"}, {"version": "v3", "created": "Thu, 12 Apr 2018 01:07:30 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Tran", "Du", ""], ["Wang", "Heng", ""], ["Torresani", "Lorenzo", ""], ["Ray", "Jamie", ""], ["LeCun", "Yann", ""], ["Paluri", "Manohar", ""]]}, {"id": "1711.11249", "submitter": "Yi Fang", "authors": "Daitao Xing, Zichen Li, Xin Chen, Yi Fang", "title": "ArbiText: Arbitrary-Oriented Text Detection in Unconstrained Scene", "comments": "10pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary-oriented text detection in the wild is a very challenging task, due\nto the aspect ratio, scale, orientation, and illumination variations. In this\npaper, we propose a novel method, namely Arbitrary-oriented Text (or ArbText\nfor short) detector, for efficient text detection in unconstrained natural\nscene images. Specifically, we first adopt the circle anchors rather than the\nrectangular ones to represent bounding boxes, which is more robust to\norientation variations. Subsequently, we incorporate a pyramid pooling module\ninto the Single Shot MultiBox Detector framework, in order to simultaneously\nexplore the local and global visual information, which can, therefore, generate\nmore confidential detection results. Experiments on established scene-text\ndatasets, such as the ICDAR 2015 and MSRA-TD500 datasets, have demonstrated the\nsupe rior performance of the proposed method, compared to the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 06:30:11 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Xing", "Daitao", ""], ["Li", "Zichen", ""], ["Chen", "Xin", ""], ["Fang", "Yi", ""]]}, {"id": "1711.11266", "submitter": "Chenxing Xia", "authors": "Chenxing Xia and Hanling Zhang and Keqin Li", "title": "A novel graph structure for salient object detection based on divergence\n  background and compact foreground", "comments": "22 pages,16 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient and discriminative model for salient\nobject detection. Our method is carried out in a stepwise mechanism based on\nboth divergence background and compact foreground cues. In order to effectively\nenhance the distinction between nodes along object boundaries and the\nsimilarity among object regions, a graph is constructed by introducing the\nconcept of virtual node. To remove incorrect outputs, a scheme for selecting\nbackground seeds and a method for generating compactness foreground regions are\nintroduced, respectively. Different from prior methods, we calculate the\nsaliency value of each node based on the relationship between the corresponding\nnode and the virtual node. In order to achieve significant performance\nimprovement consistently, we propose an Extended Manifold Ranking (EMR)\nalgorithm, which subtly combines suppressed / active nodes and mid-level\ninformation. Extensive experimental results demonstrate that the proposed\nalgorithm performs favorably against the state-of-art saliency detection\nmethods in terms of different evaluation metrics on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 08:41:26 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Xia", "Chenxing", ""], ["Zhang", "Hanling", ""], ["Li", "Keqin", ""]]}, {"id": "1711.11317", "submitter": "Bo Hu", "authors": "Bo Hu, Ye Tang, Eric I-Chao Chang, Yubo Fan, Maode Lai and Yan Xu", "title": "Unsupervised Learning for Cell-level Visual Representation in\n  Histopathology Images with Generative Adversarial Networks", "comments": "Accepted for publication in IEEE Journal of Biomedical and Health\n  Informatics", "journal-ref": null, "doi": "10.1109/JBHI.2018.2852639", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual attributes of cells, such as the nuclear morphology and chromatin\nopenness, are critical for histopathology image analysis. By learning\ncell-level visual representation, we can obtain a rich mix of features that are\nhighly reusable for various tasks, such as cell-level classification, nuclei\nsegmentation, and cell counting. In this paper, we propose a unified generative\nadversarial networks architecture with a new formulation of loss to perform\nrobust cell-level visual representation learning in an unsupervised setting.\nOur model is not only label-free and easily trained but also capable of\ncell-level unsupervised classification with interpretable visualization, which\nachieves promising results in the unsupervised classification of bone marrow\ncellular components. Based on the proposed cell-level visual representation\nlearning, we further develop a pipeline that exploits the varieties of cellular\nelements to perform histopathology image classification, the advantages of\nwhich are demonstrated on bone marrow datasets.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 10:53:56 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 15:04:55 GMT"}, {"version": "v3", "created": "Sun, 1 Apr 2018 09:07:35 GMT"}, {"version": "v4", "created": "Sat, 7 Jul 2018 07:31:18 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Hu", "Bo", ""], ["Tang", "Ye", ""], ["Chang", "Eric I-Chao", ""], ["Fan", "Yubo", ""], ["Lai", "Maode", ""], ["Xu", "Yan", ""]]}, {"id": "1711.11326", "submitter": "Alessandro Artusi PhD", "authors": "Alessandro Artusi, Thomas Richter, Touradj Ebrahimi, Rafal K. Mantiuk", "title": "High Dynamic Range Imaging Technology", "comments": "Lecture Notes", "journal-ref": "IEEE Signal Processing Magazine ( Volume: 34, Issue: 5, Sept. 2017\n  )", "doi": "10.1109/MSP.2017.2716957", "report-no": null, "categories": "cs.GR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this lecture note, we describe high dynamic range (HDR) imaging systems;\nsuch systems are able to represent luminances of much larger brightness and,\ntypically, also a larger range of colors than conventional standard dynamic\nrange (SDR) imaging systems. The larger luminance range greatly improve the\noverall quality of visual content, making it appears much more realistic and\nappealing to observers. HDR is one of the key technologies of the future\nimaging pipeline, which will change the way the digital visual content is\nrepresented and manipulated today.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 11:38:11 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Artusi", "Alessandro", ""], ["Richter", "Thomas", ""], ["Ebrahimi", "Touradj", ""], ["Mantiuk", "Rafal K.", ""]]}, {"id": "1711.11339", "submitter": "James Pritts", "authors": "James Pritts, Zuzana Kukelova, Viktor Larsson, Ondrej Chum", "title": "Radially-Distorted Conjugate Translations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the first minimal solvers that jointly solve for\naffine-rectification and radial lens distortion from coplanar repeated\npatterns. Even with imagery from moderately distorted lenses, plane\nrectification using the pinhole camera model is inaccurate or invalid. The\nproposed solvers incorporate lens distortion into the camera model and extend\naccurate rectification to wide-angle imagery, which is now common from consumer\ncameras. The solvers are derived from constraints induced by the conjugate\ntranslations of an imaged scene plane, which are integrated with the division\nmodel for radial lens distortion. The hidden-variable trick with ideal\nsaturation is used to reformulate the constraints so that the solvers generated\nby the Grobner-basis method are stable, small and fast.\n  Rectification and lens distortion are recovered from either one conjugately\ntranslated affine-covariant feature or two independently translated\nsimilarity-covariant features. The proposed solvers are used in a \\RANSAC-based\nestimator, which gives accurate rectifications after few iterations. The\nproposed solvers are evaluated against the state-of-the-art and demonstrate\nsignificantly better rectifications on noisy measurements. Qualitative results\non diverse imagery demonstrate high-accuracy undistortions and rectifications.\nThe source code is publicly available at https://github.com/prittjam/repeats.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 12:24:31 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 14:05:59 GMT"}, {"version": "v3", "created": "Thu, 21 Jun 2018 12:47:57 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Pritts", "James", ""], ["Kukelova", "Zuzana", ""], ["Larsson", "Viktor", ""], ["Chum", "Ondrej", ""]]}, {"id": "1711.11379", "submitter": "Wei Zeng", "authors": "Wei Zeng, Theo Gevers", "title": "3DContextNet: K-d Tree Guided Hierarchical Learning of Point Clouds\n  Using Local and Global Contextual Cues", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification and segmentation of 3D point clouds are important tasks in\ncomputer vision. Because of the irregular nature of point clouds, most of the\nexisting methods convert point clouds into regular 3D voxel grids before they\nare used as input for ConvNets. Unfortunately, voxel representations are highly\ninsensitive to the geometrical nature of 3D data. More recent methods encode\npoint clouds to higher dimensional features to cover the global 3D space.\nHowever, these models are not able to sufficiently capture the local structures\nof point clouds.\n  Therefore, in this paper, we propose a method that exploits both local and\nglobal contextual cues imposed by the k-d tree. The method is designed to learn\nrepresentation vectors progressively along the tree structure. Experiments on\nchallenging benchmarks show that the proposed model provides discriminative\npoint set features. For the task of 3D scene semantic segmentation, our method\nsignificantly outperforms the state-of-the-art on the Stanford Large-Scale 3D\nIndoor Spaces Dataset (S3DIS).\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 13:26:55 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 10:16:02 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 13:45:45 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Zeng", "Wei", ""], ["Gevers", "Theo", ""]]}, {"id": "1711.11386", "submitter": "Kerem Can Tezcan", "authors": "Kerem C. Tezcan, Christian F. Baumgartner, Roger Luechinger, Klaas P.\n  Pruessmann, Ender Konukoglu", "title": "MR image reconstruction using deep density priors", "comments": "Published in IEEE TMI. Main text and supplementary material, 19 pages\n  total", "journal-ref": "IEEE Transactions on Medical Imaging, December 2018", "doi": "10.1109/TMI.2018.2887072", "report-no": null, "categories": "cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for Magnetic Resonance (MR) image reconstruction from undersampled\nmeasurements exploit prior information to compensate for missing k-space data.\nDeep learning (DL) provides a powerful framework for extracting such\ninformation from existing image datasets, through learning, and then using it\nfor reconstruction. Leveraging this, recent methods employed DL to learn\nmappings from undersampled to fully sampled images using paired datasets,\nincluding undersampled and corresponding fully sampled images, integrating\nprior knowledge implicitly. In this article, we propose an alternative approach\nthat learns the probability distribution of fully sampled MR images using\nunsupervised DL, specifically Variational Autoencoders (VAE), and use this as\nan explicit prior term in reconstruction, completely decoupling the encoding\noperation from the prior. The resulting reconstruction algorithm enjoys a\npowerful image prior to compensate for missing k-space data without requiring\npaired datasets for training nor being prone to associated sensitivities, such\nas deviations in undersampling patterns used in training and test time or coil\nsettings. We evaluated the proposed method with T1 weighted images from a\npublicly available dataset, multi-coil complex images acquired from healthy\nvolunteers (N=8) and images with white matter lesions. The proposed algorithm,\nusing the VAE prior, produced visually high quality reconstructions and\nachieved low RMSE values, outperforming most of the alternative methods on the\nsame dataset. On multi-coil complex data, the algorithm yielded accurate\nmagnitude and phase reconstruction results. In the experiments on images with\nwhite matter lesions, the method faithfully reconstructed the lesions.\n  Keywords: Reconstruction, MRI, prior probability, machine learning, deep\nlearning, unsupervised learning, density estimation\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 13:36:58 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 10:34:44 GMT"}, {"version": "v3", "created": "Wed, 17 Jan 2018 13:42:41 GMT"}, {"version": "v4", "created": "Wed, 19 Dec 2018 18:00:04 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Tezcan", "Kerem C.", ""], ["Baumgartner", "Christian F.", ""], ["Luechinger", "Roger", ""], ["Pruessmann", "Klaas P.", ""], ["Konukoglu", "Ender", ""]]}, {"id": "1711.11443", "submitter": "Pierre Stock", "authors": "Pierre Stock and Moustapha Cisse", "title": "ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and\n  Uncovering Biases", "comments": "ECCV 2018 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ConvNets and Imagenet have driven the recent success of deep learning for\nimage classification. However, the marked slowdown in performance improvement\ncombined with the lack of robustness of neural networks to adversarial examples\nand their tendency to exhibit undesirable biases question the reliability of\nthese methods. This work investigates these questions from the perspective of\nthe end-user by using human subject studies and explanations. The contribution\nof this study is threefold. We first experimentally demonstrate that the\naccuracy and robustness of ConvNets measured on Imagenet are vastly\nunderestimated. Next, we show that explanations can mitigate the impact of\nmisclassified adversarial examples from the perspective of the end-user. We\nfinally introduce a novel tool for uncovering the undesirable biases learned by\na model. These contributions also show that explanations are a valuable tool\nboth for improving our understanding of ConvNets' predictions and for designing\nmore reliable models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 14:50:55 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 16:57:30 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Stock", "Pierre", ""], ["Cisse", "Moustapha", ""]]}, {"id": "1711.11453", "submitter": "Bernhard Kratzwald", "authors": "Bernhard Kratzwald, Zhiwu Huang, Danda Pani Paudel, Acharya Dinesh,\n  Luc Van Gool", "title": "Improving Video Generation for Multi-functional Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to improve the state-of-the-art video generative\nadversarial networks (GANs) with a view towards multi-functional applications.\nOur improved video GAN model does not separate foreground from background nor\ndynamic from static patterns, but learns to generate the entire video clip\nconjointly. Our model can thus be trained to generate - and learn from - a\nbroad set of videos with no restriction. This is achieved by designing a robust\none-stream video generation architecture with an extension of the\nstate-of-the-art Wasserstein GAN framework that allows for better convergence.\nThe experimental results show that our improved video GAN model outperforms\nstate-of-theart video generative models on multiple challenging datasets.\nFurthermore, we demonstrate the superiority of our model by successfully\nextending it to three challenging problems: video colorization, video\ninpainting, and future prediction. To the best of our knowledge, this is the\nfirst work using GANs to colorize and inpaint video clips.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 14:55:16 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 20:56:50 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Kratzwald", "Bernhard", ""], ["Huang", "Zhiwu", ""], ["Paudel", "Danda Pani", ""], ["Dinesh", "Acharya", ""], ["Van Gool", "Luc", ""]]}, {"id": "1711.11473", "submitter": "Domen Tabernik", "authors": "Domen Tabernik, Matej Kristan, Ale\\v{s} Leonardis", "title": "Spatially-Adaptive Filter Units for Deep Neural Networks", "comments": "Accepted to Computer Vision and Pattern Recognition 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical deep convolutional networks increase receptive field size by either\ngradual resolution reduction or application of hand-crafted dilated\nconvolutions to prevent increase in the number of parameters. In this paper we\npropose a novel displaced aggregation unit (DAU) that does not require\nhand-crafting. In contrast to classical filters with units (pixels) placed on a\nfixed regular grid, the displacement of the DAUs are learned, which enables\nfilters to spatially-adapt their receptive field to a given problem. We\nextensively demonstrate the strength of DAUs on a classification and semantic\nsegmentation tasks. Compared to ConvNets with regular filter, ConvNets with\nDAUs achieve comparable performance at faster convergence and up to 3-times\nreduction in parameters. Furthermore, DAUs allow us to study deep networks from\nnovel perspectives. We study spatial distributions of DAU filters and analyze\nthe number of parameters allocated for spatial coverage in a filter.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 15:49:13 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 16:30:46 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Tabernik", "Domen", ""], ["Kristan", "Matej", ""], ["Leonardis", "Ale\u0161", ""]]}, {"id": "1711.11479", "submitter": "Thomas Lucas", "authors": "Thomas Lucas and Jakob Verbeek", "title": "Auxiliary Guided Autoregressive Variational Autoencoders", "comments": "Published as a conference paper at ECML-PKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative modeling of high-dimensional data is a key problem in machine\nlearning. Successful approaches include latent variable models and\nautoregressive models. The complementary strengths of these approaches, to\nmodel global and local image statistics respectively, suggest hybrid models\nthat encode global image structure into latent variables while autoregressively\nmodeling low level detail. Previous approaches to such hybrid models restrict\nthe capacity of the autoregressive decoder to prevent degenerate models that\nignore the latent variables and only rely on autoregressive modeling. Our\ncontribution is a training procedure relying on an auxiliary loss function that\ncontrols which information is captured by the latent variables and what is left\nto the autoregressive decoder. Our approach can leverage arbitrarily powerful\nautoregressive decoders, achieves state-of-the art quantitative performance\namong models with latent variables, and generates qualitatively convincing\nsamples.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 15:57:24 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 06:49:26 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Lucas", "Thomas", ""], ["Verbeek", "Jakob", ""]]}, {"id": "1711.11503", "submitter": "Andreas Veit", "authors": "Andreas Veit, Serge Belongie", "title": "Convolutional Networks with Adaptive Inference Graphs", "comments": "IJCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do convolutional networks really need a fixed feed-forward structure? What\nif, after identifying the high-level concept of an image, a network could move\ndirectly to a layer that can distinguish fine-grained differences? Currently, a\nnetwork would first need to execute sometimes hundreds of intermediate layers\nthat specialize in unrelated aspects. Ideally, the more a network already knows\nabout an image, the better it should be at deciding which layer to compute\nnext. In this work, we propose convolutional networks with adaptive inference\ngraphs (ConvNet-AIG) that adaptively define their network topology conditioned\non the input image. Following a high-level structure similar to residual\nnetworks (ResNets), ConvNet-AIG decides for each input image on the fly which\nlayers are needed. In experiments on ImageNet we show that ConvNet-AIG learns\ndistinct inference graphs for different categories. Both ConvNet-AIG with 50\nand 101 layers outperform their ResNet counterpart, while using 20% and 38%\nless computations respectively. By grouping parameters into layers for related\nclasses and only executing relevant layers, ConvNet-AIG improves both\nefficiency and overall classification quality. Lastly, we also study the effect\nof adaptive inference graphs on the susceptibility towards adversarial\nexamples. We observe that ConvNet-AIG shows a higher robustness than ResNets,\ncomplementing other known defense mechanisms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 16:45:25 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 19:21:00 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 20:20:25 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Veit", "Andreas", ""], ["Belongie", "Serge", ""]]}, {"id": "1711.11526", "submitter": "Akisato Kimura", "authors": "Akisato Kimura, Ichiro Takahashi, Masaomi Tanaka, Naoki Yasuda,\n  Naonori Ueda, Naoki Yoshida", "title": "Single-epoch supernova classification with deep convolutional neural\n  networks", "comments": "7 pages, published as a workshop paper in ICDCS2017, in June 2017", "journal-ref": "Published in: 2017 IEEE 37th International Conference on\n  Distributed Computing Systems Workshops (ICDCSW)", "doi": "10.1109/ICDCSW.2017.47", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supernovae Type-Ia (SNeIa) play a significant role in exploring the history\nof the expansion of the Universe, since they are the best-known standard\ncandles with which we can accurately measure the distance to the objects.\nFinding large samples of SNeIa and investigating their detailed characteristics\nhave become an important issue in cosmology and astronomy. Existing methods\nrelied on a photometric approach that first measures the luminance of supernova\ncandidates precisely and then fits the results to a parametric function of\ntemporal changes in luminance. However, it inevitably requires multi-epoch\nobservations and complex luminance measurements. In this work, we present a\nnovel method for classifying SNeIa simply from single-epoch observation images\nwithout any complex measurements, by effectively integrating the\nstate-of-the-art computer vision methodology into the standard photometric\napproach. Our method first builds a convolutional neural network for estimating\nthe luminance of supernovae from telescope images, and then constructs another\nneural network for the classification, where the estimated luminance and\nobservation dates are used as features for classification. Both of the neural\nnetworks are integrated into a single deep neural network to classify SNeIa\ndirectly from observation images. Experimental results show the effectiveness\nof the proposed method and reveal classification performance comparable to\nexisting photometric methods with multi-epoch observations.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 17:30:39 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Kimura", "Akisato", ""], ["Takahashi", "Ichiro", ""], ["Tanaka", "Masaomi", ""], ["Yasuda", "Naoki", ""], ["Ueda", "Naonori", ""], ["Yoshida", "Naoki", ""]]}, {"id": "1711.11543", "submitter": "Abhishek Das", "authors": "Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh,\n  Dhruv Batra", "title": "Embodied Question Answering", "comments": "20 pages, 13 figures, Webpage: https://embodiedqa.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where\nan agent is spawned at a random location in a 3D environment and asked a\nquestion (\"What color is the car?\"). In order to answer, the agent must first\nintelligently navigate to explore the environment, gather information through\nfirst-person (egocentric) vision, and then answer the question (\"orange\").\n  This challenging task requires a range of AI skills -- active perception,\nlanguage understanding, goal-driven navigation, commonsense reasoning, and\ngrounding of language into actions. In this work, we develop the environments,\nend-to-end-trained reinforcement learning agents, and evaluation protocols for\nEmbodiedQA.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:06:47 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 16:55:05 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Das", "Abhishek", ""], ["Datta", "Samyak", ""], ["Gkioxari", "Georgia", ""], ["Lee", "Stefan", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1711.11556", "submitter": "Yuhua Chen", "authors": "Yuhua Chen, Wen Li, Luc Van Gool", "title": "ROAD: Reality Oriented Adaptation for Semantic Segmentation of Urban\n  Scenes", "comments": "Add experiments on SYNTHIA, CVPR 2018 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting synthetic data to learn deep models has attracted increasing\nattention in recent years. However, the intrinsic domain difference between\nsynthetic and real images usually causes a significant performance drop when\napplying the learned model to real world scenarios. This is mainly due to two\nreasons: 1) the model overfits to synthetic images, making the convolutional\nfilters incompetent to extract informative representation for real images; 2)\nthere is a distribution difference between synthetic and real data, which is\nalso known as the domain adaptation problem. To this end, we propose a new\nreality oriented adaptation approach for urban scene semantic segmentation by\nlearning from synthetic data. First, we propose a target guided distillation\napproach to learn the real image style, which is achieved by training the\nsegmentation model to imitate a pretrained real style model using real images.\nSecond, we further take advantage of the intrinsic spatial structure presented\nin urban scene images, and propose a spatial-aware adaptation scheme to\neffectively align the distribution of two domains. These two modules can be\nreadily integrated with existing state-of-the-art semantic segmentation\nnetworks to improve their generalizability when adapting from synthetic to real\nurban scenes. We evaluate the proposed method on Cityscapes dataset by adapting\nfrom GTAV and SYNTHIA datasets, where the results demonstrate the effectiveness\nof our method.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:22:30 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2018 16:21:11 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Chen", "Yuhua", ""], ["Li", "Wen", ""], ["Van Gool", "Luc", ""]]}, {"id": "1711.11566", "submitter": "Sergey Tulyakov", "authors": "Sergey Tulyakov, Andrew Fitzgibbon, Sebastian Nowozin", "title": "Hybrid VAE: Improving Deep Generative Models using Partial Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network models trained on large labeled datasets are the\nstate-of-the-art in a large variety of computer vision tasks. In many\napplications, however, labeled data is expensive to obtain or requires a time\nconsuming manual annotation process. In contrast, unlabeled data is often\nabundant and available in large quantities. We present a principled framework\nto capitalize on unlabeled data by training deep generative models on both\nlabeled and unlabeled data. We show that such a combination is beneficial\nbecause the unlabeled data acts as a data-driven form of regularization,\nallowing generative models trained on few labeled samples to reach the\nperformance of fully-supervised generative models trained on much larger\ndatasets. We call our method Hybrid VAE (H-VAE) as it contains both the\ngenerative and the discriminative parts. We validate H-VAE on three large-scale\ndatasets of different modalities: two face datasets: (MultiPIE, CelebA) and a\nhand pose dataset (NYU Hand Pose). Our qualitative visualizations further\nsupport improvements achieved by using partial observations.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:37:37 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Tulyakov", "Sergey", ""], ["Fitzgibbon", "Andrew", ""], ["Nowozin", "Sebastian", ""]]}, {"id": "1711.11575", "submitter": "Jifeng Dai", "authors": "Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei", "title": "Relation Networks for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although it is well believed for years that modeling relations between\nobjects would help object recognition, there has not been evidence that the\nidea is working in the deep learning era. All state-of-the-art object detection\nsystems still rely on recognizing object instances individually, without\nexploiting their relations during learning.\n  This work proposes an object relation module. It processes a set of objects\nsimultaneously through interaction between their appearance feature and\ngeometry, thus allowing modeling of their relations. It is lightweight and\nin-place. It does not require additional supervision and is easy to embed in\nexisting networks. It is shown effective on improving object recognition and\nduplicate removal steps in the modern object detection pipeline. It verifies\nthe efficacy of modeling object relations in CNN based detection. It gives rise\nto the first fully end-to-end object detector.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:47:41 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 11:21:57 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Hu", "Han", ""], ["Gu", "Jiayuan", ""], ["Zhang", "Zheng", ""], ["Dai", "Jifeng", ""], ["Wei", "Yichen", ""]]}, {"id": "1711.11577", "submitter": "Jifeng Dai", "authors": "Xizhou Zhu, Jifeng Dai, Lu Yuan, Yichen Wei", "title": "Towards High Performance Video Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant progresses for image object detection in recent\nyears. Nevertheless, video object detection has received little attention,\nalthough it is more challenging and more important in practical scenarios.\n  Built upon the recent works, this work proposes a unified approach based on\nthe principle of multi-frame end-to-end learning of features and cross-frame\nmotion. Our approach extends prior works with three new techniques and steadily\npushes forward the performance envelope (speed-accuracy tradeoff), towards high\nperformance video object detection.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:48:45 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Zhu", "Xizhou", ""], ["Dai", "Jifeng", ""], ["Yuan", "Lu", ""], ["Wei", "Yichen", ""]]}, {"id": "1711.11585", "submitter": "Ting-Chun Wang", "authors": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan\n  Catanzaro", "title": "High-Resolution Image Synthesis and Semantic Manipulation with\n  Conditional GANs", "comments": "v2: CVPR camera ready, adding more results for edge-to-photo examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for synthesizing high-resolution photo-realistic\nimages from semantic label maps using conditional generative adversarial\nnetworks (conditional GANs). Conditional GANs have enabled a variety of\napplications, but the results are often limited to low-resolution and still far\nfrom realistic. In this work, we generate 2048x1024 visually appealing results\nwith a novel adversarial loss, as well as new multi-scale generator and\ndiscriminator architectures. Furthermore, we extend our framework to\ninteractive visual manipulation with two additional features. First, we\nincorporate object instance segmentation information, which enables object\nmanipulations such as removing/adding objects and changing the object category.\nSecond, we propose a method to generate diverse results given the same input,\nallowing users to edit the object appearance interactively. Human opinion\nstudies demonstrate that our method significantly outperforms existing methods,\nadvancing both the quality and the resolution of deep image synthesis and\nediting.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:57:21 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 17:55:56 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Wang", "Ting-Chun", ""], ["Liu", "Ming-Yu", ""], ["Zhu", "Jun-Yan", ""], ["Tao", "Andrew", ""], ["Kautz", "Jan", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1711.11586", "submitter": "Richard Zhang", "authors": "Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A.\n  Efros, Oliver Wang, Eli Shechtman", "title": "Toward Multimodal Image-to-Image Translation", "comments": "NIPS 2017 Final paper. v4 updated acknowledgment. Website:\n  https://junyanz.github.io/BicycleGAN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many image-to-image translation problems are ambiguous, as a single input\nimage may correspond to multiple possible outputs. In this work, we aim to\nmodel a \\emph{distribution} of possible outputs in a conditional generative\nmodeling setting. The ambiguity of the mapping is distilled in a\nlow-dimensional latent vector, which can be randomly sampled at test time. A\ngenerator learns to map the given input, combined with this latent code, to the\noutput. We explicitly encourage the connection between output and the latent\ncode to be invertible. This helps prevent a many-to-one mapping from the latent\ncode to the output during training, also known as the problem of mode collapse,\nand produces more diverse results. We explore several variants of this approach\nby employing different training objectives, network architectures, and methods\nof injecting the latent code. Our proposed method encourages bijective\nconsistency between the latent encoding and output modes. We present a\nsystematic comparison of our method and other variants on both perceptual\nrealism and diversity.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:59:01 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 02:56:04 GMT"}, {"version": "v3", "created": "Wed, 2 May 2018 23:37:28 GMT"}, {"version": "v4", "created": "Wed, 24 Oct 2018 00:29:43 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Zhu", "Jun-Yan", ""], ["Zhang", "Richard", ""], ["Pathak", "Deepak", ""], ["Darrell", "Trevor", ""], ["Efros", "Alexei A.", ""], ["Wang", "Oliver", ""], ["Shechtman", "Eli", ""]]}]