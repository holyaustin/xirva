[{"id": "2005.00031", "submitter": "Xiaoran Chen", "authors": "Xiaoran Chen, Suhang You, Kerem Can Tezcan, Ender Konukoglu", "title": "Unsupervised Lesion Detection via Image Restoration with a Normative\n  Prior", "comments": "Extended version of 'Unsupervised Lesion Detection via Image\n  Restoration with a Normative Prior' (MIDL2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised lesion detection is a challenging problem that requires\naccurately estimating normative distributions of healthy anatomy and detecting\nlesions as outliers without training examples. Recently, this problem has\nreceived increased attention from the research community following the advances\nin unsupervised learning with deep learning. Such advances allow the estimation\nof high-dimensional distributions, such as normative distributions, with higher\naccuracy than previous methods.The main approach of the recently proposed\nmethods is to learn a latent-variable model parameterized with networks to\napproximate the normative distribution using example images showing healthy\nanatomy, perform prior-projection, i.e. reconstruct the image with lesions\nusing the latent-variable model, and determine lesions based on the differences\nbetween the reconstructed and original images. While being promising, the\nprior-projection step often leads to a large number of false positives. In this\nwork, we approach unsupervised lesion detection as an image restoration problem\nand propose a probabilistic model that uses a network-based prior as the\nnormative distribution and detect lesions pixel-wise using MAP estimation. The\nprobabilistic model punishes large deviations between restored and original\nimages, reducing false positives in pixel-wise detections. Experiments with\ngliomas and stroke lesions in brain MRI using publicly available datasets show\nthat the proposed approach outperforms the state-of-the-art unsupervised\nmethods by a substantial margin, +0.13 (AUC), for both glioma and stroke\ndetection. Extensive model analysis confirms the effectiveness of MAP-based\nimage restoration.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:03:18 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Chen", "Xiaoran", ""], ["You", "Suhang", ""], ["Tezcan", "Kerem Can", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2005.00057", "submitter": "Li'an Zhuo", "authors": "Li'an Zhuo, Baochang Zhang, Hanlin Chen, Linlin Yang, Chen Chen,\n  Yanjun Zhu and David Doermann", "title": "CP-NAS: Child-Parent Neural Architecture Search for Binary Neural\n  Networks", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) proves to be among the best approaches for\nmany tasks by generating an application-adaptive neural architecture, which is\nstill challenged by high computational cost and memory consumption. At the same\ntime, 1-bit convolutional neural networks (CNNs) with binarized weights and\nactivations show their potential for resource-limited embedded devices. One\nnatural approach is to use 1-bit CNNs to reduce the computation and memory cost\nof NAS by taking advantage of the strengths of each in a unified framework. To\nthis end, a Child-Parent (CP) model is introduced to a differentiable NAS to\nsearch the binarized architecture (Child) under the supervision of a\nfull-precision model (Parent). In the search stage, the Child-Parent model uses\nan indicator generated by the child and parent model accuracy to evaluate the\nperformance and abandon operations with less potential. In the training stage,\na kernel-level CP loss is introduced to optimize the binarized network.\nExtensive experiments demonstrate that the proposed CP-NAS achieves a\ncomparable accuracy with traditional NAS on both the CIFAR and ImageNet\ndatabases. It achieves the accuracy of $95.27\\%$ on CIFAR-10, $64.3\\%$ on\nImageNet with binarized weights and activations, and a $30\\%$ faster search\nthan prior arts.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 19:09:55 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 15:38:02 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zhuo", "Li'an", ""], ["Zhang", "Baochang", ""], ["Chen", "Hanlin", ""], ["Yang", "Linlin", ""], ["Chen", "Chen", ""], ["Zhu", "Yanjun", ""], ["Doermann", "David", ""]]}, {"id": "2005.00060", "submitter": "Pu Zhao", "authors": "Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, Xue\n  Lin", "title": "Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness", "comments": "accepted by ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mode connectivity provides novel geometric insights on analyzing loss\nlandscapes and enables building high-accuracy pathways between well-trained\nneural networks. In this work, we propose to employ mode connectivity in loss\nlandscapes to study the adversarial robustness of deep neural networks, and\nprovide novel methods for improving this robustness. Our experiments cover\nvarious types of adversarial attacks applied to different network architectures\nand datasets. When network models are tampered with backdoor or error-injection\nattacks, our results demonstrate that the path connection learned using limited\namount of bonafide data can effectively mitigate adversarial effects while\nmaintaining the original accuracy on clean data. Therefore, mode connectivity\nprovides users with the power to repair backdoored or error-injected models. We\nalso use mode connectivity to investigate the loss landscapes of regular and\nrobust models against evasion attacks. Experiments show that there exists a\nbarrier in adversarial robustness loss on the path connecting regular and\nadversarially-trained models. A high correlation is observed between the\nadversarial robustness loss and the largest eigenvalue of the input Hessian\nmatrix, for which theoretical justifications are provided. Our results suggest\nthat mode connectivity offers a holistic tool and practical means for\nevaluating and improving adversarial robustness.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 19:12:50 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 03:49:28 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Zhao", "Pu", ""], ["Chen", "Pin-Yu", ""], ["Das", "Payel", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Lin", "Xue", ""]]}, {"id": "2005.00069", "submitter": "Ronan Riochet", "authors": "Ronan Riochet, Josef Sivic, Ivan Laptev and Emmanuel Dupoux", "title": "Occlusion resistant learning of intuitive physics from videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reach human performance on complex tasks, a key ability for artificial\nsystems is to understand physical interactions between objects, and predict\nfuture outcomes of a situation. This ability, often referred to as intuitive\nphysics, has recently received attention and several methods were proposed to\nlearn these physical rules from video sequences. Yet, most of these methods are\nrestricted to the case where no, or only limited, occlusions occur. In this\nwork we propose a probabilistic formulation of learning intuitive physics in 3D\nscenes with significant inter-object occlusions. In our formulation, object\npositions are modeled as latent variables enabling the reconstruction of the\nscene. We then propose a series of approximations that make this problem\ntractable. Object proposals are linked across frames using a combination of a\nrecurrent interaction network, modeling the physics in object space, and a\ncompositional renderer, modeling the way in which objects project onto pixel\nspace. We demonstrate significant improvements over state-of-the-art in the\nintuitive physics benchmark of IntPhys. We apply our method to a second dataset\nwith increasing levels of occlusions, showing it realistically predicts\nsegmentation masks up to 30 frames in the future. Finally, we also show results\non predicting motion of objects in real videos.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 19:35:54 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Riochet", "Ronan", ""], ["Sivic", "Josef", ""], ["Laptev", "Ivan", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "2005.00079", "submitter": "Sinan \\\"Ozg\\\"ur \\\"Ozg\\\"un", "authors": "Sinan \\\"Ozg\\\"ur \\\"Ozg\\\"un, Anne-Marie Rickmann, Abhijit Guha Roy,\n  Christian Wachinger", "title": "Importance Driven Continual Learning for Segmentation Across Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of neural networks to continuously learn and adapt to new tasks\nwhile retaining prior knowledge is crucial for many applications. However,\ncurrent neural networks tend to forget previously learned tasks when trained on\nnew ones, i.e., they suffer from Catastrophic Forgetting (CF). The objective of\nContinual Learning (CL) is to alleviate this problem, which is particularly\nrelevant for medical applications, where it may not be feasible to store and\naccess previously used sensitive patient data. In this work, we propose a\nContinual Learning approach for brain segmentation, where a single network is\nconsecutively trained on samples from different domains. We build upon an\nimportance driven approach and adapt it for medical image segmentation.\nParticularly, we introduce learning rate regularization to prevent the loss of\nthe network's knowledge. Our results demonstrate that directly restricting the\nadaptation of important network parameters clearly reduces Catastrophic\nForgetting for segmentation across domains.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 19:58:18 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["\u00d6zg\u00fcn", "Sinan \u00d6zg\u00fcr", ""], ["Rickmann", "Anne-Marie", ""], ["Roy", "Abhijit Guha", ""], ["Wachinger", "Christian", ""]]}, {"id": "2005.00088", "submitter": "David-Alexandre Beaupr\\'e", "authors": "David-Alexandre Beaupre and Guillaume-Alexandre Bilodeau", "title": "Domain Siamese CNNs for Sparse Multispectral Disparity Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral disparity estimation is a difficult task for many reasons: it\nhas all the same challenges as traditional visible-visible disparity estimation\n(occlusions, repetitive patterns, textureless surfaces), in addition of having\nvery few common visual information between images (e.g. color information vs.\nthermal information). In this paper, we propose a new CNN architecture able to\ndo disparity estimation between images from different spectrum, namely thermal\nand visible in our case. Our proposed model takes two patches as input and\nproceeds to do domain feature extraction for each of them. Features from both\ndomains are then merged with two fusion operations, namely correlation and\nconcatenation. These merged vectors are then forwarded to their respective\nclassification heads, which are responsible for classifying the inputs as being\nsame or not. Using two merging operations gives more robustness to our feature\nextraction process, which leads to more precise disparity estimation. Our\nmethod was tested using the publicly available LITIV 2014 and LITIV 2018\ndatasets, and showed best results when compared to other state of the art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 20:29:59 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Beaupre", "David-Alexandre", ""], ["Bilodeau", "Guillaume-Alexandre", ""]]}, {"id": "2005.00116", "submitter": "Bhuvan Malladihalli Shashidhara", "authors": "Bhuvan Malladihalli Shashidhara, Darshan Mehta, Yash Kale, Dan Morris,\n  Megan Hazen", "title": "Sequence Information Channel Concatenation for Improving Camera Trap\n  Image Burst Classification", "comments": "8 pages, 4 figures, 2 tables. Git repository can be found at:\n  https://github.com/bhuvi3/camera_trap_animal_classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera Traps are extensively used to observe wildlife in their natural\nhabitat without disturbing the ecosystem. This could help in the early\ndetection of natural or human threats to animals, and help towards ecological\nconservation. Currently, a massive number of such camera traps have been\ndeployed at various ecological conservation areas around the world, collecting\ndata for decades, thereby requiring automation to detect images containing\nanimals. Existing systems perform classification to detect if images contain\nanimals by considering a single image. However, due to challenging scenes with\nanimals camouflaged in their natural habitat, it sometimes becomes difficult to\nidentify the presence of animals from merely a single image. We hypothesize\nthat a short burst of images instead of a single image, assuming that the\nanimal moves, makes it much easier for a human as well as a machine to detect\nthe presence of animals. In this work, we explore a variety of approaches, and\nmeasure the impact of using short image sequences (burst of 3 images) on\nimproving the camera trap image classification. We show that concatenating\nmasks containing sequence information and the images from the 3-image-burst\nacross channels, improves the ROC AUC by 20% on a test-set from unseen\ncamera-sites, as compared to an equivalent model that learns from a single\nimage.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 21:47:14 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 02:57:41 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Shashidhara", "Bhuvan Malladihalli", ""], ["Mehta", "Darshan", ""], ["Kale", "Yash", ""], ["Morris", "Dan", ""], ["Hazen", "Megan", ""]]}, {"id": "2005.00127", "submitter": "Hans Dermot Doran", "authors": "Hans Dermot Doran, Monika Reif, Marco Oehler, Curdin Stoehr, Pierluigi\n  Capone", "title": "Conceptual Design of Human-Drone Communication in Collaborative\n  Environments", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robots and drones will work collaboratively and cooperatively in\ntomorrow's industry and agriculture. Before this becomes a reality, some form\nof standardised communication between man and machine must be established that\nspecifically facilitates communication between autonomous machines and both\ntrained and untrained human actors in the working environment. We present\npreliminary results on a human-drone and a drone-human language situated in the\nagricultural industry where interactions with trained and untrained workers and\nvisitors can be expected. We present basic visual indicators enhanced with\nflight patterns for drone-human interaction and human signaling based on\naircraft marshaling for humane-drone interaction. We discuss preliminary\nresults on image recognition and future work.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 22:20:56 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Doran", "Hans Dermot", ""], ["Reif", "Monika", ""], ["Oehler", "Marco", ""], ["Stoehr", "Curdin", ""], ["Capone", "Pierluigi", ""]]}, {"id": "2005.00200", "submitter": "Zhe Gan", "authors": "Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, Jingjing Liu", "title": "HERO: Hierarchical Encoder for Video+Language Omni-representation\n  Pre-training", "comments": "Accepted by EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present HERO, a novel framework for large-scale video+language\nomni-representation learning. HERO encodes multimodal inputs in a hierarchical\nstructure, where local context of a video frame is captured by a Cross-modal\nTransformer via multimodal fusion, and global video context is captured by a\nTemporal Transformer. In addition to standard Masked Language Modeling (MLM)\nand Masked Frame Modeling (MFM) objectives, we design two new pre-training\ntasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global\nand local temporal alignment; and (ii) Frame Order Modeling (FOM), where the\nmodel predicts the right order of shuffled video frames. HERO is jointly\ntrained on HowTo100M and large-scale TV datasets to gain deep understanding of\ncomplex social dynamics with multi-character interactions. Comprehensive\nexperiments demonstrate that HERO achieves new state of the art on multiple\nbenchmarks over Text-based Video/Video-moment Retrieval, Video Question\nAnswering (QA), Video-and-language Inference and Video Captioning tasks across\ndifferent domains. We also introduce two new challenging benchmarks How2QA and\nHow2R for Video QA and Retrieval, collected from diverse video content over\nmultimodalities.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 03:49:26 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 20:37:17 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Li", "Linjie", ""], ["Chen", "Yen-Chun", ""], ["Cheng", "Yu", ""], ["Gan", "Zhe", ""], ["Yu", "Licheng", ""], ["Liu", "Jingjing", ""]]}, {"id": "2005.00214", "submitter": "Ang Li", "authors": "Ang Li, Meghana Thotakuri, David A. Ross, Jo\\~ao Carreira, Alexander\n  Vostrikov, Andrew Zisserman", "title": "The AVA-Kinetics Localized Human Actions Video Dataset", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the AVA-Kinetics localized human actions video dataset.\nThe dataset is collected by annotating videos from the Kinetics-700 dataset\nusing the AVA annotation protocol, and extending the original AVA dataset with\nthese new AVA annotated Kinetics clips. The dataset contains over 230k clips\nannotated with the 80 AVA action classes for each of the humans in key-frames.\nWe describe the annotation process and provide statistics about the new\ndataset. We also include a baseline evaluation using the Video Action\nTransformer Network on the AVA-Kinetics dataset, demonstrating improved\nperformance for action classification on the AVA test set. The dataset can be\ndownloaded from https://research.google.com/ava/\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 04:17:14 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 17:40:28 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Li", "Ang", ""], ["Thotakuri", "Meghana", ""], ["Ross", "David A.", ""], ["Carreira", "Jo\u00e3o", ""], ["Vostrikov", "Alexander", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2005.00225", "submitter": "Ilja Gubins", "authors": "Ilja Gubins, Remco C. Veltkamp", "title": "Deeply Cascaded U-Net for Multi-Task Image Processing", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current practice, many image processing tasks are done sequentially (e.g.\ndenoising, dehazing, followed by semantic segmentation). In this paper, we\npropose a novel multi-task neural network architecture designed for combining\nsequential image processing tasks. We extend U-Net by additional decoding\npathways for each individual task, and explore deep cascading of outputs and\nconnectivity from one pathway to another. We demonstrate effectiveness of the\nproposed approach on denoising and semantic segmentation, as well as on\nprogressive coarse-to-fine semantic segmentation, and achieve better\nperformance than multiple individual or jointly-trained networks, with lower\nnumber of trainable parameters.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 05:06:35 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Gubins", "Ilja", ""], ["Veltkamp", "Remco C.", ""]]}, {"id": "2005.00229", "submitter": "Rahul U", "authors": "Rahul U, Ragul M, Raja Vignesh K, Tejeswinee K", "title": "Deepfake Forensics Using Recurrent Neural Networks", "comments": "This submission has been removed by arXiv administrators due to\n  copyright infringement", "journal-ref": null, "doi": "10.1729/Journal.22894", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As of late an AI based free programming device has made it simple to make\nauthentic face swaps in recordings that leaves barely any hints of control, in\nwhat are known as \"deepfake\" recordings. Situations where these genuine istic\ncounterfeit recordings are utilized to make political pain, extort somebody or\nphony fear based oppression occasions are effectively imagined. This paper\nproposes a transient mindful pipeline to automat-ically recognize deepfake\nrecordings. Our framework utilizes a convolutional neural system (CNN) to\nremove outline level highlights. These highlights are then used to prepare a\nrepetitive neural net-work (RNN) that figures out how to characterize if a\nvideo has been sub-ject to control or not. We assess our technique against a\nhuge arrangement of deepfake recordings gathered from different video sites. We\nshow how our framework can accomplish aggressive outcomes in this assignment\nwhile utilizing a basic design.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 05:27:16 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["U", "Rahul", ""], ["M", "Ragul", ""], ["K", "Raja Vignesh", ""], ["K", "Tejeswinee", ""]]}, {"id": "2005.00246", "submitter": "Ashish V. Thapliyal", "authors": "Ashish V. Thapliyal and Radu Soricut", "title": "Cross-modal Language Generation using Pivot Stabilization for Web-scale\n  Language Coverage", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal language generation tasks such as image captioning are directly\nhurt in their ability to support non-English languages by the trend of\ndata-hungry models combined with the lack of non-English annotations. We\ninvestigate potential solutions for combining existing language-generation\nannotations in English with translation capabilities in order to create\nsolutions at web-scale in both domain and language coverage. We describe an\napproach called Pivot-Language Generation Stabilization (PLuGS), which\nleverages directly at training time both existing English annotations (gold\ndata) as well as their machine-translated versions (silver data); at run-time,\nit generates first an English caption and then a corresponding target-language\ncaption. We show that PLuGS models outperform other candidate solutions in\nevaluations performed over 5 different target languages, under a large-domain\ntestset using images from the Open Images dataset. Furthermore, we find an\ninteresting effect where the English captions generated by the PLuGS models are\nbetter than the captions generated by the original, monolingual English model.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 06:58:18 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Thapliyal", "Ashish V.", ""], ["Soricut", "Radu", ""]]}, {"id": "2005.00253", "submitter": "Elahe Vahdani", "authors": "Elahe Vahdani, Longlong Jing, Yingli Tian, Matt Huenerfauth", "title": "Recognizing American Sign Language Nonmanual Signal Grammar Errors in\n  Continuous Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As part of the development of an educational tool that can help students\nachieve fluency in American Sign Language (ASL) through independent and\ninteractive practice with immediate feedback, this paper introduces a near\nreal-time system to recognize grammatical errors in continuous signing videos\nwithout necessarily identifying the entire sequence of signs. Our system\nautomatically recognizes if performance of ASL sentences contains grammatical\nerrors made by ASL students. We first recognize the ASL grammatical elements\nincluding both manual gestures and nonmanual signals independently from\nmultiple modalities (i.e. hand gestures, facial expressions, and head\nmovements) by 3D-ResNet networks. Then the temporal boundaries of grammatical\nelements from different modalities are examined to detect ASL grammatical\nmistakes by using a sliding window-based approach. We have collected a dataset\nof continuous sign language, ASL-HW-RGBD, covering different aspects of ASL\ngrammars for training and testing. Our system is able to recognize grammatical\nelements on ASL-HW-RGBD from manual gestures, facial expressions, and head\nmovements and successfully detect 8 ASL grammatical mistakes.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 07:25:07 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Vahdani", "Elahe", ""], ["Jing", "Longlong", ""], ["Tian", "Yingli", ""], ["Huenerfauth", "Matt", ""]]}, {"id": "2005.00282", "submitter": "Olly Styles", "authors": "Olly Styles and Tanaya Guha and Victor Sanchez and Alex Kot", "title": "Multi-Camera Trajectory Forecasting: Pedestrian Trajectory Prediction in\n  a Network of Cameras", "comments": "CVPR 2020 Precognition workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the task of multi-camera trajectory forecasting (MCTF), where\nthe future trajectory of an object is predicted in a network of cameras. Prior\nworks consider forecasting trajectories in a single camera view. Our work is\nthe first to consider the challenging scenario of forecasting across multiple\nnon-overlapping camera views. This has wide applicability in tasks such as\nre-identification and multi-target multi-camera tracking. To facilitate\nresearch in this new area, we release the Warwick-NTU Multi-camera Forecasting\nDatabase (WNMF), a unique dataset of multi-camera pedestrian trajectories from\na network of 15 synchronized cameras. To accurately label this large dataset\n(600 hours of video footage), we also develop a semi-automated annotation\nmethod. An effective MCTF model should proactively anticipate where and when a\nperson will re-appear in the camera network. In this paper, we consider the\ntask of predicting the next camera a pedestrian will re-appear after leaving\nthe view of another camera, and present several baseline approaches for this.\nThe labeled database is available online:\nhttps://github.com/olly-styles/Multi-Camera-Trajectory-Forecasting.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 09:28:32 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Styles", "Olly", ""], ["Guha", "Tanaya", ""], ["Sanchez", "Victor", ""], ["Kot", "Alex", ""]]}, {"id": "2005.00288", "submitter": "Saurabh Kumar", "authors": "Ravi Kumar Kushawaha, Saurabh Kumar, Biplab Banerjee, Rajbabu\n  Velmurugan", "title": "Distilling Spikes: Knowledge Distillation in Spiking Neural Networks", "comments": "Preprint: Manuscript under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNN) are energy-efficient computing architectures\nthat exchange spikes for processing information, unlike classical Artificial\nNeural Networks (ANN). Due to this, SNNs are better suited for real-life\ndeployments. However, similar to ANNs, SNNs also benefit from deeper\narchitectures to obtain improved performance. Furthermore, like the deep ANNs,\nthe memory, compute and power requirements of SNNs also increase with model\nsize, and model compression becomes a necessity. Knowledge distillation is a\nmodel compression technique that enables transferring the learning of a large\nmachine learning model to a smaller model with minimal loss in performance. In\nthis paper, we propose techniques for knowledge distillation in spiking neural\nnetworks for the task of image classification. We present ways to distill\nspikes from a larger SNN, also called the teacher network, to a smaller one,\nalso called the student network, while minimally impacting the classification\naccuracy. We demonstrate the effectiveness of the proposed method with detailed\nexperiments on three standard datasets while proposing novel distillation\nmethodologies and loss functions. We also present a multi-stage knowledge\ndistillation technique for SNNs using an intermediate network to obtain higher\nperformance from the student network. Our approach is expected to open up new\navenues for deploying high performing large SNN models on resource-constrained\nhardware platforms.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 09:36:32 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Kushawaha", "Ravi Kumar", ""], ["Kumar", "Saurabh", ""], ["Banerjee", "Biplab", ""], ["Velmurugan", "Rajbabu", ""]]}, {"id": "2005.00305", "submitter": "Abdullah Abuolaim", "authors": "Abdullah Abuolaim and Michael S. Brown", "title": "Defocus Deblurring Using Dual-Pixel Data", "comments": "Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defocus blur arises in images that are captured with a shallow depth of field\ndue to the use of a wide aperture. Correcting defocus blur is challenging\nbecause the blur is spatially varying and difficult to estimate. We propose an\neffective defocus deblurring method that exploits data available on dual-pixel\n(DP) sensors found on most modern cameras. DP sensors are used to assist a\ncamera's auto-focus by capturing two sub-aperture views of the scene in a\nsingle image shot. The two sub-aperture images are used to calculate the\nappropriate lens position to focus on a particular scene region and are\ndiscarded afterwards. We introduce a deep neural network (DNN) architecture\nthat uses these discarded sub-aperture images to reduce defocus blur. A key\ncontribution of our effort is a carefully captured dataset of 500 scenes (2000\nimages) where each scene has: (i) an image with defocus blur captured at a\nlarge aperture; (ii) the two associated DP sub-aperture views; and (iii) the\ncorresponding all-in-focus image captured with a small aperture. Our proposed\nDNN produces results that are significantly better than conventional single\nimage methods in terms of both quantitative and perceptual metrics -- all from\ndata that is already available on the camera but ignored. The dataset, code,\nand trained models are available at\nhttps://github.com/Abdullah-Abuolaim/defocus-deblurring-dual-pixel.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 10:38:00 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 19:10:42 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 23:49:50 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Abuolaim", "Abdullah", ""], ["Brown", "Michael S.", ""]]}, {"id": "2005.00306", "submitter": "Hao Dou", "authors": "Hao Dou, Chen Chen, Xiyuan Hu, Zuxing Xuan, Zhisen Hu, Silong Peng", "title": "PCA-SRGAN: Incremental Orthogonal Projection Discrimination for Face\n  Super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) have been employed for face super\nresolution but they bring distorted facial details easily and still have\nweakness on recovering realistic texture. To further improve the performance of\nGAN based models on super-resolving face images, we propose PCA-SRGAN which\npays attention to the cumulative discrimination in the orthogonal projection\nspace spanned by PCA projection matrix of face data. By feeding the principal\ncomponent projections ranging from structure to details into the discriminator,\nthe discrimination difficulty will be greatly alleviated and the generator can\nbe enhanced to reconstruct clearer contour and finer texture, helpful to\nachieve the high perception and low distortion eventually. This incremental\northogonal projection discrimination has ensured a precise optimization\nprocedure from coarse to fine and avoids the dependence on the perceptual\nregularization. We conduct experiments on CelebA and FFHQ face datasets. The\nqualitative visual effect and quantitative evaluation have demonstrated the\noverwhelming performance of our model over related works.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 10:40:57 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 10:26:21 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Dou", "Hao", ""], ["Chen", "Chen", ""], ["Hu", "Xiyuan", ""], ["Xuan", "Zuxing", ""], ["Hu", "Zhisen", ""], ["Peng", "Silong", ""]]}, {"id": "2005.00328", "submitter": "Pengyi Zhang", "authors": "Pengyi Zhang, Yunxin Zhong, Xiaoqiong Li", "title": "ACCL: Adversarial constrained-CNN loss for weakly supervised medical\n  image segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose adversarial constrained-CNN loss, a new paradigm of\nconstrained-CNN loss methods, for weakly supervised medical image segmentation.\nIn the new paradigm, prior knowledge is encoded and depicted by reference\nmasks, and is further employed to impose constraints on segmentation outputs\nthrough adversarial learning with reference masks. Unlike pseudo label methods\nfor weakly supervised segmentation, such reference masks are used to train a\ndiscriminator rather than a segmentation network, and thus are not required to\nbe paired with specific images. Our new paradigm not only greatly facilitates\nimposing prior knowledge on network's outputs, but also provides stronger and\nhigher-order constraints, i.e., distribution approximation, through adversarial\nlearning. Extensive experiments involving different medical modalities,\ndifferent anatomical structures, different topologies of the object of\ninterest, different levels of prior knowledge and weakly supervised annotations\nwith different annotation ratios is conducted to evaluate our ACCL method.\nConsistently superior segmentation results over the size constrained-CNN loss\nmethod have been achieved, some of which are close to the results of full\nsupervision, thus fully verifying the effectiveness and generalization of our\nmethod. Specifically, we report an average Dice score of 75.4% with an average\nannotation ratio of 0.65%, surpassing the prior art, i.e., the size\nconstrained-CNN loss method, by a large margin of 11.4%. Our codes are made\npublicly available at https://github.com/PengyiZhang/ACCL.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 12:16:22 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Zhang", "Pengyi", ""], ["Zhong", "Yunxin", ""], ["Li", "Xiaoqiong", ""]]}, {"id": "2005.00330", "submitter": "Shailaja Keyur Sampat", "authors": "Shailaja Keyur Sampat, Yezhou Yang and Chitta Baral", "title": "Visuo-Linguistic Question Answering (VLQA) Challenge", "comments": "Findings of EMNLP 2020 (22 pages, 13 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding images and text together is an important aspect of cognition\nand building advanced Artificial Intelligence (AI) systems. As a community, we\nhave achieved good benchmarks over language and vision domains separately,\nhowever joint reasoning is still a challenge for state-of-the-art computer\nvision and natural language processing (NLP) systems. We propose a novel task\nto derive joint inference about a given image-text modality and compile the\nVisuo-Linguistic Question Answering (VLQA) challenge corpus in a question\nanswering setting. Each dataset item consists of an image and a reading\npassage, where questions are designed to combine both visual and textual\ninformation i.e., ignoring either modality would make the question\nunanswerable. We first explore the best existing vision-language architectures\nto solve VLQA subsets and show that they are unable to reason well. We then\ndevelop a modular method with slightly better baseline performance, but it is\nstill far behind human performance. We believe that VLQA will be a good\nbenchmark for reasoning over a visuo-linguistic context. The dataset, code and\nleaderboard is available at https://shailaja183.github.io/vlqa/.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 12:18:55 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 01:06:30 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 07:45:20 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Sampat", "Shailaja Keyur", ""], ["Yang", "Yezhou", ""], ["Baral", "Chitta", ""]]}, {"id": "2005.00336", "submitter": "Vidyasagar Sadhu", "authors": "Vidyasagar Sadhu, Saman Zonouz, Dario Pompili", "title": "On-board Deep-learning-based Unmanned Aerial Vehicle Fault Cause\n  Detection and Identification", "comments": "IEEE International Conference on Robotics and Automation (ICRA), May\n  2020, 6+1 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase in use of Unmanned Aerial Vehicles (UAVs)/drones, it is\nimportant to detect and identify causes of failure in real time for proper\nrecovery from a potential crash-like scenario or post incident forensics\nanalysis. The cause of crash could be either a fault in the sensor/actuator\nsystem, a physical damage/attack, or a cyber attack on the drone's software. In\nthis paper, we propose novel architectures based on deep Convolutional and Long\nShort-Term Memory Neural Networks (CNNs and LSTMs) to detect (via Autoencoder)\nand classify drone mis-operations based on sensor data. The proposed\narchitectures are able to learn high-level features automatically from the raw\nsensor data and learn the spatial and temporal dynamics in the sensor data. We\nvalidate the proposed deep-learning architectures via simulations and\nexperiments on a real drone. Empirical results show that our solution is able\nto detect with over 90% accuracy and classify various types of drone\nmis-operations (with about 99% accuracy (simulation data) and upto 88% accuracy\n(experimental data)).\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 22:46:34 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 18:55:28 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Sadhu", "Vidyasagar", ""], ["Zonouz", "Saman", ""], ["Pompili", "Dario", ""]]}, {"id": "2005.00340", "submitter": "Rania Briq", "authors": "Yifei Zhang, Rania Briq, Julian Tanke, Juergen Gall", "title": "Adversarial Synthesis of Human Pose from Text", "comments": "DAGM GCPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on synthesizing human poses from human-level text\ndescriptions. We propose a model that is based on a conditional generative\nadversarial network. It is designed to generate 2D human poses conditioned on\nhuman-written text descriptions. The model is trained and evaluated using the\nCOCO dataset, which consists of images capturing complex everyday scenes with\nvarious human poses. We show through qualitative and quantitative results that\nthe model is capable of synthesizing plausible poses matching the given text,\nindicating that it is possible to generate poses that are consistent with the\ngiven semantic features, especially for actions with distinctive poses.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 12:32:04 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 09:38:08 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Zhang", "Yifei", ""], ["Briq", "Rania", ""], ["Tanke", "Julian", ""], ["Gall", "Juergen", ""]]}, {"id": "2005.00343", "submitter": "Dima Damen", "authors": "Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler,\n  Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby\n  Perrett, Will Price, Michael Wray", "title": "The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines", "comments": "Preprint for paper at IEEE TPAMI. arXiv admin note: substantial text\n  overlap with arXiv:1804.02748", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its introduction in 2018, EPIC-KITCHENS has attracted attention as the\nlargest egocentric video benchmark, offering a unique viewpoint on people's\ninteraction with objects, their attention, and even intention. In this paper,\nwe detail how this large-scale dataset was captured by 32 participants in their\nnative kitchen environments, and densely annotated with actions and object\ninteractions. Our videos depict nonscripted daily activities, as recording is\nstarted every time a participant entered their kitchen. Recording took place in\n4 countries by participants belonging to 10 different nationalities, resulting\nin highly diverse kitchen habits and cooking styles. Our dataset features 55\nhours of video consisting of 11.5M frames, which we densely labelled for a\ntotal of 39.6K action segments and 454.2K object bounding boxes. Our annotation\nis unique in that we had the participants narrate their own videos after\nrecording, thus reflecting true intention, and we crowd-sourced ground-truths\nbased on these. We describe our object, action and. anticipation challenges,\nand evaluate several baselines over two test splits, seen and unseen kitchens.\nWe introduce new baselines that highlight the multimodal nature of the dataset\nand the importance of explicit temporal modelling to discriminate fine-grained\nactions e.g. 'closing a tap' from 'opening' it up.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 21:57:04 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Damen", "Dima", ""], ["Doughty", "Hazel", ""], ["Farinella", "Giovanni Maria", ""], ["Fidler", "Sanja", ""], ["Furnari", "Antonino", ""], ["Kazakos", "Evangelos", ""], ["Moltisanti", "Davide", ""], ["Munro", "Jonathan", ""], ["Perrett", "Toby", ""], ["Price", "Will", ""], ["Wray", "Michael", ""]]}, {"id": "2005.00355", "submitter": "Bahram Lavi", "authors": "Bahram Lavi, Ihsan Ullah, Mehdi Fatan, and Anderson Rocha", "title": "Survey on Reliable Deep Learning-Based Person Re-Identification Models:\n  Are We There Yet?", "comments": "24 pages, 6 figures, and 2 tables, considered over than 100 papers.\n  arXiv admin note: substantial text overlap with arXiv:1807.05284", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent video-surveillance (IVS) is currently an active research field in\ncomputer vision and machine learning and provides useful tools for surveillance\noperators and forensic video investigators. Person re-identification (PReID) is\none of the most critical problems in IVS, and it consists of recognizing\nwhether or not an individual has already been observed over a camera in a\nnetwork. Solutions to PReID have myriad applications including retrieval of\nvideo-sequences showing an individual of interest or even pedestrian tracking\nover multiple camera views. Different techniques have been proposed to increase\nthe performance of PReID in the literature, and more recently researchers\nutilized deep neural networks (DNNs) given their compelling performance on\nsimilar vision problems and fast execution at test time. Given the importance\nand wide range of applications of re-identification solutions, our objective\nherein is to discuss the work carried out in the area and come up with a survey\nof state-of-the-art DNN models being used for this task. We present\ndescriptions of each model along with their evaluation on a set of benchmark\ndatasets. Finally, we show a detailed comparison among these models, which are\nfollowed by some discussions on their limitations that can work as guidelines\nfor future research.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 16:09:16 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Lavi", "Bahram", ""], ["Ullah", "Ihsan", ""], ["Fatan", "Mehdi", ""], ["Rocha", "Anderson", ""]]}, {"id": "2005.00356", "submitter": "Nagabhushan Somraj", "authors": "Nagabhushan Somraj, Manoj Surya Kashi, S. P. Arun and Rajiv\n  Soundararajan", "title": "Understanding the Perceived Quality of Video Predictions", "comments": "Project website:\n  https://nagabhushansn95.github.io/publications/2020/pvqa.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of video prediction models is believed to be a fundamental approach\nto representation learning for videos. While a plethora of generative models\nfor predicting the future frame pixel values given the past few frames exist,\nthe quantitative evaluation of the predicted frames has been found to be\nextremely challenging. In this context, we study the problem of quality\nassessment of predicted videos. We create the Indian Institute of Science\nPredicted Videos Quality Assessment (IISc PVQA) Database consisting of 300\nvideos, obtained by applying different prediction models on different datasets,\nand accompanying human opinion scores. We collected subjective ratings of\nquality from 50 human participants for these videos. Our subjective study\nreveals that human observers were highly consistent in their judgments of\nquality of predicted videos. We benchmark several popularly used measures for\nevaluating video prediction and show that they do not adequately correlate with\nthese subjective scores. We introduce two new features to effectively capture\nthe quality of predicted videos, motion-compensated cosine similarities of deep\nfeatures of predicted frames with past frames, and deep features extracted from\nrescaled frame differences. We show that our feature design leads to state of\nthe art quality prediction in accordance with human judgments on our IISc PVQA\nDatabase. The database and code are publicly available on our project website:\nhttps://nagabhushansn95.github.io/publications/2020/pvqa\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 13:05:22 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 03:31:08 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 10:37:35 GMT"}, {"version": "v4", "created": "Tue, 22 Jun 2021 08:29:16 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Somraj", "Nagabhushan", ""], ["Kashi", "Manoj Surya", ""], ["Arun", "S. P.", ""], ["Soundararajan", "Rajiv", ""]]}, {"id": "2005.00363", "submitter": "Baichuan Huang", "authors": "Baichuan Huang, Hongwei Yi, Can Huang, Yijia He, Jingbin Liu, Xiao Liu", "title": "M^3VSNet: Unsupervised Multi-metric Multi-view Stereo Network", "comments": "The original top-level version is arXiv:2004.09722v2 but I upload the\n  similar version to arXiv:2005.00363 mistakenly, which is overlapped with\n  arXiv:2004.09722v2. So the submission is to make the two addresses keeping\n  the same version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present Multi-view stereo (MVS) methods with supervised learning-based\nnetworks have an impressive performance comparing with traditional MVS methods.\nHowever, the ground-truth depth maps for training are hard to be obtained and\nare within limited kinds of scenarios. In this paper, we propose a novel\nunsupervised multi-metric MVS network, named M^3VSNet, for dense point cloud\nreconstruction without any supervision. To improve the robustness and\ncompleteness of point cloud reconstruction, we propose a novel multi-metric\nloss function that combines pixel-wise and feature-wise loss function to learn\nthe inherent constraints from different perspectives of matching\ncorrespondences. Besides, we also incorporate the normal-depth consistency in\nthe 3D point cloud format to improve the accuracy and continuity of the\nestimated depth maps. Experimental results show that M3VSNet establishes the\nstate-of-the-arts unsupervised method and achieves comparable performance with\nprevious supervised MVSNet on the DTU dataset and demonstrates the powerful\ngeneralization ability on the Tanks and Temples benchmark with effective\nimprovement. Our code is available at https://github.com/whubaichuan/M3VSNet\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 09:26:51 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 03:07:12 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Huang", "Baichuan", ""], ["Yi", "Hongwei", ""], ["Huang", "Can", ""], ["He", "Yijia", ""], ["Liu", "Jingbin", ""], ["Liu", "Xiao", ""]]}, {"id": "2005.00364", "submitter": "Arghya Pal", "authors": "Arghya Pal, Vineeth N Balasubramanian", "title": "Generative Adversarial Data Programming", "comments": "arXiv admin note: text overlap with arXiv:1803.05137", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paucity of large curated hand-labeled training data forms a major\nbottleneck in the deployment of machine learning models in computer vision and\nother fields. Recent work (Data Programming) has shown how distant supervision\nsignals in the form of labeling functions can be used to obtain labels for\ngiven data in near-constant time. In this work, we present Adversarial Data\nProgramming (ADP), which presents an adversarial methodology to generate data\nas well as a curated aggregated label, given a set of weak labeling functions.\nMore interestingly, such labeling functions are often easily generalizable,\nthus allowing our framework to be extended to different setups, including\nself-supervised labeled image generation, zero-shot text to labeled image\ngeneration, transfer learning, and multi-task learning.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 07:06:44 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Pal", "Arghya", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2005.00375", "submitter": "Zhenqiang Li", "authors": "Zhenqiang Li, Weimin Wang, Zuoyue Li, Yifei Huang, Yoichi Sato", "title": "Towards Visually Explaining Video Understanding Networks with\n  Perturbation", "comments": "Accepted by WACV2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  ''Making black box models explainable'' is a vital problem that accompanies\nthe development of deep learning networks. For networks taking visual\ninformation as input, one basic but challenging explanation method is to\nidentify and visualize the input pixels/regions that dominate the network's\nprediction. However, most existing works focus on explaining networks taking a\nsingle image as input and do not consider the temporal relationship that exists\nin videos. Providing an easy-to-use visual explanation method that is\napplicable to diversified structures of video understanding networks still\nremains an open challenge. In this paper, we investigate a generic\nperturbation-based method for visually explaining video understanding networks.\nBesides, we propose a novel loss function to enhance the method by constraining\nthe smoothness of its results in both spatial and temporal dimensions. The\nmethod enables the comparison of explanation results between different network\nstructures to become possible and can also avoid generating the pathological\nadversarial explanations for video inputs. Experimental comparison results\nverified the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 13:41:38 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 15:30:07 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Zhenqiang", ""], ["Wang", "Weimin", ""], ["Li", "Zuoyue", ""], ["Huang", "Yifei", ""], ["Sato", "Yoichi", ""]]}, {"id": "2005.00383", "submitter": "Yue Qian", "authors": "Yue Qian, Junhui Hou, Qijian Zhang, Yiming Zeng, Sam Kwong, and Ying\n  He", "title": "MOPS-Net: A Matrix Optimization-driven Network forTask-Oriented 3D Point\n  Cloud Downsampling", "comments": "15 pages, 16 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the problem of task-oriented downsampling over 3D point\nclouds, which aims to downsample a point cloud while maintaining the\nperformance of subsequent applications applied to the downsampled sparse points\nas much as possible. Designing from the perspective of matrix optimization, we\npropose MOPS-Net, a novel interpretable deep learning-based method, which is\nfundamentally different from the existing deep learning-based methods due to\nits interpretable feature. The optimization problem is challenging due to its\ndiscrete and combinatorial nature. We tackle the challenges by relaxing the\nbinary constraint of the variables, and formulate a constrained and\ndifferentiable matrix optimization problem. We then design a deep neural\nnetwork to mimic the matrix optimization by exploring both the local and global\nstructures of the input data. MOPS-Net can be end-to-end trained with a task\nnetwork and is permutation-invariant, making it robust to the input. We also\nextend MOPS-Net such that a single network after one-time training is capable\nof handling arbitrary downsampling ratios. Extensive experimental results show\nthat MOPS-Net can achieve favorable performance against state-of-the-art deep\nlearning-based methods over various tasks, including classification,\nreconstruction, and registration. Besides, we validate the robustness of\nMOPS-Net on noisy data.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 14:01:53 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 04:07:07 GMT"}, {"version": "v3", "created": "Sun, 4 Apr 2021 15:07:13 GMT"}, {"version": "v4", "created": "Mon, 12 Apr 2021 17:36:29 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Qian", "Yue", ""], ["Hou", "Junhui", ""], ["Zhang", "Qijian", ""], ["Zeng", "Yiming", ""], ["Kwong", "Sam", ""], ["He", "Ying", ""]]}, {"id": "2005.00419", "submitter": "Tzu-Heng Lin", "authors": "Tzu-Heng Lin", "title": "Aggregation and Finetuning for Clothes Landmark Detection", "comments": "Technical report, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landmark detection for clothes is a fundamental problem for many\napplications. In this paper, a new training scheme for clothes landmark\ndetection: $\\textit{Aggregation and Finetuning}$, is proposed. We investigate\nthe homogeneity among landmarks of different categories of clothes, and utilize\nit to design the procedure of training. Extensive experiments show that our\nmethod outperforms current state-of-the-art methods by a large margin. Our\nmethod also won the 1st place in the DeepFashion2 Challenge 2020 - Clothes\nLandmark Estimation Track with an AP of 0.590 on the test set, and 0.615 on the\nvalidation set. Code will be publicly available at\nhttps://github.com/lzhbrian/deepfashion2-kps-agg-finetune .\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 14:47:08 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Lin", "Tzu-Heng", ""]]}, {"id": "2005.00430", "submitter": "A. Joseph Antony", "authors": "Mark Marsden, Kevin McGuinness, Joseph Antony, Haolin Wei, Milan\n  Redzic, Jian Tang, Zhilan Hu, Alan Smeaton, Noel E O'Connor", "title": "Investigating Class-level Difficulty Factors in Multi-label\n  Classification Problems", "comments": "Published in ICME 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This work investigates the use of class-level difficulty factors in\nmulti-label classification problems for the first time. Four class-level\ndifficulty factors are proposed: frequency, visual variation, semantic\nabstraction, and class co-occurrence. Once computed for a given multi-label\nclassification dataset, these difficulty factors are shown to have several\npotential applications including the prediction of class-level performance\nacross datasets and the improvement of predictive performance through\ndifficulty weighted optimisation. Significant improvements to mAP and AUC\nperformance are observed for two challenging multi-label datasets (WWW Crowd\nand Visual Genome) with the inclusion of difficulty weighted optimisation. The\nproposed technique does not require any additional computational complexity\nduring training or inference and can be extended over time with inclusion of\nother class-level difficulty factors.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:06:53 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Marsden", "Mark", ""], ["McGuinness", "Kevin", ""], ["Antony", "Joseph", ""], ["Wei", "Haolin", ""], ["Redzic", "Milan", ""], ["Tang", "Jian", ""], ["Hu", "Zhilan", ""], ["Smeaton", "Alan", ""], ["O'Connor", "Noel E", ""]]}, {"id": "2005.00450", "submitter": "Ciprian Corneanu", "authors": "Ciprian Corneanu, Meysam Madadi, Sergio Escalera, Aleix Martinez", "title": "Computing the Testing Error without a Testing Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have revolutionized computer vision. We now have\nDNNs that achieve top (performance) results in many problems, including object\nrecognition, facial expression analysis, and semantic segmentation, to name but\na few. The design of the DNNs that achieve top results is, however, non-trivial\nand mostly done by trail-and-error. That is, typically, researchers will derive\nmany DNN architectures (i.e., topologies) and then test them on multiple\ndatasets. However, there are no guarantees that the selected DNN will perform\nwell in the real world. One can use a testing set to estimate the performance\ngap between the training and testing sets, but avoiding\noverfitting-to-the-testing-data is almost impossible. Using a sequestered\ntesting dataset may address this problem, but this requires a constant update\nof the dataset, a very expensive venture. Here, we derive an algorithm to\nestimate the performance gap between training and testing that does not require\nany testing dataset. Specifically, we derive a number of persistent topology\nmeasures that identify when a DNN is learning to generalize to unseen samples.\nThis allows us to compute the DNN's testing error on unseen samples, even when\nwe do not have access to them. We provide extensive experimental validation on\nmultiple networks and datasets to demonstrate the feasibility of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:35:50 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Corneanu", "Ciprian", ""], ["Madadi", "Meysam", ""], ["Escalera", "Sergio", ""], ["Martinez", "Aleix", ""]]}, {"id": "2005.00463", "submitter": "George Awad", "authors": "Keith Curtis, George Awad, Shahzad Rajput, and Ian Soboroff", "title": "HLVU : A New Challenge to Test Deep Understanding of Movies the Way\n  Humans do", "comments": null, "journal-ref": null, "doi": "10.1145/3372278.3390742", "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new evaluation challenge and direction in the area\nof High-level Video Understanding. The challenge we are proposing is designed\nto test automatic video analysis and understanding, and how accurately systems\ncan comprehend a movie in terms of actors, entities, events and their\nrelationship to each other. A pilot High-Level Video Understanding (HLVU)\ndataset of open source movies were collected for human assessors to build a\nknowledge graph representing each of them. A set of queries will be derived\nfrom the knowledge graph to test systems on retrieving relationships among\nactors, as well as reasoning and retrieving non-visual concepts. The objective\nis to benchmark if a computer system can \"understand\" non-explicit but obvious\nrelationships the same way humans do when they watch the same movies. This is\nlong-standing problem that is being addressed in the text domain and this\nproject moves similar research to the video domain. Work of this nature is\nfoundational to future video analytics and video understanding technologies.\nThis work can be of interest to streaming services and broadcasters hoping to\nprovide more intuitive ways for their customers to interact with and consume\nvideo content.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:58:13 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Curtis", "Keith", ""], ["Awad", "George", ""], ["Rajput", "Shahzad", ""], ["Soboroff", "Ian", ""]]}, {"id": "2005.00499", "submitter": "Kamran Ali", "authors": "Kamran Ali and Charles E. Hughes", "title": "An Efficient Integration of Disentangled Attended Expression and\n  Identity FeaturesFor Facial Expression Transfer andSynthesis", "comments": "10 Pages, excluding references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an Attention-based Identity Preserving Generative\nAdversarial Network (AIP-GAN) to overcome the identity leakage problem from a\nsource image to a generated face image, an issue that is encountered in a\ncross-subject facial expression transfer and synthesis process. Our key insight\nis that the identity preserving network should be able to disentangle and\ncompose shape, appearance, and expression information for efficient facial\nexpression transfer and synthesis. Specifically, the expression encoder of our\nAIP-GAN disentangles the expression information from the input source image by\npredicting its facial landmarks using our supervised spatial and channel-wise\nattention module. Similarly, the disentangled expression-agnostic identity\nfeatures are extracted from the input target image by inferring its combined\nintrinsic-shape and appearance image employing our self-supervised spatial and\nchannel-wise attention mod-ule. To leverage the expression and identity\ninformation encoded by the intermediate layers of both of our encoders, we\ncombine these features with the features learned by the intermediate layers of\nour decoder using a cross-encoder bilinear pooling operation. Experimental\nresults show the promising performance of our AIP-GAN based technique.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 17:14:53 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Ali", "Kamran", ""], ["Hughes", "Charles E.", ""]]}, {"id": "2005.00559", "submitter": "Zhan Xu", "authors": "Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Chris Landreth and Karan\n  Singh", "title": "RigNet: Neural Rigging for Articulated Characters", "comments": "SIGGRAPH 2020. Project page https://zhan-xu.github.io/rig-net/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RigNet, an end-to-end automated method for producing animation\nrigs from input character models. Given an input 3D model representing an\narticulated character, RigNet predicts a skeleton that matches the animator\nexpectations in joint placement and topology. It also estimates surface skin\nweights based on the predicted skeleton. Our method is based on a deep\narchitecture that directly operates on the mesh representation without making\nassumptions on shape class and structure. The architecture is trained on a\nlarge and diverse collection of rigged models, including their mesh, skeletons\nand corresponding skin weights. Our evaluation is three-fold: we show better\nresults than prior art when quantitatively compared to animator rigs;\nqualitatively we show that our rigs can be expressively posed and animated at\nmultiple levels of detail; and finally, we evaluate the impact of various\nalgorithm choices on our output rigs.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 18:12:44 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 19:38:56 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Xu", "Zhan", ""], ["Zhou", "Yang", ""], ["Kalogerakis", "Evangelos", ""], ["Landreth", "Chris", ""], ["Singh", "Karan", ""]]}, {"id": "2005.00570", "submitter": "Boqing Gong", "authors": "Dan Kondratyuk, Mingxing Tan, Matthew Brown, and Boqing Gong", "title": "When Ensembling Smaller Models is More Efficient than Single Large\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembling is a simple and popular technique for boosting evaluation\nperformance by training multiple models (e.g., with different initializations)\nand aggregating their predictions. This approach is commonly reserved for the\nlargest models, as it is commonly held that increasing the model size provides\na more substantial reduction in error than ensembling smaller models. However,\nwe show results from experiments on CIFAR-10 and ImageNet that ensembles can\noutperform single models with both higher accuracy and requiring fewer total\nFLOPs to compute, even when those individual models' weights and\nhyperparameters are highly optimized. Furthermore, this gap in improvement\nwidens as models become large. This presents an interesting observation that\noutput diversity in ensembling can often be more efficient than training larger\nmodels, especially when the models approach the size of what their dataset can\nfoster. Instead of using the common practice of tuning a single large model,\none can use ensembles as a more flexible trade-off between a model's inference\nspeed and accuracy. This also potentially eases hardware design, e.g., an\neasier way to parallelize the model across multiple workers for real-time or\ndistributed inference.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 18:56:18 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Kondratyuk", "Dan", ""], ["Tan", "Mingxing", ""], ["Brown", "Matthew", ""], ["Gong", "Boqing", ""]]}, {"id": "2005.00589", "submitter": "Nancy Xin Ru Wang", "authors": "Xinyi Zheng, Doug Burdick, Lucian Popa, Xu Zhong, Nancy Xin Ru Wang", "title": "Global Table Extractor (GTE): A Framework for Joint Table Identification\n  and Cell Structure Recognition Using Visual Context", "comments": null, "journal-ref": "Winter Conference for Applications in Computer Vision (WACV) 2021", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Documents are often used for knowledge sharing and preservation in business\nand science, within which are tables that capture most of the critical data.\nUnfortunately, most documents are stored and distributed as PDF or scanned\nimages, which fail to preserve logical table structure. Recent vision-based\ndeep learning approaches have been proposed to address this gap, but most still\ncannot achieve state-of-the-art results. We present Global Table Extractor\n(GTE), a vision-guided systematic framework for joint table detection and cell\nstructured recognition, which could be built on top of any object detection\nmodel. With GTE-Table, we invent a new penalty based on the natural cell\ncontainment constraint of tables to train our table network aided by cell\nlocation predictions. GTE-Cell is a new hierarchical cell detection network\nthat leverages table styles. Further, we design a method to automatically label\ntable and cell structure in existing documents to cheaply create a large corpus\nof training and test data. We use this to enhance PubTabNet with cell labels\nand create FinTabNet, real-world and complex scientific and financial datasets\nwith detailed table structure annotations to help train and test structure\nrecognition. Our framework surpasses previous state-of-the-art results on the\nICDAR 2013 and ICDAR 2019 table competition in both table detection and cell\nstructure recognition with a significant 5.8% improvement in the full table\nextraction system. Further experiments demonstrate a greater than 45%\nimprovement in cell structure recognition when compared to a vanilla RetinaNet\nobject detection model in our new out-of-domain FinTabNet.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 20:14:49 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 04:45:25 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Zheng", "Xinyi", ""], ["Burdick", "Doug", ""], ["Popa", "Lucian", ""], ["Zhong", "Xu", ""], ["Wang", "Nancy Xin Ru", ""]]}, {"id": "2005.00596", "submitter": "Zhuolin Jiang", "authors": "Zhuolin Jiang, Jan Silovsky, Man-Hung Siu, William Hartmann, Herbert\n  Gish, Sancar Adali", "title": "Learning from Noisy Labels with Noise Modeling Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label image classification has generated significant interest in recent\nyears and the performance of such systems often suffers from the not so\ninfrequent occurrence of incorrect or missing labels in the training data. In\nthis paper, we extend the state-of the-art of training classifiers to jointly\ndeal with both forms of errorful data. We accomplish this by modeling noisy and\nmissing labels in multi-label images with a new Noise Modeling Network (NMN)\nthat follows our convolutional neural network (CNN), integrates with it,\nforming an end-to-end deep learning system, which can jointly learn the noise\ndistribution and CNN parameters. The NMN learns the distribution of noise\npatterns directly from the noisy data without the need for any clean training\ndata. The NMN can model label noise that depends only on the true label or is\nalso dependent on the image features. We show that the integrated NMN/CNN\nlearning system consistently improves the classification performance, for\ndifferent levels of label noise, on the MSR-COCO dataset and MSR-VTT dataset.\nWe also show that noise performance improvements are obtained when multiple\ninstance learning methods are used.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 20:32:22 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Jiang", "Zhuolin", ""], ["Silovsky", "Jan", ""], ["Siu", "Man-Hung", ""], ["Hartmann", "William", ""], ["Gish", "Herbert", ""], ["Adali", "Sancar", ""]]}, {"id": "2005.00619", "submitter": "Gabriel Ilharco", "authors": "Gabriel Ilharco, Rowan Zellers, Ali Farhadi, Hannaneh Hajishirzi", "title": "Probing Contextual Language Models for Common Ground with Visual\n  Representations", "comments": "Proceedings of the 2021 North American Chapter of the Association for\n  Computational Linguistics (NAACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of large-scale contextual language models has attracted great\ninterest in probing what is encoded in their representations. In this work, we\nconsider a new question: to what extent contextual representations of concrete\nnouns are aligned with corresponding visual representations? We design a\nprobing model that evaluates how effective are text-only representations in\ndistinguishing between matching and non-matching visual representations. Our\nfindings show that language representations alone provide a strong signal for\nretrieving image patches from the correct object categories. Moreover, they are\neffective in retrieving specific instances of image patches; textual context\nplays an important role in this process. Visually grounded language models\nslightly outperform text-only language models in instance retrieval, but\ngreatly under-perform humans. We hope our analyses inspire future research in\nunderstanding and improving the visual capabilities of language models.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 21:28:28 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 17:19:20 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 22:12:40 GMT"}, {"version": "v4", "created": "Tue, 27 Oct 2020 16:40:01 GMT"}, {"version": "v5", "created": "Tue, 13 Apr 2021 16:02:39 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Ilharco", "Gabriel", ""], ["Zellers", "Rowan", ""], ["Farhadi", "Ali", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2005.00656", "submitter": "Nathan Drenkow", "authors": "Neil Fendley, Max Lennon, I-Jeng Wang, Philippe Burlina, Nathan\n  Drenkow", "title": "Jacks of All Trades, Masters Of None: Addressing Distributional Shift\n  and Obtrusiveness via Transparent Patch Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the development of effective adversarial patch attacks and -- for\nthe first time -- jointly address the antagonistic objectives of attack success\nand obtrusiveness via the design of novel semi-transparent patches. This work\nis motivated by our pursuit of a systematic performance analysis of patch\nattack robustness with regard to geometric transformations. Specifically, we\nfirst elucidate a) key factors underpinning patch attack success and b) the\nimpact of distributional shift between training and testing/deployment when\ncast under the Expectation over Transformation (EoT) formalism. By focusing our\nanalysis on three principal classes of transformations (rotation, scale, and\nlocation), our findings provide quantifiable insights into the design of\neffective patch attacks and demonstrate that scale, among all factors,\nsignificantly impacts patch attack success. Working from these findings, we\nthen focus on addressing how to overcome the principal limitations of scale for\nthe deployment of attacks in real physical settings: namely the obtrusiveness\nof large patches. Our strategy is to turn to the novel design of\nirregularly-shaped, semi-transparent partial patches which we construct via a\nnew optimization process that jointly addresses the antagonistic goals of\nmitigating obtrusiveness and maximizing effectiveness. Our study -- we hope --\nwill help encourage more focus in the community on the issues of obtrusiveness,\nscale, and success in patch attacks.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 23:50:37 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Fendley", "Neil", ""], ["Lennon", "Max", ""], ["Wang", "I-Jeng", ""], ["Burlina", "Philippe", ""], ["Drenkow", "Nathan", ""]]}, {"id": "2005.00670", "submitter": "Morihiro Mizutani", "authors": "Morihiro Mizutani, Akifumi Okuno, Geewook Kim, Hidetoshi Shimodaira", "title": "Stochastic Neighbor Embedding of Multimodal Relational Data for\n  Image-Text Simultaneous Visualization", "comments": "20 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal relational data analysis has become of increasing importance in\nrecent years, for exploring across different domains of data, such as images\nand their text tags obtained from social networking services (e.g., Flickr). A\nvariety of data analysis methods have been developed for visualization; to give\nan example, t-Stochastic Neighbor Embedding (t-SNE) computes low-dimensional\nfeature vectors so that their similarities keep those of the observed data\nvectors. However, t-SNE is designed only for a single domain of data but not\nfor multimodal data; this paper aims at visualizing multimodal relational data\nconsisting of data vectors in multiple domains with relations across these\nvectors. By extending t-SNE, we herein propose Multimodal Relational Stochastic\nNeighbor Embedding (MR-SNE), that (1) first computes augmented relations, where\nwe observe the relations across domains and compute those within each of\ndomains via the observed data vectors, and (2) jointly embeds the augmented\nrelations to a low-dimensional space. Through visualization of Flickr and\nAnimal with Attributes 2 datasets, proposed MR-SNE is compared with other graph\nembedding-based approaches; MR-SNE demonstrates the promising performance.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 00:39:29 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mizutani", "Morihiro", ""], ["Okuno", "Akifumi", ""], ["Kim", "Geewook", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "2005.00673", "submitter": "Zheng Tang", "authors": "Zheng Tang, Milind Naphade, Stan Birchfield, Jonathan Tremblay,\n  William Hodge, Ratnesh Kumar, Shuo Wang, Xiaodong Yang", "title": "PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification\n  Using Highly Randomized Synthetic Data", "comments": "Accepted by ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In comparison with person re-identification (ReID), which has been widely\nstudied in the research community, vehicle ReID has received less attention.\nVehicle ReID is challenging due to 1) high intra-class variability (caused by\nthe dependency of shape and appearance on viewpoint), and 2) small inter-class\nvariability (caused by the similarity in shape and appearance between vehicles\nproduced by different manufacturers). To address these challenges, we propose a\nPose-Aware Multi-Task Re-Identification (PAMTRI) framework. This approach\nincludes two innovations compared with previous methods. First, it overcomes\nviewpoint-dependency by explicitly reasoning about vehicle pose and shape via\nkeypoints, heatmaps and segments from pose estimation. Second, it jointly\nclassifies semantic vehicle attributes (colors and types) while performing\nReID, through multi-task learning with the embedded pose representations. Since\nmanually labeling images with detailed pose and attribute information is\nprohibitive, we create a large-scale highly randomized synthetic dataset with\nautomatically annotated vehicle attributes for training. Extensive experiments\nvalidate the effectiveness of each proposed component, showing that PAMTRI\nachieves significant improvement over state-of-the-art on two mainstream\nvehicle ReID benchmarks: VeRi and CityFlow-ReID. Code and models are available\nat https://github.com/NVlabs/PAMTRI.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 01:29:09 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Tang", "Zheng", ""], ["Naphade", "Milind", ""], ["Birchfield", "Stan", ""], ["Tremblay", "Jonathan", ""], ["Hodge", "William", ""], ["Kumar", "Ratnesh", ""], ["Wang", "Shuo", ""], ["Yang", "Xiaodong", ""]]}, {"id": "2005.00695", "submitter": "Hongyang Zhang", "authors": "Sen Wu, Hongyang R. Zhang, Gregory Valiant, Christopher R\\'e", "title": "On the Generalization Effects of Linear Transformations in Data\n  Augmentation", "comments": "International Conference on Machine learning (ICML) 2020. Added\n  experimental results on ImageNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a powerful technique to improve performance in\napplications such as image and text classification tasks. Yet, there is little\nrigorous understanding of why and how various augmentations work. In this work,\nwe consider a family of linear transformations and study their effects on the\nridge estimator in an over-parametrized linear regression setting. First, we\nshow that transformations which preserve the labels of the data can improve\nestimation by enlarging the span of the training data. Second, we show that\ntransformations which mix data can improve estimation by playing a\nregularization effect. Finally, we validate our theoretical insights on MNIST.\nBased on the insights, we propose an augmentation scheme that searches over the\nspace of transformations by how uncertain the model is about the transformed\ndata. We validate our proposed scheme on image and text datasets. For example,\nour method outperforms RandAugment by 1.24% on CIFAR-100 using\nWide-ResNet-28-10. Furthermore, we achieve comparable accuracy to the SoTA\nAdversarial AutoAugment on CIFAR datasets.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 04:10:21 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 06:00:23 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Wu", "Sen", ""], ["Zhang", "Hongyang R.", ""], ["Valiant", "Gregory", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "2005.00706", "submitter": "Frank F. Xu", "authors": "Frank F. Xu, Lei Ji, Botian Shi, Junyi Du, Graham Neubig, Yonatan\n  Bisk, Nan Duan", "title": "A Benchmark for Structured Procedural Knowledge Extraction from Cooking\n  Videos", "comments": "Accepted by NLP Beyond Text - First International Workshop on Natural\n  Language Processing Beyond Text @ EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watching instructional videos are often used to learn about procedures. Video\ncaptioning is one way of automatically collecting such knowledge. However, it\nprovides only an indirect, overall evaluation of multimodal models with no\nfiner-grained quantitative measure of what they have learned. We propose\ninstead, a benchmark of structured procedural knowledge extracted from cooking\nvideos. This work is complementary to existing tasks, but requires models to\nproduce interpretable structured knowledge in the form of verb-argument tuples.\nOur manually annotated open-vocabulary resource includes 356 instructional\ncooking videos and 15,523 video clip/sentence-level annotations. Our analysis\nshows that the proposed task is challenging and standard modeling approaches\nlike unsupervised segmentation, semantic role labeling, and visual action\ndetection perform poorly when forced to predict every action of a procedure in\na structured form.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 05:15:20 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 13:54:27 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Xu", "Frank F.", ""], ["Ji", "Lei", ""], ["Shi", "Botian", ""], ["Du", "Junyi", ""], ["Neubig", "Graham", ""], ["Bisk", "Yonatan", ""], ["Duan", "Nan", ""]]}, {"id": "2005.00724", "submitter": "Sanjay Subramanian", "authors": "Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer\n  Singh, Jonathan Berant, Matt Gardner", "title": "Obtaining Faithful Interpretations from Compositional Neural Networks", "comments": "ACL 2020; first three authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural module networks (NMNs) are a popular approach for modeling\ncompositionality: they achieve high accuracy when applied to problems in\nlanguage and vision, while reflecting the compositional structure of the\nproblem in the network architecture. However, prior work implicitly assumed\nthat the structure of the network modules, describing the abstract reasoning\nprocess, provides a faithful explanation of the model's reasoning; that is,\nthat all modules perform their intended behaviour. In this work, we propose and\nconduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2\nand DROP, two datasets which require composing multiple reasoning steps. We\nfind that the intermediate outputs differ from the expected output,\nillustrating that the network structure does not provide a faithful explanation\nof model behaviour. To remedy that, we train the model with auxiliary\nsupervision and propose particular choices for module architecture that yield\nmuch better faithfulness, at a minimal cost to accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 06:50:35 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 15:52:28 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Subramanian", "Sanjay", ""], ["Bogin", "Ben", ""], ["Gupta", "Nitish", ""], ["Wolfson", "Tomer", ""], ["Singh", "Sameer", ""], ["Berant", "Jonathan", ""], ["Gardner", "Matt", ""]]}, {"id": "2005.00725", "submitter": "Numan Khurshid", "authors": "Numan Khurshid, Talha Hanif, Mohbat Tharani, Murtaza Taj", "title": "Cross-View Image Retrieval -- Ground to Aerial Image Retrieval through\n  Deep Learning", "comments": "International Conference on Neural Information Processing\n  (ICONIP-2019)", "journal-ref": null, "doi": "10.1007/978-3-030-36711-4_19", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval aims to measure the content similarity between\ndifferent types of data. The idea has been previously applied to visual, text,\nand speech data. In this paper, we present a novel cross-modal retrieval method\nspecifically for multi-view images, called Cross-view Image Retrieval CVIR. Our\napproach aims to find a feature space as well as an embedding space in which\nsamples from street-view images are compared directly to satellite-view images\n(and vice-versa). For this comparison, a novel deep metric learning based\nsolution \"DeepCVIR\" has been proposed. Previous cross-view image datasets are\ndeficient in that they (1) lack class information; (2) were originally\ncollected for cross-view image geolocalization task with coupled images; (3) do\nnot include any images from off-street locations. To train, compare, and\nevaluate the performance of cross-view image retrieval, we present a new 6\nclass cross-view image dataset termed as CrossViewRet which comprises of images\nincluding freeway, mountain, palace, river, ship, and stadium with 700\nhigh-resolution dual-view images for each class. Results show that the proposed\nDeepCVIR outperforms conventional matching approaches on the CVIR task for the\ngiven dataset and would also serve as the baseline for future research.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 06:52:16 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Khurshid", "Numan", ""], ["Hanif", "Talha", ""], ["Tharani", "Mohbat", ""], ["Taj", "Murtaza", ""]]}, {"id": "2005.00727", "submitter": "Nikolaos Passalis", "authors": "Nikolaos Passalis, Maria Tzelepi, Anastasios Tefas", "title": "Heterogeneous Knowledge Distillation using Information Flow Modeling", "comments": "Accepted at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation (KD) methods are capable of transferring the knowledge\nencoded in a large and complex teacher into a smaller and faster student. Early\nmethods were usually limited to transferring the knowledge only between the\nlast layers of the networks, while latter approaches were capable of performing\nmulti-layer KD, further increasing the accuracy of the student. However,\ndespite their improved performance, these methods still suffer from several\nlimitations that restrict both their efficiency and flexibility. First,\nexisting KD methods typically ignore that neural networks undergo through\ndifferent learning phases during the training process, which often requires\ndifferent types of supervision for each one. Furthermore, existing multi-layer\nKD methods are usually unable to effectively handle networks with significantly\ndifferent architectures (heterogeneous KD). In this paper we propose a novel KD\nmethod that works by modeling the information flow through the various layers\nof the teacher model and then train a student model to mimic this information\nflow. The proposed method is capable of overcoming the aforementioned\nlimitations by using an appropriate supervision scheme during the different\nphases of the training process, as well as by designing and training an\nappropriate auxiliary teacher model that acts as a proxy model capable of\n\"explaining\" the way the teacher works to the student. The effectiveness of the\nproposed method is demonstrated using four image datasets and several different\nevaluation setups.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 06:56:56 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Passalis", "Nikolaos", ""], ["Tzelepi", "Maria", ""], ["Tefas", "Anastasios", ""]]}, {"id": "2005.00728", "submitter": "Jesse Thomason", "authors": "Homero Roman Roman, Yonatan Bisk, Jesse Thomason, Asli Celikyilmaz,\n  Jianfeng Gao", "title": "RMM: A Recursive Mental Model for Dialog Navigation", "comments": "Findings of Empirical Methods in Natural Language Processing (EMNLP\n  Findings), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language-guided robots must be able to both ask humans questions and\nunderstand answers. Much existing work focuses only on the latter. In this\npaper, we go beyond instruction following and introduce a two-agent task where\none agent navigates and asks questions that a second, guiding agent answers.\nInspired by theory of mind, we propose the Recursive Mental Model (RMM). The\nnavigating agent models the guiding agent to simulate answers given candidate\ngenerated questions. The guiding agent in turn models the navigating agent to\nsimulate navigation steps it would take to generate answers. We use the\nprogress agents make towards the goal as a reinforcement learning reward signal\nto directly inform not only navigation actions, but also both question and\nanswer generation. We demonstrate that RMM enables better generalization to\nnovel environments. Interlocutor modelling may be a way forward for human-agent\ndialogue where robots need to both ask and answer questions.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 06:57:14 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 02:16:27 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Roman", "Homero Roman", ""], ["Bisk", "Yonatan", ""], ["Thomason", "Jesse", ""], ["Celikyilmaz", "Asli", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2005.00754", "submitter": "Yuying Chen", "authors": "Yuying Chen, Congcong Liu, Bertram Shi and Ming Liu", "title": "CoMoGCN: Coherent Motion Aware Trajectory Prediction with Graph\n  Representation", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting human trajectories is critical for tasks such as robot crowd\nnavigation and autonomous driving. Modeling social interactions is of great\nimportance for accurate group-wise motion prediction. However, most existing\nmethods do not consider information about coherence within the crowd, but\nrather only pairwise interactions. In this work, we propose a novel framework,\ncoherent motion aware graph convolutional network (CoMoGCN), for trajectory\nprediction in crowded scenes with group constraints. First, we cluster\npedestrian trajectories into groups according to motion coherence. Then, we use\ngraph convolutional networks to aggregate crowd information efficiently. The\nCoMoGCN also takes advantage of variational autoencoders to capture the\nmultimodal nature of the human trajectories by modeling the distribution. Our\nmethod achieves state-of-the-art performance on several different trajectory\nprediction benchmarks, and the best average performance among all benchmarks\nconsidered.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 09:10:30 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 08:47:12 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Chen", "Yuying", ""], ["Liu", "Congcong", ""], ["Shi", "Bertram", ""], ["Liu", "Ming", ""]]}, {"id": "2005.00762", "submitter": "Yixing Huang", "authors": "Lin Yuan, Yixing Huang and Andreas Maier", "title": "Projection Inpainting Using Partial Convolution for Metal Artifact\n  Reduction", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer tomography, due to the presence of metal implants in the patient\nbody, reconstructed images will suffer from metal artifacts. In order to reduce\nmetal artifacts, metals are typically removed in projection images. Therefore,\nthe metal corrupted projection areas need to be inpainted. For deep learning\ninpainting methods, convolutional neural networks (CNNs) are widely used, for\nexample, the U-Net. However, such CNNs use convolutional filter responses on\nboth valid and corrupted pixel values, resulting in unsatisfactory image\nquality. In this work, partial convolution is applied for projection\ninpainting, which only relies on valid pixels values. The U-Net with partial\nconvolution and conventional convolution are compared for metal artifact\nreduction. Our experiments demonstrate that the U-Net with partial convolution\nis able to inpaint the metal corrupted areas better than that with conventional\nconvolution.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 09:32:35 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Yuan", "Lin", ""], ["Huang", "Yixing", ""], ["Maier", "Andreas", ""]]}, {"id": "2005.00777", "submitter": "Shuyue Jia", "authors": "Yimin Hou, Shuyue Jia, Xiangmin Lun, Yan Shi, Yang Li", "title": "Deep Feature Mining via Attention-based BiLSTM-GCN for Human Motor\n  Imagery Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognition accuracy and response time are both critically essential ahead of\nbuilding practical electroencephalography (EEG) based brain-computer interface\n(BCI). Recent approaches, however, have either compromised in the\nclassification accuracy or responding time. This paper presents a novel deep\nlearning approach designed towards remarkably accurate and responsive motor\nimagery (MI) recognition based on scalp EEG. Bidirectional Long Short-term\nMemory (BiLSTM) with the Attention mechanism manages to derive relevant\nfeatures from raw EEG signals. The connected graph convolutional neural network\n(GCN) promotes the decoding performance by cooperating with the topological\nstructure of features, which are estimated from the overall data. The\n0.4-second detection framework has shown effective and efficient prediction\nbased on individual and group-wise training, with 98.81% and 94.64% accuracy,\nrespectively, which outperformed all the state-of-the-art studies. The\nintroduced deep feature mining approach can precisely recognize human motion\nintents from raw EEG signals, which paves the road to translate the EEG based\nMI recognition to practical BCI systems.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 10:03:40 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 03:29:42 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Hou", "Yimin", ""], ["Jia", "Shuyue", ""], ["Lun", "Xiangmin", ""], ["Shi", "Yan", ""], ["Li", "Yang", ""]]}, {"id": "2005.00828", "submitter": "Ali Hamdi", "authors": "Ali Hamdi, Flora Salim, Du Yong Kim", "title": "DroTrack: High-speed Drone-based Object Tracking Under Uncertainty", "comments": "10 pages, 12 figures, FUZZ-IEEE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DroTrack, a high-speed visual single-object tracking framework for\ndrone-captured video sequences. Most of the existing object tracking methods\nare designed to tackle well-known challenges, such as occlusion and cluttered\nbackgrounds. The complex motion of drones, i.e., multiple degrees of freedom in\nthree-dimensional space, causes high uncertainty. The uncertainty problem leads\nto inaccurate location predictions and fuzziness in scale estimations. DroTrack\nsolves such issues by discovering the dependency between object representation\nand motion geometry. We implement an effective object segmentation based on\nFuzzy C Means (FCM). We incorporate the spatial information into the membership\nfunction to cluster the most discriminative segments. We then enhance the\nobject segmentation by using a pre-trained Convolution Neural Network (CNN)\nmodel. DroTrack also leverages the geometrical angular motion to estimate a\nreliable object scale. We discuss the experimental results and performance\nevaluation using two datasets of 51,462 drone-captured frames. The combination\nof the FCM segmentation and the angular scaling increased DroTrack precision by\nup to $9\\%$ and decreased the centre location error by $162$ pixels on average.\nDroTrack outperforms all the high-speed trackers and achieves comparable\nresults in comparison to deep learning trackers. DroTrack offers high frame\nrates up to 1000 frame per second (fps) with the best location precision, more\nthan a set of state-of-the-art real-time trackers.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 13:16:16 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Hamdi", "Ali", ""], ["Salim", "Flora", ""], ["Kim", "Du Yong", ""]]}, {"id": "2005.00836", "submitter": "Saman Zadtootaghaj", "authors": "Markus Utke, Saman Zadtootaghaj, Steven Schmidt, Sebastian M\\\"oller", "title": "Towards Deep Learning Methods for Quality Assessment of\n  Computer-Generated Imagery", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video gaming streaming services are growing rapidly due to new services such\nas passive video streaming, e.g. Twitch.tv, and cloud gaming, e.g. Nvidia\nGeforce Now. In contrast to traditional video content, gaming content has\nspecial characteristics such as extremely high motion for some games, special\nmotion patterns, synthetic content and repetitive content, which makes the\nstate-of-the-art video and image quality metrics perform weaker for this\nspecial computer generated content. In this paper, we outline our plan to build\na deep learningbased quality metric for video gaming quality assessment. In\naddition, we present initial results by training the network based on VMAF\nvalues as a ground truth to give some insights on how to build a metric in\nfuture. The paper describes the method that is used to choose an appropriate\nConvolutional Neural Network architecture. Furthermore, we estimate the size of\nthe required subjective quality dataset which achieves a sufficiently high\nperformance. The results show that by taking around 5k images for training of\nthe last six modules of Xception, we can obtain a relatively high performance\nmetric to assess the quality of distorted video games.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 14:08:39 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Utke", "Markus", ""], ["Zadtootaghaj", "Saman", ""], ["Schmidt", "Steven", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2005.00844", "submitter": "Nathanael Lemessa Baisa", "authors": "Nathanael L. Baisa", "title": "Derivation of a Constant Velocity Motion Model for Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion models play a great role in visual tracking applications for\npredicting the possible locations of objects in the next frame. Unlike target\ntracking in radar or aerospace domain which considers only points, object\ntracking in computer vision involves sizes of objects. Constant velocity motion\nmodel is the most widely used motion model for visual tracking, however, there\nis no clear and understandable derivation involving sizes of objects specially\nfor new researchers joining this research field. In this document, we derive\nthe constant velocity motion model that incorporates sizes of objects that, we\nthink, can help the new researchers to adapt to it very quickly.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 14:40:18 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 12:09:10 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 20:00:43 GMT"}, {"version": "v4", "created": "Tue, 20 Oct 2020 22:05:50 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Baisa", "Nathanael L.", ""]]}, {"id": "2005.00890", "submitter": "Aythami Morales", "authors": "Alejandro Acien and Aythami Morales and Julian Fierrez and Ruben\n  Vera-Rodriguez", "title": "BeCAPTCHA-Mouse: Synthetic Mouse Trajectories and Improved Bot Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We first study the suitability of behavioral biometrics to distinguish\nbetween computers and humans, commonly named as bot detection. We then present\nBeCAPTCHA-Mouse, a bot detector based on: i) a neuromotor model of mouse\ndynamics to obtain a novel feature set for the classification of human and bot\nsamples; and ii) a learning framework involving real and synthetically\ngenerated mouse trajectories. We propose two new mouse trajectory synthesis\nmethods for generating realistic data: a) a function-based method based on\nheuristic functions, and b) a data-driven method based on Generative\nAdversarial Networks (GANs) in which a Generator synthesizes human-like\ntrajectories from a Gaussian noise input. Experiments are conducted on a new\ntestbed also introduced here and available in GitHub: BeCAPTCHA-Mouse\nBenchmark; useful for research in bot detection and other mouse-based HCI\napplications. Our benchmark data consists of 15,000 mouse trajectories\nincluding real data from 58 users and bot data with various levels of realism.\nOur experiments show that BeCAPTCHA-Mouse is able to detect bot trajectories of\nhigh realism with 93% of accuracy in average using only one mouse trajectory.\nWhen our approach is fused with state-of-the-art mouse dynamic features, the\nbot detection accuracy increases relatively by more than 36%, proving that\nmouse-based bot detection is a fast, easy, and reliable tool to complement\ntraditional CAPTCHA systems.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 17:40:49 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 18:35:31 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Acien", "Alejandro", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""], ["Vera-Rodriguez", "Ruben", ""]]}, {"id": "2005.00908", "submitter": "Shengjie Li", "authors": "Malihe Alikhani, Piyush Sharma, Shengjie Li, Radu Soricut and Matthew\n  Stone", "title": "Clue: Cross-modal Coherence Modeling for Caption Generation", "comments": "Accepted as a long paper to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use coherence relations inspired by computational models of discourse to\nstudy the information needs and goals of image captioning. Using an annotation\nprotocol specifically devised for capturing image--caption coherence relations,\nwe annotate 10,000 instances from publicly-available image--caption pairs. We\nintroduce a new task for learning inferences in imagery and text, coherence\nrelation prediction, and show that these coherence annotations can be exploited\nto learn relation classifiers as an intermediary step, and also train\ncoherence-aware, controllable image captioning models. The results show a\ndramatic improvement in the consistency and quality of the generated captions\nwith respect to information needs specified via coherence relations.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 19:28:52 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Alikhani", "Malihe", ""], ["Sharma", "Piyush", ""], ["Li", "Shengjie", ""], ["Soricut", "Radu", ""], ["Stone", "Matthew", ""]]}, {"id": "2005.00922", "submitter": "Francis Engelmann", "authors": "Francis Engelmann, J\\\"org St\\\"uckler, Bastian Leibe", "title": "SAMP: Shape and Motion Priors for 4D Vehicle Reconstruction", "comments": null, "journal-ref": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2017", "doi": "10.1109/WACV.2017.51", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the pose and shape of vehicles in 3D from a movable platform still\nremains a challenging task due to the projective sensing principle of cameras,\ndifficult surface properties e.g. reflections or transparency, and illumination\nchanges between images. In this paper, we propose to use 3D shape and motion\npriors to regularize the estimation of the trajectory and the shape of vehicles\nin sequences of stereo images. We represent shapes by 3D signed distance\nfunctions and embed them in a low-dimensional manifold. Our optimization method\nallows for imposing a common shape across all image observations along an\nobject track. We employ a motion model to regularize the trajectory to\nplausible object motions. We evaluate our method on the KITTI dataset and show\nstate-of-the-art results in terms of shape reconstruction and pose estimation\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 21:23:54 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Engelmann", "Francis", ""], ["St\u00fcckler", "J\u00f6rg", ""], ["Leibe", "Bastian", ""]]}, {"id": "2005.00925", "submitter": "Bingyu Xin", "authors": "Bingyu Xin, Yifan Hu, Yefeng Zheng, Hongen Liao", "title": "Multi-Modality Generative Adversarial Networks with Tumor Consistency\n  Loss for Brain MR Image Synthesis", "comments": "5 pages, 3 figures, accepted to IEEE ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance (MR) images of different modalities can provide\ncomplementary information for clinical diagnosis, but whole modalities are\noften costly to access. Most existing methods only focus on synthesizing\nmissing images between two modalities, which limits their robustness and\nefficiency when multiple modalities are missing. To address this problem, we\npropose a multi-modality generative adversarial network (MGAN) to synthesize\nthree high-quality MR modalities (FLAIR, T1 and T1ce) from one MR modality T2\nsimultaneously. The experimental results show that the quality of the\nsynthesized images by our proposed methods is better than the one synthesized\nby the baseline model, pix2pix. Besides, for MR brain image synthesis, it is\nimportant to preserve the critical tumor information in the generated\nmodalities, so we further introduce a multi-modality tumor consistency loss to\nMGAN, called TC-MGAN. We use the synthesized modalities by TC-MGAN to boost the\ntumor segmentation accuracy, and the results demonstrate its effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 21:33:15 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Xin", "Bingyu", ""], ["Hu", "Yifan", ""], ["Zheng", "Yefeng", ""], ["Liao", "Hongen", ""]]}, {"id": "2005.00945", "submitter": "Shmuel Friedland", "authors": "Shmuel Friedland", "title": "Tensor optimal transport, distance between sets of measures and tensor\n  scaling", "comments": "32 pages, some of the results in arXiv:1905.11384 are repeated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the optimal transport problem for $d>2$ discrete measures. This is a\nlinear programming problem on $d$-tensors. It gives a way to compute a\n\"distance\" between two sets of discrete measures. We introduce an entropic\nregularization term, which gives rise to a scaling of tensors. We give a\nvariation of the celebrated Sinkhorn scaling algorithm. We show that this\nalgorithm can be viewed as a partial minimization algorithm of a strictly\nconvex function. Under appropriate conditions the rate of convergence is\ngeometric and we estimate the rate. Our results are generalizations of known\nresults for the classical case of two discrete measures.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 23:49:31 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 22:27:51 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Friedland", "Shmuel", ""]]}, {"id": "2005.00946", "submitter": "Rick Chang", "authors": "Jen-Hao Rick Chang, Anat Levin, B. V. K. Vijaya Kumar, Aswin C.\n  Sankaranarayanan", "title": "Towards Occlusion-Aware Multifocal Displays", "comments": "SIGGRAPH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual system uses numerous cues for depth perception, including\ndisparity, accommodation, motion parallax and occlusion. It is incumbent upon\nvirtual-reality displays to satisfy these cues to provide an immersive user\nexperience. Multifocal displays, one of the classic approaches to satisfy the\naccommodation cue, place virtual content at multiple focal planes, each at a di\nerent depth. However, the content on focal planes close to the eye do not\nocclude those farther away; this deteriorates the occlusion cue as well as\nreduces contrast at depth discontinuities due to leakage of the defocus blur.\nThis paper enables occlusion-aware multifocal displays using a novel ConeTilt\noperator that provides an additional degree of freedom -- tilting the light\ncone emitted at each pixel of the display panel. We show that, for scenes with\nrelatively simple occlusion con gurations, tilting the light cones provides the\nsame e ect as physical occlusion. We demonstrate that ConeTilt can be easily\nimplemented by a phase-only spatial light modulator. Using a lab prototype, we\nshow results that demonstrate the presence of occlusion cues and the increased\ncontrast of the display at depth edges.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 23:51:11 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Chang", "Jen-Hao Rick", ""], ["Levin", "Anat", ""], ["Kumar", "B. V. K. Vijaya", ""], ["Sankaranarayanan", "Aswin C.", ""]]}, {"id": "2005.00953", "submitter": "Rao Muhammad Umer", "authors": "Rao Muhammad Umer, Gian Luca Foresti, Christian Micheloni", "title": "Deep Generative Adversarial Residual Convolutional Networks for\n  Real-World Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most current deep learning based single image super-resolution (SISR) methods\nfocus on designing deeper / wider models to learn the non-linear mapping\nbetween low-resolution (LR) inputs and the high-resolution (HR) outputs from a\nlarge number of paired (LR/HR) training data. They usually take as assumption\nthat the LR image is a bicubic down-sampled version of the HR image. However,\nsuch degradation process is not available in real-world settings i.e. inherent\nsensor noise, stochastic noise, compression artifacts, possible mismatch\nbetween image degradation process and camera device. It reduces significantly\nthe performance of current SISR methods due to real-world image corruptions. To\naddress these problems, we propose a deep Super-Resolution Residual\nConvolutional Generative Adversarial Network (SRResCGAN) to follow the\nreal-world degradation settings by adversarial training the model with\npixel-wise supervision in the HR domain from its generated LR counterpart. The\nproposed network exploits the residual learning by minimizing the energy-based\nobjective function with powerful image regularization and convex optimization\ntechniques. We demonstrate our proposed approach in quantitative and\nqualitative experiments that generalize robustly to real input and it is easy\nto deploy for other down-scaling operators and mobile/embedded devices.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 00:12:38 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Umer", "Rao Muhammad", ""], ["Foresti", "Gian Luca", ""], ["Micheloni", "Christian", ""]]}, {"id": "2005.00959", "submitter": "Tom Tirer", "authors": "Tom Tirer, Raja Giryes", "title": "On the Convergence Rate of Projected Gradient Descent for a\n  Back-Projection based Objective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ill-posed linear inverse problems appear in many scientific setups, and are\ntypically addressed by solving optimization problems, which are composed of\ndata fidelity and prior terms. Recently, several works have considered a\nback-projection (BP) based fidelity term as an alternative to the common least\nsquares (LS), and demonstrated excellent results for popular inverse problems.\nThese works have also empirically shown that using the BP term, rather than the\nLS term, requires fewer iterations of optimization algorithms. In this paper,\nwe examine the convergence rate of the projected gradient descent (PGD)\nalgorithm for the BP objective. Our analysis allows to identify an inherent\nsource for its faster convergence compared to using the LS objective, while\nmaking only mild assumptions. We also analyze the more general proximal\ngradient method under a relaxed contraction condition on the proximal mapping\nof the prior. This analysis further highlights the advantage of BP when the\nlinear measurement operator is badly conditioned. Numerical experiments with\nboth $\\ell_1$-norm and GAN-based priors corroborate our theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 00:58:23 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 18:04:47 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Tirer", "Tom", ""], ["Giryes", "Raja", ""]]}, {"id": "2005.00966", "submitter": "Ruxin Wang", "authors": "Ruxin Wang, Shuyuan Chen, Chaojie Ji, Jianping Fan, and Ye Li", "title": "Boundary-aware Context Neural Network for Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation can provide a reliable basis for further clinical\nanalysis and disease diagnosis. The performance of medical image segmentation\nhas been significantly advanced with the convolutional neural networks (CNNs).\nHowever, most existing CNNs-based methods often produce unsatisfactory\nsegmentation mask without accurate object boundaries. This is caused by the\nlimited context information and inadequate discriminative feature maps after\nconsecutive pooling and convolution operations. In that the medical image is\ncharacterized by the high intra-class variation, inter-class indistinction and\nnoise, extracting powerful context and aggregating discriminative features for\nfine-grained segmentation are still challenging today. In this paper, we\nformulate a boundary-aware context neural network (BA-Net) for 2D medical image\nsegmentation to capture richer context and preserve fine spatial information.\nBA-Net adopts encoder-decoder architecture. In each stage of encoder network,\npyramid edge extraction module is proposed for obtaining edge information with\nmultiple granularities firstly. Then we design a mini multi-task learning\nmodule for jointly learning to segment object masks and detect lesion\nboundaries. In particular, a new interactive attention is proposed to bridge\ntwo tasks for achieving information complementarity between different tasks,\nwhich effectively leverages the boundary information for offering a strong cue\nto better segmentation prediction. At last, a cross feature fusion module aims\nto selectively aggregate multi-level features from the whole encoder network.\nBy cascaded three modules, richer context and fine-grain features of each stage\nare encoded. Extensive experiments on five datasets show that the proposed\nBA-Net outperforms state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 02:35:49 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wang", "Ruxin", ""], ["Chen", "Shuyuan", ""], ["Ji", "Chaojie", ""], ["Fan", "Jianping", ""], ["Li", "Ye", ""]]}, {"id": "2005.00974", "submitter": "Zihao Wang", "authors": "Srutarshi Banerjee, Zihao W. Wang, Henry H. Chopp, Oliver Cossairt,\n  Aggelos Katsaggelos", "title": "Lossy Event Compression based on Image-derived Quad Trees and Poisson\n  Disk Sampling", "comments": "8 main pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With several advantages over conventional RGB cameras, event cameras have\nprovided new opportunities for tackling visual tasks under challenging\nscenarios with fast motion, high dynamic range, and/or power constraint. Yet\nunlike image/video compression, the performance of event compression algorithm\nis far from satisfying and practical. The main challenge for compressing events\nis the unique event data form, i.e., a stream of asynchronously fired event\ntuples each encoding the 2D spatial location, timestamp, and polarity (denoting\nan increase or decrease in brightness). Since events only encode temporal\nvariations, they lack spatial structure which is crucial for compression. To\naddress this problem, we propose a novel event compression algorithm based on a\nquad tree (QT) segmentation map derived from the adjacent intensity images. The\nQT informs 2D spatial priority within the 3D space-time volume. In the event\nencoding step, events are first aggregated over time to form polarity-based\nevent histograms. The histograms are then variably sampled via Poisson Disk\nSampling prioritized by the QT based segmentation map. Next, differential\nencoding and run length encoding are employed for encoding the spatial and\npolarity information of the sampled events, respectively, followed by Huffman\nencoding to produce the final encoded events. Our Poisson Disk Sampling based\nLossy Event Compression (PDS-LEC) algorithm performs rate-distortion based\noptimal allocation. On average, our algorithm achieves greater than 6x\ncompression compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 03:18:43 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 07:41:48 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Banerjee", "Srutarshi", ""], ["Wang", "Zihao W.", ""], ["Chopp", "Henry H.", ""], ["Cossairt", "Oliver", ""], ["Katsaggelos", "Aggelos", ""]]}, {"id": "2005.00976", "submitter": "Xiang Li", "authors": "Xiang Li and Songcan Chen", "title": "A Concise yet Effective model for Non-Aligned Incomplete Multi-view and\n  Missing Multi-label Learning", "comments": "15 pages, 7 figures", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reality, learning from multi-view multi-label data inevitably confronts\nthree challenges: missing labels, incomplete views, and non-aligned views.\nExisting methods mainly concern the first two and commonly need multiple\nassumptions to attack them, making even state-of-the-arts involve at least two\nexplicit hyper-parameters such that model selection is quite difficult. More\nroughly, they will fail in handling the third challenge, let alone addressing\nthe three jointly. In this paper, we aim at meeting these under the least\nassumption by building a concise yet effective model with just one\nhyper-parameter. To ease insufficiency of available labels, we exploit not only\nthe consensus of multiple views but also the global and local structures hidden\namong multiple labels. Specifically, we introduce an indicator matrix to tackle\nthe first two challenges in a regression form while aligning the same\nindividual labels and all labels of different views in a common label space to\nbattle the third challenge. In aligning, we characterize the global and local\nstructures of multiple labels to be high-rank and low-rank, respectively.\nSubsequently, an efficient algorithm with linear time complexity in the number\nof samples is established. Finally, even without view-alignment, our method\nsubstantially outperforms state-of-the-arts with view-alignment on five real\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 03:38:24 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 12:01:24 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Li", "Xiang", ""], ["Chen", "Songcan", ""]]}, {"id": "2005.00983", "submitter": "Moktari Mostofa", "authors": "Moktari Mostofa, Syeda Nyma Ferdous, Benjamin S.Riggan, and Nasser M.\n  Nasrabadi", "title": "Joint-SRVDNet: Joint Super Resolution and Vehicle Detection Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many domestic and military applications, aerial vehicle detection and\nsuper-resolutionalgorithms are frequently developed and applied independently.\nHowever, aerial vehicle detection on super-resolved images remains a\nchallenging task due to the lack of discriminative information in the\nsuper-resolved images. To address this problem, we propose a Joint\nSuper-Resolution and Vehicle DetectionNetwork (Joint-SRVDNet) that tries to\ngenerate discriminative, high-resolution images of vehicles fromlow-resolution\naerial images. First, aerial images are up-scaled by a factor of 4x using a\nMulti-scaleGenerative Adversarial Network (MsGAN), which has multiple\nintermediate outputs with increasingresolutions. Second, a detector is trained\non super-resolved images that are upscaled by factor 4x usingMsGAN architecture\nand finally, the detection loss is minimized jointly with the super-resolution\nloss toencourage the target detector to be sensitive to the subsequent\nsuper-resolution training. The network jointlylearns hierarchical and\ndiscriminative features of targets and produces optimal super-resolution\nresults. Weperform both quantitative and qualitative evaluation of our proposed\nnetwork on VEDAI, xView and DOTAdatasets. The experimental results show that\nour proposed framework achieves better visual quality than thestate-of-the-art\nmethods for aerial super-resolution with 4x up-scaling factor and improves the\naccuracy ofaerial vehicle detection.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 04:28:44 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mostofa", "Moktari", ""], ["Ferdous", "Syeda Nyma", ""], ["Riggan", "Benjamin S.", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2005.00986", "submitter": "Mengyun Shi", "authors": "Mengyun Shi, Van Dyk Lewis", "title": "Using Artificial Intelligence to Analyze Fashion Trends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing fashion trends is essential in the fashion industry. Current\nfashion forecasting firms, such as WGSN, utilize the visual information from\naround the world to analyze and predict fashion trends. However, analyzing\nfashion trends is time-consuming and extremely labor intensive, requiring\nindividual employees' manual editing and classification. To improve the\nefficiency of data analysis of such image-based information and lower the cost\nof analyzing fashion images, this study proposes a data-driven quantitative\nabstracting approach using an artificial intelligence (A.I.) algorithm.\nSpecifically, an A.I. model was trained on fashion images from a large-scale\ndataset under different scenarios, for example in online stores and street\nsnapshots. This model was used to detect garments and classify clothing\nattributes such as textures, garment style, and details for runway photos and\nvideos. It was found that the A.I. model can generate rich attribute\ndescriptions of detected regions and accurately bind the garments in the\nimages. Adoption of A.I. algorithm demonstrated promising results and the\npotential to classify garment types and details automatically, which can make\nthe process of trend forecasting more cost-effective and faster.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 04:46:12 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Shi", "Mengyun", ""], ["Lewis", "Van Dyk", ""]]}, {"id": "2005.01004", "submitter": "Giang Nguyen", "authors": "Giang Nguyen, Shuan Chen, Tae Joon Jun, Daeyoung Kim", "title": "Explaining How Deep Neural Networks Forget by Deep Visualization", "comments": "12 pages, 4 figures, 1 table. arXiv admin note: substantial text\n  overlap with arXiv:2001.01578", "journal-ref": "ICPR 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explaining the behaviors of deep neural networks, usually considered as black\nboxes, is critical especially when they are now being adopted over diverse\naspects of human life. Taking the advantages of interpretable machine learning\n(interpretable ML), this paper proposes a novel tool called Catastrophic\nForgetting Dissector (or CFD) to explain catastrophic forgetting in continual\nlearning settings. We also introduce a new method called Critical Freezing\nbased on the observations of our tool. Experiments on ResNet articulate how\ncatastrophic forgetting happens, particularly showing which components of this\nfamous network are forgetting. Our new continual learning algorithm defeats\nvarious recent techniques by a significant margin, proving the capability of\nthe investigation. Critical freezing not only attacks catastrophic forgetting\nbut also exposes explainability.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 06:44:38 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 04:13:58 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Nguyen", "Giang", ""], ["Chen", "Shuan", ""], ["Jun", "Tae Joon", ""], ["Kim", "Daeyoung", ""]]}, {"id": "2005.01014", "submitter": "Xiaoshui Huang", "authors": "Xiaoshui Huang, Guofeng Mei, Jian Zhang", "title": "Feature-metric Registration: A Fast Semi-supervised Approach for Robust\n  Point Cloud Registration without Correspondences", "comments": "CVPR2020 final", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast feature-metric point cloud registration framework, which\nenforces the optimisation of registration by minimising a feature-metric\nprojection error without correspondences. The advantage of the feature-metric\nprojection error is robust to noise, outliers and density difference in\ncontrast to the geometric projection error. Besides, minimising the\nfeature-metric projection error does not need to search the correspondences so\nthat the optimisation speed is fast. The principle behind the proposed method\nis that the feature difference is smallest if point clouds are aligned very\nwell. We train the proposed method in a semi-supervised or unsupervised\napproach, which requires limited or no registration label data. Experiments\ndemonstrate our method obtains higher accuracy and robustness than the\nstate-of-the-art methods. Besides, experimental results show that the proposed\nmethod can handle significant noise and density difference, and solve both\nsame-source and cross-source point cloud registration.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 07:26:59 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Huang", "Xiaoshui", ""], ["Mei", "Guofeng", ""], ["Zhang", "Jian", ""]]}, {"id": "2005.01016", "submitter": "Andreas Toftegaard Kristensen", "authors": "Andreas Toftegaard Kristensen, Robert Giterman, Alexios\n  Balatsoukas-Stimming, and Andreas Burg", "title": "Lupulus: A Flexible Hardware Accelerator for Neural Networks", "comments": "To be presented at the 2020 International Conference on Acoustics,\n  Speech, and Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have become indispensable for a wide range of applications,\nbut they suffer from high computational- and memory-requirements, requiring\noptimizations from the algorithmic description of the network to the hardware\nimplementation. Moreover, the high rate of innovation in machine learning makes\nit important that hardware implementations provide a high level of\nprogrammability to support current and future requirements of neural networks.\nIn this work, we present a flexible hardware accelerator for neural networks,\ncalled Lupulus, supporting various methods for scheduling and mapping of\noperations onto the accelerator. Lupulus was implemented in a 28nm FD-SOI\ntechnology and demonstrates a peak performance of 380 GOPS/GHz with latencies\nof 21.4ms and 183.6ms for the convolutional layers of AlexNet and VGG-16,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 07:35:36 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Kristensen", "Andreas Toftegaard", ""], ["Giterman", "Robert", ""], ["Balatsoukas-Stimming", "Alexios", ""], ["Burg", "Andreas", ""]]}, {"id": "2005.01047", "submitter": "Yaroslav Khaustov", "authors": "Ya. Ye. Khaustov (1), D. Ye (1), Ye. Ryzhov (1), E. Lychkovskyy (2)\n  and Yu. A. Nastishin (1) ((1) Hetman Petro Sahaidachnyi National Army\n  Academy, (2) Lviv Danylo Halytsky National Medical University)", "title": "Fusion of visible and infrared images via complex function", "comments": "12 pages with 7 figures, submitted to Military Technical Collection\n  22 (2020) see http://vtz.asv.gov.ua", "journal-ref": null, "doi": "10.33577/2312-4458.22.2020", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an algorithm for the fusion of partial images collected from the\nvisual and infrared cameras such that the visual and infrared images are the\nreal and imaginary parts of a complex function. The proposed image fusion\nalgorithm of the complex function is a generalization for the algorithm of\nconventional image addition in the same way as the addition of complex numbers\nis the generalization for the addition of real numbers. The proposed algorithm\nof the complex function is simple in use and non-demanding in computer power.\nThe complex form of the fused image opens a possibility to form the fused image\neither as the amplitude image or as a phase image, which in turn can be in\nseveral forms. We show theoretically that the local contrast of the fused phase\nimages is higher than those of the partial images as well as in comparison with\nthe images obtained by the algorithm of the simple or weighted addition.\nExperimental image quality assessment of the fused phase images performed using\nthe histograms, entropy shows the higher quality of the phase images in\ncomparison with those of the input partial images as well as those obtained\nwith different fusion methods reported in the literature. Keywords: digital\nimage processing, image fusion, infrared imaging, image quality assessment\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 10:55:31 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Khaustov", "Ya. Ye.", ""], ["Ye", "D.", ""], ["Ryzhov", "Ye.", ""], ["Lychkovskyy", "E.", ""], ["Nastishin", "Yu. A.", ""]]}, {"id": "2005.01056", "submitter": "Kai Zhang", "authors": "Kai Zhang, Shuhang Gu, Radu Timofte, Taizhang Shang, Qiuju Dai,\n  Shengchen Zhu, Tong Yang, Yandong Guo, Younghyun Jo, Sejong Yang, Seon Joo\n  Kim, Lin Zha, Jiande Jiang, Xinbo Gao, Wen Lu, Jing Liu, Kwangjin Yoon,\n  Taegyun Jeon, Kazutoshi Akita, Takeru Ooba, Norimichi Ukita, Zhipeng Luo,\n  Yuehan Yao, Zhenyu Xu, Dongliang He, Wenhao Wu, Yukang Ding, Chao Li, Fu Li,\n  Shilei Wen, Jianwei Li, Fuzhi Yang, Huan Yang, Jianlong Fu, Byung-Hoon Kim,\n  JaeHyun Baek, Jong Chul Ye, Yuchen Fan, Thomas S. Huang, Junyeop Lee,\n  Bokyeung Lee, Jungki Min, Gwantae Kim, Kanghyu Lee, Jaihyun Park, Mykola\n  Mykhailych, Haoyu Zhong, Yukai Shi, Xiaojun Yang, Zhijing Yang, Liang Lin,\n  Tongtong Zhao, Jinjia Peng, Huibing Wang, Zhi Jin, Jiahao Wu, Yifu Chen,\n  Chenming Shang, Huanrong Zhang, Jeongki Min, Hrishikesh P S, Densen\n  Puthussery, Jiji C V", "title": "NTIRE 2020 Challenge on Perceptual Extreme Super-Resolution: Methods and\n  Results", "comments": "CVPRW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the NTIRE 2020 challenge on perceptual extreme\nsuper-resolution with focus on proposed solutions and results. The challenge\ntask was to super-resolve an input image with a magnification factor 16 based\non a set of prior examples of low and corresponding high resolution images. The\ngoal is to obtain a network design capable to produce high resolution results\nwith the best perceptual quality and similar to the ground truth. The track had\n280 registered participants, and 19 teams submitted the final results. They\ngauge the state-of-the-art in single image super-resolution.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 11:30:51 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zhang", "Kai", ""], ["Gu", "Shuhang", ""], ["Timofte", "Radu", ""], ["Shang", "Taizhang", ""], ["Dai", "Qiuju", ""], ["Zhu", "Shengchen", ""], ["Yang", "Tong", ""], ["Guo", "Yandong", ""], ["Jo", "Younghyun", ""], ["Yang", "Sejong", ""], ["Kim", "Seon Joo", ""], ["Zha", "Lin", ""], ["Jiang", "Jiande", ""], ["Gao", "Xinbo", ""], ["Lu", "Wen", ""], ["Liu", "Jing", ""], ["Yoon", "Kwangjin", ""], ["Jeon", "Taegyun", ""], ["Akita", "Kazutoshi", ""], ["Ooba", "Takeru", ""], ["Ukita", "Norimichi", ""], ["Luo", "Zhipeng", ""], ["Yao", "Yuehan", ""], ["Xu", "Zhenyu", ""], ["He", "Dongliang", ""], ["Wu", "Wenhao", ""], ["Ding", "Yukang", ""], ["Li", "Chao", ""], ["Li", "Fu", ""], ["Wen", "Shilei", ""], ["Li", "Jianwei", ""], ["Yang", "Fuzhi", ""], ["Yang", "Huan", ""], ["Fu", "Jianlong", ""], ["Kim", "Byung-Hoon", ""], ["Baek", "JaeHyun", ""], ["Ye", "Jong Chul", ""], ["Fan", "Yuchen", ""], ["Huang", "Thomas S.", ""], ["Lee", "Junyeop", ""], ["Lee", "Bokyeung", ""], ["Min", "Jungki", ""], ["Kim", "Gwantae", ""], ["Lee", "Kanghyu", ""], ["Park", "Jaihyun", ""], ["Mykhailych", "Mykola", ""], ["Zhong", "Haoyu", ""], ["Shi", "Yukai", ""], ["Yang", "Xiaojun", ""], ["Yang", "Zhijing", ""], ["Lin", "Liang", ""], ["Zhao", "Tongtong", ""], ["Peng", "Jinjia", ""], ["Wang", "Huibing", ""], ["Jin", "Zhi", ""], ["Wu", "Jiahao", ""], ["Chen", "Yifu", ""], ["Shang", "Chenming", ""], ["Zhang", "Huanrong", ""], ["Min", "Jeongki", ""], ["S", "Hrishikesh P", ""], ["Puthussery", "Densen", ""], ["C", "Jiji", "V"]]}, {"id": "2005.01091", "submitter": "Abhijith Punnappurath", "authors": "Abhijith Punnappurath and Michael S. Brown", "title": "A Little Bit More: Bitplane-Wise Bit-Depth Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging sensors digitize incoming scene light at a dynamic range of 10--12\nbits (i.e., 1024--4096 tonal values). The sensor image is then processed\nonboard the camera and finally quantized to only 8 bits (i.e., 256 tonal\nvalues) to conform to prevailing encoding standards. There are a number of\nimportant applications, such as high-bit-depth displays and photo editing,\nwhere it is beneficial to recover the lost bit depth. Deep neural networks are\neffective at this bit-depth reconstruction task. Given the quantized\nlow-bit-depth image as input, existing deep learning methods employ a\nsingle-shot approach that attempts to either (1) directly estimate the\nhigh-bit-depth image, or (2) directly estimate the residual between the high-\nand low-bit-depth images. In contrast, we propose a training and inference\nstrategy that recovers the residual image bitplane-by-bitplane. Our\nbitplane-wise learning framework has the advantage of allowing for multiple\nlevels of supervision during training and is able to obtain state-of-the-art\nresults using a simple network architecture. We test our proposed method\nextensively on several image datasets and demonstrate an improvement from 0.5dB\nto 2.3dB PSNR over prior methods depending on the quantization level.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 14:06:33 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Punnappurath", "Abhijith", ""], ["Brown", "Michael S.", ""]]}, {"id": "2005.01094", "submitter": "Gong Cheng", "authors": "Gong Cheng, Xingxing Xie, Junwei Han, Lei Guo, Gui-Song Xia", "title": "Remote Sensing Image Scene Classification Meets Deep Learning:\n  Challenges, Methods, Benchmarks, and Opportunities", "comments": "This manuscript is the accepted version for IEEE JSTARS", "journal-ref": "IEEE Journal of Selected Topics in Applied Earth Observations and\n  Remote Sensing, 13: 3735-3756, 2020", "doi": "10.1109/JSTARS.2020.3005403", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Remote sensing image scene classification, which aims at labeling remote\nsensing images with a set of semantic categories based on their contents, has\nbroad applications in a range of fields. Propelled by the powerful feature\nlearning capabilities of deep neural networks, remote sensing image scene\nclassification driven by deep learning has drawn remarkable attention and\nachieved significant breakthroughs. However, to the best of our knowledge, a\ncomprehensive review of recent achievements regarding deep learning for scene\nclassification of remote sensing images is still lacking. Considering the rapid\nevolution of this field, this paper provides a systematic survey of deep\nlearning methods for remote sensing image scene classification by covering more\nthan 160 papers. To be specific, we discuss the main challenges of remote\nsensing image scene classification and survey (1) Autoencoder-based remote\nsensing image scene classification methods, (2) Convolutional Neural\nNetwork-based remote sensing image scene classification methods, and (3)\nGenerative Adversarial Network-based remote sensing image scene classification\nmethods. In addition, we introduce the benchmarks used for remote sensing image\nscene classification and summarize the performance of more than two dozen of\nrepresentative algorithms on three commonly-used benchmark data sets. Finally,\nwe discuss the promising opportunities for further research.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 14:18:00 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 09:58:28 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Cheng", "Gong", ""], ["Xie", "Xingxing", ""], ["Han", "Junwei", ""], ["Guo", "Lei", ""], ["Xia", "Gui-Song", ""]]}, {"id": "2005.01115", "submitter": "Weiya Fan", "authors": "Weiya Fan", "title": "Deep Encoder-Decoder Neural Network for Fingerprint Image Denoising and\n  Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint image denoising is a very important step in fingerprint\nidentification. to improve the denoising effect of fingerprint image,we have\ndesigns a fingerprint denoising algorithm based on deep encoder-decoder\nnetwork,which encoder subnet to learn the fingerprint features of noisy\nimages.the decoder subnet reconstructs the original fingerprint image based on\nthe features to achieve denoising, while using the dilated convolution in the\nnetwork to increase the receptor field without increasing the complexity and\nimprove the network inference speed. In addition, feature fusion at different\nlevels of the network is achieved through the introduction of residual\nlearning, which further restores the detailed features of the fingerprint and\nimproves the denoising effect. Finally, the experimental results show that the\nalgorithm enables better recovery of edge, line and curve features in\nfingerprint images, with better visual effects and higher peak signal-to-noise\nratio (PSNR) compared to other methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 15:24:22 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Fan", "Weiya", ""]]}, {"id": "2005.01116", "submitter": "Xingchen Zhang", "authors": "Xingchen Zhang", "title": "Multi-focus Image Fusion: A Benchmark", "comments": "12 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-focus image fusion (MFIF) has attracted considerable interests due to\nits numerous applications. While much progress has been made in recent years\nwith efforts on developing various MFIF algorithms, some issues significantly\nhinder the fair and comprehensive performance comparison of MFIF methods, such\nas the lack of large-scale test set and the random choices of objective\nevaluation metrics in the literature. To solve these issues, this paper\npresents a multi-focus image fusion benchmark (MFIFB) which consists a test set\nof 105 image pairs, a code library of 30 MFIF algorithms, and 20 evaluation\nmetrics. MFIFB is the first benchmark in the field of MFIF and provides the\ncommunity a platform to compare MFIF algorithms fairly and comprehensively.\nExtensive experiments have been conducted using the proposed MFIFB to\nunderstand the performance of these algorithms. By analyzing the experimental\nresults, effective MFIF algorithms are identified. More importantly, some\nobservations on the status of the MFIF field are given, which can help to\nunderstand this field better.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 15:25:43 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zhang", "Xingchen", ""]]}, {"id": "2005.01123", "submitter": "Liangjian Wen PhD.", "authors": "Liangjian Wen, Yiji Zhou, Lirong He, Mingyuan Zhou, Zenglin Xu", "title": "Mutual Information Gradient Estimation for Representation Learning", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual Information (MI) plays an important role in representation learning.\nHowever, MI is unfortunately intractable in continuous and high-dimensional\nsettings. Recent advances establish tractable and scalable MI estimators to\ndiscover useful representation. However, most of the existing methods are not\ncapable of providing an accurate estimation of MI with low-variance when the MI\nis large. We argue that directly estimating the gradients of MI is more\nappealing for representation learning than estimating MI in itself. To this\nend, we propose the Mutual Information Gradient Estimator (MIGE) for\nrepresentation learning based on the score estimation of implicit\ndistributions. MIGE exhibits a tight and smooth gradient estimation of MI in\nthe high-dimensional and large-MI settings. We expand the applications of MIGE\nin both unsupervised learning of deep representations based on InfoMax and the\nInformation Bottleneck method. Experimental results have indicated significant\nperformance improvement in learning useful representation.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 16:05:58 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wen", "Liangjian", ""], ["Zhou", "Yiji", ""], ["He", "Lirong", ""], ["Zhou", "Mingyuan", ""], ["Xu", "Zenglin", ""]]}, {"id": "2005.01178", "submitter": "Yu Chen", "authors": "Meng Yuan, Seyed Yahya Nikouei, Alem Fitwi, Yu Chen, Yunxi Dong", "title": "Minor Privacy Protection Through Real-time Video Processing at the Edge", "comments": "Accepted by the 2nd International Workshop on Smart City\n  Communication and Networking at the ICCCN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collection of a lot of personal information about individuals, including\nthe minor members of a family, by closed-circuit television (CCTV) cameras\ncreates a lot of privacy concerns. Particularly, revealing children's\nidentifications or activities may compromise their well-being. In this paper,\nwe investigate lightweight solutions that are affordable to edge surveillance\nsystems, which is made feasible and accurate to identify minors such that\nappropriate privacy-preserving measures can be applied accordingly. State of\nthe art deep learning architectures are modified and re-purposed in a cascaded\nfashion to maximize the accuracy of our model. A pipeline extracts faces from\nthe input frames and classifies each one to be of an adult or a child. Over\n20,000 labeled sample points are used for classification. We explore the timing\nand resources needed for such a model to be used in the Edge-Fog architecture\nat the edge of the network, where we can achieve near real-time performance on\nthe CPU. Quantitative experimental results show the superiority of our proposed\nmodel with an accuracy of 92.1% in classification compared to some other face\nrecognition based child detection approaches.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 20:19:15 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Yuan", "Meng", ""], ["Nikouei", "Seyed Yahya", ""], ["Fitwi", "Alem", ""], ["Chen", "Yu", ""], ["Dong", "Yunxi", ""]]}, {"id": "2005.01233", "submitter": "Seungjun Nah", "authors": "Seungjun Nah, Sanghyun Son, Radu Timofte and Kyoung Mu Lee", "title": "AIM 2019 Challenge on Video Temporal Super-Resolution: Methods and\n  Results", "comments": "Published in ICCV 2019 Workshop (Advances in Image Manipulation)", "journal-ref": "2019 IEEE/CVF International Conference on Computer Vision Workshop\n  (ICCVW), Seoul, Korea (South), 2019, pp. 3388-3398", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos contain various types and strengths of motions that may look\nunnaturally discontinuous in time when the recorded frame rate is low. This\npaper reviews the first AIM challenge on video temporal super-resolution (frame\ninterpolation) with a focus on the proposed solutions and results. From\nlow-frame-rate (15 fps) video sequences, the challenge participants are asked\nto submit higher-framerate (60 fps) video sequences by estimating temporally\nintermediate frames. We employ the REDS VTSR dataset derived from diverse\nvideos captured in a hand-held camera for training and evaluation purposes. The\ncompetition had 62 registered participants, and a total of 8 teams competed in\nthe final testing phase. The challenge winning methods achieve the\nstate-of-the-art in video temporal superresolution.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 01:51:23 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Nah", "Seungjun", ""], ["Son", "Sanghyun", ""], ["Timofte", "Radu", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2005.01234", "submitter": "Wanqi Xue", "authors": "Wanqi Xue, Wei Wang", "title": "One-Shot Image Classification by Learning to Restore Prototypes", "comments": "Published as a conference paper in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot image classification aims to train image classifiers over the\ndataset with only one image per category. It is challenging for modern deep\nneural networks that typically require hundreds or thousands of images per\nclass. In this paper, we adopt metric learning for this problem, which has been\napplied for few- and many-shot image classification by comparing the distance\nbetween the test image and the center of each class in the feature space.\nHowever, for one-shot learning, the existing metric learning approaches would\nsuffer poor performance because the single training image may not be\nrepresentative of the class. For example, if the image is far away from the\nclass center in the feature space, the metric-learning based algorithms are\nunlikely to make correct predictions for the test images because the decision\nboundary is shifted by this noisy image. To address this issue, we propose a\nsimple yet effective regression model, denoted by RestoreNet, which learns a\nclass agnostic transformation on the image feature to move the image closer to\nthe class center in the feature space. Experiments demonstrate that RestoreNet\nobtains superior performance over the state-of-the-art methods on a broad range\nof datasets. Moreover, RestoreNet can be easily combined with other methods to\nachieve further improvement.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 02:11:30 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Xue", "Wanqi", ""], ["Wang", "Wei", ""]]}, {"id": "2005.01239", "submitter": "Violetta Shevchenko", "authors": "Violetta Shevchenko, Damien Teney, Anthony Dick, Anton van den Hengel", "title": "Visual Question Answering with Prior Class Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel mechanism to embed prior knowledge in a model for visual\nquestion answering. The open-set nature of the task is at odds with the\nubiquitous approach of training of a fixed classifier. We show how to exploit\nadditional information pertaining to the semantics of candidate answers. We\nextend the answer prediction process with a regression objective in a semantic\nspace, in which we project candidate answers using prior knowledge derived from\nword embeddings. We perform an extensive study of learned representations with\nthe GQA dataset, revealing that important semantic information is captured in\nthe relations between embeddings in the answer space. Our method brings\nimprovements in consistency and accuracy over a range of question types.\nExperiments with novel answers, unseen during training, indicate the method's\npotential for open-set prediction.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 02:46:31 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Shevchenko", "Violetta", ""], ["Teney", "Damien", ""], ["Dick", "Anthony", ""], ["Hengel", "Anton van den", ""]]}, {"id": "2005.01244", "submitter": "Seungjun Nah", "authors": "Seungjun Nah, Sanghyun Son, Radu Timofte and Kyoung Mu Lee", "title": "NTIRE 2020 Challenge on Image and Video Deblurring", "comments": "To be published in CVPR 2020 Workshop (New Trends in Image\n  Restoration and Enhancement)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion blur is one of the most common degradation artifacts in dynamic scene\nphotography. This paper reviews the NTIRE 2020 Challenge on Image and Video\nDeblurring. In this challenge, we present the evaluation results from 3\ncompetition tracks as well as the proposed solutions. Track 1 aims to develop\nsingle-image deblurring methods focusing on restoration quality. On Track 2,\nthe image deblurring methods are executed on a mobile platform to find the\nbalance of the running speed and the restoration accuracy. Track 3 targets\ndeveloping video deblurring methods that exploit the temporal relation between\ninput frames. In each competition, there were 163, 135, and 102 registered\nparticipants and in the final testing phase, 9, 4, and 7 teams competed. The\nwinning methods demonstrate the state-ofthe-art performance on image and video\ndeblurring tasks.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 03:17:30 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 03:39:13 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Nah", "Seungjun", ""], ["Son", "Sanghyun", ""], ["Timofte", "Radu", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "2005.01333", "submitter": "Hong Wang", "authors": "Hong Wang, Qi Xie, Qian Zhao, Deyu Meng", "title": "A Model-driven Deep Neural Network for Single Image Rain Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) methods have achieved state-of-the-art performance in the\ntask of single image rain removal. Most of current DL architectures, however,\nare still lack of sufficient interpretability and not fully integrated with\nphysical structures inside general rain streaks. To this issue, in this paper,\nwe propose a model-driven deep neural network for the task, with fully\ninterpretable network structures. Specifically, based on the convolutional\ndictionary learning mechanism for representing rain, we propose a novel single\nimage deraining model and utilize the proximal gradient descent technique to\ndesign an iterative algorithm only containing simple operators for solving the\nmodel. Such a simple implementation scheme facilitates us to unfold it into a\nnew deep network architecture, called rain convolutional dictionary network\n(RCDNet), with almost every network module one-to-one corresponding to each\noperation involved in the algorithm. By end-to-end training the proposed\nRCDNet, all the rain kernels and proximal operators can be automatically\nextracted, faithfully characterizing the features of both rain and clean\nbackground layers, and thus naturally lead to its better deraining performance,\nespecially in real scenarios. Comprehensive experiments substantiate the\nsuperiority of the proposed network, especially its well generality to diverse\ntesting scenarios and good interpretability for all its modules, as compared\nwith state-of-the-arts both visually and quantitatively. The source codes are\navailable at \\url{https://github.com/hongwang01/RCDNet}.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 09:13:25 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wang", "Hong", ""], ["Xie", "Qi", ""], ["Zhao", "Qian", ""], ["Meng", "Deyu", ""]]}, {"id": "2005.01338", "submitter": "Keyan Ding", "authors": "Keyan Ding, Kede Ma, Shiqi Wang, Eero P. Simoncelli", "title": "Comparison of Image Quality Models for Optimization of Image Processing\n  Systems", "comments": null, "journal-ref": "International Journal of Computer Vision, 2021", "doi": "10.1007/s11263-020-01419-7", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of objective image quality assessment (IQA) models has been\nevaluated primarily by comparing model predictions to human quality judgments.\nPerceptual datasets gathered for this purpose have provided useful benchmarks\nfor improving IQA methods, but their heavy use creates a risk of overfitting.\nHere, we perform a large-scale comparison of IQA models in terms of their use\nas objectives for the optimization of image processing algorithms.\nSpecifically, we use eleven full-reference IQA models to train deep neural\nnetworks for four low-level vision tasks: denoising, deblurring,\nsuper-resolution, and compression. Subjective testing on the optimized images\nallows us to rank the competing models in terms of their perceptual\nperformance, elucidate their relative advantages and disadvantages in these\ntasks, and propose a set of desirable properties for incorporation into future\nIQA models.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 09:26:40 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 05:35:07 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 12:59:48 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Ding", "Keyan", ""], ["Ma", "Kede", ""], ["Wang", "Shiqi", ""], ["Simoncelli", "Eero P.", ""]]}, {"id": "2005.01344", "submitter": "Xi Li", "authors": "Junyi Feng, Songyuan Li, Yifeng Chen, Fuxian Huang, Jiabao Cui, and Xi\n  Li", "title": "How to Train Your Dragon: Tamed Warping Network for Semantic Video\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time semantic segmentation on high-resolution videos is challenging due\nto the strict requirements of speed. Recent approaches have utilized the\ninter-frame continuity to reduce redundant computation by warping the feature\nmaps across adjacent frames, greatly speeding up the inference phase. However,\ntheir accuracy drops significantly owing to the imprecise motion estimation and\nerror accumulation. In this paper, we propose to introduce a simple and\neffective correction stage right after the warping stage to form a framework\nnamed Tamed Warping Network (TWNet), aiming to improve the accuracy and\nrobustness of warping-based models. The experimental results on the Cityscapes\ndataset show that with the correction, the accuracy (mIoU) significantly\nincreases from 67.3% to 71.6%, and the speed edges down from 65.5 FPS to 61.8\nFPS. For non-rigid categories such as \"human\" and \"object\", the improvements of\nIoU are even higher than 18 percentage points.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 09:36:03 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 05:03:08 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Feng", "Junyi", ""], ["Li", "Songyuan", ""], ["Chen", "Yifeng", ""], ["Huang", "Fuxian", ""], ["Cui", "Jiabao", ""], ["Li", "Xi", ""]]}, {"id": "2005.01351", "submitter": "Purnendu Mishra", "authors": "Purnendu Mishra and Kishor Sarawadekar", "title": "Anchors Based Method for Fingertips Position Estimation from a Monocular\n  RGB Image using Deep Neural Network", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Virtual, augmented, and mixed reality, the use of hand gestures is\nincreasingly becoming popular to reduce the difference between the virtual and\nreal world. The precise location of the fingertip is essential/crucial for a\nseamless experience. Much of the research work is based on using depth\ninformation for the estimation of the fingertips position. However, most of the\nwork using RGB images for fingertips detection is limited to a single finger.\nThe detection of multiple fingertips from a single RGB image is very\nchallenging due to various factors. In this paper, we propose a deep neural\nnetwork (DNN) based methodology to estimate the fingertips position. We\nchristened this methodology as an Anchor based Fingertips Position Estimation\n(ABFPE), and it is a two-step process. The fingertips location is estimated\nusing regression by computing the difference in the location of a fingertip\nfrom the nearest anchor point. The proposed framework performs the best with\nlimited dependence on hand detection results. In our experiments on the\nSCUT-Ego-Gesture dataset, we achieved the fingertips detection error of 2.3552\npixels on a video frame with a resolution of $640 \\times 480$ and about\n$92.98\\%$ of test images have average pixel errors of five pixels.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 09:45:56 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 06:57:58 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Mishra", "Purnendu", ""], ["Sarawadekar", "Kishor", ""]]}, {"id": "2005.01385", "submitter": "Narinder Punn", "authors": "Narinder Singh Punn, Sanjay Kumar Sonbhadra, Sonali Agarwal, Gaurav\n  Rai", "title": "Monitoring COVID-19 social distancing with person detection and tracking\n  via fine-tuned YOLO v3 and Deepsort techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rampant coronavirus disease 2019 (COVID-19) has brought global crisis\nwith its deadly spread to more than 180 countries, and about 3,519,901\nconfirmed cases along with 247,630 deaths globally as on May 4, 2020. The\nabsence of any active therapeutic agents and the lack of immunity against\nCOVID-19 increases the vulnerability of the population. Since there are no\nvaccines available, social distancing is the only feasible approach to fight\nagainst this pandemic. Motivated by this notion, this article proposes a deep\nlearning based framework for automating the task of monitoring social\ndistancing using surveillance video. The proposed framework utilizes the YOLO\nv3 object detection model to segregate humans from the background and Deepsort\napproach to track the identified people with the help of bounding boxes and\nassigned IDs. The results of the YOLO v3 model are further compared with other\npopular state-of-the-art models, e.g. faster region-based CNN (convolution\nneural network) and single shot detector (SSD) in terms of mean average\nprecision (mAP), frames per second (FPS) and loss values defined by object\nclassification and localization. Later, the pairwise vectorized L2 norm is\ncomputed based on the three-dimensional feature space obtained by using the\ncentroid coordinates and dimensions of the bounding box. The violation index\nterm is proposed to quantize the non adoption of social distancing protocol.\nFrom the experimental analysis, it is observed that the YOLO v3 with Deepsort\ntracking scheme displayed best results with balanced mAP and FPS score to\nmonitor the social distancing in real-time.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 10:58:20 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 17:47:49 GMT"}, {"version": "v3", "created": "Sat, 24 Apr 2021 16:15:26 GMT"}, {"version": "v4", "created": "Tue, 27 Apr 2021 05:08:05 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Punn", "Narinder Singh", ""], ["Sonbhadra", "Sanjay Kumar", ""], ["Agarwal", "Sonali", ""], ["Rai", "Gaurav", ""]]}, {"id": "2005.01400", "submitter": "Abhinav Shukla", "authors": "Abhinav Shukla, Stavros Petridis, Maja Pantic", "title": "Does Visual Self-Supervision Improve Learning of Speech Representations\n  for Emotion Recognition?", "comments": "Accepted for publication in IEEE Transactions on Affective Computing;\n  v3: Publication-ready version including additional experiments and discussion", "journal-ref": null, "doi": "10.1109/TAFFC.2021.3062406", "report-no": null, "categories": "eess.AS cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning has attracted plenty of recent research interest.\nHowever, most works for self-supervision in speech are typically unimodal and\nthere has been limited work that studies the interaction between audio and\nvisual modalities for cross-modal self-supervision. This work (1) investigates\nvisual self-supervision via face reconstruction to guide the learning of audio\nrepresentations; (2) proposes an audio-only self-supervision approach for\nspeech representation learning; (3) shows that a multi-task combination of the\nproposed visual and audio self-supervision is beneficial for learning richer\nfeatures that are more robust in noisy conditions; (4) shows that\nself-supervised pretraining can outperform fully supervised training and is\nespecially useful to prevent overfitting on smaller sized datasets. We evaluate\nour learned audio representations for discrete emotion recognition, continuous\naffect recognition and automatic speech recognition. We outperform existing\nself-supervised methods for all tested downstream tasks. Our results\ndemonstrate the potential of visual self-supervision for audio feature learning\nand suggest that joint visual and audio self-supervision leads to more\ninformative audio representations for speech and emotion recognition.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 11:33:40 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 16:46:13 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 11:35:38 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Shukla", "Abhinav", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "2005.01431", "submitter": "Ziwei Zhang", "authors": "Ziwei Zhang, Chi Su, Liang Zheng, Xiaodong Xie", "title": "Correlating Edge, Pose with Parsing", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to existing studies, human body edge and pose are two beneficial\nfactors to human parsing. The effectiveness of each of the high-level features\n(edge and pose) is confirmed through the concatenation of their features with\nthe parsing features. Driven by the insights, this paper studies how human\nsemantic boundaries and keypoint locations can jointly improve human parsing.\nCompared with the existing practice of feature concatenation, we find that\nuncovering the correlation among the three factors is a superior way of\nleveraging the pivotal contextual cues provided by edges and poses. To capture\nsuch correlations, we propose a Correlation Parsing Machine (CorrPM) employing\na heterogeneous non-local block to discover the spatial affinity among feature\nmaps from the edge, pose and parsing. The proposed CorrPM allows us to report\nnew state-of-the-art accuracy on three human parsing datasets. Importantly,\ncomparative studies confirm the advantages of feature correlation over the\nconcatenation.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 12:39:13 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zhang", "Ziwei", ""], ["Su", "Chi", ""], ["Zheng", "Liang", ""], ["Xie", "Xiaodong", ""]]}, {"id": "2005.01433", "submitter": "Masahiro Oda Dr.", "authors": "Masahiro Oda, Takefumi Yamaguchi, Hideki Fukuoka, Yuta Ueno, Kensaku\n  Mori", "title": "Automated eye disease classification method from anterior eye image\n  using anatomical structure focused image classification technique", "comments": "Accepted paper as a poster presentation at SPIE Medical Imaging 2020,\n  Houston, TX, USA", "journal-ref": "Proceedings of SPIE Medical Imaging 2020: Computer-Aided\n  Diagnosis, Vol.11314, 1131446", "doi": "10.1117/12.2549951", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an automated classification method of infective and\nnon-infective diseases from anterior eye images. Treatments for cases of\ninfective and non-infective diseases are different. Distinguishing them from\nanterior eye images is important to decide a treatment plan. Ophthalmologists\ndistinguish them empirically. Quantitative classification of them based on\ncomputer assistance is necessary. We propose an automated classification method\nof anterior eye images into cases of infective or non-infective disease.\nAnterior eye images have large variations of the eye position and brightness of\nillumination. This makes the classification difficult. If we focus on the\ncornea, positions of opacified areas in the corneas are different between cases\nof the infective and non-infective diseases. Therefore, we solve the anterior\neye image classification task by using an object detection approach targeting\nthe cornea. This approach can be said as \"anatomical structure focused image\nclassification\". We use the YOLOv3 object detection method to detect corneas of\ninfective disease and corneas of non-infective disease. The detection result is\nused to define a classification result of a image. In our experiments using\nanterior eye images, 88.3% of images were correctly classified by the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 12:42:54 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Oda", "Masahiro", ""], ["Yamaguchi", "Takefumi", ""], ["Fukuoka", "Hideki", ""], ["Ueno", "Yuta", ""], ["Mori", "Kensaku", ""]]}, {"id": "2005.01449", "submitter": "Chun-Guang Li", "authors": "Ying Chen, Chun-Guang Li, and Chong You", "title": "Stochastic Sparse Subspace Clustering", "comments": "16 pages, 9 figures and 8 tables. This work is accepted by IEEE\n  Conference on Computer Vision and Pattern Recognition (CVPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art subspace clustering methods are based on self-expressive\nmodel, which represents each data point as a linear combination of other data\npoints. By enforcing such representation to be sparse, sparse subspace\nclustering is guaranteed to produce a subspace-preserving data affinity where\ntwo points are connected only if they are from the same subspace. On the other\nhand, however, data points from the same subspace may not be well-connected,\nleading to the issue of over-segmentation. We introduce dropout to address the\nissue of over-segmentation, which is based on randomly dropping out data points\nin self-expressive model. In particular, we show that dropout is equivalent to\nadding a squared $\\ell_2$ norm regularization on the representation\ncoefficients, therefore induces denser solutions. Then, we reformulate the\noptimization problem as a consensus problem over a set of small-scale\nsubproblems. This leads to a scalable and flexible sparse subspace clustering\napproach, termed Stochastic Sparse Subspace Clustering, which can effectively\nhandle large scale datasets. Extensive experiments on synthetic data and real\nworld datasets validate the efficiency and effectiveness of our proposal.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 13:09:17 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Chen", "Ying", ""], ["Li", "Chun-Guang", ""], ["You", "Chong", ""]]}, {"id": "2005.01456", "submitter": "Arthur Ouaknine", "authors": "A. Ouaknine, A. Newson, J. Rebut, F. Tupin and P. P\\'erez", "title": "CARRADA Dataset: Camera and Automotive Radar with Range-Angle-Doppler\n  Annotations", "comments": "9 pages, 5 figues. Accepted at ICPR 2020. Erratum: results in Table\n  III have been updated since the ICPR proceedings, models are selected using\n  the PP metric instead of the previously used PR metric", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High quality perception is essential for autonomous driving (AD) systems. To\nreach the accuracy and robustness that are required by such systems, several\ntypes of sensors must be combined. Currently, mostly cameras and laser scanners\n(lidar) are deployed to build a representation of the world around the vehicle.\nWhile radar sensors have been used for a long time in the automotive industry,\nthey are still under-used for AD despite their appealing characteristics\n(notably, their ability to measure the relative speed of obstacles and to\noperate even in adverse weather conditions). To a large extent, this situation\nis due to the relative lack of automotive datasets with real radar signals that\nare both raw and annotated. In this work, we introduce CARRADA, a dataset of\nsynchronized camera and radar recordings with range-angle-Doppler annotations.\nWe also present a semi-automatic annotation approach, which was used to\nannotate the dataset, and a radar semantic segmentation baseline, which we\nevaluate on several metrics. Both our code and dataset are available online.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 13:14:29 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 13:12:01 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 18:12:15 GMT"}, {"version": "v4", "created": "Fri, 21 May 2021 12:36:17 GMT"}, {"version": "v5", "created": "Tue, 25 May 2021 17:04:05 GMT"}, {"version": "v6", "created": "Wed, 26 May 2021 13:52:09 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Ouaknine", "A.", ""], ["Newson", "A.", ""], ["Rebut", "J.", ""], ["Tupin", "F.", ""], ["P\u00e9rez", "P.", ""]]}, {"id": "2005.01467", "submitter": "Maxence Bouvier", "authors": "Maxence Bouvier, Alexandre Valentian, Thomas Mesquida, Fran\\c{c}ois\n  Rummens, Marina Reyboz, Elisa Vianello, Edith Beign\\'e", "title": "Spiking Neural Networks Hardware Implementations and Challenges: a\n  Survey", "comments": "Pre-print version of the file authorized for publication", "journal-ref": null, "doi": "10.1145/3304103", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing is henceforth a major research field for both academic\nand industrial actors. As opposed to Von Neumann machines, brain-inspired\nprocessors aim at bringing closer the memory and the computational elements to\nefficiently evaluate machine-learning algorithms. Recently, Spiking Neural\nNetworks, a generation of cognitive algorithms employing computational\nprimitives mimicking neuron and synapse operational principles, have become an\nimportant part of deep learning. They are expected to improve the computational\nperformance and efficiency of neural networks, but are best suited for hardware\nable to support their temporal dynamics. In this survey, we present the state\nof the art of hardware implementations of spiking neural networks and the\ncurrent trends in algorithm elaboration from model selection to training\nmechanisms. The scope of existing solutions is extensive; we thus present the\ngeneral framework and study on a case-by-case basis the relevant\nparticularities. We describe the strategies employed to leverage the\ncharacteristics of these event-driven algorithms at the hardware level and\ndiscuss their related advantages and challenges.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 13:24:00 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Bouvier", "Maxence", ""], ["Valentian", "Alexandre", ""], ["Mesquida", "Thomas", ""], ["Rummens", "Fran\u00e7ois", ""], ["Reyboz", "Marina", ""], ["Vianello", "Elisa", ""], ["Beign\u00e9", "Edith", ""]]}, {"id": "2005.01468", "submitter": "Dailin Lv", "authors": "Dailin Lv, Wuteng Qi, Yunxiang Li, Lingling Sun, Yaqi Wang", "title": "A cascade network for Detecting COVID-19 using chest x-rays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The worldwide spread of pneumonia caused by a novel coronavirus poses an\nunprecedented challenge to the world's medical resources and prevention and\ncontrol measures. Covid-19 attacks not only the lungs, making it difficult to\nbreathe and life-threatening, but also the heart, kidneys, brain and other\nvital organs of the body, with possible sequela. At present, the detection of\nCOVID-19 needs to be realized by the reverse transcription-polymerase Chain\nReaction (RT-PCR). However, many countries are in the outbreak period of the\nepidemic, and the medical resources are very limited. They cannot provide\nsufficient numbers of gene sequence detection, and many patients may not be\nisolated and treated in time. Given this situation, we researched the\nanalytical and diagnostic capabilities of deep learning on chest radiographs\nand proposed Cascade-SEMEnet which is cascaded with SEME-ResNet50 and\nSEME-DenseNet169. The two cascade networks of Cascade - SEMEnet both adopt\nlarge input sizes and SE-Structure and use MoEx and histogram equalization to\nenhance the data. We first used SEME-ResNet50 to screen chest X-ray and\ndiagnosed three classes: normal, bacterial, and viral pneumonia. Then we used\nSEME-DenseNet169 for fine-grained classification of viral pneumonia and\ndetermined if it is caused by COVID-19. To exclude the influence of\nnon-pathological features on the network, we preprocessed the data with U-Net\nduring the training of SEME-DenseNet169. The results showed that our network\nachieved an accuracy of 85.6\\% in determining the type of pneumonia infection\nand 97.1\\% in the fine-grained classification of COVID-19. We used Grad-CAM to\nvisualize the judgment based on the model and help doctors understand the chest\nradiograph while verifying the effectivene.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 09:56:56 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Lv", "Dailin", ""], ["Qi", "Wuteng", ""], ["Li", "Yunxiang", ""], ["Sun", "Lingling", ""], ["Wang", "Yaqi", ""]]}, {"id": "2005.01499", "submitter": "Nupur Kumari", "authors": "Gunjan Aggarwal, Abhishek Sinha, Nupur Kumari, Mayank Singh", "title": "On the Benefits of Models with Perceptually-Aligned Gradients", "comments": "Accepted at ICLR 2020 Workshop: Towards Trustworthy ML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial robust models have been shown to learn more robust and\ninterpretable features than standard trained models. As shown in\n[\\cite{tsipras2018robustness}], such robust models inherit useful interpretable\nproperties where the gradient aligns perceptually well with images, and adding\na large targeted adversarial perturbation leads to an image resembling the\ntarget class. We perform experiments to show that interpretable and\nperceptually aligned gradients are present even in models that do not show high\nrobustness to adversarial attacks. Specifically, we perform adversarial\ntraining with attack for different max-perturbation bound. Adversarial training\nwith low max-perturbation bound results in models that have interpretable\nfeatures with only slight drop in performance over clean samples. In this\npaper, we leverage models with interpretable perceptually-aligned features and\nshow that adversarial training with low max-perturbation bound can improve the\nperformance of models for zero-shot and weakly supervised localization tasks.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 14:05:38 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Aggarwal", "Gunjan", ""], ["Sinha", "Abhishek", ""], ["Kumari", "Nupur", ""], ["Singh", "Mayank", ""]]}, {"id": "2005.01508", "submitter": "Safa Messaoud", "authors": "Safa Messaoud, Maghav Kumar, and Alexander G. Schwing", "title": "Can We Learn Heuristics For Graphical Model Inference Using\n  Reinforcement Learning?", "comments": "CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial optimization is frequently used in computer vision. For\ninstance, in applications like semantic segmentation, human pose estimation and\naction recognition, programs are formulated for solving inference in\nConditional Random Fields (CRFs) to produce a structured output that is\nconsistent with visual features of the image. However, solving inference in\nCRFs is in general intractable, and approximation methods are computationally\ndemanding and limited to unary, pairwise and hand-crafted forms of higher order\npotentials. In this paper, we show that we can learn program heuristics, i.e.,\npolicies, for solving inference in higher order CRFs for the task of semantic\nsegmentation, using reinforcement learning. Our method solves inference tasks\nefficiently without imposing any constraints on the form of the potentials. We\nshow compelling results on the Pascal VOC and MOTS datasets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 19:24:04 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 02:20:13 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Messaoud", "Safa", ""], ["Kumar", "Maghav", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "2005.01509", "submitter": "Dailin Lv", "authors": "Yaqi Wang, Lingling Sun, Yifang Zhang, Dailin Lv, Zhixing Li, Wuteng\n  Qi", "title": "An Adaptive Enhancement Based Hybrid CNN Model for Digital Dental X-ray\n  Positions Classification", "comments": "arXiv admin note: text overlap with arXiv:1802.03086 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of dental radiographs is an important part of the diagnostic process\nin daily clinical practice. Interpretation by an expert includes teeth\ndetection and numbering. In this project, a novel solution based on adaptive\nhistogram equalization and convolution neural network (CNN) is proposed, which\nautomatically performs the task for dental x-rays. In order to improve the\ndetection accuracy, we propose three pre-processing techniques to supplement\nthe baseline CNN based on some prior domain knowledge. Firstly, image\nsharpening and median filtering are used to remove impulse noise, and the edge\nis enhanced to some extent. Next, adaptive histogram equalization is used to\novercome the problem of excessive amplification noise of HE. Finally, a\nmulti-CNN hybrid model is proposed to classify six different locations of\ndental slices. The results showed that the accuracy and specificity of the test\nset exceeded 90\\%, and the AUC reached 0.97. In addition, four dentists were\ninvited to manually annotate the test data set (independently) and then compare\nit with the labels obtained by our proposed algorithm. The results show that\nour method can effectively identify the X-ray location of teeth.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 13:55:44 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wang", "Yaqi", ""], ["Sun", "Lingling", ""], ["Zhang", "Yifang", ""], ["Lv", "Dailin", ""], ["Li", "Zhixing", ""], ["Qi", "Wuteng", ""]]}, {"id": "2005.01577", "submitter": "Mingkui Tan", "authors": "Yifan Zhang, Shuaicheng Niu, Zhen Qiu, Ying Wei, Peilin Zhao, Jianhua\n  Yao, Junzhou Huang, Qingyao Wu, and Mingkui Tan", "title": "COVID-DA: Deep Domain Adaptation from Typical Pneumonia to COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of novel coronavirus disease 2019 (COVID-19) has already\ninfected millions of people and is still rapidly spreading all over the globe.\nMost COVID-19 patients suffer from lung infection, so one important diagnostic\nmethod is to screen chest radiography images, e.g., X-Ray or CT images.\nHowever, such examinations are time-consuming and labor-intensive, leading to\nlimited diagnostic efficiency. To solve this issue, AI-based technologies, such\nas deep learning, have been used recently as effective computer-aided means to\nimprove diagnostic efficiency. However, one practical and critical difficulty\nis the limited availability of annotated COVID-19 data, due to the prohibitive\nannotation costs and urgent work of doctors to fight against the pandemic. This\nmakes the learning of deep diagnosis models very challenging. To address this,\nmotivated by that typical pneumonia has similar characteristics with COVID-19\nand many pneumonia datasets are publicly available, we propose to conduct\ndomain knowledge adaptation from typical pneumonia to COVID-19. There are two\nmain challenges: 1) the discrepancy of data distributions between domains; 2)\nthe task difference between the diagnosis of typical pneumonia and COVID-19. To\naddress them, we propose a new deep domain adaptation method for COVID-19\ndiagnosis, namely COVID-DA. Specifically, we alleviate the domain discrepancy\nvia feature adversarial adaptation and handle the task difference issue via a\nnovel classifier separation scheme. In this way, COVID-DA is able to diagnose\nCOVID-19 effectively with only a small number of COVID-19 annotations.\nExtensive experiments verify the effectiveness of COVID-DA and its great\npotential for real-world applications.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:13:40 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zhang", "Yifan", ""], ["Niu", "Shuaicheng", ""], ["Qiu", "Zhen", ""], ["Wei", "Ying", ""], ["Zhao", "Peilin", ""], ["Yao", "Jianhua", ""], ["Huang", "Junzhou", ""], ["Wu", "Qingyao", ""], ["Tan", "Mingkui", ""]]}, {"id": "2005.01578", "submitter": "Pedro Ricardo Ariel Salvador Bassi Electrical Engineer", "authors": "Pedro R. A. S. Bassi, Romis Attux", "title": "A Deep Convolutional Neural Network for COVID-19 Detection Using Chest\n  X-Rays", "comments": null, "journal-ref": null, "doi": "10.1007/s42600-021-00132-9", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: We present image classifiers based on Dense Convolutional Networks\nand transfer learning to classify chest X-ray images according to three labels:\nCOVID-19, pneumonia and normal.\n  Methods: We fine-tuned neural networks pretrained on ImageNet and applied a\ntwice transfer learning approach, using NIH ChestX-ray14 dataset as an\nintermediate step. We also suggested a novelty called output neuron keeping,\nwhich changes the twice transfer learning technique. In order to clarify the\nmodus operandi of the models, we used Layer-wise Relevance Propagation (LRP) to\ngenerate heatmaps.\n  Results: We were able to reach test accuracy of 100% on our test dataset.\nTwice transfer learning and output neuron keeping showed promising results\nimproving performances, mainly in the beginning of the training process.\nAlthough LRP revealed that words on the X-rays can influence the networks'\npredictions, we discovered this had only a very small effect on accuracy.\n  Conclusion: Although clinical studies and larger datasets are still needed to\nfurther ensure good generalization, the state-of-the-art performances we\nachieved show that, with the help of artificial intelligence, chest X-rays can\nbecome a cheap and accurate auxiliary method for COVID-19 diagnosis. Heatmaps\ngenerated by LRP improve the interpretability of the deep neural networks and\nindicate an analytical path for future research on diagnosis. Twice transfer\nlearning with output neuron keeping improved performances.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 13:20:42 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 18:15:40 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 06:48:36 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2021 04:08:10 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Bassi", "Pedro R. A. S.", ""], ["Attux", "Romis", ""]]}, {"id": "2005.01583", "submitter": "Benjamin Lee", "authors": "Benjamin Charles Germain Lee, Jaime Mears, Eileen Jakeway, Meghan\n  Ferriter, Chris Adams, Nathan Yarasavage, Deborah Thomas, Kate Zwaard, Daniel\n  S. Weld", "title": "The Newspaper Navigator Dataset: Extracting And Analyzing Visual Content\n  from 16 Million Historic Newspaper Pages in Chronicling America", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Chronicling America is a product of the National Digital Newspaper Program, a\npartnership between the Library of Congress and the National Endowment for the\nHumanities to digitize historic newspapers. Over 16 million pages of historic\nAmerican newspapers have been digitized for Chronicling America to date,\ncomplete with high-resolution images and machine-readable METS/ALTO OCR. Of\nconsiderable interest to Chronicling America users is a semantified corpus,\ncomplete with extracted visual content and headlines. To accomplish this, we\nintroduce a visual content recognition model trained on bounding box\nannotations of photographs, illustrations, maps, comics, and editorial cartoons\ncollected as part of the Library of Congress's Beyond Words crowdsourcing\ninitiative and augmented with additional annotations including those of\nheadlines and advertisements. We describe our pipeline that utilizes this deep\nlearning model to extract 7 classes of visual content: headlines, photographs,\nillustrations, maps, comics, editorial cartoons, and advertisements, complete\nwith textual content such as captions derived from the METS/ALTO OCR, as well\nas image embeddings for fast image similarity querying. We report the results\nof running the pipeline on 16.3 million pages from the Chronicling America\ncorpus and describe the resulting Newspaper Navigator dataset, the largest\ndataset of extracted visual content from historic newspapers ever produced. The\nNewspaper Navigator dataset, finetuned visual content recognition model, and\nall source code are placed in the public domain for unrestricted re-use.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 15:51:13 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Lee", "Benjamin Charles Germain", ""], ["Mears", "Jaime", ""], ["Jakeway", "Eileen", ""], ["Ferriter", "Meghan", ""], ["Adams", "Chris", ""], ["Yarasavage", "Nathan", ""], ["Thomas", "Deborah", ""], ["Zwaard", "Kate", ""], ["Weld", "Daniel S.", ""]]}, {"id": "2005.01595", "submitter": "Simon-Martin Schr\\\"oder", "authors": "Simon-Martin Schr\\\"oder, Rainer Kiko and Reinhard Koch", "title": "MorphoCluster: Efficient Annotation of Plankton images by Clustering", "comments": "27 pages, 11 figures. Submitted to MDPI Sensors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present MorphoCluster, a software tool for data-driven, fast\nand accurate annotation of large image data sets. While already having\nsurpassed the annotation rate of human experts, volume and complexity of marine\ndata will continue to increase in the coming years. Still, this data requires\ninterpretation. MorphoCluster augments the human ability to discover patterns\nand perform object classification in large amounts of data by embedding\nunsupervised clustering in an interactive process. By aggregating similar\nimages into clusters, our novel approach to image annotation increases\nconsistency, multiplies the throughput of an annotator and allows experts to\nadapt the granularity of their sorting scheme to the structure in the data. By\nsorting a set of 1.2M objects into 280 data-driven classes in 71 hours (16k\nobjects per hour), with 90% of these classes having a precision of 0.889 or\nhigher. This shows that MorphoCluster is at the same time fast, accurate and\nconsistent, provides a fine-grained and data-driven classification and enables\nnovelty detection. MorphoCluster is available as open-source software at\nhttps://github.com/morphocluster.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 16:08:03 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Schr\u00f6der", "Simon-Martin", ""], ["Kiko", "Rainer", ""], ["Koch", "Reinhard", ""]]}, {"id": "2005.01600", "submitter": "Michael Glinsky", "authors": "Michael E. Glinsky, Thomas W. Moore, William E. Lewis, Matthew R.\n  Weis, Christopher A. Jennings, David J. Ampleford, Patrick F. Knapp, Eric C.\n  Harding, Matthew R. Gomez, Adam J. Harvey-Thompson", "title": "Quantification of MagLIF morphology using the Mallat Scattering\n  Transformation", "comments": "19 pages, 18 figures, 3 tables, 4 animations, accepted for\n  publication in Physics of Plasmas; arXiv admin note: substantial text overlap\n  with arXiv:1911.02359", "journal-ref": "Phys. Plasmas 27, 112703 (2020)", "doi": "10.1063/5.0010781", "report-no": "Sandia National Laboratories Report: SAND2020-10785 J and\n  SAND2020-11336 O", "categories": "physics.plasm-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The morphology of the stagnated plasma resulting from Magnetized Liner\nInertial Fusion (MagLIF) is measured by imaging the self-emission x-rays coming\nfrom the multi-keV plasma. Equivalent diagnostic response can be generated by\nintegrated radiation-magnetohydrodynamic (rad-MHD) simulations from programs\nsuch as HYDRA and GORGON. There have been only limited quantitative ways to\ncompare the image morphology, that is the texture, of simulations and\nexperiments. We have developed a metric of image morphology based on the Mallat\nScattering Transformation (MST), a transformation that has proved to be\neffective at distinguishing textures, sounds, and written characters. This\nmetric is designed, demonstrated, and refined by classifying ensembles (i.e.,\nclasses) of synthetic stagnation images, and by regressing an ensemble of\nsynthetic stagnation images to the morphology (i.e., model) parameters used to\ngenerate the synthetic images. We use this metric to quantitatively compare\nsimulations to experimental images, experimental images to each other, and to\nestimate the morphological parameters of the experimental images with\nuncertainty. This coordinate space has proved very adept at doing a\nsophisticated relative background subtraction in the MST space. This was needed\nto compare the experimental self emission images to the rad-MHD simulation\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 15:41:14 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 21:00:25 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Glinsky", "Michael E.", ""], ["Moore", "Thomas W.", ""], ["Lewis", "William E.", ""], ["Weis", "Matthew R.", ""], ["Jennings", "Christopher A.", ""], ["Ampleford", "David J.", ""], ["Knapp", "Patrick F.", ""], ["Harding", "Eric C.", ""], ["Gomez", "Matthew R.", ""], ["Harvey-Thompson", "Adam J.", ""]]}, {"id": "2005.01607", "submitter": "Tian Xia", "authors": "Tian Xia, Agisilaos Chartsias, Sotirios A. Tsaftaris", "title": "Pseudo-healthy synthesis with pathology disentanglement and adversarial\n  learning", "comments": "This paper has been accepted by Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2020.101719", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-healthy synthesis is the task of creating a subject-specific `healthy'\nimage from a pathological one. Such images can be helpful in tasks such as\nanomaly detection and understanding changes induced by pathology and disease.\nIn this paper, we present a model that is encouraged to disentangle the\ninformation of pathology from what seems to be healthy. We disentangle what\nappears to be healthy and where disease is as a segmentation map, which are\nthen recombined by a network to reconstruct the input disease image. We train\nour models adversarially using either paired or unpaired settings, where we\npair disease images and maps when available. We quantitatively and\nsubjectively, with a human study, evaluate the quality of pseudo-healthy images\nusing several criteria. We show in a series of experiments, performed on ISLES,\nBraTS and Cam-CAN datasets, that our method is better than several baselines\nand methods from the literature. We also show that due to better training\nprocesses we could recover deformations, on surrounding tissue, caused by\ndisease. Our implementation is publicly available at\nhttps://github.com/xiat0616/pseudo-healthy-synthesis. This paper has been\naccepted by Medical Image Analysis:\nhttps://doi.org/10.1016/j.media.2020.101719.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 15:38:05 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 12:33:28 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 11:39:10 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Xia", "Tian", ""], ["Chartsias", "Agisilaos", ""], ["Tsaftaris", "Sotirios A.", ""]]}, {"id": "2005.01616", "submitter": "Ruohan Gao", "authors": "Ruohan Gao, Changan Chen, Ziad Al-Halah, Carl Schissler, Kristen\n  Grauman", "title": "VisualEchoes: Spatial Image Representation Learning through Echolocation", "comments": "Appears in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several animal species (e.g., bats, dolphins, and whales) and even visually\nimpaired humans have the remarkable ability to perform echolocation: a\nbiological sonar used to perceive spatial layout and locate objects in the\nworld. We explore the spatial cues contained in echoes and how they can benefit\nvision tasks that require spatial reasoning. First we capture echo responses in\nphoto-realistic 3D indoor scene environments. Then we propose a novel\ninteraction-based representation learning framework that learns useful visual\nfeatures via echolocation. We show that the learned image features are useful\nfor multiple downstream vision tasks requiring spatial reasoning---monocular\ndepth estimation, surface normal estimation, and visual navigation---with\nresults comparable or even better than heavily supervised pre-training. Our\nwork opens a new path for representation learning for embodied agents, where\nsupervision comes from interacting with the physical world.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 16:16:58 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 17:13:38 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Gao", "Ruohan", ""], ["Chen", "Changan", ""], ["Al-Halah", "Ziad", ""], ["Schissler", "Carl", ""], ["Grauman", "Kristen", ""]]}, {"id": "2005.01632", "submitter": "Jun Hayakawa", "authors": "Jun Hayakawa, Behzad Dariush", "title": "Ego-motion and Surrounding Vehicle State Estimation Using a Monocular\n  Camera", "comments": null, "journal-ref": "2019 IEEE Intelligent Vehicles Symposium (IV)", "doi": "10.1109/IVS.2019.8814037", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding ego-motion and surrounding vehicle state is essential to enable\nautomated driving and advanced driving assistance technologies. Typical\napproaches to solve this problem use fusion of multiple sensors such as LiDAR,\ncamera, and radar to recognize surrounding vehicle state, including position,\nvelocity, and orientation. Such sensing modalities are overly complex and\ncostly for production of personal use vehicles. In this paper, we propose a\nnovel machine learning method to estimate ego-motion and surrounding vehicle\nstate using a single monocular camera. Our approach is based on a combination\nof three deep neural networks to estimate the 3D vehicle bounding box, depth,\nand optical flow from a sequence of images. The main contribution of this paper\nis a new framework and algorithm that integrates these three networks in order\nto estimate the ego-motion and surrounding vehicle state. To realize more\naccurate 3D position estimation, we address ground plane correction in\nreal-time. The efficacy of the proposed method is demonstrated through\nexperimental evaluations that compare our results to ground truth data\navailable from other sensors including Can-Bus and LiDAR.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 16:41:38 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 04:25:48 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 00:52:46 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Hayakawa", "Jun", ""], ["Dariush", "Behzad", ""]]}, {"id": "2005.01655", "submitter": "Arjun Akula", "authors": "Arjun R Akula, Spandana Gella, Yaser Al-Onaizan, Song-Chun Zhu, Siva\n  Reddy", "title": "Words aren't enough, their order matters: On the Robustness of Grounding\n  Visual Referring Expressions", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual referring expression recognition is a challenging task that requires\nnatural language understanding in the context of an image. We critically\nexamine RefCOCOg, a standard benchmark for this task, using a human study and\nshow that 83.7% of test instances do not require reasoning on linguistic\nstructure, i.e., words are enough to identify the target object, the word order\ndoesn't matter. To measure the true progress of existing models, we split the\ntest set into two sets, one which requires reasoning on linguistic structure\nand the other which doesn't. Additionally, we create an out-of-distribution\ndataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that\nthe target object changes. Using these datasets, we empirically show that\nexisting methods fail to exploit linguistic structure and are 12% to 23% lower\nin performance than the established progress for this task. We also propose two\nmethods, one based on contrastive learning and the other based on multi-task\nlearning, to increase the robustness of ViLBERT, the current state-of-the-art\nmodel for this task. Our datasets are publicly available at\nhttps://github.com/aws/aws-refcocog-adv\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 17:09:15 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Akula", "Arjun R", ""], ["Gella", "Spandana", ""], ["Al-Onaizan", "Yaser", ""], ["Zhu", "Song-Chun", ""], ["Reddy", "Siva", ""]]}, {"id": "2005.01683", "submitter": "Neel Dey", "authors": "Neel Dey, Antong Chen, Soheil Ghafurian", "title": "Group Equivariant Generative Adversarial Networks", "comments": "Accepted by the International Conference on Learning Representations\n  (ICLR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent improvements in generative adversarial visual synthesis incorporate\nreal and fake image transformation in a self-supervised setting, leading to\nincreased stability and perceptual fidelity. However, these approaches\ntypically involve image augmentations via additional regularizers in the GAN\nobjective and thus spend valuable network capacity towards approximating\ntransformation equivariance instead of their desired task. In this work, we\nexplicitly incorporate inductive symmetry priors into the network architectures\nvia group-equivariant convolutional networks. Group-convolutions have higher\nexpressive power with fewer samples and lead to better gradient feedback\nbetween generator and discriminator. We show that group-equivariance integrates\nseamlessly with recent techniques for GAN training across regularizers,\narchitectures, and loss functions. We demonstrate the utility of our methods\nfor conditional synthesis by improving generation in the limited data regime\nacross symmetric imaging datasets and even find benefits for natural images\nwith preferred orientation.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 17:38:49 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 18:00:21 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Dey", "Neel", ""], ["Chen", "Antong", ""], ["Ghafurian", "Soheil", ""]]}, {"id": "2005.01698", "submitter": "Fredrik K. Gustafsson", "authors": "Fredrik K. Gustafsson, Martin Danelljan, Radu Timofte, Thomas B.\n  Sch\\\"on", "title": "How to Train Your Energy-Based Model for Regression", "comments": "BMVC 2020. Code is available at\n  https://github.com/fregu856/ebms_regression", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-based models (EBMs) have become increasingly popular within computer\nvision in recent years. While they are commonly employed for generative image\nmodeling, recent work has applied EBMs also for regression tasks, achieving\nstate-of-the-art performance on object detection and visual tracking. Training\nEBMs is however known to be challenging. While a variety of different\ntechniques have been explored for generative modeling, the application of EBMs\nto regression is not a well-studied problem. How EBMs should be trained for\nbest possible regression performance is thus currently unclear. We therefore\naccept the task of providing the first detailed study of this problem. To that\nend, we propose a simple yet highly effective extension of noise contrastive\nestimation, and carefully compare its performance to six popular methods from\nliterature on the tasks of 1D regression and object detection. The results of\nthis comparison suggest that our training method should be considered the go-to\napproach. We also apply our method to the visual tracking task, achieving\nstate-of-the-art performance on five datasets. Notably, our tracker achieves\n63.7% AUC on LaSOT and 78.7% Success on TrackingNet. Code is available at\nhttps://github.com/fregu856/ebms_regression.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 17:55:01 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 10:08:52 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Gustafsson", "Fredrik K.", ""], ["Danelljan", "Martin", ""], ["Timofte", "Radu", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "2005.01703", "submitter": "Minyoung Huh", "authors": "Minyoung Huh, Richard Zhang, Jun-Yan Zhu, Sylvain Paris, Aaron\n  Hertzmann", "title": "Transforming and Projecting Images into Class-conditional Generative\n  Networks", "comments": "Accepted to ECCV2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for projecting an input image into the space of a\nclass-conditional generative neural network. We propose a method that optimizes\nfor transformation to counteract the model biases in generative neural\nnetworks. Specifically, we demonstrate that one can solve for image\ntranslation, scale, and global color transformation, during the projection\noptimization to address the object-center bias and color bias of a Generative\nAdversarial Network. This projection process poses a difficult optimization\nproblem, and purely gradient-based optimizations fail to find good solutions.\nWe describe a hybrid optimization strategy that finds good projections by\nestimating transformations and class parameters. We show the effectiveness of\nour method on real images and further demonstrate how the corresponding\nprojections lead to better editability of these images.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 17:57:47 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 18:10:52 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Huh", "Minyoung", ""], ["Zhang", "Richard", ""], ["Zhu", "Jun-Yan", ""], ["Paris", "Sylvain", ""], ["Hertzmann", "Aaron", ""]]}, {"id": "2005.01770", "submitter": "Devashish Prasad", "authors": "Devashish Prasad, Kshitij Kapadni, Ayan Gadpal, Manish Visave, Kavita\n  Sultanpure", "title": "HOG, LBP and SVM based Traffic Density Estimation at Intersection", "comments": "paper accepted at IEEE PuneCon 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increased amount of vehicular traffic on roads is a significant issue. High\namount of vehicular traffic creates traffic congestion, unwanted delays,\npollution, money loss, health issues, accidents, emergency vehicle passage and\ntraffic violations that ends up in the decline in productivity. In peak hours,\nthe issues become even worse. Traditional traffic management and control\nsystems fail to tackle this problem. Currently, the traffic lights at\nintersections aren't adaptive and have fixed time delays. There's a necessity\nof an optimized and sensible control system which would enhance the efficiency\nof traffic flow. Smart traffic systems perform estimation of traffic density\nand create the traffic lights modification consistent with the quantity of\ntraffic. We tend to propose an efficient way to estimate the traffic density on\nintersection using image processing and machine learning techniques in real\ntime. The proposed methodology takes pictures of traffic at junction to\nestimate the traffic density. We use Histogram of Oriented Gradients (HOG),\nLocal Binary Patterns (LBP) and Support Vector Machine (SVM) based approach for\ntraffic density estimation. The strategy is computationally inexpensive and can\nrun efficiently on raspberry pi board. Code is released at\nhttps://github.com/DevashishPrasad/Smart-Traffic-Junction.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 18:08:35 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Prasad", "Devashish", ""], ["Kapadni", "Kshitij", ""], ["Gadpal", "Ayan", ""], ["Visave", "Manish", ""], ["Sultanpure", "Kavita", ""]]}, {"id": "2005.01802", "submitter": "Filip Sroubek", "authors": "Ales Zita, Filip Sroubek", "title": "Learning-based Tracking of Fast Moving Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking fast moving objects, which appear as blurred streaks in video\nsequences, is a difficult task for standard trackers as the object position\ndoes not overlap in consecutive video frames and texture information of the\nobjects is blurred. Up-to-date approaches tuned for this task are based on\nbackground subtraction with static background and slow deblurring algorithms.\nIn this paper, we present a tracking-by-segmentation approach implemented using\nstate-of-the-art deep learning methods that performs near-realtime tracking on\nreal-world video sequences. We implemented a physically plausible FMO sequence\ngenerator to be a robust foundation for our training pipeline and demonstrate\nthe ease of fast generator and network adaptation for different FMO scenarios\nin terms of foreground variations.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 19:20:09 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Zita", "Ales", ""], ["Sroubek", "Filip", ""]]}, {"id": "2005.01805", "submitter": "Mark Loyman", "authors": "Mark Loyman and Hayit Greenspan", "title": "Semi-supervised lung nodule retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Content based image retrieval (CBIR) provides the clinician with visual\ninformation that can support, and hopefully improve, his or her decision making\nprocess. Given an input query image, a CBIR system provides as its output a set\nof images, ranked by similarity to the query image. Retrieved images may come\nwith relevant information, such as biopsy-based malignancy labeling, or\ncategorization. Ground truth on similarity between dataset elements (e.g.\nbetween nodules) is not readily available, thus greatly challenging machine\nlearning methods. Such annotations are particularly difficult to obtain, due to\nthe subjective nature of the task, with high inter-observer variability\nrequiring multiple expert annotators. Consequently, past approaches have\nfocused on manual feature extraction, while current approaches use auxiliary\ntasks, such as a binary classification task (e.g. malignancy), for which\nground-true is more readily accessible. However, in a previous study, we have\nshown that binary auxiliary tasks are inferior to the usage of a rough\nsimilarity estimate that are derived from data annotations. The current study\nsuggests a semi-supervised approach that involves two steps: 1) Automatic\nannotation of a given partially labeled dataset; 2) Learning a semantic\nsimilarity metric space based on the predicated annotations. The proposed\nsystem is demonstrated in lung nodule retrieval using the LIDC dataset, and\nshows that it is feasible to learn embedding from predicted ratings. The\nsemi-supervised approach has demonstrated a significantly higher discriminative\nability than the fully-unsupervised reference.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 19:26:14 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Loyman", "Mark", ""], ["Greenspan", "Hayit", ""]]}, {"id": "2005.01807", "submitter": "Nitin Rathi", "authors": "Nitin Rathi, Gopalakrishnan Srinivasan, Priyadarshini Panda, Kaushik\n  Roy", "title": "Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike\n  Timing Dependent Backpropagation", "comments": "International Conference on Learning Representations (ICLR), 2020\n  https://openreview.net/forum?id=B1xSperKvH&noteId=B1xSperKvH", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or\nspikes) which can potentially lead to higher energy-efficiency in neuromorphic\nhardware implementations. Many works have shown that an SNN for inference can\nbe formed by copying the weights from a trained Artificial Neural Network (ANN)\nand setting the firing threshold for each layer as the maximum input received\nin that layer. These type of converted SNNs require a large number of time\nsteps to achieve competitive accuracy which diminishes the energy savings. The\nnumber of time steps can be reduced by training SNNs with spike-based\nbackpropagation from scratch, but that is computationally expensive and slow.\nTo address these challenges, we present a computationally-efficient training\ntechnique for deep SNNs. We propose a hybrid training methodology: 1) take a\nconverted SNN and use its weights and thresholds as an initialization step for\nspike-based backpropagation, and 2) perform incremental spike-timing dependent\nbackpropagation (STDB) on this carefully initialized network to obtain an SNN\nthat converges within few epochs and requires fewer time steps for input\nprocessing. STDB is performed with a novel surrogate gradient function defined\nusing neuron's spike time. The proposed training methodology converges in less\nthan 20 epochs of spike-based backpropagation for most standard image\nclassification datasets, thereby greatly reducing the training complexity\ncompared to training SNNs from scratch. We perform experiments on CIFAR-10,\nCIFAR-100, and ImageNet datasets for both VGG and ResNet architectures. We\nachieve top-1 accuracy of 65.19% for ImageNet dataset on SNN with 250 time\nsteps, which is 10X faster compared to converted SNNs with similar accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 19:30:43 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Rathi", "Nitin", ""], ["Srinivasan", "Gopalakrishnan", ""], ["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "2005.01856", "submitter": "Maximilian Ilse", "authors": "Maximilian Ilse, Jakub M. Tomczak, Patrick Forr\\'e", "title": "Selecting Data Augmentation for Simulating Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models trained with purely observational data and the\nprinciple of empirical risk minimization \\citep{vapnik_principles_1992} can\nfail to generalize to unseen domains. In this paper, we focus on the case where\nthe problem arises through spurious correlation between the observed domains\nand the actual task labels. We find that many domain generalization methods do\nnot explicitly take this spurious correlation into account. Instead, especially\nin more application-oriented research areas like medical imaging or robotics,\ndata augmentation techniques that are based on heuristics are used to learn\ndomain invariant features. To bridge the gap between theory and practice, we\ndevelop a causal perspective on the problem of domain generalization. We argue\nthat causal concepts can be used to explain the success of data augmentation by\ndescribing how they can weaken the spurious correlation between the observed\ndomains and the task labels. We demonstrate that data augmentation can serve as\na tool for simulating interventional data. We use these theoretical insights to\nderive a simple algorithm that is able to select data augmentation techniques\nthat will lead to better domain generalization.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 21:33:29 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 14:59:40 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 13:16:20 GMT"}, {"version": "v4", "created": "Mon, 26 Oct 2020 10:52:21 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ilse", "Maximilian", ""], ["Tomczak", "Jakub M.", ""], ["Forr\u00e9", "Patrick", ""]]}, {"id": "2005.01864", "submitter": "Jonathon Shlens", "authors": "Wei Han, Zhengdong Zhang, Benjamin Caine, Brandon Yang, Christoph\n  Sprunk, Ouais Alsharif, Jiquan Ngiam, Vijay Vasudevan, Jonathon Shlens,\n  Zhifeng Chen", "title": "Streaming Object Detection for 3-D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles operate in a dynamic environment, where the speed with\nwhich a vehicle can perceive and react impacts the safety and efficacy of the\nsystem. LiDAR provides a prominent sensory modality that informs many existing\nperceptual systems including object detection, segmentation, motion estimation,\nand action recognition. The latency for perceptual systems based on point cloud\ndata can be dominated by the amount of time for a complete rotational scan\n(e.g. 100 ms). This built-in data capture latency is artificial, and based on\ntreating the point cloud as a camera image in order to leverage camera-inspired\narchitectures. However, unlike camera sensors, most LiDAR point cloud data is\nnatively a streaming data source in which laser reflections are sequentially\nrecorded based on the precession of the laser beam. In this work, we explore\nhow to build an object detector that removes this artificial latency\nconstraint, and instead operates on native streaming data in order to\nsignificantly reduce latency. This approach has the added benefit of reducing\nthe peak computational burden on inference hardware by spreading the\ncomputation over the acquisition time for a scan. We demonstrate a family of\nstreaming detection systems based on sequential modeling through a series of\nmodifications to the traditional detection meta-architecture. We highlight how\nthis model may achieve competitive if not superior predictive performance with\nstate-of-the-art, traditional non-streaming detection systems while achieving\nsignificant latency gains (e.g. 1/15'th - 1/3'rd of peak latency). Our results\nshow that operating on LiDAR data in its native streaming formulation offers\nseveral advantages for self driving object detection -- advantages that we hope\nwill be useful for any LiDAR perception system where minimizing latency is\ncritical for safe and efficient operation.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 21:55:15 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Han", "Wei", ""], ["Zhang", "Zhengdong", ""], ["Caine", "Benjamin", ""], ["Yang", "Brandon", ""], ["Sprunk", "Christoph", ""], ["Alsharif", "Ouais", ""], ["Ngiam", "Jiquan", ""], ["Vasudevan", "Vijay", ""], ["Shlens", "Jonathon", ""], ["Chen", "Zhifeng", ""]]}, {"id": "2005.01878", "submitter": "Sorour Mohajerani", "authors": "Sorour Mohajerani, Mark S. Drew, Parvaneh Saeedi", "title": "Illumination-Invariant Image from 4-Channel Images: The Effect of\n  Near-Infrared Data in Shadow Removal", "comments": "Accepted for oral presentation in London Imaging Meeting 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing the effect of illumination variation in images has been proved to be\nbeneficial in many computer vision applications such as object recognition and\nsemantic segmentation. Although generating illumination-invariant images has\nbeen studied in the literature before, it has not been investigated on real\n4-channel (4D) data. In this study, we examine the quality of\nillumination-invariant images generated from red, green, blue, and\nnear-infrared (RGBN) data. Our experiments show that the near-infrared channel\nsubstantively contributes toward removing illumination. As shown in our\nnumerical and visual results, the illumination-invariant image obtained by RGBN\ndata is superior compared to that obtained by RGB alone.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 22:51:36 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Mohajerani", "Sorour", ""], ["Drew", "Mark S.", ""], ["Saeedi", "Parvaneh", ""]]}, {"id": "2005.01903", "submitter": "Siqi Liu", "authors": "Siqi Liu, Bogdan Georgescu, Zhoubing Xu, Youngjin Yoo, Guillaume\n  Chabin, Shikha Chaganti, Sasa Grbic, Sebastian Piat, Brian Teixeira, Abishek\n  Balachandran, Vishwanath RS, Thomas Re, Dorin Comaniciu", "title": "3D Tomographic Pattern Synthesis for Enhancing the Quantification of\n  COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Coronavirus Disease (COVID-19) has affected 1.8 million people and\nresulted in more than 110,000 deaths as of April 12, 2020. Several studies have\nshown that tomographic patterns seen on chest Computed Tomography (CT), such as\nground-glass opacities, consolidations, and crazy paving pattern, are\ncorrelated with the disease severity and progression. CT imaging can thus\nemerge as an important modality for the management of COVID-19 patients.\nAI-based solutions can be used to support CT based quantitative reporting and\nmake reading efficient and reproducible if quantitative biomarkers, such as the\nPercentage of Opacity (PO), can be automatically computed. However, COVID-19\nhas posed unique challenges to the development of AI, specifically concerning\nthe availability of appropriate image data and annotations at scale. In this\npaper, we propose to use synthetic datasets to augment an existing COVID-19\ndatabase to tackle these challenges. We train a Generative Adversarial Network\n(GAN) to inpaint COVID-19 related tomographic patterns on chest CTs from\npatients without infectious diseases. Additionally, we leverage location priors\nderived from manually labeled COVID-19 chest CTs patients to generate\nappropriate abnormality distributions. Synthetic data are used to improve both\nlung segmentation and segmentation of COVID-19 patterns by adding 20% of\nsynthetic data to the real COVID-19 training data. We collected 2143 chest CTs,\ncontaining 327 COVID-19 positive cases, acquired from 12 sites across 7\ncountries. By testing on 100 COVID-19 positive and 100 control cases, we show\nthat synthetic data can help improve both lung segmentation (+6.02% lesion\ninclusion rate) and abnormality segmentation (+2.78% dice coefficient), leading\nto an overall more accurate PO computation (+2.82% Pearson coefficient).\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 01:31:40 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Liu", "Siqi", ""], ["Georgescu", "Bogdan", ""], ["Xu", "Zhoubing", ""], ["Yoo", "Youngjin", ""], ["Chabin", "Guillaume", ""], ["Chaganti", "Shikha", ""], ["Grbic", "Sasa", ""], ["Piat", "Sebastian", ""], ["Teixeira", "Brian", ""], ["Balachandran", "Abishek", ""], ["RS", "Vishwanath", ""], ["Re", "Thomas", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "2005.01923", "submitter": "Muhammad Ali Farooq", "authors": "Muhammad Ali Farooq and Peter Corcoran", "title": "Generating Thermal Image Data Samples using 3D Facial Modelling\n  Techniques and Deep Learning Methodologies", "comments": "Paper accpeted in QOMEX IEEE 2020 Conference copyright submitted to\n  IEEE", "journal-ref": null, "doi": "10.1109/QoMEX48832.2020.9123079", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for generating synthetic data have become of increasing importance to\nbuild large datasets required for Convolution Neural Networks (CNN) based deep\nlearning techniques for a wide range of computer vision applications. In this\nwork, we extend existing methodologies to show how 2D thermal facial data can\nbe mapped to provide 3D facial models. For the proposed research work we have\nused tufts datasets for generating 3D varying face poses by using a single\nfrontal face pose. The system works by refining the existing image quality by\nperforming fusion based image preprocessing operations. The refined outputs\nhave better contrast adjustments, decreased noise level and higher exposedness\nof the dark regions. It makes the facial landmarks and temperature patterns on\nthe human face more discernible and visible when compared to original raw data.\nDifferent image quality metrics are used to compare the refined version of\nimages with original images. In the next phase of the proposed study, the\nrefined version of images is used to create 3D facial geometry structures by\nusing Convolution Neural Networks (CNN). The generated outputs are then\nimported in blender software to finally extract the 3D thermal facial outputs\nof both males and females. The same technique is also used on our thermal face\ndata acquired using prototype thermal camera (developed under Heliaus EU\nproject) in an indoor lab environment which is then used for generating\nsynthetic 3D face data along with varying yaw face angles and lastly facial\ndepth map is generated.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 02:55:14 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 11:02:04 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Farooq", "Muhammad Ali", ""], ["Corcoran", "Peter", ""]]}, {"id": "2005.01927", "submitter": "Rui Liu", "authors": "Rui Liu, Chengxi Yang, Wenxiu Sun, Xiaogang Wang, Hongsheng Li", "title": "StereoGAN: Bridging Synthetic-to-Real Domain Gap by Joint Optimization\n  of Domain Translation and Stereo Matching", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale synthetic datasets are beneficial to stereo matching but usually\nintroduce known domain bias. Although unsupervised image-to-image translation\nnetworks represented by CycleGAN show great potential in dealing with domain\ngap, it is non-trivial to generalize this method to stereo matching due to the\nproblem of pixel distortion and stereo mismatch after translation. In this\npaper, we propose an end-to-end training framework with domain translation and\nstereo matching networks to tackle this challenge. First, joint optimization\nbetween domain translation and stereo matching networks in our end-to-end\nframework makes the former facilitate the latter one to the maximum extent.\nSecond, this framework introduces two novel losses, i.e., bidirectional\nmulti-scale feature re-projection loss and correlation consistency loss, to\nhelp translate all synthetic stereo images into realistic ones as well as\nmaintain epipolar constraints. The effective combination of above two\ncontributions leads to impressive stereo-consistent translation and disparity\nestimation accuracy. In addition, a mode seeking regularization term is added\nto endow the synthetic-to-real translation results with higher fine-grained\ndiversity. Extensive experiments demonstrate the effectiveness of the proposed\nframework on bridging the synthetic-to-real domain gap on stereo matching.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 03:11:38 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Liu", "Rui", ""], ["Yang", "Chengxi", ""], ["Sun", "Wenxiu", ""], ["Wang", "Xiaogang", ""], ["Li", "Hongsheng", ""]]}, {"id": "2005.01928", "submitter": "Thomas Lacombe", "authors": "Thomas Lacombe, Hugues Favreliere, Maurice Pillet", "title": "Modal features for image texture classification", "comments": "12 pages, 5 figures. Accepted and to be published in Pattern\n  Recognition Letters", "journal-ref": null, "doi": "10.1016/j.patrec.2020.04.036", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction is a key step in image processing for pattern recognition\nand machine learning processes. Its purpose lies in reducing the dimensionality\nof the input data through the computing of features which accurately describe\nthe original information. In this article, a new feature extraction method\nbased on Discrete Modal Decomposition (DMD) is introduced, to extend the group\nof space and frequency based features. These new features are called modal\nfeatures. Initially aiming to decompose a signal into a modal basis built from\na vibration mechanics problem, the DMD projection is applied to images in order\nto extract modal features with two approaches. The first one, called full scale\nDMD, consists in exploiting directly the decomposition resulting coordinates as\nfeatures. The second one, called filtering DMD, consists in using the DMD modes\nas filters to obtain features through a local transformation process.\nExperiments are performed on image texture classification tasks including\nseveral widely used data bases, compared to several classic feature extraction\nmethods. We show that the DMD approach achieves good classification\nperformances, comparable to the state of the art techniques, with a lower\nextraction time.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 03:13:54 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Lacombe", "Thomas", ""], ["Favreliere", "Hugues", ""], ["Pillet", "Maurice", ""]]}, {"id": "2005.01939", "submitter": "K L Navaneet", "authors": "K L Navaneet, Ansu Mathew, Shashank Kashyap, Wei-Chih Hung, Varun\n  Jampani and R. Venkatesh Babu", "title": "From Image Collections to Point Clouds with Self-supervised Shape and\n  Pose Networks", "comments": "Accepted to CVPR 2020; Codes are available at\n  https://github.com/val-iisc/ssl_3d_recon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing 3D models from 2D images is one of the fundamental problems in\ncomputer vision. In this work, we propose a deep learning technique for 3D\nobject reconstruction from a single image. Contrary to recent works that either\nuse 3D supervision or multi-view supervision, we use only single view images\nwith no pose information during training as well. This makes our approach more\npractical requiring only an image collection of an object category and the\ncorresponding silhouettes. We learn both 3D point cloud reconstruction and pose\nestimation networks in a self-supervised manner, making use of differentiable\npoint cloud renderer to train with 2D supervision. A key novelty of the\nproposed technique is to impose 3D geometric reasoning into predicted 3D point\nclouds by rotating them with randomly sampled poses and then enforcing cycle\nconsistency on both 3D reconstructions and poses. In addition, using\nsingle-view supervision allows us to do test-time optimization on a given test\nimage. Experiments on the synthetic ShapeNet and real-world Pix3D datasets\ndemonstrate that our approach, despite using less supervision, can achieve\ncompetitive performance compared to pose-supervised and multi-view supervised\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 04:25:16 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Navaneet", "K L", ""], ["Mathew", "Ansu", ""], ["Kashyap", "Shashank", ""], ["Hung", "Wei-Chih", ""], ["Jampani", "Varun", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "2005.01947", "submitter": "Smit Marvaniya", "authors": "Smit Marvaniya, Umamaheswari Devi, Jagabondhu Hazra, Shashank Mujumdar\n  and Nitin Gupta", "title": "Small, Sparse, but Substantial: Techniques for Segmenting Small\n  Agricultural Fields Using Sparse Ground Data", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": "10.1080/01431161.2020.1834166", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent thrust on digital agriculture (DA) has renewed significant\nresearch interest in the automated delineation of agricultural fields. Most\nprior work addressing this problem have focused on detecting medium to large\nfields, while there is strong evidence that around 40\\% of the fields\nworld-wide and 70% of the fields in Asia and Africa are small. The lack of\nadequate labeled images for small fields, huge variations in their color,\ntexture, and shape, and faint boundary lines separating them make it difficult\nto develop an end-to-end learning model for detecting such fields. Hence, in\nthis paper, we present a multi-stage approach that uses a combination of\nmachine learning and image processing techniques. In the first stage, we\nleverage state-of-the-art edge detection algorithms such as holistically-nested\nedge detection (HED) to extract first-level contours and polygons. In the\nsecond stage, we propose image-processing techniques to identify polygons that\nare non-fields, over-segmentations, or noise and eliminate them. The next stage\ntackles under-segmentations using a combination of a novel ``cut-point'' based\ntechnique and localized second-level edge detection to obtain individual\nparcels. Since a few small, non-cropped but vegetated or constructed pockets\ncan be interspersed in areas that are predominantly croplands, in the final\nstage, we train a classifier for identifying each parcel from the previous\nstage as an agricultural field or not. In an evaluation using high-resolution\nimagery, we show that our approach has a high F-Score of 0.84 in areas with\nlarge fields and reasonable accuracy with an F-Score of 0.73 in areas with\nsmall fields, which is encouraging.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 05:26:19 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Marvaniya", "Smit", ""], ["Devi", "Umamaheswari", ""], ["Hazra", "Jagabondhu", ""], ["Mujumdar", "Shashank", ""], ["Gupta", "Nitin", ""]]}, {"id": "2005.01969", "submitter": "Jiancheng Yang", "authors": "Jiancheng Yang, Yi He, Xiaoyang Huang, Jingwei Xu, Xiaodan Ye, Guangyu\n  Tao, Bingbing Ni", "title": "AlignShift: Bridging the Gap of Imaging Thickness in 3D Anisotropic\n  Volumes", "comments": "MICCAI 2020 (early accepted). Camera ready version. Code is available\n  at https://github.com/M3DV/AlignShift", "journal-ref": null, "doi": "10.1007/978-3-030-59719-1_55", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses a fundamental challenge in 3D medical image processing:\nhow to deal with imaging thickness. For anisotropic medical volumes, there is a\nsignificant performance gap between thin-slice (mostly 1mm) and thick-slice\n(mostly 5mm) volumes. Prior arts tend to use 3D approaches for the thin-slice\nand 2D approaches for the thick-slice, respectively. We aim at a unified\napproach for both thin- and thick-slice medical volumes. Inspired by recent\nadvances in video analysis, we propose AlignShift, a novel parameter-free\noperator to convert theoretically any 2D pretrained network into\nthickness-aware 3D network. Remarkably, the converted networks behave like 3D\nfor the thin-slice, nevertheless degenerate to 2D for the thick-slice\nadaptively. The unified thickness-aware representation learning is achieved by\nshifting and fusing aligned \"virtual slices\" as per the input imaging\nthickness. Extensive experiments on public large-scale DeepLesion benchmark,\nconsisting of 32K lesions for universal lesion detection, validate the\neffectiveness of our method, which outperforms previous state of the art by\nconsiderable margins without whistles and bells. More importantly, to our\nknowledge, this is the first method that bridges the performance gap between\nthin- and thick-slice volumes by a unified framework. To improve research\nreproducibility, our code in PyTorch is open source at\nhttps://github.com/M3DV/AlignShift.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 06:54:26 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 10:03:05 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Yang", "Jiancheng", ""], ["He", "Yi", ""], ["Huang", "Xiaoyang", ""], ["Xu", "Jingwei", ""], ["Ye", "Xiaodan", ""], ["Tao", "Guangyu", ""], ["Ni", "Bingbing", ""]]}, {"id": "2005.01996", "submitter": "Andreas Lugmayr", "authors": "Andreas Lugmayr, Martin Danelljan, Radu Timofte, Namhyuk Ahn, Dongwoon\n  Bai, Jie Cai, Yun Cao, Junyang Chen, Kaihua Cheng, SeYoung Chun, Wei Deng,\n  Mostafa El-Khamy, Chiu Man Ho, Xiaozhong Ji, Amin Kheradmand, Gwantae Kim,\n  Hanseok Ko, Kanghyu Lee, Jungwon Lee, Hao Li, Ziluan Liu, Zhi-Song Liu, Shuai\n  Liu, Yunhua Lu, Zibo Meng, Pablo Navarrete Michelini, Christian Micheloni,\n  Kalpesh Prajapati, Haoyu Ren, Yong Hyeok Seo, Wan-Chi Siu, Kyung-Ah Sohn,\n  Ying Tai, Rao Muhammad Umer, Shuangquan Wang, Huibing Wang, Timothy Haoning\n  Wu, Haoning Wu, Biao Yang, Fuzhi Yang, Jaejun Yoo, Tongtong Zhao, Yuanbo\n  Zhou, Haijie Zhuo, Ziyao Zong, Xueyi Zou", "title": "NTIRE 2020 Challenge on Real-World Image Super-Resolution: Methods and\n  Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the NTIRE 2020 challenge on real world super-resolution.\nIt focuses on the participating methods and final results. The challenge\naddresses the real world setting, where paired true high and low-resolution\nimages are unavailable. For training, only one set of source input images is\ntherefore provided along with a set of unpaired high-quality target images. In\nTrack 1: Image Processing artifacts, the aim is to super-resolve images with\nsynthetically generated image processing artifacts. This allows for\nquantitative benchmarking of the approaches \\wrt a ground-truth image. In Track\n2: Smartphone Images, real low-quality smart phone images have to be\nsuper-resolved. In both tracks, the ultimate goal is to achieve the best\nperceptual quality, evaluated using a human study. This is the second challenge\non the subject, following AIM 2019, targeting to advance the state-of-the-art\nin super-resolution. To measure the performance we use the benchmark protocol\nfrom AIM 2019. In total 22 teams competed in the final testing phase,\ndemonstrating new and innovative solutions to the problem.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 08:17:04 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Lugmayr", "Andreas", ""], ["Danelljan", "Martin", ""], ["Timofte", "Radu", ""], ["Ahn", "Namhyuk", ""], ["Bai", "Dongwoon", ""], ["Cai", "Jie", ""], ["Cao", "Yun", ""], ["Chen", "Junyang", ""], ["Cheng", "Kaihua", ""], ["Chun", "SeYoung", ""], ["Deng", "Wei", ""], ["El-Khamy", "Mostafa", ""], ["Ho", "Chiu Man", ""], ["Ji", "Xiaozhong", ""], ["Kheradmand", "Amin", ""], ["Kim", "Gwantae", ""], ["Ko", "Hanseok", ""], ["Lee", "Kanghyu", ""], ["Lee", "Jungwon", ""], ["Li", "Hao", ""], ["Liu", "Ziluan", ""], ["Liu", "Zhi-Song", ""], ["Liu", "Shuai", ""], ["Lu", "Yunhua", ""], ["Meng", "Zibo", ""], ["Michelini", "Pablo Navarrete", ""], ["Micheloni", "Christian", ""], ["Prajapati", "Kalpesh", ""], ["Ren", "Haoyu", ""], ["Seo", "Yong Hyeok", ""], ["Siu", "Wan-Chi", ""], ["Sohn", "Kyung-Ah", ""], ["Tai", "Ying", ""], ["Umer", "Rao Muhammad", ""], ["Wang", "Shuangquan", ""], ["Wang", "Huibing", ""], ["Wu", "Timothy Haoning", ""], ["Wu", "Haoning", ""], ["Yang", "Biao", ""], ["Yang", "Fuzhi", ""], ["Yoo", "Jaejun", ""], ["Zhao", "Tongtong", ""], ["Zhou", "Yuanbo", ""], ["Zhuo", "Haijie", ""], ["Zong", "Ziyao", ""], ["Zou", "Xueyi", ""]]}, {"id": "2005.02000", "submitter": "Adriano Lucieri", "authors": "Adriano Lucieri, Muhammad Naseer Bajwa, Stephan Alexander Braun,\n  Muhammad Imran Malik, Andreas Dengel and Sheraz Ahmed", "title": "On Interpretability of Deep Learning based Skin Lesion Classifiers using\n  Concept Activation Vectors", "comments": "Accepted for the IEEE International Joint Conference on Neural\n  Networks (IJCNN) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based medical image classifiers have shown remarkable prowess\nin various application areas like ophthalmology, dermatology, pathology, and\nradiology. However, the acceptance of these Computer-Aided Diagnosis (CAD)\nsystems in real clinical setups is severely limited primarily because their\ndecision-making process remains largely obscure. This work aims at elucidating\na deep learning based medical image classifier by verifying that the model\nlearns and utilizes similar disease-related concepts as described and employed\nby dermatologists. We used a well-trained and high performing neural network\ndeveloped by REasoning for COmplex Data (RECOD) Lab for classification of three\nskin tumours, i.e. Melanocytic Naevi, Melanoma and Seborrheic Keratosis and\nperformed a detailed analysis on its latent space. Two well established and\npublicly available skin disease datasets, PH2 and derm7pt, are used for\nexperimentation. Human understandable concepts are mapped to RECOD image\nclassification model with the help of Concept Activation Vectors (CAVs),\nintroducing a novel training and significance testing paradigm for CAVs. Our\nresults on an independent evaluation set clearly shows that the classifier\nlearns and encodes human understandable concepts in its latent representation.\nAdditionally, TCAV scores (Testing with CAVs) suggest that the neural network\nindeed makes use of disease-related concepts in the correct way when making\npredictions. We anticipate that this work can not only increase confidence of\nmedical practitioners on CAD but also serve as a stepping stone for further\ndevelopment of CAV-based neural network interpretation methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 08:27:16 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Lucieri", "Adriano", ""], ["Bajwa", "Muhammad Naseer", ""], ["Braun", "Stephan Alexander", ""], ["Malik", "Muhammad Imran", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "2005.02066", "submitter": "Dongnan Liu", "authors": "Dongnan Liu, Donghao Zhang, Yang Song, Fan Zhang, Lauren O'Donnell,\n  Heng Huang, Mei Chen, Weidong Cai", "title": "Unsupervised Instance Segmentation in Microscopy Images via Panoptic\n  Domain Adaptation and Task Re-weighting", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) for nuclei instance segmentation is\nimportant for digital pathology, as it alleviates the burden of labor-intensive\nannotation and domain shift across datasets. In this work, we propose a Cycle\nConsistency Panoptic Domain Adaptive Mask R-CNN (CyC-PDAM) architecture for\nunsupervised nuclei segmentation in histopathology images, by learning from\nfluorescence microscopy images. More specifically, we first propose a nuclei\ninpainting mechanism to remove the auxiliary generated objects in the\nsynthesized images. Secondly, a semantic branch with a domain discriminator is\ndesigned to achieve panoptic-level domain adaptation. Thirdly, in order to\navoid the influence of the source-biased features, we propose a task\nre-weighting mechanism to dynamically add trade-off weights for the\ntask-specific loss functions. Experimental results on three datasets indicate\nthat our proposed method outperforms state-of-the-art UDA methods\nsignificantly, and demonstrates a similar performance as fully supervised\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 11:08:26 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Liu", "Dongnan", ""], ["Zhang", "Donghao", ""], ["Song", "Yang", ""], ["Zhang", "Fan", ""], ["O'Donnell", "Lauren", ""], ["Huang", "Heng", ""], ["Chen", "Mei", ""], ["Cai", "Weidong", ""]]}, {"id": "2005.02105", "submitter": "Nadica Miljkovi\\'c", "authors": "Nadica Miljkovi\\'c and Milica S. Isakovi\\'c", "title": "Effect of the sEMG electrode (re)placement and feature set size on the\n  hand movement recognition", "comments": "19 pages, 4 figures, 5 tables", "journal-ref": "Nadica Miljkovi\\'c, Milica S. Isakovi\\'c, Effect of the sEMG\n  electrode (re)placement and feature set size on the hand movement\n  recognition, Biomedical Signal Processing and Control, Volume 64, 2021,\n  102292, ISSN 1746-8094", "doi": "10.1016/j.bspc.2020.102292", "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repositioning of recording electrode array across repeated electromyography\nmeasurements may result in a displacement error in hand movement classification\nsystems. In order to examine if the classifier re-training could reach\nsatisfactory results when electrode array is translated along or rotated around\nsubject's forearm for varying number of features, we recorded surface\nelectromyography signals in 10 healthy volunteers for three types of grasp and\n6 wrist movements. For feature extraction we applied principal component\nanalysis and the feature set size varied from one to 8 principal components. We\ncompared results of re-trained classifier with results from leave-one-out\ncross-validation classification procedure for three classifiers: LDA (Linear\nDiscriminant Analysis), QDA (Quadratic Discriminant Analysis), and ANN\n(Artificial Neural Network). Our results showed that there was no significant\ndifference in classification accuracy when the array electrode was repositioned\nindicating successful classification re-training and optimal feature set\nselection. The results also indicate expectedly that the number of principal\ncomponents plays a key role for acceptable classification accuracy ~90 %. For\nthe largest dataset (9 hand movements), LDA and QDA outperformed ANN, while for\nthree grasping movements ANN showed promising results. Interestingly, we showed\nthat interaction between electrode array position and the feature set size is\nnot statistically significant. This study emphasizes the importance of testing\nthe interaction of factors that influence classification accuracy and\nclassifier selection altogether with their impact independently in order to\nestablish guiding principles for design of hand movement recognition system.\nData recorded for this study are stored on Zenodo repository (doi:\n10.5281/zenodo.4039550).\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 12:36:26 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 07:56:24 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Miljkovi\u0107", "Nadica", ""], ["Isakovi\u0107", "Milica S.", ""]]}, {"id": "2005.02113", "submitter": "Haoxin Li", "authors": "Haoxin Li, Wei-Shi Zheng, Yu Tao, Haifeng Hu, Jian-Huang Lai", "title": "Adaptive Interaction Modeling via Graph Operations Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interaction modeling is important for video action analysis. Recently,\nseveral works design specific structures to model interactions in videos.\nHowever, their structures are manually designed and non-adaptive, which require\nstructures design efforts and more importantly could not model interactions\nadaptively. In this paper, we automate the process of structures design to\nlearn adaptive structures for interaction modeling. We propose to search the\nnetwork structures with differentiable architecture search mechanism, which\nlearns to construct adaptive structures for different videos to facilitate\nadaptive interaction modeling. To this end, we first design the search space\nwith several basic graph operations that explicitly capture different relations\nin videos. We experimentally demonstrate that our architecture search framework\nlearns to construct adaptive interaction modeling structures, which provides\nmore understanding about the relations between the structures and some\ninteraction characteristics, and also releases the requirement of structures\ndesign efforts. Additionally, we show that the designed basic graph operations\nin the search space are able to model different interactions in videos. The\nexperiments on two interaction datasets show that our method achieves\ncompetitive performance with state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 13:01:09 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Li", "Haoxin", ""], ["Zheng", "Wei-Shi", ""], ["Tao", "Yu", ""], ["Hu", "Haifeng", ""], ["Lai", "Jian-Huang", ""]]}, {"id": "2005.02118", "submitter": "Muhammad E. H. Chowdhury", "authors": "Mahmoud Dahmani, Muhammad E. H. Chowdhury, Amith Khandakar, Tawsifur\n  Rahman, Khaled Al-Jayyousi, Abdalla Hefny, and Serkan Kiranyaz", "title": "An Intelligent and Low-cost Eye-tracking System for Motorized Wheelchair\n  Control", "comments": "Accepted for publication in Sensor, 19 Figure, 3 Tables", "journal-ref": "Sensors 2020, 20(14), 3936", "doi": "10.3390/s20143936", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 34 developed and 156 developing countries, there are about 132 million\ndisabled people who need a wheelchair constituting 1.86% of the world\npopulation. Moreover, there are millions of people suffering from diseases\nrelated to motor disabilities, which cause inability to produce controlled\nmovement in any of the limbs or even head.The paper proposes a system to aid\npeople with motor disabilities by restoring their ability to move effectively\nand effortlessly without having to rely on others utilizing an eye-controlled\nelectric wheelchair. The system input was images of the users eye that were\nprocessed to estimate the gaze direction and the wheelchair was moved\naccordingly. To accomplish such a feat, four user-specific methods were\ndeveloped, implemented and tested; all of which were based on a benchmark\ndatabase created by the authors.The first three techniques were automatic,\nemploy correlation and were variants of template matching, while the last one\nuses convolutional neural networks (CNNs). Different metrics to quantitatively\nevaluate the performance of each algorithm in terms of accuracy and latency\nwere computed and overall comparison is presented. CNN exhibited the best\nperformance (i.e. 99.3% classification accuracy), and thus it was the model of\nchoice for the gaze estimator, which commands the wheelchair motion. The system\nwas evaluated carefully on 8 subjects achieving 99% accuracy in changing\nillumination conditions outdoor and indoor. This required modifying a motorized\nwheelchair to adapt it to the predictions output by the gaze estimation\nalgorithm. The wheelchair control can bypass any decision made by the gaze\nestimator and immediately halt its motion with the help of an array of\nproximity sensors, if the measured distance goes below a well-defined safety\nmargin.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 23:08:33 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Dahmani", "Mahmoud", ""], ["Chowdhury", "Muhammad E. H.", ""], ["Khandakar", "Amith", ""], ["Rahman", "Tawsifur", ""], ["Al-Jayyousi", "Khaled", ""], ["Hefny", "Abdalla", ""], ["Kiranyaz", "Serkan", ""]]}, {"id": "2005.02120", "submitter": "Mehrdad Shafiei Dizaji", "authors": "Mehrdad S. Dizaji, Devin K. Harris, Bernie Kassner, Jeffrey C. Hill", "title": "Computer Vision-Based Health Monitoring of Mecklenburg Bridge Using 3D\n  Digital Image Correlation", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A collaborative investigation between the University of Virginia (UVA) and\nthe Virginia Transportation Research Council was performed on the Mecklenburg\nBridge (I-85 over Route 1 in Mecklenburg County). The research team aided the\nVirginia Department of Transportation - Richmond District in the\ncharacterization of the bridge behavior of one of the bridge beams that had\nbeen repaired due to a previous web buckling and crippling failure. The\ninvestigation focused on collecting full-field three-dimensional digital image\ncorrelation (3D-DIC) deformation measurements during the dropping sequence\n(removal of jacking to support beam on bearing/pier). Additionally,\nmeasurements were taken of the section prior to and after dropping using a\nhandheld laser scanner to assess the potential of lateral deformation or\nout-of-plane buckling. Results from the study demonstrated that buckling of the\ntested beam did not occur, but did provided a series of approaches that can be\nused to evaluate the effectiveness of repaired steel beam ends. Specifically,\nthe results provided an approach that could estimate the dead load distribution\nthrough back-calculation.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 00:56:06 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Dizaji", "Mehrdad S.", ""], ["Harris", "Devin K.", ""], ["Kassner", "Bernie", ""], ["Hill", "Jeffrey C.", ""]]}, {"id": "2005.02121", "submitter": "Subhash Nerella", "authors": "Subhash Nerella, Azra Bihorac, Patrick Tighe, Parisa Rashidi", "title": "Facial Action Unit Detection on ICU Data for Pain Assessment", "comments": "4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current day pain assessment methods rely on patient self-report or by an\nobserver like the Intensive Care Unit (ICU) nurses. Patient self-report is\nsubjective to the individual and suffers due to poor recall. Pain assessment by\nmanual observation is limited by the number of administrations per day and\nstaff workload. Previous studies showed the feasibility of automatic pain\nassessment by detecting Facial Action Units (AUs). Pain is observed to be\nassociated with certain facial action units (AUs). This method of pain\nassessment can overcome the pitfalls of present-day pain assessment techniques.\nAll the previous studies are limited to controlled environment data. In this\nstudy, we evaluated the performance of OpenFace an open-source facial behavior\nanalysis tool and AU R-CNN on the real-world ICU data. Presence of assisted\nbreathing devices, variable lighting of ICUs, patient orientation with respect\nto camera significantly affected the performance of the models, although these\nshowed the state-of-the-art results in facial behavior analysis tasks. In this\nstudy, we show the need for automated pain assessment system which is trained\non real-world ICU data for clinically acceptable pain assessment system.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 17:12:56 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Nerella", "Subhash", ""], ["Bihorac", "Azra", ""], ["Tighe", "Patrick", ""], ["Rashidi", "Parisa", ""]]}, {"id": "2005.02122", "submitter": "Naeem Ul Islam", "authors": "Naeem Ul Islam, Sungmin Lee, and Jaebyung Park", "title": "Prominent Attribute Modification using Attribute Dependent Generative\n  Adversarial Network", "comments": null, "journal-ref": "Ubiquitous Robots 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modifying the facial images with desired attributes is important, though\nchallenging tasks in computer vision, where it aims to modify single or\nmultiple attributes of the face image. Some of the existing methods are either\nbased on attribute independent approaches where the modification is done in the\nlatent representation or attribute dependent approaches. The attribute\nindependent methods are limited in performance as they require the desired\npaired data for changing the desired attributes. Secondly, the attribute\nindependent constraint may result in the loss of information and, hence, fail\nin generating the required attributes in the face image. In contrast, the\nattribute dependent approaches are effective as these approaches are capable of\nmodifying the required features along with preserving the information in the\ngiven image. However, attribute dependent approaches are sensitive and require\na careful model design in generating high-quality results. To address this\nproblem, we propose an attribute dependent face modification approach. The\nproposed approach is based on two generators and two discriminators that\nutilize the binary as well as the real representation of the attributes and, in\nreturn, generate high-quality attribute modification results. Experiments on\nthe CelebA dataset show that our method effectively performs the multiple\nattribute editing with preserving other facial details intactly.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 13:38:05 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Islam", "Naeem Ul", ""], ["Lee", "Sungmin", ""], ["Park", "Jaebyung", ""]]}, {"id": "2005.02123", "submitter": "Yu-Kai Huang", "authors": "Yu-Kai Huang, Yueh-Cheng Liu, Tsung-Han Wu, Hung-Ting Su and Winston\n  H. Hsu", "title": "Expanding Sparse Guidance for Stereo Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of image based stereo estimation suffers from lighting\nvariations, repetitive patterns and homogeneous appearance. Moreover, to\nachieve good performance, stereo supervision requires sufficient\ndensely-labeled data, which are hard to obtain. In this work, we leverage small\namount of data with very sparse but accurate disparity cues from LiDAR to\nbridge the gap. We propose a novel sparsity expansion technique to expand the\nsparse cues concerning RGB images for local feature enhancement. The feature\nenhancement method can be easily applied to any stereo estimation algorithms\nwith cost volume at the test stage. Extensive experiments on stereo datasets\ndemonstrate the effectiveness and robustness across different backbones on\ndomain adaption and self-supervision scenario. Our sparsity expansion method\noutperforms previous methods in terms of disparity by more than 2 pixel error\non KITTI Stereo 2012 and 3 pixel error on KITTI Stereo 2015. Our approach\nsignificantly boosts the existing state-of-the-art stereo algorithms with\nextremely sparse cues.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 06:41:11 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Huang", "Yu-Kai", ""], ["Liu", "Yueh-Cheng", ""], ["Wu", "Tsung-Han", ""], ["Su", "Hung-Ting", ""], ["Hsu", "Winston H.", ""]]}, {"id": "2005.02130", "submitter": "Mahdi Zolnouri", "authors": "Mahdi Zolnouri and Xinlin Li and Vahid Partovi Nia", "title": "Importance of Data Loading Pipeline in Training Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large-scale deep neural networks is a long, time-consuming\noperation, often requiring many GPUs to accelerate. In large models, the time\nspent loading data takes a significant portion of model training time. As GPU\nservers are typically expensive, tricks that can save training time are\nvaluable.Slow training is observed especially on real-world applications where\nexhaustive data augmentation operations are required. Data augmentation\ntechniques include: padding, rotation, adding noise, down sampling, up\nsampling, etc. These additional operations increase the need to build an\nefficient data loading pipeline, and to explore existing tools to speed up\ntraining time. We focus on the comparison of two main tools designed for this\ntask, namely binary data format to accelerate data reading, and NVIDIA DALI to\naccelerate data augmentation. Our study shows improvement on the order of 20%\nto 40% if such dedicated tools are used.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 14:19:48 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Zolnouri", "Mahdi", ""], ["Li", "Xinlin", ""], ["Nia", "Vahid Partovi", ""]]}, {"id": "2005.02132", "submitter": "Xuanyu Yin", "authors": "Xuanyu Yin, Yoko Sasaki, Weimin Wang, Kentaro Shimizu", "title": "3D Object Detection Method Based on YOLO and K-Means for Image and Point\n  Clouds", "comments": "arXiv admin note: substantial text overlap with arXiv:2004.11465", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lidar based 3D object detection and classification tasks are essential for\nautonomous driving(AD). A lidar sensor can provide the 3D point cloud data\nreconstruction of the surrounding environment. However, real time detection in\n3D point clouds still needs a strong algorithmic. This paper proposes a 3D\nobject detection method based on point cloud and image which consists of there\nparts.(1)Lidar-camera calibration and undistorted image transformation.\n(2)YOLO-based detection and PointCloud extraction, (3)K-means based point cloud\nsegmentation and detection experiment test and evaluation in depth image. In\nour research, camera can capture the image to make the Real-time 2D object\ndetection by using YOLO, we transfer the bounding box to node whose function is\nmaking 3d object detection on point cloud data from Lidar. By comparing whether\n2D coordinate transferred from the 3D point is in the object bounding box or\nnot can achieve High-speed 3D object recognition function in GPU. The accuracy\nand precision get imporved after k-means clustering in point cloud. The speed\nof our detection method is a advantage faster than PointNet.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 04:32:36 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Yin", "Xuanyu", ""], ["Sasaki", "Yoko", ""], ["Wang", "Weimin", ""], ["Shimizu", "Kentaro", ""]]}, {"id": "2005.02133", "submitter": "Uche Osahor", "authors": "Uche Osahor, Hadi Kazemi, Ali Dabouei, Nasser Nasrabadi", "title": "Quality Guided Sketch-to-Photo Image Synthesis", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Facial sketches drawn by artists are widely used for visual identification\napplications and mostly by law enforcement agencies, but the quality of these\nsketches depend on the ability of the artist to clearly replicate all the key\nfacial features that could aid in capturing the true identity of a subject.\nRecent works have attempted to synthesize these sketches into plausible visual\nimages to improve visual recognition and identification. However, synthesizing\nphoto-realistic images from sketches proves to be an even more challenging\ntask, especially for sensitive applications such as suspect identification. In\nthis work, we propose a novel approach that adopts a generative adversarial\nnetwork that synthesizes a single sketch into multiple synthetic images with\nunique attributes like hair color, sex, etc. We incorporate a hybrid\ndiscriminator which performs attribute classification of multiple target\nattributes, a quality guided encoder that minimizes the perceptual\ndissimilarity of the latent space embedding of the synthesized and real image\nat different layers in the network and an identity preserving network that\nmaintains the identity of the synthesised image throughout the training\nprocess. Our approach is aimed at improving the visual appeal of the\nsynthesised images while incorporating multiple attribute assignment to the\ngenerator without compromising the identity of the synthesised image. We\nsynthesised sketches using XDOG filter for the CelebA, WVU Multi-modal and\nCelebA-HQ datasets and from an auxiliary generator trained on sketches from\nCUHK, IIT-D and FERET datasets. Our results are impressive compared to current\nstate of the art.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 16:00:01 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Osahor", "Uche", ""], ["Kazemi", "Hadi", ""], ["Dabouei", "Ali", ""], ["Nasrabadi", "Nasser", ""]]}, {"id": "2005.02134", "submitter": "Gibran Benitez-Garcia", "authors": "Gibran Benitez-Garcia, Jesus Olivares-Mercado, Gabriel Sanchez-Perez,\n  and Keiji Yanai", "title": "IPN Hand: A Video Dataset and Benchmark for Real-Time Continuous Hand\n  Gesture Recognition", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new benchmark dataset named IPN Hand with\nsufficient size, variety, and real-world elements able to train and evaluate\ndeep neural networks. This dataset contains more than 4,000 gesture samples and\n800,000 RGB frames from 50 distinct subjects. We design 13 different static and\ndynamic gestures focused on interaction with touchless screens. We especially\nconsider the scenario when continuous gestures are performed without transition\nstates, and when subjects perform natural movements with their hands as\nnon-gesture actions. Gestures were collected from about 30 diverse scenes, with\nreal-world variation in background and illumination. With our dataset, the\nperformance of three 3D-CNN models is evaluated on the tasks of isolated and\ncontinuous real-time HGR. Furthermore, we analyze the possibility of increasing\nthe recognition accuracy by adding multiple modalities derived from RGB frames,\ni.e., optical flow and semantic segmentation, while keeping the real-time\nperformance of the 3D-CNN model. Our empirical study also provides a comparison\nwith the publicly available nvGesture (NVIDIA) dataset. The experimental\nresults show that the state-of-the-art ResNext-101 model decreases about 30%\naccuracy when using our real-world dataset, demonstrating that the IPN Hand\ndataset can be used as a benchmark, and may help the community to step forward\nin the continuous HGR. Our dataset and pre-trained models used in the\nevaluation are publicly available at https://github.com/GibranBenitez/IPN-hand.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 08:52:32 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 14:50:42 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Benitez-Garcia", "Gibran", ""], ["Olivares-Mercado", "Jesus", ""], ["Sanchez-Perez", "Gabriel", ""], ["Yanai", "Keiji", ""]]}, {"id": "2005.02136", "submitter": "Hamish Nicholson", "authors": "Hamish Nicholson", "title": "Psychophysical Evaluation of Deep Re-Identification Models", "comments": "Submitted as a senior thesis at Harvard College", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian re-identification (ReID) is the task of continuously recognising\nthe sameindividual across time and camera views. Researchers of pedestrian ReID\nand theirGPUs spend enormous energy producing novel algorithms, challenging\ndatasets,and readily accessible tools to successfully improve results on\nstandard metrics.Yet practitioners in biometrics, surveillance, and autonomous\ndriving have not re-alized benefits that reflect these metrics. Different\ndetections, slight occlusions,changes in perspective, and other banal\nperturbations render the best neural net-works virtually useless. This work\nmakes two contributions. First, we introducethe ReID community to a budding\narea of computer vision research in model eval-uation. By adapting established\nprinciples of psychophysical evaluation from psy-chology, we can quantify the\nperformance degradation and begin research thatwill improve the utility of\npedestrian ReID models; not just their performance ontest sets. Second, we\nintroduce NuscenesReID, a challenging new ReID datasetdesigned to reflect the\nreal world autonomous vehicle conditions in which ReIDalgorithms are used. We\nshow that, despite performing well on existing ReIDdatasets, most models are\nnot robust to synthetic augmentations or to the morerealistic NuscenesReID\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 17:11:50 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Nicholson", "Hamish", ""]]}, {"id": "2005.02137", "submitter": "Taehyeong Kim", "authors": "Taehyeong Kim, Injune Hwang, Gi-Cheon Kang, Won-Seok Choi, Hyunseo\n  Kim, Byoung-Tak Zhang", "title": "Label Propagation Adaptive Resonance Theory for Semi-supervised\n  Continuous Learning", "comments": "5 pages, 2 figures, 1 table, accepted in ICASSP 2020", "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9054655", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning and continuous learning are fundamental paradigms\nfor human-level intelligence. To deal with real-world problems where labels are\nrarely given and the opportunity to access the same data is limited, it is\nnecessary to apply these two paradigms in a joined fashion. In this paper, we\npropose Label Propagation Adaptive Resonance Theory (LPART) for semi-supervised\ncontinuous learning. LPART uses an online label propagation mechanism to\nperform classification and gradually improves its accuracy as the observed data\naccumulates. We evaluated the proposed model on visual (MNIST, SVHN, CIFAR-10)\nand audio (NSynth) datasets by adjusting the ratio of the labeled and unlabeled\ndata. The accuracies are much higher when both labeled and unlabeled data are\nused, demonstrating the significant advantage of LPART in environments where\nthe data labels are scarce.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 09:12:56 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Kim", "Taehyeong", ""], ["Hwang", "Injune", ""], ["Kang", "Gi-Cheon", ""], ["Choi", "Won-Seok", ""], ["Kim", "Hyunseo", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "2005.02138", "submitter": "Nicholas Sharp", "authors": "Nicholas Sharp, Maks Ovsjanikov", "title": "PointTriNet: Learned Triangulation of 3D Point Sets", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers a new task in geometric deep learning: generating a\ntriangulation among a set of points in 3D space. We present PointTriNet, a\ndifferentiable and scalable approach enabling point set triangulation as a\nlayer in 3D learning pipelines. The method iteratively applies two neural\nnetworks: a classification network predicts whether a candidate triangle should\nappear in the triangulation, while a proposal network suggests additional\ncandidates. Both networks are structured as PointNets over nearby points and\ntriangles, using a novel triangle-relative input encoding. Since these learning\nproblems operate on local geometric data, our method is efficient and scalable,\nand generalizes to unseen shape categories. Our networks are trained in an\nunsupervised manner from a collection of shapes represented as point clouds. We\ndemonstrate the effectiveness of this approach for classical meshing tasks,\nrobustness to outliers, and as a component in end-to-end learning systems.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 01:58:35 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 12:37:01 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Sharp", "Nicholas", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "2005.02142", "submitter": "Guillermo Arturo Mart\\'inez-Mascorro Mr.", "authors": "Guillermo A. Mart\\'inez-Mascorro, Jos\\'e R. Abreu-Pederzini, Jos\\'e C.\n  Ortiz-Bayliss, Hugo Terashima-Mar\\'in", "title": "Suspicious Behavior Detection on Shoplifting Cases for Crime Prevention\n  by Using 3D Convolutional Neural Networks", "comments": null, "journal-ref": "Computation 2021, 9(2), 24", "doi": "10.3390/computation9020024", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crime generates significant losses, both human and economic. Every year,\nbillions of dollars are lost due to attacks, crimes, and scams. Surveillance\nvideo camera networks are generating vast amounts of data, and the surveillance\nstaff can not process all the information in real-time. The human sight has its\nlimitations, where the visual focus is among the most critical ones when\ndealing with surveillance. A crime can occur in a different screen segment or\non a distinct monitor, and the staff may not notice it. Our proposal focuses on\nshoplifting crimes by analyzing special situations that an average person will\nconsider as typical conditions, but may lead to a crime. While other approaches\nidentify the crime itself, we instead model suspicious behavior -- the one that\nmay occur before a person commits a crime -- by detecting precise segments of a\nvideo with a high probability to contain a shoplifting crime. By doing so, we\nprovide the staff with more opportunities to act and prevent crime. We\nimplemented a 3DCNN model as a video feature extractor and tested its\nperformance on a dataset composed of daily-action and shoplifting samples. The\nresults are encouraging since it correctly identifies 75% of the cases where a\ncrime is about to happen.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 22:06:16 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Mart\u00ednez-Mascorro", "Guillermo A.", ""], ["Abreu-Pederzini", "Jos\u00e9 R.", ""], ["Ortiz-Bayliss", "Jos\u00e9 C.", ""], ["Terashima-Mar\u00edn", "Hugo", ""]]}, {"id": "2005.02148", "submitter": "Mohammed Belkhatir", "authors": "Tracey K. M. Lee, Mohammed Belkhatir, Saeid Sanei", "title": "Vision-based techniques for gait recognition", "comments": null, "journal-ref": null, "doi": "10.1007/s11042-013-1574-x", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global security concerns have raised a proliferation of video surveillance\ndevices. Intelligent surveillance systems seek to discover possible threats\nautomatically and raise alerts. Being able to identify the surveyed object can\nhelp determine its threat level. The current generation of devices provide\ndigital video data to be analysed for time varying features to assist in the\nidentification process. Commonly, people queue up to access a facility and\napproach a video camera in full frontal view. In this environment, a variety of\nbiometrics are available - for example, gait which includes temporal features\nlike stride period. Gait can be measured unobtrusively at a distance. The video\ndata will also include face features, which are short-range biometrics. In this\nway, one can combine biometrics naturally using one set of data. In this paper\nwe survey current techniques of gait recognition and modelling with the\nenvironment in which the research was conducted. We also discuss in detail the\nissues arising from deriving gait data, such as perspective and occlusion\neffects, together with the associated computer vision challenges of reliable\ntracking of human movement. Then, after highlighting these issues and\nchallenges related to gait processing, we proceed to discuss the frameworks\ncombining gait with other biometrics. We then provide motivations for a novel\nparadigm in biometrics-based human recognition, i.e. the use of the\nfronto-normal view of gait as a far-range biometrics combined with biometrics\noperating at a near distance.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 06:33:01 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Lee", "Tracey K. M.", ""], ["Belkhatir", "Mohammed", ""], ["Sanei", "Saeid", ""]]}, {"id": "2005.02152", "submitter": "Jaya Sreevalsan-Nair", "authors": "Jaya Sreevalsan-Nair and Pragyan Mohapatra", "title": "Augmented Semantic Signatures of Airborne LiDAR Point Clouds for\n  Comparison", "comments": "18 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LiDAR point clouds provide rich geometric information, which is particularly\nuseful for the analysis of complex scenes of urban regions. Finding structural\nand semantic differences between two different three-dimensional point clouds,\nsay, of the same region but acquired at different time instances is an\nimportant problem. A comparison of point clouds involves computationally\nexpensive registration and segmentation. We are interested in capturing the\nrelative differences in the geometric uncertainty and semantic content of the\npoint cloud without the registration process. Hence, we propose an\norientation-invariant geometric signature of the point cloud, which integrates\nits probabilistic geometric and semantic classifications. We study different\nproperties of the geometric signature, which are an image-based encoding of\ngeometric uncertainty and semantic content. We explore different metrics to\ndetermine differences between these signatures, which in turn compare point\nclouds without performing point-to-point registration. Our results show that\nthe differences in the signatures corroborate with the geometric and semantic\ndifferences of the point clouds.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:27:07 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 04:32:43 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Sreevalsan-Nair", "Jaya", ""], ["Mohapatra", "Pragyan", ""]]}, {"id": "2005.02153", "submitter": "Yunlian Lv", "authors": "Yunlian Lv, Ning Xie, Yimin Shi, Zijiao Wang, and Heng Tao Shen", "title": "Improving Target-driven Visual Navigation with Attention on 3D Spatial\n  Relationships", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied artificial intelligence (AI) tasks shift from tasks focusing on\ninternet images to active settings involving embodied agents that perceive and\nact within 3D environments. In this paper, we investigate the target-driven\nvisual navigation using deep reinforcement learning (DRL) in 3D indoor scenes,\nwhose navigation task aims to train an agent that can intelligently make a\nseries of decisions to arrive at a pre-specified target location from any\npossible starting positions only based on egocentric views. However, most\nnavigation methods currently struggle against several challenging problems,\nsuch as data efficiency, automatic obstacle avoidance, and generalization.\nGeneralization problem means that agent does not have the ability to transfer\nnavigation skills learned from previous experience to unseen targets and\nscenes. To address these issues, we incorporate two designs into classic DRL\nframework: attention on 3D knowledge graph (KG) and target skill extension\n(TSE) module. On the one hand, our proposed method combines visual features and\n3D spatial representations to learn navigation policy. On the other hand, TSE\nmodule is used to generate sub-targets which allow agent to learn from\nfailures. Specifically, our 3D spatial relationships are encoded through\nrecently popular graph convolutional network (GCN). Considering the real world\nsettings, our work also considers open action and adds actionable targets into\nconventional navigation situations. Those more difficult settings are applied\nto test whether DRL agent really understand its task, navigating environment,\nand can carry out reasoning. Our experiments, performed in the AI2-THOR, show\nthat our model outperforms the baselines in both SR and SPL metrics, and\nimproves generalization ability across targets and scenes.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 08:46:38 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Lv", "Yunlian", ""], ["Xie", "Ning", ""], ["Shi", "Yimin", ""], ["Wang", "Zijiao", ""], ["Shen", "Heng Tao", ""]]}, {"id": "2005.02154", "submitter": "Xiu-Shen Wei", "authors": "Benyi Hu, Ren-Jie Song, Xiu-Shen Wei, Yazhou Yao, Xian-Sheng Hua, and\n  Yuehu Liu", "title": "PyRetri: A PyTorch-based Library for Unsupervised Image Retrieval by\n  Deep Convolutional Neural Networks", "comments": "Accepted by ACM Multimedia Conference 2020. PyRetri is open-source\n  and available at https://github.com/PyRetri/PyRetri", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress of applying deep learning methods to the field\nof content-based image retrieval, there has not been a software library that\ncovers these methods in a unified manner. In order to fill this gap, we\nintroduce PyRetri, an open source library for deep learning based unsupervised\nimage retrieval. The library encapsulates the retrieval process in several\nstages and provides functionality that covers various prominent methods for\neach stage. The idea underlying its design is to provide a unified platform for\ndeep learning based image retrieval research, with high usability and\nextensibility. To the best of our knowledge, this is the first open-source\nlibrary for unsupervised image retrieval by deep learning.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 10:17:18 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 13:12:10 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Hu", "Benyi", ""], ["Song", "Ren-Jie", ""], ["Wei", "Xiu-Shen", ""], ["Yao", "Yazhou", ""], ["Hua", "Xian-Sheng", ""], ["Liu", "Yuehu", ""]]}, {"id": "2005.02155", "submitter": "AKM Shahariar Azad Rabby", "authors": "Jannatul Ferdous, Suvrajit Karmaker, A K M Shahariar Azad Rabby, Syed\n  Akhter Hossain", "title": "MatriVasha: A Multipurpose Comprehensive Database for Bangla Handwritten\n  Compound Characters", "comments": "19 fig, 2 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  At present, recognition of the Bangla handwriting compound character has been\nan essential issue for many years. In recent years there have been\napplication-based researches in machine learning, and deep learning, which is\ngained interest, and most notably is handwriting recognition because it has a\ntremendous application such as Bangla OCR. MatrriVasha, the project which can\nrecognize Bangla, handwritten several compound characters. Currently, compound\ncharacter recognition is an important topic due to its variant application, and\nhelps to create old forms, and information digitization with reliability. But\nunfortunately, there is a lack of a comprehensive dataset that can categorize\nall types of Bangla compound characters. MatrriVasha is an attempt to align\ncompound character, and it's challenging because each person has a unique style\nof writing shapes. After all, MatrriVasha has proposed a dataset that intends\nto recognize Bangla 120(one hundred twenty) compound characters that consist of\n2552(two thousand five hundred fifty-two) isolated handwritten characters\nwritten unique writers which were collected from within Bangladesh. This\ndataset faced problems in terms of the district, age, and gender-based written\nrelated research because the samples were collected that includes a verity of\nthe district, age group, and the equal number of males, and females. As of now,\nour proposed dataset is so far the most extensive dataset for Bangla compound\ncharacters. It is intended to frame the acknowledgment technique for\nhandwritten Bangla compound character. In the future, this dataset will be made\npublicly available to help to widen the research.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 06:38:12 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 07:59:45 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Ferdous", "Jannatul", ""], ["Karmaker", "Suvrajit", ""], ["Rabby", "A K M Shahariar Azad", ""], ["Hossain", "Syed Akhter", ""]]}, {"id": "2005.02157", "submitter": "Mohammadhossein Toutiaee", "authors": "Mohammadhossein Toutiaee, Soheyla Amirian, John A. Miller, Sheng Li", "title": "Stereotype-Free Classification of Fictitious Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equal Opportunity and Fairness are receiving increasing attention in\nartificial intelligence. Stereotyping is another source of discrimination,\nwhich yet has been unstudied in literature. GAN-made faces would be exposed to\nsuch discrimination, if they are classified by human perception. It is possible\nto eliminate the human impact on fictitious faces classification task by the\nuse of statistical approaches. We present a novel approach through penalized\nregression to label stereotype-free GAN-generated synthetic unlabeled images.\nThe proposed approach aids labeling new data (fictitious output images) by\nminimizing a penalized version of the least squares cost function between\nrealistic pictures and target pictures.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 04:37:54 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Toutiaee", "Mohammadhossein", ""], ["Amirian", "Soheyla", ""], ["Miller", "John A.", ""], ["Li", "Sheng", ""]]}, {"id": "2005.02159", "submitter": "Karim Makki", "authors": "K. Makki, B. Borotikar, M. Garetier, S. Brochard, D. Ben Salem, F.\n  Rousseau", "title": "A fast and memory-efficient algorithm for smooth interpolation of\n  polyrigid transformations: application to human joint tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log Euclidean polyrigid registration framework provides a way to smoothly\nestimate and interpolate poly-rigid/affine transformations for which the\ninvertibility is guaranteed. This powerful and flexible mathematical framework\nis currently being used to track the human joint dynamics by first imposing\nbone rigidity constraints in order to synthetize the spatio-temporal joint\ndeformations later. However, since no closed-form exists, then a\ncomputationally expensive integration of ordinary differential equations (ODEs)\nis required to perform image registration using this framework. To tackle this\nproblem, the exponential map for solving these ODEs is computed using the\nscaling and squaring method in the literature. In this paper, we propose an\nalgorithm using a matrix diagonalization based method for smooth interpolation\nof homogeneous polyrigid transformations of human joints during motion. The use\nof this alternative computational approach to integrate ODEs is well motivated\nby the fact that bone rigid transformations satisfy the mechanical constraints\nof human joint motion, which provide conditions that guarantee the\ndiagonalizability of local bone transformations and consequently of the\nresulting joint transformations. In a comparison with the scaling and squaring\nmethod, we discuss the usefulness of the matrix eigendecomposition technique\nwhich reduces significantly the computational burden associated with the\ncomputation of matrix exponential over a dense regular grid. Finally, we have\napplied the method to enhance the temporal resolution of dynamic MRI sequences\nof the ankle joint. To conclude, numerical experiments show that the\neigendecomposition method is more capable of balancing the trade-off between\naccuracy, computation time, and memory requirements.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 14:30:40 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 12:20:52 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 18:51:57 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Makki", "K.", ""], ["Borotikar", "B.", ""], ["Garetier", "M.", ""], ["Brochard", "S.", ""], ["Salem", "D. Ben", ""], ["Rousseau", "F.", ""]]}, {"id": "2005.02160", "submitter": "Hailey James", "authors": "Hailey James, Otkrist Gupta, Dan Raviv", "title": "Printing and Scanning Attack for Image Counter Forensics", "comments": "10 pages, 5 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Examining the authenticity of images has become increasingly important as\nmanipulation tools become more accessible and advanced. Recent work has shown\nthat while CNN-based image manipulation detectors can successfully identify\nmanipulations, they are also vulnerable to adversarial attacks, ranging from\nsimple double JPEG compression to advanced pixel-based perturbation. In this\npaper we explore another method of highly plausible attack: printing and\nscanning. We demonstrate the vulnerability of two state-of-the-art models to\nthis type of attack. We also propose a new machine learning model that performs\ncomparably to these state-of-the-art models when trained and validated on\nprinted and scanned images. Of the three models, our proposed model outperforms\nthe others when trained and validated on images from a single printer. To\nfacilitate this exploration, we create a dataset of over 6,000 printed and\nscanned image blocks. Further analysis suggests that variation between images\nproduced from different printers is significant, large enough that good\nvalidation accuracy on images from one printer does not imply similar\nvalidation accuracy on identical images from a different printer.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 00:32:15 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 17:01:59 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["James", "Hailey", ""], ["Gupta", "Otkrist", ""], ["Raviv", "Dan", ""]]}, {"id": "2005.02162", "submitter": "Etienne David", "authors": "E. David, S. Madec, P. Sadeghi-Tehran, H. Aasen, B. Zheng, S. Liu, N.\n  Kirchgessner, G. Ishikawa, K. Nagasawa, M.A. Badhon, C. Pozniak, B. de Solan,\n  A. Hund, S.C. Chapman, F. Baret, I. Stavness, W. Guo", "title": "Global Wheat Head Detection (GWHD) dataset: a large and diverse dataset\n  of high resolution RGB labelled images to develop and benchmark wheat head\n  detection methods", "comments": "16 pages, 7 figures, Dataset paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of wheat heads is an important task allowing to estimate pertinent\ntraits including head population density and head characteristics such as\nsanitary state, size, maturity stage and the presence of awns. Several studies\ndeveloped methods for wheat head detection from high-resolution RGB imagery.\nThey are based on computer vision and machine learning and are generally\ncalibrated and validated on limited datasets. However, variability in\nobservational conditions, genotypic differences, development stages, head\norientation represents a challenge in computer vision. Further, possible\nblurring due to motion or wind and overlap between heads for dense populations\nmake this task even more complex. Through a joint international collaborative\neffort, we have built a large, diverse and well-labelled dataset, the Global\nWheat Head detection (GWHD) dataset. It contains 4,700 high-resolution RGB\nimages and 190,000 labelled wheat heads collected from several countries around\nthe world at different growth stages with a wide range of genotypes. Guidelines\nfor image acquisition, associating minimum metadata to respect FAIR principles\nand consistent head labelling methods are proposed when developing new head\ndetection datasets. The GWHD is publicly available at\nhttp://www.global-wheat.com/ and aimed at developing and benchmarking methods\nfor wheat head detection.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 14:20:26 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 07:34:36 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["David", "E.", ""], ["Madec", "S.", ""], ["Sadeghi-Tehran", "P.", ""], ["Aasen", "H.", ""], ["Zheng", "B.", ""], ["Liu", "S.", ""], ["Kirchgessner", "N.", ""], ["Ishikawa", "G.", ""], ["Nagasawa", "K.", ""], ["Badhon", "M. A.", ""], ["Pozniak", "C.", ""], ["de Solan", "B.", ""], ["Hund", "A.", ""], ["Chapman", "S. C.", ""], ["Baret", "F.", ""], ["Stavness", "I.", ""], ["Guo", "W.", ""]]}, {"id": "2005.02163", "submitter": "Anthony Bagnall Dr", "authors": "Anthony Bagnall, Paul Southam, James Large and Richard Harvey", "title": "Detecting Electric Devices in 3D Images of Bags", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aviation and transport security industries face the challenge of\nscreening high volumes of baggage for threats and contraband in the minimum\ntime possible. Automation and semi-automation of this procedure offers the\npotential to increase security by detecting more threats and improve the\ncustomer experience by speeding up the process. Traditional 2D x-ray images are\noften extremely difficult to examine due to the fact that they are tightly\npacked and contain a wide variety of cluttered and occluded objects. Because of\nthese limitations, major airports are introducing 3D x-ray Computed Tomography\n(CT) baggage scanning. We investigate whether we can automate the process of\ndetecting electric devices in these 3D images of luggage. Detecting electrical\ndevices is of particular concern as they can be used to conceal explosives.\nGiven the massive volume of luggage that needs to be screened for this threat,\nthe best way to automate the detection is to first filter whether a bag\ncontains an electric device or not, and if it does, to identify the number of\ndevices and their location. We present an algorithm, Unpack, Predict, eXtract,\nRepack (UXPR), which involves unpacking through segmenting the data at a range\nof scales using an algorithm known as the Sieve, predicting whether a segment\nis electrical or not based on the histogram of voxel intensities, then\nrepacking the bag by ensembling the segments and predictions to identify the\ndevices in bags. Through a range of experiments using data provided by ALERT\n(Awareness and Localization of Explosives-Related Threats) we show that this\nsystem can find a high proportion of devices with unsupervised segmentation if\na similar device has been seen before, and shows promising results for\ndetecting devices not seen at all based on the properties of its constituent\nparts.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 11:30:42 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Bagnall", "Anthony", ""], ["Southam", "Paul", ""], ["Large", "James", ""], ["Harvey", "Richard", ""]]}, {"id": "2005.02165", "submitter": "S.H. Shabbeer Basha", "authors": "S.H.Shabbeer Basha, Sravan Kumar Vinakota, Viswanath Pulabaigari,\n  Snehasis Mukherjee, Shiv Ram Dubey", "title": "AutoTune: Automatically Tuning Convolutional Neural Networks for\n  Improved Transfer Learning", "comments": "This paper is published in Neural Networks journal", "journal-ref": null, "doi": "10.1016/j.neunet.2020.10.009", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning enables solving a specific task having limited data by\nusing the pre-trained deep networks trained on large-scale datasets. Typically,\nwhile transferring the learned knowledge from source task to the target task,\nthe last few layers are fine-tuned (re-trained) over the target dataset.\nHowever, these layers are originally designed for the source task that might\nnot be suitable for the target task. In this paper, we introduce a mechanism\nfor automatically tuning the Convolutional Neural Networks (CNN) for improved\ntransfer learning. The pre-trained CNN layers are tuned with the knowledge from\ntarget data using Bayesian Optimization. First, we train the final layer of the\nbase CNN model by replacing the number of neurons in the softmax layer with the\nnumber of classes involved in the target task. Next, the pre-trained CNN is\ntuned automatically by observing the classification performance on the\nvalidation data (greedy criteria). To evaluate the performance of the proposed\nmethod, experiments are conducted on three benchmark datasets, e.g.,\nCalTech-101, CalTech-256, and Stanford Dogs. The classification results\nobtained through the proposed AutoTune method outperforms the standard baseline\ntransfer learning methods over the three datasets by achieving $95.92\\%$,\n$86.54\\%$, and $84.67\\%$ accuracy over CalTech-101, CalTech-256, and Stanford\nDogs, respectively. The experimental results obtained in this study depict that\ntuning of the pre-trained CNN layers with the knowledge from the target dataset\nconfesses better transfer learning ability. The source codes are available at\nhttps://github.com/JekyllAndHyde8999/AutoTune_CNN_TransferLearning.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 10:42:06 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 05:35:23 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Basha", "S. H. Shabbeer", ""], ["Vinakota", "Sravan Kumar", ""], ["Pulabaigari", "Viswanath", ""], ["Mukherjee", "Snehasis", ""], ["Dubey", "Shiv Ram", ""]]}, {"id": "2005.02166", "submitter": "Fariborz Taherkhani", "authors": "Fariborz Taherkhani, Veeru Talreja, Jeremy Dawson, Matthew C. Valenti,\n  and Nasser M. Nasrabadi", "title": "PF-cpGAN: Profile to Frontal Coupled GAN for Face Recognition in the\n  Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, due to the emergence of deep learning, face recognition has\nachieved exceptional success. However, many of these deep face recognition\nmodels perform relatively poorly in handling profile faces compared to frontal\nfaces. The major reason for this poor performance is that it is inherently\ndifficult to learn large pose invariant deep representations that are useful\nfor profile face recognition. In this paper, we hypothesize that the profile\nface domain possesses a gradual connection with the frontal face domain in the\ndeep feature space. We look to exploit this connection by projecting the\nprofile faces and frontal faces into a common latent space and perform\nverification or retrieval in the latent domain. We leverage a coupled\ngenerative adversarial network (cpGAN) structure to find the hidden\nrelationship between the profile and frontal images in a latent common\nembedding subspace. Specifically, the cpGAN framework consists of two GAN-based\nsub-networks, one dedicated to the frontal domain and the other dedicated to\nthe profile domain. Each sub-network tends to find a projection that maximizes\nthe pair-wise correlation between two feature domains in a common embedding\nfeature subspace. The efficacy of our approach compared with the\nstate-of-the-art is demonstrated using the CFP, CMU MultiPIE, IJB-A, and IJB-C\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 09:01:54 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Taherkhani", "Fariborz", ""], ["Talreja", "Veeru", ""], ["Dawson", "Jeremy", ""], ["Valenti", "Matthew C.", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2005.02167", "submitter": "Brian Goodwin", "authors": "Brian D Goodwin, Corey Jaskolski, Can Zhong, Herick Asmani", "title": "Intra-model Variability in COVID-19 Classification Using Chest X-ray\n  Images", "comments": "7 pages, 5 figures; Writing, analysis, and design carried out by\n  authors Brian and Corey; experiments carried out by authors Can and Herick;\n  results and code located at\n  https://github.com/synthetaic/COVID19-IntraModel-Variability and\n  https://covidresearch.ai/datasets/dataset?id=2", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  X-ray and computed tomography (CT) scanning technologies for COVID-19\nscreening have gained significant traction in AI research since the start of\nthe coronavirus pandemic. Despite these continuous advancements for COVID-19\nscreening, many concerns remain about model reliability when used in a clinical\nsetting. Much has been published, but with limited transparency in expected\nmodel performance. We set out to address this limitation through a set of\nexperiments to quantify baseline performance metrics and variability for\nCOVID-19 detection in chest x-ray for 12 common deep learning architectures.\nSpecifically, we adopted an experimental paradigm controlling for\ntrain-validation-test split and model architecture where the source of\nprediction variability originates from model weight initialization, random data\naugmentation transformations, and batch shuffling. Each model architecture was\ntrained 5 separate times on identical train-validation-test splits of a\npublicly available x-ray image dataset provided by Cohen et al. (2020). Results\nindicate that even within model architectures, model behavior varies in a\nmeaningful way between trained models. Best performing models achieve a false\nnegative rate of 3 out of 20 for detecting COVID-19 in a hold-out set. While\nthese results show promise in using AI for COVID-19 screening, they further\nsupport the urgent need for diverse medical imaging datasets for model training\nin a way that yields consistent prediction outcomes. It is our hope that these\nmodeling results accelerate work in building a more robust dataset and a viable\nscreening tool for COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 21:20:32 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Goodwin", "Brian D", ""], ["Jaskolski", "Corey", ""], ["Zhong", "Can", ""], ["Asmani", "Herick", ""]]}, {"id": "2005.02171", "submitter": "Amjad Rehman Dr", "authors": "Amjad Rehman (PSU and UTM)", "title": "Neural Computing for Online Arabic Handwriting Character Recognition\n  using Hard Stroke Features Mining", "comments": "16 pages", "journal-ref": "IJICIC 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Arabic cursive character recognition is still a big challenge due to\nthe existing complexities including Arabic cursive script styles, writing\nspeed, writer mood and so forth. Due to these unavoidable constraints, the\naccuracy of online Arabic character's recognition is still low and retain space\nfor improvement. In this research, an enhanced method of detecting the desired\ncritical points from vertical and horizontal direction-length of handwriting\nstroke features of online Arabic script recognition is proposed. Each extracted\nstroke feature divides every isolated character into some meaningful pattern\nknown as tokens. A minimum feature set is extracted from these tokens for\nclassification of characters using a multilayer perceptron with a\nback-propagation learning algorithm and modified sigmoid function-based\nactivation function. In this work, two milestones are achieved; firstly, attain\na fixed number of tokens, secondly, minimize the number of the most repetitive\ntokens. For experiments, handwritten Arabic characters are selected from the\nOHASD benchmark dataset to test and evaluate the proposed method. The proposed\nmethod achieves an average accuracy of 98.6% comparable in state of art\ncharacter recognition techniques.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 23:17:08 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 16:40:50 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2021 10:58:36 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Rehman", "Amjad", "", "PSU and UTM"]]}, {"id": "2005.02177", "submitter": "Yuanrui Dong", "authors": "Yuanrui Dong, Peng Zhao, Hanqiao Yu, Cong Zhao and Shusen Yang", "title": "CDC: Classification Driven Compression for Bandwidth Efficient\n  Edge-Cloud Collaborative Deep Learning", "comments": "Accepted by IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging edge-cloud collaborative Deep Learning (DL) paradigm aims at\nimproving the performance of practical DL implementations in terms of cloud\nbandwidth consumption, response latency, and data privacy preservation.\nFocusing on bandwidth efficient edge-cloud collaborative training of DNN-based\nclassifiers, we present CDC, a Classification Driven Compression framework that\nreduces bandwidth consumption while preserving classification accuracy of\nedge-cloud collaborative DL. Specifically, to reduce bandwidth consumption, for\nresource-limited edge servers, we develop a lightweight autoencoder with a\nclassification guidance for compression with classification driven feature\npreservation, which allows edges to only upload the latent code of raw data for\naccurate global training on the Cloud. Additionally, we design an adjustable\nquantization scheme adaptively pursuing the tradeoff between bandwidth\nconsumption and classification accuracy under different network conditions,\nwhere only fine-tuning is required for rapid compression ratio adjustment.\nResults of extensive experiments demonstrate that, compared with DNN training\nwith raw data, CDC consumes 14.9 times less bandwidth with an accuracy loss no\nmore than 1.06%, and compared with DNN training with data compressed by AE\nwithout guidance, CDC introduces at least 100% lower accuracy loss.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 07:40:32 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Dong", "Yuanrui", ""], ["Zhao", "Peng", ""], ["Yu", "Hanqiao", ""], ["Zhao", "Cong", ""], ["Yang", "Shusen", ""]]}, {"id": "2005.02181", "submitter": "Wei Ji Ma", "authors": "Wei Ji Ma and Benjamin Peters", "title": "A neural network walks into a lab: towards using deep nets as models for\n  human behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What might sound like the beginning of a joke has become an attractive\nprospect for many cognitive scientists: the use of deep neural network models\n(DNNs) as models of human behavior in perceptual and cognitive tasks. Although\nDNNs have taken over machine learning, attempts to use them as models of human\nbehavior are still in the early stages. Can they become a versatile model class\nin the cognitive scientist's toolbox? We first argue why DNNs have the\npotential to be interesting models of human behavior. We then discuss how that\npotential can be more fully realized. On the one hand, we argue that the cycle\nof training, testing, and revising DNNs needs to be revisited through the lens\nof the cognitive scientist's goals. Specifically, we argue that methods for\nassessing the goodness of fit between DNN models and human behavior have to\ndate been impoverished. On the other hand, cognitive science might have to\nstart using more complex tasks (including richer stimulus spaces), but doing so\nmight be beneficial for DNN-independent reasons as well. Finally, we highlight\navenues where traditional cognitive process models and DNNs may show productive\nsynergy.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 11:17:36 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Ma", "Wei Ji", ""], ["Peters", "Benjamin", ""]]}, {"id": "2005.02183", "submitter": "Weihua He", "authors": "Weihua He, YuJie Wu, Lei Deng, Guoqi Li, Haoyu Wang, Yang Tian, Wei\n  Ding, Wenhui Wang, Yuan Xie", "title": "Comparing SNNs and RNNs on Neuromorphic Vision Datasets: Similarities\n  and Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic data, recording frameless spike events, have attracted\nconsiderable attention for the spatiotemporal information components and the\nevent-driven processing fashion. Spiking neural networks (SNNs) represent a\nfamily of event-driven models with spatiotemporal dynamics for neuromorphic\ncomputing, which are widely benchmarked on neuromorphic data. Interestingly,\nresearchers in the machine learning community can argue that recurrent\n(artificial) neural networks (RNNs) also have the capability to extract\nspatiotemporal features although they are not event-driven. Thus, the question\nof \"what will happen if we benchmark these two kinds of models together on\nneuromorphic data\" comes out but remains unclear. In this work, we make a\nsystematic study to compare SNNs and RNNs on neuromorphic data, taking the\nvision datasets as a case study. First, we identify the similarities and\ndifferences between SNNs and RNNs (including the vanilla RNNs and LSTM) from\nthe modeling and learning perspectives. To improve comparability and fairness,\nwe unify the supervised learning algorithm based on backpropagation through\ntime (BPTT), the loss function exploiting the outputs at all timesteps, the\nnetwork structure with stacked fully-connected or convolutional layers, and the\nhyper-parameters during training. Especially, given the mainstream loss\nfunction used in RNNs, we modify it inspired by the rate coding scheme to\napproach that of SNNs. Furthermore, we tune the temporal resolution of datasets\nto test model robustness and generalization. At last, a series of contrast\nexperiments are conducted on two types of neuromorphic datasets: DVS-converted\n(N-MNIST) and DVS-captured (DVS Gesture).\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 10:19:37 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["He", "Weihua", ""], ["Wu", "YuJie", ""], ["Deng", "Lei", ""], ["Li", "Guoqi", ""], ["Wang", "Haoyu", ""], ["Tian", "Yang", ""], ["Ding", "Wei", ""], ["Wang", "Wenhui", ""], ["Xie", "Yuan", ""]]}, {"id": "2005.02186", "submitter": "Jingyi Shen", "authors": "Jingyi Shen, Han-Wei Shen", "title": "An Information-theoretic Visual Analysis Framework for Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success of Convolutional Neural Networks (CNNs) in Computer\nVision and Natural Language Processing, the working mechanism behind CNNs is\nstill under extensive discussions and research. Driven by a strong demand for\nthe theoretical explanation of neural networks, some researchers utilize\ninformation theory to provide insight into the black box model. However, to the\nbest of our knowledge, employing information theory to quantitatively analyze\nand qualitatively visualize neural networks has not been extensively studied in\nthe visualization community. In this paper, we combine information entropies\nand visualization techniques to shed light on how CNN works. Specifically, we\nfirst introduce a data model to organize the data that can be extracted from\nCNN models. Then we propose two ways to calculate entropy under different\ncircumstances. To provide a fundamental understanding of the basic building\nblocks of CNNs (e.g., convolutional layers, pooling layers, normalization\nlayers) from an information-theoretic perspective, we develop a visual analysis\nsystem, CNNSlicer. CNNSlicer allows users to interactively explore the amount\nof information changes inside the model. With case studies on the widely used\nbenchmark datasets (MNIST and CIFAR-10), we demonstrate the effectiveness of\nour system in opening the blackbox of CNNs.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 21:36:50 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Shen", "Jingyi", ""], ["Shen", "Han-Wei", ""]]}, {"id": "2005.02190", "submitter": "Antonino Furnari", "authors": "Antonino Furnari and Giovanni Maria Farinella", "title": "Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video", "comments": "arXiv admin note: substantial text overlap with arXiv:1905.09035", "journal-ref": "Published in IEEE Transaction on Pattern Analysis and Machine\n  Interaction, 2020", "doi": "10.1109/TPAMI.2020.2992889", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of egocentric action anticipation, i.e.,\npredicting what actions the camera wearer will perform in the near future and\nwhich objects they will interact with. Specifically, we contribute\nRolling-Unrolling LSTM, a learning architecture to anticipate actions from\negocentric videos. The method is based on three components: 1) an architecture\ncomprised of two LSTMs to model the sub-tasks of summarizing the past and\ninferring the future, 2) a Sequence Completion Pre-Training technique which\nencourages the LSTMs to focus on the different sub-tasks, and 3) a Modality\nATTention (MATT) mechanism to efficiently fuse multi-modal predictions\nperformed by processing RGB frames, optical flow fields and object-based\nfeatures. The proposed approach is validated on EPIC-Kitchens, EGTEA Gaze+ and\nActivityNet. The experiments show that the proposed architecture is\nstate-of-the-art in the domain of egocentric videos, achieving top performances\nin the 2019 EPIC-Kitchens egocentric action anticipation challenge. The\napproach also achieves competitive performance on ActivityNet with respect to\nmethods not based on unsupervised pre-training and generalizes to the tasks of\nearly action recognition and action recognition. To encourage research on this\nchallenging topic, we made our code, trained models, and pre-extracted features\navailable at our web page: http://iplab.dmi.unict.it/rulstm.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 14:13:41 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 13:56:58 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Furnari", "Antonino", ""], ["Farinella", "Giovanni Maria", ""]]}, {"id": "2005.02220", "submitter": "Cong Hung Mai", "authors": "Mai Cong Hung, Ryohei Nakatsu, Naoko Tosa, Takashi Kusumi, Koji\n  Koyamada", "title": "Learning of Art Style Using AI and Its Evaluation Based on Psychological\n  Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GANs (Generative adversarial networks) is a new AI technology that can\nperform deep learning with less training data and has the capability of\nachieving transformation between two image sets. Using GAN we have carried out\na comparison between several art sets with different art style. We have\nprepared several image sets; a flower photo set (A), an art image set (B1) of\nImpressionism drawings, an art image set of abstract paintings (B2), an art\nimage set of Chinese figurative paintings, (B3), and an art image set of\nabstract images (B4) created by Naoko Tosa, one of the authors. Transformation\nbetween set A to each of B was carried out using GAN and four image sets (B1,\nB2, B3, B4) was obtained. Using these four image sets we have carried out\npsychological experiment by asking subjects consisting of 23 students to fill\nin questionnaires. By analyzing the obtained questionnaires, we have found the\nfollowings. Abstract drawings and figurative drawings are clearly judged to be\ndifferent. Figurative drawings in West and East were judged to be similar.\nAbstract images by Naoko Tosa were judged as similar to Western abstract\nimages. These results show that AI could be used as an analysis tool to reveal\ndifferences between art genres.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 07:19:37 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Hung", "Mai Cong", ""], ["Nakatsu", "Ryohei", ""], ["Tosa", "Naoko", ""], ["Kusumi", "Takashi", ""], ["Koyamada", "Koji", ""]]}, {"id": "2005.02231", "submitter": "Deepta Rajan", "authors": "Deepta Rajan, Jayaraman J. Thiagarajan, Alexandros Karargyris,\n  Satyananda Kashyap", "title": "Self-Training with Improved Regularization for Sample-Efficient Chest\n  X-Ray Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated diagnostic assistants in healthcare necessitate accurate AI models\nthat can be trained with limited labeled data, can cope with severe class\nimbalances and can support simultaneous prediction of multiple disease\nconditions. To this end, we present a deep learning framework that utilizes a\nnumber of key components to enable robust modeling in such challenging\nscenarios. Using an important use-case in chest X-ray classification, we\nprovide several key insights on the effective use of data augmentation,\nself-training via distillation and confidence tempering for small data learning\nin medical imaging. Our results show that using 85% lesser labeled data, we can\nbuild predictive models that match the performance of classifiers trained in a\nlarge-scale data setting.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 02:36:00 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 18:46:26 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Rajan", "Deepta", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Karargyris", "Alexandros", ""], ["Kashyap", "Satyananda", ""]]}, {"id": "2005.02258", "submitter": "Huazhu Fu", "authors": "Huazhu Fu, Fei Li, Xu Sun, Xingxing Cao, Jingan Liao, Jose Ignacio\n  Orlando, Xing Tao, Yuexiang Li, Shihao Zhang, Mingkui Tan, Chenglang Yuan,\n  Cheng Bian, Ruitao Xie, Jiongcheng Li, Xiaomeng Li, Jing Wang, Le Geng,\n  Panming Li, Huaying Hao, Jiang Liu, Yan Kong, Yongyong Ren, Hrvoje Bogunovic,\n  Xiulan Zhang, Yanwu Xu", "title": "AGE Challenge: Angle Closure Glaucoma Evaluation in Anterior Segment\n  Optical Coherence Tomography", "comments": "Accepted to Medical Image Analysis (MedIA). AGE Challenge website at:\n  https://age.grand-challenge.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Angle closure glaucoma (ACG) is a more aggressive disease than open-angle\nglaucoma, where the abnormal anatomical structures of the anterior chamber\nangle (ACA) may cause an elevated intraocular pressure and gradually lead to\nglaucomatous optic neuropathy and eventually to visual impairment and\nblindness. Anterior Segment Optical Coherence Tomography (AS-OCT) imaging\nprovides a fast and contactless way to discriminate angle closure from open\nangle. Although many medical image analysis algorithms have been developed for\nglaucoma diagnosis, only a few studies have focused on AS-OCT imaging. In\nparticular, there is no public AS-OCT dataset available for evaluating the\nexisting methods in a uniform way, which limits progress in the development of\nautomated techniques for angle closure detection and assessment. To address\nthis, we organized the Angle closure Glaucoma Evaluation challenge (AGE), held\nin conjunction with MICCAI 2019. The AGE challenge consisted of two tasks:\nscleral spur localization and angle closure classification. For this challenge,\nwe released a large dataset of 4800 annotated AS-OCT images from 199 patients,\nand also proposed an evaluation framework to benchmark and compare different\nmodels. During the AGE challenge, over 200 teams registered online, and more\nthan 1100 results were submitted for online evaluation. Finally, eight teams\nparticipated in the onsite challenge. In this paper, we summarize these eight\nonsite challenge methods and analyze their corresponding results for the two\ntasks. We further discuss limitations and future directions. In the AGE\nchallenge, the top-performing approach had an average Euclidean Distance of 10\npixels (10um) in scleral spur localization, while in the task of angle closure\nclassification, all the algorithms achieved satisfactory performances, with two\nbest obtaining an accuracy rate of 100%.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 14:55:01 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 18:12:01 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 17:04:35 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Fu", "Huazhu", ""], ["Li", "Fei", ""], ["Sun", "Xu", ""], ["Cao", "Xingxing", ""], ["Liao", "Jingan", ""], ["Orlando", "Jose Ignacio", ""], ["Tao", "Xing", ""], ["Li", "Yuexiang", ""], ["Zhang", "Shihao", ""], ["Tan", "Mingkui", ""], ["Yuan", "Chenglang", ""], ["Bian", "Cheng", ""], ["Xie", "Ruitao", ""], ["Li", "Jiongcheng", ""], ["Li", "Xiaomeng", ""], ["Wang", "Jing", ""], ["Geng", "Le", ""], ["Li", "Panming", ""], ["Hao", "Huaying", ""], ["Liu", "Jiang", ""], ["Kong", "Yan", ""], ["Ren", "Yongyong", ""], ["Bogunovic", "Hrvoje", ""], ["Zhang", "Xiulan", ""], ["Xu", "Yanwu", ""]]}, {"id": "2005.02264", "submitter": "Adrian Boguszewski", "authors": "Adrian Boguszewski, Dominik Batorski, Natalia Ziemba-Jankowska, Tomasz\n  Dziedzic, Anna Zambrzycka", "title": "LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands,\n  Water and Roads from Aerial Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring of land cover and land use is crucial in natural resources\nmanagement. Automatic visual mapping can carry enormous economic value for\nagriculture, forestry, or public administration. Satellite or aerial images\ncombined with computer vision and deep learning enable precise assessment and\ncan significantly speed up change detection. Aerial imagery usually provides\nimages with much higher pixel resolution than satellite data allowing more\ndetailed mapping. However, there is still a lack of aerial datasets made for\nthe segmentation, covering rural areas with a resolution of tens centimeters\nper pixel, manual fine labels, and highly publicly important environmental\ninstances like buildings, woods, water, or roads.\n  Here we introduce LandCover.ai (Land Cover from Aerial Imagery) dataset for\nsemantic segmentation. We collected images of 216.27 sq. km rural areas across\nPoland, a country in Central Europe, 39.51 sq. km with resolution 50 cm per\npixel and 176.76 sq. km with resolution 25 cm per pixel and manually fine\nannotated four following classes of objects: buildings, woodlands, water, and\nroads. Additionally, we report simple benchmark results, achieving 85.56% of\nmean intersection over union on the test set. It proves that the automatic\nmapping of land cover is possible with a relatively small, cost-efficient,\nRGB-only dataset. The dataset is publicly available at https://landcover.ai\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 15:00:49 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 11:59:12 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 13:45:27 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Boguszewski", "Adrian", ""], ["Batorski", "Dominik", ""], ["Ziemba-Jankowska", "Natalia", ""], ["Dziedzic", "Tomasz", ""], ["Zambrzycka", "Anna", ""]]}, {"id": "2005.02269", "submitter": "Agnieszka Miko{\\l}ajczyk", "authors": "Agnieszka Miko{\\l}ajczyk, Micha{\\l} Grochowski, Arkadiusz Kwasigroch", "title": "Towards explainable classifiers using the counterfactual approach --\n  global explanations for discovering bias in data", "comments": "Accepted for publication in Journal of Artificial Intelligence and\n  Soft Computing Research; 12 pages, 4 figures, code available, 8-pages\n  appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes summarized attribution-based post-hoc explanations for the\ndetection and identification of bias in data. A global explanation is proposed,\nand a step-by-step framework on how to detect and test bias is introduced.\nSince removing unwanted bias is often a complicated and tremendous task, it is\nautomatically inserted, instead. Then, the bias is evaluated with the proposed\ncounterfactual approach. The obtained results are validated on a sample skin\nlesion dataset. Using the proposed method, a number of possible bias causing\nartifacts are successfully identified and confirmed in dermoscopy images. In\nparticular, it is confirmed that black frames have a strong influence on\nConvolutional Neural Network's prediction: 22% of them changed the prediction\nfrom benign to malignant.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 15:05:33 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 11:47:07 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Miko\u0142ajczyk", "Agnieszka", ""], ["Grochowski", "Micha\u0142", ""], ["Kwasigroch", "Arkadiusz", ""]]}, {"id": "2005.02291", "submitter": "Dario Fuoli", "authors": "Dario Fuoli, Zhiwu Huang, Martin Danelljan, Radu Timofte, Hua Wang,\n  Longcun Jin, Dewei Su, Jing Liu, Jaehoon Lee, Michal Kudelski, Lukasz Bala,\n  Dmitry Hrybov, Marcin Mozejko, Muchen Li, Siyao Li, Bo Pang, Cewu Lu, Chao\n  Li, Dongliang He, Fu Li, Shilei Wen", "title": "NTIRE 2020 Challenge on Video Quality Mapping: Methods and Results", "comments": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the NTIRE 2020 challenge on video quality mapping (VQM),\nwhich addresses the issues of quality mapping from source video domain to\ntarget video domain. The challenge includes both a supervised track (track 1)\nand a weakly-supervised track (track 2) for two benchmark datasets. In\nparticular, track 1 offers a new Internet video benchmark, requiring algorithms\nto learn the map from more compressed videos to less compressed videos in a\nsupervised training manner. In track 2, algorithms are required to learn the\nquality mapping from one device to another when their quality varies\nsubstantially and weakly-aligned video pairs are available. For track 1, in\ntotal 7 teams competed in the final test phase, demonstrating novel and\neffective solutions to the problem. For track 2, some existing methods are\nevaluated, showing promising solutions to the weakly-supervised video quality\nmapping problem.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 15:45:16 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 16:50:39 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 22:12:40 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Fuoli", "Dario", ""], ["Huang", "Zhiwu", ""], ["Danelljan", "Martin", ""], ["Timofte", "Radu", ""], ["Wang", "Hua", ""], ["Jin", "Longcun", ""], ["Su", "Dewei", ""], ["Liu", "Jing", ""], ["Lee", "Jaehoon", ""], ["Kudelski", "Michal", ""], ["Bala", "Lukasz", ""], ["Hrybov", "Dmitry", ""], ["Mozejko", "Marcin", ""], ["Li", "Muchen", ""], ["Li", "Siyao", ""], ["Pang", "Bo", ""], ["Lu", "Cewu", ""], ["Li", "Chao", ""], ["He", "Dongliang", ""], ["Li", "Fu", ""], ["Wen", "Shilei", ""]]}, {"id": "2005.02313", "submitter": "Sukrut Rao", "authors": "Sukrut Rao, David Stutz, Bernt Schiele", "title": "Adversarial Training against Location-Optimized Adversarial Patches", "comments": "20 pages, 6 tables, 4 figures, 2 algorithms, European Conference on\n  Computer Vision Workshops 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown to be susceptible to adversarial\nexamples -- small, imperceptible changes constructed to cause\nmis-classification in otherwise highly accurate image classifiers. As a\npractical alternative, recent work proposed so-called adversarial patches:\nclearly visible, but adversarially crafted rectangular patches in images. These\npatches can easily be printed and applied in the physical world. While defenses\nagainst imperceptible adversarial examples have been studied extensively,\nrobustness against adversarial patches is poorly understood. In this work, we\nfirst devise a practical approach to obtain adversarial patches while actively\noptimizing their location within the image. Then, we apply adversarial training\non these location-optimized adversarial patches and demonstrate significantly\nimproved robustness on CIFAR10 and GTSRB. Additionally, in contrast to\nadversarial training on imperceptible adversarial examples, our adversarial\npatch training does not reduce accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 16:17:00 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 08:00:26 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Rao", "Sukrut", ""], ["Stutz", "David", ""], ["Schiele", "Bernt", ""]]}, {"id": "2005.02315", "submitter": "Chenglong Li", "authors": "Zhengzheng Tu, Zhun Li, Chenglong Li, Yang Lang, Jin Tang", "title": "Multi-interactive Dual-decoder for RGB-thermal Salient Object Detection", "comments": "Accepted by IEEE TIP", "journal-ref": null, "doi": "10.1109/TIP.2021.3087412", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-thermal salient object detection (SOD) aims to segment the common\nprominent regions of visible image and corresponding thermal infrared image\nthat we call it RGBT SOD. Existing methods don't fully explore and exploit the\npotentials of complementarity of different modalities and multi-type cues of\nimage contents, which play a vital role in achieving accurate results. In this\npaper, we propose a multi-interactive dual-decoder to mine and model the\nmulti-type interactions for accurate RGBT SOD. In specific, we first encode two\nmodalities into multi-level multi-modal feature representations. Then, we\ndesign a novel dual-decoder to conduct the interactions of multi-level\nfeatures, two modalities and global contexts. With these interactions, our\nmethod works well in diversely challenging scenarios even in the presence of\ninvalid modality. Finally, we carry out extensive experiments on public RGBT\nand RGBD SOD datasets, and the results show that the proposed method achieves\nthe outstanding performance against state-of-the-art algorithms. The source\ncode has been released\nat:https://github.com/lz118/Multi-interactive-Dual-decoder.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 16:21:17 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 08:44:40 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 12:56:34 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Tu", "Zhengzheng", ""], ["Li", "Zhun", ""], ["Li", "Chenglong", ""], ["Lang", "Yang", ""], ["Tang", "Jin", ""]]}, {"id": "2005.02356", "submitter": "Shiqian Ma", "authors": "Shixiang Chen, Zengde Deng, Shiqian Ma, Anthony Man-Cho So", "title": "Manifold Proximal Point Algorithms for Dual Principal Component Pursuit\n  and Orthogonal Dictionary Learning", "comments": "Accepted in IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing the $\\ell_1$ norm of a linear map over\nthe sphere, which arises in various machine learning applications such as\northogonal dictionary learning (ODL) and robust subspace recovery (RSR). The\nproblem is numerically challenging due to its nonsmooth objective and nonconvex\nconstraint, and its algorithmic aspects have not been well explored. In this\npaper, we show how the manifold structure of the sphere can be exploited to\ndesign fast algorithms for tackling this problem. Specifically, our\ncontribution is threefold. First, we present a manifold proximal point\nalgorithm (ManPPA) for the problem and show that it converges at a sublinear\nrate. Furthermore, we show that ManPPA can achieve a quadratic convergence rate\nwhen applied to the ODL and RSR problems. Second, we propose a stochastic\nvariant of ManPPA called StManPPA, which is well suited for large-scale\ncomputation, and establish its sublinear convergence rate. Both ManPPA and\nStManPPA have provably faster convergence rates than existing subgradient-type\nmethods. Third, using ManPPA as a building block, we propose a new approach to\nsolving a matrix analog of the problem, in which the sphere is replaced by the\nStiefel manifold. The results from our extensive numerical experiments on the\nODL and RSR problems demonstrate the efficiency and efficacy of our proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 17:40:03 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 13:40:06 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Chen", "Shixiang", ""], ["Deng", "Zengde", ""], ["Ma", "Shiqian", ""], ["So", "Anthony Man-Cho", ""]]}, {"id": "2005.02357", "submitter": "Niv Cohen", "authors": "Niv Cohen and Yedid Hoshen", "title": "Sub-Image Anomaly Detection with Deep Pyramid Correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor (kNN) methods utilizing deep pre-trained features exhibit\nvery strong anomaly detection performance when applied to entire images. A\nlimitation of kNN methods is the lack of segmentation map describing where the\nanomaly lies inside the image. In this work we present a novel anomaly\nsegmentation approach based on alignment between an anomalous image and a\nconstant number of the similar normal images. Our method, Semantic Pyramid\nAnomaly Detection (SPADE) uses correspondences based on a multi-resolution\nfeature pyramid. SPADE is shown to achieve state-of-the-art performance on\nunsupervised anomaly detection and localization while requiring virtually no\ntraining time.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 17:43:35 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 18:52:41 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 16:28:51 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Cohen", "Niv", ""], ["Hoshen", "Yedid", ""]]}, {"id": "2005.02359", "submitter": "Yedid Hoshen", "authors": "Liron Bergman and Yedid Hoshen", "title": "Classification-Based Anomaly Detection for General Data", "comments": "ICLR'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection, finding patterns that substantially deviate from those\nseen previously, is one of the fundamental problems of artificial intelligence.\nRecently, classification-based methods were shown to achieve superior results\non this task. In this work, we present a unifying view and propose an open-set\nmethod, GOAD, to relax current generalization assumptions. Furthermore, we\nextend the applicability of transformation-based methods to non-image data\nusing random affine transformations. Our method is shown to obtain\nstate-of-the-art accuracy and is applicable to broad data types. The strong\nperformance of our method is extensively validated on multiple datasets from\ndifferent domains.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 17:44:40 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Bergman", "Liron", ""], ["Hoshen", "Yedid", ""]]}, {"id": "2005.02436", "submitter": "Hiroshi Sasaki", "authors": "Hiroshi Sasaki, Chris G. Willcocks, Toby P. Breckon", "title": "Data Augmentation via Mixed Class Interpolation using Cycle-Consistent\n  Generative Adversarial Networks Applied to Cross-Domain Imagery", "comments": "9 pages, 9 figures, accepted at the 25th International Conference on\n  Pattern Recognition (ICPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning driven object detection and classification within\nnon-visible imagery has an important role in many fields such as night vision,\nall-weather surveillance and aviation security. However, such applications\noften suffer due to the limited quantity and variety of non-visible spectral\ndomain imagery, in contrast to the high data availability of visible-band\nimagery that readily enables contemporary deep learning driven detection and\nclassification approaches. To address this problem, this paper proposes and\nevaluates a novel data augmentation approach that leverages the more readily\navailable visible-band imagery via a generative domain transfer model. The\nmodel can synthesise large volumes of non-visible domain imagery by\nimage-to-image (I2I) translation from the visible image domain. Furthermore, we\nshow that the generation of interpolated mixed class (non-visible domain) image\nexamples via our novel Conditional CycleGAN Mixup Augmentation (C2GMA)\nmethodology can lead to a significant improvement in the quality of non-visible\ndomain classification tasks that otherwise suffer due to limited data\navailability. Focusing on classification within the Synthetic Aperture Radar\n(SAR) domain, our approach is evaluated on a variation of the Statoil/C-CORE\nIceberg Classifier Challenge dataset and achieves 75.4% accuracy, demonstrating\na significant improvement when compared against traditional data augmentation\nstrategies (Rotation, Mixup, and MixCycleGAN).\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 18:53:38 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 22:29:13 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Sasaki", "Hiroshi", ""], ["Willcocks", "Chris G.", ""], ["Breckon", "Toby P.", ""]]}, {"id": "2005.02450", "submitter": "Abdullatif Baba", "authors": "Abdullatif Baba", "title": "Iris segmentation techniques to recognize the behavior of a vigilant\n  driver", "comments": null, "journal-ref": null, "doi": "10.1109/AECT47998.2020.9194159", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we clarify how to recognize different levels of vigilance for\nvehicle drivers. In order to avoid the classical problems of crisp logic, we\npreferred to employ a fuzzy logic-based system that depends on two variables to\nmake the final decision. Two iris segmentation techniques are well illustrated.\nA new technique for pupil position detection is also provided here with the\npossibility to correct the pupil detected position when dealing with some noisy\ncases.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 19:49:46 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Baba", "Abdullatif", ""]]}, {"id": "2005.02460", "submitter": "Abdullatif Baba", "authors": "Abdullatif Baba", "title": "A new design of a flying robot, with advanced computer vision techniques\n  to perform self-maintenance of smart grids", "comments": null, "journal-ref": null, "doi": "10.1016/j.jksuci.2020.07.009", "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a full design of a flying robot to investigate the\nstate of power grid components and to perform the appropriate maintenance\nprocedures according to each fail or defect that could be recognized. To\nrealize this purpose; different types of sensors including thermal and aerial\nvision-based systems are employed in this design. The main features and\ntechnical specifications of this robot are presented and discussed here in\ndetail. Some essential and advanced computer vision techniques are exploited in\nthis work to take some readings and measurements from the robot's surroundings.\nFrom each given image, many sub-images containing different electrical\ncomponents are extracted using a new region proposal approach that relies on\nDiscrete Wavelet Transform, to be classified later by utilizing a Convolutional\nNeural Network.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 20:06:32 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Baba", "Abdullatif", ""]]}, {"id": "2005.02463", "submitter": "Ramy Mounir", "authors": "Ramy Mounir, Roman Gula, J\\\"orn Theuerkauf, Sudeep Sarkar", "title": "Spatio-Temporal Event Segmentation and Localization for Wildlife\n  Extended Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Using offline training schemes, researchers have tackled the event\nsegmentation problem by providing full or weak-supervision through manually\nannotated labels or self-supervised epoch-based training. Most works consider\nvideos that are at most 10's of minutes long. We present a self-supervised\nperceptual prediction framework capable of temporal event segmentation by\nbuilding stable representations of objects over time and demonstrate it on long\nvideos, spanning several days. The approach is deceptively simple but quite\neffective. We rely on predictions of high-level features computed by a standard\ndeep learning backbone. For prediction, we use an LSTM, augmented with an\nattention mechanism, trained in a self-supervised manner using the prediction\nerror. The self-learned attention maps effectively localize and track the\nevent-related objects in each frame. The proposed approach does not require\nlabels. It requires only a single pass through the video, with no separate\ntraining set. Given the lack of datasets of very long videos, we demonstrate\nour method on video from 10 days (254 hours) of continuous wildlife monitoring\ndata that we had collected with required permissions. We find that the approach\nis robust to various environmental conditions such as day/night conditions,\nrain, sharp shadows, and windy conditions. For the task of temporally locating\nevents, we had an 80% recall rate at 20% false-positive rate for frame-level\nsegmentation. At the activity level, we had an 80% activity recall rate for one\nfalse activity detection every 50 minutes. We will make the dataset, which is\nthe first of its kind, and the code available to the research community.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 20:11:48 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 11:28:16 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 17:41:11 GMT"}, {"version": "v4", "created": "Sun, 18 Jul 2021 19:35:14 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Mounir", "Ramy", ""], ["Gula", "Roman", ""], ["Theuerkauf", "J\u00f6rn", ""], ["Sarkar", "Sudeep", ""]]}, {"id": "2005.02472", "submitter": "Alireza Zareian", "authors": "Manling Li, Alireza Zareian, Qi Zeng, Spencer Whitehead, Di Lu, Heng\n  Ji, Shih-Fu Chang", "title": "Cross-media Structured Common Space for Multimedia Event Extraction", "comments": "Accepted as an oral paper at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new task, MultiMedia Event Extraction (M2E2), which aims to\nextract events and their arguments from multimedia documents. We develop the\nfirst benchmark and collect a dataset of 245 multimedia news articles with\nextensively annotated events and arguments. We propose a novel method, Weakly\nAligned Structured Embedding (WASE), that encodes structured representations of\nsemantic information from textual and visual data into a common embedding\nspace. The structures are aligned across modalities by employing a weakly\nsupervised training strategy, which enables exploiting available resources\nwithout explicit cross-media annotation. Compared to uni-modal state-of-the-art\nmethods, our approach achieves 4.0% and 9.8% absolute F-score gains on text\nevent argument role labeling and visual event extraction. Compared to\nstate-of-the-art multimedia unstructured representations, we achieve 8.3% and\n5.0% absolute F-score gains on multimedia event extraction and argument role\nlabeling, respectively. By utilizing images, we extract 21.4% more event\nmentions than traditional text-only methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 20:21:53 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Li", "Manling", ""], ["Zareian", "Alireza", ""], ["Zeng", "Qi", ""], ["Whitehead", "Spencer", ""], ["Lu", "Di", ""], ["Ji", "Heng", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2005.02494", "submitter": "Kwot Sin Lee", "authors": "Kwot Sin Lee, Christopher Town", "title": "Mimicry: Towards the Reproducibility of GAN Research", "comments": "Accepted to the AI for Content Creation Workshop at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancing the state of Generative Adversarial Networks (GANs) research\nrequires one to make careful and accurate comparisons with existing works. Yet,\nthis is often difficult to achieve in practice when models are often\nimplemented differently using varying frameworks, and evaluated using different\nprocedures even when the same metric is used. To mitigate these issues, we\nintroduce Mimicry, a lightweight PyTorch library that provides implementations\nof popular state-of-the-art GANs and evaluation metrics to closely reproduce\nreported scores in the literature. We provide comprehensive baseline\nperformances of different GANs on seven widely-used datasets by training these\nGANs under the same conditions, and evaluating them across three popular GAN\nmetrics using the same procedures. The library can be found at\nhttps://github.com/kwotsin/mimicry.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 21:07:26 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Lee", "Kwot Sin", ""], ["Town", "Christopher", ""]]}, {"id": "2005.02523", "submitter": "Abdullah-Al-Zubaer Imran", "authors": "Abdullah-Al-Zubaer Imran, Chao Huang, Hui Tang, Wei Fan, Yuan Xiao,\n  Dingjun Hao, Zhen Qian, Demetri Terzopoulos", "title": "Partly Supervised Multitask Learning", "comments": "10 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning has recently been attracting attention as an\nalternative to fully supervised models that require large pools of labeled\ndata. Moreover, optimizing a model for multiple tasks can provide better\ngeneralizability than single-task learning. Leveraging self-supervision and\nadversarial training, we propose a novel general purpose semi-supervised,\nmultiple-task model---namely, self-supervised, semi-supervised, multitask\nlearning (S$^4$MTL)---for accomplishing two important tasks in medical imaging,\nsegmentation and diagnostic classification. Experimental results on chest and\nspine X-ray datasets suggest that our S$^4$MTL model significantly outperforms\nsemi-supervised single task, semi/fully-supervised multitask, and\nfully-supervised single task models, even with a 50\\% reduction of class and\nsegmentation labels. We hypothesize that our proposed model can be effective in\ntackling limited annotation problems for joint training, not only in medical\nimaging domains, but also for general-purpose vision tasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 22:42:12 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Imran", "Abdullah-Al-Zubaer", ""], ["Huang", "Chao", ""], ["Tang", "Hui", ""], ["Fan", "Wei", ""], ["Xiao", "Yuan", ""], ["Hao", "Dingjun", ""], ["Qian", "Zhen", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "2005.02545", "submitter": "Nachiket Deo", "authors": "Kaouther Messaoud, Nachiket Deo, Mohan M. Trivedi, Fawzi Nashashibi", "title": "Trajectory Prediction for Autonomous Driving based on Multi-Head\n  Attention with Joint Agent-Map Representation", "comments": "Revised submission for RA-L", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the trajectories of surrounding agents is an essential ability for\nautonomous vehicles navigating through complex traffic scenes. The future\ntrajectories of agents can be inferred using two important cues: the locations\nand past motion of agents, and the static scene structure. Due to the high\nvariability in scene structure and agent configurations, prior work has\nemployed the attention mechanism, applied separately to the scene and agent\nconfiguration to learn the most salient parts of both cues. However, the two\ncues are tightly linked. The agent configuration can inform what part of the\nscene is most relevant to prediction. The static scene in turn can help\ndetermine the relative influence of agents on each other's motion. Moreover,\nthe distribution of future trajectories is multimodal, with modes corresponding\nto the agent's intent. The agent's intent also informs what part of the scene\nand agent configuration is relevant to prediction. We thus propose a novel\napproach applying multi-head attention by considering a joint representation of\nthe static scene and surrounding agents. We use each attention head to generate\na distinct future trajectory to address multimodality of future trajectories.\nOur model achieves state of the art results on the nuScenes prediction\nbenchmark and generates diverse future trajectories compliant with scene\nstructure and agent configuration.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 00:39:45 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 03:12:03 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 22:41:33 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Messaoud", "Kaouther", ""], ["Deo", "Nachiket", ""], ["Trivedi", "Mohan M.", ""], ["Nashashibi", "Fawzi", ""]]}, {"id": "2005.02551", "submitter": "Ho Kei Cheng", "authors": "Ho Kei Cheng (HKUST), Jihoon Chung (HKUST), Yu-Wing Tai (Tencent),\n  Chi-Keung Tang (HKUST)", "title": "CascadePSP: Toward Class-Agnostic and Very High-Resolution Segmentation\n  via Global and Local Refinement", "comments": "Accepted to CVPR2020. Project page:\n  https://github.com/hkchengrex/CascadePSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art semantic segmentation methods were almost exclusively\ntrained on images within a fixed resolution range. These segmentations are\ninaccurate for very high-resolution images since using bicubic upsampling of\nlow-resolution segmentation does not adequately capture high-resolution details\nalong object boundaries. In this paper, we propose a novel approach to address\nthe high-resolution segmentation problem without using any high-resolution\ntraining data. The key insight is our CascadePSP network which refines and\ncorrects local boundaries whenever possible. Although our network is trained\nwith low-resolution segmentation data, our method is applicable to any\nresolution even for very high-resolution images larger than 4K. We present\nquantitative and qualitative studies on different datasets to show that\nCascadePSP can reveal pixel-accurate segmentation boundaries using our novel\nrefinement module without any finetuning. Thus, our method can be regarded as\nclass-agnostic. Finally, we demonstrate the application of our model to scene\nparsing in multi-class segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 01:38:03 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Cheng", "Ho Kei", "", "HKUST"], ["Chung", "Jihoon", "", "HKUST"], ["Tai", "Yu-Wing", "", "Tencent"], ["Tang", "Chi-Keung", "", "HKUST"]]}, {"id": "2005.02552", "submitter": "Shuya Ding", "authors": "Guanlin Li, Shuya Ding, Jun Luo, Chang Liu", "title": "Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder", "comments": null, "journal-ref": "CVPR 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas adversarial training is employed as the main defence strategy against\nspecific adversarial samples, it has limited generalization capability and\nincurs excessive time complexity. In this paper, we propose an attack-agnostic\ndefence framework to enhance the intrinsic robustness of neural networks,\nwithout jeopardizing the ability of generalizing clean samples. Our Feature\nPyramid Decoder (FPD) framework applies to all block-based convolutional neural\nnetworks (CNNs). It implants denoising and image restoration modules into a\ntargeted CNN, and it also constraints the Lipschitz constant of the\nclassification layer. Moreover, we propose a two-phase strategy to train the\nFPD-enhanced CNN, utilizing $\\epsilon$-neighbourhood noisy images with\nmulti-task and self-supervised learning. Evaluated against a variety of\nwhite-box and black-box attacks, we demonstrate that FPD-enhanced CNNs gain\nsufficient robustness against general adversarial samples on MNIST, SVHN and\nCALTECH. In addition, if we further conduct adversarial training, the\nFPD-enhanced CNNs perform better than their non-enhanced versions.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 01:40:26 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Li", "Guanlin", ""], ["Ding", "Shuya", ""], ["Luo", "Jun", ""], ["Liu", "Chang", ""]]}, {"id": "2005.02561", "submitter": "Romain Mormont", "authors": "Romain Mormont, Pierre Geurts, Rapha\\\"el Mar\\'ee", "title": "Multi-task pre-training of deep neural networks for digital pathology", "comments": "Accepted for publication in the IEEE Journal of Biomedical and Health\n  Informatics, special issue on Computational Pathology", "journal-ref": null, "doi": "10.1109/JBHI.2020.2992878", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate multi-task learning as a way of pre-training\nmodels for classification tasks in digital pathology. It is motivated by the\nfact that many small and medium-size datasets have been released by the\ncommunity over the years whereas there is no large scale dataset similar to\nImageNet in the domain. We first assemble and transform many digital pathology\ndatasets into a pool of 22 classification tasks and almost 900k images. Then,\nwe propose a simple architecture and training scheme for creating a\ntransferable model and a robust evaluation and selection protocol in order to\nevaluate our method. Depending on the target task, we show that our models used\nas feature extractors either improve significantly over ImageNet pre-trained\nmodels or provide comparable performance. Fine-tuning improves performance over\nfeature extraction and is able to recover the lack of specificity of ImageNet\nfeatures, as both pre-training sources yield comparable performance.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 08:50:17 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 08:16:31 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Mormont", "Romain", ""], ["Geurts", "Pierre", ""], ["Mar\u00e9e", "Rapha\u00ebl", ""]]}, {"id": "2005.02589", "submitter": "Anirudh Som", "authors": "Anirudh Som, Narayanan Krishnamurthi, Matthew Buman and Pavan Turaga", "title": "Unsupervised Pre-trained Models from Healthy ADLs Improve Parkinson's\n  Disease Classification of Gait Patterns", "comments": "Accepted in the 42nd Annual International Conferences of the IEEE\n  Engineering in Medicine and Biology Society (EMBC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application and use of deep learning algorithms for different healthcare\napplications is gaining interest at a steady pace. However, use of such\nalgorithms can prove to be challenging as they require large amounts of\ntraining data that capture different possible variations. This makes it\ndifficult to use them in a clinical setting since in most health applications\nresearchers often have to work with limited data. Less data can cause the deep\nlearning model to over-fit. In this paper, we ask how can we use data from a\ndifferent environment, different use-case, with widely differing data\ndistributions. We exemplify this use case by using single-sensor accelerometer\ndata from healthy subjects performing activities of daily living - ADLs (source\ndataset), to extract features relevant to multi-sensor accelerometer gait data\n(target dataset) for Parkinson's disease classification. We train the\npre-trained model using the source dataset and use it as a feature extractor.\nWe show that the features extracted for the target dataset can be used to train\nan effective classification model. Our pre-trained source model consists of a\nconvolutional autoencoder, and the target classification model is a simple\nmulti-layer perceptron model. We explore two different pre-trained source\nmodels, trained using different activity groups, and analyze the influence the\nchoice of pre-trained model has over the task of Parkinson's disease\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 04:08:19 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 00:56:01 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Som", "Anirudh", ""], ["Krishnamurthi", "Narayanan", ""], ["Buman", "Matthew", ""], ["Turaga", "Pavan", ""]]}, {"id": "2005.02591", "submitter": "Yuecong Xu", "authors": "Yuecong Xu, Jianfei Yang, Kezhi Mao, Jianxiong Yin and Simon See", "title": "Exploiting Inter-Frame Regional Correlation for Efficient Action\n  Recognition", "comments": "24 pages (exclude reference), 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal feature extraction is an important issue in video-based action\nrecognition. Optical flow is a popular method to extract temporal feature,\nwhich produces excellent performance thanks to its capacity of capturing\npixel-level correlation information between consecutive frames. However, such a\npixel-level correlation is extracted at the cost of high computational\ncomplexity and large storage resource. In this paper, we propose a novel\ntemporal feature extraction method, named Attentive Correlated Temporal Feature\n(ACTF), by exploring inter-frame correlation within a certain region. The\nproposed ACTF exploits both bilinear and linear correlation between successive\nframes on the regional level. Our method has the advantage of achieving\nperformance comparable to or better than optical flow-based methods while\navoiding the introduction of optical flow. Experimental results demonstrate our\nproposed method achieves the state-of-the-art performances of 96.3% on UCF101\nand 76.3% on HMDB51 benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 04:28:00 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Xu", "Yuecong", ""], ["Yang", "Jianfei", ""], ["Mao", "Kezhi", ""], ["Yin", "Jianxiong", ""], ["See", "Simon", ""]]}, {"id": "2005.02634", "submitter": "Kai Zhao", "authors": "Kai Zhao, Xin-Yu Zhang, Qi Han, and Ming-Ming Cheng", "title": "Dependency Aware Filter Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are typically over-parameterized,\nbringing considerable computational overhead and memory footprint in inference.\nPruning a proportion of unimportant filters is an efficient way to mitigate the\ninference cost. For this purpose, identifying unimportant convolutional filters\nis the key to effective filter pruning. Previous work prunes filters according\nto either their weight norms or the corresponding batch-norm scaling factors,\nwhile neglecting the sequential dependency between adjacent layers. In this\npaper, we further develop the norm-based importance estimation by taking the\ndependency between the adjacent layers into consideration. Besides, we propose\na novel mechanism to dynamically control the sparsity-inducing regularization\nso as to achieve the desired sparsity. In this way, we can identify unimportant\nfilters and search for the optimal network architecture within certain resource\nbudgets in a more principled manner. Comprehensive experimental results\ndemonstrate the proposed method performs favorably against the existing strong\nbaseline on the CIFAR, SVHN, and ImageNet datasets. The training sources will\nbe publicly available after the review process.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 07:41:22 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Zhao", "Kai", ""], ["Zhang", "Xin-Yu", ""], ["Han", "Qi", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2005.02641", "submitter": "Yiting Li", "authors": "Yiting Li, Yu Cheng, Lu Liu, Sichao Tian, Haiyue Zhu, Cheng Xiang,\n  Prahlad Vadakkepat, Cheksing Teo, and Tongheng Lee", "title": "Low-shot Object Detection via Classification Refinement", "comments": "Submitted to NIPS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to address the problem of low-shot object detection, where\nonly a few training samples are available for each category. Regarding the fact\nthat conventional fully supervised approaches usually suffer huge performance\ndrop with rare classes where data is insufficient, our study reveals that there\nexists more serious misalignment between classification confidence and\nlocalization accuracy on rarely labeled categories, and the prone to\noverfitting class-specific parameters is the crucial cause of this issue. In\nthis paper, we propose a novel low-shot classification correction network\n(LSCN) which can be adopted into any anchor-based detector to directly enhance\nthe detection accuracy on data-rare categories, without sacrificing the\nperformance on base categories. Specially, we sample false positive proposals\nfrom a base detector to train a separate classification correction network.\nDuring inference, the well-trained correction network removes false positives\nfrom the base detector. The proposed correction network is data-efficient yet\nhighly effective with four carefully designed components, which are Unified\nrecognition, Global receptive field, Inter-class separation, and Confidence\ncalibration. Experiments show our proposed method can bring significant\nperformance gains to rarely labeled categories and outperforms previous work on\nCOCO and PASCAL VOC by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 08:05:08 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Li", "Yiting", ""], ["Cheng", "Yu", ""], ["Liu", "Lu", ""], ["Tian", "Sichao", ""], ["Zhu", "Haiyue", ""], ["Xiang", "Cheng", ""], ["Vadakkepat", "Prahlad", ""], ["Teo", "Cheksing", ""], ["Lee", "Tongheng", ""]]}, {"id": "2005.02643", "submitter": "Wonsik Jung", "authors": "Wonsik Jung, Eunji Jun, Heung-Il Suk", "title": "Deep Recurrent Model for Individualized Prediction of Alzheimer's\n  Disease Progression", "comments": "17 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease (AD) is known as one of the major causes of dementia and\nis characterized by slow progression over several years, with no treatments or\navailable medicines. In this regard, there have been efforts to identify the\nrisk of developing AD in its earliest time. While many of the previous works\nconsidered cross-sectional analysis, more recent studies have focused on the\ndiagnosis and prognosis of AD with longitudinal or time series data in a way of\ndisease progression modeling (DPM). Under the same problem settings, in this\nwork, we propose a novel computational framework that can predict the\nphenotypic measurements of MRI biomarkers and trajectories of clinical status\nalong with cognitive scores at multiple future time points. However, in\nhandling time series data, it generally faces with many unexpected missing\nobservations. In regard to such an unfavorable situation, we define a secondary\nproblem of estimating those missing values and tackle it in a systematic way by\ntaking account of temporal and multivariate relations inherent in time series\ndata. Concretely, we propose a deep recurrent network that jointly tackles the\nfour problems of (i) missing value imputation, (ii) phenotypic measurements\nforecasting, (iii) trajectory estimation of the cognitive score, and (iv)\nclinical status prediction of a subject based on his/her longitudinal imaging\nbiomarkers. Notably, the learnable model parameters of our network are trained\nin an end-to-end manner with our circumspectly defined loss function. In our\nexperiments over TADPOLE challenge cohort, we measured performance for various\nmetrics and compared our method to competing methods in the literature.\nExhaustive analyses and ablation studies were also conducted to better confirm\nthe effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 08:08:00 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 11:28:42 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Jung", "Wonsik", ""], ["Jun", "Eunji", ""], ["Suk", "Heung-Il", ""]]}, {"id": "2005.02669", "submitter": "Anh Duc Le Dr.", "authors": "Anh Duc Le", "title": "Automated Transcription for Pre-Modern Japanese Kuzushiji Documents by\n  Random Lines Erasure and Curriculum Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing the full-page of Japanese historical documents is a challenging\nproblem due to the complex layout/background and difficulty of writing styles,\nsuch as cursive and connected characters. Most of the previous methods divided\nthe recognition process into character segmentation and recognition. However,\nthose methods provide only character bounding boxes and classes without text\ntranscription. In this paper, we enlarge our previous humaninspired recognition\nsystem from multiple lines to the full-page of Kuzushiji documents. The\nhuman-inspired recognition system simulates human eye movement during the\nreading process. For the lack of training data, we propose a random text line\nerasure approach that randomly erases text lines and distorts documents. For\nthe convergence problem of the recognition system for fullpage documents, we\nemploy curriculum learning that trains the recognition system step by step from\nthe easy level (several text lines of documents) to the difficult level\n(full-page documents). We tested the step training approach and random text\nline erasure approach on the dataset of the Kuzushiji recognition competition\non Kaggle. The results of the experiments demonstrate the effectiveness of our\nproposed approaches. These results are competitive with other participants of\nthe Kuzushiji recognition competition.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 09:17:28 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Le", "Anh Duc", ""]]}, {"id": "2005.02671", "submitter": "Marek Kowalski", "authors": "Marek Kowalski, Stephan J. Garbin, Virginia Estellers, Tadas\n  Baltru\\v{s}aitis, Matthew Johnson, Jamie Shotton", "title": "CONFIG: Controllable Neural Face Image Generation", "comments": "includes supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our ability to sample realistic natural images, particularly faces, has\nadvanced by leaps and bounds in recent years, yet our ability to exert\nfine-tuned control over the generative process has lagged behind. If this new\ntechnology is to find practical uses, we need to achieve a level of control\nover generative networks which, without sacrificing realism, is on par with\nthat seen in computer graphics and character animation. To this end we propose\nConfigNet, a neural face model that allows for controlling individual aspects\nof output images in semantically meaningful ways and that is a significant step\non the path towards finely-controllable neural rendering. ConfigNet is trained\non real face images as well as synthetic face renders. Our novel method uses\nsynthetic data to factorize the latent space into elements that correspond to\nthe inputs of a traditional rendering pipeline, separating aspects such as head\npose, facial expression, hair style, illumination, and many others which are\nvery hard to annotate in real data. The real images, which are presented to the\nnetwork without labels, extend the variety of the generated images and\nencourage realism. Finally, we propose an evaluation criterion using an\nattribute detection network combined with a user study and demonstrate\nstate-of-the-art individual control over attributes in the output images.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 09:19:46 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 15:10:21 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 10:13:56 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Kowalski", "Marek", ""], ["Garbin", "Stephan J.", ""], ["Estellers", "Virginia", ""], ["Baltru\u0161aitis", "Tadas", ""], ["Johnson", "Matthew", ""], ["Shotton", "Jamie", ""]]}, {"id": "2005.02690", "submitter": "Jiayu Huo", "authors": "Xi Ouyang, Jiayu Huo, Liming Xia, Fei Shan, Jun Liu, Zhanhao Mo, Fuhua\n  Yan, Zhongxiang Ding, Qi Yang, Bin Song, Feng Shi, Huan Yuan, Ying Wei,\n  Xiaohuan Cao, Yaozong Gao, Dijia Wu, Qian Wang, Dinggang Shen", "title": "Dual-Sampling Attention Network for Diagnosis of COVID-19 from Community\n  Acquired Pneumonia", "comments": "accepted by IEEE Transactions on Medical Imaging, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus disease (COVID-19) is rapidly spreading all over the world,\nand has infected more than 1,436,000 people in more than 200 countries and\nterritories as of April 9, 2020. Detecting COVID-19 at early stage is essential\nto deliver proper healthcare to the patients and also to protect the uninfected\npopulation. To this end, we develop a dual-sampling attention network to\nautomatically diagnose COVID- 19 from the community acquired pneumonia (CAP) in\nchest computed tomography (CT). In particular, we propose a novel online\nattention module with a 3D convolutional network (CNN) to focus on the\ninfection regions in lungs when making decisions of diagnoses. Note that there\nexists imbalanced distribution of the sizes of the infection regions between\nCOVID-19 and CAP, partially due to fast progress of COVID-19 after symptom\nonset. Therefore, we develop a dual-sampling strategy to mitigate the\nimbalanced learning. Our method is evaluated (to our best knowledge) upon the\nlargest multi-center CT data for COVID-19 from 8 hospitals. In the\ntraining-validation stage, we collect 2186 CT scans from 1588 patients for a\n5-fold cross-validation. In the testing stage, we employ another independent\nlarge-scale testing dataset including 2796 CT scans from 2057 patients. Results\nshow that our algorithm can identify the COVID-19 images with the area under\nthe receiver operating characteristic curve (AUC) value of 0.944, accuracy of\n87.5%, sensitivity of 86.9%, specificity of 90.1%, and F1-score of 82.0%. With\nthis performance, the proposed algorithm could potentially aid radiologists\nwith COVID-19 diagnosis from CAP, especially in the early stage of the COVID-19\noutbreak.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 09:56:51 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 03:43:05 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Ouyang", "Xi", ""], ["Huo", "Jiayu", ""], ["Xia", "Liming", ""], ["Shan", "Fei", ""], ["Liu", "Jun", ""], ["Mo", "Zhanhao", ""], ["Yan", "Fuhua", ""], ["Ding", "Zhongxiang", ""], ["Yang", "Qi", ""], ["Song", "Bin", ""], ["Shi", "Feng", ""], ["Yuan", "Huan", ""], ["Wei", "Ying", ""], ["Cao", "Xiaohuan", ""], ["Gao", "Yaozong", ""], ["Wu", "Dijia", ""], ["Wang", "Qian", ""], ["Shen", "Dinggang", ""]]}, {"id": "2005.02696", "submitter": "Li Wang", "authors": "Li Wang, Dawei Zhao, Tao Wu, Hao Fu, Zhiyu Wang, Liang Xiao, Xin Xu\n  and Bin Dai", "title": "Drosophila-Inspired 3D Moving Object Detection Based on Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D moving object detection is one of the most critical tasks in dynamic scene\nanalysis. In this paper, we propose a novel Drosophila-inspired 3D moving\nobject detection method using Lidar sensors. According to the theory of\nelementary motion detector, we have developed a motion detector based on the\nshallow visual neural pathway of Drosophila. This detector is sensitive to the\nmovement of objects and can well suppress background noise. Designing neural\ncircuits with different connection modes, the approach searches for motion\nareas in a coarse-to-fine fashion and extracts point clouds of each motion area\nto form moving object proposals. An improved 3D object detection network is\nthen used to estimate the point clouds of each proposal and efficiently\ngenerates the 3D bounding boxes and the object categories. We evaluate the\nproposed approach on the widely-used KITTI benchmark, and state-of-the-art\nperformance was obtained by using the proposed approach on the task of motion\ndetection.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 10:04:23 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Wang", "Li", ""], ["Zhao", "Dawei", ""], ["Wu", "Tao", ""], ["Fu", "Hao", ""], ["Wang", "Zhiyu", ""], ["Xiao", "Liang", ""], ["Xu", "Xin", ""], ["Dai", "Bin", ""]]}, {"id": "2005.02699", "submitter": "Jing Wu", "authors": "Jing Wu, Xiang Zhang, Mingyi Zhou, Ce Zhu", "title": "ProbaNet: Proposal-balanced Network for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Candidate object proposals generated by object detectors based on\nconvolutional neural network (CNN) encounter easy-hard samples imbalance\nproblem, which can affect overall performance. In this study, we propose a\nProposal-balanced Network (ProbaNet) for alleviating the imbalance problem.\nFirstly, ProbaNet increases the probability of choosing hard samples for\ntraining by discarding easy samples through threshold truncation. Secondly,\nProbaNet emphasizes foreground proposals by increasing their weights. To\nevaluate the effectiveness of ProbaNet, we train models based on different\nbenchmarks. Mean Average Precision (mAP) of the model using ProbaNet achieves\n1.2$\\%$ higher than the baseline on PASCAL VOC 2007. Furthermore, it is\ncompatible with existing two-stage detectors and offers a very small amount of\nadditional computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 10:07:39 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 10:32:55 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Wu", "Jing", ""], ["Zhang", "Xiang", ""], ["Zhou", "Mingyi", ""], ["Zhu", "Ce", ""]]}, {"id": "2005.02704", "submitter": "Sourya Dipta Das", "authors": "Aritra Mukherjee, Sourya Dipta Das, Jasorsi Ghosh, Ananda S.\n  Chowdhury, Sanjoy Kumar Saha", "title": "Fast Geometric Surface based Segmentation of Point Cloud from Lidar Data", "comments": "Accepted to PReMI 2019( Pattern Recognition and Machine Intelligence\n  2019). International Conference on Pattern Recognition and Machine\n  Intelligence. Springer, Cham, 2019", "journal-ref": null, "doi": "10.1007/978-3-030-34869-4_45", "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping the environment has been an important task for robot navigation and\nSimultaneous Localization And Mapping (SLAM). LIDAR provides a fast and\naccurate 3D point cloud map of the environment which helps in map building.\nHowever, processing millions of points in the point cloud becomes a\ncomputationally expensive task. In this paper, a methodology is presented to\ngenerate the segmented surfaces in real time and these can be used in modeling\nthe 3D objects. At first an algorithm is proposed for efficient map building\nfrom single shot data of spinning Lidar. It is based on fast meshing and\nsub-sampling. It exploits the physical design and the working principle of the\nspinning Lidar sensor. The generated mesh surfaces are then segmented by\nestimating the normal and considering their homogeneity. The segmented surfaces\ncan be used as proposals for predicting geometrically accurate model of objects\nin the robots activity environment. The proposed methodology is compared with\nsome popular point cloud segmentation methods to highlight the efficacy in\nterms of accuracy and speed.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 10:17:16 GMT"}], "update_date": "2020-06-14", "authors_parsed": [["Mukherjee", "Aritra", ""], ["Das", "Sourya Dipta", ""], ["Ghosh", "Jasorsi", ""], ["Chowdhury", "Ananda S.", ""], ["Saha", "Sanjoy Kumar", ""]]}, {"id": "2005.02706", "submitter": "Chen-Han Tsai", "authors": "Chen-Han Tsai, Nahum Kiryati, Eli Konen, Iris Eshed, Arnaldo Mayer", "title": "Knee Injury Detection using MRI with Efficiently-Layered Network (ELNet)", "comments": "11 pages, 4 figures, Accepted to the Medical Imaging and Deep\n  Learning (MIDL) Conference 2020", "journal-ref": "Proceedings of Machine Learning Research 121 (2020) 784-794", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is a widely-accepted imaging technique for\nknee injury analysis. Its advantage of capturing knee structure in three\ndimensions makes it the ideal tool for radiologists to locate potential tears\nin the knee. In order to better confront the ever growing workload of\nmusculoskeletal (MSK) radiologists, automated tools for patients' triage are\nbecoming a real need, reducing delays in the reading of pathological cases. In\nthis work, we present the Efficiently-Layered Network (ELNet), a convolutional\nneural network (CNN) architecture optimized for the task of initial knee MRI\ndiagnosis for triage. Unlike past approaches, we train ELNet from scratch\ninstead of using a transfer-learning approach. The proposed method is validated\nquantitatively and qualitatively, and compares favorably against\nstate-of-the-art MRNet while using a single imaging stack (axial or coronal) as\ninput. Additionally, we demonstrate our model's capability to locate tears in\nthe knee despite the absence of localization information during training.\nLastly, the proposed model is extremely lightweight ($<$ 1MB) and therefore\neasy to train and deploy in real clinical settings. The code for our model is\nprovided at: https://github.com/mxtsai/ELNet.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 10:21:16 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 20:42:58 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 08:14:54 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Tsai", "Chen-Han", ""], ["Kiryati", "Nahum", ""], ["Konen", "Eli", ""], ["Eshed", "Iris", ""], ["Mayer", "Arnaldo", ""]]}, {"id": "2005.02730", "submitter": "Firas Laakom", "authors": "Firas Laakom, Jenni Raitoharju, Alexandros Iosifidis, Uygar Tuna,\n  Jarno Nikkanen and Moncef Gabbouj", "title": "Probabilistic Color Constancy", "comments": "5 pages, 1 figure", "journal-ref": "2020 IEEE International Conference on Image Processing (ICIP)", "doi": "10.1109/ICIP40778.2020.9190893", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel unsupervised color constancy method, called\nProbabilistic Color Constancy (PCC). We define a framework for estimating the\nillumination of a scene by weighting the contribution of different image\nregions using a graph-based representation of the image. To estimate the weight\nof each (super-)pixel, we rely on two assumptions: (Super-)pixels with similar\ncolors contribute similarly and darker (super-)pixels contribute less. The\nresulting system has one global optimum solution. The proposed method achieves\ncompetitive performance, compared to the state-of-the-art, on INTEL-TAU\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 11:03:05 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Laakom", "Firas", ""], ["Raitoharju", "Jenni", ""], ["Iosifidis", "Alexandros", ""], ["Tuna", "Uygar", ""], ["Nikkanen", "Jarno", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2005.02760", "submitter": "Jan Egger", "authors": "Alexander Prutsch, Antonio Pepe, Jan Egger", "title": "Design and Development of a Web-based Tool for Inpainting of Dissected\n  Aortae in Angiography Images", "comments": "9 figures, 14 references", "journal-ref": "The 24th Central European Seminar on Computer Graphics (CESCG),\n  pp. 1-8, May. 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging is an important tool for the diagnosis and the evaluation of\nan aortic dissection (AD); a serious condition of the aorta, which could lead\nto a life-threatening aortic rupture. AD patients need life-long medical\nmonitoring of the aortic enlargement and of the disease progression, subsequent\nto the diagnosis of the aortic dissection. Since there is a lack of\n'healthy-dissected' image pairs from medical studies, the application of\ninpainting techniques offers an alternative source for generating them by doing\na virtual regression from dissected aortae to healthy aortae; an indirect way\nto study the origin of the disease. The proposed inpainting tool combines a\nneural network, which was trained on the task of inpainting aortic dissections,\nwith an easy-to-use user interface. To achieve this goal, the inpainting tool\nhas been integrated within the 3D medical image viewer of StudierFenster\n(www.studierfenster.at). By designing the tool as a web application, we\nsimplify the usage of the neural network and reduce the initial learning curve.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 12:22:21 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Prutsch", "Alexander", ""], ["Pepe", "Antonio", ""], ["Egger", "Jan", ""]]}, {"id": "2005.02790", "submitter": "Naiyan Wang", "authors": "Hao He, Hengchen Dai, Naiyan Wang", "title": "UST: Unifying Spatio-Temporal Context for Trajectory Prediction in\n  Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory prediction has always been a challenging problem for autonomous\ndriving, since it needs to infer the latent intention from the behaviors and\ninteractions from traffic participants. This problem is intrinsically hard,\nbecause each participant may behave differently under different environments\nand interactions. This key is to effectively model the interlaced influence\nfrom both spatial context and temporal context. Existing work usually encodes\nthese two types of context separately, which would lead to inferior modeling of\nthe scenarios. In this paper, we first propose a unified approach to treat time\nand space dimensions equally for modeling spatio-temporal context. The proposed\nmodule is simple and easy to implement within several lines of codes. In\ncontrast to existing methods which heavily rely on recurrent neural network for\ntemporal context and hand-crafted structure for spatial context, our method\ncould automatically partition the spatio-temporal space to adapt the data.\nLastly, we test our proposed framework on two recently proposed trajectory\nprediction dataset ApolloScape and Argoverse. We show that the proposed method\nsubstantially outperforms the previous state-of-the-art methods while\nmaintaining its simplicity. These encouraging results further validate the\nsuperiority of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 13:02:57 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["He", "Hao", ""], ["Dai", "Hengchen", ""], ["Wang", "Naiyan", ""]]}, {"id": "2005.02818", "submitter": "Wei Xiong", "authors": "Wei Xiong, Ding Liu, Xiaohui Shen, Chen Fang, Jiebo Luo", "title": "Unsupervised Real-world Low-light Image Enhancement with Decoupled\n  Networks", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional learning-based approaches to low-light image enhancement\ntypically require a large amount of paired training data, which are difficult\nto acquire in real-world scenarios. Recently, unsupervised models for this task\nhave been explored to eliminate the use of paired data. However, these methods\nprimarily tackle the problem of illumination enhancement, and usually fail to\nsuppress the noises that ubiquitously exist in images taken under real-world\nlow-light conditions. In this paper, we address the real-world low-light image\nenhancement problem by decoupling this task into two sub-tasks: illumination\nenhancement and noise suppression. We propose to learn a two-stage GAN-based\nframework to enhance the real-world low-light images in a fully unsupervised\nfashion. In addition to conventional benchmark datasets, a new unpaired\nlow-light image enhancement dataset is built and used to thoroughly evaluate\nthe performance of our model. Extensive experiments show that our method\noutperforms the state-of-the-art unsupervised image enhancement methods in\nterms of both illumination enhancement and noise reduction.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 13:37:08 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Xiong", "Wei", ""], ["Liu", "Ding", ""], ["Shen", "Xiaohui", ""], ["Fang", "Chen", ""], ["Luo", "Jiebo", ""]]}, {"id": "2005.02832", "submitter": "Ngo Le Huy Hien Mr.", "authors": "Nguyen Van Hieu, Ngo Le Huy Hien", "title": "Automatic Plant Image Identification of Vietnamese species using Deep\n  Learning Models", "comments": "7 pages, 8 figures, 2 tables, Published with International Journal of\n  Engineering Trends and Technology (IJETT)", "journal-ref": "International Journal of Engineering Trends and Technology\n  68.4(2020):25-31. Published by Seventh Sense Research Group", "doi": "10.14445/22315381/IJETT-V68I4P205S", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is complicated to distinguish among thousands of plant species in the\nnatural ecosystem, and many efforts have been investigated to address the\nissue. In Vietnam, the task of identifying one from 12,000 species requires\nspecialized experts in flora management, with thorough training skills and\nin-depth knowledge. Therefore, with the advance of machine learning, automatic\nplant identification systems have been proposed to benefit various\nstakeholders, including botanists, pharmaceutical laboratories, taxonomists,\nforestry services, and organizations. The concept has fueled an interest in\nresearch and application from global researchers and engineers in both fields\nof machine learning and computer vision. In this paper, the Vietnamese plant\nimage dataset was collected from an online encyclopedia of Vietnamese\norganisms, together with the Encyclopedia of Life, to generate a total of\n28,046 environmental images of 109 plant species in Vietnam. A comparative\nevaluation of four deep convolutional feature extraction models, which are\nMobileNetV2, VGG16, ResnetV2, and Inception Resnet V2, is presented. Those\nmodels have been tested on the Support Vector Machine (SVM) classifier to\nexperiment with the purpose of plant image identification. The proposed models\nachieve promising recognition rates, and MobilenetV2 attained the highest with\n83.9%. This result demonstrates that machine learning models are potential for\nplant species identification in the natural environment, and future works need\nto examine proposing higher accuracy systems on a larger dataset to meet the\ncurrent application demand.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 09:59:10 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Van Hieu", "Nguyen", ""], ["Hien", "Ngo Le Huy", ""]]}, {"id": "2005.02870", "submitter": "Toshiaki Koike-Akino", "authors": "Toshiaki Koike-Akino and Ye Wang", "title": "Stochastic Bottleneck: Rateless Auto-Encoder for Flexible Dimensionality\n  Reduction", "comments": "14 pages, 12 figures, ISIT 2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new concept of rateless auto-encoders (RL-AEs) that enable a\nflexible latent dimensionality, which can be seamlessly adjusted for varying\ndistortion and dimensionality requirements. In the proposed RL-AEs, instead of\na deterministic bottleneck architecture, we use an over-complete representation\nthat is stochastically regularized with weighted dropouts, in a manner\nanalogous to sparse AE (SAE). Unlike SAEs, our RL-AEs employ monotonically\nincreasing dropout rates across the latent representation nodes such that the\nlatent variables become sorted by importance like in principal component\nanalysis (PCA). This is motivated by the rateless property of conventional PCA,\nwhere the least important principal components can be discarded to realize\nvariable rate dimensionality reduction that gracefully degrades the distortion.\nIn contrast, since the latent variables of conventional AEs are equally\nimportant for data reconstruction, they cannot be simply discarded to further\nreduce the dimensionality after the AE model is trained. Our proposed\nstochastic bottleneck framework enables seamless rate adaptation with high\nreconstruction performance, without requiring predetermined latent\ndimensionality at training. We experimentally demonstrate that the proposed\nRL-AEs can achieve variable dimensionality reduction while achieving low\ndistortion compared to conventional AEs.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 14:47:42 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Koike-Akino", "Toshiaki", ""], ["Wang", "Ye", ""]]}, {"id": "2005.02903", "submitter": "Hassan Mansour", "authors": "Ajinkya Kadu and Hassan Mansour and Petros T. Boufounos", "title": "High-Contrast Reflection Tomography with Total-Variation Constraints", "comments": null, "journal-ref": "IEEE Transactions on Computational Imaging, vol. 6, pp. 1523-1536,\n  2020", "doi": "10.1109/TCI.2020.3038171", "report-no": null, "categories": "eess.SP cs.CE cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse scattering is the process of estimating the spatial distribution of\nthe scattering potential of an object by measuring the scattered wavefields\naround it. In this paper, we consider reflection tomography of high contrast\nobjects that commonly occurs in ground-penetrating radar, exploration\ngeophysics, terahertz imaging, ultrasound, and electron microscopy. Unlike\nconventional transmission tomography, the reflection regime is severely\nill-posed since the measured wavefields contain far less spatial frequency\ninformation of the target object. We propose a constrained incremental\nfrequency inversion framework that requires no side information from a\nbackground model of the object. Our framework solves a sequence of regularized\nleast-squares subproblems that ensure consistency with the measured scattered\nwavefield while imposing total-variation and non-negativity constraints. We\npropose a proximal Quasi-Newton method to solve the resulting subproblem and\ndevise an automatic parameter selection routine to determine the constraint of\neach subproblem. We validate the performance of our approach on synthetic\nlow-resolution phantoms and with a mismatched forward model test on a\nhigh-resolution phantom.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 15:26:55 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 14:44:29 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Kadu", "Ajinkya", ""], ["Mansour", "Hassan", ""], ["Boufounos", "Petros T.", ""]]}, {"id": "2005.02905", "submitter": "Gullal Singh Cheema", "authors": "Gullal Singh Cheema, Saket Anand", "title": "Automatic Detection and Recognition of Individuals in Patterned Species", "comments": "12 pages, ECML-PKDD 2017", "journal-ref": null, "doi": "10.1007/978-3-319-71273-4_3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual animal biometrics is rapidly gaining popularity as it enables a\nnon-invasive and cost-effective approach for wildlife monitoring applications.\nWidespread usage of camera traps has led to large volumes of collected images,\nmaking manual processing of visual content hard to manage. In this work, we\ndevelop a framework for automatic detection and recognition of individuals in\ndifferent patterned species like tigers, zebras and jaguars. Most existing\nsystems primarily rely on manual input for localizing the animal, which does\nnot scale well to large datasets. In order to automate the detection process\nwhile retaining robustness to blur, partial occlusion, illumination and pose\nvariations, we use the recently proposed Faster-RCNN object detection framework\nto efficiently detect animals in images. We further extract features from\nAlexNet of the animal's flank and train a logistic regression (or Linear SVM)\nclassifier to recognize the individuals. We primarily test and evaluate our\nframework on a camera trap tiger image dataset that contains images that vary\nin overall image quality, animal pose, scale and lighting. We also evaluate our\nrecognition system on zebra and jaguar images to show generalization to other\npatterned species. Our framework gives perfect detection results in camera\ntrapped tiger images and a similar or better individual recognition performance\nwhen compared with state-of-the-art recognition techniques.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 15:29:21 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Cheema", "Gullal Singh", ""], ["Anand", "Saket", ""]]}, {"id": "2005.02933", "submitter": "Mikael Brudfors", "authors": "Mikael Brudfors, Ya\\\"el Balbastre, John Ashburner", "title": "Groupwise Multimodal Image Registration using Joint Total Variation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-52791-4_15", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical imaging it is common practice to acquire a wide range of\nmodalities (MRI, CT, PET, etc.), to highlight different structures or\npathologies. As patient movement between scans or scanning session is\nunavoidable, registration is often an essential step before any subsequent\nimage analysis. In this paper, we introduce a cost function based on joint\ntotal variation for such multimodal image registration. This cost function has\nthe advantage of enabling principled, groupwise alignment of multiple images,\nwhilst being insensitive to strong intensity non-uniformities. We evaluate our\nalgorithm on rigidly aligning both simulated and real 3D brain scans. This\nvalidation shows robustness to strong intensity non-uniformities and low\nregistration errors for CT/PET to MRI alignment. Our implementation is publicly\navailable at https://github.com/brudfors/coregistration-njtv.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 16:11:32 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Brudfors", "Mikael", ""], ["Balbastre", "Ya\u00ebl", ""], ["Ashburner", "John", ""]]}, {"id": "2005.02936", "submitter": "Ankita Shukla", "authors": "Ankita Shukla, Pavan Turaga and Saket Anand", "title": "GraCIAS: Grassmannian of Corrupted Images for Adversarial Security", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Input transformation based defense strategies fall short in defending against\nstrong adversarial attacks. Some successful defenses adopt approaches that\neither increase the randomness within the applied transformations, or make the\ndefense computationally intensive, making it substantially more challenging for\nthe attacker. However, it limits the applicability of such defenses as a\npre-processing step, similar to computationally heavy approaches that use\nretraining and network modifications to achieve robustness to perturbations. In\nthis work, we propose a defense strategy that applies random image corruptions\nto the input image alone, constructs a self-correlation based subspace followed\nby a projection operation to suppress the adversarial perturbation. Due to its\nsimplicity, the proposed defense is computationally efficient as compared to\nthe state-of-the-art, and yet can withstand huge perturbations. Further, we\ndevelop proximity relationships between the projection operator of a clean\nimage and of its adversarially perturbed version, via bounds relating geodesic\ndistance on the Grassmannian to matrix Frobenius norms. We empirically show\nthat our strategy is complementary to other weak defenses like JPEG compression\nand can be seamlessly integrated with them to create a stronger defense. We\npresent extensive experiments on the ImageNet dataset across four different\nmodels namely InceptionV3, ResNet50, VGG16 and MobileNet models with\nperturbation magnitude set to {\\epsilon} = 16. Unlike state-of-the-art\napproaches, even without any retraining, the proposed strategy achieves an\nabsolute improvement of ~ 4.5% in defense accuracy on ImageNet.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 16:17:12 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 15:11:24 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Shukla", "Ankita", ""], ["Turaga", "Pavan", ""], ["Anand", "Saket", ""]]}, {"id": "2005.02958", "submitter": "Zehao Chen", "authors": "Zehao Chen and Hua Yang", "title": "Attentive Semantic Exploring for Manipulated Face Detection", "comments": null, "journal-ref": "2021 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), 2021, pp. 1985-1989", "doi": "10.1109/ICASSP39728.2021.9414225", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face manipulation methods develop rapidly in recent years, whose potential\nrisk to society accounts for the emerging of researches on detection methods.\nHowever, due to the diversity of manipulation methods and the high quality of\nfake images, detection methods suffer from a lack of generalization ability. To\nsolve the problem, we find that segmenting images into semantic fragments could\nbe effective, as discriminative defects and distortions are closely related to\nsuch fragments. Besides, to highlight discriminative regions in fragments and\nto measure contribution to the final prediction of each fragment is efficient\nfor the improvement of generalization ability. Therefore, we propose a novel\nmanipulated face detection method based on Multilevel Facial Semantic\nSegmentation and Cascade Attention Mechanism. To evaluate our method, we\nreconstruct two datasets: GGFI and FFMI, and also collect two open-source\ndatasets. Experiments on four datasets verify the advantages of our approach\nagainst other state-of-the-arts, especially its generalization ability.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 17:08:56 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 06:07:21 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Chen", "Zehao", ""], ["Yang", "Hua", ""]]}, {"id": "2005.02969", "submitter": "Cameron Kyle-Davidson Mr", "authors": "Cameron Kyle-Davidson, Adrian G. Bors, Karla K. Evans", "title": "Generating Memorable Images Based on Human Visual Memory Schemas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research study proposes using Generative Adversarial Networks (GAN) that\nincorporate a two-dimensional measure of human memorability to generate\nmemorable or non-memorable images of scenes. The memorability of the generated\nimages is evaluated by modelling Visual Memory Schemas (VMS), which correspond\nto mental representations that human observers use to encode an image into\nmemory. The VMS model is based upon the results of memory experiments conducted\non human observers, and provides a 2D map of memorability. We impose a\nmemorability constraint upon the latent space of a GAN by employing a VMS map\nprediction model as an auxiliary loss. We assess the difference in memorability\nbetween images generated to be memorable or non-memorable through an\nindependent computational measure of memorability, and additionally assess the\neffect of memorability on the realness of the generated images.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 17:23:44 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Kyle-Davidson", "Cameron", ""], ["Bors", "Adrian G.", ""], ["Evans", "Karla K.", ""]]}, {"id": "2005.02987", "submitter": "Florian Jug", "authors": "Tim-Oliver Buchholz, Mangal Prakash, Alexander Krull, Florian Jug", "title": "DenoiSeg: Joint Denoising and Segmentation", "comments": "10 pages, 4 figures, 2 pages supplement (4 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microscopy image analysis often requires the segmentation of objects, but\ntraining data for this task is typically scarce and hard to obtain. Here we\npropose DenoiSeg, a new method that can be trained end-to-end on only a few\nannotated ground truth segmentations. We achieve this by extending Noise2Void,\na self-supervised denoising scheme that can be trained on noisy images alone,\nto also predict dense 3-class segmentations. The reason for the success of our\nmethod is that segmentation can profit from denoising, especially when\nperformed jointly within the same network. The network becomes a denoising\nexpert by seeing all available raw data, while co-learning to segment, even if\nonly a few segmentation labels are available. This hypothesis is additionally\nfueled by our observation that the best segmentation results on high quality\n(very low noise) raw data are obtained when moderate amounts of synthetic noise\nare added. This renders the denoising-task non-trivial and unleashes the\ndesired co-learning effect. We believe that DenoiSeg offers a viable way to\ncircumvent the tremendous hunger for high quality training data and effectively\nenables few-shot learning of dense segmentations.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 17:42:54 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 21:58:18 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Buchholz", "Tim-Oliver", ""], ["Prakash", "Mangal", ""], ["Krull", "Alexander", ""], ["Jug", "Florian", ""]]}, {"id": "2005.03059", "submitter": "Reza Rawassizadeh", "authors": "Tahereh Javaheri, Morteza Homayounfar, Zohreh Amoozgar, Reza Reiazi,\n  Fatemeh Homayounieh, Engy Abbas, Azadeh Laali, Amir Reza Radmard, Mohammad\n  Hadi Gharib, Seyed Ali Javad Mousavi, Omid Ghaemi, Rosa Babaei, Hadi Karimi\n  Mobin, Mehdi Hosseinzadeh, Rana Jahanban-Esfahlan, Khaled Seidi, Mannudeep K.\n  Kalra, Guanglan Zhang, L.T. Chitkushev, Benjamin Haibe-Kains, Reza\n  Malekzadeh, Reza Rawassizadeh", "title": "CovidCTNet: An Open-Source Deep Learning Approach to Identify Covid-19\n  Using CT Image", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Coronavirus disease 2019 (Covid-19) is highly contagious with limited\ntreatment options. Early and accurate diagnosis of Covid-19 is crucial in\nreducing the spread of the disease and its accompanied mortality. Currently,\ndetection by reverse transcriptase polymerase chain reaction (RT-PCR) is the\ngold standard of outpatient and inpatient detection of Covid-19. RT-PCR is a\nrapid method, however, its accuracy in detection is only ~70-75%. Another\napproved strategy is computed tomography (CT) imaging. CT imaging has a much\nhigher sensitivity of ~80-98%, but similar accuracy of 70%. To enhance the\naccuracy of CT imaging detection, we developed an open-source set of algorithms\ncalled CovidCTNet that successfully differentiates Covid-19 from\ncommunity-acquired pneumonia (CAP) and other lung diseases. CovidCTNet\nincreases the accuracy of CT imaging detection to 90% compared to radiologists\n(70%). The model is designed to work with heterogeneous and small sample sizes\nindependent of the CT imaging hardware. In order to facilitate the detection of\nCovid-19 globally and assist radiologists and physicians in the screening\nprocess, we are releasing all algorithms and parametric details in an\nopen-source format. Open-source sharing of our CovidCTNet enables developers to\nrapidly improve and optimize services, while preserving user privacy and data\nownership.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 18:16:59 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 20:05:09 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 00:47:50 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Javaheri", "Tahereh", ""], ["Homayounfar", "Morteza", ""], ["Amoozgar", "Zohreh", ""], ["Reiazi", "Reza", ""], ["Homayounieh", "Fatemeh", ""], ["Abbas", "Engy", ""], ["Laali", "Azadeh", ""], ["Radmard", "Amir Reza", ""], ["Gharib", "Mohammad Hadi", ""], ["Mousavi", "Seyed Ali Javad", ""], ["Ghaemi", "Omid", ""], ["Babaei", "Rosa", ""], ["Mobin", "Hadi Karimi", ""], ["Hosseinzadeh", "Mehdi", ""], ["Jahanban-Esfahlan", "Rana", ""], ["Seidi", "Khaled", ""], ["Kalra", "Mannudeep K.", ""], ["Zhang", "Guanglan", ""], ["Chitkushev", "L. T.", ""], ["Haibe-Kains", "Benjamin", ""], ["Malekzadeh", "Reza", ""], ["Rawassizadeh", "Reza", ""]]}, {"id": "2005.03080", "submitter": "Oktay Karaku\\c{s} Dr", "authors": "Oktay Karaku\\c{s}, Nantheera Anantrasirichai, Amazigh Aguersif, Stein\n  Silva, Adrian Basarab, Alin Achim", "title": "Detection of Line Artefacts in Lung Ultrasound Images of COVID-19\n  Patients via Non-Convex Regularization", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TUFFC.2020.3016092", "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel method for line artefacts quantification in\nlung ultrasound (LUS) images of COVID-19 patients. We formulate this as a\nnon-convex regularisation problem involving a sparsity-enforcing, Cauchy-based\npenalty function, and the inverse Radon transform. We employ a simple local\nmaxima detection technique in the Radon transform domain, associated with known\nclinical definitions of line artefacts. Despite being non-convex, the proposed\ntechnique is guaranteed to convergence through our proposed Cauchy proximal\nsplitting (CPS) method and accurately identifies both horizontal and vertical\nline artefacts in LUS images. In order to reduce the number of false and missed\ndetection, our method includes a two-stage validation mechanism, which is\nperformed in both Radon and image domains. We evaluate the performance of the\nproposed method in comparison to the current state-of-the-art B-line\nidentification method and show a considerable performance gain with 87%\ncorrectly detected B-lines in LUS images of nine COVID-19 patients. In\naddition, owing to its fast convergence, our proposed method is readily\napplicable for processing LUS image sequences.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 19:16:04 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 22:15:37 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2020 15:18:00 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Karaku\u015f", "Oktay", ""], ["Anantrasirichai", "Nantheera", ""], ["Aguersif", "Amazigh", ""], ["Silva", "Stein", ""], ["Basarab", "Adrian", ""], ["Achim", "Alin", ""]]}, {"id": "2005.03086", "submitter": "Hao Tan", "authors": "Yubo Zhang, Hao Tan, Mohit Bansal", "title": "Diagnosing the Environment Bias in Vision-and-Language Navigation", "comments": "IJCAI 2020 (9 pages; first two authors contributed equally)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-Language Navigation (VLN) requires an agent to follow\nnatural-language instructions, explore the given environments, and reach the\ndesired target locations. These step-by-step navigational instructions are\ncrucial when the agent is navigating new environments about which it has no\nprior knowledge. Most recent works that study VLN observe a significant\nperformance drop when tested on unseen environments (i.e., environments not\nused in training), indicating that the neural agent models are highly biased\ntowards training environments. Although this issue is considered as one of the\nmajor challenges in VLN research, it is still under-studied and needs a clearer\nexplanation. In this work, we design novel diagnosis experiments via\nenvironment re-splitting and feature replacement, looking into possible reasons\nfor this environment bias. We observe that neither the language nor the\nunderlying navigational graph, but the low-level visual appearance conveyed by\nResNet features directly affects the agent model and contributes to this\nenvironment bias in results. According to this observation, we explore several\nkinds of semantic representations that contain less low-level visual\ninformation, hence the agent learned with these features could be better\ngeneralized to unseen testing environments. Without modifying the baseline\nagent model and its training method, our explored semantic features\nsignificantly decrease the performance gaps between seen and unseen on multiple\ndatasets (i.e. R2R, R4R, and CVDN) and achieve competitive unseen results to\nprevious state-of-the-art models. Our code and features are available at:\nhttps://github.com/zhangybzbo/EnvBiasVLN\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 19:24:33 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Zhang", "Yubo", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "2005.03101", "submitter": "Xinjiang Wang", "authors": "Xinjiang Wang, Shilong Zhang, Zhuoran Yu, Litong Feng, Wayne Zhang", "title": "Scale-Equalizing Pyramid Convolution for Object Detection", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature pyramid has been an efficient method to extract features at different\nscales. Development over this method mainly focuses on aggregating contextual\ninformation at different levels while seldom touching the inter-level\ncorrelation in the feature pyramid. Early computer vision methods extracted\nscale-invariant features by locating the feature extrema in both spatial and\nscale dimension. Inspired by this, a convolution across the pyramid level is\nproposed in this study, which is termed pyramid convolution and is a modified\n3-D convolution. Stacked pyramid convolutions directly extract 3-D (scale and\nspatial) features and outperforms other meticulously designed feature fusion\nmodules. Based on the viewpoint of 3-D convolution, an integrated batch\nnormalization that collects statistics from the whole feature pyramid is\nnaturally inserted after the pyramid convolution. Furthermore, we also show\nthat the naive pyramid convolution, together with the design of RetinaNet head,\nactually best applies for extracting features from a Gaussian pyramid, whose\nproperties can hardly be satisfied by a feature pyramid. In order to alleviate\nthis discrepancy, we build a scale-equalizing pyramid convolution (SEPC) that\naligns the shared pyramid convolution kernel only at high-level feature maps.\nBeing computationally efficient and compatible with the head design of most\nsingle-stage object detectors, the SEPC module brings significant performance\nimprovement ($>4$AP increase on MS-COCO2017 dataset) in state-of-the-art\none-stage object detectors, and a light version of SEPC also has $\\sim3.5$AP\ngain with only around 7% inference time increase. The pyramid convolution also\nfunctions well as a stand-alone module in two-stage object detectors and is\nable to improve the performance by $\\sim2$AP. The source code can be found at\nhttps://github.com/jshilong/SEPC.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 19:34:56 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Wang", "Xinjiang", ""], ["Zhang", "Shilong", ""], ["Yu", "Zhuoran", ""], ["Feng", "Litong", ""], ["Zhang", "Wayne", ""]]}, {"id": "2005.03106", "submitter": "Rayson Laroca", "authors": "Gabriel Salomon, Rayson Laroca, David Menotti", "title": "Deep Learning for Image-based Automatic Dial Meter Reading: Dataset and\n  Baselines", "comments": "Accepted for presentation at the 2020 International Joint Conference\n  on Neural Networks (IJCNN)", "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9207318", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart meters enable remote and automatic electricity, water and gas\nconsumption reading and are being widely deployed in developed countries.\nNonetheless, there is still a huge number of non-smart meters in operation.\nImage-based Automatic Meter Reading (AMR) focuses on dealing with this type of\nmeter readings. We estimate that the Energy Company of Paran\\'a (Copel), in\nBrazil, performs more than 850,000 readings of dial meters per month. Those\nmeters are the focus of this work. Our main contributions are: (i) a public\nreal-world dial meter dataset (shared upon request) called UFPR-ADMR; (ii) a\ndeep learning-based recognition baseline on the proposed dataset; and (iii) a\ndetailed error analysis of the main issues present in AMR for dial meters. To\nthe best of our knowledge, this is the first work to introduce deep learning\napproaches to multi-dial meter reading, and perform experiments on\nunconstrained images. We achieved a 100.0% F1-score on the dial detection stage\nwith both Faster R-CNN and YOLO, while the recognition rates reached 93.6% for\ndials and 75.25% for meters using Faster R-CNN (ResNext-101).\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 19:48:23 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 13:51:06 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Salomon", "Gabriel", ""], ["Laroca", "Rayson", ""], ["Menotti", "David", ""]]}, {"id": "2005.03119", "submitter": "Po-Yao Huang", "authors": "Po-Yao Huang, Junjie Hu, Xiaojun Chang, Alexander Hauptmann", "title": "Unsupervised Multimodal Neural Machine Translation with Pseudo Visual\n  Pivoting", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised machine translation (MT) has recently achieved impressive\nresults with monolingual corpora only. However, it is still challenging to\nassociate source-target sentences in the latent space. As people speak\ndifferent languages biologically share similar visual systems, the potential of\nachieving better alignment through visual content is promising yet\nunder-explored in unsupervised multimodal MT (MMT). In this paper, we\ninvestigate how to utilize visual content for disambiguation and promoting\nlatent space alignment in unsupervised MMT. Our model employs multimodal\nback-translation and features pseudo visual pivoting in which we learn a shared\nmultilingual visual-semantic embedding space and incorporate visually-pivoted\ncaptioning as additional weak supervision. The experimental results on the\nwidely used Multi30K dataset show that the proposed model significantly\nimproves over the state-of-the-art methods and generalizes well when the images\nare not available at the testing time.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 20:11:46 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Huang", "Po-Yao", ""], ["Hu", "Junjie", ""], ["Chang", "Xiaojun", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "2005.03141", "submitter": "Zifan Wang", "authors": "Zifan Wang, Yilin Yang, Ankit Shrivastava, Varun Rawal and Zihao Ding", "title": "Towards Frequency-Based Explanation for Robust CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current explanation techniques towards a transparent Convolutional Neural\nNetwork (CNN) mainly focuses on building connections between the\nhuman-understandable input features with models' prediction, overlooking an\nalternative representation of the input, the frequency components\ndecomposition. In this work, we present an analysis of the connection between\nthe distribution of frequency components in the input dataset and the reasoning\nprocess the model learns from the data. We further provide quantification\nanalysis about the contribution of different frequency components toward the\nmodel's prediction. We show that the vulnerability of the model against tiny\ndistortions is a result of the model is relying on the high-frequency features,\nthe target features of the adversarial (black and white-box) attackers, to make\nthe prediction. We further show that if the model develops stronger association\nbetween the low-frequency component with true labels, the model is more robust,\nwhich is the explanation of why adversarially trained models are more robust\nagainst tiny distortions.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 21:22:35 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Wang", "Zifan", ""], ["Yang", "Yilin", ""], ["Shrivastava", "Ankit", ""], ["Rawal", "Varun", ""], ["Ding", "Zihao", ""]]}, {"id": "2005.03155", "submitter": "Shanxin Yuan", "authors": "Shanxin Yuan, Radu Timofte, Ales Leonardis, Gregory Slabaugh, Xiaotong\n  Luo, Jiangtao Zhang, Yanyun Qu, Ming Hong, Yuan Xie, Cuihua Li, Dejia Xu,\n  Yihao Chu, Qingyan Sun, Shuai Liu, Ziyao Zong, Nan Nan, Chenghua Li, Sangmin\n  Kim, Hyungjoon Nam, Jisu Kim, Jechang Jeong, Manri Cheon, Sung-Jun Yoon,\n  Byungyeon Kang, Junwoo Lee, Bolun Zheng, Xiaohong Liu, Linhui Dai, Jun Chen,\n  Xi Cheng, Zhenyong Fu, Jian Yang, Chul Lee, An Gia Vien, Hyunkook Park,\n  Sabari Nathan, M.Parisa Beham, S Mohamed Mansoor Roomi, Florian Lemarchand,\n  Maxime Pelcat, Erwan Nogues, Densen Puthussery, Hrishikesh P S, Jiji C V,\n  Ashish Sinha, Xuan Zhao", "title": "NTIRE 2020 Challenge on Image Demoireing: Methods and Results", "comments": null, "journal-ref": "CVPRW 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the Challenge on Image Demoireing that was part of the New\nTrends in Image Restoration and Enhancement (NTIRE) workshop, held in\nconjunction with CVPR 2020. Demoireing is a difficult task of removing moire\npatterns from an image to reveal an underlying clean image. The challenge was\ndivided into two tracks. Track 1 targeted the single image demoireing problem,\nwhich seeks to remove moire patterns from a single image. Track 2 focused on\nthe burst demoireing problem, where a set of degraded moire images of the same\nscene were provided as input, with the goal of producing a single demoired\nimage as output. The methods were ranked in terms of their fidelity, measured\nusing the peak signal-to-noise ratio (PSNR) between the ground truth clean\nimages and the restored images produced by the participants' methods. The\ntracks had 142 and 99 registered participants, respectively, with a total of 14\nand 6 submissions in the final testing stage. The entries span the current\nstate-of-the-art in image and burst image demoireing problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 22:05:58 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Yuan", "Shanxin", ""], ["Timofte", "Radu", ""], ["Leonardis", "Ales", ""], ["Slabaugh", "Gregory", ""], ["Luo", "Xiaotong", ""], ["Zhang", "Jiangtao", ""], ["Qu", "Yanyun", ""], ["Hong", "Ming", ""], ["Xie", "Yuan", ""], ["Li", "Cuihua", ""], ["Xu", "Dejia", ""], ["Chu", "Yihao", ""], ["Sun", "Qingyan", ""], ["Liu", "Shuai", ""], ["Zong", "Ziyao", ""], ["Nan", "Nan", ""], ["Li", "Chenghua", ""], ["Kim", "Sangmin", ""], ["Nam", "Hyungjoon", ""], ["Kim", "Jisu", ""], ["Jeong", "Jechang", ""], ["Cheon", "Manri", ""], ["Yoon", "Sung-Jun", ""], ["Kang", "Byungyeon", ""], ["Lee", "Junwoo", ""], ["Zheng", "Bolun", ""], ["Liu", "Xiaohong", ""], ["Dai", "Linhui", ""], ["Chen", "Jun", ""], ["Cheng", "Xi", ""], ["Fu", "Zhenyong", ""], ["Yang", "Jian", ""], ["Lee", "Chul", ""], ["Vien", "An Gia", ""], ["Park", "Hyunkook", ""], ["Nathan", "Sabari", ""], ["Beham", "M. Parisa", ""], ["Roomi", "S Mohamed Mansoor", ""], ["Lemarchand", "Florian", ""], ["Pelcat", "Maxime", ""], ["Nogues", "Erwan", ""], ["Puthussery", "Densen", ""], ["S", "Hrishikesh P", ""], ["C", "Jiji", "V"], ["Sinha", "Ashish", ""], ["Zhao", "Xuan", ""]]}, {"id": "2005.03190", "submitter": "Heng Yang", "authors": "Heng Yang", "title": "A Dynamical Perspective on Point Cloud Registration", "comments": "Preliminary results, 10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a dynamical perspective on the classical problem of 3D point cloud\nregistration with correspondences. A point cloud is considered as a rigid body\nconsisting of particles. The problem of registering two point clouds is\nformulated as a dynamical system, where the dynamic model point cloud\ntranslates and rotates in a viscous environment towards the static scene point\ncloud, under forces and torques induced by virtual springs placed between each\npair of corresponding points. We first show that the potential energy of the\nsystem recovers the objective function of the maximum likelihood estimation. We\nthen adopt Lyapunov analysis, particularly the invariant set theorem, to\nanalyze the rigid body dynamics and show that the system globally\nasymptotically tends towards the set of equilibrium points, where the globally\noptimal registration solution lies in. We conjecture that, besides the globally\noptimal equilibrium point, the system has either three or infinite \"spurious\"\nequilibrium points, and these spurious equilibria are all locally unstable. The\ncase of three spurious equilibria corresponds to generic shape of the point\ncloud, while the case of infinite spurious equilibria happens when the point\ncloud exhibits symmetry. Therefore, simulating the dynamics with random\nperturbations guarantees to obtain the globally optimal registration solution.\nNumerical experiments support our analysis and conjecture.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 01:00:29 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Yang", "Heng", ""]]}, {"id": "2005.03194", "submitter": "Chen Chen", "authors": "Talal Alatiah and Chen Chen", "title": "Recognizing Exercises and Counting Repetitions in Real Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence technology has made its way absolutely necessary in a\nvariety of industries including the fitness industry. Human pose estimation is\none of the important researches in the field of Computer Vision for the last\nfew years. In this project, pose estimation and deep machine learning\ntechniques are combined to analyze the performance and report feedback on the\nrepetitions of performed exercises in real-time. Involving machine learning\ntechnology in the fitness industry could help the judges to count repetitions\nof any exercise during Weightlifting or CrossFit competitions.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 01:30:48 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Alatiah", "Talal", ""], ["Chen", "Chen", ""]]}, {"id": "2005.03201", "submitter": "Lele Chen", "authors": "Lele Chen, Guofeng Cui, Ziyi Kou, Haitian Zheng, Chenliang Xu", "title": "What comprises a good talking-head video generation?: A Survey and\n  Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the years, performance evaluation has become essential in computer\nvision, enabling tangible progress in many sub-fields. While talking-head video\ngeneration has become an emerging research topic, existing evaluations on this\ntopic present many limitations. For example, most approaches use human subjects\n(e.g., via Amazon MTurk) to evaluate their research claims directly. This\nsubjective evaluation is cumbersome, unreproducible, and may impend the\nevolution of new research. In this work, we present a carefully-designed\nbenchmark for evaluating talking-head video generation with standardized\ndataset pre-processing strategies. As for evaluation, we either propose new\nmetrics or select the most appropriate ones to evaluate results in what we\nconsider as desired properties for a good talking-head video, namely, identity\npreserving, lip synchronization, high video quality, and natural-spontaneous\nmotion. By conducting a thoughtful analysis across several state-of-the-art\ntalking-head generation approaches, we aim to uncover the merits and drawbacks\nof current methods and point out promising directions for future work. All the\nevaluation code is available at:\nhttps://github.com/lelechen63/talking-head-generation-survey.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 01:58:05 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Chen", "Lele", ""], ["Cui", "Guofeng", ""], ["Kou", "Ziyi", ""], ["Zheng", "Haitian", ""], ["Xu", "Chenliang", ""]]}, {"id": "2005.03209", "submitter": "Harshala Gammulle", "authors": "Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Hierarchical Attention Network for Action Segmentation", "comments": "Published in Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The temporal segmentation of events is an essential task and a precursor for\nthe automatic recognition of human actions in the video. Several attempts have\nbeen made to capture frame-level salient aspects through attention but they\nlack the capacity to effectively map the temporal relationships in between the\nframes as they only capture a limited span of temporal dependencies. To this\nend we propose a complete end-to-end supervised learning approach that can\nbetter learn relationships between actions over time, thus improving the\noverall segmentation performance. The proposed hierarchical recurrent attention\nframework analyses the input video at multiple temporal scales, to form\nembeddings at frame level and segment level, and perform fine-grained action\nsegmentation. This generates a simple, lightweight, yet extremely effective\narchitecture for segmenting continuous video streams and has multiple\napplication domains. We evaluate our system on multiple challenging public\nbenchmark datasets, including MERL Shopping, 50 salads, and Georgia Tech\nEgocentric datasets, and achieves state-of-the-art performance. The evaluated\ndatasets encompass numerous video capture settings which are inclusive of\nstatic overhead camera views and dynamic, ego-centric head-mounted camera\nviews, demonstrating the direct applicability of the proposed framework in a\nvariety of settings.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 02:39:18 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Gammulle", "Harshala", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2005.03221", "submitter": "Nantheera Anantrasirichai", "authors": "Nantheera Anantrasirichai, Juliet Biggs, Krisztina Kelevitz, Zahra\n  Sadeghi, Tim Wright, James Thompson, Alin Achim, David Bull", "title": "Deep Learning Framework for Detecting Ground Deformation in the Built\n  Environment using Satellite InSAR data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large volumes of Sentinel-1 data produced over Europe are being used to\ndevelop pan-national ground motion services. However, simple analysis\ntechniques like thresholding cannot detect and classify complex deformation\nsignals reliably making providing usable information to a broad range of\nnon-expert stakeholders a challenge. Here we explore the applicability of deep\nlearning approaches by adapting a pre-trained convolutional neural network\n(CNN) to detect deformation in a national-scale velocity field. For our\nproof-of-concept, we focus on the UK where previously identified deformation is\nassociated with coal-mining, ground water withdrawal, landslides and\ntunnelling. The sparsity of measurement points and the presence of spike noise\nmake this a challenging application for deep learning networks, which involve\ncalculations of the spatial convolution between images. Moreover, insufficient\nground truth data exists to construct a balanced training data set, and the\ndeformation signals are slower and more localised than in previous\napplications. We propose three enhancement methods to tackle these problems: i)\nspatial interpolation with modified matrix completion, ii) a synthetic training\ndataset based on the characteristics of real UK velocity map, and iii) enhanced\nover-wrapping techniques. Using velocity maps spanning 2015-2019, our framework\ndetects several areas of coal mining subsidence, uplift due to dewatering,\nslate quarries, landslides and tunnel engineering works. The results\ndemonstrate the potential applicability of the proposed framework to the\ndevelopment of automated ground motion analysis systems.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 03:14:00 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 03:20:00 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Anantrasirichai", "Nantheera", ""], ["Biggs", "Juliet", ""], ["Kelevitz", "Krisztina", ""], ["Sadeghi", "Zahra", ""], ["Wright", "Tim", ""], ["Thompson", "James", ""], ["Achim", "Alin", ""], ["Bull", "David", ""]]}, {"id": "2005.03222", "submitter": "Amena Khatun", "authors": "Amena Khatun, Simon Denman, Sridha Sridharan and Clinton Fookes", "title": "End-to-End Domain Adaptive Attention Network for Cross-Domain Person\n  Re-Identification", "comments": "submitted to IEEE Transactions on Information Forensics and Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) remains challenging in a real-world\nscenario, as it requires a trained network to generalise to totally unseen\ntarget data in the presence of variations across domains. Recently, generative\nadversarial models have been widely adopted to enhance the diversity of\ntraining data. These approaches, however, often fail to generalise to other\ndomains, as existing generative person re-identification models have a\ndisconnect between the generative component and the discriminative feature\nlearning stage. To address the on-going challenges regarding model\ngeneralisation, we propose an end-to-end domain adaptive attention network to\njointly translate images between domains and learn discriminative re-id\nfeatures in a single framework. To address the domain gap challenge, we\nintroduce an attention module for image translation from source to target\ndomains without affecting the identity of a person. More specifically,\nattention is directed to the background instead of the entire image of the\nperson, ensuring identifying characteristics of the subject are preserved. The\nproposed joint learning network results in a significant performance\nimprovement over state-of-the-art methods on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 03:17:43 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Khatun", "Amena", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2005.03225", "submitter": "Ziyuan Zhao", "authors": "Ziyuan Zhao, Xiaoyan Yang, Bharadwaj Veeravalli, Zeng Zeng", "title": "Deeply Supervised Active Learning for Finger Bones Segmentation", "comments": "Accepted version to be published in the 42nd IEEE Annual\n  International Conference of the IEEE Engineering in Medicine and Biology\n  Society, EMBC 2020, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation is a prerequisite yet challenging task for medical image\nanalysis. In this paper, we introduce a novel deeply supervised active learning\napproach for finger bones segmentation. The proposed architecture is fine-tuned\nin an iterative and incremental learning manner. In each step, the deep\nsupervision mechanism guides the learning process of hidden layers and selects\nsamples to be labeled. Extensive experiments demonstrated that our method\nachieves competitive segmentation results using less labeled samples as\ncompared with full annotation.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 03:27:40 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Zhao", "Ziyuan", ""], ["Yang", "Xiaoyan", ""], ["Veeravalli", "Bharadwaj", ""], ["Zeng", "Zeng", ""]]}, {"id": "2005.03227", "submitter": "Feng Shi", "authors": "Hengyuan Kang, Liming Xia, Fuhua Yan, Zhibin Wan, Feng Shi, Huan Yuan,\n  Huiting Jiang, Dijia Wu, He Sui, Changqing Zhang, and Dinggang Shen", "title": "Diagnosis of Coronavirus Disease 2019 (COVID-19) with Structured Latent\n  Multi-View Representation Learning", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging (2020)", "doi": "10.1109/TMI.2020.2992546", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the outbreak of Coronavirus Disease 2019 (COVID-19) has spread\nrapidly across the world. Due to the large number of affected patients and\nheavy labor for doctors, computer-aided diagnosis with machine learning\nalgorithm is urgently needed, and could largely reduce the efforts of\nclinicians and accelerate the diagnosis process. Chest computed tomography (CT)\nhas been recognized as an informative tool for diagnosis of the disease. In\nthis study, we propose to conduct the diagnosis of COVID-19 with a series of\nfeatures extracted from CT images. To fully explore multiple features\ndescribing CT images from different views, a unified latent representation is\nlearned which can completely encode information from different aspects of\nfeatures and is endowed with promising class structure for separability.\nSpecifically, the completeness is guaranteed with a group of backward neural\nnetworks (each for one type of features), while by using class labels the\nrepresentation is enforced to be compact within COVID-19/community-acquired\npneumonia (CAP) and also a large margin is guaranteed between different types\nof pneumonia. In this way, our model can well avoid overfitting compared to the\ncase of directly projecting highdimensional features into classes. Extensive\nexperimental results show that the proposed method outperforms all comparison\nmethods, and rather stable performances are observed when varying the numbers\nof training data.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 15:19:15 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kang", "Hengyuan", ""], ["Xia", "Liming", ""], ["Yan", "Fuhua", ""], ["Wan", "Zhibin", ""], ["Shi", "Feng", ""], ["Yuan", "Huan", ""], ["Jiang", "Huiting", ""], ["Wu", "Dijia", ""], ["Sui", "He", ""], ["Zhang", "Changqing", ""], ["Shen", "Dinggang", ""]]}, {"id": "2005.03228", "submitter": "Chenhao Xie", "authors": "Chenhao Xie, Qiao Cheng, Jiaqing Liang, Lihan Chen, Yanghua Xiao", "title": "Collective Loss Function for Positive and Unlabeled Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People learn to discriminate between classes without explicit exposure to\nnegative examples. On the contrary, traditional machine learning algorithms\noften rely on negative examples, otherwise the model would be prone to collapse\nand always-true predictions. Therefore, it is crucial to design the learning\nobjective which leads the model to converge and to perform predictions\nunbiasedly without explicit negative signals. In this paper, we propose a\nCollectively loss function to learn from only Positive and Unlabeled data\n(cPU). We theoretically elicit the loss function from the setting of PU\nlearning. We perform intensive experiments on the benchmark and real-world\ndatasets. The results show that cPU consistently outperforms the current\nstate-of-the-art PU learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 03:30:22 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Xie", "Chenhao", ""], ["Cheng", "Qiao", ""], ["Liang", "Jiaqing", ""], ["Chen", "Lihan", ""], ["Xiao", "Yanghua", ""]]}, {"id": "2005.03229", "submitter": "Pengfei Wei Dr.", "authors": "Pengfei Wei, Yiping Ke, Xinghua Qu, Tze-Yun Leong", "title": "Subdomain Adaptation with Manifolds Discrepancy Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing domain divergence is a key step in transfer learning problems.\nExisting works focus on the minimization of global domain divergence. However,\ntwo domains may consist of several shared subdomains, and differ from each\nother in each subdomain. In this paper, we take the local divergence of\nsubdomains into account in transfer. Specifically, we propose to use\nlow-dimensional manifold to represent subdomain, and align the local data\ndistribution discrepancy in each manifold across domains. A Manifold Maximum\nMean Discrepancy (M3D) is developed to measure the local distribution\ndiscrepancy in each manifold. We then propose a general framework, called\nTransfer with Manifolds Discrepancy Alignment (TMDA), to couple the discovery\nof data manifolds with the minimization of M3D. We instantiate TMDA in the\nsubspace learning case considering both the linear and nonlinear mappings. We\nalso instantiate TMDA in the deep learning framework. Extensive experimental\nstudies demonstrate that TMDA is a promising method for various transfer\nlearning tasks.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 04:18:47 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Wei", "Pengfei", ""], ["Ke", "Yiping", ""], ["Qu", "Xinghua", ""], ["Leong", "Tze-Yun", ""]]}, {"id": "2005.03230", "submitter": "Matin Hosseini", "authors": "Matin Hosseini, Anthony Maida", "title": "Hierarchical Predictive Coding Models in a Deep-Learning Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian predictive coding is a putative neuromorphic method for acquiring\nhigher-level neural representations to account for sensory input. Although\noriginating in the neuroscience community, there are also efforts in the\nmachine learning community to study these models. This paper reviews some of\nthe more well known models. Our review analyzes module connectivity and\npatterns of information transfer, seeking to find general principles used\nacross the models. We also survey some recent attempts to cast these models\nwithin a deep learning framework. A defining feature of Bayesian predictive\ncoding is that it uses top-down, reconstructive mechanisms to predict incoming\nsensory inputs or their lower-level representations. Discrepancies between the\npredicted and the actual inputs, known as prediction errors, then give rise to\nfuture learning that refines and improves the predictive accuracy of learned\nhigher-level representations. Predictive coding models intended to describe\ncomputations in the neocortex emerged prior to the development of deep learning\nand used a communication structure between modules that we name the Rao-Ballard\nprotocol. This protocol was derived from a Bayesian generative model with some\nrather strong statistical assumptions. The RB protocol provides a rubric to\nassess the fidelity of deep learning models that claim to implement predictive\ncoding.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 03:39:57 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 00:42:39 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Hosseini", "Matin", ""], ["Maida", "Anthony", ""]]}, {"id": "2005.03232", "submitter": "Ziyuan Zhao", "authors": "Peisheng Qian, Ziyuan Zhao, Haobing Liu, Yingcai Wang, Yu Peng, Sheng\n  Hu, Jing Zhang, Yue Deng, Zeng Zeng", "title": "Multi-Target Deep Learning for Algal Detection and Classification", "comments": "Accepted version to be published in the 42nd IEEE Annual\n  International Conference of the IEEE Engineering in Medicine and Biology\n  Society, EMBC 2020, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water quality has a direct impact on industry, agriculture, and public\nhealth. Algae species are common indicators of water quality. It is because\nalgal communities are sensitive to changes in their habitats, giving valuable\nknowledge on variations in water quality. However, water quality analysis\nrequires professional inspection of algal detection and classification under\nmicroscopes, which is very time-consuming and tedious. In this paper, we\npropose a novel multi-target deep learning framework for algal detection and\nclassification. Extensive experiments were carried out on a large-scale colored\nmicroscopic algal dataset. Experimental results demonstrate that the proposed\nmethod leads to the promising performance on algal detection, class\nidentification and genus identification.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 03:40:29 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Qian", "Peisheng", ""], ["Zhao", "Ziyuan", ""], ["Liu", "Haobing", ""], ["Wang", "Yingcai", ""], ["Peng", "Yu", ""], ["Hu", "Sheng", ""], ["Zhang", "Jing", ""], ["Deng", "Yue", ""], ["Zeng", "Zeng", ""]]}, {"id": "2005.03264", "submitter": "Liang Sun", "authors": "Liang Sun, Zhanhao Mo, Fuhua Yan, Liming Xia, Fei Shan, Zhongxiang\n  Ding, Wei Shao, Feng Shi, Huan Yuan, Huiting Jiang, Dijia Wu, Ying Wei,\n  Yaozong Gao, Wanchun Gao, He Sui, Daoqiang Zhang, Dinggang Shen", "title": "Adaptive Feature Selection Guided Deep Forest for COVID-19\n  Classification with Chest CT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest computed tomography (CT) becomes an effective tool to assist the\ndiagnosis of coronavirus disease-19 (COVID-19). Due to the outbreak of COVID-19\nworldwide, using the computed-aided diagnosis technique for COVID-19\nclassification based on CT images could largely alleviate the burden of\nclinicians. In this paper, we propose an Adaptive Feature Selection guided Deep\nForest (AFS-DF) for COVID-19 classification based on chest CT images.\nSpecifically, we first extract location-specific features from CT images. Then,\nin order to capture the high-level representation of these features with the\nrelatively small-scale data, we leverage a deep forest model to learn\nhigh-level representation of the features. Moreover, we propose a feature\nselection method based on the trained deep forest model to reduce the\nredundancy of features, where the feature selection could be adaptively\nincorporated with the COVID-19 classification model. We evaluated our proposed\nAFS-DF on COVID-19 dataset with 1495 patients of COVID-19 and 1027 patients of\ncommunity acquired pneumonia (CAP). The accuracy (ACC), sensitivity (SEN),\nspecificity (SPE) and AUC achieved by our method are 91.79%, 93.05%, 89.95% and\n96.35%, respectively. Experimental results on the COVID-19 dataset suggest that\nthe proposed AFS-DF achieves superior performance in COVID-19 vs. CAP\nclassification, compared with 4 widely used machine learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 06:00:02 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Sun", "Liang", ""], ["Mo", "Zhanhao", ""], ["Yan", "Fuhua", ""], ["Xia", "Liming", ""], ["Shan", "Fei", ""], ["Ding", "Zhongxiang", ""], ["Shao", "Wei", ""], ["Shi", "Feng", ""], ["Yuan", "Huan", ""], ["Jiang", "Huiting", ""], ["Wu", "Dijia", ""], ["Wei", "Ying", ""], ["Gao", "Yaozong", ""], ["Gao", "Wanchun", ""], ["Sui", "He", ""], ["Zhang", "Daoqiang", ""], ["Shen", "Dinggang", ""]]}, {"id": "2005.03286", "submitter": "Fabio Poiesi", "authors": "Matteo Bortolon, Paul Chippendale, Stefano Messelodi and Fabio Poiesi", "title": "Multi-view data capture using edge-synchronised mobiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view data capture permits free-viewpoint video (FVV) content creation.\nTo this end, several users must capture video streams, calibrated in both time\nand pose, framing the same object/scene, from different viewpoints.\nNew-generation network architectures (e.g. 5G) promise lower latency and larger\nbandwidth connections supported by powerful edge computing, properties that\nseem ideal for reliable FVV capture. We have explored this possibility, aiming\nto remove the need for bespoke synchronisation hardware when capturing a scene\nfrom multiple viewpoints, making it possible through off-the-shelf mobiles. We\npropose a novel and scalable data capture architecture that exploits edge\nresources to synchronise and harvest frame captures. We have designed an edge\ncomputing unit that supervises the relaying of timing triggers to and from\nmultiple mobiles, in addition to synchronising frame harvesting. We empirically\nshow the benefits of our edge computing unit by analysing latencies and show\nthe quality of 3D reconstruction outputs against an alternative and popular\ncentralised solution based on Unity3D.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 07:13:20 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Bortolon", "Matteo", ""], ["Chippendale", "Paul", ""], ["Messelodi", "Stefano", ""], ["Poiesi", "Fabio", ""]]}, {"id": "2005.03293", "submitter": "Nirbhay Kumar Tagore", "authors": "Nirbhay Kumar Tagore, Ayushman Singh, Sumanth Manche, Pratik\n  Chattopadhyay", "title": "Deep Learning based Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated person re-identification in a multi-camera surveillance setup is\nvery important for effective tracking and monitoring crowd movement. In the\nrecent years, few deep learning based re-identification approaches have been\ndeveloped which are quite accurate but time-intensive, and hence not very\nsuitable for practical purposes. In this paper, we propose an efficient\nhierarchical re-identification approach in which color histogram based\ncomparison is first employed to find the closest matches in the gallery set,\nand next deep feature based comparison is carried out using Siamese network.\nReduction in search space after the first level of matching helps in achieving\na fast response time as well as improving the accuracy of prediction by the\nSiamese network by eliminating vastly dissimilar elements. A silhouette\npart-based feature extraction scheme is adopted in each level of hierarchy to\npreserve the relative locations of the different body structures and make the\nappearance descriptors more discriminating in nature. The proposed approach has\nbeen evaluated on five public data sets and also a new data set captured by our\nteam in our laboratory. Results reveal that it outperforms most\nstate-of-the-art approaches in terms of overall accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 07:30:28 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Tagore", "Nirbhay Kumar", ""], ["Singh", "Ayushman", ""], ["Manche", "Sumanth", ""], ["Chattopadhyay", "Pratik", ""]]}, {"id": "2005.03297", "submitter": "Yunshan Ma", "authors": "Yunshan Ma, Yujuan Ding, Xun Yang, Lizi Liao, Wai Keung Wong, Tat-Seng\n  Chua", "title": "Knowledge Enhanced Neural Fashion Trend Forecasting", "comments": "8 pages, 9 figures, ICMR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion trend forecasting is a crucial task for both academia and industry.\nAlthough some efforts have been devoted to tackling this challenging task, they\nonly studied limited fashion elements with highly seasonal or simple patterns,\nwhich could hardly reveal the real fashion trends. Towards insightful fashion\ntrend forecasting, this work focuses on investigating fine-grained fashion\nelement trends for specific user groups. We first contribute a large-scale\nfashion trend dataset (FIT) collected from Instagram with extracted time series\nfashion element records and user information. Further-more, to effectively\nmodel the time series data of fashion elements with rather complex patterns, we\npropose a Knowledge EnhancedRecurrent Network model (KERN) which takes\nadvantage of the capability of deep recurrent neural networks in modeling\ntime-series data. Moreover, it leverages internal and external knowledge in\nfashion domain that affects the time-series patterns of fashion element trends.\nSuch incorporation of domain knowledge further enhances the deep learning model\nin capturing the patterns of specific fashion elements and predicting the\nfuture trends. Extensive experiments demonstrate that the proposed KERN model\ncan effectively capture the complicated patterns of objective fashion elements,\ntherefore making preferable fashion trend forecast.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 07:42:17 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 09:14:20 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ma", "Yunshan", ""], ["Ding", "Yujuan", ""], ["Yang", "Xun", ""], ["Liao", "Lizi", ""], ["Wong", "Wai Keung", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2005.03315", "submitter": "Nantheera Anantrasirichai", "authors": "Nantheera Anantrasirichai, Fan Zhang, Alexandra Malyugina, Paul Hill,\n  and Angeliki Katsenou", "title": "Encoding in the Dark Grand Challenge: An Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A big part of the video content we consume from video providers consists of\ngenres featuring low-light aesthetics. Low light sequences have special\ncharacteristics, such as spatio-temporal varying acquisition noise and light\nflickering, that make the encoding process challenging. To deal with the\nspatio-temporal incoherent noise, higher bitrates are used to achieve high\nobjective quality. Additionally, the quality assessment metrics and methods\nhave not been designed, trained or tested for this type of content. This has\ninspired us to trigger research in that area and propose a Grand Challenge on\nencoding low-light video sequences. In this paper, we present an overview of\nthe proposed challenge, and test state-of-the-art methods that will be part of\nthe benchmark methods at the stage of the participants' deliverable assessment.\nFrom this exploration, our results show that VVC already achieves a high\nperformance compared to simply denoising the video source prior to encoding.\nMoreover, the quality of the video streams can be further improved by employing\na post-processing image enhancement method.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 08:22:56 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Anantrasirichai", "Nantheera", ""], ["Zhang", "Fan", ""], ["Malyugina", "Alexandra", ""], ["Hill", "Paul", ""], ["Katsenou", "Angeliki", ""]]}, {"id": "2005.03318", "submitter": "Suranga Seneviratne", "authors": "Jiawei Zhao, Rahat Masood, Suranga Seneviratne", "title": "A Review of Computer Vision Methods in Network Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network security has become an area of significant importance more than ever\nas highlighted by the eye-opening numbers of data breaches, attacks on critical\ninfrastructure, and malware/ransomware/cryptojacker attacks that are reported\nalmost every day. Increasingly, we are relying on networked infrastructure and\nwith the advent of IoT, billions of devices will be connected to the internet,\nproviding attackers with more opportunities to exploit. Traditional machine\nlearning methods have been frequently used in the context of network security.\nHowever, such methods are more based on statistical features extracted from\nsources such as binaries, emails, and packet flows.\n  On the other hand, recent years witnessed a phenomenal growth in computer\nvision mainly driven by the advances in the area of convolutional neural\nnetworks. At a glance, it is not trivial to see how computer vision methods are\nrelated to network security. Nonetheless, there is a significant amount of work\nthat highlighted how methods from computer vision can be applied in network\nsecurity for detecting attacks or building security solutions. In this paper,\nwe provide a comprehensive survey of such work under three topics; i) phishing\nattempt detection, ii) malware detection, and iii) traffic anomaly detection.\nNext, we review a set of such commercial products for which public information\nis available and explore how computer vision methods are effectively used in\nthose products. Finally, we discuss existing research gaps and future research\ndirections, especially focusing on how network security research community and\nthe industry can leverage the exponential growth of computer vision methods to\nbuild much secure networked systems.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 08:29:11 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Zhao", "Jiawei", ""], ["Masood", "Rahat", ""], ["Seneviratne", "Suranga", ""]]}, {"id": "2005.03337", "submitter": "Qiufu Li", "authors": "Qiufu Li, Linlin Shen, Sheng Guo, Zhihui Lai", "title": "Wavelet Integrated CNNs for Noise-Robust Image Classification", "comments": "CVPR accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are generally prone to noise\ninterruptions, i.e., small image noise can cause drastic changes in the output.\nTo suppress the noise effect to the final predication, we enhance CNNs by\nreplacing max-pooling, strided-convolution, and average-pooling with Discrete\nWavelet Transform (DWT). We present general DWT and Inverse DWT (IDWT) layers\napplicable to various wavelets like Haar, Daubechies, and Cohen, etc., and\ndesign wavelet integrated CNNs (WaveCNets) using these layers for image\nclassification. In WaveCNets, feature maps are decomposed into the\nlow-frequency and high-frequency components during the down-sampling. The\nlow-frequency component stores main information including the basic object\nstructures, which is transmitted into the subsequent layers to extract robust\nhigh-level features. The high-frequency components, containing most of the data\nnoise, are dropped during inference to improve the noise-robustness of the\nWaveCNets. Our experimental results on ImageNet and ImageNet-C (the noisy\nversion of ImageNet) show that WaveCNets, the wavelet integrated versions of\nVGG, ResNets, and DenseNet, achieve higher accuracy and better noise-robustness\nthan their vanilla versions.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 09:10:41 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 07:51:21 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Li", "Qiufu", ""], ["Shen", "Linlin", ""], ["Guo", "Sheng", ""], ["Lai", "Zhihui", ""]]}, {"id": "2005.03341", "submitter": "Enze Xie", "authors": "Wenjia Wang, Enze Xie, Xuebo Liu, Wenhai Wang, Ding Liang, Chunhua\n  Shen, and Xiang Bai", "title": "Scene Text Image Super-Resolution in the Wild", "comments": "Accepted by ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-resolution text images are often seen in natural scenes such as documents\ncaptured by mobile phones. Recognizing low-resolution text images is\nchallenging because they lose detailed content information, leading to poor\nrecognition accuracy. An intuitive solution is to introduce super-resolution\n(SR) techniques as pre-processing. However, previous single image\nsuper-resolution (SISR) methods are trained on synthetic low-resolution images\n(e.g.Bicubic down-sampling), which is simple and not suitable for real\nlow-resolution text recognition. To this end, we pro-pose a real scene text SR\ndataset, termed TextZoom. It contains paired real low-resolution and\nhigh-resolution images which are captured by cameras with different focal\nlength in the wild. It is more authentic and challenging than synthetic data,\nas shown in Fig. 1. We argue improv-ing the recognition accuracy is the\nultimate goal for Scene Text SR. In this purpose, a new Text Super-Resolution\nNetwork termed TSRN, with three novel modules is developed. (1) A sequential\nresidual block is proposed to extract the sequential information of the text\nimages. (2) A boundary-aware loss is designed to sharpen the character\nboundaries. (3) A central alignment module is proposed to relieve the\nmisalignment problem in TextZoom. Extensive experiments on TextZoom demonstrate\nthat our TSRN largely improves the recognition accuracy by over 13%of CRNN, and\nby nearly 9.0% of ASTER and MORAN compared to synthetic SR data. Furthermore,\nour TSRN clearly outperforms 7 state-of-the-art SR methods in boosting the\nrecognition accuracy of LR images in TextZoom. For example, it outperforms\nLapSRN by over 5% and 8%on the recognition accuracy of ASTER and CRNN. Our\nresults suggest that low-resolution text recognition in the wild is far from\nbeing solved, thus more research effort is needed.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 09:18:59 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 05:42:50 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2020 03:27:43 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wang", "Wenjia", ""], ["Xie", "Enze", ""], ["Liu", "Xuebo", ""], ["Wang", "Wenhai", ""], ["Liang", "Ding", ""], ["Shen", "Chunhua", ""], ["Bai", "Xiang", ""]]}, {"id": "2005.03345", "submitter": "Masahiro Oda Dr.", "authors": "Masahiro Oda, Natsuki Shimizu, Ken'ichi Karasawa, Yukitaka Nimura,\n  Takayuki Kitasaka, Kazunari Misawa, Michitaka Fujiwara, Daniel Rueckert,\n  Kensaku Mori", "title": "Regression Forest-Based Atlas Localization and Direction Specific Atlas\n  Generation for Pancreas Segmentation", "comments": "Accepted paper as a poster presentation at MICCAI 2016 (International\n  Conference on Medical Image Computing and Computer-Assisted Intervention),\n  Athens, Greece", "journal-ref": "Published in Proceedings of MICCAI 2016, LNCS 9901, pp 556-563", "doi": "10.1007/978-3-319-46723-8_64", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a fully automated atlas-based pancreas segmentation\nmethod from CT volumes utilizing atlas localization by regression forest and\natlas generation using blood vessel information. Previous probabilistic\natlas-based pancreas segmentation methods cannot deal with spatial variations\nthat are commonly found in the pancreas well. Also, shape variations are not\nrepresented by an averaged atlas. We propose a fully automated pancreas\nsegmentation method that deals with two types of variations mentioned above.\nThe position and size of the pancreas is estimated using a regression forest\ntechnique. After localization, a patient-specific probabilistic atlas is\ngenerated based on a new image similarity that reflects the blood vessel\nposition and direction information around the pancreas. We segment it using the\nEM algorithm with the atlas as prior followed by the graph-cut. In evaluation\nresults using 147 CT volumes, the Jaccard index and the Dice overlap of the\nproposed method were 62.1% and 75.1%, respectively. Although we automated all\nof the segmentation processes, segmentation results were superior to the other\nstate-of-the-art methods in the Dice overlap.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 09:20:55 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Oda", "Masahiro", ""], ["Shimizu", "Natsuki", ""], ["Karasawa", "Ken'ichi", ""], ["Nimura", "Yukitaka", ""], ["Kitasaka", "Takayuki", ""], ["Misawa", "Kazunari", ""], ["Fujiwara", "Michitaka", ""], ["Rueckert", "Daniel", ""], ["Mori", "Kensaku", ""]]}, {"id": "2005.03354", "submitter": "Shaopeng Guo", "authors": "Shaopeng Guo and Yujie Wang and Quanquan Li and Junjie Yan", "title": "DMCP: Differentiable Markov Channel Pruning for Neural Networks", "comments": "CVPR2020 Oral. Code has been released at https://github.com/zx55/dmcp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works imply that the channel pruning can be regarded as searching\noptimal sub-structure from unpruned networks. However, existing works based on\nthis observation require training and evaluating a large number of structures,\nwhich limits their application. In this paper, we propose a novel\ndifferentiable method for channel pruning, named Differentiable Markov Channel\nPruning (DMCP), to efficiently search the optimal sub-structure. Our method is\ndifferentiable and can be directly optimized by gradient descent with respect\nto standard task loss and budget regularization (e.g. FLOPs constraint). In\nDMCP, we model the channel pruning as a Markov process, in which each state\nrepresents for retaining the corresponding channel during pruning, and\ntransitions between states denote the pruning process. In the end, our method\nis able to implicitly select the proper number of channels in each layer by the\nMarkov process with optimized transitions. To validate the effectiveness of our\nmethod, we perform extensive experiments on Imagenet with ResNet and\nMobilenetV2. Results show our method can achieve consistent improvement than\nstate-of-the-art pruning methods in various FLOPs settings. The code is\navailable at https://github.com/zx55/dmcp\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 09:39:55 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 03:41:52 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Guo", "Shaopeng", ""], ["Wang", "Yujie", ""], ["Li", "Quanquan", ""], ["Yan", "Junjie", ""]]}, {"id": "2005.03356", "submitter": "Seongho Choi", "authors": "Seongho Choi, Kyoung-Woon On, Yu-Jung Heo, Ahjeong Seo, Youwon Jang,\n  Minsu Lee, Byoung-Tak Zhang", "title": "DramaQA: Character-Centered Video Story Understanding with Hierarchical\n  QA", "comments": "15 pages, 11 figures, accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent progress on computer vision and natural language processing,\ndeveloping a machine that can understand video story is still hard to achieve\ndue to the intrinsic difficulty of video story. Moreover, researches on how to\nevaluate the degree of video understanding based on human cognitive process\nhave not progressed as yet. In this paper, we propose a novel video question\nanswering (Video QA) task, DramaQA, for a comprehensive understanding of the\nvideo story. The DramaQA focuses on two perspectives: 1) Hierarchical QAs as an\nevaluation metric based on the cognitive developmental stages of human\nintelligence. 2) Character-centered video annotations to model local coherence\nof the story. Our dataset is built upon the TV drama \"Another Miss Oh\" and it\ncontains 17,983 QA pairs from 23,928 various length video clips, with each QA\npair belonging to one of four difficulty levels. We provide 217,308 annotated\nimages with rich character-centered annotations, including visual bounding\nboxes, behaviors and emotions of main characters, and coreference resolved\nscripts. Additionally, we suggest Multi-level Context Matching model which\nhierarchically understands character-centered representations of video to\nanswer questions. We release our dataset and model publicly for research\npurposes, and we expect our work to provide a new perspective on video story\nunderstanding research.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 09:44:58 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 02:59:37 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Choi", "Seongho", ""], ["On", "Kyoung-Woon", ""], ["Heo", "Yu-Jung", ""], ["Seo", "Ahjeong", ""], ["Jang", "Youwon", ""], ["Lee", "Minsu", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "2005.03358", "submitter": "Feitong Tan", "authors": "Feitong Tan, Hao Zhu, Zhaopeng Cui, Siyu Zhu, Marc Pollefeys, Ping Tan", "title": "Self-Supervised Human Depth Estimation from Monocular Videos", "comments": "Accepted by IEEE Conference on Computer Vision and Patten Recognition\n  (CVPR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous methods on estimating detailed human depth often require supervised\ntraining with `ground truth' depth data. This paper presents a self-supervised\nmethod that can be trained on YouTube videos without known depth, which makes\ntraining data collection simple and improves the generalization of the learned\nnetwork. The self-supervised learning is achieved by minimizing a\nphoto-consistency loss, which is evaluated between a video frame and its\nneighboring frames warped according to the estimated depth and the 3D non-rigid\nmotion of the human body. To solve this non-rigid motion, we first estimate a\nrough SMPL model at each video frame and compute the non-rigid body motion\naccordingly, which enables self-supervised learning on estimating the shape\ndetails. Experiments demonstrate that our method enjoys better generalization\nand performs much better on data in the wild.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 09:45:11 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Tan", "Feitong", ""], ["Zhu", "Hao", ""], ["Cui", "Zhaopeng", ""], ["Zhu", "Siyu", ""], ["Pollefeys", "Marc", ""], ["Tan", "Ping", ""]]}, {"id": "2005.03367", "submitter": "Joyce Nakatumba-Nabende Dr.", "authors": "Jeremy Francis Tusubira, Benjamin Akera, Solomon Nsumba, Joyce\n  Nakatumba-Nabende, Ernest Mwebaze", "title": "Scoring Root Necrosis in Cassava Using Semantic Segmentation", "comments": "15 pages, 5 figures. Accepted as a poster in The 1st International\n  Workshop and Prize Challenge on Agriculture vision: Challenges &\n  Opportunities for Computer Vision In Agriculture in conjunction with IEEE/CVF\n  CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cassava a major food crop in many parts of Africa, has majorly been affected\nby Cassava Brown Streak Disease (CBSD). The disease affects tuberous roots and\npresents symptoms that include a yellow/brown, dry, corky necrosis within the\nstarch-bearing tissues. Cassava breeders currently depend on visual inspection\nto score necrosis in roots based on a qualitative score which is quite\nsubjective. In this paper we present an approach to automate root necrosis\nscoring using deep convolutional neural networks with semantic segmentation.\nOur experiments show that the UNet model performs this task with high accuracy\nachieving a mean Intersection over Union (IoU) of 0.90 on the test set. This\nmethod provides a means to use a quantitative measure for necrosis scoring on\nroot cross-sections. This is done by segmentation and classifying the\nnecrotized and non-necrotized pixels of cassava root cross-sections without any\nadditional feature engineering.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 10:09:20 GMT"}], "update_date": "2020-05-17", "authors_parsed": [["Tusubira", "Jeremy Francis", ""], ["Akera", "Benjamin", ""], ["Nsumba", "Solomon", ""], ["Nakatumba-Nabende", "Joyce", ""], ["Mwebaze", "Ernest", ""]]}, {"id": "2005.03372", "submitter": "Peng Wang", "authors": "Peng Wang, Lingjie Liu, Nenglun Chen, Hung-Kuo Chu, Christian\n  Theobalt, Wenping Wang", "title": "Vid2Curve: Simultaneous Camera Motion Estimation and Thin Structure\n  Reconstruction from an RGB Video", "comments": "Accepted by SIGGRAPH 2020", "journal-ref": null, "doi": "10.1145/3386569.3392476", "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thin structures, such as wire-frame sculptures, fences, cables, power lines,\nand tree branches, are common in the real world. It is extremely challenging to\nacquire their 3D digital models using traditional image-based or depth-based\nreconstruction methods because thin structures often lack distinct point\nfeatures and have severe self-occlusion. We propose the first approach that\nsimultaneously estimates camera motion and reconstructs the geometry of complex\n3D thin structures in high quality from a color video captured by a handheld\ncamera. Specifically, we present a new curve-based approach to estimate\naccurate camera poses by establishing correspondences between featureless thin\nobjects in the foreground in consecutive video frames, without requiring visual\ntexture in the background scene to lock on. Enabled by this effective\ncurve-based camera pose estimation strategy, we develop an iterative\noptimization method with tailored measures on geometry, topology as well as\nself-occlusion handling for reconstructing 3D thin structures. Extensive\nvalidations on a variety of thin structures show that our method achieves\naccurate camera pose estimation and faithful reconstruction of 3D thin\nstructures with complex shape and topology at a level that has not been\nattained by other existing reconstruction methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 10:39:20 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 00:48:19 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 04:57:24 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Wang", "Peng", ""], ["Liu", "Lingjie", ""], ["Chen", "Nenglun", ""], ["Chu", "Hung-Kuo", ""], ["Theobalt", "Christian", ""], ["Wang", "Wenping", ""]]}, {"id": "2005.03382", "submitter": "Behrouz Bolourian Haghighi", "authors": "Behrouz Bolourian Haghighi, Amir Hossein Taherinia, Ahad Harati,\n  Modjtaba Rouhani", "title": "WSMN: An optimized multipurpose blind watermarking in Shearlet domain\n  using MLP and NSGA-II", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital watermarking is a remarkable issue in the field of information\nsecurity to avoid the misuse of images in multimedia networks. Although access\nto unauthorized persons can be prevented through cryptography, it cannot be\nsimultaneously used for copyright protection or content authentication with the\npreservation of image integrity. Hence, this paper presents an optimized\nmultipurpose blind watermarking in Shearlet domain with the help of smart\nalgorithms including MLP and NSGA-II. In this method, four copies of the robust\ncopyright logo are embedded in the approximate coefficients of Shearlet by\nusing an effective quantization technique. Furthermore, an embedded random\nsequence as a semi-fragile authentication mark is effectively extracted from\ndetails by the neural network. Due to performing an effective optimization\nalgorithm for selecting optimum embedding thresholds, and also distinguishing\nthe texture of blocks, the imperceptibility and robustness have been preserved.\nThe experimental results reveal the superiority of the scheme with regard to\nthe quality of watermarked images and robustness against hybrid attacks over\nother state-of-the-art schemes. The average PSNR and SSIM of the dual\nwatermarked images are 38 dB and 0.95, respectively; Besides, it can\neffectively extract the copyright logo and locates forgery regions under severe\nattacks with satisfactory accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 11:14:46 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Haghighi", "Behrouz Bolourian", ""], ["Taherinia", "Amir Hossein", ""], ["Harati", "Ahad", ""], ["Rouhani", "Modjtaba", ""]]}, {"id": "2005.03388", "submitter": "Li Weng Dr.", "authors": "Li Weng, Valerie Gouet-Brunet, Bahman Soheilian", "title": "Semantic Signatures for Large-scale Visual Localization", "comments": "12 pages, 22 figures, Multimedia Tools and Applications (2020)", "journal-ref": null, "doi": "10.1007/s11042-020-08992-6", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization is a useful alternative to standard localization\ntechniques. It works by utilizing cameras. In a typical scenario, features are\nextracted from captured images and compared with geo-referenced databases.\nLocation information is then inferred from the matching results. Conventional\nschemes mainly use low-level visual features. These approaches offer good\naccuracy but suffer from scalability issues. In order to assist localization in\nlarge urban areas, this work explores a different path by utilizing high-level\nsemantic information. It is found that object information in a street view can\nfacilitate localization. A novel descriptor scheme called \"semantic signature\"\nis proposed to summarize this information. A semantic signature consists of\ntype and angle information of visible objects at a spatial location. Several\nmetrics and protocols are proposed for signature comparison and retrieval. They\nillustrate different trade-offs between accuracy and complexity. Extensive\nsimulation results confirm the potential of the proposed scheme in large-scale\napplications. This paper is an extended version of a conference paper in\nCBMI'18. A more efficient retrieval protocol is presented with additional\nexperiment results.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 11:33:10 GMT"}], "update_date": "2020-06-28", "authors_parsed": [["Weng", "Li", ""], ["Gouet-Brunet", "Valerie", ""], ["Soheilian", "Bahman", ""]]}, {"id": "2005.03405", "submitter": "Feng Shi", "authors": "Xiaofeng Zhu, Bin Song, Feng Shi, Yanbo Chen, Rongyao Hu, Jiangzhang\n  Gan, Wenhai Zhang, Man Li, Liye Wang, Yaozong Gao, Fei Shan, Dinggang Shen", "title": "Joint Prediction and Time Estimation of COVID-19 Developing Severe\n  Symptoms using Chest CT Scan", "comments": null, "journal-ref": "Medical Image Analysis (2020)", "doi": "10.1016/j.media.2020.101824", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapidly worldwide spread of Coronavirus disease (COVID-19), it is of\ngreat importance to conduct early diagnosis of COVID-19 and predict the time\nthat patients might convert to the severe stage, for designing effective\ntreatment plan and reducing the clinicians' workloads. In this study, we\npropose a joint classification and regression method to determine whether the\npatient would develop severe symptoms in the later time, and if yes, predict\nthe possible conversion time that the patient would spend to convert to the\nsevere stage. To do this, the proposed method takes into account 1) the weight\nfor each sample to reduce the outliers' influence and explore the problem of\nimbalance classification, and 2) the weight for each feature via a sparsity\nregularization term to remove the redundant features of high-dimensional data\nand learn the shared information across the classification task and the\nregression task. To our knowledge, this study is the first work to predict the\ndisease progression and the conversion time, which could help clinicians to\ndeal with the potential severe cases in time or even save the patients' lives.\nExperimental analysis was conducted on a real data set from two hospitals with\n422 chest computed tomography (CT) scans, where 52 cases were converted to\nsevere on average 5.64 days and 34 cases were severe at admission. Results show\nthat our method achieves the best classification (e.g., 85.91% of accuracy) and\nregression (e.g., 0.462 of the correlation coefficient) performance, compared\nto all comparison methods. Moreover, our proposed method yields 76.97% of\naccuracy for predicting the severe cases, 0.524 of the correlation coefficient,\nand 0.55 days difference for the converted time.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 12:16:37 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Zhu", "Xiaofeng", ""], ["Song", "Bin", ""], ["Shi", "Feng", ""], ["Chen", "Yanbo", ""], ["Hu", "Rongyao", ""], ["Gan", "Jiangzhang", ""], ["Zhang", "Wenhai", ""], ["Li", "Man", ""], ["Wang", "Liye", ""], ["Gao", "Yaozong", ""], ["Shan", "Fei", ""], ["Shen", "Dinggang", ""]]}, {"id": "2005.03412", "submitter": "Boaz Arad", "authors": "Boaz Arad, Radu Timofte, Ohad Ben-Shahar, Yi-Tun Lin, Graham\n  Finlayson, Shai Givati, and others", "title": "NTIRE 2020 Challenge on Spectral Reconstruction from an RGB Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the second challenge on spectral reconstruction from RGB\nimages, i.e., the recovery of whole-scene hyperspectral (HS) information from a\n3-channel RGB image. As in the previous challenge, two tracks were provided:\n(i) a \"Clean\" track where HS images are estimated from noise-free RGBs, the RGB\nimages are themselves calculated numerically using the ground-truth HS images\nand supplied spectral sensitivity functions (ii) a \"Real World\" track,\nsimulating capture by an uncalibrated and unknown camera, where the HS images\nare recovered from noisy JPEG-compressed RGB images. A new, larger-than-ever,\nnatural hyperspectral image data set is presented, containing a total of 510 HS\nimages. The Clean and Real World tracks had 103 and 78 registered participants\nrespectively, with 14 teams competing in the final testing phase. A description\nof the proposed methods, alongside their challenge scores and an extensive\nevaluation of top performing methods is also provided. They gauge the\nstate-of-the-art in spectral reconstruction from an RGB image.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 12:23:56 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Arad", "Boaz", ""], ["Timofte", "Radu", ""], ["Ben-Shahar", "Ohad", ""], ["Lin", "Yi-Tun", ""], ["Finlayson", "Graham", ""], ["Givati", "Shai", ""], ["others", "", ""]]}, {"id": "2005.03415", "submitter": "Wojciech Dudzik", "authors": "Wojciech Dudzik, Damian Kosowski", "title": "Kunster -- AR Art Video Maker -- Real time video neural style transfer\n  on mobile devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural style transfer is a well-known branch of deep learning research, with\nmany interesting works and two major drawbacks. Most of the works in the field\nare hard to use by non-expert users and substantial hardware resources are\nrequired. In this work, we present a solution to both of these problems. We\nhave applied neural style transfer to real-time video (over 25 frames per\nsecond), which is capable of running on mobile devices. We also investigate the\nworks on achieving temporal coherence and present the idea of fine-tuning,\nalready trained models, to achieve stable video. What is more, we also analyze\nthe impact of the common deep neural network architecture on the performance of\nmobile devices with regard to number of layers and filters present. In the\nexperiment section we present the results of our work with respect to the iOS\ndevices and discuss the problems present in current Android devices as well as\nfuture possibilities. At the end we present the qualitative results of\nstylization and quantitative results of performance tested on the iPhone 11 Pro\nand iPhone 6s. The presented work is incorporated in Kunster - AR Art Video\nMaker application available in the Apple's App Store.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 12:30:48 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Dudzik", "Wojciech", ""], ["Kosowski", "Damian", ""]]}, {"id": "2005.03452", "submitter": "Christopher Zach", "authors": "Rasmus Kj{\\ae}r H{\\o}ier, Christopher Zach", "title": "Lifted Regression/Reconstruction Networks", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose lifted regression/reconstruction networks (LRRNs),\nwhich combine lifted neural networks with a guaranteed Lipschitz continuity\nproperty for the output layer. Lifted neural networks explicitly optimize an\nenergy model to infer the unit activations and therefore---in contrast to\nstandard feed-forward neural networks---allow bidirectional feedback between\nlayers. So far lifted neural networks have been modelled around standard\nfeed-forward architectures. We propose to take further advantage of the\nfeedback property by letting the layers simultaneously perform regression and\nreconstruction. The resulting lifted network architecture allows to control the\ndesired amount of Lipschitz continuity, which is an important feature to obtain\nadversarially robust regression and classification methods. We analyse and\nnumerically demonstrate applications for unsupervised and supervised learning.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 13:24:46 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["H\u00f8ier", "Rasmus Kj\u00e6r", ""], ["Zach", "Christopher", ""]]}, {"id": "2005.03457", "submitter": "Radu Timofte", "authors": "Codruta O. Ancuti, Cosmin Ancuti, Florin-Alexandru Vasluianu, Radu\n  Timofte, Jing Liu, Haiyan Wu, Yuan Xie, Yanyun Qu, Lizhuang Ma, Ziling Huang,\n  Qili Deng, Ju-Chin Chao, Tsung-Shan Yang, Peng-Wen Chen, Po-Min Hsu, Tzu-Yi\n  Liao, Chung-En Sun, Pei-Yuan Wu, Jeonghyeok Do, Jongmin Park, Munchurl Kim,\n  Kareem Metwaly, Xuelu Li, Tiantong Guo, Vishal Monga, Mingzhao Yu,\n  Venkateswararao Cherukuri, Shiue-Yuan Chuang, Tsung-Nan Lin, David Lee,\n  Jerome Chang, Zhan-Han Wang, Yu-Bang Chang, Chang-Hong Lin, Yu Dong, Hongyu\n  Zhou, Xiangzhen Kong, Sourya Dipta Das, Saikat Dutta, Xuan Zhao, Bing Ouyang,\n  Dennis Estrada, Meiqi Wang, Tianqi Su, Siyi Chen, Bangyong Sun, Vincent\n  Whannou de Dravo, Zhe Yu, Pratik Narang, Aryan Mehra, Navaneeth Raghunath,\n  Murari Mandal", "title": "NTIRE 2020 Challenge on NonHomogeneous Dehazing", "comments": "CVPR Workshops Proceedings 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the NTIRE 2020 Challenge on NonHomogeneous Dehazing of\nimages (restoration of rich details in hazy image). We focus on the proposed\nsolutions and their results evaluated on NH-Haze, a novel dataset consisting of\n55 pairs of real haze free and nonhomogeneous hazy images recorded outdoor.\nNH-Haze is the first realistic nonhomogeneous haze dataset that provides ground\ntruth images. The nonhomogeneous haze has been produced using a professional\nhaze generator that imitates the real conditions of haze scenes. 168\nparticipants registered in the challenge and 27 teams competed in the final\ntesting phase. The proposed solutions gauge the state-of-the-art in image\ndehazing.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 13:29:56 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Ancuti", "Codruta O.", ""], ["Ancuti", "Cosmin", ""], ["Vasluianu", "Florin-Alexandru", ""], ["Timofte", "Radu", ""], ["Liu", "Jing", ""], ["Wu", "Haiyan", ""], ["Xie", "Yuan", ""], ["Qu", "Yanyun", ""], ["Ma", "Lizhuang", ""], ["Huang", "Ziling", ""], ["Deng", "Qili", ""], ["Chao", "Ju-Chin", ""], ["Yang", "Tsung-Shan", ""], ["Chen", "Peng-Wen", ""], ["Hsu", "Po-Min", ""], ["Liao", "Tzu-Yi", ""], ["Sun", "Chung-En", ""], ["Wu", "Pei-Yuan", ""], ["Do", "Jeonghyeok", ""], ["Park", "Jongmin", ""], ["Kim", "Munchurl", ""], ["Metwaly", "Kareem", ""], ["Li", "Xuelu", ""], ["Guo", "Tiantong", ""], ["Monga", "Vishal", ""], ["Yu", "Mingzhao", ""], ["Cherukuri", "Venkateswararao", ""], ["Chuang", "Shiue-Yuan", ""], ["Lin", "Tsung-Nan", ""], ["Lee", "David", ""], ["Chang", "Jerome", ""], ["Wang", "Zhan-Han", ""], ["Chang", "Yu-Bang", ""], ["Lin", "Chang-Hong", ""], ["Dong", "Yu", ""], ["Zhou", "Hongyu", ""], ["Kong", "Xiangzhen", ""], ["Das", "Sourya Dipta", ""], ["Dutta", "Saikat", ""], ["Zhao", "Xuan", ""], ["Ouyang", "Bing", ""], ["Estrada", "Dennis", ""], ["Wang", "Meiqi", ""], ["Su", "Tianqi", ""], ["Chen", "Siyi", ""], ["Sun", "Bangyong", ""], ["de Dravo", "Vincent Whannou", ""], ["Yu", "Zhe", ""], ["Narang", "Pratik", ""], ["Mehra", "Aryan", ""], ["Raghunath", "Navaneeth", ""], ["Mandal", "Murari", ""]]}, {"id": "2005.03459", "submitter": "Wanling Gao", "authors": "Wanling Gao, Fei Tang, Jianfeng Zhan, Xu Wen, Lei Wang, Zheng Cao,\n  Chuanxin Lan, Chunjie Luo, Xiaoli Liu, Zihan Jiang", "title": "Scenario-distilling AI Benchmarking", "comments": "23 pages. arXiv admin note: text overlap with arXiv:2002.07162", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern real-world application scenarios like Internet services not only\nconsist of diversity of AI and non-AI modules with very long and complex\nexecution paths, but also have huge code size, which raises serious\nbenchmarking or evaluating challenges. Using AI components or micro benchmarks\nalone can lead to error-prone conclusions. This paper presents a\nscenario-distilling methodology to attack the above challenge. We formalize a\nreal-world application scenario as a Directed Acyclic Graph-based model, and\npropose the rules to distill it into the permutation of essential AI and non-AI\ntasks as a high-level scenario benchmark specification. Together with seventeen\nindustry partners, we extract nine typical application scenarios, and identify\nthe primary components. We design and implement a highly extensible,\nconfigurable, and flexible benchmark framework, on the basis of which, we\nimplement two Internet service AI scenario benchmarks as proxies to two\nreal-world application scenarios. We claim scenario, component and micro\nbenchmarks should be considered as three indispensable parts for evaluating.\nOur evaluation shows the advantage of our methodology against using component\nor micro AI benchmarks alone. The specifications, source code, testbed, and\nresults are publicly available from\n\\url{https://www.benchcouncil.org/aibench-scenario/index.html}.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 01:24:25 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 11:11:14 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Gao", "Wanling", ""], ["Tang", "Fei", ""], ["Zhan", "Jianfeng", ""], ["Wen", "Xu", ""], ["Wang", "Lei", ""], ["Cao", "Zheng", ""], ["Lan", "Chuanxin", ""], ["Luo", "Chunjie", ""], ["Liu", "Xiaoli", ""], ["Jiang", "Zihan", ""]]}, {"id": "2005.03463", "submitter": "Rito Murase", "authors": "Rito Murase, Masanori Suganuma and Takayuki Okatani", "title": "How Can CNNs Use Image Position for Segmentation?", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution is an equivariant operation, and image position does not affect\nits result. A recent study shows that the zero-padding employed in\nconvolutional layers of CNNs provides position information to the CNNs. The\nstudy further claims that the position information enables accurate inference\nfor several tasks, such as object recognition, segmentation, etc. However,\nthere is a technical issue with the design of the experiments of the study, and\nthus the correctness of the claim is yet to be verified. Moreover, the absolute\nimage position may not be essential for the segmentation of natural images, in\nwhich target objects will appear at any image position. In this study, we\ninvestigate how positional information is and can be utilized for segmentation\ntasks. Toward this end, we consider {\\em positional encoding} (PE) that adds\nchannels embedding image position to the input images and compare PE with\nseveral padding methods. Considering the above nature of natural images, we\nchoose medical image segmentation tasks, in which the absolute position appears\nto be relatively important, as the same organs (of different patients) are\ncaptured in similar sizes and positions. We draw a mixed conclusion from the\nexperimental results; the positional encoding certainly works in some cases,\nbut the absolute image position may not be so important for segmentation tasks\nas we think.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 13:38:13 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Murase", "Rito", ""], ["Suganuma", "Masanori", ""], ["Okatani", "Takayuki", ""]]}, {"id": "2005.03492", "submitter": "Xiaoxue Chen", "authors": "Xiaoxue Chen, Lianwen Jin, Yuanzhi Zhu, Canjie Luo, and Tianwei Wang", "title": "Text Recognition in the Wild: A Survey", "comments": "Accepted by ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The history of text can be traced back over thousands of years. Rich and\nprecise semantic information carried by text is important in a wide range of\nvision-based application scenarios. Therefore, text recognition in natural\nscenes has been an active research field in computer vision and pattern\nrecognition. In recent years, with the rise and development of deep learning,\nnumerous methods have shown promising in terms of innovation, practicality, and\nefficiency. This paper aims to (1) summarize the fundamental problems and the\nstate-of-the-art associated with scene text recognition; (2) introduce new\ninsights and ideas; (3) provide a comprehensive review of publicly available\nresources; (4) point out directions for future work. In summary, this\nliterature review attempts to present the entire picture of the field of scene\ntext recognition. It provides a comprehensive reference for people entering\nthis field, and could be helpful to inspire future research. Related resources\nare available at our Github repository:\nhttps://github.com/HCIILAB/Scene-Text-Recognition.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 13:57:04 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 08:51:04 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 07:06:27 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Chen", "Xiaoxue", ""], ["Jin", "Lianwen", ""], ["Zhu", "Yuanzhi", ""], ["Luo", "Canjie", ""], ["Wang", "Tianwei", ""]]}, {"id": "2005.03501", "submitter": "Annika Reinke", "authors": "Lena Maier-Hein, Martin Wagner, Tobias Ross, Annika Reinke, Sebastian\n  Bodenstedt, Peter M. Full, Hellena Hempe, Diana Mindroc-Filimon, Patrick\n  Scholz, Thuy Nuong Tran, Pierangela Bruno, Anna Kisilenko, Benjamin M\\\"uller,\n  Tornike Davitashvili, Manuela Capek, Minu Tizabi, Matthias Eisenmann, Tim J.\n  Adler, Janek Gr\\\"ohl, Melanie Schellenberg, Silvia Seidlitz, T. Y. Emmy Lai,\n  B\\\"unyamin Pekdemir, Veith Roethlingshoefer, Fabian Both, Sebastian Bittel,\n  Marc Mengler, Lars M\\\"undermann, Martin Apitz, Annette Kopp-Schneider,\n  Stefanie Speidel, Hannes G. Kenngott, Beat P. M\\\"uller-Stich", "title": "Heidelberg Colorectal Data Set for Surgical Data Science in the Sensor\n  Operating Room", "comments": "Submitted to Nature Scientific Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based tracking of medical instruments is an integral part of surgical\ndata science applications. Previous research has addressed the tasks of\ndetecting, segmenting and tracking medical instruments based on laparoscopic\nvideo data. However, the proposed methods still tend to fail when applied to\nchallenging images and do not generalize well to data they have not been\ntrained on. This paper introduces the Heidelberg Colorectal (HeiCo) data set -\nthe first publicly available data set enabling comprehensive benchmarking of\nmedical instrument detection and segmentation algorithms with a specific\nemphasis on method robustness and generalization capabilities. Our data set\ncomprises 30 laparoscopic videos and corresponding sensor data from medical\ndevices in the operating room for three different types of laparoscopic\nsurgery. Annotations include surgical phase labels for all video frames as well\nas information on instrument presence and corresponding instance-wise\nsegmentation masks for surgical instruments (if any) in more than 10,000\nindividual frames. The data has successfully been used to organize\ninternational competitions within the Endoscopic Vision Challenges 2017 and\n2019.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 14:04:29 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 12:21:51 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 06:35:39 GMT"}, {"version": "v4", "created": "Fri, 9 Oct 2020 10:18:50 GMT"}, {"version": "v5", "created": "Tue, 23 Feb 2021 14:32:49 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Maier-Hein", "Lena", ""], ["Wagner", "Martin", ""], ["Ross", "Tobias", ""], ["Reinke", "Annika", ""], ["Bodenstedt", "Sebastian", ""], ["Full", "Peter M.", ""], ["Hempe", "Hellena", ""], ["Mindroc-Filimon", "Diana", ""], ["Scholz", "Patrick", ""], ["Tran", "Thuy Nuong", ""], ["Bruno", "Pierangela", ""], ["Kisilenko", "Anna", ""], ["M\u00fcller", "Benjamin", ""], ["Davitashvili", "Tornike", ""], ["Capek", "Manuela", ""], ["Tizabi", "Minu", ""], ["Eisenmann", "Matthias", ""], ["Adler", "Tim J.", ""], ["Gr\u00f6hl", "Janek", ""], ["Schellenberg", "Melanie", ""], ["Seidlitz", "Silvia", ""], ["Lai", "T. Y. Emmy", ""], ["Pekdemir", "B\u00fcnyamin", ""], ["Roethlingshoefer", "Veith", ""], ["Both", "Fabian", ""], ["Bittel", "Sebastian", ""], ["Mengler", "Marc", ""], ["M\u00fcndermann", "Lars", ""], ["Apitz", "Martin", ""], ["Kopp-Schneider", "Annette", ""], ["Speidel", "Stefanie", ""], ["Kenngott", "Hannes G.", ""], ["M\u00fcller-Stich", "Beat P.", ""]]}, {"id": "2005.03560", "submitter": "Radu Timofte", "authors": "Codruta O. Ancuti, Cosmin Ancuti, Radu Timofte", "title": "NH-HAZE: An Image Dehazing Benchmark with Non-Homogeneous Hazy and\n  Haze-Free Images", "comments": "CVPR 2020 Workshops proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image dehazing is an ill-posed problem that has been extensively studied in\nthe recent years. The objective performance evaluation of the dehazing methods\nis one of the major obstacles due to the lacking of a reference dataset. While\nthe synthetic datasets have shown important limitations, the few realistic\ndatasets introduced recently assume homogeneous haze over the entire scene.\nSince in many real cases haze is not uniformly distributed we introduce\nNH-HAZE, a non-homogeneous realistic dataset with pairs of real hazy and\ncorresponding haze-free images. This is the first non-homogeneous image\ndehazing dataset and contains 55 outdoor scenes. The non-homogeneous haze has\nbeen introduced in the scene using a professional haze generator that imitates\nthe real conditions of hazy scenes. Additionally, this work presents an\nobjective assessment of several state-of-the-art single image dehazing methods\nthat were evaluated using NH-HAZE dataset.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 15:50:37 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Ancuti", "Codruta O.", ""], ["Ancuti", "Cosmin", ""], ["Timofte", "Radu", ""]]}, {"id": "2005.03566", "submitter": "Xiangxiang Chu", "authors": "Xiangxiang Chu and Bo Zhang and Xudong Li", "title": "Noisy Differentiable Architecture Search", "comments": "Make use of noise to address collapse from excessive skip connections\n  in DARTS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simplicity is the ultimate sophistication. Differentiable Architecture Search\n(DARTS) has now become one of the mainstream paradigms of neural architecture\nsearch. However, it largely suffers from several disturbing factors of\noptimization process whose results are unstable to reproduce. FairDARTS points\nout that skip connections natively have an unfair advantage in exclusive\ncompetition which primarily leads to dramatic performance collapse. While\nFairDARTS turns the unfair competition into a collaborative one, we instead\nimpede such unfair advantage by injecting unbiased random noise into skip\noperations' output. In effect, the optimizer should perceive this difficulty at\neach training step and refrain from overshooting on skip connections, but in a\nlong run it still converges to the right solution area since no bias is added\nto the gradient. We name this novel approach as NoisyDARTS. Our experiments on\nCIFAR-10 and ImageNet attest that it can effectively break the skip\nconnection's unfair advantage and yield better performance. It generates a\nseries of models that achieve state-of-the-art results on both datasets. Code\nwill be made available at https://github.com/xiaomi-automl/NoisyDARTS.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 15:53:52 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 14:42:33 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Chu", "Xiangxiang", ""], ["Zhang", "Bo", ""], ["Li", "Xudong", ""]]}, {"id": "2005.03572", "submitter": "Dongwei Ren", "authors": "Zhaohui Zheng and Ping Wang and Dongwei Ren and Wei Liu and Rongguang\n  Ye and Qinghua Hu and Wangmeng Zuo", "title": "Enhancing Geometric Factors in Model Learning and Inference for Object\n  Detection and Instance Segmentation", "comments": "This work has been accepted to IEEE Transactions on Cybernetics. The\n  source codes are available at https://github.com/Zzh-tju/CIoU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based object detection and instance segmentation have achieved\nunprecedented progress. In this paper, we propose Complete-IoU (CIoU) loss and\nCluster-NMS for enhancing geometric factors in both bounding box regression and\nNon-Maximum Suppression (NMS), leading to notable gains of average precision\n(AP) and average recall (AR), without the sacrifice of inference efficiency. In\nparticular, we consider three geometric factors, i.e., overlap area, normalized\ncentral point distance and aspect ratio, which are crucial for measuring\nbounding box regression in object detection and instance segmentation. The\nthree geometric factors are then incorporated into CIoU loss for better\ndistinguishing difficult regression cases. The training of deep models using\nCIoU loss results in consistent AP and AR improvements in comparison to widely\nadopted $\\ell_n$-norm loss and IoU-based loss. Furthermore, we propose\nCluster-NMS, where NMS during inference is done by implicitly clustering\ndetected boxes and usually requires less iterations. Cluster-NMS is very\nefficient due to its pure GPU implementation, and geometric factors can be\nincorporated to improve both AP and AR. In the experiments, CIoU loss and\nCluster-NMS have been applied to state-of-the-art instance segmentation (e.g.,\nYOLACT and BlendMask-RT), and object detection (e.g., YOLO v3, SSD and Faster\nR-CNN) models. Taking YOLACT on MS COCO as an example, our method achieves\nperformance gains as +1.7 AP and +6.2 AR$_{100}$ for object detection, and +0.9\nAP and +3.5 AR$_{100}$ for instance segmentation, with 27.1 FPS on one NVIDIA\nGTX 1080Ti GPU. All the source code and trained models are available at\nhttps://github.com/Zzh-tju/CIoU\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 16:00:27 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 09:38:09 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 08:20:23 GMT"}, {"version": "v4", "created": "Mon, 5 Jul 2021 08:21:41 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zheng", "Zhaohui", ""], ["Wang", "Ping", ""], ["Ren", "Dongwei", ""], ["Liu", "Wei", ""], ["Ye", "Rongguang", ""], ["Hu", "Qinghua", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "2005.03597", "submitter": "Kai Jia", "authors": "Kai Jia, Martin Rinard", "title": "Efficient Exact Verification of Binarized Neural Networks", "comments": "To be published in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.LO cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concerned with the reliability of neural networks, researchers have developed\nverification techniques to prove their robustness. Most verifiers work with\nreal-valued networks. Unfortunately, the exact (complete and sound) verifiers\nface scalability challenges and provide no correctness guarantees due to\nfloating point errors. We argue that Binarized Neural Networks (BNNs) provide\ncomparable robustness and allow exact and significantly more efficient\nverification. We present a new system, EEV, for efficient and exact\nverification of BNNs. EEV consists of two parts: (i) a novel SAT solver that\nspeeds up BNN verification by natively handling the reified cardinality\nconstraints arising in BNN encodings; and (ii) strategies to train\nsolver-friendly robust BNNs by inducing balanced layer-wise sparsity and low\ncardinality bounds, and adaptively cancelling the gradients. We demonstrate the\neffectiveness of EEV by presenting the first exact verification results for\nL-inf-bounded adversarial robustness of nontrivial convolutional BNNs on the\nMNIST and CIFAR10 datasets. Compared to exact verification of real-valued\nnetworks of the same architectures on the same tasks, EEV verifies BNNs\nhundreds to thousands of times faster, while delivering comparable verifiable\naccuracy in most cases.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 16:34:30 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 04:00:16 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Jia", "Kai", ""], ["Rinard", "Martin", ""]]}, {"id": "2005.03626", "submitter": "Antonio Busson", "authors": "Antonio Jos\\'e G. Busson, S\\'ergio Colcher, Ruy Luiz Milidi\\'u, Bruno\n  Pereira Dias, and Andr\\'e Bulc\\~ao", "title": "Seismic Shot Gather Noise Localization Using a Multi-Scale\n  Feature-Fusion-Based Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based models, such as convolutional neural networks, have\nadvanced various segments of computer vision. However, this technology is\nrarely applied to seismic shot gather noise localization problem. This letter\npresents an investigation on the effectiveness of a multi-scale\nfeature-fusion-based network for seismic shot-gather noise localization.\nHerein, we describe the following: (1) the construction of a real-world dataset\nof seismic noise localization based on 6,500 seismograms; (2) a multi-scale\nfeature-fusion-based detector that uses the MobileNet combined with the Feature\nPyramid Net as the backbone; and (3) the Single Shot multi-box detector for box\nclassification/regression. Additionally, we propose the use of the Focal Loss\nfunction that improves the detector's prediction accuracy. The proposed\ndetector achieves an AP@0.5 of 78.67\\% in our empirical evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 17:23:55 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Busson", "Antonio Jos\u00e9 G.", ""], ["Colcher", "S\u00e9rgio", ""], ["Milidi\u00fa", "Ruy Luiz", ""], ["Dias", "Bruno Pereira", ""], ["Bulc\u00e3o", "Andr\u00e9", ""]]}, {"id": "2005.03632", "submitter": "Sreejita Ghosh", "authors": "Sreejita Ghosh, Peter Tino, Kerstin Bunte", "title": "Visualisation and knowledge discovery from interpretable models", "comments": "Accepted for proceedings of the International Joint Conference on\n  Neural Networks (IJCNN) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing number of sectors which affect human lives, are using Machine\nLearning (ML) tools. Hence the need for understanding their working mechanism\nand evaluating their fairness in decision-making, are becoming paramount,\nushering in the era of Explainable AI (XAI). In this contribution we introduced\na few intrinsically interpretable models which are also capable of dealing with\nmissing values, in addition to extracting knowledge from the dataset and about\nthe problem. These models are also capable of visualisation of the classifier\nand decision boundaries: they are the angle based variants of Learning Vector\nQuantization. We have demonstrated the algorithms on a synthetic dataset and a\nreal-world one (heart disease dataset from the UCI repository). The newly\ndeveloped classifiers helped in investigating the complexities of the UCI\ndataset as a multiclass problem. The performance of the developed classifiers\nwere comparable to those reported in literature for this dataset, with\nadditional value of interpretability, when the dataset was treated as a binary\nclass problem.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 17:37:06 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 08:22:02 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Ghosh", "Sreejita", ""], ["Tino", "Peter", ""], ["Bunte", "Kerstin", ""]]}, {"id": "2005.03654", "submitter": "Elena Ericheva", "authors": "Ivan Drokin, Elena Ericheva", "title": "Deep Learning on Point Clouds for False Positive Reduction at Nodule\n  Detection in Chest CT Scans", "comments": null, "journal-ref": "In: van der Aalst W.M.P. et al. (eds) Analysis of Images, Social\n  Networks and Texts. AIST 2020. Lecture Notes in Computer Science, vol 12602.\n  Springer, Cham", "doi": "10.1007/978-3-030-72610-2_15", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on a novel approach for false-positive reduction (FPR) of\nnodule candidates in Computer-aided detection (CADe) systems following the\nsuspicious lesions detection stage. Contrary to typical decisions in medical\nimage analysis, the proposed approach considers input data not as a 2D or 3D\nimage, but rather as a point cloud, and uses deep learning models for point\nclouds. We discovered that point cloud models require less memory and are\nfaster both in training and inference compared to traditional CNN 3D, they\nachieve better performance and do not impose restrictions on the size of the\ninput image, i.e. no restrictions on the size of the nodule candidate. We\npropose an algorithm for transforming 3D CT scan data to point cloud. In some\ncases, the volume of the nodule candidate can be much smaller than the\nsurrounding context, for example, in the case of subpleural localization of the\nnodule. Therefore, we developed an algorithm for sampling points from a point\ncloud constructed from a 3D image of the candidate region. The algorithm is\nable to guarantee the capture of both context and candidate information as part\nof the point cloud of the nodule candidate. We designed and set up an\nexperiment in creating a dataset from an open LIDC-IDRI database for a feature\nof the FPR task, and is herein described in detail. Data augmentation was\napplied both to avoid overfitting and as an upsampling method. Experiments were\nconducted with PointNet, PointNet++, and DGCNN. We show that the proposed\napproach outperforms baseline CNN 3D models and resulted in 85.98 FROC versus\n77.26 FROC for baseline models. We compare our algorithm with published SOTA\nand demonstrate that even without significant modifications it works at the\nappropriate performance level on LUNA2016 and shows SOTA on LIDC-IDRI.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 17:59:54 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 08:31:26 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Drokin", "Ivan", ""], ["Ericheva", "Elena", ""]]}, {"id": "2005.03684", "submitter": "Daniel Fried", "authors": "Daniel Fried, Jean-Baptiste Alayrac, Phil Blunsom, Chris Dyer, Stephen\n  Clark, Aida Nematzadeh", "title": "Learning to Segment Actions from Observation and Narration", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a generative segmental model of task structure, guided by narration,\nto action segmentation in video. We focus on unsupervised and weakly-supervised\nsettings where no action labels are known during training. Despite its\nsimplicity, our model performs competitively with previous work on a dataset of\nnaturalistic instructional videos. Our model allows us to vary the sources of\nsupervision used in training, and we find that both task structure and\nnarrative language provide large benefits in segmentation quality.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 18:03:57 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 03:21:27 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Fried", "Daniel", ""], ["Alayrac", "Jean-Baptiste", ""], ["Blunsom", "Phil", ""], ["Dyer", "Chris", ""], ["Clark", "Stephen", ""], ["Nematzadeh", "Aida", ""]]}, {"id": "2005.03691", "submitter": "Ryoichi Ishikawa", "authors": "Richard Sahala Hartanto, Ryoichi Ishikawa, Menandro Roxas, Takeshi\n  Oishi", "title": "A Hand Motion-guided Articulation and Segmentation Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method for simultaneous articulation model\nestimation and segmentation of an articulated object in RGB-D images using\nhuman hand motion. Our method uses the hand motion in the processes of the\ninitial articulation model estimation, ICP-based model parameter optimization,\nand region selection of the target object. The hand motion gives an initial\nguess of the articulation model: prismatic or revolute joint. The method\nestimates the joint parameters by aligning the RGB-D images with the constraint\nof the hand motion. Finally, the target regions are selected from the cluster\nregions which move symmetrically along with the articulation model. Our\nexperimental results show the robustness of the proposed method for the various\nobjects.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 18:32:08 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Hartanto", "Richard Sahala", ""], ["Ishikawa", "Ryoichi", ""], ["Roxas", "Menandro", ""], ["Oishi", "Takeshi", ""]]}, {"id": "2005.03697", "submitter": "Mathilde Bateson", "authors": "Mathilde Bateson, Hoel Kervadec, Jose Dolz, Herve Lombaert, Ismail Ben\n  Ayed", "title": "Source-Relaxed Domain Adaptation for Image Segmentation", "comments": null, "journal-ref": "Medical Image Computing and Computer Assisted Intervention MICCAI\n  2020. Lecture Notes in Computer Science vol 12261. Springer Cham", "doi": "10.1007/978-3-030-59710-8_48", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) has drawn high interests for its capacity to adapt a\nmodel trained on labeled source data to perform well on unlabeled or weakly\nlabeled target data from a different domain. Most common DA techniques require\nthe concurrent access to the input images of both the source and target\ndomains. However, in practice, it is common that the source images are not\navailable in the adaptation phase. This is a very frequent DA scenario in\nmedical imaging, for instance, when the source and target images come from\ndifferent clinical sites. We propose a novel formulation for adapting\nsegmentation networks, which relaxes such a constraint. Our formulation is\nbased on minimizing a label-free entropy loss defined over target-domain data,\nwhich we further guide with a domain invariant prior on the segmentation\nregions. Many priors can be used, derived from anatomical information. Here, a\nclass-ratio prior is learned via an auxiliary network and integrated in the\nform of a Kullback-Leibler (KL) divergence in our overall loss function. We\nshow the effectiveness of our prior-aware entropy minimization in adapting\nspine segmentation across different MRI modalities. Our method yields\ncomparable results to several state-of-the-art adaptation techniques, even\nthough is has access to less information, the source images being absent in the\nadaptation phase. Our straight-forward adaptation strategy only uses one\nnetwork, contrary to popular adversarial techniques, which cannot perform\nwithout the presence of the source images. Our framework can be readily used\nwith various priors and segmentation problems.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 18:46:01 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 20:22:45 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bateson", "Mathilde", ""], ["Kervadec", "Hoel", ""], ["Dolz", "Jose", ""], ["Lombaert", "Herve", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "2005.03709", "submitter": "Takato Otsuzuki", "authors": "Takato Otsuzuki, Hideaki Hayashi, Yuchen Zheng and Seiichi Uchida", "title": "Regularized Pooling", "comments": "12 pages, 10 figures, accepted for ICANN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In convolutional neural networks (CNNs), pooling operations play important\nroles such as dimensionality reduction and deformation compensation. In\ngeneral, max pooling, which is the most widely used operation for local\npooling, is performed independently for each kernel. However, the deformation\nmay be spatially smooth over the neighboring kernels. This means that max\npooling is too flexible to compensate for actual deformations. In other words,\nits excessive flexibility risks canceling the essential spatial differences\nbetween classes. In this paper, we propose regularized pooling, which enables\nthe value selection direction in the pooling operation to be spatially smooth\nacross adjacent kernels so as to compensate only for actual deformations. The\nresults of experiments on handwritten character images and texture images\nshowed that regularized pooling not only improves recognition accuracy but also\naccelerates the convergence of learning compared with conventional pooling\noperations.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 09:02:17 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 07:10:34 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Otsuzuki", "Takato", ""], ["Hayashi", "Hideaki", ""], ["Zheng", "Yuchen", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2005.03717", "submitter": "Kiru Park", "authors": "Kiru Park, Timothy Patten, Markus Vincze", "title": "Neural Object Learning for 6D Pose Estimation Using a Few Cluttered\n  Images", "comments": "ECCV 2020 (Spotlight)", "journal-ref": null, "doi": "10.1007/978-3-030-58548-8_38", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methods for 6D pose estimation of objects assume either textured 3D\nmodels or real images that cover the entire range of target poses. However, it\nis difficult to obtain textured 3D models and annotate the poses of objects in\nreal scenarios. This paper proposes a method, Neural Object Learning (NOL),\nthat creates synthetic images of objects in arbitrary poses by combining only a\nfew observations from cluttered images. A novel refinement step is proposed to\nalign inaccurate poses of objects in source images, which results in better\nquality images. Evaluations performed on two public datasets show that the\nrendered images created by NOL lead to state-of-the-art performance in\ncomparison to methods that use 13 times the number of real images. Evaluations\non our new dataset show multiple objects can be trained and recognized\nsimultaneously using a sequence of a fixed scene.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 19:33:06 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 15:28:16 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Park", "Kiru", ""], ["Patten", "Timothy", ""], ["Vincze", "Markus", ""]]}, {"id": "2005.03743", "submitter": "Xiao Chen", "authors": "Hao Sheng, Xiao Chen, Jingyi Su, Ram Rajagopal, and Andrew Ng", "title": "Effective Data Fusion with Generalized Vegetation Index: Evidence from\n  Land Cover Segmentation in Agriculture", "comments": "CVPR 2020 - Vision for Agriculture;\n  https://www.agriculture-vision.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  How can we effectively leverage the domain knowledge from remote sensing to\nbetter segment agriculture land cover from satellite images? In this paper, we\npropose a novel, model-agnostic, data-fusion approach for vegetation-related\ncomputer vision tasks. Motivated by the various Vegetation Indices (VIs), which\nare introduced by domain experts, we systematically reviewed the VIs that are\nwidely used in remote sensing and their feasibility to be incorporated in deep\nneural networks. To fully leverage the Near-Infrared channel, the traditional\nRed-Green-Blue channels, and Vegetation Index or its variants, we propose a\nGeneralized Vegetation Index (GVI), a lightweight module that can be easily\nplugged into many neural network architectures to serve as an additional\ninformation input. To smoothly train models with our GVI, we developed an\nAdditive Group Normalization (AGN) module that does not require extra\nparameters of the prescribed neural networks. Our approach has improved the\nIoUs of vegetation-related classes by 0.9-1.3 percent and consistently improves\nthe overall mIoU by 2 percent on our baseline.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 20:41:20 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Sheng", "Hao", ""], ["Chen", "Xiao", ""], ["Su", "Jingyi", ""], ["Rajagopal", "Ram", ""], ["Ng", "Andrew", ""]]}, {"id": "2005.03748", "submitter": "Manit Zaveri Maulinkumar", "authors": "Manit Zaveri, Shivam Kalra, Morteza Babaie, Sultaan Shah, Savvas\n  Damskinos, Hany Kashani, H.R. Tizhoosh", "title": "Recognizing Magnification Levels in Microscopic Snapshots", "comments": "4 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in digital imaging has transformed computer vision and\nmachine learning to new tools for analyzing pathology images. This trend could\nautomate some of the tasks in the diagnostic pathology and elevate the\npathologist workload. The final step of any cancer diagnosis procedure is\nperformed by the expert pathologist. These experts use microscopes with high\nlevel of optical magnification to observe minute characteristics of the tissue\nacquired through biopsy and fixed on glass slides. Switching between different\nmagnifications, and finding the magnification level at which they identify the\npresence or absence of malignant tissues is important. As the majority of\npathologists still use light microscopy, compared to digital scanners, in many\ninstance a mounted camera on the microscope is used to capture snapshots from\nsignificant field-of-views. Repositories of such snapshots usually do not\ncontain the magnification information. In this paper, we extract deep features\nof the images available on TCGA dataset with known magnification to train a\nclassifier for magnification recognition. We compared the results with LBP, a\nwell-known handcrafted feature extraction method. The proposed approach\nachieved a mean accuracy of 96% when a multi-layer perceptron was trained as a\nclassifier.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 20:48:29 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Zaveri", "Manit", ""], ["Kalra", "Shivam", ""], ["Babaie", "Morteza", ""], ["Shah", "Sultaan", ""], ["Damskinos", "Savvas", ""], ["Kashani", "Hany", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "2005.03770", "submitter": "Jan Achterhold", "authors": "Nathanael Bosch, Jan Achterhold, Laura Leal-Taix\\'e, J\\\"org St\\\"uckler", "title": "Planning from Images with Deep Latent Gaussian Process Dynamics", "comments": "Accepted for publication at the 2nd Annual Conference on Learning for\n  Dynamics and Control (L4DC) 2020, with supplementary material. First two\n  authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning is a powerful approach to control problems with known environment\ndynamics. In unknown environments the agent needs to learn a model of the\nsystem dynamics to make planning applicable. This is particularly challenging\nwhen the underlying states are only indirectly observable through images. We\npropose to learn a deep latent Gaussian process dynamics (DLGPD) model that\nlearns low-dimensional system dynamics from environment interactions with\nvisual observations. The method infers latent state representations from\nobservations using neural networks and models the system dynamics in the\nlearned latent space with Gaussian processes. All parts of the model can be\ntrained jointly by optimizing a lower bound on the likelihood of transitions in\nimage space. We evaluate the proposed approach on the pendulum swing-up task\nwhile using the learned dynamics model for planning in latent space in order to\nsolve the control problem. We also demonstrate that our method can quickly\nadapt a trained agent to changes in the system dynamics from just a few\nrollouts. We compare our approach to a state-of-the-art purely deep learning\nbased method and demonstrate the advantages of combining Gaussian processes\nwith deep learning for data efficiency and transfer learning.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 21:29:45 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Bosch", "Nathanael", ""], ["Achterhold", "Jan", ""], ["Leal-Taix\u00e9", "Laura", ""], ["St\u00fcckler", "J\u00f6rg", ""]]}, {"id": "2005.03780", "submitter": "Steven Reeves", "authors": "Steven I Reeves, Dongwook Lee, Anurag Singh, and Kunal Verma", "title": "A Gaussian Process Upsampling Model for Improvements in Optical\n  Character Recognition", "comments": "12 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Character Recognition and extraction is a key tool in the automatic\nevaluation of documents in a financial context. However, the image data\nprovided to automated systems can have unreliable quality, and can be\ninherently low-resolution or downsampled and compressed by a transmitting\nprogram. In this paper, we illustrate the efficacy of a Gaussian Process\nupsampling model for the purposes of improving OCR and extraction through\nupsampling low resolution documents.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 22:13:22 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Reeves", "Steven I", ""], ["Lee", "Dongwook", ""], ["Singh", "Anurag", ""], ["Verma", "Kunal", ""]]}, {"id": "2005.03788", "submitter": "Xinshao Wang Dr", "authors": "Xinshao Wang, Yang Hua, Elyor Kodirov, David A. Clifton, Neil M.\n  Robertson", "title": "ProSelfLC: Progressive Self Label Correction for Training Robust Deep\n  Neural Networks", "comments": "ProSelfLC is the first method to trust self knowledge progressively\n  and adaptively. ProSelfLC redirects and promotes entropy minimisation, which\n  is in marked contrast to recent practices of confidence penalty [42, 33, 6]", "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To train robust deep neural networks (DNNs), we systematically study several\ntarget modification approaches, which include output regularisation, self and\nnon-self label correction (LC). Two key issues are discovered: (1) Self LC is\nthe most appealing as it exploits its own knowledge and requires no extra\nmodels. However, how to automatically decide the trust degree of a learner as\ntraining goes is not well answered in the literature? (2) Some methods penalise\nwhile the others reward low-entropy predictions, prompting us to ask which one\nis better?\n  To resolve the first issue, taking two well-accepted propositions--deep\nneural networks learn meaningful patterns before fitting noise [3] and minimum\nentropy regularisation principle [10]--we propose a novel end-to-end method\nnamed ProSelfLC, which is designed according to learning time and entropy.\nSpecifically, given a data point, we progressively increase trust in its\npredicted label distribution versus its annotated one if a model has been\ntrained for enough time and the prediction is of low entropy (high confidence).\nFor the second issue, according to ProSelfLC, we empirically prove that it is\nbetter to redefine a meaningful low-entropy status and optimise the learner\ntoward it. This serves as a defence of entropy minimisation.\n  We demonstrate the effectiveness of ProSelfLC through extensive experiments\nin both clean and noisy settings. The source code is available at\nhttps://github.com/XinshaoAmosWang/ProSelfLC-CVPR2021.\n  Keywords: entropy minimisation, maximum entropy, confidence penalty, self\nknowledge distillation, label correction, label noise, semi-supervised\nlearning, output regularisation\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 22:35:04 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 22:10:17 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2020 13:36:09 GMT"}, {"version": "v4", "created": "Mon, 29 Jun 2020 11:04:32 GMT"}, {"version": "v5", "created": "Fri, 9 Oct 2020 12:45:28 GMT"}, {"version": "v6", "created": "Wed, 2 Jun 2021 12:27:53 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wang", "Xinshao", ""], ["Hua", "Yang", ""], ["Kodirov", "Elyor", ""], ["Clifton", "David A.", ""], ["Robertson", "Neil M.", ""]]}, {"id": "2005.03793", "submitter": "Chenyou Fan", "authors": "Chenyou Fan, Ping Liu", "title": "Federated Generative Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies training generative adversarial networks under the\nfederated learning setting. Generative adversarial networks (GANs) have\nachieved advancement in various real-world applications, such as image editing,\nstyle transfer, scene generations, etc. However, like other deep learning\nmodels, GANs are also suffering from data limitation problems in real cases. To\nboost the performance of GANs in target tasks, collecting images as many as\npossible from different sources becomes not only important but also essential.\nFor example, to build a robust and accurate bio-metric verification system,\nhuge amounts of images might be collected from surveillance cameras, and/or\nuploaded from cellphones by users accepting agreements. In an ideal case,\nutilize all those data uploaded from public and private devices for model\ntraining is straightforward. Unfortunately, in the real scenarios, this is hard\ndue to a few reasons. At first, some data face the serious concern of leakage,\nand therefore it is prohibitive to upload them to a third-party server for\nmodel training; at second, the images collected by different kinds of devices,\nprobably have distinctive biases due to various factors, $\\textit{e.g.}$,\ncollector preferences, geo-location differences, which is also known as \"domain\nshift\". To handle those problems, we propose a novel generative learning scheme\nutilizing a federated learning framework. Following the configuration of\nfederated learning, we conduct model training and aggregation on one center and\na group of clients. Specifically, our method learns the distributed generative\nmodels in clients, while the models trained in each client are fused into one\nunified and versatile model in the center. We perform extensive experiments to\ncompare different federation strategies, and empirically examine the\neffectiveness of federation under different levels of parallelism and data\nskewness.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 23:06:49 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 14:35:27 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 05:02:05 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Fan", "Chenyou", ""], ["Liu", "Ping", ""]]}, {"id": "2005.03795", "submitter": "Anuradha Kar", "authors": "Anuradha Kar", "title": "MLGaze: Machine Learning-Based Analysis of Gaze Error Patterns in\n  Consumer Eye Tracking Systems", "comments": "https://github.com/anuradhakar49/MLGaze", "journal-ref": null, "doi": "10.3390/vision4020025", "report-no": null, "categories": "eess.SP cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analyzing the gaze accuracy characteristics of an eye tracker is a critical\ntask as its gaze data is frequently affected by non-ideal operating conditions\nin various consumer eye tracking applications. In this study, gaze error\npatterns produced by a commercial eye tracking device were studied with the\nhelp of machine learning algorithms, such as classifiers and regression models.\nGaze data were collected from a group of participants under multiple conditions\nthat commonly affect eye trackers operating on desktop and handheld platforms.\nThese conditions (referred here as error sources) include user distance, head\npose, and eye-tracker pose variations, and the collected gaze data were used to\ntrain the classifier and regression models. It was seen that while the impact\nof the different error sources on gaze data characteristics were nearly\nimpossible to distinguish by visual inspection or from data statistics, machine\nlearning models were successful in identifying the impact of the different\nerror sources and predicting the variability in gaze error levels due to these\nconditions. The objective of this study was to investigate the efficacy of\nmachine learning methods towards the detection and prediction of gaze error\npatterns, which would enable an in-depth understanding of the data quality and\nreliability of eye trackers under unconstrained operating conditions. Coding\nresources for all the machine learning methods adopted in this study were\nincluded in an open repository named MLGaze to allow researchers to replicate\nthe principles presented here using data from their own eye trackers.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 23:07:02 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Kar", "Anuradha", ""]]}, {"id": "2005.03804", "submitter": "Aidean Sharghi", "authors": "Aidean Sharghi, Niels da Vitoria Lobo, Mubarak Shah", "title": "Text Synopsis Generation for Egocentric Videos", "comments": "ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass utilization of body-worn cameras has led to a huge corpus of available\negocentric video. Existing video summarization algorithms can accelerate\nbrowsing such videos by selecting (visually) interesting shots from them.\nNonetheless, since the system user still has to watch the summary videos,\nbrowsing large video databases remain a challenge. Hence, in this work, we\npropose to generate a textual synopsis, consisting of a few sentences\ndescribing the most important events in a long egocentric videos. Users can\nread the short text to gain insight about the video, and more importantly,\nefficiently search through the content of a large video database using text\nqueries. Since egocentric videos are long and contain many activities and\nevents, using video-to-text algorithms results in thousands of descriptions,\nmany of which are incorrect. Therefore, we propose a multi-task learning scheme\nto simultaneously generate descriptions for video segments and summarize the\nresulting descriptions in an end-to-end fashion. We Input a set of video shots\nand the network generates a text description for each shot. Next,\nvisual-language content matching unit that is trained with a weakly supervised\nobjective, identifies the correct descriptions. Finally, the last component of\nour network, called purport network, evaluates the descriptions all together to\nselect the ones containing crucial information. Out of thousands of\ndescriptions generated for the video, a few informative sentences are returned\nto the user. We validate our framework on the challenging UT Egocentric video\ndataset, where each video is between 3 to 5 hours long, associated with over\n3000 textual descriptions on average. The generated textual summaries,\nincluding only 5 percent (or less) of the generated descriptions, are compared\nto groundtruth summaries in text domain using well-established metrics in\nnatural language processing.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 00:28:00 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 16:29:38 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Sharghi", "Aidean", ""], ["Lobo", "Niels da Vitoria", ""], ["Shah", "Mubarak", ""]]}, {"id": "2005.03819", "submitter": "Xiang Li", "authors": "Xiang Li, Lin Zhang, Yau Pun Chen, Yu-Wing Tai, Chi-Keung Tang", "title": "One-Shot Object Detection without Fine-Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has revolutionized object detection thanks to large-scale\ndatasets, but their object categories are still arguably very limited. In this\npaper, we attempt to enrich such categories by addressing the one-shot object\ndetection problem, where the number of annotated training examples for learning\nan unseen class is limited to one. We introduce a two-stage model consisting of\na first stage Matching-FCOS network and a second stage Structure-Aware Relation\nModule, the combination of which integrates metric learning with an anchor-free\nFaster R-CNN-style detection pipeline, eventually eliminating the need to\nfine-tune on the support images. We also propose novel training strategies that\neffectively improve detection performance. Extensive quantitative and\nqualitative evaluations were performed and our method exceeds the\nstate-of-the-art one-shot performance consistently on multiple datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 01:59:23 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Li", "Xiang", ""], ["Zhang", "Lin", ""], ["Chen", "Yau Pun", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "2005.03823", "submitter": "Eugene Bagdasaryan", "authors": "Eugene Bagdasaryan and Vitaly Shmatikov", "title": "Blind Backdoors in Deep Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a new method for injecting backdoors into machine learning\nmodels, based on compromising the loss-value computation in the model-training\ncode. We use it to demonstrate new classes of backdoors strictly more powerful\nthan those in the prior literature: single-pixel and physical backdoors in\nImageNet models, backdoors that switch the model to a covert, privacy-violating\ntask, and backdoors that do not require inference-time input modifications.\n  Our attack is blind: the attacker cannot modify the training data, nor\nobserve the execution of his code, nor access the resulting model. The attack\ncode creates poisoned training inputs \"on the fly,\" as the model is training,\nand uses multi-objective optimization to achieve high accuracy on both the main\nand backdoor tasks. We show how a blind attack can evade any known defense and\npropose new ones.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 02:15:53 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 22:09:05 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 17:25:32 GMT"}, {"version": "v4", "created": "Fri, 19 Feb 2021 04:45:28 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Bagdasaryan", "Eugene", ""], ["Shmatikov", "Vitaly", ""]]}, {"id": "2005.03824", "submitter": "John McManigle Jr", "authors": "John McManigle, Raquel Bartz, Lawrence Carin", "title": "Y-Net for Chest X-Ray Preprocessing: Simultaneous Classification of\n  Geometry and Segmentation of Annotations", "comments": "Accepted EMBC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, convolutional neural networks (CNNs) have emerged as\nthe leading algorithms in image classification and segmentation. Recent\npublication of large medical imaging databases have accelerated their use in\nthe biomedical arena. While training data for photograph classification\nbenefits from aggressive geometric augmentation, medical diagnosis --\nespecially in chest radiographs -- depends more strongly on feature location.\nDiagnosis classification results may be artificially enhanced by reliance on\nradiographic annotations. This work introduces a general pre-processing step\nfor chest x-ray input into machine learning algorithms. A modified Y-Net\narchitecture based on the VGG11 encoder is used to simultaneously learn\ngeometric orientation (similarity transform parameters) of the chest and\nsegmentation of radiographic annotations. Chest x-rays were obtained from\npublished databases. The algorithm was trained with 1000 manually labeled\nimages with augmentation. Results were evaluated by expert clinicians, with\nacceptable geometry in 95.8% and annotation mask in 96.2% (n=500), compared to\n27.0% and 34.9% respectively in control images (n=241). We hypothesize that\nthis pre-processing step will improve robustness in future diagnostic\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 02:16:17 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["McManigle", "John", ""], ["Bartz", "Raquel", ""], ["Carin", "Lawrence", ""]]}, {"id": "2005.03832", "submitter": "Feng Shi", "authors": "Kelei He, Wei Zhao, Xingzhi Xie, Wen Ji, Mingxia Liu, Zhenyu Tang,\n  Feng Shi, Yang Gao, Jun Liu, Junfeng Zhang, and Dinggang Shen", "title": "Synergistic Learning of Lung Lobe Segmentation and Hierarchical\n  Multi-Instance Classification for Automated Severity Assessment of COVID-19\n  in CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding chest CT imaging of the coronavirus disease 2019 (COVID-19)\nwill help detect infections early and assess the disease progression.\nEspecially, automated severity assessment of COVID-19 in CT images plays an\nessential role in identifying cases that are in great need of intensive\nclinical care. However, it is often challenging to accurately assess the\nseverity of this disease in CT images, due to variable infection regions in the\nlungs, similar imaging biomarkers, and large inter-case variations. To this\nend, we propose a synergistic learning framework for automated severity\nassessment of COVID-19 in 3D CT images, by jointly performing lung lobe\nsegmentation and multi-instance classification. Considering that only a few\ninfection regions in a CT image are related to the severity assessment, we\nfirst represent each input image by a bag that contains a set of 2D image\npatches (with each cropped from a specific slice). A multi-task multi-instance\ndeep network (called M$^2$UNet) is then developed to assess the severity of\nCOVID-19 patients and also segment the lung lobe simultaneously. Our M$^2$UNet\nconsists of a patch-level encoder, a segmentation sub-network for lung lobe\nsegmentation, and a classification sub-network for severity assessment (with a\nunique hierarchical multi-instance learning strategy). Here, the context\ninformation provided by segmentation can be implicitly employed to improve the\nperformance of severity assessment. Extensive experiments were performed on a\nreal COVID-19 CT image dataset consisting of 666 chest CT images, with results\nsuggesting the effectiveness of our proposed method compared to several\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 03:16:15 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 08:20:04 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["He", "Kelei", ""], ["Zhao", "Wei", ""], ["Xie", "Xingzhi", ""], ["Ji", "Wen", ""], ["Liu", "Mingxia", ""], ["Tang", "Zhenyu", ""], ["Shi", "Feng", ""], ["Gao", "Yang", ""], ["Liu", "Jun", ""], ["Zhang", "Junfeng", ""], ["Shen", "Dinggang", ""]]}, {"id": "2005.03837", "submitter": "Jie Li", "authors": "Jie Li, Rongrong Ji, Hong Liu, Jianzhuang Liu, Bineng Zhong, Cheng\n  Deng, Qi Tian", "title": "Projection & Probability-Driven Black-Box Attack", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating adversarial examples in a black-box setting retains a significant\nchallenge with vast practical application prospects. In particular, existing\nblack-box attacks suffer from the need for excessive queries, as it is\nnon-trivial to find an appropriate direction to optimize in the\nhigh-dimensional space. In this paper, we propose Projection &\nProbability-driven Black-box Attack (PPBA) to tackle this problem by reducing\nthe solution space and providing better optimization. For reducing the solution\nspace, we first model the adversarial perturbation optimization problem as a\nprocess of recovering frequency-sparse perturbations with compressed sensing,\nunder the setting that random noise in the low-frequency space is more likely\nto be adversarial. We then propose a simple method to construct a low-frequency\nconstrained sensing matrix, which works as a plug-and-play projection matrix to\nreduce the dimensionality. Such a sensing matrix is shown to be flexible enough\nto be integrated into existing methods like NES and Bandits$_{TD}$. For better\noptimization, we perform a random walk with a probability-driven strategy,\nwhich utilizes all queries over the whole progress to make full use of the\nsensing matrix for a less query budget. Extensive experiments show that our\nmethod requires at most 24% fewer queries with a higher attack success rate\ncompared with state-of-the-art approaches. Finally, the attack method is\nevaluated on the real-world online service, i.e., Google Cloud Vision API,\nwhich further demonstrates our practical potentials.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 03:37:50 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Li", "Jie", ""], ["Ji", "Rongrong", ""], ["Liu", "Hong", ""], ["Liu", "Jianzhuang", ""], ["Zhong", "Bineng", ""], ["Deng", "Cheng", ""], ["Tian", "Qi", ""]]}, {"id": "2005.03844", "submitter": "Zhenpei Yang", "authors": "Zhenpei Yang, Yuning Chai, Dragomir Anguelov, Yin Zhou, Pei Sun,\n  Dumitru Erhan, Sean Rafferty, Henrik Kretzschmar", "title": "SurfelGAN: Synthesizing Realistic Sensor Data for Autonomous Driving", "comments": null, "journal-ref": "CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving system development is critically dependent on the ability\nto replay complex and diverse traffic scenarios in simulation. In such\nscenarios, the ability to accurately simulate the vehicle sensors such as\ncameras, lidar or radar is essential. However, current sensor simulators\nleverage gaming engines such as Unreal or Unity, requiring manual creation of\nenvironments, objects and material properties. Such approaches have limited\nscalability and fail to produce realistic approximations of camera, lidar, and\nradar data without significant additional work.\n  In this paper, we present a simple yet effective approach to generate\nrealistic scenario sensor data, based only on a limited amount of lidar and\ncamera data collected by an autonomous vehicle. Our approach uses\ntexture-mapped surfels to efficiently reconstruct the scene from an initial\nvehicle pass or set of passes, preserving rich information about object 3D\ngeometry and appearance, as well as the scene conditions. We then leverage a\nSurfelGAN network to reconstruct realistic camera images for novel positions\nand orientations of the self-driving vehicle and moving objects in the scene.\nWe demonstrate our approach on the Waymo Open Dataset and show that it can\nsynthesize realistic camera data for simulated scenarios. We also create a\nnovel dataset that contains cases in which two self-driving vehicles observe\nthe same scene at the same time. We use this dataset to provide additional\nevaluation and demonstrate the usefulness of our SurfelGAN model.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 04:01:14 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 05:37:24 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Yang", "Zhenpei", ""], ["Chai", "Yuning", ""], ["Anguelov", "Dragomir", ""], ["Zhou", "Yin", ""], ["Sun", "Pei", ""], ["Erhan", "Dumitru", ""], ["Rafferty", "Sean", ""], ["Kretzschmar", "Henrik", ""]]}, {"id": "2005.03846", "submitter": "Mingshuang Luo", "authors": "Mingshuang Luo, Shuang Yang, Xilin Chen, Zitao Liu, Shiguang Shan", "title": "Synchronous Bidirectional Learning for Multilingual Lip Reading", "comments": "13 pages,2 figures,5 tables; Accepted in BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip reading has received increasing attention in recent years. This paper\nfocuses on the synergy of multilingual lip reading. There are about as many as\n7000 languages in the world, which implies that it is impractical to train\nseparate lip reading models with large-scale data for each language. Although\neach language has its own linguistic and pronunciation rules, the lip movements\nof all languages share similar patterns due to the common structures of human\norgans. Based on this idea, we try to explore the synergized learning of\nmultilingual lip reading in this paper, and further propose a synchronous\nbidirectional learning (SBL) framework for effective synergy of multilingual\nlip reading. We firstly introduce phonemes as our modeling units for the\nmultilingual setting here. Phonemes are more closely related with the lip\nmovements than the alphabet letters. At the same time, similar phonemes always\nlead to similar visual patterns no matter which type the target language is.\nThen, a novel SBL block is proposed to learn the rules for each language in a\nfill-in-the-blank way. Specifically, the model has to learn to infer the target\nunit given its bidirectional context, which could represent the composition\nrules of phonemes for each language. To make the learning process more targeted\nat each particular language, an extra task of predicting the language identity\nis introduced in the learning process. Finally, a thorough comparison on LRW\n(English) and LRW-1000 (Mandarin) is performed, which shows the promising\nbenefits from the synergized learning of different languages and also reports a\nnew state-of-the-art result on both datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 04:19:57 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 02:45:23 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 01:04:28 GMT"}, {"version": "v4", "created": "Fri, 14 Aug 2020 15:34:49 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Luo", "Mingshuang", ""], ["Yang", "Shuang", ""], ["Chen", "Xilin", ""], ["Liu", "Zitao", ""], ["Shan", "Shiguang", ""]]}, {"id": "2005.03860", "submitter": "Yujiao Shi", "authors": "Yujiao Shi, Xin Yu, Dylan Campbell, Hongdong Li", "title": "Where am I looking at? Joint Location and Orientation Estimation by\n  Cross-View Matching", "comments": "accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-view geo-localization is the problem of estimating the position and\norientation (latitude, longitude and azimuth angle) of a camera at ground level\ngiven a large-scale database of geo-tagged aerial (e.g., satellite) images.\nExisting approaches treat the task as a pure location estimation problem by\nlearning discriminative feature descriptors, but neglect orientation alignment.\nIt is well-recognized that knowing the orientation between ground and aerial\nimages can significantly reduce matching ambiguity between these two views,\nespecially when the ground-level images have a limited Field of View (FoV)\ninstead of a full field-of-view panorama. Therefore, we design a Dynamic\nSimilarity Matching network to estimate cross-view orientation alignment during\nlocalization. In particular, we address the cross-view domain gap by applying a\npolar transform to the aerial images to approximately align the images up to an\nunknown azimuth angle. Then, a two-stream convolutional network is used to\nlearn deep features from the ground and polar-transformed aerial images.\nFinally, we obtain the orientation by computing the correlation between\ncross-view features, which also provides a more accurate measure of feature\nsimilarity, improving location recall. Experiments on standard datasets\ndemonstrate that our method significantly improves state-of-the-art\nperformance. Remarkably, we improve the top-1 location recall rate on the CVUSA\ndataset by a factor of 1.5x for panoramas with known orientation, by a factor\nof 3.3x for panoramas with unknown orientation, and by a factor of 6x for\n180-degree FoV images with unknown orientation.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 05:21:16 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Shi", "Yujiao", ""], ["Yu", "Xin", ""], ["Campbell", "Dylan", ""], ["Li", "Hongdong", ""]]}, {"id": "2005.03871", "submitter": "Xin Wen", "authors": "Xin Wen, Tianyang Li, Zhizhong Han, Yu-Shen Liu", "title": "Point Cloud Completion by Skip-attention Network with Hierarchical\n  Folding", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud completion aims to infer the complete geometries for missing\nregions of 3D objects from incomplete ones. Previous methods usually predict\nthe complete point cloud based on the global shape representation extracted\nfrom the incomplete input. However, the global representation often suffers\nfrom the information loss of structure details on local regions of incomplete\npoint cloud. To address this problem, we propose Skip-Attention Network\n(SA-Net) for 3D point cloud completion. Our main contributions lie in the\nfollowing two-folds. First, we propose a skip-attention mechanism to\neffectively exploit the local structure details of incomplete point clouds\nduring the inference of missing parts. The skip-attention mechanism selectively\nconveys geometric information from the local regions of incomplete point clouds\nfor the generation of complete ones at different resolutions, where the\nskip-attention reveals the completion process in an interpretable way. Second,\nin order to fully utilize the selected geometric information encoded by\nskip-attention mechanism at different resolutions, we propose a novel\nstructure-preserving decoder with hierarchical folding for complete shape\ngeneration. The hierarchical folding preserves the structure of complete point\ncloud generated in upper layer by progressively detailing the local regions,\nusing the skip-attentioned geometry at the same resolution. We conduct\ncomprehensive experiments on ShapeNet and KITTI datasets, which demonstrate\nthat the proposed SA-Net outperforms the state-of-the-art point cloud\ncompletion methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 06:23:51 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 14:10:05 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Wen", "Xin", ""], ["Li", "Tianyang", ""], ["Han", "Zhizhong", ""], ["Liu", "Yu-Shen", ""]]}, {"id": "2005.03876", "submitter": "Cristina Palmero", "authors": "Cristina Palmero, Abhishek Sharma, Karsten Behrendt, Kapil\n  Krishnakumar, Oleg V. Komogortsev, Sachin S. Talathi", "title": "OpenEDS2020: Open Eyes Dataset", "comments": "Description of dataset used in OpenEDS2020 challenge:\n  https://research.fb.com/programs/openeds-2020-challenge/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the second edition of OpenEDS dataset, OpenEDS2020, a novel\ndataset of eye-image sequences captured at a frame rate of 100 Hz under\ncontrolled illumination, using a virtual-reality head-mounted display mounted\nwith two synchronized eye-facing cameras. The dataset, which is anonymized to\nremove any personally identifiable information on participants, consists of 80\nparticipants of varied appearance performing several gaze-elicited tasks, and\nis divided in two subsets: 1) Gaze Prediction Dataset, with up to 66,560\nsequences containing 550,400 eye-images and respective gaze vectors, created to\nfoster research in spatio-temporal gaze estimation and prediction approaches;\nand 2) Eye Segmentation Dataset, consisting of 200 sequences sampled at 5 Hz,\nwith up to 29,500 images, of which 5% contain a semantic segmentation label,\ndevised to encourage the use of temporal information to propagate labels to\ncontiguous frames. Baseline experiments have been evaluated on OpenEDS2020, one\nfor each task, with average angular error of 5.37 degrees when performing gaze\nprediction on 1 to 5 frames into the future, and a mean intersection over union\nscore of 84.1% for semantic segmentation. As its predecessor, OpenEDS dataset,\nwe anticipate that this new dataset will continue creating opportunities to\nresearchers in eye tracking, machine learning and computer vision communities,\nto advance the state of the art for virtual reality applications. The dataset\nis available for download upon request at\nhttp://research.fb.com/programs/openeds-2020-challenge/.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 06:53:05 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Palmero", "Cristina", ""], ["Sharma", "Abhishek", ""], ["Behrendt", "Karsten", ""], ["Krishnakumar", "Kapil", ""], ["Komogortsev", "Oleg V.", ""], ["Talathi", "Sachin S.", ""]]}, {"id": "2005.03888", "submitter": "Chong You", "authors": "Chong You and Chun-Guang Li and Daniel P. Robinson and Rene Vidal", "title": "Is an Affine Constraint Needed for Affine Subspace Clustering?", "comments": "ICCV 2019. Including proofs that are omitted in the conference\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering methods based on expressing each data point as a linear\ncombination of other data points have achieved great success in computer vision\napplications such as motion segmentation, face and digit clustering. In face\nclustering, the subspaces are linear and subspace clustering methods can be\napplied directly. In motion segmentation, the subspaces are affine and an\nadditional affine constraint on the coefficients is often enforced. However,\nsince affine subspaces can always be embedded into linear subspaces of one\nextra dimension, it is unclear if the affine constraint is really necessary.\nThis paper shows, both theoretically and empirically, that when the dimension\nof the ambient space is high relative to the sum of the dimensions of the\naffine subspaces, the affine constraint has a negligible effect on clustering\nperformance. Specifically, our analysis provides conditions that guarantee the\ncorrectness of affine subspace clustering methods both with and without the\naffine constraint, and shows that these conditions are satisfied for\nhigh-dimensional data. Underlying our analysis is the notion of affinely\nindependent subspaces, which not only provides geometrically interpretable\ncorrectness conditions, but also clarifies the relationships between existing\nresults for affine subspace clustering.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 07:52:17 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["You", "Chong", ""], ["Li", "Chun-Guang", ""], ["Robinson", "Daniel P.", ""], ["Vidal", "Rene", ""]]}, {"id": "2005.03922", "submitter": "Haocheng Feng", "authors": "Haocheng Feng and Zhibin Hong and Haixiao Yue and Yang Chen and Keyao\n  Wang and Junyu Han and Jingtuo Liu and Errui Ding", "title": "Learning Generalized Spoof Cues for Face Anti-spoofing", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing face anti-spoofing (FAS) methods focus on modeling the decision\nboundaries for some predefined spoof types. However, the diversity of the spoof\nsamples including the unknown ones hinders the effective decision boundary\nmodeling and leads to weak generalization capability. In this paper, we\nreformulate FAS in an anomaly detection perspective and propose a\nresidual-learning framework to learn the discriminative live-spoof differences\nwhich are defined as the spoof cues. The proposed framework consists of a spoof\ncue generator and an auxiliary classifier. The generator minimizes the spoof\ncues of live samples while imposes no explicit constraint on those of spoof\nsamples to generalize well to unseen attacks. In this way, anomaly detection is\nimplicitly used to guide spoof cue generation, leading to discriminative\nfeature learning. The auxiliary classifier serves as a spoof cue amplifier and\nmakes the spoof cues more discriminative. We conduct extensive experiments and\nthe experimental results show the proposed method consistently outperforms the\nstate-of-the-art methods. The code will be publicly available at\nhttps://github.com/vis-var/lgsc-for-fas.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 09:22:13 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Feng", "Haocheng", ""], ["Hong", "Zhibin", ""], ["Yue", "Haixiao", ""], ["Chen", "Yang", ""], ["Wang", "Keyao", ""], ["Han", "Junyu", ""], ["Liu", "Jingtuo", ""], ["Ding", "Errui", ""]]}, {"id": "2005.03924", "submitter": "Shuchao Pang", "authors": "Shuchao Pang, Anan Du, Mehmet A. Orgun, Yan Wang, Quanzheng Sheng,\n  Shoujin Wang, Xiaoshui Huang, Zhemei Yu", "title": "Beyond CNNs: Exploiting Further Inherent Symmetries in Medical Images\n  for Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic tumor segmentation is a crucial step in medical image analysis for\ncomputer-aided diagnosis. Although the existing methods based on convolutional\nneural networks (CNNs) have achieved the state-of-the-art performance, many\nchallenges still remain in medical tumor segmentation. This is because regular\nCNNs can only exploit translation invariance, ignoring further inherent\nsymmetries existing in medical images such as rotations and reflections. To\nmitigate this shortcoming, we propose a novel group equivariant segmentation\nframework by encoding those inherent symmetries for learning more precise\nrepresentations. First, kernel-based equivariant operations are devised on\nevery orientation, which can effectively address the gaps of learning\nsymmetries in existing approaches. Then, to keep segmentation networks globally\nequivariant, we design distinctive group layers with layerwise symmetry\nconstraints. By exploiting further symmetries, novel segmentation CNNs can\ndramatically reduce the sample complexity and the redundancy of filters (by\nroughly 2/3) over regular CNNs. More importantly, based on our novel framework,\nwe show that a newly built GER-UNet outperforms its regular CNN-based\ncounterpart and the state-of-the-art segmentation methods on real-world\nclinical data. Specifically, the group layers of our segmentation framework can\nbe seamlessly integrated into any popular CNN-based segmentation architectures.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 09:36:50 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Pang", "Shuchao", ""], ["Du", "Anan", ""], ["Orgun", "Mehmet A.", ""], ["Wang", "Yan", ""], ["Sheng", "Quanzheng", ""], ["Wang", "Shoujin", ""], ["Huang", "Xiaoshui", ""], ["Yu", "Zhemei", ""]]}, {"id": "2005.03948", "submitter": "Yin Tang", "authors": "Yin Tang, Qi Teng, Lei Zhang, Fuhong Min and Jun He", "title": "Layer-wise training convolutional neural networks with smaller filters\n  for human activity recognition using wearable sensors", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convolutional neural networks (CNNs) have set latest\nstate-of-the-art on various human activity recognition (HAR) datasets. However,\ndeep CNNs often require more computing resources, which limits their\napplications in embedded HAR. Although many successful methods have been\nproposed to reduce memory and FLOPs of CNNs, they often involve special network\narchitectures designed for visual tasks, which are not suitable for deep HAR\ntasks with time series sensor signals, due to remarkable discrepancy.\nTherefore, it is necessary to develop lightweight deep models to perform HAR.\nAs filter is the basic unit in constructing CNNs, it deserves further research\nwhether re-designing smaller filters is applicable for deep HAR. In the paper,\ninspired by the idea, we proposed a lightweight CNN using Lego filters for HAR.\nA set of lower-dimensional filters is used as Lego bricks to be stacked for\nconventional filters, which does not rely on any special network structure. The\nlocal loss function is used to train model. To our knowledge, this is the first\npaper that proposes lightweight CNN for HAR in ubiquitous and wearable\ncomputing arena. The experiment results on five public HAR datasets, UCI-HAR\ndataset, OPPORTUNITY dataset, UNIMIB-SHAR dataset, PAMAP2 dataset, and WISDM\ndataset collected from either smartphones or multiple sensor nodes, indicate\nthat our novel Lego CNN with local loss can greatly reduce memory and\ncomputation cost over CNN, while achieving higher accuracy. That is to say, the\nproposed model is smaller, faster and more accurate. Finally, we evaluate the\nactual performance on an Android smartphone.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 10:30:03 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 14:21:22 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Tang", "Yin", ""], ["Teng", "Qi", ""], ["Zhang", "Lei", ""], ["Min", "Fuhong", ""], ["He", "Jun", ""]]}, {"id": "2005.03950", "submitter": "Mingjie Jiang", "authors": "Mingjie Jiang, Xinqi Fan, Hong Yan", "title": "RetinaMask: A Face Mask detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease 2019 has affected the world seriously. One major\nprotection method for people is to wear masks in public areas. Furthermore,\nmany public service providers require customers to use the service only if they\nwear masks correctly. However, there are only a few research studies about face\nmask detection based on image analysis. In this paper, we propose\nRetinaFaceMask, which is a high-accuracy and efficient face mask detector. The\nproposed RetinaFaceMask is a one-stage detector, which consists of a feature\npyramid network to fuse high-level semantic information with multiple feature\nmaps, and a novel context attention module to focus on detecting face masks. In\naddition, we also propose a novel cross-class object removal algorithm to\nreject predictions with low confidences and the high intersection of union.\nExperiment results show that RetinaFaceMask achieves state-of-the-art results\non a public face mask dataset with $2.3\\%$ and $1.5\\%$ higher than the baseline\nresult in the face and mask detection precision, respectively, and $11.0\\%$ and\n$5.9\\%$ higher than baseline for recall. Besides, we also explore the\npossibility of implementing RetinaFaceMask with a light-weighted neural network\nMobileNet for embedded or mobile devices.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 10:45:16 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 07:40:31 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Jiang", "Mingjie", ""], ["Fan", "Xinqi", ""], ["Yan", "Hong", ""]]}, {"id": "2005.03959", "submitter": "Zhaoyi Wan", "authors": "Zhaoyi Wan, Jielei Zhang, Liang Zhang, Jiebo Luo, Cong Yao", "title": "On Vocabulary Reliance in Scene Text Recognition", "comments": "CVPR'20", "journal-ref": "EEE/CVF Conferences on Computer Vision and Pattern Recognition\n  (CVPR), Seattle, WA, June 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pursuit of high performance on public benchmarks has been the driving\nforce for research in scene text recognition, and notable progress has been\nachieved. However, a close investigation reveals a startling fact that the\nstate-of-the-art methods perform well on images with words within vocabulary\nbut generalize poorly to images with words outside vocabulary. We call this\nphenomenon \"vocabulary reliance\". In this paper, we establish an analytical\nframework to conduct an in-depth study on the problem of vocabulary reliance in\nscene text recognition. Key findings include: (1) Vocabulary reliance is\nubiquitous, i.e., all existing algorithms more or less exhibit such\ncharacteristic; (2) Attention-based decoders prove weak in generalizing to\nwords outside vocabulary and segmentation-based decoders perform well in\nutilizing visual features; (3) Context modeling is highly coupled with the\nprediction layers. These findings provide new insights and can benefit future\nresearch in scene text recognition. Furthermore, we propose a simple yet\neffective mutual learning strategy to allow models of two families\n(attention-based and segmentation-based) to learn collaboratively. This remedy\nalleviates the problem of vocabulary reliance and improves the overall scene\ntext recognition performance.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 11:16:58 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Wan", "Zhaoyi", ""], ["Zhang", "Jielei", ""], ["Zhang", "Liang", ""], ["Luo", "Jiebo", ""], ["Yao", "Cong", ""]]}, {"id": "2005.03990", "submitter": "Mohammad Rahimzadeh", "authors": "Mohammad Rahimzadeh, Abolfazl Attar", "title": "Detecting and Counting Pistachios based on Deep Learning", "comments": "This is a preprint of an article published in the Iran Journal of\n  Computer Science. The final authenticated version is available online at\n  https://doi.org/10.1007/s42044-021-00090-6. The dataset and the code are\n  available at: https://github.com/mr7495/Pesteh-Set\n  https://github.com/mr7495/Pistachio-Counting", "journal-ref": null, "doi": "10.1007/s42044-021-00090-6", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pistachios are nutritious nuts that are sorted based on the shape of their\nshell into two categories: Open-mouth and Closed-mouth. The open-mouth\npistachios are higher in price, value, and demand than the closed-mouth\npistachios. Because of these differences, it is considerable for production\ncompanies to precisely count the number of each kind. This paper aims to\npropose a new system for counting the different types of pistachios with\ncomputer vision. We have introduced and shared a new dataset of pistachios,\nincluding six videos with a total length of 167 seconds and 3927 labeled\npistachios. Unlike many other works, our model counts pistachios in videos, not\nimages. Counting objects in videos need assigning each object between the video\nframes so that each object be counted once. The main two challenges in our work\nare the existence of pistachios' occlusion and deformation of pistachios in\ndifferent frames because open-mouth pistachios that move and roll on the\ntransportation line may appear as closed-mouth in some frames and open-mouth in\nother frames. Our novel model first is trained on the RetinaNet object detector\nnetwork using our dataset to detect different types of pistachios in video\nframes. After gathering the detections, we apply them to a new counter\nalgorithm based on a new tracker to assign pistachios in consecutive frames\nwith high accuracy. Our model is able to assign pistachios that turn and change\ntheir appearance (e.g., open-mouth pistachios that look closed-mouth) to each\nother so does not count them incorrectly. Our algorithm performs very fast and\nachieves good counting results. The computed accuracy of our algorithm on six\nvideos (9486 frames) is 94.75%.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 15:10:06 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 18:44:38 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 03:34:15 GMT"}, {"version": "v4", "created": "Mon, 3 May 2021 21:26:55 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Rahimzadeh", "Mohammad", ""], ["Attar", "Abolfazl", ""]]}, {"id": "2005.03991", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Mahdi Soltanolkotabi", "title": "Compressive sensing with un-trained neural networks: Gradient descent\n  finds the smoothest approximation", "comments": "arXiv admin note: text overlap with arXiv:1910.14634", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Un-trained convolutional neural networks have emerged as highly successful\ntools for image recovery and restoration. They are capable of solving standard\ninverse problems such as denoising and compressive sensing with excellent\nresults by simply fitting a neural network model to measurements from a single\nimage or signal without the need for any additional training data. For some\napplications, this critically requires additional regularization in the form of\nearly stopping the optimization. For signal recovery from a few measurements,\nhowever, un-trained convolutional networks have an intriguing self-regularizing\nproperty: Even though the network can perfectly fit any image, the network\nrecovers a natural image from few measurements when trained with gradient\ndescent until convergence. In this paper, we provide numerical evidence for\nthis property and study it theoretically. We show that---without any further\nregularization---an un-trained convolutional neural network can approximately\nreconstruct signals and images that are sufficiently structured, from a near\nminimal number of random measurements.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 15:57:25 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Heckel", "Reinhard", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "2005.03995", "submitter": "Mor Avi-Aharon", "authors": "Mor Avi-Aharon, Assaf Arbelle, and Tammy Riklin Raviv", "title": "DeepHist: Differentiable Joint and Color Histogram Layers for\n  Image-to-Image Translation", "comments": "arXiv admin note: text overlap with arXiv:1912.06044", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the DeepHist - a novel Deep Learning framework for augmenting a\nnetwork by histogram layers and demonstrate its strength by addressing\nimage-to-image translation problems. Specifically, given an input image and a\nreference color distribution we aim to generate an output image with the\nstructural appearance (content) of the input (source) yet with the colors of\nthe reference. The key idea is a new technique for a differentiable\nconstruction of joint and color histograms of the output images. We further\ndefine a color distribution loss based on the Earth Mover's Distance between\nthe output's and the reference's color histograms and a Mutual Information loss\nbased on the joint histograms of the source and the output images. Promising\nresults are shown for the tasks of color transfer, image colorization and edges\n$\\rightarrow$ photo, where the color distribution of the output image is\ncontrolled. Comparison to Pix2Pix and CyclyGANs are shown.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 20:07:58 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Avi-Aharon", "Mor", ""], ["Arbelle", "Assaf", ""], ["Raviv", "Tammy Riklin", ""]]}, {"id": "2005.04014", "submitter": "Mehmet Yamac", "authors": "Mehmet Yamac, Mete Ahishali, Aysen Degerli, Serkan Kiranyaz, Muhammad\n  E. H. Chowdhury, Moncef Gabbouj", "title": "Convolutional Sparse Support Estimator Based Covid-19 Recognition from\n  X-ray Images", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Coronavirus disease (Covid-19) has been the main agenda of the whole world\nsince it came in sight in December 2019. It has already caused thousands of\ncausalities and infected several millions worldwide. Any technological tool\nthat can be provided to healthcare practitioners to save time, effort, and\npossibly lives has crucial importance. The main tools practitioners currently\nuse to diagnose Covid-19 are Reverse Transcription-Polymerase Chain reaction\n(RT-PCR) and Computed Tomography (CT), which require significant time,\nresources and acknowledged experts. X-ray imaging is a common and easily\naccessible tool that has great potential for Covid-19 diagnosis. In this study,\nwe propose a novel approach for Covid-19 recognition from chest X-ray images.\nDespite the importance of the problem, recent studies in this domain produced\nnot so satisfactory results due to the limited datasets available for training.\nRecall that Deep Learning techniques can generally provide state-of-the-art\nperformance in many classification tasks when trained properly over large\ndatasets, such data scarcity can be a crucial obstacle when using them for\nCovid-19 detection. Alternative approaches such as representation-based\nclassification (collaborative or sparse representation) might provide\nsatisfactory performance with limited size datasets, but they generally fall\nshort in performance or speed compared to Machine Learning methods. To address\nthis deficiency, Convolution Support Estimation Network (CSEN) has recently\nbeen proposed as a bridge between model-based and Deep Learning approaches by\nproviding a non-iterative real-time mapping from query sample to ideally sparse\nrepresentation coefficient' support, which is critical information for class\ndecision in representation based techniques.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 13:11:40 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Yamac", "Mehmet", ""], ["Ahishali", "Mete", ""], ["Degerli", "Aysen", ""], ["Kiranyaz", "Serkan", ""], ["Chowdhury", "Muhammad E. H.", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2005.04043", "submitter": "Feng Shi", "authors": "Donglin Di, Feng Shi, Fuhua Yan, Liming Xia, Zhanhao Mo, Zhongxiang\n  Ding, Fei Shan, Shengrui Li, Ying Wei, Ying Shao, Miaofei Han, Yaozong Gao,\n  He Sui, Yue Gao, Dinggang Shen", "title": "Hypergraph Learning for Identification of COVID-19 with CT Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus disease, named COVID-19, has become the largest global public\nhealth crisis since it started in early 2020. CT imaging has been used as a\ncomplementary tool to assist early screening, especially for the rapid\nidentification of COVID-19 cases from community acquired pneumonia (CAP) cases.\nThe main challenge in early screening is how to model the confusing cases in\nthe COVID-19 and CAP groups, with very similar clinical manifestations and\nimaging features. To tackle this challenge, we propose an Uncertainty\nVertex-weighted Hypergraph Learning (UVHL) method to identify COVID-19 from CAP\nusing CT images. In particular, multiple types of features (including regional\nfeatures and radiomics features) are first extracted from CT image for each\ncase. Then, the relationship among different cases is formulated by a\nhypergraph structure, with each case represented as a vertex in the hypergraph.\nThe uncertainty of each vertex is further computed with an uncertainty score\nmeasurement and used as a weight in the hypergraph. Finally, a learning process\nof the vertex-weighted hypergraph is used to predict whether a new testing case\nbelongs to COVID-19 or not. Experiments on a large multi-center pneumonia\ndataset, consisting of 2,148 COVID-19 cases and 1,182 CAP cases from five\nhospitals, are conducted to evaluate the performance of the proposed method.\nResults demonstrate the effectiveness and robustness of our proposed method on\nthe identification of COVID-19 in comparison to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 11:26:32 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Di", "Donglin", ""], ["Shi", "Feng", ""], ["Yan", "Fuhua", ""], ["Xia", "Liming", ""], ["Mo", "Zhanhao", ""], ["Ding", "Zhongxiang", ""], ["Shan", "Fei", ""], ["Li", "Shengrui", ""], ["Wei", "Ying", ""], ["Shao", "Ying", ""], ["Han", "Miaofei", ""], ["Gao", "Yaozong", ""], ["Sui", "He", ""], ["Gao", "Yue", ""], ["Shen", "Dinggang", ""]]}, {"id": "2005.04063", "submitter": "Yajie Wang", "authors": "Pengyao Zhao, Quanli Liu, Wei Wang and Qiang Guo", "title": "TSDM: Tracking by SiamRPN++ with a Depth-refiner and a Mask-generator", "comments": "6 Pages, 6 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a generic object tracking, depth (D) information provides informative cues\nfor foreground-background separation and target bounding box regression.\nHowever, so far, few trackers have used depth information to play the important\nrole aforementioned due to the lack of a suitable model. In this paper, a RGB-D\ntracker named TSDM is proposed, which is composed of a Mask-generator (M-g),\nSiamRPN++ and a Depth-refiner (D-r). The M-g generates the background masks,\nand updates them as the target 3D position changes. The D-r optimizes the\ntarget bounding box estimated by SiamRPN++, based on the spatial depth\ndistribution difference between the target and the surrounding background.\nExtensive evaluation on the Princeton Tracking Benchmark and the Visual Object\nTracking challenge shows that our tracker outperforms the state-of-the-art by a\nlarge margin while achieving 23 FPS. In addition, a light-weight variant can\nrun at 31 FPS and thus it is practical for real world applications. Code and\nmodels of TSDM are available at https://github.com/lql-team/TSDM.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 14:25:41 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Zhao", "Pengyao", ""], ["Liu", "Quanli", ""], ["Wang", "Wei", ""], ["Guo", "Qiang", ""]]}, {"id": "2005.04065", "submitter": "David Schedl", "authors": "Indrajit Kurmi and David C. Schedl and Oliver Bimber", "title": "Fast Automatic Visibility Optimization for Thermal Synthetic Aperture\n  Visualization", "comments": "5 pages, 4 figures, 1 table, in IEEE Geoscience and Remote Sensing\n  Letters, 2020", "journal-ref": null, "doi": "10.1109/LGRS.2020.2987471", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we describe and validate the first fully automatic parameter\noptimization for thermal synthetic aperture visualization. It replaces previous\nmanual exploration of the parameter space, which is time consuming and error\nprone. We prove that the visibility of targets in thermal integral images is\nproportional to the variance of the targets' image. Since this is invariant to\nocclusion it represents a suitable objective function for optimization. Our\nfindings have the potential to enable fully autonomous search and recuse\noperations with camera drones.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 14:28:03 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Kurmi", "Indrajit", ""], ["Schedl", "David C.", ""], ["Bimber", "Oliver", ""]]}, {"id": "2005.04069", "submitter": "Ziyuan Zhao", "authors": "Jiapan Gu, Ziyuan Zhao, Zeng Zeng, Yuzhe Wang, Zhengyiren Qiu,\n  Bharadwaj Veeravalli, Brian Kim Poh Goh, Glenn Kunnath Bonney, Krishnakumar\n  Madhavan, Chan Wan Ying, Lim Kheng Choon, Thng Choon Hua, Pierce KH Chow", "title": "Multi-Phase Cross-modal Learning for Noninvasive Gene Mutation\n  Prediction in Hepatocellular Carcinoma", "comments": "Accepted version to be published in the 42nd IEEE Annual\n  International Conference of the IEEE Engineering in Medicine and Biology\n  Society, EMBC 2020, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV eess.IV q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hepatocellular carcinoma (HCC) is the most common type of primary liver\ncancer and the fourth most common cause of cancer-related death worldwide.\nUnderstanding the underlying gene mutations in HCC provides great prognostic\nvalue for treatment planning and targeted therapy. Radiogenomics has revealed\nan association between non-invasive imaging features and molecular genomics.\nHowever, imaging feature identification is laborious and error-prone. In this\npaper, we propose an end-to-end deep learning framework for mutation prediction\nin APOB, COL11A1 and ATRX genes using multiphasic CT scans. Considering\nintra-tumour heterogeneity (ITH) in HCC, multi-region sampling technology is\nimplemented to generate the dataset for experiments. Experimental results\ndemonstrate the effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 14:36:59 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Gu", "Jiapan", ""], ["Zhao", "Ziyuan", ""], ["Zeng", "Zeng", ""], ["Wang", "Yuzhe", ""], ["Qiu", "Zhengyiren", ""], ["Veeravalli", "Bharadwaj", ""], ["Goh", "Brian Kim Poh", ""], ["Bonney", "Glenn Kunnath", ""], ["Madhavan", "Krishnakumar", ""], ["Ying", "Chan Wan", ""], ["Choon", "Lim Kheng", ""], ["Hua", "Thng Choon", ""], ["Chow", "Pierce KH", ""]]}, {"id": "2005.04078", "submitter": "Lennart Reiher", "authors": "Lennart Reiher, Bastian Lampe, Lutz Eckstein", "title": "A Sim2Real Deep Learning Approach for the Transformation of Images from\n  Multiple Vehicle-Mounted Cameras to a Semantically Segmented Image in Bird's\n  Eye View", "comments": "Accepted to be published as part of the 23rd IEEE International\n  Conference on Intelligent Transportation Systems (ITSC), Rhodes, Greece,\n  September 20-23, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate environment perception is essential for automated driving. When\nusing monocular cameras, the distance estimation of elements in the environment\nposes a major challenge. Distances can be more easily estimated when the camera\nperspective is transformed to a bird's eye view (BEV). For flat surfaces,\nInverse Perspective Mapping (IPM) can accurately transform images to a BEV.\nThree-dimensional objects such as vehicles and vulnerable road users are\ndistorted by this transformation making it difficult to estimate their position\nrelative to the sensor. This paper describes a methodology to obtain a\ncorrected 360{\\deg} BEV image given images from multiple vehicle-mounted\ncameras. The corrected BEV image is segmented into semantic classes and\nincludes a prediction of occluded areas. The neural network approach does not\nrely on manually labeled data, but is trained on a synthetic dataset in such a\nway that it generalizes well to real-world data. By using semantically\nsegmented images as input, we reduce the reality gap between simulated and\nreal-world data and are able to show that our method can be successfully\napplied in the real world. Extensive experiments conducted on the synthetic\ndata demonstrate the superiority of our approach compared to IPM. Source code\nand datasets are available at https://github.com/ika-rwth-aachen/Cam2BEV\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 14:54:13 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Reiher", "Lennart", ""], ["Lampe", "Bastian", ""], ["Eckstein", "Lutz", ""]]}, {"id": "2005.04111", "submitter": "Wei Wang", "authors": "Wei Wang, Zhihui Wang, Yuankai Xiang, Jing Sun, Haojie Li, Fuming Sun,\n  Zhengming Ding", "title": "Sparsely-Labeled Source Assisted Domain Adaptation", "comments": "22 pages, 6 figures, submitted to the Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain Adaptation (DA) aims to generalize the classifier learned from the\nsource domain to the target domain. Existing DA methods usually assume that\nrich labels could be available in the source domain. However, there are usually\na large number of unlabeled data but only a few labeled data in the source\ndomain, and how to transfer knowledge from this sparsely-labeled source domain\nto the target domain is still a challenge, which greatly limits their\napplication in the wild. This paper proposes a novel Sparsely-Labeled Source\nAssisted Domain Adaptation (SLSA-DA) algorithm to address the challenge with\nlimited labeled source domain samples. Specifically, due to the label scarcity\nproblem, the projected clustering is conducted on both the source and target\ndomains, so that the discriminative structures of data could be leveraged\nelegantly. Then the label propagation is adopted to propagate the labels from\nthose limited labeled source samples to the whole unlabeled data progressively,\nso that the cluster labels are revealed correctly. Finally, we jointly align\nthe marginal and conditional distributions to mitigate the cross-domain\nmismatch problem, and optimize those three procedures iteratively. However, it\nis nontrivial to incorporate those three procedures into a unified optimization\nframework seamlessly since some variables to be optimized are implicitly\ninvolved in their formulas, thus they could not promote to each other.\nRemarkably, we prove that the projected clustering and conditional distribution\nalignment could be reformulated as different expressions, thus the implicit\nvariables are revealed in different optimization steps. As such, the variables\nrelated to those three quantities could be optimized in a unified optimization\nframework and facilitate to each other, to improve the recognition performance\nobviously.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 15:37:35 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Wang", "Wei", ""], ["Wang", "Zhihui", ""], ["Xiang", "Yuankai", ""], ["Sun", "Jing", ""], ["Li", "Haojie", ""], ["Sun", "Fuming", ""], ["Ding", "Zhengming", ""]]}, {"id": "2005.04117", "submitter": "Abdelrahman Abdelhamed", "authors": "Abdelrahman Abdelhamed, Mahmoud Afifi, Radu Timofte, Michael S. Brown,\n  Yue Cao, Zhilu Zhang, Wangmeng Zuo, Xiaoling Zhang, Jiye Liu, Wendong Chen,\n  Changyuan Wen, Meng Liu, Shuailin Lv, Yunchao Zhang, Zhihong Pan, Baopu Li,\n  Teng Xi, Yanwen Fan, Xiyu Yu, Gang Zhang, Jingtuo Liu, Junyu Han, Errui Ding,\n  Songhyun Yu, Bumjun Park, Jechang Jeong, Shuai Liu, Ziyao Zong, Nan Nan,\n  Chenghua Li, Zengli Yang, Long Bao, Shuangquan Wang, Dongwoon Bai, Jungwon\n  Lee, Youngjung Kim, Kyeongha Rho, Changyeop Shin, Sungho Kim, Pengliang Tang,\n  Yiyun Zhao, Yuqian Zhou, Yuchen Fan, Thomas Huang, Zhihao Li, Nisarg A. Shah,\n  Wei Liu, Qiong Yan, Yuzhi Zhao, Marcin Mo\\.zejko, Tomasz Latkowski, Lukasz\n  Treszczotko, Micha{\\l} Szafraniuk, Krzysztof Trojanowski, Yanhong Wu, Pablo\n  Navarrete Michelini, Fengshuo Hu, Yunhua Lu, Sujin Kim, Wonjin Kim, Jaayeon\n  Lee, Jang-Hwan Choi, Magauiya Zhussip, Azamat Khassenov, Jong Hyun Kim,\n  Hwechul Cho, Priya Kansal, Sabari Nathan, Zhangyu Ye, Xiwen Lu, Yaqi Wu,\n  Jiangxin Yang, Yanlong Cao, Siliang Tang, Yanpeng Cao, Matteo Maggioni,\n  Ioannis Marras, Thomas Tanay, Gregory Slabaugh, Youliang Yan, Myungjoo Kang,\n  Han-Soo Choi, Kyungmin Song, Shusong Xu, Xiaomu Lu, Tingniao Wang, Chunxia\n  Lei, Bin Liu, Rajat Gupta, Vineet Kumar", "title": "NTIRE 2020 Challenge on Real Image Denoising: Dataset, Methods and\n  Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the NTIRE 2020 challenge on real image denoising with\nfocus on the newly introduced dataset, the proposed methods and their results.\nThe challenge is a new version of the previous NTIRE 2019 challenge on real\nimage denoising that was based on the SIDD benchmark. This challenge is based\non a newly collected validation and testing image datasets, and hence, named\nSIDD+. This challenge has two tracks for quantitatively evaluating image\ndenoising performance in (1) the Bayer-pattern rawRGB and (2) the standard RGB\n(sRGB) color spaces. Each track ~250 registered participants. A total of 22\nteams, proposing 24 methods, competed in the final phase of the challenge. The\nproposed methods by the participating teams represent the current\nstate-of-the-art performance in image denoising targeting real noisy images.\nThe newly collected SIDD+ datasets are publicly available at:\nhttps://bit.ly/siddplus_data.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 15:46:19 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Abdelhamed", "Abdelrahman", ""], ["Afifi", "Mahmoud", ""], ["Timofte", "Radu", ""], ["Brown", "Michael S.", ""], ["Cao", "Yue", ""], ["Zhang", "Zhilu", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Xiaoling", ""], ["Liu", "Jiye", ""], ["Chen", "Wendong", ""], ["Wen", "Changyuan", ""], ["Liu", "Meng", ""], ["Lv", "Shuailin", ""], ["Zhang", "Yunchao", ""], ["Pan", "Zhihong", ""], ["Li", "Baopu", ""], ["Xi", "Teng", ""], ["Fan", "Yanwen", ""], ["Yu", "Xiyu", ""], ["Zhang", "Gang", ""], ["Liu", "Jingtuo", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""], ["Yu", "Songhyun", ""], ["Park", "Bumjun", ""], ["Jeong", "Jechang", ""], ["Liu", "Shuai", ""], ["Zong", "Ziyao", ""], ["Nan", "Nan", ""], ["Li", "Chenghua", ""], ["Yang", "Zengli", ""], ["Bao", "Long", ""], ["Wang", "Shuangquan", ""], ["Bai", "Dongwoon", ""], ["Lee", "Jungwon", ""], ["Kim", "Youngjung", ""], ["Rho", "Kyeongha", ""], ["Shin", "Changyeop", ""], ["Kim", "Sungho", ""], ["Tang", "Pengliang", ""], ["Zhao", "Yiyun", ""], ["Zhou", "Yuqian", ""], ["Fan", "Yuchen", ""], ["Huang", "Thomas", ""], ["Li", "Zhihao", ""], ["Shah", "Nisarg A.", ""], ["Liu", "Wei", ""], ["Yan", "Qiong", ""], ["Zhao", "Yuzhi", ""], ["Mo\u017cejko", "Marcin", ""], ["Latkowski", "Tomasz", ""], ["Treszczotko", "Lukasz", ""], ["Szafraniuk", "Micha\u0142", ""], ["Trojanowski", "Krzysztof", ""], ["Wu", "Yanhong", ""], ["Michelini", "Pablo Navarrete", ""], ["Hu", "Fengshuo", ""], ["Lu", "Yunhua", ""], ["Kim", "Sujin", ""], ["Kim", "Wonjin", ""], ["Lee", "Jaayeon", ""], ["Choi", "Jang-Hwan", ""], ["Zhussip", "Magauiya", ""], ["Khassenov", "Azamat", ""], ["Kim", "Jong Hyun", ""], ["Cho", "Hwechul", ""], ["Kansal", "Priya", ""], ["Nathan", "Sabari", ""], ["Ye", "Zhangyu", ""], ["Lu", "Xiwen", ""], ["Wu", "Yaqi", ""], ["Yang", "Jiangxin", ""], ["Cao", "Yanlong", ""], ["Tang", "Siliang", ""], ["Cao", "Yanpeng", ""], ["Maggioni", "Matteo", ""], ["Marras", "Ioannis", ""], ["Tanay", "Thomas", ""], ["Slabaugh", "Gregory", ""], ["Yan", "Youliang", ""], ["Kang", "Myungjoo", ""], ["Choi", "Han-Soo", ""], ["Song", "Kyungmin", ""], ["Xu", "Shusong", ""], ["Lu", "Xiaomu", ""], ["Wang", "Tingniao", ""], ["Lei", "Chunxia", ""], ["Liu", "Bin", ""], ["Gupta", "Rajat", ""], ["Kumar", "Vineet", ""]]}, {"id": "2005.04136", "submitter": "Yoojin Choi", "authors": "Yoojin Choi, Jihwan Choi, Mostafa El-Khamy, Jungwon Lee", "title": "Data-Free Network Quantization With Adversarial Knowledge Distillation", "comments": "CVPR 2020 Joint Workshop on Efficient Deep Learning in Computer\n  Vision (EDLCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network quantization is an essential procedure in deep learning for\ndevelopment of efficient fixed-point inference models on mobile or edge\nplatforms. However, as datasets grow larger and privacy regulations become\nstricter, data sharing for model compression gets more difficult and\nrestricted. In this paper, we consider data-free network quantization with\nsynthetic data. The synthetic data are generated from a generator, while no\ndata are used in training the generator and in quantization. To this end, we\npropose data-free adversarial knowledge distillation, which minimizes the\nmaximum distance between the outputs of the teacher and the (quantized) student\nfor any adversarial samples from a generator. To generate adversarial samples\nsimilar to the original data, we additionally propose matching statistics from\nthe batch normalization layers for generated data and the original data in the\nteacher. Furthermore, we show the gain of producing diverse adversarial samples\nby using multiple generators and multiple students. Our experiments show the\nstate-of-the-art data-free model compression and quantization results for\n(wide) residual networks and MobileNet on SVHN, CIFAR-10, CIFAR-100, and\nTiny-ImageNet datasets. The accuracy losses compared to using the original\ndatasets are shown to be very minimal.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 16:24:55 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Choi", "Yoojin", ""], ["Choi", "Jihwan", ""], ["El-Khamy", "Mostafa", ""], ["Lee", "Jungwon", ""]]}, {"id": "2005.04143", "submitter": "Xiaozhen Xie", "authors": "Haijin Zeng, Xiaozhen Xie, Jifeng Ning", "title": "Hyperspectral Image Restoration via Global Total Variation Regularized\n  Local nonconvex Low-Rank matrix Approximation", "comments": "Accepted for publication in IEEE IGARSS 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several bandwise total variation (TV) regularized low-rank (LR)-based models\nhave been proposed to remove mixed noise in hyperspectral images (HSIs).\nConventionally, the rank of LR matrix is approximated using nuclear norm (NN).\nThe NN is defined by adding all singular values together, which is essentially\na $L_1$-norm of the singular values. It results in non-negligible approximation\nerrors and thus the resulting matrix estimator can be significantly biased.\nMoreover, these bandwise TV-based methods exploit the spatial information in a\nseparate manner. To cope with these problems, we propose a spatial-spectral TV\n(SSTV) regularized non-convex local LR matrix approximation (NonLLRTV) method\nto remove mixed noise in HSIs. From one aspect, local LR of HSIs is formulated\nusing a non-convex $L_{\\gamma}$-norm, which provides a closer approximation to\nthe matrix rank than the traditional NN. From another aspect, HSIs are assumed\nto be piecewisely smooth in the global spatial domain. The TV regularization is\neffective in preserving the smoothness and removing Gaussian noise. These facts\ninspire the integration of the NonLLR with TV regularization. To address the\nlimitations of bandwise TV, we use the SSTV regularization to simultaneously\nconsider global spatial structure and spectral correlation of neighboring\nbands. Experiment results indicate that the use of local non-convex penalty and\nglobal SSTV can boost the preserving of spatial piecewise smoothness and\noverall structural information.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 16:42:18 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Zeng", "Haijin", ""], ["Xie", "Xiaozhen", ""], ["Ning", "Jifeng", ""]]}, {"id": "2005.04153", "submitter": "Vasco Lopes Ferrinho", "authors": "Vasco Lopes, Paulo Fazendeiro", "title": "A Hybrid Method for Training Convolutional Neural Networks", "comments": "1 figure, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence algorithms have been steadily increasing in\npopularity and usage. Deep Learning, allows neural networks to be trained using\nhuge datasets and also removes the need for human extracted features, as it\nautomates the feature learning process. In the hearth of training deep neural\nnetworks, such as Convolutional Neural Networks, we find backpropagation, that\nby computing the gradient of the loss function with respect to the weights of\nthe network for a given input, it allows the weights of the network to be\nadjusted to better perform in the given task. In this paper, we propose a\nhybrid method that uses both backpropagation and evolutionary strategies to\ntrain Convolutional Neural Networks, where the evolutionary strategies are used\nto help to avoid local minimas and fine-tune the weights, so that the network\nachieves higher accuracy results. We show that the proposed hybrid method is\ncapable of improving upon regular training in the task of image classification\nin CIFAR-10, where a VGG16 model was used and the final test results increased\n0.61%, in average, when compared to using only backpropagation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 17:52:48 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Lopes", "Vasco", ""], ["Fazendeiro", "Paulo", ""]]}, {"id": "2005.04184", "submitter": "Donald Reising", "authors": "Mohamed Fadul, Donald Reising, T. Daniel Loveless, Abdul Ofoli", "title": "Preprint: Using RF-DNA Fingerprints To Classify OFDM Transmitters Under\n  Rayleigh Fading Conditions", "comments": "13 pages, 14 total figures/images, Currently under review by the IEEE\n  Transactions on Information Forensics and Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) is a collection of Internet connected devices\ncapable of interacting with the physical world and computer systems. It is\nestimated that the IoT will consist of approximately fifty billion devices by\nthe year 2020. In addition to the sheer numbers, the need for IoT security is\nexacerbated by the fact that many of the edge devices employ weak to no\nencryption of the communication link. It has been estimated that almost 70% of\nIoT devices use no form of encryption. Previous research has suggested the use\nof Specific Emitter Identification (SEI), a physical layer technique, as a\nmeans of augmenting bit-level security mechanism such as encryption. The work\npresented here integrates a Nelder-Mead based approach for estimating the\nRayleigh fading channel coefficients prior to the SEI approach known as RF-DNA\nfingerprinting. The performance of this estimator is assessed for degrading\nsignal-to-noise ratio and compared with least square and minimum mean squared\nerror channel estimators. Additionally, this work presents classification\nresults using RF-DNA fingerprints that were extracted from received signals\nthat have undergone Rayleigh fading channel correction using Minimum Mean\nSquared Error (MMSE) equalization. This work also performs radio discrimination\nusing RF-DNA fingerprints generated from the normalized magnitude-squared and\nphase response of Gabor coefficients as well as two classifiers. Discrimination\nof four 802.11a Wi-Fi radios achieves an average percent correct classification\nof 90% or better for signal-to-noise ratios of 18 and 21 dB or greater using a\nRayleigh fading channel comprised of two and five paths, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 13:53:25 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Fadul", "Mohamed", ""], ["Reising", "Donald", ""], ["Loveless", "T. Daniel", ""], ["Ofoli", "Abdul", ""]]}, {"id": "2005.04208", "submitter": "Max Bain", "authors": "Max Bain, Arsha Nagrani, Andrew Brown, Andrew Zisserman", "title": "Condensed Movies: Story Based Retrieval with Contextual Embeddings", "comments": "Appears in: Asian Conference on Computer Vision 2020 (ACCV 2020) -\n  Oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our objective in this work is long range understanding of the narrative\nstructure of movies. Instead of considering the entire movie, we propose to\nlearn from the `key scenes' of the movie, providing a condensed look at the\nfull storyline. To this end, we make the following three contributions: (i) We\ncreate the Condensed Movies Dataset (CMD) consisting of the key scenes from\nover 3K movies: each key scene is accompanied by a high level semantic\ndescription of the scene, character face-tracks, and metadata about the movie.\nThe dataset is scalable, obtained automatically from YouTube, and is freely\navailable for anybody to download and use. It is also an order of magnitude\nlarger than existing movie datasets in the number of movies; (ii) We provide a\ndeep network baseline for text-to-video retrieval on our dataset, combining\ncharacter, speech and visual cues into a single video embedding; and finally\n(iii) We demonstrate how the addition of context from other video clips\nimproves retrieval performance.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 17:55:03 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 23:42:02 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Bain", "Max", ""], ["Nagrani", "Arsha", ""], ["Brown", "Andrew", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2005.04255", "submitter": "Junhua Mao", "authors": "Zhishuai Zhang, Jiyang Gao, Junhua Mao, Yukai Liu, Dragomir Anguelov,\n  Congcong Li", "title": "STINet: Spatio-Temporal-Interactive Network for Pedestrian Detection and\n  Trajectory Prediction", "comments": null, "journal-ref": "CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting pedestrians and predicting future trajectories for them are\ncritical tasks for numerous applications, such as autonomous driving. Previous\nmethods either treat the detection and prediction as separate tasks or simply\nadd a trajectory regression head on top of a detector. In this work, we present\na novel end-to-end two-stage network: Spatio-Temporal-Interactive Network\n(STINet). In addition to 3D geometry modeling of pedestrians, we model the\ntemporal information for each of the pedestrians. To do so, our method predicts\nboth current and past locations in the first stage, so that each pedestrian can\nbe linked across frames and the comprehensive spatio-temporal information can\nbe captured in the second stage. Also, we model the interaction among objects\nwith an interaction graph, to gather the information among the neighboring\nobjects. Comprehensive experiments on the Lyft Dataset and the recently\nreleased large-scale Waymo Open Dataset for both object detection and future\ntrajectory prediction validate the effectiveness of the proposed method. For\nthe Waymo Open Dataset, we achieve a bird-eyes-view (BEV) detection AP of 80.73\nand trajectory prediction average displacement error (ADE) of 33.67cm for\npedestrians, which establish the state-of-the-art for both tasks.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 18:43:01 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Zhang", "Zhishuai", ""], ["Gao", "Jiyang", ""], ["Mao", "Junhua", ""], ["Liu", "Yukai", ""], ["Anguelov", "Dragomir", ""], ["Li", "Congcong", ""]]}, {"id": "2005.04258", "submitter": "Vivek Singh", "authors": "Walid Bekhtaoui, Ruhan Sa, Brian Teixeira, Vivek Singh, Klaus\n  Kirchberg, Yao-jen Chang, Ankur Kapoor", "title": "View Invariant Human Body Detection and Pose Estimation from Multiple\n  Depth Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud based methods have produced promising results in areas such as 3D\nobject detection in autonomous driving. However, most of the recent point cloud\nwork focuses on single depth sensor data, whereas less work has been done on\nindoor monitoring applications, such as operation room monitoring in hospitals\nor indoor surveillance. In these scenarios multiple cameras are often used to\ntackle occlusion problems. We propose an end-to-end multi-person 3D pose\nestimation network, Point R-CNN, using multiple point cloud sources. We conduct\nextensive experiments to simulate challenging real world cases, such as\nindividual camera failures, various target appearances, and complex cluttered\nscenes with the CMU panoptic dataset and the MVOR operation room dataset.\nUnlike most of the previous methods that attempt to use multiple sensor\ninformation by building complex fusion models, which often lead to poor\ngeneralization, we take advantage of the efficiency of concatenating point\nclouds to fuse the information at the input level. In the meantime, we show our\nend-to-end network greatly outperforms cascaded state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 19:06:28 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Bekhtaoui", "Walid", ""], ["Sa", "Ruhan", ""], ["Teixeira", "Brian", ""], ["Singh", "Vivek", ""], ["Kirchberg", "Klaus", ""], ["Chang", "Yao-jen", ""], ["Kapoor", "Ankur", ""]]}, {"id": "2005.04259", "submitter": "Jiyang Gao", "authors": "Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong\n  Li, Cordelia Schmid", "title": "VectorNet: Encoding HD Maps and Agent Dynamics from Vectorized\n  Representation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavior prediction in dynamic, multi-agent systems is an important problem\nin the context of self-driving cars, due to the complex representations and\ninteractions of road components, including moving agents (e.g. pedestrians and\nvehicles) and road context information (e.g. lanes, traffic lights). This paper\nintroduces VectorNet, a hierarchical graph neural network that first exploits\nthe spatial locality of individual road components represented by vectors and\nthen models the high-order interactions among all components. In contrast to\nmost recent approaches, which render trajectories of moving agents and road\ncontext information as bird-eye images and encode them with convolutional\nneural networks (ConvNets), our approach operates on a vector representation.\nBy operating on the vectorized high definition (HD) maps and agent\ntrajectories, we avoid lossy rendering and computationally intensive ConvNet\nencoding steps. To further boost VectorNet's capability in learning context\nfeatures, we propose a novel auxiliary task to recover the randomly masked out\nmap entities and agent trajectories based on their context. We evaluate\nVectorNet on our in-house behavior prediction benchmark and the recently\nreleased Argoverse forecasting dataset. Our method achieves on par or better\nperformance than the competitive rendering approach on both benchmarks while\nsaving over 70% of the model parameters with an order of magnitude reduction in\nFLOPs. It also outperforms the state of the art on the Argoverse dataset.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 19:07:03 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Gao", "Jiyang", ""], ["Sun", "Chen", ""], ["Zhao", "Hang", ""], ["Shen", "Yi", ""], ["Anguelov", "Dragomir", ""], ["Li", "Congcong", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2005.04292", "submitter": "Siddarth Sairaj S", "authors": "Siddarth S, Sainath G, Vignesh S", "title": "Deep Residual Network based food recognition for enhanced Augmented\n  Reality application", "comments": "Total Pages:7 Total Figures:10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network based learning approaches is widely utilized for image\nclassification or object detection based problems with remarkable outcomes.\nRealtime Object state estimation of objects can be used to track and estimate\nthe features that the object of the current frame possesses without causing any\nsignificant delay and misclassification. A system that can detect the features\nof such objects in the present state from camera images can be used to enhance\nthe application of Augmented Reality for improving user experience and\ndelivering information in a much perceptual way. The focus behind this paper is\nto determine the most suitable model to create a low-latency assistance AR to\naid users by providing them nutritional information about the food that they\nconsume in order to promote healthier life choices. Hence the dataset has been\ncollected and acquired in such a manner, and we conduct various tests in order\nto identify the most suitable DNN in terms of performance and complexity and\nestablish a system that renders such information realtime to the user.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 21:08:58 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 17:47:47 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["S", "Siddarth", ""], ["G", "Sainath", ""], ["S", "Vignesh", ""]]}, {"id": "2005.04298", "submitter": "Jinkyu Kim", "authors": "Jinkyu Kim, Mayank Bansal", "title": "Attentional Bottleneck: Towards an Interpretable Deep Driving Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are a key component of behavior prediction and motion\ngeneration for self-driving cars. One of their main drawbacks is a lack of\ntransparency: they should provide easy to interpret rationales for what\ntriggers certain behaviors. We propose an architecture called Attentional\nBottleneck with the goal of improving transparency. Our key idea is to combine\nvisual attention, which identifies what aspects of the input the model is\nusing, with an information bottleneck that enables the model to only use\naspects of the input which are important. This not only provides sparse and\ninterpretable attention maps (e.g. focusing only on specific vehicles in the\nscene), but it adds this transparency at no cost to model accuracy. In fact, we\nfind slight improvements in accuracy when applying Attentional Bottleneck to\nthe ChauffeurNet model, whereas we find that the accuracy deteriorates with a\ntraditional visual attention model.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 21:51:15 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Kim", "Jinkyu", ""], ["Bansal", "Mayank", ""]]}, {"id": "2005.04305", "submitter": "Danny Hernandez", "authors": "Danny Hernandez, Tom B. Brown", "title": "Measuring the Algorithmic Efficiency of Neural Networks", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three factors drive the advance of AI: algorithmic innovation, data, and the\namount of compute available for training. Algorithmic progress has\ntraditionally been more difficult to quantify than compute and data. In this\nwork, we argue that algorithmic progress has an aspect that is both\nstraightforward to measure and interesting: reductions over time in the compute\nneeded to reach past capabilities. We show that the number of floating-point\noperations required to train a classifier to AlexNet-level performance on\nImageNet has decreased by a factor of 44x between 2012 and 2019. This\ncorresponds to algorithmic efficiency doubling every 16 months over a period of\n7 years. By contrast, Moore's Law would only have yielded an 11x cost\nimprovement. We observe that hardware and algorithmic efficiency gains multiply\nand can be on a similar scale over meaningful horizons, which suggests that a\ngood model of AI progress should integrate measures from both.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 22:26:37 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Hernandez", "Danny", ""], ["Brown", "Tom B.", ""]]}, {"id": "2005.04311", "submitter": "Abdullah-Al-Zubaer Imran", "authors": "Abdullah-Al-Zubaer Imran and Demetri Terzopoulos", "title": "Progressive Adversarial Semantic Segmentation", "comments": "9 pages, 5 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image computing has advanced rapidly with the advent of deep learning\ntechniques such as convolutional neural networks. Deep convolutional neural\nnetworks can perform exceedingly well given full supervision. However, the\nsuccess of such fully-supervised models for various image analysis tasks (e.g.,\nanatomy or lesion segmentation from medical images) is limited to the\navailability of massive amounts of labeled data. Given small sample sizes, such\nmodels are prohibitively data biased with large domain shift. To tackle this\nproblem, we propose a novel end-to-end medical image segmentation model, namely\nProgressive Adversarial Semantic Segmentation (PASS), which can make improved\nsegmentation predictions without requiring any domain-specific data during\ntraining time. Our extensive experimentation with 8 public diabetic retinopathy\nand chest X-ray datasets, confirms the effectiveness of PASS for accurate\nvascular and pulmonary segmentation, both for in-domain and cross-domain\nevaluations.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 22:48:00 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Imran", "Abdullah-Al-Zubaer", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "2005.04319", "submitter": "Benjamin C.K. Tee", "authors": "Hian Hian See, Brian Lim, Si Li, Haicheng Yao, Wen Cheng, Harold Soh,\n  and Benjamin C.K. Tee", "title": "ST-MNIST -- The Spiking Tactile MNIST Neuromorphic Dataset", "comments": "Corresponding authors: Benjamin C.K. Tee and Harold Soh For dataset,\n  see http://www.benjamintee.com/stmnist 10 Pages, 4 Figures and 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tactile sensing is an essential modality for smart robots as it enables them\nto interact flexibly with physical objects in their environment. Recent\nadvancements in electronic skins have led to the development of data-driven\nmachine learning methods that exploit this important sensory modality. However,\ncurrent datasets used to train such algorithms are limited to standard\nsynchronous tactile sensors. There is a dearth of neuromorphic event-based\ntactile datasets, principally due to the scarcity of large-scale event-based\ntactile sensors. Having such datasets is crucial for the development and\nevaluation of new algorithms that process spatio-temporal event-based data. For\nexample, evaluating spiking neural networks on conventional frame-based\ndatasets is considered sub-optimal. Here, we debut a novel neuromorphic Spiking\nTactile MNIST (ST-MNIST) dataset, which comprises handwritten digits obtained\nby human participants writing on a neuromorphic tactile sensor array. We also\ndescribe an initial effort to evaluate our ST-MNIST dataset using existing\nartificial and spiking neural network models. The classification accuracies\nprovided herein can serve as performance benchmarks for future work. We\nanticipate that our ST-MNIST dataset will be of interest and useful to the\nneuromorphic and robotics research communities.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 23:44:14 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["See", "Hian Hian", ""], ["Lim", "Brian", ""], ["Li", "Si", ""], ["Yao", "Haicheng", ""], ["Cheng", "Wen", ""], ["Soh", "Harold", ""], ["Tee", "Benjamin C. K.", ""]]}, {"id": "2005.04345", "submitter": "Shiori Sagawa", "authors": "Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, Percy Liang", "title": "An Investigation of Why Overparameterization Exacerbates Spurious\n  Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study why overparameterization -- increasing model size well beyond the\npoint of zero training error -- can hurt test error on minority groups despite\nimproving average test error when there are spurious correlations in the data.\nThrough simulations and experiments on two image datasets, we identify two key\nproperties of the training data that drive this behavior: the proportions of\nmajority versus minority groups, and the signal-to-noise ratio of the spurious\ncorrelations. We then analyze a linear setting and theoretically show how the\ninductive bias of models towards \"memorizing\" fewer examples can cause\noverparameterization to hurt. Our analysis leads to a counterintuitive approach\nof subsampling the majority group, which empirically achieves low minority\nerror in the overparameterized regime, even though the standard approach of\nupweighting the minority fails. Overall, our results suggest a tension between\nusing overparameterized models versus using all the training data for achieving\nlow worst-group error.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 01:59:13 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 22:56:30 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 19:32:58 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Sagawa", "Shiori", ""], ["Raghunathan", "Aditi", ""], ["Koh", "Pang Wei", ""], ["Liang", "Percy", ""]]}, {"id": "2005.04370", "submitter": "Jianhui Yu", "authors": "Jianhui Yu, Chaoyi Zhang, Yang Song, Weidong Cai", "title": "ICE-GAN: Identity-aware and Capsule-Enhanced GAN with Graph-based\n  Reasoning for Micro-Expression Recognition and Synthesis", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-expressions are reflections of people's true feelings and motives,\nwhich attract an increasing number of researchers into the study of automatic\nfacial micro-expression recognition. The short detection window, the subtle\nfacial muscle movements, and the limited training samples make micro-expression\nrecognition challenging. To this end, we propose a novel Identity-aware and\nCapsule-Enhanced Generative Adversarial Network with graph-based reasoning\n(ICE-GAN), introducing micro-expression synthesis as an auxiliary task to\nassist recognition. The generator produces synthetic faces with controllable\nmicro-expressions and identity-aware features, whose long-ranged dependencies\nare captured through the graph reasoning module (GRM), and the discriminator\ndetects the image authenticity and expression classes. Our ICE-GAN was\nevaluated on Micro-Expression Grand Challenge 2019 (MEGC2019) with a\nsignificant improvement (12.9%) over the winner and surpassed other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 05:37:44 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 05:39:26 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Yu", "Jianhui", ""], ["Zhang", "Chaoyi", ""], ["Song", "Yang", ""], ["Cai", "Weidong", ""]]}, {"id": "2005.04373", "submitter": "Sungbin Lim", "authors": "Woonhyuk Baek and Ildoo Kim and Sungwoong Kim and Sungbin Lim", "title": "AutoCLINT: The Winning Method in AutoCV Challenge 2019", "comments": "9 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NeurIPS 2019 AutoDL challenge is a series of six automated machine learning\ncompetitions. Particularly, AutoCV challenges mainly focused on classification\ntasks on visual domain. In this paper, we introduce the winning method in the\ncompetition, AutoCLINT. The proposed method implements an autonomous training\nstrategy, including efficient code optimization, and applies an automated data\naugmentation to achieve the fast adaptation of pretrained networks. We\nimplement a light version of Fast AutoAugment to search for data augmentation\npolicies efficiently for the arbitrarily given image domains. We also\nempirically analyze the components of the proposed method and provide ablation\nstudies focusing on AutoCV datasets.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 05:50:38 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Baek", "Woonhyuk", ""], ["Kim", "Ildoo", ""], ["Kim", "Sungwoong", ""], ["Lim", "Sungbin", ""]]}, {"id": "2005.04397", "submitter": "Jiannan Zhao", "authors": "Jiannan Zhao, Hongxin Wang, and Shigang Yue", "title": "Enhancing LGMD's Looming Selectivity for UAV with Spatial-temporal\n  Distributed Presynaptic Connections", "comments": "15 pages, 17 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collision detection is one of the most challenging tasks for Unmanned Aerial\nVehicles (UAVs). This is especially true for small or micro UAVs, due to their\nlimited computational power. In nature, flying insects with compact and simple\nvisual systems demonstrate their remarkable ability to navigate and avoid\ncollision in complex environments. A good example of this is provided by\nlocusts. They can avoid collisions in a dense swarm through the activity of a\nmotion based visual neuron called the Lobula Giant Movement Detector (LGMD).\nThe defining feature of the LGMD neuron is its preference for looming. As a\nflying insect's visual neuron, LGMD is considered to be an ideal basis for\nbuilding UAV's collision detecting system. However, existing LGMD models cannot\ndistinguish looming clearly from other visual cues such as complex background\nmovements caused by UAV agile flights. To address this issue, we proposed a new\nmodel implementing distributed spatial-temporal synaptic interactions, which is\ninspired by recent findings in locusts' synaptic morphology. We first\nintroduced the locally distributed excitation to enhance the excitation caused\nby visual motion with preferred velocities. Then radially extending temporal\nlatency for inhibition is incorporated to compete with the distributed\nexcitation and selectively suppress the non-preferred visual motions.\nSystematic experiments have been conducted to verify the performance of the\nproposed model for UAV agile flights. The results have demonstrated that this\nnew model enhances the looming selectivity in complex flying scenes\nconsiderably, and has potential to be implemented on embedded collision\ndetection systems for small or micro UAVs.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 09:15:02 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 02:55:02 GMT"}, {"version": "v3", "created": "Sat, 17 Apr 2021 04:38:13 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhao", "Jiannan", ""], ["Wang", "Hongxin", ""], ["Yue", "Shigang", ""]]}, {"id": "2005.04400", "submitter": "Franz G\\\"otz-Hahn", "authors": "Franz G\\\"otz-Hahn, Vlad Hosu, Dietmar Saupe", "title": "Comment on \"No-Reference Video Quality Assessment Based on the Temporal\n  Pooling of Deep Features\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Neural Processing Letters 50,3 (2019) a machine learning approach to blind\nvideo quality assessment was proposed. It is based on temporal pooling of\nfeatures of video frames, taken from the last pooling layer of deep\nconvolutional neural networks. The method was validated on two established\nbenchmark datasets and gave results far better than the previous\nstate-of-the-art. In this letter we report the results from our careful\nreimplementations. The performance results, claimed in the paper, cannot be\nreached, and are even below the state-of-the-art by a large margin. We show\nthat the originally reported wrong performance results are a consequence of two\ncases of data leakage. Information from outside the training dataset was used\nin the fine-tuning stage and in the model evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 09:28:01 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["G\u00f6tz-Hahn", "Franz", ""], ["Hosu", "Vlad", ""], ["Saupe", "Dietmar", ""]]}, {"id": "2005.04401", "submitter": "Kevin Bui", "authors": "Kevin Bui, Fredrick Park, Yifei Lou, Jack Xin", "title": "A Weighted Difference of Anisotropic and Isotropic Total Variation for\n  Relaxed Mumford-Shah Color and Multiphase Image Segmentation", "comments": "latest version has typos fixed; Clean, official version will be on\n  SIAM Journal on Imaging Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a class of piecewise-constant image segmentation models, we propose to\nincorporate a weighted difference of anisotropic and isotropic total variation\n(AITV) to regularize the partition boundaries in an image. In particular, we\nreplace the total variation regularization in the Chan-Vese segmentation model\nand a fuzzy region competition model by the proposed AITV. To deal with the\nnonconvex nature of AITV, we apply the difference-of-convex algorithm (DCA), in\nwhich the subproblems can be minimized by the primal-dual hybrid gradient\nmethod with linesearch. The convergence of the DCA scheme is analyzed. In\naddition, a generalization to color image segmentation is discussed. In the\nnumerical experiments, we compare the proposed models with the classic convex\napproaches and the two-stage segmentation methods (smoothing and then\nthresholding) on various images, showing that our models are effective in image\nsegmentation and robust with respect to impulsive noises.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 09:35:44 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 07:58:22 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 06:42:27 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 01:22:36 GMT"}, {"version": "v5", "created": "Wed, 31 Mar 2021 18:48:59 GMT"}, {"version": "v6", "created": "Sun, 18 Jul 2021 02:07:57 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Bui", "Kevin", ""], ["Park", "Fredrick", ""], ["Lou", "Yifei", ""], ["Xin", "Jack", ""]]}, {"id": "2005.04408", "submitter": "Xu Yao", "authors": "Xu Yao, Gilles Puy, Patrick P\\'erez", "title": "Photo style transfer with consistency losses", "comments": null, "journal-ref": "In 2019 IEEE International Conference on Image Processing (ICIP)\n  (pp. 2314-2318). IEEE", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of style transfer between two photos and propose a new\nway to preserve photorealism. Using the single pair of photos available as\ninput, we train a pair of deep convolution networks (convnets), each of which\ntransfers the style of one photo to the other. To enforce photorealism, we\nintroduce a content preserving mechanism by combining a cycle-consistency loss\nwith a self-consistency loss. Experimental results show that this method does\nnot suffer from typical artifacts observed in methods working in the same\nsettings. We then further analyze some properties of these trained convnets.\nFirst, we notice that they can be used to stylize other unseen images with same\nknown style. Second, we show that retraining only a small subset of the network\nparameters can be sufficient to adapt these convnets to new styles.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 09:58:06 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Yao", "Xu", ""], ["Puy", "Gilles", ""], ["P\u00e9rez", "Patrick", ""]]}, {"id": "2005.04410", "submitter": "Xu Yao", "authors": "Xu Yao, Gilles Puy, Alasdair Newson, Yann Gousseau, Pierre Hellier", "title": "High Resolution Face Age Editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face age editing has become a crucial task in film post-production, and is\nalso becoming popular for general purpose photography. Recently, adversarial\ntraining has produced some of the most visually impressive results for image\nmanipulation, including the face aging/de-aging task. In spite of considerable\nprogress, current methods often present visual artifacts and can only deal with\nlow-resolution images. In order to achieve aging/de-aging with the high quality\nand robustness necessary for wider use, these problems need to be addressed.\nThis is the goal of the present work. We present an encoder-decoder\narchitecture for face age editing. The core idea of our network is to create\nboth a latent space containing the face identity, and a feature modulation\nlayer corresponding to the age of the individual. We then combine these two\nelements to produce an output image of the person with a desired target age.\nOur architecture is greatly simplified with respect to other approaches, and\nallows for continuous age editing on high resolution images in a single unified\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 09:59:51 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Yao", "Xu", ""], ["Puy", "Gilles", ""], ["Newson", "Alasdair", ""], ["Gousseau", "Yann", ""], ["Hellier", "Pierre", ""]]}, {"id": "2005.04414", "submitter": "Jun He", "authors": "Jun He, Richang Hong, Xueliang Liu, Mingliang Xu, Zhengjun Zha and\n  Meng Wang", "title": "Memory-Augmented Relation Network for Few-Shot Learning", "comments": "To be submitted to ACM Multimedia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric-based few-shot learning methods concentrate on learning transferable\nfeature embedding that generalizes well from seen categories to unseen\ncategories under the supervision of limited number of labelled instances.\nHowever, most of them treat each individual instance in the working context\nseparately without considering its relationships with the others. In this work,\nwe investigate a new metric-learning method, Memory-Augmented Relation Network\n(MRN), to explicitly exploit these relationships. In particular, for an\ninstance, we choose the samples that are visually similar from the working\ncontext, and perform weighted information propagation to attentively aggregate\nhelpful information from the chosen ones to enhance its representation. In MRN,\nwe also formulate the distance metric as a learnable relation module which\nlearns to compare for similarity measurement, and augment the working context\nwith memory slots, both contributing to its generality. We empirically\ndemonstrate that MRN yields significant improvement over its ancestor and\nachieves competitive or even better performance when compared with other\nfew-shot learning approaches on the two major benchmark datasets, i.e.\nminiImagenet and tieredImagenet.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 10:09:13 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 03:25:36 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["He", "Jun", ""], ["Hong", "Richang", ""], ["Liu", "Xueliang", ""], ["Xu", "Mingliang", ""], ["Zha", "Zhengjun", ""], ["Wang", "Meng", ""]]}, {"id": "2005.04425", "submitter": "Kiyoharu Aizawa Dr. Prof.", "authors": "Kiyoharu Aizawa, Azuma Fujimoto, Atsushi Otsubo, Toru Ogawa, Yusuke\n  Matsui, Koki Tsubota, Hikaru Ikuta", "title": "Building a Manga Dataset \"Manga109\" with Annotations for Multimedia\n  Applications", "comments": "10 pages, 8 figures", "journal-ref": "IEEE MultiMedia 2020", "doi": "10.1109/MMUL.2020.2987895", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manga, or comics, which are a type of multimodal artwork, have been left\nbehind in the recent trend of deep learning applications because of the lack of\na proper dataset. Hence, we built Manga109, a dataset consisting of a variety\nof 109 Japanese comic books (94 authors and 21,142 pages) and made it publicly\navailable by obtaining author permissions for academic use. We carefully\nannotated the frames, speech texts, character faces, and character bodies; the\ntotal number of annotations exceeds 500k. This dataset provides numerous manga\nimages and annotations, which will be beneficial for use in machine learning\nalgorithms and their evaluation. In addition to academic use, we obtained\nfurther permission for a subset of the dataset for industrial use. In this\narticle, we describe the details of the dataset and present a few examples of\nmultimedia processing applications (detection, retrieval, and generation) that\napply existing deep learning methods and are made possible by the dataset.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 12:26:58 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 14:07:55 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Aizawa", "Kiyoharu", ""], ["Fujimoto", "Azuma", ""], ["Otsubo", "Atsushi", ""], ["Ogawa", "Toru", ""], ["Matsui", "Yusuke", ""], ["Tsubota", "Koki", ""], ["Ikuta", "Hikaru", ""]]}, {"id": "2005.04437", "submitter": "Sravan Mylavarapu", "authors": "Sravan Mylavarapu, Mahtab Sandhu, Priyesh Vijayan, K Madhava Krishna,\n  Balaraman Ravindran, Anoop Namboodiri", "title": "Understanding Dynamic Scenes using Graph Convolution Networks", "comments": "To appear at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel Multi-Relational Graph Convolutional Network (MRGCN) based\nframework to model on-road vehicle behaviors from a sequence of temporally\nordered frames as grabbed by a moving monocular camera. The input to MRGCN is a\nmulti-relational graph where the graph's nodes represent the active and passive\nagents/objects in the scene, and the bidirectional edges that connect every\npair of nodes are encodings of their Spatio-temporal relations. We show that\nthis proposed explicit encoding and usage of an intermediate spatio-temporal\ninteraction graph to be well suited for our tasks over learning end-end\ndirectly on a set of temporally ordered spatial relations. We also propose an\nattention mechanism for MRGCNs that conditioned on the scene dynamically scores\nthe importance of information from different interaction types. The proposed\nframework achieves significant performance gain over prior methods on\nvehicle-behavior classification tasks on four datasets. We also show a seamless\ntransfer of learning to multiple datasets without resorting to fine-tuning.\nSuch behavior prediction methods find immediate relevance in a variety of\nnavigation tasks such as behavior planning, state estimation, and applications\nrelating to the detection of traffic violations over videos.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 13:05:06 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 06:11:03 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 05:33:55 GMT"}, {"version": "v4", "created": "Thu, 13 Aug 2020 11:20:01 GMT"}, {"version": "v5", "created": "Fri, 14 Aug 2020 15:56:51 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Mylavarapu", "Sravan", ""], ["Sandhu", "Mahtab", ""], ["Vijayan", "Priyesh", ""], ["Krishna", "K Madhava", ""], ["Ravindran", "Balaraman", ""], ["Namboodiri", "Anoop", ""]]}, {"id": "2005.04463", "submitter": "Cunyuan Gao", "authors": "Cunyuan Gao, Yi Hu, Yi Zhang, Rui Yao, Yong Zhou, Jiaqi Zhao", "title": "Vehicle Re-Identification Based on Complementary Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present our solution to the vehicle re-identification\n(vehicle Re-ID) track in AI City Challenge 2020 (AIC2020). The purpose of\nvehicle Re-ID is to retrieve the same vehicle appeared across multiple cameras,\nand it could make a great contribution to the Intelligent Traffic System(ITS)\nand smart city. Due to the vehicle's orientation, lighting and inter-class\nsimilarity, it is difficult to achieve robust and discriminative representation\nfeature. For the vehicle Re-ID track in AIC2020, our method is to fuse features\nextracted from different networks in order to take advantages of these networks\nand achieve complementary features. For each single model, several methods such\nas multi-loss, filter grafting, semi-supervised are used to increase the\nrepresentation ability as better as possible. Top performance in City-Scale\nMulti-Camera Vehicle Re-Identification demonstrated the advantage of our\nmethods, and we got 5-th place in the vehicle Re-ID track of AIC2020. The codes\nare available at https://github.com/gggcy/AIC2020_ReID.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 15:24:51 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Gao", "Cunyuan", ""], ["Hu", "Yi", ""], ["Zhang", "Yi", ""], ["Yao", "Rui", ""], ["Zhou", "Yong", ""], ["Zhao", "Jiaqi", ""]]}, {"id": "2005.04473", "submitter": "Fabricio Breve", "authors": "Fabricio Breve, Carlos Norberto Fischer", "title": "Visually Impaired Aid using Convolutional Neural Networks, Transfer\n  Learning, and Particle Competition and Cooperation", "comments": "BREVE, Fabricio Aparecido; FISCHER, Carlos Norberto. Visually\n  Impaired Aid using Convolutional Neural Networks, Transfer Learning, and\n  Particle Competition and Cooperation In: 2020 International Joint Conference\n  on Neural Networks (IJCNN 2020), 2020, Glasgow, UK. Proceedings of 2020\n  International Joint Conference on Neural Networks (IJCNN 2020), 2020.\n  (accepted for publication)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigation and mobility are some of the major problems faced by visually\nimpaired people in their daily lives. Advances in computer vision led to the\nproposal of some navigation systems. However, most of them require expensive\nand/or heavy hardware. In this paper we propose the use of convolutional neural\nnetworks (CNN), transfer learning, and semi-supervised learning (SSL) to build\na framework aimed at the visually impaired aid. It has low computational costs\nand, therefore, may be implemented on current smartphones, without relying on\nany additional equipment. The smartphone camera can be used to automatically\ntake pictures of the path ahead. Then, they will be immediately classified,\nproviding almost instantaneous feedback to the user. We also propose a dataset\nto train the classifiers, including indoor and outdoor situations with\ndifferent types of light, floor, and obstacles. Many different CNN\narchitectures are evaluated as feature extractors and classifiers, by\nfine-tuning weights pre-trained on a much larger dataset. The graph-based SSL\nmethod, known as particle competition and cooperation, is also used for\nclassification, allowing feedback from the user to be incorporated without\nretraining the underlying network. 92\\% and 80\\% classification accuracy is\nachieved in the proposed dataset in the best supervised and SSL scenarios,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 16:11:48 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Breve", "Fabricio", ""], ["Fischer", "Carlos Norberto", ""]]}, {"id": "2005.04490", "submitter": "Huabin Liu", "authors": "Weiyao Lin, Huabin Liu, Shizhan Liu, Yuxi Li, Rui Qian, Tao Wang, Ning\n  Xu, Hongkai Xiong, Guo-Jun Qi, Nicu Sebe", "title": "Human in Events: A Large-Scale Benchmark for Human-centric Video\n  Analysis in Complex Events", "comments": "Dataset for Large-scale Human-centric Video Analysis in Complex\n  Events (http://humaninevents.org)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the development of modern smart cities, human-centric video\nanalysis has been encountering the challenge of analyzing diverse and complex\nevents in real scenes. A complex event relates to dense crowds, anomalous, or\ncollective behaviors. However, limited by the scale of existing video datasets,\nfew human analysis approaches have reported their performance on such complex\nevents. To this end, we present a new large-scale dataset, named\nHuman-in-Events or HiEve (Human-centric video analysis in complex Events), for\nthe understanding of human motions, poses, and actions in a variety of\nrealistic events, especially in crowd and complex events. It contains a record\nnumber of poses (>1M), the largest number of action instances (>56k) under\ncomplex events, as well as one of the largest numbers of trajectories lasting\nfor longer time (with an average trajectory length of >480 frames). Based on\nthis dataset, we present an enhanced pose estimation baseline by utilizing the\npotential of action information to guide the learning of more powerful 2D pose\nfeatures. We demonstrate that the proposed method is able to boost the\nperformance of existing pose estimation pipelines on our HiEve dataset.\nFurthermore, we conduct extensive experiments to benchmark recent video\nanalysis approaches together with our baseline methods, demonstrating that\nHiEve is a challenging dataset for human-centric video analysis. We expect that\nthe dataset will advance the development of cutting-edge techniques in\nhuman-centric analysis and the understanding of complex events. The dataset is\navailable at http://humaninevents.org\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 18:24:52 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 15:44:19 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 12:47:25 GMT"}, {"version": "v4", "created": "Thu, 11 Mar 2021 02:50:11 GMT"}, {"version": "v5", "created": "Sun, 14 Mar 2021 06:24:52 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Lin", "Weiyao", ""], ["Liu", "Huabin", ""], ["Liu", "Shizhan", ""], ["Li", "Yuxi", ""], ["Qian", "Rui", ""], ["Wang", "Tao", ""], ["Xu", "Ning", ""], ["Xiong", "Hongkai", ""], ["Qi", "Guo-Jun", ""], ["Sebe", "Nicu", ""]]}, {"id": "2005.04492", "submitter": "Omkar Gune", "authors": "Omkar Gune, Mainak Pal, Preeti Mukherjee, Biplab Banerjee and Subhasis\n  Chaudhuri", "title": "Generative Model-driven Structure Aligning Discriminative Embeddings for\n  Transductive Zero-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot Learning (ZSL) is a transfer learning technique which aims at\ntransferring knowledge from seen classes to unseen classes. This knowledge\ntransfer is possible because of underlying semantic space which is common to\nseen and unseen classes. Most existing approaches learn a projection function\nusing labelled seen class data which maps visual data to semantic data. In this\nwork, we propose a shallow but effective neural network-based model for\nlearning such a projection function which aligns the visual and semantic data\nin the latent space while simultaneously making the latent space embeddings\ndiscriminative. As the above projection function is learned using the seen\nclass data, the so-called projection domain shift exists. We propose a\ntransductive approach to reduce the effect of domain shift, where we utilize\nunlabeled visual data from unseen classes to generate corresponding semantic\nfeatures for unseen class visual samples. While these semantic features are\ninitially generated using a conditional variational auto-encoder, they are used\nalong with the seen class data to improve the projection function. We\nexperiment on both inductive and transductive setting of ZSL and generalized\nZSL and show superior performance on standard benchmark datasets AWA1, AWA2,\nCUB, SUN, FLO, and APY. We also show the efficacy of our model in the case of\nextremely less labelled data regime on different datasets in the context of\nZSL.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 18:48:20 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Gune", "Omkar", ""], ["Pal", "Mainak", ""], ["Mukherjee", "Preeti", ""], ["Banerjee", "Biplab", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2005.04541", "submitter": "Miaohua Zhang", "authors": "Miaohua Zhang, Yongsheng Gao, Changming Sun, Michael Blumenstein", "title": "A Robust Matching Pursuit Algorithm Using Information Theoretic Learning", "comments": "Accepted by \"Pattern Recognition\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current orthogonal matching pursuit (OMP) algorithms calculate the\ncorrelation between two vectors using the inner product operation and minimize\nthe mean square error, which are both suboptimal when there are non-Gaussian\nnoises or outliers in the observation data. To overcome these problems, a new\nOMP algorithm is developed based on the information theoretic learning (ITL),\nwhich is built on the following new techniques: (1) an ITL-based correlation\n(ITL-Correlation) is developed as a new similarity measure which can better\nexploit higher-order statistics of the data, and is robust against many\ndifferent types of noise and outliers in a sparse representation framework; (2)\na non-second order statistic measurement and minimization method is developed\nto improve the robustness of OMP by overcoming the limitation of Gaussianity\ninherent in cost function based on second-order moments. The experimental\nresults on both simulated and real-world data consistently demonstrate the\nsuperiority of the proposed OMP algorithm in data recovery, image\nreconstruction, and classification.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 01:36:00 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Zhang", "Miaohua", ""], ["Gao", "Yongsheng", ""], ["Sun", "Changming", ""], ["Blumenstein", "Michael", ""]]}, {"id": "2005.04551", "submitter": "Yihui He", "authors": "Yihui He, Rui Yan, Katerina Fragkiadaki, Shoou-I Yu", "title": "Epipolar Transformers", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach to localize 3D human joints in a synchronized and\ncalibrated multi-view setup consists of two-steps: (1) apply a 2D detector\nseparately on each view to localize joints in 2D, and (2) perform robust\ntriangulation on 2D detections from each view to acquire the 3D joint\nlocations. However, in step 1, the 2D detector is limited to solving\nchallenging cases which could potentially be better resolved in 3D, such as\nocclusions and oblique viewing angles, purely in 2D without leveraging any 3D\ninformation. Therefore, we propose the differentiable \"epipolar transformer\",\nwhich enables the 2D detector to leverage 3D-aware features to improve 2D pose\nestimation. The intuition is: given a 2D location p in the current view, we\nwould like to first find its corresponding point p' in a neighboring view, and\nthen combine the features at p' with the features at p, thus leading to a\n3D-aware feature at p. Inspired by stereo matching, the epipolar transformer\nleverages epipolar constraints and feature matching to approximate the features\nat p'. Experiments on InterHand and Human3.6M show that our approach has\nconsistent improvements over the baselines. Specifically, in the condition\nwhere no external data is used, our Human3.6M model trained with ResNet-50\nbackbone and image size 256 x 256 outperforms state-of-the-art by 4.23 mm and\nachieves MPJPE 26.9 mm.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 02:22:54 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["He", "Yihui", ""], ["Yan", "Rui", ""], ["Fragkiadaki", "Katerina", ""], ["Yu", "Shoou-I", ""]]}, {"id": "2005.04559", "submitter": "Mahdi Biparva", "authors": "Mahdi Biparva, John Tsotsos", "title": "Compact Neural Representation Using Attentive Network Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have evolved to become power demanding and consequently\ndifficult to apply to small-size mobile platforms. Network parameter reduction\nmethods have been introduced to systematically deal with the computational and\nmemory complexity of deep networks. We propose to examine the ability of\nattentive connection pruning to deal with redundancy reduction in neural\nnetworks as a contribution to the reduction of computational demand. In this\nwork, we describe a Top-Down attention mechanism that is added to a Bottom-Up\nfeedforward network to select important connections and subsequently prune\nredundant ones at all parametric layers. Our method not only introduces a novel\nhierarchical selection mechanism as the basis of pruning but also remains\ncompetitive with previous baseline methods in the experimental evaluation. We\nconduct experiments using different network architectures on popular benchmark\ndatasets to show high compression ratio is achievable with negligible loss of\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 03:20:01 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Biparva", "Mahdi", ""], ["Tsotsos", "John", ""]]}, {"id": "2005.04563", "submitter": "Omobayode Fagbohungbe", "authors": "Omobayode Fagbohungbe, Sheikh Rufsan Reza, Xishuang Dong, Lijun Qian", "title": "Efficient Privacy Preserving Edge Computing Framework for Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 03:36:32 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Fagbohungbe", "Omobayode", ""], ["Reza", "Sheikh Rufsan", ""], ["Dong", "Xishuang", ""], ["Qian", "Lijun", ""]]}, {"id": "2005.04564", "submitter": "Xianxu Hou", "authors": "Xianxu Hou, Jingxin Liu, Bolei Xu, Xiaolong Wang, Bozhi Liu, Guoping\n  Qiu", "title": "Class-Aware Domain Adaptation for Improving Adversarial Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have demonstrated convolutional neural networks are vulnerable\nto adversarial examples, i.e., inputs to machine learning models that an\nattacker has intentionally designed to cause the models to make a mistake. To\nimprove the adversarial robustness of neural networks, adversarial training has\nbeen proposed to train networks by injecting adversarial examples into the\ntraining data. However, adversarial training could overfit to a specific type\nof adversarial attack and also lead to standard accuracy drop on clean images.\nTo this end, we propose a novel Class-Aware Domain Adaptation (CADA) method for\nadversarial defense without directly applying adversarial training.\nSpecifically, we propose to learn domain-invariant features for adversarial\nexamples and clean images via a domain discriminator. Furthermore, we introduce\na class-aware component into the discriminator to increase the discriminative\npower of the network for adversarial examples. We evaluate our newly proposed\napproach using multiple benchmark datasets. The results demonstrate that our\nmethod can significantly improve the state-of-the-art of adversarial robustness\nfor various attacks and maintain high performances on clean images.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 03:45:19 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Hou", "Xianxu", ""], ["Liu", "Jingxin", ""], ["Xu", "Bolei", ""], ["Wang", "Xiaolong", ""], ["Liu", "Bozhi", ""], ["Qiu", "Guoping", ""]]}, {"id": "2005.04567", "submitter": "Qin Li", "authors": "Qin Li, Huachun Tan, Xizhu Jiang, Yuankai Wu, Linhui Ye", "title": "Non-recurrent Traffic Congestion Detection with a Coupled Scalable\n  Bayesian Robust Tensor Factorization Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-recurrent traffic congestion (NRTC) usually brings unexpected delays to\ncommuters. Hence, it is critical to accurately detect and recognize the NRTC in\na real-time manner. The advancement of road traffic detectors and loop\ndetectors provides researchers with a large-scale multivariable\ntemporal-spatial traffic data, which allows the deep research on NRTC to be\nconducted. However, it remains a challenging task to construct an analytical\nframework through which the natural spatial-temporal structural properties of\nmultivariable traffic information can be effectively represented and exploited\nto better understand and detect NRTC. In this paper, we present a novel\nanalytical training-free framework based on coupled scalable Bayesian robust\ntensor factorization (Coupled SBRTF). The framework can couple multivariable\ntraffic data including traffic flow, road speed, and occupancy through sharing\na similar or the same sparse structure. And, it naturally captures the\nhigh-dimensional spatial-temporal structural properties of traffic data by\ntensor factorization. With its entries revealing the distribution and magnitude\nof NRTC, the shared sparse structure of the framework compasses sufficiently\nabundant information about NRTC. While the low-rank part of the framework,\nexpresses the distribution of general expected traffic condition as an\nauxiliary product. Experimental results on real-world traffic data show that\nthe proposed method outperforms coupled Bayesian robust principal component\nanalysis (coupled BRPCA), the rank sparsity tensor decomposition (RSTD), and\nstandard normal deviates (SND) in detecting NRTC. The proposed method performs\neven better when only traffic data in weekdays are utilized, and hence can\nprovide more precise estimation of NRTC for daily commuters.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 03:58:18 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Li", "Qin", ""], ["Tan", "Huachun", ""], ["Jiang", "Xizhu", ""], ["Wu", "Yuankai", ""], ["Ye", "Linhui", ""]]}, {"id": "2005.04573", "submitter": "Li Zhang", "authors": "Li Zhang and Mingliang Wang and Mingxia Liu and Daoqiang Zhang", "title": "A Survey on Deep Learning for Neuroimaging-based Brain Disorder Analysis", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been recently used for the analysis of neuroimages, such as\nstructural magnetic resonance imaging (MRI), functional MRI, and positron\nemission tomography (PET), and has achieved significant performance\nimprovements over traditional machine learning in computer-aided diagnosis of\nbrain disorders. This paper reviews the applications of deep learning methods\nfor neuroimaging-based brain disorder analysis. We first provide a\ncomprehensive overview of deep learning techniques and popular network\narchitectures, by introducing various types of deep neural networks and recent\ndevelopments. We then review deep learning methods for computer-aided analysis\nof four typical brain disorders, including Alzheimer's disease, Parkinson's\ndisease, Autism spectrum disorder, and Schizophrenia, where the first two\ndiseases are neurodegenerative disorders and the last two are\nneurodevelopmental and psychiatric disorders, respectively. More importantly,\nwe discuss the limitations of existing studies and present possible future\ndirections.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 04:20:50 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Zhang", "Li", ""], ["Wang", "Mingliang", ""], ["Liu", "Mingxia", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "2005.04580", "submitter": "Feifan Lv", "authors": "Feifan Lv, Yinqiang Zheng, Yicheng Li, Feng Lu", "title": "An Integrated Enhancement Solution for 24-hour Colorful Imaging", "comments": "AAAI 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current industry practice for 24-hour outdoor imaging is to use a silicon\ncamera supplemented with near-infrared (NIR) illumination. This will result in\ncolor images with poor contrast at daytime and absence of chrominance at\nnighttime. For this dilemma, all existing solutions try to capture RGB and NIR\nimages separately. However, they need additional hardware support and suffer\nfrom various drawbacks, including short service life, high price, specific\nusage scenario, etc. In this paper, we propose a novel and integrated\nenhancement solution that produces clear color images, whether at abundant\nsunlight daytime or extremely low-light nighttime. Our key idea is to separate\nthe VIS and NIR information from mixed signals, and enhance the VIS signal\nadaptively with the NIR signal as assistance. To this end, we build an optical\nsystem to collect a new VIS-NIR-MIX dataset and present a physically meaningful\nimage processing algorithm based on CNN. Extensive experiments show outstanding\nresults, which demonstrate the effectiveness of our solution.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 05:11:34 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Lv", "Feifan", ""], ["Zheng", "Yinqiang", ""], ["Li", "Yicheng", ""], ["Lu", "Feng", ""]]}, {"id": "2005.04596", "submitter": "Ritam Guha Mr.", "authors": "Ritam Guha, Manosij Ghosh, Pawan Kumar Singh, Ram Sarkar, Mita\n  Nasipuri", "title": "A Hybrid Swarm and Gravitation based feature selection algorithm for\n  Handwritten Indic Script Classification problem", "comments": "37 pages, 22 figures, submitted to Multimedia Tools and Applications,\n  Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In any multi-script environment, handwritten script classification is of\nparamount importance before the document images are fed to their respective\nOptical Character Recognition (OCR) engines. Over the years, this complex\npattern classification problem has been solved by researchers proposing various\nfeature vectors mostly having large dimension, thereby increasing the\ncomputation complexity of the whole classification model. Feature Selection\n(FS) can serve as an intermediate step to reduce the size of the feature\nvectors by restricting them only to the essential and relevant features. In our\npaper, we have addressed this issue by introducing a new FS algorithm, called\nHybrid Swarm and Gravitation based FS (HSGFS). This algorithm is made to run on\n3 feature vectors introduced in the literature recently - Distance-Hough\nTransform (DHT), Histogram of Oriented Gradients (HOG) and Modified log-Gabor\n(MLG) filter Transform. Three state-of-the-art classifiers namely, Multi-Layer\nPerceptron (MLP), K-Nearest Neighbour (KNN) and Support Vector Machine (SVM)\nare used for the handwritten script classification. Handwritten datasets,\nprepared at block, text-line and word level, consisting of officially\nrecognized 12 Indic scripts are used for the evaluation of our method. An\naverage improvement in the range of 2-5 % is achieved in the classification\naccuracies by utilizing only about 75-80 % of the original feature vectors on\nall three datasets. The proposed methodology also shows better performance when\ncompared to some popularly used FS models.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 07:27:55 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Guha", "Ritam", ""], ["Ghosh", "Manosij", ""], ["Singh", "Pawan Kumar", ""], ["Sarkar", "Ram", ""], ["Nasipuri", "Mita", ""]]}, {"id": "2005.04597", "submitter": "Ad\\'elie Garin", "authors": "Ad\\'elie Garin, Teresa Heiss, Kelly Maggs, Bea Bleile, Vanessa Robins", "title": "Duality in Persistent Homology of Images", "comments": "This is an extended abstract for the SoCG Young Researchers Forum\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the relationship between the persistent homology barcodes of two\ndual filtered CW complexes. Applied to greyscale digital images, we obtain an\nalgorithm to convert barcodes between the two different (dual) topological\nmodels of pixel connectivity.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 07:29:24 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 07:16:26 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Garin", "Ad\u00e9lie", ""], ["Heiss", "Teresa", ""], ["Maggs", "Kelly", ""], ["Bleile", "Bea", ""], ["Robins", "Vanessa", ""]]}, {"id": "2005.04605", "submitter": "Miaohua Zhang", "authors": "Miaohua Zhang, Yongsheng Gao, Changming Sun, Michael Blumenstein", "title": "Robust Tensor Decomposition for Image Representation Based on\n  Generalized Correntropy", "comments": "13 pages", "journal-ref": null, "doi": "10.1109/TIP.2020.3033151", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional tensor decomposition methods, e.g., two dimensional principal\ncomponent analysis and two dimensional singular value decomposition, that\nminimize mean square errors, are sensitive to outliers. To overcome this\nproblem, in this paper we propose a new robust tensor decomposition method\nusing generalized correntropy criterion (Corr-Tensor). A Lagrange multiplier\nmethod is used to effectively optimize the generalized correntropy objective\nfunction in an iterative manner. The Corr-Tensor can effectively improve the\nrobustness of tensor decomposition with the existence of outliers without\nintroducing any extra computational cost. Experimental results demonstrated\nthat the proposed method significantly reduces the reconstruction error on face\nreconstruction and improves the accuracies on handwritten digit recognition and\nfacial image clustering.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 08:46:52 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Zhang", "Miaohua", ""], ["Gao", "Yongsheng", ""], ["Sun", "Changming", ""], ["Blumenstein", "Michael", ""]]}, {"id": "2005.04613", "submitter": "Brojeshwar Bhowmick", "authors": "Vignesh Prasad, Dipanjan Das, Brojeshwar Bhowmick", "title": "Variational Clustering: Leveraging Variational Autoencoders for Image\n  Clustering", "comments": null, "journal-ref": "IJCNN 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have shown their ability to learn strong\nfeature representations for images. The task of image clustering naturally\nrequires good feature representations to capture the distribution of the data\nand subsequently differentiate data points from one another. Often these two\naspects are dealt with independently and thus traditional feature learning\nalone does not suffice in partitioning the data meaningfully. Variational\nAutoencoders (VAEs) naturally lend themselves to learning data distributions in\na latent space. Since we wish to efficiently discriminate between different\nclusters in the data, we propose a method based on VAEs where we use a Gaussian\nMixture prior to help cluster the images accurately. We jointly learn the\nparameters of both the prior and the posterior distributions. Our method\nrepresents a true Gaussian Mixture VAE. This way, our method simultaneously\nlearns a prior that captures the latent distribution of the images and a\nposterior to help discriminate well between data points. We also propose a\nnovel reparametrization of the latent space consisting of a mixture of discrete\nand continuous variables. One key takeaway is that our method generalizes\nbetter across different datasets without using any pre-training or learnt\nmodels, unlike existing methods, allowing it to be trained from scratch in an\nend-to-end manner. We verify our efficacy and generalizability experimentally\nby achieving state-of-the-art results among unsupervised methods on a variety\nof datasets. To the best of our knowledge, we are the first to pursue image\nclustering using VAEs in a purely unsupervised manner on real image datasets.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 09:34:48 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Prasad", "Vignesh", ""], ["Das", "Dipanjan", ""], ["Bhowmick", "Brojeshwar", ""]]}, {"id": "2005.04618", "submitter": "Brojeshwar Bhowmick", "authors": "Puneet Gupta, Brojeshwar Bhowmick, Arpan Pal", "title": "MOMBAT: Heart Rate Monitoring from Face Video using Pulse Modeling and\n  Bayesian Tracking", "comments": null, "journal-ref": "Computers in Biology and Medicine, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-invasive yet inexpensive method for heart rate (HR) monitoring is of\ngreat importance in many real-world applications including healthcare,\npsychology understanding, affective computing and biometrics. Face videos are\ncurrently utilized for such HR monitoring, but unfortunately this can lead to\nerrors due to the noise introduced by facial expressions, out-of-plane\nmovements, camera parameters (like focus change) and environmental factors. We\nalleviate these issues by proposing a novel face video based HR monitoring\nmethod MOMBAT, that is, MOnitoring using Modeling and BAyesian Tracking. We\nutilize out-of-plane face movements to define a novel quality estimation\nmechanism. Subsequently, we introduce a Fourier basis based modeling to\nreconstruct the cardiovascular pulse signal at the locations containing the\npoor quality, that is, the locations affected by out-of-plane face movements.\nFurthermore, we design a Bayesian decision theory based HR tracking mechanism\nto rectify the spurious HR estimates. Experimental results reveal that our\nproposed method, MOMBAT outperforms state-of-the-art HR monitoring methods and\nperforms HR monitoring with an average absolute error of 1.329 beats per minute\nand the Pearson correlation between estimated and actual heart rate is 0.9746.\nMoreover, it demonstrates that HR monitoring is significantly\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 09:41:16 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Gupta", "Puneet", ""], ["Bhowmick", "Brojeshwar", ""], ["Pal", "Arpan", ""]]}, {"id": "2005.04619", "submitter": "Miaohua Zhang", "authors": "Miaohua Zhang, Yongsheng Gao, and Jun Zhou", "title": "A Unified Weight Learning and Low-Rank Regression Model for Robust\n  Complex Error Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important problems in regression-based error model is\nmodeling the complex representation error caused by various corruptions and\nenvironment changes in images. For example, in robust face recognition, images\nare often affected by varying types and levels of corruptions, such as random\npixel corruptions, block occlusions, or disguises. However, existing works are\nnot robust enough to solve this problem due to they cannot model the complex\ncorrupted errors very well. In this paper, we address this problem by a unified\nsparse weight learning and low-rank approximation regression model, which\nenables the random noises and contiguous occlusions in images to be treated\nsimultaneously. For the random noise, we define a generalized correntropy (GC)\nfunction to match the error distribution. For the structured error caused by\nocclusions or disguises, we propose a GC function based rank approximation to\nmeasure the rank of error matrices. Since the proposed objective function is\nnon-convex, an effective iterative optimization algorithm is developed to\nachieve the optimal weight learning and low-rank approximation. Extensive\nexperimental results on three public face databases show that the proposed\nmodel can fit the error distribution and structure very well, thus obtain\nbetter recognition accuracies in comparison with the existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 09:50:14 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 00:47:40 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 01:45:19 GMT"}, {"version": "v4", "created": "Wed, 23 Sep 2020 00:51:20 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Zhang", "Miaohua", ""], ["Gao", "Yongsheng", ""], ["Zhou", "Jun", ""]]}, {"id": "2005.04621", "submitter": "Mateusz Ochal", "authors": "Mateusz Ochal, Jose Vazquez, Yvan Petillot, Sen Wang", "title": "A Comparison of Few-Shot Learning Methods for Underwater Optical and\n  Sonar Image Classification", "comments": "Accepted to IEEE Global OCEANS 2020: Singapore - U.S. Gulf Coast", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks generally perform well in underwater\nobject recognition tasks on both optical and sonar images. Many such methods\nrequire hundreds, if not thousands, of images per class to generalize well to\nunseen examples. However, obtaining and labeling sufficiently large volumes of\ndata can be relatively costly and time-consuming, especially when observing\nrare objects or performing real-time operations. Few-Shot Learning (FSL)\nefforts have produced many promising methods to deal with low data\navailability. However, little attention has been given in the underwater\ndomain, where the style of images poses additional challenges for object\nrecognition algorithms. To the best of our knowledge, this is the first paper\nto evaluate and compare several supervised and semi-supervised Few-Shot\nLearning (FSL) methods using underwater optical and side-scan sonar imagery.\nOur results show that FSL methods offer a significant advantage over the\ntraditional transfer learning methods that fine-tune pre-trained models. We\nhope that our work will help apply FSL to autonomous underwater systems and\nexpand their learning capabilities.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 10:11:16 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 15:18:43 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ochal", "Mateusz", ""], ["Vazquez", "Jose", ""], ["Petillot", "Yvan", ""], ["Wang", "Sen", ""]]}, {"id": "2005.04623", "submitter": "Mateusz Michalkiewicz", "authors": "Mateusz Michalkiewicz, Eugene Belilovsky, Mahsa Baktashmotlagh, Anders\n  Eriksson", "title": "A Simple and Scalable Shape Representation for 3D Reconstruction", "comments": "9 pages plus 3 pages of references. 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning applied to the reconstruction of 3D shapes has seen growing\ninterest. A popular approach to 3D reconstruction and generation in recent\nyears has been the CNN encoder-decoder model usually applied in voxel space.\nHowever, this often scales very poorly with the resolution limiting the\neffectiveness of these models. Several sophisticated alternatives for decoding\nto 3D shapes have been proposed typically relying on complex deep learning\narchitectures for the decoder model. In this work, we show that this additional\ncomplexity is not necessary, and that we can actually obtain high quality 3D\nreconstruction using a linear decoder, obtained from principal component\nanalysis on the signed distance function (SDF) of the surface. This approach\nallows easily scaling to larger resolutions. We show in multiple experiments\nthat our approach is competitive with state-of-the-art methods. It also allows\nthe decoder to be fine-tuned on the target task using a loss designed\nspecifically for SDF transforms, obtaining further gains.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 10:22:50 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Michalkiewicz", "Mateusz", ""], ["Belilovsky", "Eugene", ""], ["Baktashmotlagh", "Mahsa", ""], ["Eriksson", "Anders", ""]]}, {"id": "2005.04625", "submitter": "Wang Zhu", "authors": "Wang Zhu, Hexiang Hu, Jiacheng Chen, Zhiwei Deng, Vihan Jain, Eugene\n  Ie, Fei Sha", "title": "BabyWalk: Going Farther in Vision-and-Language Navigation by Taking Baby\n  Steps", "comments": "Accepted by ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to follow instructions is of fundamental importance to autonomous\nagents for vision-and-language navigation (VLN). In this paper, we study how an\nagent can navigate long paths when learning from a corpus that consists of\nshorter ones. We show that existing state-of-the-art agents do not generalize\nwell. To this end, we propose BabyWalk, a new VLN agent that is learned to\nnavigate by decomposing long instructions into shorter ones (BabySteps) and\ncompleting them sequentially. A special design memory buffer is used by the\nagent to turn its past experiences into contexts for future steps. The learning\nprocess is composed of two phases. In the first phase, the agent uses imitation\nlearning from demonstration to accomplish BabySteps. In the second phase, the\nagent uses curriculum-based reinforcement learning to maximize rewards on\nnavigation tasks with increasingly longer instructions. We create two new\nbenchmark datasets (of long navigation tasks) and use them in conjunction with\nexisting ones to examine BabyWalk's generalization ability. Empirical results\nshow that BabyWalk achieves state-of-the-art results on several metrics, in\nparticular, is able to follow long instructions better. The codes and the\ndatasets are released on our project page https://github.com/Sha-Lab/babywalk.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 10:46:41 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 22:02:05 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhu", "Wang", ""], ["Hu", "Hexiang", ""], ["Chen", "Jiacheng", ""], ["Deng", "Zhiwei", ""], ["Jain", "Vihan", ""], ["Ie", "Eugene", ""], ["Sha", "Fei", ""]]}, {"id": "2005.04661", "submitter": "Mu Li", "authors": "Mu Li, Kai Zhang, Wangmeng Zuo, Radu Timofte, David Zhang", "title": "Learning Context-Based Non-local Entropy Modeling for Image Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The entropy of the codes usually serves as the rate loss in the recent\nlearned lossy image compression methods. Precise estimation of the\nprobabilistic distribution of the codes plays a vital role in the performance.\nHowever, existing deep learning based entropy modeling methods generally assume\nthe latent codes are statistically independent or depend on some side\ninformation or local context, which fails to take the global similarity within\nthe context into account and thus hinder the accurate entropy estimation. To\naddress this issue, we propose a non-local operation for context modeling by\nemploying the global similarity within the context. Specifically, we first\nintroduce the proxy similarity functions and spatial masks to handle the\nmissing reference problem in context modeling. Then, we combine the local and\nthe global context via a non-local attention block and employ it in masked\nconvolutional networks for entropy modeling. The entropy model is further\nadopted as the rate loss in a joint rate-distortion optimization to guide the\ntraining of the analysis transform and the synthesis transform network in\ntransforming coding framework. Considering that the width of the transforms is\nessential in training low distortion models, we finally produce a U-Net block\nin the transforms to increase the width with manageable memory consumption and\ntime complexity. Experiments on Kodak and Tecnick datasets demonstrate the\nsuperiority of the proposed context-based non-local attention block in entropy\nmodeling and the U-Net block in low distortion compression against the existing\nimage compression standards and recent deep image compression models.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 13:28:18 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Li", "Mu", ""], ["Zhang", "Kai", ""], ["Zuo", "Wangmeng", ""], ["Timofte", "Radu", ""], ["Zhang", "David", ""]]}, {"id": "2005.04668", "submitter": "Yuanjie Shao", "authors": "Yuanjie Shao, Lerenhan Li, Wenqi Ren, Changxin Gao and Nong Sang", "title": "Domain Adaptation for Image Dehazing", "comments": "Accepted by IEEE Conference on Computer Vision and Patten Recognition\n  (CVPR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image dehazing using learning-based methods has achieved state-of-the-art\nperformance in recent years. However, most existing methods train a dehazing\nmodel on synthetic hazy images, which are less able to generalize well to real\nhazy images due to domain shift. To address this issue, we propose a domain\nadaptation paradigm, which consists of an image translation module and two\nimage dehazing modules. Specifically, we first apply a bidirectional\ntranslation network to bridge the gap between the synthetic and real domains by\ntranslating images from one domain to another. And then, we use images before\nand after translation to train the proposed two image dehazing networks with a\nconsistency constraint. In this phase, we incorporate the real hazy image into\nthe dehazing training via exploiting the properties of the clear image (e.g.,\ndark channel prior and image gradient smoothing) to further improve the domain\nadaptivity. By training image translation and dehazing network in an end-to-end\nmanner, we can obtain better effects of both image translation and dehazing.\nExperimental results on both synthetic and real-world images demonstrate that\nour model performs favorably against the state-of-the-art dehazing algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 13:54:56 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Shao", "Yuanjie", ""], ["Li", "Lerenhan", ""], ["Ren", "Wenqi", ""], ["Gao", "Changxin", ""], ["Sang", "Nong", ""]]}, {"id": "2005.04671", "submitter": "Miaohua Zhang", "authors": "Miaohua Zhang, Yongsheng Gao", "title": "A Generalized Kernel Risk Sensitive Loss for Robust Two-Dimensional\n  Singular Value Decomposition", "comments": "Under Consideration by \"Pattern Recognition\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-dimensional singular decomposition (2DSVD) has been widely used for image\nprocessing tasks, such as image reconstruction, classification, and clustering.\nHowever, traditional 2DSVD algorithm is based on the mean square error (MSE)\nloss, which is sensitive to outliers. To overcome this problem, we propose a\nrobust 2DSVD framework based on a generalized kernel risk sensitive loss\n(GKRSL-2DSVD) which is more robust to noise and and outliers. Since the\nproposed objective function is non-convex, a majorization-minimization\nalgorithm is developed to efficiently solve it with guaranteed convergence. The\nproposed framework has inherent properties of processing non-centered data,\nrotational invariant, being easily extended to higher order spaces.\nExperimental results on public databases demonstrate that the performance of\nthe proposed method on different applications significantly outperforms that of\nall the benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 14:02:40 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 04:20:50 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhang", "Miaohua", ""], ["Gao", "Yongsheng", ""]]}, {"id": "2005.04690", "submitter": "Longteng Guo", "authors": "Longteng Guo, Jing Liu, Xinxin Zhu, Xingjian He, Jie Jiang, Hanqing Lu", "title": "Non-Autoregressive Image Captioning with Counterfactuals-Critical\n  Multi-Agent Learning", "comments": "IJCAI 2020 (copyright held by IJCAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most image captioning models are autoregressive, i.e. they generate each word\nby conditioning on previously generated words, which leads to heavy latency\nduring inference. Recently, non-autoregressive decoding has been proposed in\nmachine translation to speed up the inference time by generating all words in\nparallel. Typically, these models use the word-level cross-entropy loss to\noptimize each word independently. However, such a learning process fails to\nconsider the sentence-level consistency, thus resulting in inferior generation\nquality of these non-autoregressive models. In this paper, we propose a\nNon-Autoregressive Image Captioning (NAIC) model with a novel training\nparadigm: Counterfactuals-critical Multi-Agent Learning (CMAL). CMAL formulates\nNAIC as a multi-agent reinforcement learning system where positions in the\ntarget sequence are viewed as agents that learn to cooperatively maximize a\nsentence-level reward. Besides, we propose to utilize massive unlabeled images\nto boost captioning performance. Extensive experiments on MSCOCO image\ncaptioning benchmark show that our NAIC model achieves a performance comparable\nto state-of-the-art autoregressive models, while brings 13.9x decoding speedup.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 15:09:44 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Guo", "Longteng", ""], ["Liu", "Jing", ""], ["Zhu", "Xinxin", ""], ["He", "Xingjian", ""], ["Jiang", "Jie", ""], ["Lu", "Hanqing", ""]]}, {"id": "2005.04697", "submitter": "Jonathan Frawley", "authors": "Jonathan Frawley, Chris G. Willcocks, Maged Habib, Caspar Geenen,\n  David H. Steel and Boguslaw Obara", "title": "Segmentation of Macular Edema Datasets with Small Residual 3D U-Net\n  Architectures", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the application of deep convolutional neural networks\nwith prohibitively small datasets to the problem of macular edema segmentation.\nIn particular, we investigate several different heavily regularized\narchitectures. We find that, contrary to popular belief, neural architectures\nwithin this application setting are able to achieve close to human-level\nperformance on unseen test images without requiring large numbers of training\nexamples. Annotating these 3D datasets is difficult, with multiple criteria\nrequired. It takes an experienced clinician two days to annotate a single 3D\nimage, whereas our trained model achieves similar performance in less than a\nsecond. We found that an approach which uses targeted dataset augmentation,\nalongside architectural simplification with an emphasis on residual design, has\nacceptable generalization performance - despite relying on fewer than 15\ntraining examples.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 15:34:46 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Frawley", "Jonathan", ""], ["Willcocks", "Chris G.", ""], ["Habib", "Maged", ""], ["Geenen", "Caspar", ""], ["Steel", "David H.", ""], ["Obara", "Boguslaw", ""]]}, {"id": "2005.04703", "submitter": "Yuzhi Zhao", "authors": "Yuzhi Zhao, Lai-Man Po, Qiong Yan, Wei Liu, Tingyu Lin", "title": "Hierarchical Regression Network for Spectral Reconstruction from RGB\n  Images", "comments": "1st Place in CVPRW 2020 NTIRE Spectral Reconstruction Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing visual image with a hyperspectral camera has been successfully\napplied to many areas due to its narrow-band imaging technology. Hyperspectral\nreconstruction from RGB images denotes a reverse process of hyperspectral\nimaging by discovering an inverse response function. Current works mainly map\nRGB images directly to corresponding spectrum but do not consider context\ninformation explicitly. Moreover, the use of encoder-decoder pair in current\nalgorithms leads to loss of information. To address these problems, we propose\na 4-level Hierarchical Regression Network (HRNet) with PixelShuffle layer as\ninter-level interaction. Furthermore, we adopt a residual dense block to remove\nartifacts of real world RGB images and a residual global block to build\nattention mechanism for enlarging perceptive field. We evaluate proposed HRNet\nwith other architectures and techniques by participating in NTIRE 2020\nChallenge on Spectral Reconstruction from RGB Images. The HRNet is the winning\nmethod of track 2 - real world images and ranks 3rd on track 1 - clean images.\nPlease visit the project web page\nhttps://github.com/zhaoyuzhi/Hierarchical-Regression-Network-for-Spectral-Reconstruction-from-RGB-Images\nto try our codes and pre-trained models.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 16:06:11 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Zhao", "Yuzhi", ""], ["Po", "Lai-Man", ""], ["Yan", "Qiong", ""], ["Liu", "Wei", ""], ["Lin", "Tingyu", ""]]}, {"id": "2005.04757", "submitter": "Kihyuk Sohn", "authors": "Kihyuk Sohn, Zizhao Zhang, Chun-Liang Li, Han Zhang, Chen-Yu Lee, and\n  Tomas Pfister", "title": "A Simple Semi-Supervised Learning Framework for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning (SSL) has a potential to improve the predictive\nperformance of machine learning models using unlabeled data. Although there has\nbeen remarkable recent progress, the scope of demonstration in SSL has mainly\nbeen on image classification tasks. In this paper, we propose STAC, a simple\nyet effective SSL framework for visual object detection along with a data\naugmentation strategy. STAC deploys highly confident pseudo labels of localized\nobjects from an unlabeled image and updates the model by enforcing consistency\nvia strong augmentations. We propose experimental protocols to evaluate the\nperformance of semi-supervised object detection using MS-COCO and show the\nefficacy of STAC on both MS-COCO and VOC07. On VOC07, STAC improves the\nAP$^{0.5}$ from $76.30$ to $79.08$; on MS-COCO, STAC demonstrates $2{\\times}$\nhigher data efficiency by achieving 24.38 mAP using only 5\\% labeled data than\nsupervised baseline that marks 23.86\\% using 10\\% labeled data. The code is\navailable at https://github.com/google-research/ssl_detection/.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 19:15:51 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 04:12:25 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Sohn", "Kihyuk", ""], ["Zhang", "Zizhao", ""], ["Li", "Chun-Liang", ""], ["Zhang", "Han", ""], ["Lee", "Chen-Yu", ""], ["Pfister", "Tomas", ""]]}, {"id": "2005.04777", "submitter": "Mathias Rothermel", "authors": "Mathias Rothermel, Ke Gong, Dieter Fritsch, Konrad Schindler, Norbert\n  Haala", "title": "Photometric Multi-View Mesh Refinement for High-Resolution Satellite\n  Images", "comments": "Accepted for publication in ISPRS Journal of Photogrammetry and\n  Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modern high-resolution satellite sensors collect optical imagery with ground\nsampling distances (GSDs) of 30-50cm, which has sparked a renewed interest in\nphotogrammetric 3D surface reconstruction from satellite data. State-of-the-art\nreconstruction methods typically generate 2.5D elevation data. Here, we present\nan approach to recover full 3D surface meshes from multi-view satellite\nimagery. The proposed method takes as input a coarse initial mesh and refines\nit by iteratively updating all vertex positions to maximize the\nphoto-consistency between images. Photo-consistency is measured in image space,\nby transferring texture from one image to another via the surface. We derive\nthe equations to propagate changes in texture similarity through the rational\nfunction model (RFM), often also referred to as rational polynomial coefficient\n(RPC) model. Furthermore, we devise a hierarchical scheme to optimize the\nsurface with gradient descent. In experiments with two different datasets, we\nshow that the refinement improves the initial digital elevation models (DEMs)\ngenerated with conventional dense image matching. Moreover, we demonstrate that\nour method is able to reconstruct true 3D geometry, such as facade structures,\nif off-nadir views are available.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 20:37:54 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 20:26:34 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Rothermel", "Mathias", ""], ["Gong", "Ke", ""], ["Fritsch", "Dieter", ""], ["Schindler", "Konrad", ""], ["Haala", "Norbert", ""]]}, {"id": "2005.04790", "submitter": "Douwe Kiela", "authors": "Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet\n  Singh, Pratik Ringshia, Davide Testuggine", "title": "The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new challenge set for multimodal classification,\nfocusing on detecting hate speech in multimodal memes. It is constructed such\nthat unimodal models struggle and only multimodal models can succeed: difficult\nexamples (\"benign confounders\") are added to the dataset to make it hard to\nrely on unimodal signals. The task requires subtle reasoning, yet is\nstraightforward to evaluate as a binary classification problem. We provide\nbaseline performance numbers for unimodal models, as well as for multimodal\nmodels with various degrees of sophistication. We find that state-of-the-art\nmethods perform poorly compared to humans (64.73% vs. 84.7% accuracy),\nillustrating the difficulty of the task and highlighting the challenge that\nthis important problem poses to the community.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 21:31:00 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 18:01:54 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 18:43:54 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kiela", "Douwe", ""], ["Firooz", "Hamed", ""], ["Mohan", "Aravind", ""], ["Goswami", "Vedanuj", ""], ["Singh", "Amanpreet", ""], ["Ringshia", "Pratik", ""], ["Testuggine", "Davide", ""]]}, {"id": "2005.04810", "submitter": "Jingwei Song", "authors": "Jingwei Song and Mitesh Patel", "title": "A closed-form solution to estimate uncertainty in non-rigid structure\n  from motion", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-Definite Programming (SDP) with low-rank prior has been widely applied\nin Non-Rigid Structure from Motion (NRSfM). Based on a low-rank constraint, it\navoids the inherent ambiguity of basis number selection in conventional\nbase-shape or base-trajectory methods. Despite the efficiency in deformable\nshape reconstruction, it remains unclear how to assess the uncertainty of the\nrecovered shape from the SDP process. In this paper, we present a statistical\ninference on the element-wise uncertainty quantification of the estimated\ndeforming 3D shape points in the case of the exact low-rank SDP problem. A\nclosed-form uncertainty quantification method is proposed and tested. Moreover,\nwe extend the exact low-rank uncertainty quantification to the approximate\nlow-rank scenario with a numerical optimal rank selection method, which enables\nsolving practical application in SDP based NRSfM scenario. The proposed method\nprovides an independent module to the SDP method and only requires the\nstatistic information of the input 2D tracked points. Extensive experiments\nprove that the output 3D points have identical normal distribution to the 2D\ntrackings, the proposed method and quantify the uncertainty accurately, and\nsupports that it has desirable effects on routinely SDP low-rank based NRSfM\nsolver.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 23:22:58 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 19:43:30 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 19:16:55 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Song", "Jingwei", ""], ["Patel", "Mitesh", ""]]}, {"id": "2005.04813", "submitter": "Alessio Del Bue", "authors": "Marco Cristani, Alessio Del Bue, Vittorio Murino, Francesco Setti and\n  Alessandro Vinciarelli", "title": "The Visual Social Distancing Problem", "comments": "9 pages, 5 figures. All the authors equally contributed to this\n  manuscript and they are listed by alphabetical order. Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main and most effective measures to contain the recent viral\noutbreak is the maintenance of the so-called Social Distancing (SD). To comply\nwith this constraint, workplaces, public institutions, transports and schools\nwill likely adopt restrictions over the minimum inter-personal distance between\npeople. Given this actual scenario, it is crucial to massively measure the\ncompliance to such physical constraint in our life, in order to figure out the\nreasons of the possible breaks of such distance limitations, and understand if\nthis implies a possible threat given the scene context. All of this, complying\nwith privacy policies and making the measurement acceptable. To this end, we\nintroduce the Visual Social Distancing (VSD) problem, defined as the automatic\nestimation of the inter-personal distance from an image, and the\ncharacterization of the related people aggregations. VSD is pivotal for a\nnon-invasive analysis to whether people comply with the SD restriction, and to\nprovide statistics about the level of safety of specific areas whenever this\nconstraint is violated. We then discuss how VSD relates with previous\nliterature in Social Signal Processing and indicate which existing Computer\nVision methods can be used to manage such problem. We conclude with future\nchallenges related to the effectiveness of VSD systems, ethical implications\nand future application scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 00:04:34 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Cristani", "Marco", ""], ["Del Bue", "Alessio", ""], ["Murino", "Vittorio", ""], ["Setti", "Francesco", ""], ["Vinciarelli", "Alessandro", ""]]}, {"id": "2005.04823", "submitter": "Faisal R. Al-Osaimi", "authors": "Faisal R. Al-Osaimi", "title": "Learning Descriptors Invariance Through Equivalence Relations Within\n  Manifold: A New Approach to Expression Invariant 3D Face Recognition", "comments": "This paper was submitted to pattern recognition in 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unique approach for the dichotomy between useful and\nadverse variations of key-point descriptors, namely the identity and the\nexpression variations in the descriptor (feature) space. The descriptors\nvariations are learned from training examples. Based on the labels of the\ntraining data, the equivalence relations among the descriptors are established.\nBoth types of descriptor variations are represented by a graph embedded in the\ndescriptor manifold. The invariant recognition is then conducted as a graph\nsearch problem. A heuristic graph search algorithm suitable for the recognition\nunder this setup was devised. The proposed approach was tests on the FRGC v2.0,\nthe Bosphorus and the 3D TEC datasets. It has shown to enhance the recognition\nperformance, under expression variations in particular, by considerable\nmargins.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 01:23:39 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Al-Osaimi", "Faisal R.", ""]]}, {"id": "2005.04848", "submitter": "Chang Shu", "authors": "Chang Shu, Xi Chen, Qiwei Xie, Chi Xiao, Hua Han", "title": "Non-iterative Simultaneous Rigid Registration Method for Serial Sections\n  of Biological Tissue", "comments": "appears in IEEE International Symposium on Biomedical Imaging 2018\n  (ISBI 2018)", "journal-ref": "2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI\n  2018), Washington, DC, 2018, pp. 436-440", "doi": "10.1109/ISBI.2018.8363610", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel non-iterative algorithm to simultaneously\nestimate optimal rigid transformation for serial section images, which is a key\ncomponent in volume reconstruction of serial sections of biological tissue. In\norder to avoid error accumulation and propagation caused by current algorithms,\nwe add extra condition that the position of the first and the last section\nimages should remain unchanged. This constrained simultaneous registration\nproblem has not been solved before. Our algorithm method is non-iterative, it\ncan simultaneously compute rigid transformation for a large number of serial\nsection images in a short time. We prove that our algorithm gets optimal\nsolution under ideal condition. And we test our algorithm with synthetic data\nand real data to verify our algorithm's effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 03:44:10 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Shu", "Chang", ""], ["Chen", "Xi", ""], ["Xie", "Qiwei", ""], ["Xiao", "Chi", ""], ["Han", "Hua", ""]]}, {"id": "2005.04854", "submitter": "Geng Zhan", "authors": "Geng Zhan, Dan Xu, Guo Lu, Wei Wu, Chunhua Shen, Wanli Ouyang", "title": "Scope Head for Accurate Localization in Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing anchor-based and anchor-free object detectors in multi-stage or\none-stage pipelines have achieved very promising detection performance.\nHowever, they still encounter the design difficulty in hand-crafted 2D anchor\ndefinition and the learning complexity in 1D direct location regression. To\ntackle these issues, in this paper, we propose a novel detector coined as\nScopeNet, which models anchors of each location as a mutually dependent\nrelationship. This approach quantizes the prediction space and employs a\ncoarse-to-fine strategy for localization. It achieves superior flexibility as\nin the regression based anchor-free methods, while produces more precise\nprediction. Besides, an inherit anchor selection score is learned to indicate\nthe localization quality of the detection result, and we propose to better\nrepresent the confidence of a detection box by combining the\ncategory-classification score and the anchor-selection score. With our concise\nand effective design, the proposed ScopeNet achieves state-of-the-art results\non COCO\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 04:00:09 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 02:07:38 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Zhan", "Geng", ""], ["Xu", "Dan", ""], ["Lu", "Guo", ""], ["Wu", "Wei", ""], ["Shen", "Chunhua", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2005.04884", "submitter": "Linfeng Wang", "authors": "Linfeng Wang, Shu Kong, Zachary Pincus, Charless Fowlkes", "title": "Celeganser: Automated Analysis of Nematode Morphology and Age", "comments": "Computer Vision for Microscopy Image Analysis (CVMI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nematode Caenorhabditis elegans (C. elegans) serves as an important model\norganism in a wide variety of biological studies. In this paper we introduce a\npipeline for automated analysis of C. elegans imagery for the purpose of\nstudying life-span, health-span and the underlying genetic determinants of\naging. Our system detects and segments the worm, and predicts body coordinates\nat each pixel location inside the worm. These coordinates provide dense\ncorrespondence across individual animals to allow for meaningful comparative\nanalysis. We show that a model pre-trained to perform body-coordinate\nregression extracts rich features that can be used to predict the age of\nindividual worms with high accuracy. This lays the ground for future research\nin quantifying the relation between organs' physiologic and biochemical state,\nand individual life/health-span.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 06:57:58 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Wang", "Linfeng", ""], ["Kong", "Shu", ""], ["Pincus", "Zachary", ""], ["Fowlkes", "Charless", ""]]}, {"id": "2005.04886", "submitter": "Yang Song", "authors": "Yi-hong Zhang, Jing Zhang, Yang Song, Chaomin Shen, Guang Yang", "title": "Gleason Score Prediction using Deep Learning in Tissue Microarray Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prostate cancer (PCa) is one of the most common cancers in men around the\nworld. The most accurate method to evaluate lesion levels of PCa is microscopic\ninspection of stained biopsy tissue and estimate the Gleason score of tissue\nmicroarray (TMA) image by expert pathologists. However, it is time-consuming\nfor pathologists to identify the cellular and glandular patterns for Gleason\ngrading in large TMA images. We used Gleason2019 Challenge dataset to build a\nconvolutional neural network (CNN) model to segment TMA images to regions of\ndifferent Gleason grades and predict the Gleason score according to the grading\nsegmentation. We used a pre-trained model of prostate segmentation to increase\nthe accuracy of the Gleason grade segmentation. The model achieved a mean Dice\nof 75.6% on the test cohort and ranked 4th in the Gleason2019 Challenge with a\nscore of 0.778 combined of Cohen's kappa and the f1-score.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 07:00:42 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Zhang", "Yi-hong", ""], ["Zhang", "Jing", ""], ["Song", "Yang", ""], ["Shen", "Chaomin", ""], ["Yang", "Guang", ""]]}, {"id": "2005.04906", "submitter": "Yuta Tokuoka", "authors": "Yuta Tokuoka, Shuji Suzuki, Yohei Sugawara", "title": "An Inductive Transfer Learning Approach using Cycle-consistent\n  Adversarial Domain Adaptation with Application to Brain Tumor Segmentation", "comments": null, "journal-ref": "Proceedings of the 2019 6th International Conference on Biomedical\n  and Bioinformatics Engineering, November 2019, Pages 44-48", "doi": "10.1145/3375923.3375948", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advances in supervised machine learning for medical image\nanalysis applications, the annotated medical image datasets of various domains\nare being shared extensively. Given that the annotation labelling requires\nmedical expertise, such labels should be applied to as many learning tasks as\npossible. However, the multi-modal nature of each annotated image renders it\ndifficult to share the annotation label among diverse tasks. In this work, we\nprovide an inductive transfer learning (ITL) approach to adopt the annotation\nlabel of the source domain datasets to tasks of the target domain datasets\nusing Cycle-GAN based unsupervised domain adaptation (UDA). To evaluate the\napplicability of the ITL approach, we adopted the brain tissue annotation label\non the source domain dataset of Magnetic Resonance Imaging (MRI) images to the\ntask of brain tumor segmentation on the target domain dataset of MRI. The\nresults confirm that the segmentation accuracy of brain tumor segmentation\nimproved significantly. The proposed ITL approach can make significant\ncontribution to the field of medical image analysis, as we develop a\nfundamental tool to improve and promote various tasks using medical images.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 08:01:59 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Tokuoka", "Yuta", ""], ["Suzuki", "Shuji", ""], ["Sugawara", "Yohei", ""]]}, {"id": "2005.04909", "submitter": "David Stap", "authors": "David Stap, Maurits Bleeker, Sarah Ibrahimi, Maartje ter Hoeve", "title": "Conditional Image Generation and Manipulation for User-Specified Content", "comments": "Accepted to the AI for content creation workshop at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Generative Adversarial Networks (GANs) have improved\nsteadily towards generating increasingly impressive real-world images. It is\nuseful to steer the image generation process for purposes such as content\ncreation. This can be done by conditioning the model on additional information.\nHowever, when conditioning on additional information, there still exists a\nlarge set of images that agree with a particular conditioning. This makes it\nunlikely that the generated image is exactly as envisioned by a user, which is\nproblematic for practical content creation scenarios such as generating facial\ncomposites or stock photos. To solve this problem, we propose a single pipeline\nfor text-to-image generation and manipulation. In the first part of our\npipeline we introduce textStyleGAN, a model that is conditioned on text. In the\nsecond part of our pipeline we make use of the pre-trained weights of\ntextStyleGAN to perform semantic facial image manipulation. The approach works\nby finding semantic directions in latent space. We show that this method can be\nused to manipulate facial images for a wide range of attributes. Finally, we\nintroduce the CelebTD-HQ dataset, an extension to CelebA-HQ, consisting of\nfaces and corresponding textual descriptions.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 08:05:00 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Stap", "David", ""], ["Bleeker", "Maurits", ""], ["Ibrahimi", "Sarah", ""], ["ter Hoeve", "Maartje", ""]]}, {"id": "2005.04917", "submitter": "Heikki Arponen Dr", "authors": "Heikki Arponen and Tom E. Bishop", "title": "Learning to hash with semantic similarity metrics and empirical KL\n  divergence", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to hash is an efficient paradigm for exact and approximate nearest\nneighbor search from massive databases. Binary hash codes are typically\nextracted from an image by rounding output features from a CNN, which is\ntrained on a supervised binary similar/ dissimilar task. Drawbacks of this\napproach are: (i) resulting codes do not necessarily capture semantic\nsimilarity of the input data (ii) rounding results in information loss,\nmanifesting as decreased retrieval performance and (iii) Using only class-wise\nsimilarity as a target can lead to trivial solutions, simply encoding\nclassifier outputs rather than learning more intricate relations, which is not\ndetected by most performance metrics. We overcome (i) via a novel loss function\nencouraging the relative hash code distances of learned features to match those\nderived from their targets. We address (ii) via a differentiable estimate of\nthe KL divergence between network outputs and a binary target distribution,\nresulting in minimal information loss when the features are rounded to binary.\nFinally, we resolve (iii) by focusing on a hierarchical precision metric.\nEfficiency of the methods is demonstrated with semantic image retrieval on the\nCIFAR-100, ImageNet and Conceptual Captions datasets, using similarities\ninferred from the WordNet label hierarchy or sentence embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 08:20:26 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Arponen", "Heikki", ""], ["Bishop", "Tom E.", ""]]}, {"id": "2005.04945", "submitter": "Zhiqing Guo", "authors": "Zhiqing Guo, Gaobo Yang, Jiyou Chen, Xingming Sun", "title": "Fake face detection via adaptive manipulation traces extraction network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of face image manipulation (FIM) techniques such as\nFace2Face and Deepfake, more fake face images are spreading over the internet,\nwhich brings serious challenges to public confidence. Face image forgery\ndetection has made considerable progresses in exposing specific FIM, but it is\nstill in scarcity of a robust fake face detector to expose face image forgeries\nunder complex scenarios such as with further compression, blurring, scaling,\netc. Due to the relatively fixed structure, convolutional neural network (CNN)\ntends to learn image content representations. However, CNN should learn subtle\nmanipulation traces for image forensics tasks. Thus, we propose an adaptive\nmanipulation traces extraction network (AMTEN), which serves as pre-processing\nto suppress image content and highlight manipulation traces. AMTEN exploits an\nadaptive convolution layer to predict manipulation traces in the image, which\nare reused in subsequent layers to maximize manipulation artifacts by updating\nweights during the back-propagation pass. A fake face detector, namely\nAMTENnet, is constructed by integrating AMTEN with CNN. Experimental results\nprove that the proposed AMTEN achieves desirable pre-processing. When detecting\nfake face images generated by various FIM techniques, AMTENnet achieves an\naverage accuracy up to 98.52%, which outperforms the state-of-the-art works.\nWhen detecting face images with unknown post-processing operations, the\ndetector also achieves an average accuracy of 95.17%.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 09:16:39 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 06:18:36 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Guo", "Zhiqing", ""], ["Yang", "Gaobo", ""], ["Chen", "Jiyou", ""], ["Sun", "Xingming", ""]]}, {"id": "2005.04966", "submitter": "Junnan Li Dr", "authors": "Junnan Li, Pan Zhou, Caiming Xiong, Steven C.H. Hoi", "title": "Prototypical Contrastive Learning of Unsupervised Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Prototypical Contrastive Learning (PCL), an unsupervised\nrepresentation learning method that addresses the fundamental limitations of\ninstance-wise contrastive learning. PCL not only learns low-level features for\nthe task of instance discrimination, but more importantly, it implicitly\nencodes semantic structures of the data into the learned embedding space.\nSpecifically, we introduce prototypes as latent variables to help find the\nmaximum-likelihood estimation of the network parameters in an\nExpectation-Maximization framework. We iteratively perform E-step as finding\nthe distribution of prototypes via clustering and M-step as optimizing the\nnetwork via contrastive learning. We propose ProtoNCE loss, a generalized\nversion of the InfoNCE loss for contrastive learning, which encourages\nrepresentations to be closer to their assigned prototypes. PCL outperforms\nstate-of-the-art instance-wise contrastive learning methods on multiple\nbenchmarks with substantial improvement in low-resource transfer learning. Code\nand pretrained models are available at https://github.com/salesforce/PCL.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 09:53:36 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 14:33:55 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 02:18:41 GMT"}, {"version": "v4", "created": "Tue, 28 Jul 2020 03:03:28 GMT"}, {"version": "v5", "created": "Tue, 30 Mar 2021 04:07:54 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Li", "Junnan", ""], ["Zhou", "Pan", ""], ["Xiong", "Caiming", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2005.04968", "submitter": "John Wilhelm", "authors": "Sebastian M\\\"uksch, Theo Olausson, John Wilhelm, Pavlos Andreadis", "title": "Quantitative Analysis of Image Classification Techniques for\n  Memory-Constrained Devices", "comments": "9 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks, or CNNs, are the state of the art for image\nclassification, but typically come at the cost of a large memory footprint.\nThis limits their usefulness in applications relying on embedded devices, where\nmemory is often a scarce resource. Recently, there has been significant\nprogress in the field of image classification on such memory-constrained\ndevices, with novel contributions like the ProtoNN, Bonsai and FastGRNN\nalgorithms. These have been shown to reach up to 98.2% accuracy on optical\ncharacter recognition using MNIST-10, with a memory footprint as little as 6KB.\nHowever, their potential on more complex multi-class and multi-channel image\nclassification has yet to be determined. In this paper, we compare CNNs with\nProtoNN, Bonsai and FastGRNN when applied to 3-channel image classification\nusing CIFAR-10. For our analysis, we use the existing Direct Convolution\nalgorithm to implement the CNNs memory-optimally and propose new methods of\nadjusting the FastGRNN model to work with multi-channel images. We extend the\nevaluation of each algorithm to a memory size budget of 8KB, 16KB, 32KB, 64KB\nand 128KB to show quantitatively that Direct Convolution CNNs perform best for\nall chosen budgets, with a top performance of 65.7% accuracy at a memory\nfootprint of 58.23KB.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 09:54:54 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 16:47:50 GMT"}, {"version": "v3", "created": "Sat, 19 Sep 2020 19:53:27 GMT"}, {"version": "v4", "created": "Sun, 15 Nov 2020 15:36:42 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["M\u00fcksch", "Sebastian", ""], ["Olausson", "Theo", ""], ["Wilhelm", "John", ""], ["Andreadis", "Pavlos", ""]]}, {"id": "2005.04974", "submitter": "Fernando Navarro", "authors": "Fernando Navarro, Anjany Sekuboyina, Diana Waldmannstetter, Jan C.\n  Peeken, Stephanie E. Combs and Bjoern H. Menze", "title": "Deep Reinforcement Learning for Organ Localization in CT", "comments": "Accepted paper in MIDL 2020", "journal-ref": "https://openreview.net/forum?id=0vDeD2UD0S&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DMIDL.io%2F2020%2FConference%2FAuthors%23your-submissions)", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust localization of organs in computed tomography scans is a constant\npre-processing requirement for organ-specific image retrieval, radiotherapy\nplanning, and interventional image analysis. In contrast to current solutions\nbased on exhaustive search or region proposals, which require large amounts of\nannotated data, we propose a deep reinforcement learning approach for organ\nlocalization in CT. In this work, an artificial agent is actively self-taught\nto localize organs in CT by learning from its asserts and mistakes. Within the\ncontext of reinforcement learning, we propose a novel set of actions tailored\nfor organ localization in CT. Our method can use as a plug-and-play module for\nlocalizing any organ of interest. We evaluate the proposed solution on the\npublic VISCERAL dataset containing CT scans with varying fields of view and\nmultiple organs. We achieved an overall intersection over union of 0.63, an\nabsolute median wall distance of 2.25 mm, and a median distance between\ncentroids of 3.65 mm.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 10:06:13 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Navarro", "Fernando", ""], ["Sekuboyina", "Anjany", ""], ["Waldmannstetter", "Diana", ""], ["Peeken", "Jan C.", ""], ["Combs", "Stephanie E.", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "2005.05005", "submitter": "Lingbo Yang", "authors": "Lingbo Yang, Chang Liu, Pan Wang, Shanshe Wang, Peiran Ren, Siwei Ma,\n  Wen Gao", "title": "HiFaceGAN: Face Renovation via Collaborative Suppression and\n  Replenishment", "comments": "Published in ACM Multimedia 2020", "journal-ref": null, "doi": "10.1145/3394171.3413965", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Existing face restoration researches typically relies on either the\ndegradation prior or explicit guidance labels for training, which often results\nin limited generalization ability over real-world images with heterogeneous\ndegradations and rich background contents. In this paper, we investigate the\nmore challenging and practical \"dual-blind\" version of the problem by lifting\nthe requirements on both types of prior, termed as \"Face Renovation\"(FR).\nSpecifically, we formulated FR as a semantic-guided generation problem and\ntackle it with a collaborative suppression and replenishment (CSR) approach.\nThis leads to HiFaceGAN, a multi-stage framework containing several nested CSR\nunits that progressively replenish facial details based on the hierarchical\nsemantic guidance extracted from the front-end content-adaptive suppression\nmodules. Extensive experiments on both synthetic and real face images have\nverified the superior performance of HiFaceGAN over a wide range of challenging\nrestoration subtasks, demonstrating its versatility, robustness and\ngeneralization ability towards real-world face processing applications.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 11:33:17 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 12:12:36 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yang", "Lingbo", ""], ["Liu", "Chang", ""], ["Wang", "Pan", ""], ["Wang", "Shanshe", ""], ["Ren", "Peiran", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "2005.05050", "submitter": "Joao Cartucho", "authors": "Jian Zhan, Joao Cartucho and Stamatia Giannarou", "title": "Autonomous Tissue Scanning under Free-Form Motion for Intraoperative\n  Tissue Characterisation", "comments": "7 pages, 5 figures, ICRA 2020", "journal-ref": null, "doi": "10.1109/icra40945.2020.9197294", "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Minimally Invasive Surgery (MIS), tissue scanning with imaging probes is\nrequired for subsurface visualisation to characterise the state of the tissue.\nHowever, scanning of large tissue surfaces in the presence of deformation is a\nchallenging task for the surgeon. Recently, robot-assisted local tissue\nscanning has been investigated for motion stabilisation of imaging probes to\nfacilitate the capturing of good quality images and reduce the surgeon's\ncognitive load. Nonetheless, these approaches require the tissue surface to be\nstatic or deform with periodic motion. To eliminate these assumptions, we\npropose a visual servoing framework for autonomous tissue scanning, able to\ndeal with free-form tissue deformation. The 3D structure of the surgical scene\nis recovered and a feature-based method is proposed to estimate the motion of\nthe tissue in real-time. A desired scanning trajectory is manually defined on a\nreference frame and continuously updated using projective geometry to follow\nthe tissue motion and control the movement of the robotic arm. The advantage of\nthe proposed method is that it does not require the learning of the tissue\nmotion prior to scanning and can deal with free-form deformation. We deployed\nthis framework on the da Vinci surgical robot using the da Vinci Research Kit\n(dVRK) for Ultrasound tissue scanning. Since the framework does not rely on\ninformation from the Ultrasound data, it can be easily extended to other\nprobe-based imaging modalities.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 12:50:13 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 10:02:23 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 12:37:53 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zhan", "Jian", ""], ["Cartucho", "Joao", ""], ["Giannarou", "Stamatia", ""]]}, {"id": "2005.05074", "submitter": "Said Boumaraf", "authors": "Said Boumaraf, Xiabi Liu, Chokri Ferkous, and Xiaohong Ma", "title": "A New Computer-Aided Diagnosis System with Modified Genetic Feature\n  Selection for BI-RADS Classification of Breast Masses in Mammograms", "comments": null, "journal-ref": null, "doi": "10.1155/2020/7695207", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mammography remains the most prevalent imaging tool for early breast cancer\nscreening. The language used to describe abnormalities in mammographic reports\nis based on the breast Imaging Reporting and Data System (BI-RADS). Assigning a\ncorrect BI-RADS category to each examined mammogram is a strenuous and\nchallenging task for even experts. This paper proposes a new and effective\ncomputer-aided diagnosis (CAD) system to classify mammographic masses into four\nassessment categories in BI-RADS. The mass regions are first enhanced by means\nof histogram equalization and then semiautomatically segmented based on the\nregion growing technique. A total of 130 handcrafted BI-RADS features are then\nextrcated from the shape, margin, and density of each mass, together with the\nmass size and the patient's age, as mentioned in BI-RADS mammography. Then, a\nmodified feature selection method based on the genetic algorithm (GA) is\nproposed to select the most clinically significant BI-RADS features. Finally, a\nback-propagation neural network (BPN) is employed for classification, and its\naccuracy is used as the fitness in GA. A set of 500 mammogram images from the\ndigital database of screening mammography (DDSM) is used for evaluation. Our\nsystem achieves classification accuracy, positive predictive value, negative\npredictive value, and Matthews correlation coefficient of 84.5%, 84.4%, 94.8%,\nand 79.3%, respectively. To our best knowledge, this is the best current result\nfor BI-RADS classification of breast masses in mammography, which makes the\nproposed system promising to support radiologists for deciding proper patient\nmanagement based on the automatically assigned BI-RADS categories.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 13:06:25 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Boumaraf", "Said", ""], ["Liu", "Xiabi", ""], ["Ferkous", "Chokri", ""], ["Ma", "Xiaohong", ""]]}, {"id": "2005.05123", "submitter": "Harald Hanselmann", "authors": "Harald Hanselmann and Hermann Ney", "title": "Fine-Grained Visual Classification with Efficient End-to-end\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term fine-grained visual classification (FGVC) refers to classification\ntasks where the classes are very similar and the classification model needs to\nbe able to find subtle differences to make the correct prediction.\nState-of-the-art approaches often include a localization step designed to help\na classification network by localizing the relevant parts of the input images.\nHowever, this usually requires multiple iterations or passes through a full\nclassification network or complex training schedules. In this work we present\nan efficient localization module that can be fused with a classification\nnetwork in an end-to-end setup. On the one hand the module is trained by the\ngradient flowing back from the classification network. On the other hand, two\nself-supervised loss functions are introduced to increase the localization\naccuracy. We evaluate the new model on the three benchmark datasets\nCUB200-2011, Stanford Cars and FGVC-Aircraft and are able to achieve\ncompetitive recognition performance.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 14:07:06 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Hanselmann", "Harald", ""], ["Ney", "Hermann", ""]]}, {"id": "2005.05125", "submitter": "Martin R\\\"unz", "authors": "Kejie Li, Martin R\\\"unz, Meng Tang, Lingni Ma, Chen Kong, Tanner\n  Schmidt, Ian Reid, Lourdes Agapito, Julian Straub, Steven Lovegrove, Richard\n  Newcombe", "title": "FroDO: From Detections to 3D Objects", "comments": "To be published in CVPR 2020. The first two authors contributed\n  equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object-oriented maps are important for scene understanding since they jointly\ncapture geometry and semantics, allow individual instantiation and meaningful\nreasoning about objects. We introduce FroDO, a method for accurate 3D\nreconstruction of object instances from RGB video that infers object location,\npose and shape in a coarse-to-fine manner. Key to FroDO is to embed object\nshapes in a novel learnt space that allows seamless switching between sparse\npoint cloud and dense DeepSDF decoding. Given an input sequence of localized\nRGB frames, FroDO first aggregates 2D detections to instantiate a\ncategory-aware 3D bounding box per object. A shape code is regressed using an\nencoder network before optimizing shape and pose further under the learnt shape\npriors using sparse and dense shape representations. The optimization uses\nmulti-view geometric, photometric and silhouette losses. We evaluate on\nreal-world datasets, including Pix3D, Redwood-OS, and ScanNet, for single-view,\nmulti-view, and multi-object reconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 14:08:29 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Li", "Kejie", ""], ["R\u00fcnz", "Martin", ""], ["Tang", "Meng", ""], ["Ma", "Lingni", ""], ["Kong", "Chen", ""], ["Schmidt", "Tanner", ""], ["Reid", "Ian", ""], ["Agapito", "Lourdes", ""], ["Straub", "Julian", ""], ["Lovegrove", "Steven", ""], ["Newcombe", "Richard", ""]]}, {"id": "2005.05135", "submitter": "Stefano Cerri", "authors": "Stefano Cerri, Oula Puonti, Dominik S. Meier, Jens Wuerfel, Mark\n  M\\\"uhlau, Hartwig R. Siebner, Koen Van Leemput", "title": "A Contrast-Adaptive Method for Simultaneous Whole-Brain and Lesion\n  Segmentation in Multiple Sclerosis", "comments": null, "journal-ref": null, "doi": "10.1016/j.neuroimage.2020.117471", "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present a method for the simultaneous segmentation of white matter\nlesions and normal-appearing neuroanatomical structures from multi-contrast\nbrain MRI scans of multiple sclerosis patients. The method integrates a novel\nmodel for white matter lesions into a previously validated generative model for\nwhole-brain segmentation. By using separate models for the shape of anatomical\nstructures and their appearance in MRI, the algorithm can adapt to data\nacquired with different scanners and imaging protocols without retraining. We\nvalidate the method using four disparate datasets, showing robust performance\nin white matter lesion segmentation while simultaneously segmenting dozens of\nother brain structures. We further demonstrate that the contrast-adaptive\nmethod can also be safely applied to MRI scans of healthy controls, and\nreplicate previously documented atrophy patterns in deep gray matter structures\nin MS. The algorithm is publicly available as part of the open-source\nneuroimaging package FreeSurfer.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 14:25:35 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 14:57:36 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Cerri", "Stefano", ""], ["Puonti", "Oula", ""], ["Meier", "Dominik S.", ""], ["Wuerfel", "Jens", ""], ["M\u00fchlau", "Mark", ""], ["Siebner", "Hartwig R.", ""], ["Van Leemput", "Koen", ""]]}, {"id": "2005.05175", "submitter": "Matthew Gadd", "authors": "David Williams, Daniele De Martini, Matthew Gadd, Letizia Marchegiani,\n  Paul Newman", "title": "Keep off the Grass: Permissible Driving Routes from Radar with Weak\n  Audio Supervision", "comments": "accepted for publication at the IEEE Intelligent Transportation\n  Systems Conference (ITSC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable outdoor deployment of mobile robots requires the robust\nidentification of permissible driving routes in a given environment. The\nperformance of LiDAR and vision-based perception systems deteriorates\nsignificantly if certain environmental factors are present e.g. rain, fog,\ndarkness. Perception systems based on FMCW scanning radar maintain full\nperformance regardless of environmental conditions and with a longer range than\nalternative sensors. Learning to segment a radar scan based on driveability in\na fully supervised manner is not feasible as labelling each radar scan on a\nbin-by-bin basis is both difficult and time-consuming to do by hand. We\ntherefore weakly supervise the training of the radar-based classifier through\nan audio-based classifier that is able to predict the terrain type underneath\nthe robot. By combining odometry, GPS and the terrain labels from the audio\nclassifier, we are able to construct a terrain labelled trajectory of the robot\nin the environment which is then used to label the radar scans. Using a\ncurriculum learning procedure, we then train a radar segmentation network to\ngeneralise beyond the initial labelling and to detect all permissible driving\nroutes in the environment.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 15:11:20 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 07:28:19 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Williams", "David", ""], ["De Martini", "Daniele", ""], ["Gadd", "Matthew", ""], ["Marchegiani", "Letizia", ""], ["Newman", "Paul", ""]]}, {"id": "2005.05179", "submitter": "Zichao Zhang", "authors": "Zichao Zhang, Torsten Sattler, Davide Scaramuzza", "title": "Reference Pose Generation for Long-term Visual Localization via Learned\n  Features and View Synthesis", "comments": "25 pages, 16 figures. Int J Comput Vis (2020)", "journal-ref": null, "doi": "10.1007/s11263-020-01399-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Localization is one of the key enabling technologies for autonomous\ndriving and augmented reality. High quality datasets with accurate 6\nDegree-of-Freedom (DoF) reference poses are the foundation for benchmarking and\nimproving existing methods. Traditionally, reference poses have been obtained\nvia Structure-from-Motion (SfM). However, SfM itself relies on local features\nwhich are prone to fail when images were taken under different conditions,\ne.g., day/ night changes. At the same time, manually annotating feature\ncorrespondences is not scalable and potentially inaccurate. In this work, we\npropose a semi-automated approach to generate reference poses based on feature\nmatching between renderings of a 3D model and real images via learned features.\nGiven an initial pose estimate, our approach iteratively refines the pose based\non feature matches against a rendering of the model from the current pose\nestimate. We significantly improve the nighttime reference poses of the popular\nAachen Day-Night dataset, showing that state-of-the-art visual localization\nmethods perform better (up to $47\\%$) than predicted by the original reference\nposes. We extend the dataset with new nighttime test images, provide\nuncertainty estimates for our new reference poses, and introduce a new\nevaluation criterion. We will make our reference poses and our framework\npublicly available upon publication.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 15:13:07 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 15:31:07 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2020 04:07:27 GMT"}, {"version": "v4", "created": "Wed, 30 Dec 2020 14:29:28 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhang", "Zichao", ""], ["Sattler", "Torsten", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2005.05207", "submitter": "Eungyeup Kim", "authors": "Junsoo Lee, Eungyeup Kim, Yunsung Lee, Dongjun Kim, Jaehyuk Chang,\n  Jaegul Choo", "title": "Reference-Based Sketch Image Colorization using Augmented-Self Reference\n  and Dense Semantic Correspondence", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the automatic colorization task of a sketch image given an\nalready-colored reference image. Colorizing a sketch image is in high demand in\ncomics, animation, and other content creation applications, but it suffers from\ninformation scarcity of a sketch image. To address this, a reference image can\nrender the colorization process in a reliable and user-driven manner. However,\nit is difficult to prepare for a training data set that has a sufficient amount\nof semantically meaningful pairs of images as well as the ground truth for a\ncolored image reflecting a given reference (e.g., coloring a sketch of an\noriginally blue car given a reference green car). To tackle this challenge, we\npropose to utilize the identical image with geometric distortion as a virtual\nreference, which makes it possible to secure the ground truth for a colored\noutput image. Furthermore, it naturally provides the ground truth for dense\nsemantic correspondence, which we utilize in our internal attention mechanism\nfor color transfer from reference to sketch input. We demonstrate the\neffectiveness of our approach in various types of sketch image colorization via\nquantitative as well as qualitative evaluation against existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 15:52:50 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Lee", "Junsoo", ""], ["Kim", "Eungyeup", ""], ["Lee", "Yunsung", ""], ["Kim", "Dongjun", ""], ["Chang", "Jaehyuk", ""], ["Choo", "Jaegul", ""]]}, {"id": "2005.05218", "submitter": "Eshal Zahra", "authors": "Eshal Zahra and Bostan Ali and Wajahat Siddique", "title": "Medical Image Segmentation Using a U-Net type of Architecture", "comments": "6 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have been proven to be very effective in\nimage related analysis and tasks, such as image segmentation, image\nclassification, image generation, etc. Recently many sophisticated CNN based\narchitectures have been proposed for the purpose of image segmentation. Some of\nthese newly designed networks are used for the specific purpose of medical\nimage segmentation, models like V-Net, U-Net and their variants. It has been\nshown that U-Net produces very promising results in the domain of medical image\nsegmentation.However, in this paper, we argue that the architecture of U-Net,\nwhen combined with a supervised training strategy at the bottleneck layer, can\nproduce comparable results with the original U-Net architecture. More\nspecifically, we introduce a fully supervised FC layers based pixel-wise loss\nat the bottleneck of the encoder branch of U-Net. The two layer based FC\nsub-net will train the bottleneck representation to contain more semantic\ninformation, which will be used by the decoder layers to predict the final\nsegmentation map. The FC layer based sub-net is trained by employing the\npixel-wise cross entropy loss, while the U-Net architectures trained by using\nL1 loss.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 16:10:18 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Zahra", "Eshal", ""], ["Ali", "Bostan", ""], ["Siddique", "Wajahat", ""]]}, {"id": "2005.05220", "submitter": "Christian Etmann", "authors": "Christian Etmann and Rihuan Ke and Carola-Bibiane Sch\\\"onlieb", "title": "iUNets: Fully invertible U-Nets with Learnable Up- and Downsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  U-Nets have been established as a standard architecture for image-to-image\nlearning problems such as segmentation and inverse problems in imaging. For\nlarge-scale data, as it for example appears in 3D medical imaging, the U-Net\nhowever has prohibitive memory requirements. Here, we present a new\nfully-invertible U-Net-based architecture called the iUNet, which employs novel\nlearnable and invertible up- and downsampling operations, thereby making the\nuse of memory-efficient backpropagation possible. This allows us to train\ndeeper and larger networks in practice, under the same GPU memory restrictions.\nDue to its invertibility, the iUNet can furthermore be used for constructing\nnormalizing flows.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 16:14:13 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 11:17:53 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 09:32:29 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Etmann", "Christian", ""], ["Ke", "Rihuan", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2005.05232", "submitter": "Matthia Sabatelli", "authors": "Matthia Sabatelli, Mike Kestemont, Pierre Geurts", "title": "On the Transferability of Winning Tickets in Non-Natural Image Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the generalization properties of pruned neural networks that are the\nwinners of the lottery ticket hypothesis on datasets of natural images. We\nanalyse their potential under conditions in which training data is scarce and\ncomes from a non-natural domain. Specifically, we investigate whether pruned\nmodels that are found on the popular CIFAR-10/100 and Fashion-MNIST datasets,\ngeneralize to seven different datasets that come from the fields of digital\npathology and digital heritage. Our results show that there are significant\nbenefits in transferring and training sparse architectures over larger\nparametrized models, since in all of our experiments pruned networks, winners\nof the lottery ticket hypothesis, significantly outperform their larger\nunpruned counterparts. These results suggest that winning initializations do\ncontain inductive biases that are generic to some extent, although, as reported\nby our experiments on the biomedical datasets, their generalization properties\ncan be more limiting than what has been so far observed in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 16:26:00 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 12:41:28 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Sabatelli", "Matthia", ""], ["Kestemont", "Mike", ""], ["Geurts", "Pierre", ""]]}, {"id": "2005.05267", "submitter": "Sharif Amit Kamran", "authors": "Sharif Amit Kamran, Khondker Fariha Hossain, Alireza Tavakkoli,\n  Stewart Lee Zuckerbrod, Salah A. Baker, Kenton M. Sanders", "title": "Fundus2Angio: A Conditional GAN Architecture for Generating Fluorescein\n  Angiography Images from Retinal Fundus Photography", "comments": "14 pages, Accepted to 15th International Symposium on Visual\n  Computing 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Carrying out clinical diagnosis of retinal vascular degeneration using\nFluorescein Angiography (FA) is a time consuming process and can pose\nsignificant adverse effects on the patient. Angiography requires insertion of a\ndye that may cause severe adverse effects and can even be fatal. Currently,\nthere are no non-invasive systems capable of generating Fluorescein Angiography\nimages. However, retinal fundus photography is a non-invasive imaging technique\nthat can be completed in a few seconds. In order to eliminate the need for FA,\nwe propose a conditional generative adversarial network (GAN) to translate\nfundus images to FA images. The proposed GAN consists of a novel residual block\ncapable of generating high quality FA images. These images are important tools\nin the differential diagnosis of retinal diseases without the need for invasive\nprocedure with possible side effects. Our experiments show that the proposed\narchitecture outperforms other state-of-the-art generative networks.\nFurthermore, our proposed model achieves better qualitative results\nindistinguishable from real angiograms.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:09:29 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 08:46:05 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Kamran", "Sharif Amit", ""], ["Hossain", "Khondker Fariha", ""], ["Tavakkoli", "Alireza", ""], ["Zuckerbrod", "Stewart Lee", ""], ["Baker", "Salah A.", ""], ["Sanders", "Kenton M.", ""]]}, {"id": "2005.05269", "submitter": "Anis Koubaa", "authors": "Adel Ammar, Anis Koubaa", "title": "Deep-Learning-based Automated Palm Tree Counting and Geolocation in\n  Large Farms from Aerial Geotagged Images", "comments": "First version of the paper, 3 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a deep learning framework for the automated\ncounting and geolocation of palm trees from aerial images using convolutional\nneural networks. For this purpose, we collected aerial images in a palm tree\nFarm in the Kharj region, in Riyadh Saudi Arabia, using DJI drones, and we\nbuilt a dataset of around 10,000 instances of palms trees. Then, we developed a\nconvolutional neural network model using the state-of-the-art, Faster R-CNN\nalgorithm. Furthermore, using the geotagged metadata of aerial images, we used\nphotogrammetry concepts and distance corrections to detect the geographical\nlocation of detected palms trees automatically. This geolocation technique was\ntested on two different types of drones (DJI Mavic Pro, and Phantom 4 Pro), and\nwas assessed to provide an average geolocation accuracy of 2.8m. This GPS\ntagging allows us to uniquely identify palm trees and count their number from a\nseries of drone images, while correctly dealing with the issue of image\noverlapping. Moreover, it can be generalized to the geolocation of any other\nobjects in UAV images.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:11:49 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Ammar", "Adel", ""], ["Koubaa", "Anis", ""]]}, {"id": "2005.05274", "submitter": "Dongsuk Kim", "authors": "Dongsuk Kim and Geonhee Lee and Myungjae Lee and Shin Uk Kang and\n  Dongmin Kim", "title": "Normalized Convolutional Neural Network", "comments": "6pages typo errors ,errata are fixed.(p1,2,4,5)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Normalized Convolutional Neural Network(NCNN). NCNN\nis more fitted to a convolutional operator than other nomralizaiton methods.\nThe normalized process is similar to a normalization methods, but NCNN is more\nadapative to sliced-inputs and corresponding the convolutional kernel. Therefor\nNCNN can be targeted to micro-batch training. Normalizaing of NC is conducted\nduring convolutional process. In short, NC process is not usual normalization\nand can not be realized in deep learning framework optimizing standard\nconvolution process. Hence we named this method 'Normalized Convolution'. As a\nresult, NC process has universal property which means NC can be applied to any\nAI tasks involving convolution neural layer . Since NC don't need other\nnormalization layer, NCNN looks like convolutional version of Self Normalizing\nNetwork.(SNN). Among micro-batch trainings, NCNN outperforms other\nbatch-independent normalization methods. NCNN archives these superiority by\nstandardizing rows of im2col matrix of inputs, which theoretically smooths the\ngradient of loss. The code need to manipulate standard convolution neural\nnetworks step by step. The code is available : https://github.com/kimdongsuk1/\nNormalizedCNN.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:20:26 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 17:05:15 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 10:19:32 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Kim", "Dongsuk", ""], ["Lee", "Geonhee", ""], ["Lee", "Myungjae", ""], ["Kang", "Shin Uk", ""], ["Kim", "Dongmin", ""]]}, {"id": "2005.05287", "submitter": "Prateek Khandelwal", "authors": "Prateek Khandelwal, Anuj Khandelwal, Snigdha Agarwal, Deep Thomas,\n  Naveen Xavier, Arun Raghuraman (for Group Data and Analytics, Aditya Birla\n  Group)", "title": "Using Computer Vision to enhance Safety of Workforce in Manufacturing in\n  a Post COVID World", "comments": "6 pages, 7 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic forced governments across the world to impose lockdowns\nto prevent virus transmissions. This resulted in the shutdown of all economic\nactivity and accordingly the production at manufacturing plants across most\nsectors was halted. While there is an urgency to resume production, there is an\neven greater need to ensure the safety of the workforce at the plant site.\nReports indicate that maintaining social distancing and wearing face masks\nwhile at work clearly reduces the risk of transmission. We decided to use\ncomputer vision on CCTV feeds to monitor worker activity and detect violations\nwhich trigger real time voice alerts on the shop floor. This paper describes an\nefficient and economic approach of using AI to create a safe environment in a\nmanufacturing setup. We demonstrate our approach to build a robust social\ndistancing measurement algorithm using a mix of modern-day deep learning and\nclassic projective geometry techniques. We have deployed our solution at\nmanufacturing plants across the Aditya Birla Group (ABG). We have also\ndescribed our face mask detection approach which provides a high accuracy\nacross a range of customized masks.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:40:58 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 12:16:12 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Khandelwal", "Prateek", "", "for Group Data and Analytics, Aditya Birla\n  Group"], ["Khandelwal", "Anuj", "", "for Group Data and Analytics, Aditya Birla\n  Group"], ["Agarwal", "Snigdha", "", "for Group Data and Analytics, Aditya Birla\n  Group"], ["Thomas", "Deep", "", "for Group Data and Analytics, Aditya Birla\n  Group"], ["Xavier", "Naveen", "", "for Group Data and Analytics, Aditya Birla\n  Group"], ["Raghuraman", "Arun", "", "for Group Data and Analytics, Aditya Birla\n  Group"]]}, {"id": "2005.05371", "submitter": "Abeer M. Mahmoud", "authors": "Nora Youssef, Abeer M. Mahmoud and El-Sayed M. El-Horbaty", "title": "A Parallel Hybrid Technique for Multi-Noise Removal from Grayscale\n  Medical Images", "comments": null, "journal-ref": "IOSR Journal of Computer Engineering (IOSR-JCE) e-ISSN:\n  2278-0661,p-ISSN: 2278-8727, Volume 18, Issue 5, Ver. V (Sep. - Oct. 2016),\n  PP 121-128 www.iosrjournals.org", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging is the technique used to create images of the human body or\nparts of it for clinical purposes. Medical images always have large sizes and\nthey are commonly corrupted by single or multiple noise type at the same time,\ndue to various reasons, these two reasons are the triggers for moving toward\nparallel image processing to find alternatives of image de-noising techniques.\nThis paper presents a parallel hybrid filter implementation for gray scale\nmedical image de-noising. The hybridization is between adaptive median and\nwiener filters. Parallelization is implemented on the adaptive median filter to\novercome the latency of neighborhood operation, parfor implicit parallelism\npowered by MatLab 2013a is used. The implementation is tested on an image of\n2.5 MB size, which is divided into 2, 4 and 8 partitions; a comparison between\nthe proposed implementation and sequential implementation is given, in terms of\ntime. Thus, each case has the best time when assigned to number of threads\nequal to the number of its partitions. Moreover, Speed up and efficiency are\ncalculated for the algorithm and they show a measured enhancement.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 23:01:21 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Youssef", "Nora", ""], ["Mahmoud", "Abeer M.", ""], ["El-Horbaty", "El-Sayed M.", ""]]}, {"id": "2005.05402", "submitter": "Jie Lei", "authors": "Jie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara L. Berg, Mohit\n  Bansal", "title": "MART: Memory-Augmented Recurrent Transformer for Coherent Video\n  Paragraph Captioning", "comments": "ACL 2020 (12 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating multi-sentence descriptions for videos is one of the most\nchallenging captioning tasks due to its high requirements for not only visual\nrelevance but also discourse-based coherence across the sentences in the\nparagraph. Towards this goal, we propose a new approach called Memory-Augmented\nRecurrent Transformer (MART), which uses a memory module to augment the\ntransformer architecture. The memory module generates a highly summarized\nmemory state from the video segments and the sentence history so as to help\nbetter prediction of the next sentence (w.r.t. coreference and repetition\naspects), thus encouraging coherent paragraph generation. Extensive\nexperiments, human evaluations, and qualitative analyses on two popular\ndatasets ActivityNet Captions and YouCookII show that MART generates more\ncoherent and less repetitive paragraph captions than baseline methods, while\nmaintaining relevance to the input video events. All code is available\nopen-source at: https://github.com/jayleicn/recurrent-transformer\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 20:01:41 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Lei", "Jie", ""], ["Wang", "Liwei", ""], ["Shen", "Yelong", ""], ["Yu", "Dong", ""], ["Berg", "Tamara L.", ""], ["Bansal", "Mohit", ""]]}, {"id": "2005.05410", "submitter": "Changkyu Song", "authors": "Changkyu Song and Abdeslam Boularias", "title": "Identifying Mechanical Models through Differentiable Simulations", "comments": "to be published in Learning for DynamIcs & Control (L4DC), June\n  10-11th, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for manipulating unknown objects through a\nsequence of non-prehensile actions that displace an object from its initial\nconfiguration to a given goal configuration on a flat surface. The proposed\nmethod leverages recent progress in differentiable physics models to identify\nunknown mechanical properties of manipulated objects, such as inertia matrix,\nfriction coefficients and external forces acting on the object. To this end, a\nrecently proposed differentiable physics engine for two-dimensional objects is\nadopted in this work and extended to deal forces in the three-dimensional\nspace. The proposed model identification technique analytically computes the\ngradient of the distance between forecasted poses of objects and their actual\nobserved poses and utilizes that gradient to search for values of the\nmechanical properties that reduce the reality gap. Experiments with real\nobjects using a real robot to gather data show that the proposed approach can\nidentify the mechanical properties of heterogeneous objects on the fly.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 20:19:20 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Song", "Changkyu", ""], ["Boularias", "Abdeslam", ""]]}, {"id": "2005.05418", "submitter": "Giannis Fikioris", "authors": "Giannis Fikioris, Kostas Patroumpas, Alexander Artikis", "title": "Optimizing Vessel Trajectory Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work we introduced a trajectory detection module that can provide\nsummarized representations of vessel trajectories by consuming AIS positional\nmessages online. This methodology can provide reliable trajectory synopses with\nlittle deviations from the original course by discarding at least 70% of the\nraw data as redundant. However, such trajectory compression is very sensitive\nto parametrization. In this paper, our goal is to fine-tune the selection of\nthese parameter values. We take into account the type of each vessel in order\nto provide a suitable configuration that can yield improved trajectory\nsynopses, both in terms of approximation error and compression ratio.\nFurthermore, we employ a genetic algorithm converging to a suitable\nconfiguration per vessel type. Our tests against a publicly available AIS\ndataset have shown that compression efficiency is comparable or even better\nthan the one with default parametrization without resorting to a laborious data\ninspection.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 20:38:56 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Fikioris", "Giannis", ""], ["Patroumpas", "Kostas", ""], ["Artikis", "Alexander", ""]]}, {"id": "2005.05431", "submitter": "Neil Getty", "authors": "Neil Getty, Thomas Brettin, Dong Jin, Rick Stevens, Fangfang Xia", "title": "Deep Medical Image Analysis with Representation Learning and\n  Neuromorphic Computing", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore three representative lines of research and demonstrate the utility\nof our methods on a classification benchmark of brain cancer MRI data. First,\nwe present a capsule network that explicitly learns a representation robust to\nrotation and affine transformation. This model requires less training data and\noutperforms both the original convolutional baseline and a previous capsule\nnetwork implementation. Second, we leverage the latest domain adaptation\ntechniques to achieve a new state-of-the-art accuracy. Our experiments show\nthat non-medical images can be used to improve model performance. Finally, we\ndesign a spiking neural network trained on the Intel Loihi neuromorphic chip\n(Fig. 1 shows an inference snapshot). This model consumes much lower power\nwhile achieving reasonable accuracy given model reduction. We posit that more\nresearch in this direction combining hardware and learning advancements will\npower future medical imaging (on-device AI, few-shot prediction, adaptive\nscanning).\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 20:56:37 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Getty", "Neil", ""], ["Brettin", "Thomas", ""], ["Jin", "Dong", ""], ["Stevens", "Rick", ""], ["Xia", "Fangfang", ""]]}, {"id": "2005.05432", "submitter": "Prashant Pandey", "authors": "Prashant Pandey, Prathosh AP, Vinay Kyatham, Deepak Mishra and\n  Tathagato Rai Dastidar", "title": "Target-Independent Domain Adaptation for WBC Classification using\n  Generative Latent Search", "comments": "IEEE TMI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating the classification of camera-obtained microscopic images of White\nBlood Cells (WBCs) and related cell subtypes has assumed importance since it\naids the laborious manual process of review and diagnosis. Several\nState-Of-The-Art (SOTA) methods developed using Deep Convolutional Neural\nNetworks suffer from the problem of domain shift - severe performance\ndegradation when they are tested on data (target) obtained in a setting\ndifferent from that of the training (source). The change in the target data\nmight be caused by factors such as differences in camera/microscope types,\nlenses, lighting-conditions etc. This problem can potentially be solved using\nUnsupervised Domain Adaptation (UDA) techniques albeit standard algorithms\npresuppose the existence of a sufficient amount of unlabelled target data which\nis not always the case with medical images. In this paper, we propose a method\nfor UDA that is devoid of the need for target data. Given a test image from the\ntarget data, we obtain its 'closest-clone' from the source data that is used as\na proxy in the classifier. We prove the existence of such a clone given that\ninfinite number of data points can be sampled from the source distribution. We\npropose a method in which a latent-variable generative model based on\nvariational inference is used to simultaneously sample and find the\n'closest-clone' from the source distribution through an optimization procedure\nin the latent space. We demonstrate the efficacy of the proposed method over\nseveral SOTA UDA methods for WBC classification on datasets captured using\ndifferent imaging modalities under multiple settings.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 20:58:23 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 18:30:20 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Pandey", "Prashant", ""], ["AP", "Prathosh", ""], ["Kyatham", "Vinay", ""], ["Mishra", "Deepak", ""], ["Dastidar", "Tathagato Rai", ""]]}, {"id": "2005.05451", "submitter": "Arjun Gupta", "authors": "Arjun Gupta and Luca Carlone", "title": "Online Monitoring for Neural Network Based Monocular Pedestrian Pose\n  Estimation", "comments": "Accepted to ITSC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Several autonomy pipelines now have core components that rely on deep\nlearning approaches. While these approaches work well in nominal conditions,\nthey tend to have unexpected and severe failure modes that create concerns when\nused in safety-critical applications, including self-driving cars. There are\nseveral works that aim to characterize the robustness of networks offline, but\ncurrently there is a lack of tools to monitor the correctness of network\noutputs online during operation. We investigate the problem of online output\nmonitoring for neural networks that estimate 3D human shapes and poses from\nimages. Our first contribution is to present and evaluate model-based and\nlearning-based monitors for a human-pose-and-shape reconstruction network, and\nassess their ability to predict the output loss for a given test input. As a\nsecond contribution, we introduce an Adversarially-Trained Online Monitor (\nATOM ) that learns how to effectively predict losses from data. ATOM dominates\nmodel-based baselines and can detect bad outputs, leading to substantial\nimprovements in human pose output quality. Our final contribution is an\nextensive experimental evaluation that shows that discarding outputs flagged as\nincorrect by ATOM improves the average error by 12.5%, and the worst-case error\nby 126.5%.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 21:40:41 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Gupta", "Arjun", ""], ["Carlone", "Luca", ""]]}, {"id": "2005.05460", "submitter": "Majed El Helou", "authors": "Majed El Helou, Ruofan Zhou, Johan Barthas, Sabine S\\\"usstrunk", "title": "VIDIT: Virtual Image Dataset for Illumination Transfer", "comments": "For further information and data, see\n  https://github.com/majedelhelou/VIDIT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep image relighting is gaining more interest lately, as it allows photo\nenhancement through illumination-specific retouching without human effort.\nAside from aesthetic enhancement and photo montage, image relighting is\nvaluable for domain adaptation, whether to augment datasets for training or to\nnormalize input test data. Accurate relighting is, however, very challenging\nfor various reasons, such as the difficulty in removing and recasting shadows\nand the modeling of different surfaces. We present a novel dataset, the Virtual\nImage Dataset for Illumination Transfer (VIDIT), in an effort to create a\nreference evaluation benchmark and to push forward the development of\nillumination manipulation methods. Virtual datasets are not only an important\nstep towards achieving real-image performance but have also proven capable of\nimproving training even when real datasets are possible to acquire and\navailable. VIDIT contains 300 virtual scenes used for training, where every\nscene is captured 40 times in total: from 8 equally-spaced azimuthal angles,\neach lit with 5 different illuminants.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 21:58:03 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 10:17:15 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Helou", "Majed El", ""], ["Zhou", "Ruofan", ""], ["Barthas", "Johan", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "2005.05481", "submitter": "Jingwei Song", "authors": "Jingwei Song, Mitesh Patel, Andreas Girgensohn, Chelhwon Kim", "title": "Combining Deep Learning with Geometric Features for Image based\n  Localization in the Gastrointestinal Tract", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking monocular colonoscope in the Gastrointestinal tract (GI) is a\nchallenging problem as the images suffer from deformation, blurred textures,\nsignificant changes in appearance. They greatly restrict the tracking ability\nof conventional geometry based methods. Even though Deep Learning (DL) can\novercome these issues, limited labeling data is a roadblock to state-of-art DL\nmethod. Considering these, we propose a novel approach to combine DL method\nwith traditional feature based approach to achieve better localization with\nsmall training data. Our method fully exploits the best of both worlds by\nintroducing a Siamese network structure to perform few-shot classification to\nthe closest zone in the segmented training image set. The classified label is\nfurther adopted to initialize the pose of scope. To fully use the training\ndataset, a pre-generated triangulated map points within the zone in the\ntraining set are registered with observation and contribute to estimating the\noptimal pose of the test image. The proposed hybrid method is extensively\ntested and compared with existing methods, and the result shows significant\nimprovement over traditional geometric based or DL based localization. The\naccuracy is improved by 28.94% (Position) and 10.97% (Orientation) with respect\nto state-of-art method.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 23:04:00 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 19:25:49 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Song", "Jingwei", ""], ["Patel", "Mitesh", ""], ["Girgensohn", "Andreas", ""], ["Kim", "Chelhwon", ""]]}, {"id": "2005.05490", "submitter": "David Suter", "authors": "David Suter, Ruwan Tennakoon, Erchuan Zhang, Tat-Jun Chin and Alireza\n  Bab-Hadiashar", "title": "Monotone Boolean Functions, Feasibility/Infeasibility, LP-type problems\n  and MaxCon", "comments": "Parts under conference review, work in progress. Keywords: Monotone\n  Boolean Functions, Consensus Maximisation, LP-Type Problem, Computer Vision,\n  Robust Fitting, Matroid, Simplicial Complex, Independence Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines connections between Monotone Boolean Functions, LP-Type\nproblems and the Maximum Consensus Problem. The latter refers to a particular\ntype of robust fitting characterisation, popular in Computer Vision (MaxCon).\nIndeed, this is our main motivation but we believe the results of the study of\nthese connections are more widely applicable to LP-type problems (at least\n'thresholded versions', as we describe), and perhaps even more widely. We\nillustrate, with examples from Computer Vision, how the resulting perspectives\nsuggest new algorithms. Indeed, we focus, in the experimental part, on how the\nInfluence (a property of Boolean Functions that takes on a special form if the\nfunction is Monotone) can guide a search for the MaxCon solution.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 23:51:15 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Suter", "David", ""], ["Tennakoon", "Ruwan", ""], ["Zhang", "Erchuan", ""], ["Chin", "Tat-Jun", ""], ["Bab-Hadiashar", "Alireza", ""]]}, {"id": "2005.05495", "submitter": "Kiana Harris", "authors": "Jianyu Mao, Kiana Harris, Nae-Rong Chang, Caleb Pennell, Yiming Ren", "title": "Train and Deploy an Image Classifier for Disaster Response", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With Deep Learning Image Classification becoming more powerful each year, it\nis apparent that its introduction to disaster response will increase the\nefficiency that responders can work with. Using several Neural Network Models,\nincluding AlexNet, ResNet, MobileNet, DenseNets, and 4-Layer CNN, we have\nclassified flood disaster images from a large image data set with up to 79%\naccuracy. Our models and tutorials for working with the data set have created a\nfoundation for others to classify other types of disasters contained in the\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 00:45:48 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Mao", "Jianyu", ""], ["Harris", "Kiana", ""], ["Chang", "Nae-Rong", ""], ["Pennell", "Caleb", ""], ["Ren", "Yiming", ""]]}, {"id": "2005.05496", "submitter": "Saeid Asgari Taghanaki", "authors": "Saeid Asgari Taghanaki, Mohammad Havaei, Alex Lamb, Aditya Sanghi, Ara\n  Danielyan, Tonya Custis", "title": "Jigsaw-VAE: Towards Balancing Features in Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The latent variables learned by VAEs have seen considerable interest as an\nunsupervised way of extracting features, which can then be used for downstream\ntasks. There is a growing interest in the question of whether features learned\non one environment will generalize across different environments. We\ndemonstrate here that VAE latent variables often focus on some factors of\nvariation at the expense of others - in this case we refer to the features as\n``imbalanced''. Feature imbalance leads to poor generalization when the latent\nvariables are used in an environment where the presence of features changes.\nSimilarly, latent variables trained with imbalanced features induce the VAE to\ngenerate less diverse (i.e. biased towards dominant features) samples. To\naddress this, we propose a regularization scheme for VAEs, which we show\nsubstantially addresses the feature imbalance problem. We also introduce a\nsimple metric to measure the balance of features in generated images.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 00:46:54 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Taghanaki", "Saeid Asgari", ""], ["Havaei", "Mohammad", ""], ["Lamb", "Alex", ""], ["Sanghi", "Aditya", ""], ["Danielyan", "Ara", ""], ["Custis", "Tonya", ""]]}, {"id": "2005.05501", "submitter": "Wenxiang Jiang", "authors": "Yancheng Wang, Yang Xiao, Fu Xiong, Wenxiang Jiang, Zhiguo Cao, Joey\n  Tianyi Zhou and Junsong Yuan", "title": "3DV: 3D Dynamic Voxel for Action Recognition in Depth Video", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To facilitate depth-based 3D action recognition, 3D dynamic voxel (3DV) is\nproposed as a novel 3D motion representation. With 3D space voxelization, the\nkey idea of 3DV is to encode 3D motion information within depth video into a\nregular voxel set (i.e., 3DV) compactly, via temporal rank pooling. Each\navailable 3DV voxel intrinsically involves 3D spatial and motion feature\njointly. 3DV is then abstracted as a point set and input into PointNet++ for 3D\naction recognition, in the end-to-end learning way. The intuition for\ntransferring 3DV into the point set form is that, PointNet++ is lightweight and\neffective for deep feature learning towards point set. Since 3DV may lose\nappearance clue, a multi-stream 3D action recognition manner is also proposed\nto learn motion and appearance feature jointly. To extract richer temporal\norder information of actions, we also divide the depth video into temporal\nsplits and encode this procedure in 3DV integrally. The extensive experiments\non 4 well-established benchmark datasets demonstrate the superiority of our\nproposition. Impressively, we acquire the accuracy of 82.4% and 93.5% on NTU\nRGB+D 120 [13] with the cross-subject and crosssetup test setting respectively.\n3DV's code is available at https://github.com/3huo/3DV-Action.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 01:04:34 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Wang", "Yancheng", ""], ["Xiao", "Yang", ""], ["Xiong", "Fu", ""], ["Jiang", "Wenxiang", ""], ["Cao", "Zhiguo", ""], ["Zhou", "Joey Tianyi", ""], ["Yuan", "Junsong", ""]]}, {"id": "2005.05509", "submitter": "Mohammad Rami Koujan", "authors": "Mohammad Rami Koujan, Luma Alharbawee, Giorgos Giannakakis, Nicolas\n  Pugeault, Anastasios Roussos", "title": "Real-time Facial Expression Recognition \"In The Wild'' by Disentangling\n  3D Expression from Identity", "comments": "to be published in 15th IEEE International Conference on Automatic\n  Face and Gesture Recognition (FG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human emotions analysis has been the focus of many studies, especially in the\nfield of Affective Computing, and is important for many applications, e.g.\nhuman-computer intelligent interaction, stress analysis, interactive games,\nanimations, etc. Solutions for automatic emotion analysis have also benefited\nfrom the development of deep learning approaches and the availability of vast\namount of visual facial data on the internet. This paper proposes a novel\nmethod for human emotion recognition from a single RGB image. We construct a\nlarge-scale dataset of facial videos (\\textbf{FaceVid}), rich in facial\ndynamics, identities, expressions, appearance and 3D pose variations. We use\nthis dataset to train a deep Convolutional Neural Network for estimating\nexpression parameters of a 3D Morphable Model and combine it with an effective\nback-end emotion classifier. Our proposed framework runs at 50 frames per\nsecond and is capable of robustly estimating parameters of 3D expression\nvariation and accurately recognizing facial expressions from in-the-wild\nimages. We present extensive experimental evaluation that shows that the\nproposed method outperforms the compared techniques in estimating the 3D\nexpression parameters and achieves state-of-the-art performance in recognising\nthe basic emotions from facial images, as well as recognising stress from\nfacial videos. %compared to the current state of the art in emotion recognition\nfrom facial images.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 01:32:55 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Koujan", "Mohammad Rami", ""], ["Alharbawee", "Luma", ""], ["Giannakakis", "Giorgos", ""], ["Pugeault", "Nicolas", ""], ["Roussos", "Anastasios", ""]]}, {"id": "2005.05519", "submitter": "Kaijie Xu", "authors": "Kaijie Xu, Witold Pedrycz, Zhiwu Li, Yinghui Quan, and Weike Nie", "title": "A Novel Granular-Based Bi-Clustering Method of Deep Mining the\n  Co-Expressed Genes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional clustering methods are limited when dealing with huge and\nheterogeneous groups of gene expression data, which motivates the development\nof bi-clustering methods. Bi-clustering methods are used to mine bi-clusters\nwhose subsets of samples (genes) are co-regulated under their test conditions.\nStudies show that mining bi-clusters of consistent trends and trends with\nsimilar degrees of fluctuations from the gene expression data is essential in\nbioinformatics research. Unfortunately, traditional bi-clustering methods are\nnot fully effective in discovering such bi-clusters. Therefore, we propose a\nnovel bi-clustering method by involving here the theory of Granular Computing.\nIn the proposed scheme, the gene data matrix, considered as a group of time\nseries, is transformed into a series of ordered information granules. With the\ninformation granules we build a characteristic matrix of the gene data to\ncapture the fluctuation trend of the expression value between consecutive\nconditions to mine the ideal bi-clusters. The experimental results are in\nagreement with the theoretical analysis, and show the excellent performance of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 02:04:40 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Xu", "Kaijie", ""], ["Pedrycz", "Witold", ""], ["Li", "Zhiwu", ""], ["Quan", "Yinghui", ""], ["Nie", "Weike", ""]]}, {"id": "2005.05526", "submitter": "Fei Gao", "authors": "Fei Gao, Jingjie Zhu, Zeyuan Yu, Peng Li, Tao Wang", "title": "Making Robots Draw A Vivid Portrait In Two Minutes", "comments": "7 pages, 7 figures; accepted by IROS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant progress has been made with artistic robots. However, existing\nrobots fail to produce high-quality portraits in a short time. In this work, we\npresent a drawing robot, which can automatically transfer a facial picture to a\nvivid portrait, and then draw it on paper within two minutes averagely. At the\nheart of our system is a novel portrait synthesis algorithm based on deep\nlearning. Innovatively, we employ a self-consistency loss, which makes the\nalgorithm capable of generating continuous and smooth brush-strokes. Besides,\nwe propose a componential sparsity constraint to reduce the number of\nbrush-strokes over insignificant areas. We also implement a local sketch\nsynthesis algorithm, and several pre- and post-processing techniques to deal\nwith the background and details. The portrait produced by our algorithm\nsuccessfully captures individual characteristics by using a sparse set of\ncontinuous brush-strokes. Finally, the portrait is converted to a sequence of\ntrajectories and reproduced by a 3-degree-of-freedom robotic arm. The whole\nportrait drawing robotic system is named AiSketcher. Extensive experiments show\nthat AiSketcher can produce considerably high-quality sketches for a wide range\nof pictures, including faces in-the-wild and universal images of arbitrary\ncontent. To our best knowledge, AiSketcher is the first portrait drawing robot\nthat uses neural style transfer techniques. AiSketcher has attended a quite\nnumber of exhibitions and shown remarkable performance under diverse\ncircumstances.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 03:02:24 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 10:23:22 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 07:20:32 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Gao", "Fei", ""], ["Zhu", "Jingjie", ""], ["Yu", "Zeyuan", ""], ["Li", "Peng", ""], ["Wang", "Tao", ""]]}, {"id": "2005.05528", "submitter": "Weiwei Sun", "authors": "Zizhang Wu, Weiwei Sun, Man Wang, Xiaoquan Wang, Lizhu Ding, Fan Wang", "title": "PSDet: Efficient and Universal Parking Slot Detection", "comments": "Accpeted to IV 2020, i.e., the 31st IEEE Intelligent Vehicles\n  Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While real-time parking slot detection plays a critical role in valet parking\nsystems, existing methods have limited success in real-world applications. We\nargue two reasons accounting for the unsatisfactory performance:\n\\romannumeral1, The available datasets have limited diversity, which causes the\nlow generalization ability. \\romannumeral2, Expert knowledge for parking slot\ndetection is under-estimated. Thus, we annotate a large-scale benchmark for\ntraining the network and release it for the benefit of community. Driven by the\nobservation of various parking lots in our benchmark, we propose the circular\ndescriptor to regress the coordinates of parking slot vertexes and accordingly\nlocalize slots accurately. To further boost the performance, we develop a\ntwo-stage deep architecture to localize vertexes in the coarse-to-fine manner.\nIn our benchmark and other datasets, it achieves the state-of-the-art accuracy\nwhile being real-time in practice. Benchmark is available at:\nhttps://github.com/wuzzh/Parking-slot-dataset\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 03:06:25 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Wu", "Zizhang", ""], ["Sun", "Weiwei", ""], ["Wang", "Man", ""], ["Wang", "Xiaoquan", ""], ["Ding", "Lizhu", ""], ["Wang", "Fan", ""]]}, {"id": "2005.05535", "submitter": "Liu Kunlin", "authors": "Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu, Sugasa\n  Marangonda, Chris Um\\'e, Mr. Dpfks, Carl Shift Facenheim, Luis RP, Jian\n  Jiang, Sheng Zhang, Pingyu Wu, Bo Zhou, Weiming Zhang", "title": "DeepFaceLab: Integrated, flexible and extensible face-swapping framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deepfake defense not only requires the research of detection but also\nrequires the efforts of generation methods. However, current deepfake methods\nsuffer the effects of obscure workflow and poor performance. To solve this\nproblem, we present DeepFaceLab, the current dominant deepfake framework for\nface-swapping. It provides the necessary tools as well as an easy-to-use way to\nconduct high-quality face-swapping. It also offers a flexible and loose\ncoupling structure for people who need to strengthen their pipeline with other\nfeatures without writing complicated boilerplate code. We detail the principles\nthat drive the implementation of DeepFaceLab and introduce its pipeline,\nthrough which every aspect of the pipeline can be modified painlessly by users\nto achieve their customization purpose. It is noteworthy that DeepFaceLab could\nachieve cinema-quality results with high fidelity. We demonstrate the advantage\nof our system by comparing our approach with other face-swapping methods.For\nmore information, please visit:https://github.com/iperov/DeepFaceLab/.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 03:26:55 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 09:00:36 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 14:40:07 GMT"}, {"version": "v4", "created": "Wed, 20 May 2020 06:33:52 GMT"}, {"version": "v5", "created": "Tue, 29 Jun 2021 07:07:57 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Perov", "Ivan", ""], ["Gao", "Daiheng", ""], ["Chervoniy", "Nikolay", ""], ["Liu", "Kunlin", ""], ["Marangonda", "Sugasa", ""], ["Um\u00e9", "Chris", ""], ["Dpfks", "Mr.", ""], ["Facenheim", "Carl Shift", ""], ["RP", "Luis", ""], ["Jiang", "Jian", ""], ["Zhang", "Sheng", ""], ["Wu", "Pingyu", ""], ["Zhou", "Bo", ""], ["Zhang", "Weiming", ""]]}, {"id": "2005.05550", "submitter": "Seyed Amir Hossein Hosseini", "authors": "Seyed Amir Hossein Hosseini, Burhaneddin Yaman, Steen Moeller, and\n  Mehmet Ak\\c{c}akaya", "title": "High-Fidelity Accelerated MRI Reconstruction by Scan-Specific\n  Fine-Tuning of Physics-Based Neural Networks", "comments": null, "journal-ref": "Proceedings of IEEE EMBC, 2020", "doi": "10.1109/EMBC44109.2020.9176241", "report-no": null, "categories": "eess.IV cs.CV cs.LG eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long scan duration remains a challenge for high-resolution MRI. Deep learning\nhas emerged as a powerful means for accelerated MRI reconstruction by providing\ndata-driven regularizers that are directly learned from data. These data-driven\npriors typically remain unchanged for future data in the testing phase once\nthey are learned during training. In this study, we propose to use a transfer\nlearning approach to fine-tune these regularizers for new subjects using a\nself-supervision approach. While the proposed approach can compromise the\nextremely fast reconstruction time of deep learning MRI methods, our results on\nknee MRI indicate that such adaptation can substantially reduce the remaining\nartifacts in reconstructed images. In addition, the proposed approach has the\npotential to reduce the risks of generalization to rare pathological\nconditions, which may be unavailable in the training data.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 05:10:10 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Hosseini", "Seyed Amir Hossein", ""], ["Yaman", "Burhaneddin", ""], ["Moeller", "Steen", ""], ["Ak\u00e7akaya", "Mehmet", ""]]}, {"id": "2005.05552", "submitter": "Chengcheng Ma", "authors": "Chengcheng Ma, Baoyuan Wu, Shibiao Xu, Yanbo Fan, Yong Zhang, Xiaopeng\n  Zhang, Zhifeng Li", "title": "Effective and Robust Detection of Adversarial Examples via\n  Benford-Fourier Coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples have been well known as a serious threat to deep neural\nnetworks (DNNs). In this work, we study the detection of adversarial examples,\nbased on the assumption that the output and internal responses of one DNN model\nfor both adversarial and benign examples follow the generalized Gaussian\ndistribution (GGD), but with different parameters (i.e., shape factor, mean,\nand variance). GGD is a general distribution family to cover many popular\ndistributions (e.g., Laplacian, Gaussian, or uniform). It is more likely to\napproximate the intrinsic distributions of internal responses than any specific\ndistribution. Besides, since the shape factor is more robust to different\ndatabases rather than the other two parameters, we propose to construct\ndiscriminative features via the shape factor for adversarial detection,\nemploying the magnitude of Benford-Fourier coefficients (MBF), which can be\neasily estimated using responses. Finally, a support vector machine is trained\nas the adversarial detector through leveraging the MBF features. Extensive\nexperiments in terms of image classification demonstrate that the proposed\ndetector is much more effective and robust on detecting adversarial examples of\ndifferent crafting methods and different sources, compared to state-of-the-art\nadversarial detection methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 05:20:59 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Ma", "Chengcheng", ""], ["Wu", "Baoyuan", ""], ["Xu", "Shibiao", ""], ["Fan", "Yanbo", ""], ["Zhang", "Yong", ""], ["Zhang", "Xiaopeng", ""], ["Li", "Zhifeng", ""]]}, {"id": "2005.05576", "submitter": "Chulhong Kim", "authors": "Sampa Misra, Seungwan Jeon, Seiyon Lee, Ravi Managuli, and Chulhong\n  Kim", "title": "Multi-Channel Transfer Learning of Chest X-ray Images for Screening of\n  COVID-19", "comments": "7 pages, 3 figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2019 novel coronavirus (COVID-19) has spread rapidly all over the world\nand it is affecting the whole society. The current gold standard test for\nscreening COVID-19 patients is the polymerase chain reaction test. However, the\nCOVID-19 test kits are not widely available and time-consuming. Thus, as an\nalternative, chest X-rays are being considered for quick screening. Since the\npresentation of COVID-19 in chest X-rays is varied in features and\nspecialization in reading COVID-19 chest X-rays are required thus limiting its\nuse for diagnosis. To address this challenge of reading chest X-rays by\nradiologists quickly, we present a multi-channel transfer learning model based\non ResNet architecture to facilitate the diagnosis of COVID-19 chest X-ray.\nThree ResNet-based models (Models a, b, and c) were retrained using Dataset_A\n(1579 normal and 4429 diseased), Dataset_B (4245 pneumonia and 1763\nnon-pneumonia), and Dataset_C (184 COVID-19 and 5824 Non-COVID19),\nrespectively, to classify (a) normal or diseased, (b) pneumonia or\nnon-pneumonia, and (c) COVID-19 or non-COVID19. Finally, these three models\nwere ensembled and fine-tuned using Dataset_D (1579 normal, 4245 pneumonia, and\n184 COVID-19) to classify normal, pneumonia, and COVID-19 cases. Our results\nshow that the ensemble model is more accurate than the single ResNet model,\nwhich is also re-trained using Dataset_D as it extracts more relevant semantic\nfeatures for each class. Our approach provides a precision of 94 % and a recall\nof 100%. Thus, our method could potentially help clinicians in screening\npatients for COVID-19, thus facilitating immediate triaging and treatment for\nbetter outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 07:03:46 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Misra", "Sampa", ""], ["Jeon", "Seungwan", ""], ["Lee", "Seiyon", ""], ["Managuli", "Ravi", ""], ["Kim", "Chulhong", ""]]}, {"id": "2005.05592", "submitter": "Bo Xu", "authors": "Bo Xu, Cheng Lu, Yandong Guo and Jacob Wang", "title": "Discriminative Multi-modality Speech Recognition", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision is often used as a complementary modality for audio speech recognition\n(ASR), especially in the noisy environment where performance of solo audio\nmodality significantly deteriorates. After combining visual modality, ASR is\nupgraded to the multi-modality speech recognition (MSR). In this paper, we\npropose a two-stage speech recognition model. In the first stage, the target\nvoice is separated from background noises with help from the corresponding\nvisual information of lip movements, making the model 'listen' clearly. At the\nsecond stage, the audio modality combines visual modality again to better\nunderstand the speech by a MSR sub-network, further improving the recognition\nrate. There are some other key contributions: we introduce a pseudo-3D residual\nconvolution (P3D)-based visual front-end to extract more discriminative\nfeatures; we upgrade the temporal convolution block from 1D ResNet with the\ntemporal convolutional network (TCN), which is more suitable for the temporal\ntasks; the MSR sub-network is built on the top of Element-wise-Attention Gated\nRecurrent Unit (EleAtt-GRU), which is more effective than Transformer in long\nsequences. We conducted extensive experiments on the LRS3-TED and the LRW\ndatasets. Our two-stage model (audio enhanced multi-modality speech\nrecognition, AE-MSR) consistently achieves the state-of-the-art performance by\na significant margin, which demonstrates the necessity and effectiveness of\nAE-MSR.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 07:56:03 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 07:55:21 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Xu", "Bo", ""], ["Lu", "Cheng", ""], ["Guo", "Yandong", ""], ["Wang", "Jacob", ""]]}, {"id": "2005.05594", "submitter": "Ziyi Shen", "authors": "Ziyi Shen, Huazhu Fu, Jianbing Shen and Ling Shao", "title": "Modeling and Enhancing Low-quality Retinal Fundus Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal fundus images are widely used for the clinical screening and\ndiagnosis of eye diseases. However, fundus images captured by operators with\nvarious levels of experience have a large variation in quality. Low-quality\nfundus images increase uncertainty in clinical observation and lead to the risk\nof misdiagnosis. However, due to the special optical beam of fundus imaging and\nstructure of the retina, natural image enhancement methods cannot be utilized\ndirectly to address this. In this paper, we first analyze the ophthalmoscope\nimaging system and simulate a reliable degradation of major inferior-quality\nfactors, including uneven illumination, image blurring, and artifacts. Then,\nbased on the degradation model, a clinically oriented fundus enhancement\nnetwork (cofe-Net) is proposed to suppress global degradation factors, while\nsimultaneously preserving anatomical retinal structures and pathological\ncharacteristics for clinical observation and analysis. Experiments on both\nsynthetic and real images demonstrate that our algorithm effectively corrects\nlow-quality fundus images without losing retinal details. Moreover, we also\nshow that the fundus correction method can benefit medical image analysis\napplications, e.g., retinal vessel segmentation and optic disc/cup detection.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 08:01:16 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 12:33:11 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 10:39:09 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Shen", "Ziyi", ""], ["Fu", "Huazhu", ""], ["Shen", "Jianbing", ""], ["Shao", "Ling", ""]]}, {"id": "2005.05623", "submitter": "David Varas", "authors": "Carlos Roig, David Varas, Issey Masuda, Juan Carlos Riveiro, Elisenda\n  Bou-Balust", "title": "Unsupervised Multi-label Dataset Generation from Web Data", "comments": "The 3rd Workshop on Visual Understanding by Learning from Web Data\n  2019", "journal-ref": "The 3rd Workshop on Visual Understanding by Learning from Web Data\n  (CVPR 2019)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a system towards the generation of multi-label datasets\nfrom web data in an unsupervised manner. To achieve this objective, this work\ncomprises two main contributions, namely: a) the generation of a low-noise\nunsupervised single-label dataset from web-data, and b) the augmentation of\nlabels in such dataset (from single label to multi label). The generation of a\nsingle-label dataset uses an unsupervised noise reduction phase (clustering and\nselection of clusters using anchors) obtaining a 85% of correctly labeled\nimages. An unsupervised label augmentation process is then performed to assign\nnew labels to the images in the dataset using the class activation maps and the\nuncertainty associated with each class. This process is applied to the dataset\ngenerated in this paper and a public dataset (Places365) achieving a 9.5% and\n27% of extra labels in each dataset respectively, therefore demonstrating that\nthe presented system can robustly enrich the initial dataset.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 08:57:59 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Roig", "Carlos", ""], ["Varas", "David", ""], ["Masuda", "Issey", ""], ["Riveiro", "Juan Carlos", ""], ["Bou-Balust", "Elisenda", ""]]}, {"id": "2005.05632", "submitter": "Nils Hulzebosch", "authors": "Nils Hulzebosch, Sarah Ibrahimi, Marcel Worring", "title": "Detecting CNN-Generated Facial Images in Real-World Scenarios", "comments": "Accepted to the workshop on Media Forensics at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial, CNN-generated images are now of such high quality that humans\nhave trouble distinguishing them from real images. Several algorithmic\ndetection methods have been proposed, but these appear to generalize poorly to\ndata from unknown sources, making them infeasible for real-world scenarios. In\nthis work, we present a framework for evaluating detection methods under\nreal-world conditions, consisting of cross-model, cross-data, and\npost-processing evaluation, and we evaluate state-of-the-art detection methods\nusing the proposed framework. Furthermore, we examine the usefulness of\ncommonly used image pre-processing methods. Lastly, we evaluate human\nperformance on detecting CNN-generated images, along with factors that\ninfluence this performance, by conducting an online survey. Our results suggest\nthat CNN-based detection methods are not yet robust enough to be used in\nreal-world scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 09:18:28 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Hulzebosch", "Nils", ""], ["Ibrahimi", "Sarah", ""], ["Worring", "Marcel", ""]]}, {"id": "2005.05650", "submitter": "Mingqing Xiao", "authors": "Mingqing Xiao, Shuxin Zheng, Chang Liu, Yaolong Wang, Di He, Guolin\n  Ke, Jiang Bian, Zhouchen Lin, and Tie-Yan Liu", "title": "Invertible Image Rescaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution digital images are usually downscaled to fit various display\nscreens or save the cost of storage and bandwidth, meanwhile the post-upscaling\nis adpoted to recover the original resolutions or the details in the zoom-in\nimages. However, typical image downscaling is a non-injective mapping due to\nthe loss of high-frequency information, which leads to the ill-posed problem of\nthe inverse upscaling procedure and poses great challenges for recovering\ndetails from the downscaled low-resolution images. Simply upscaling with image\nsuper-resolution methods results in unsatisfactory recovering performance. In\nthis work, we propose to solve this problem by modeling the downscaling and\nupscaling processes from a new perspective, i.e. an invertible bijective\ntransformation, which can largely mitigate the ill-posed nature of image\nupscaling. We develop an Invertible Rescaling Net (IRN) with deliberately\ndesigned framework and objectives to produce visually-pleasing low-resolution\nimages and meanwhile capture the distribution of the lost information using a\nlatent variable following a specified distribution in the downscaling process.\nIn this way, upscaling is made tractable by inversely passing a randomly-drawn\nlatent variable with the low-resolution image through the network. Experimental\nresults demonstrate the significant improvement of our model over existing\nmethods in terms of both quantitative and qualitative evaluations of image\nupscaling reconstruction from downscaled images.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 09:55:53 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Xiao", "Mingqing", ""], ["Zheng", "Shuxin", ""], ["Liu", "Chang", ""], ["Wang", "Yaolong", ""], ["He", "Di", ""], ["Ke", "Guolin", ""], ["Bian", "Jiang", ""], ["Lin", "Zhouchen", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2005.05652", "submitter": "Thomas Tilak", "authors": "Thomas Tilak (1), Arnaud Braun (1), David Chandler (1), Nicolas David\n  (1), Sylvain Galopin (1), Am\\'elie Lombard (2), Micha\\\"el Michaud (1),\n  Camille Parisel (1), Matthieu Porte (1), and Marjorie Robert (1) ((1)\n  Institut National de l'Information G\\'eographique et Foresti\\`ere, (2)\n  CEREMA)", "title": "Very High Resolution Land Cover Mapping of Urban Areas at Global Scale\n  with Convolutional Neural Networks", "comments": "8 pages, 14 figures, ISPRS Archives of the Photogrammetry, Remote\n  Sensing and Spatial Information Sciences", "journal-ref": "XXIV ISPRS Congress, Commission III, Volume XLIII-B3-2020, 2020", "doi": "10.5194/isprs-archives-XLIII-B3-2020-201-2020", "report-no": "Volume XLIII-B3-2020", "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes a methodology to produce a 7-classes land cover map of\nurban areas from very high resolution images and limited noisy labeled data.\nThe objective is to make a segmentation map of a large area (a french\ndepartment) with the following classes: asphalt, bare soil, building,\ngrassland, mineral material (permeable artificialized areas), forest and water\nfrom 20cm aerial images and Digital Height Model. We created a training dataset\non a few areas of interest aggregating databases, semi-automatic\nclassification, and manual annotation to get a complete ground truth in each\nclass. A comparative study of different encoder-decoder architectures (U-Net,\nU-Net with Resnet encoders, Deeplab v3+) is presented with different loss\nfunctions. The final product is a highly valuable land cover map computed from\nmodel predictions stitched together, binarized, and refined before\nvectorization.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 10:03:20 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Tilak", "Thomas", ""], ["Braun", "Arnaud", ""], ["Chandler", "David", ""], ["David", "Nicolas", ""], ["Galopin", "Sylvain", ""], ["Lombard", "Am\u00e9lie", ""], ["Michaud", "Micha\u00ebl", ""], ["Parisel", "Camille", ""], ["Porte", "Matthieu", ""], ["Robert", "Marjorie", ""]]}, {"id": "2005.05659", "submitter": "Max Schwarz", "authors": "Max Schwarz and Sven Behnke", "title": "Stillleben: Realistic Scene Synthesis for Deep Learning in Robotics", "comments": "Accepted for ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training data is the key ingredient for deep learning approaches, but\ndifficult to obtain for the specialized domains often encountered in robotics.\nWe describe a synthesis pipeline capable of producing training data for\ncluttered scene perception tasks such as semantic segmentation, object\ndetection, and correspondence or pose estimation. Our approach arranges object\nmeshes in physically realistic, dense scenes using physics simulation. The\narranged scenes are rendered using high-quality rasterization with randomized\nappearance and material parameters. Noise and other transformations introduced\nby the camera sensors are simulated. Our pipeline can be run online during\ntraining of a deep neural network, yielding applications in life-long learning\nand in iterative render-and-compare approaches. We demonstrate the usability by\nlearning semantic segmentation on the challenging YCB-Video dataset without\nactually using any training frames, where our method achieves performance\ncomparable to a conventionally trained model. Additionally, we show successful\napplication in a real-world regrasping system.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 10:11:00 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Schwarz", "Max", ""], ["Behnke", "Sven", ""]]}, {"id": "2005.05701", "submitter": "Thomas Kurbiel", "authors": "Thomas Kurbiel and Shahrzad Khaleghian", "title": "RetinotopicNet: An Iterative Attention Mechanism Using Local Descriptors\n  with Global Context", "comments": "7 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) were the driving force behind many\nadvancements in Computer Vision research in recent years. This progress has\nspawned many practical applications and we see an increased need to efficiently\nmove CNNs to embedded systems today. However traditional CNNs lack the property\nof scale and rotation invariance: two of the most frequently encountered\ntransformations in natural images. As a consequence CNNs have to learn\ndifferent features for same objects at different scales. This redundancy is the\nmain reason why CNNs need to be very deep in order to achieve the desired\naccuracy. In this paper we develop an efficient solution by reproducing how\nnature has solved the problem in the human brain. To this end we let our CNN\noperate on small patches extracted using the log-polar transform, which is\nknown to be scale and rotation equivariant. Patches extracted in this way have\nthe nice property of magnifying the central field and compressing the\nperiphery. Hence we obtain local descriptors with global context information.\nHowever the processing of a single patch is usually not sufficient to achieve\nhigh accuracies in e.g. classification tasks. We therefore successively jump to\nseveral different locations, called saccades, thus building an understanding of\nthe whole image. Since log-polar patches contain global context information, we\ncan efficiently calculate following saccades using only the small patches.\nSaccades efficiently compensate for the lack of translation equivariance of the\nlog-polar transform.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 11:54:56 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Kurbiel", "Thomas", ""], ["Khaleghian", "Shahrzad", ""]]}, {"id": "2005.05705", "submitter": "Sofiane Horache", "authors": "Sofiane Horache and Jean-Emmanuel Deschaud and Fran\\c{c}ois Goulette\n  and Katherine Gruel and Thierry Lejars", "title": "Automatic clustering of Celtic coins based on 3D point cloud pattern\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recognition and clustering of coins which have been struck by the same\ndie is of interest for archeological studies. Nowadays, this work can only be\nperformed by experts and is very tedious. In this paper, we propose a method to\nautomatically cluster dies, based on 3D scans of coins. It is based on three\nsteps: registration, comparison and graph-based clustering. Experimental\nresults on 90 coins coming from a Celtic treasury from the II-Ith century BC\nshow a clustering quality equivalent to expert's work.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 11:59:16 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Horache", "Sofiane", ""], ["Deschaud", "Jean-Emmanuel", ""], ["Goulette", "Fran\u00e7ois", ""], ["Gruel", "Katherine", ""], ["Lejars", "Thierry", ""]]}, {"id": "2005.05708", "submitter": "Danila Rukhovich", "authors": "Danila Rukhovich, Konstantin Sofiiuk, Danil Galeev, Olga Barinova,\n  Anton Konushin", "title": "IterDet: Iterative Scheme for Object Detection in Crowded Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based detectors usually produce a redundant set of object\nbounding boxes including many duplicate detections of the same object. These\nboxes are then filtered using non-maximum suppression (NMS) in order to select\nexactly one bounding box per object of interest. This greedy scheme is simple\nand provides sufficient accuracy for isolated objects but often fails in\ncrowded environments, since one needs to both preserve boxes for different\nobjects and suppress duplicate detections. In this work we develop an\nalternative iterative scheme, where a new subset of objects is detected at each\niteration. Detected boxes from the previous iterations are passed to the\nnetwork at the following iterations to ensure that the same object would not be\ndetected twice. This iterative scheme can be applied to both one-stage and\ntwo-stage object detectors with just minor modifications of the training and\ninference procedures. We perform extensive experiments with two different\nbaseline detectors on four datasets and show significant improvement over the\nbaseline, leading to state-of-the-art performance on CrowdHuman and WiderPerson\ndatasets. The source code and the trained models are available at\nhttps://github.com/saic-vul/iterdet.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 12:04:27 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 07:26:14 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Rukhovich", "Danila", ""], ["Sofiiuk", "Konstantin", ""], ["Galeev", "Danil", ""], ["Barinova", "Olga", ""], ["Konushin", "Anton", ""]]}, {"id": "2005.05732", "submitter": "Kfir Aberman", "authors": "Kfir Aberman, Peizhuo Li, Dani Lischinski, Olga Sorkine-Hornung,\n  Daniel Cohen-Or, Baoquan Chen", "title": "Skeleton-Aware Networks for Deep Motion Retargeting", "comments": "SIGGRAPH 2020. Project page:\n  https://deepmotionediting.github.io/retargeting , Video:\n  https://www.youtube.com/watch?v=ym8Tnmiz5N8", "journal-ref": null, "doi": "10.1145/3386569.3392462", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel deep learning framework for data-driven motion\nretargeting between skeletons, which may have different structure, yet\ncorresponding to homeomorphic graphs. Importantly, our approach learns how to\nretarget without requiring any explicit pairing between the motions in the\ntraining set. We leverage the fact that different homeomorphic skeletons may be\nreduced to a common primal skeleton by a sequence of edge merging operations,\nwhich we refer to as skeletal pooling. Thus, our main technical contribution is\nthe introduction of novel differentiable convolution, pooling, and unpooling\noperators. These operators are skeleton-aware, meaning that they explicitly\naccount for the skeleton's hierarchical structure and joint adjacency, and\ntogether they serve to transform the original motion into a collection of deep\ntemporal features associated with the joints of the primal skeleton. In other\nwords, our operators form the building blocks of a new deep motion processing\nframework that embeds the motion into a common latent space, shared by a\ncollection of homeomorphic skeletons. Thus, retargeting can be achieved simply\nby encoding to, and decoding from this latent space. Our experiments show the\neffectiveness of our framework for motion retargeting, as well as motion\nprocessing in general, compared to existing approaches. Our approach is also\nquantitatively evaluated on a synthetic dataset that contains pairs of motions\napplied to different skeletons. To the best of our knowledge, our method is the\nfirst to perform retargeting between skeletons with differently sampled\nkinematic chains, without any paired examples.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 12:51:40 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Aberman", "Kfir", ""], ["Li", "Peizhuo", ""], ["Lischinski", "Dani", ""], ["Sorkine-Hornung", "Olga", ""], ["Cohen-Or", "Daniel", ""], ["Chen", "Baoquan", ""]]}, {"id": "2005.05740", "submitter": "Yitian Li", "authors": "Yitian Li and Ruini Xue and Mengmeng Zhu and Qing Xu and Zenglin Xu", "title": "ReadNet:Towards Accurate ReID with Limited and Noisy Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) is an essential cross-camera retrieval task\nto identify pedestrians. However, the photo number of each pedestrian usually\ndiffers drastically, and thus the data limitation and imbalance problem hinders\nthe prediction accuracy greatly. Additionally, in real-world applications,\npedestrian images are captured by different surveillance cameras, so the noisy\ncamera related information, such as the lights, perspectives and resolutions,\nresult in inevitable domain gaps for ReID algorithms. These challenges bring\ndifficulties to current deep learning methods with triplet loss for coping with\nsuch problems. To address these challenges, this paper proposes ReadNet, an\nadversarial camera network (ACN) with an angular triplet loss (ATL). In detail,\nATL focuses on learning the angular distance among different identities to\nmitigate the effect of data imbalance, and guarantees a linear decision\nboundary as well, while ACN takes the camera discriminator as a game opponent\nof feature extractor to filter camera related information to bridge the\nmulti-camera gaps. ReadNet is designed to be flexible so that either ATL or ACN\ncan be deployed independently or simultaneously. The experiment results on\nvarious benchmark datasets have shown that ReadNet can deliver better\nprediction performance than current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 13:14:15 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Li", "Yitian", ""], ["Xue", "Ruini", ""], ["Zhu", "Mengmeng", ""], ["Xu", "Qing", ""], ["Xu", "Zenglin", ""]]}, {"id": "2005.05751", "submitter": "Kfir Aberman", "authors": "Kfir Aberman, Yijia Weng, Dani Lischinski, Daniel Cohen-Or, Baoquan\n  Chen", "title": "Unpaired Motion Style Transfer from Video to Animation", "comments": "SIGGRAPH 2020. Project page:\n  https://deepmotionediting.github.io/style_transfer , Video:\n  https://www.youtube.com/watch?v=m04zuBSdGrc , Code:\n  https://github.com/DeepMotionEditing/deep-motion-editing", "journal-ref": null, "doi": "10.1145/3386569.3392469", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring the motion style from one animation clip to another, while\npreserving the motion content of the latter, has been a long-standing problem\nin character animation. Most existing data-driven approaches are supervised and\nrely on paired data, where motions with the same content are performed in\ndifferent styles. In addition, these approaches are limited to transfer of\nstyles that were seen during training. In this paper, we present a novel\ndata-driven framework for motion style transfer, which learns from an unpaired\ncollection of motions with style labels, and enables transferring motion styles\nnot observed during training. Furthermore, our framework is able to extract\nmotion styles directly from videos, bypassing 3D reconstruction, and apply them\nto the 3D input motion. Our style transfer network encodes motions into two\nlatent codes, for content and for style, each of which plays a different role\nin the decoding (synthesis) process. While the content code is decoded into the\noutput motion by several temporal convolutional layers, the style code modifies\ndeep features via temporally invariant adaptive instance normalization (AdaIN).\nMoreover, while the content code is encoded from 3D joint rotations, we learn a\ncommon embedding for style from either 3D or 2D joint positions, enabling style\nextraction from videos. Our results are comparable to the state-of-the-art,\ndespite not requiring paired training data, and outperform other methods when\ntransferring previously unseen styles. To our knowledge, we are the first to\ndemonstrate style transfer directly from videos to 3D animations - an ability\nwhich enables one to extend the set of style examples far beyond motions\ncaptured by MoCap systems.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 13:21:27 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Aberman", "Kfir", ""], ["Weng", "Yijia", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Daniel", ""], ["Chen", "Baoquan", ""]]}, {"id": "2005.05761", "submitter": "Samira Masoudi", "authors": "Samira Masoudi, Syed M. Anwar, Stephanie A. Harmon, Peter L. Choyke,\n  Baris Turkbey, Ulas Bagci", "title": "Adipose Tissue Segmentation in Unlabeled Abdomen MRI using Cross\n  Modality Domain Adaptation", "comments": "5 pages,7 figures, EMBC 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abdominal fat quantification is critical since multiple vital organs are\nlocated within this region. Although computed tomography (CT) is a highly\nsensitive modality to segment body fat, it involves ionizing radiations which\nmakes magnetic resonance imaging (MRI) a preferable alternative for this\npurpose. Additionally, the superior soft tissue contrast in MRI could lead to\nmore accurate results. Yet, it is highly labor intensive to segment fat in MRI\nscans. In this study, we propose an algorithm based on deep learning\ntechnique(s) to automatically quantify fat tissue from MR images through a\ncross modality adaptation. Our method does not require supervised labeling of\nMR scans, instead, we utilize a cycle generative adversarial network (C-GAN) to\nconstruct a pipeline that transforms the existing MR scans into their\nequivalent synthetic CT (s-CT) images where fat segmentation is relatively\neasier due to the descriptive nature of HU (hounsfield unit) in CT images. The\nfat segmentation results for MRI scans were evaluated by expert radiologist.\nQualitative evaluation of our segmentation results shows average success score\nof 3.80/5 and 4.54/5 for visceral and subcutaneous fat segmentation in MR\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:41:39 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Masoudi", "Samira", ""], ["Anwar", "Syed M.", ""], ["Harmon", "Stephanie A.", ""], ["Choyke", "Peter L.", ""], ["Turkbey", "Baris", ""], ["Bagci", "Ulas", ""]]}, {"id": "2005.05776", "submitter": "Xiyang Liu", "authors": "Xiyang Liu, Jie Yang, Wenrui Ding", "title": "Adaptive Mixture Regression Network with Local Counting Map for Crowd\n  Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crowd counting task aims at estimating the number of people located in an\nimage or a frame from videos. Existing methods widely adopt density maps as the\ntraining targets to optimize the point-to-point loss. While in testing phase,\nwe only focus on the differences between the crowd numbers and the global\nsummation of density maps, which indicate the inconsistency between the\ntraining targets and the evaluation criteria. To solve this problem, we\nintroduce a new target, named local counting map (LCM), to obtain more accurate\nresults than density map based approaches. Moreover, we also propose an\nadaptive mixture regression framework with three modules in a coarse-to-fine\nmanner to further improve the precision of the crowd estimation: scale-aware\nmodule (SAM), mixture regression module (MRM) and adaptive soft interval module\n(ASIM). Specifically, SAM fully utilizes the context and multi-scale\ninformation from different convolutional features; MRM and ASIM perform more\nprecise counting regression on local patches of images. Compared with current\nmethods, the proposed method reports better performances on the typical\ndatasets. The source code is available at\nhttps://github.com/xiyang1012/Local-Crowd-Counting.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 13:54:05 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 06:53:41 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Liu", "Xiyang", ""], ["Yang", "Jie", ""], ["Ding", "Wenrui", ""]]}, {"id": "2005.05777", "submitter": "Axel Barroso Laguna", "authors": "Axel Barroso-Laguna, Yannick Verdie, Benjamin Busam, Krystian\n  Mikolajczyk", "title": "HDD-Net: Hybrid Detector Descriptor with Mutual Interactive Learning", "comments": null, "journal-ref": "Asian Conference on Computer Vision (ACCV), 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local feature extraction remains an active research area due to the advances\nin fields such as SLAM, 3D reconstructions, or AR applications. The success in\nthese applications relies on the performance of the feature detector and\ndescriptor. While the detector-descriptor interaction of most methods is based\non unifying in single network detections and descriptors, we propose a method\nthat treats both extractions independently and focuses on their interaction in\nthe learning process rather than by parameter sharing. We formulate the\nclassical hard-mining triplet loss as a new detector optimisation term to\nrefine candidate positions based on the descriptor map. We propose a dense\ndescriptor that uses a multi-scale approach and a hybrid combination of\nhand-crafted and learned features to obtain rotation and scale robustness by\ndesign. We evaluate our method extensively on different benchmarks and show\nimprovements over the state of the art in terms of image matching on HPatches\nand 3D reconstruction quality while keeping on par on camera localisation\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 13:55:04 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 09:14:34 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Barroso-Laguna", "Axel", ""], ["Verdie", "Yannick", ""], ["Busam", "Benjamin", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "2005.05815", "submitter": "Aditya M. Deshpande", "authors": "Aditya M. Deshpande and Ali A. Minai and Manish Kumar", "title": "One-Shot Recognition of Manufacturing Defects in Steel Surfaces", "comments": "Accepted for publication in NAMRC 48", "journal-ref": "Procedia Manufacturing 48 (2020) 1064-1071", "doi": "10.1016/j.promfg.2020.05.146", "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality control is an essential process in manufacturing to make the product\ndefect-free as well as to meet customer needs. The automation of this process\nis important to maintain high quality along with the high manufacturing\nthroughput. With recent developments in deep learning and computer vision\ntechnologies, it has become possible to detect various features from the images\nwith near-human accuracy. However, many of these approaches are data intensive.\nTraining and deployment of such a system on manufacturing floors may become\nexpensive and time-consuming. The need for large amounts of training data is\none of the limitations of the applicability of these approaches in real-world\nmanufacturing systems. In this work, we propose the application of a Siamese\nconvolutional neural network to do one-shot recognition for such a task. Our\nresults demonstrate how one-shot learning can be used in quality control of\nsteel by identification of defects on the steel surface. This method can\nsignificantly reduce the requirements of training data and can also be run in\nreal-time.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 14:30:03 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Deshpande", "Aditya M.", ""], ["Minai", "Ali A.", ""], ["Kumar", "Manish", ""]]}, {"id": "2005.05824", "submitter": "Aysan Aghazadeh", "authors": "Aysan Aghazadeh, Maryam Amirmazlaghani", "title": "A Distributed Approximate Nearest Neighbor Method for Real-Time Face\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, face recognition and more generally image recognition have many\napplications in the modern world and are widely used in our daily tasks. This\npaper aims to propose a distributed approximate nearest neighbor (ANN) method\nfor real-time face recognition using a big dataset that involves a lot of\nclasses. The proposed approach is based on using a clustering method to\nseparate the dataset into different clusters and on specifying the importance\nof each cluster by defining cluster weights. To this end, reference instances\nare selected from each cluster based on the cluster weights using a maximum\nlikelihood approach. This process leads to a more informed selection of\ninstances, so it enhances the performance of the algorithm. Experimental\nresults confirm the efficiency of the proposed method and its out-performance\nin terms of accuracy and the processing time.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 14:39:31 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 22:14:18 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Aghazadeh", "Aysan", ""], ["Amirmazlaghani", "Maryam", ""]]}, {"id": "2005.05839", "submitter": "Zixiang Zhao", "authors": "Zixiang Zhao, Shuang Xu, Chunxia Zhang, Junmin Liu, Jiangshe Zhang", "title": "Bayesian Fusion for Infrared and Visible Images", "comments": null, "journal-ref": null, "doi": "10.1016/j.sigpro.2020.107734", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared and visible image fusion has been a hot issue in image fusion. In\nthis task, a fused image containing both the gradient and detailed texture\ninformation of visible images as well as the thermal radiation and highlighting\ntargets of infrared images is expected to be obtained. In this paper, a novel\nBayesian fusion model is established for infrared and visible images. In our\nmodel, the image fusion task is cast into a regression problem. To measure the\nvariable uncertainty, we formulate the model in a hierarchical Bayesian manner.\nAiming at making the fused image satisfy human visual system, the model\nincorporates the total-variation(TV) penalty. Subsequently, the model is\nefficiently inferred by the expectation-maximization(EM) algorithm. We test our\nalgorithm on TNO and NIR image fusion datasets with several state-of-the-art\napproaches. Compared with the previous methods, the novel model can generate\nbetter fused images with high-light targets and rich texture details, which can\nimprove the reliability of the target automatic detection and recognition\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 14:57:19 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Zhao", "Zixiang", ""], ["Xu", "Shuang", ""], ["Zhang", "Chunxia", ""], ["Liu", "Junmin", ""], ["Zhang", "Jiangshe", ""]]}, {"id": "2005.05856", "submitter": "Philipe A. Dias", "authors": "Philipe A. Dias and Henry Medeiros", "title": "Probabilistic Semantic Segmentation Refinement by Monte Carlo Region\n  Growing", "comments": "Submitted to IEEE Transactions on Image Processing (April 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation with fine-grained pixel-level accuracy is a fundamental\ncomponent of a variety of computer vision applications. However, despite the\nlarge improvements provided by recent advances in the architectures of\nconvolutional neural networks, segmentations provided by modern\nstate-of-the-art methods still show limited boundary adherence. We introduce a\nfully unsupervised post-processing algorithm that exploits Monte Carlo sampling\nand pixel similarities to propagate high-confidence pixel labels into regions\nof low-confidence classification. Our algorithm, which we call probabilistic\nRegion Growing Refinement (pRGR), is based on a rigorous mathematical\nfoundation in which clusters are modelled as multivariate normally distributed\nsets of pixels. Exploiting concepts of Bayesian estimation and variance\nreduction techniques, pRGR performs multiple refinement iterations at varied\nreceptive fields sizes, while updating cluster statistics to adapt to local\nimage features. Experiments using multiple modern semantic segmentation\nnetworks and benchmark datasets demonstrate the effectiveness of our approach\nfor the refinement of segmentation predictions at different levels of\ncoarseness, as well as the suitability of the variance estimates obtained in\nthe Monte Carlo iterations as uncertainty measures that are highly correlated\nwith segmentation accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 15:23:57 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Dias", "Philipe A.", ""], ["Medeiros", "Henry", ""]]}, {"id": "2005.05859", "submitter": "Vishnu Naresh Boddeti", "authors": "Zhichao Lu, Gautam Sreekumar, Erik Goodman, Wolfgang Banzhaf,\n  Kalyanmoy Deb, and Vishnu Naresh Boddeti", "title": "Neural Architecture Transfer", "comments": "Code is available at\n  https://github.com/human-analysis/neural-architecture-transfer", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2021", "doi": "10.1109/TPAMI.2021.3052758", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) has emerged as a promising avenue for\nautomatically designing task-specific neural networks. Existing NAS approaches\nrequire one complete search for each deployment specification of hardware or\nobjective. This is a computationally impractical endeavor given the potentially\nlarge number of application scenarios. In this paper, we propose Neural\nArchitecture Transfer (NAT) to overcome this limitation. NAT is designed to\nefficiently generate task-specific custom models that are competitive under\nmultiple conflicting objectives. To realize this goal we learn task-specific\nsupernets from which specialized subnets can be sampled without any additional\ntraining. The key to our approach is an integrated online transfer learning and\nmany-objective evolutionary search procedure. A pre-trained supernet is\niteratively adapted while simultaneously searching for task-specific subnets.\nWe demonstrate the efficacy of NAT on 11 benchmark image classification tasks\nranging from large-scale multi-class to small-scale fine-grained datasets. In\nall cases, including ImageNet, NATNets improve upon the state-of-the-art under\nmobile settings ($\\leq$ 600M Multiply-Adds). Surprisingly, small-scale\nfine-grained datasets benefit the most from NAT. At the same time, the\narchitecture search and transfer is orders of magnitude more efficient than\nexisting NAS methods. Overall, the experimental evaluation indicates that,\nacross diverse image classification tasks and computational objectives, NAT is\nan appreciably more effective alternative to conventional transfer learning of\nfine-tuning weights of an existing network architecture learned on standard\ndatasets. Code is available at\nhttps://github.com/human-analysis/neural-architecture-transfer\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 15:30:36 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 00:32:53 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lu", "Zhichao", ""], ["Sreekumar", "Gautam", ""], ["Goodman", "Erik", ""], ["Banzhaf", "Wolfgang", ""], ["Deb", "Kalyanmoy", ""], ["Boddeti", "Vishnu Naresh", ""]]}, {"id": "2005.05868", "submitter": "Neil Getty", "authors": "Neil Getty, Zixuan Zhao, Stephan Gruessner, Liaohai Chen, Fangfang Xia", "title": "Recurrent and Spiking Modeling of Sparse Surgical Kinematics", "comments": "5 pages, 8 figures, accepted ICONS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot-assisted minimally invasive surgery is improving surgeon performance\nand patient outcomes. This innovation is also turning what has been a\nsubjective practice into motion sequences that can be precisely measured. A\ngrowing number of studies have used machine learning to analyze video and\nkinematic data captured from surgical robots. In these studies, models are\ntypically trained on benchmark datasets for representative surgical tasks to\nassess surgeon skill levels. While they have shown that novices and experts can\nbe accurately classified, it is not clear whether machine learning can separate\nhighly proficient surgeons from one another, especially without video data. In\nthis study, we explore the possibility of using only kinematic data to predict\nsurgeons of similar skill levels. We focus on a new dataset created from\nsurgical exercises on a simulation device for skill training. A simple,\nefficient encoding scheme was devised to encode kinematic sequences so that\nthey were amenable to edge learning. We report that it is possible to identify\nsurgical fellows receiving near perfect scores in the simulation exercises\nbased on their motion characteristics alone. Further, our model could be\nconverted to a spiking neural network to train and infer on the Nengo\nsimulation framework with no loss in accuracy. Overall, this study suggests\nthat building neuromorphic models from sparse motion features may be a\npotentially useful strategy for identifying surgeons and gestures with chips\ndeployed on robotic systems to offer adaptive assistance during surgery and\ntraining with additional latency and privacy benefits.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 15:41:45 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 16:01:48 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Getty", "Neil", ""], ["Zhao", "Zixuan", ""], ["Gruessner", "Stephan", ""], ["Chen", "Liaohai", ""], ["Xia", "Fangfang", ""]]}, {"id": "2005.05878", "submitter": "Shan Gu", "authors": "Shan Gu, Jianjiang Feng, Jiwen Lu, Jie Zhou", "title": "Latent Fingerprint Registration via Matching Densely Sampled Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent fingerprint matching is a very important but unsolved problem. As a\nkey step of fingerprint matching, fingerprint registration has a great impact\non the recognition performance. Existing latent fingerprint registration\napproaches are mainly based on establishing correspondences between minutiae,\nand hence will certainly fail when there are no sufficient number of extracted\nminutiae due to small fingerprint area or poor image quality. Minutiae\nextraction has become the bottleneck of latent fingerprint registration. In\nthis paper, we propose a non-minutia latent fingerprint registration method\nwhich estimates the spatial transformation between a pair of fingerprints\nthrough a dense fingerprint patch alignment and matching procedure. Given a\npair of fingerprints to match, we bypass the minutiae extraction step and take\nuniformly sampled points as key points. Then the proposed patch alignment and\nmatching algorithm compares all pairs of sampling points and produces their\nsimilarities along with alignment parameters. Finally, a set of consistent\ncorrespondences are found by spectral clustering. Extensive experiments on\nNIST27 database and MOLF database show that the proposed method achieves the\nstate-of-the-art registration performance, especially under challenging\nconditions.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 15:51:59 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 02:54:13 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Gu", "Shan", ""], ["Feng", "Jianjiang", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2005.05896", "submitter": "Zixiang Zhao", "authors": "Zixiang Zhao, Shuang Xu, Jiangshe Zhang, Chengyang Liang, Chunxia\n  Zhang, Junmin Liu", "title": "Efficient and Model-Based Infrared and Visible Image Fusion Via\n  Algorithm Unrolling", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared and visible image fusion (IVIF) expects to obtain images that retain\nthermal radiation information from infrared images and texture details from\nvisible images. In this paper, a model-based convolutional neural network (CNN)\nmodel, referred to as Algorithm Unrolling Image Fusion (AUIF), is proposed to\novercome the shortcomings of traditional CNN-based IVIF models. The proposed\nAUIF model starts with the iterative formulas of two traditional optimization\nmodels, which are established to accomplish two-scale decomposition, i.e.,\nseparating low-frequency base information and high-frequency detail information\nfrom source images. Then the algorithm unrolling is implemented where each\niteration is mapped to a CNN layer and each optimization model is transformed\ninto a trainable neural network. Compared with the general network\narchitectures, the proposed framework combines the model-based prior\ninformation and is designed more reasonably. After the unrolling operation, our\nmodel contains two decomposers (encoders) and an additional reconstructor\n(decoder). In the training phase, this network is trained to reconstruct the\ninput image. While in the test phase, the base (or detail) decomposed feature\nmaps of infrared/visible images are merged respectively by an extra fusion\nlayer, and then the decoder outputs the fusion image. Qualitative and\nquantitative comparisons demonstrate the superiority of our model, which can\nrobustly generate fusion images containing highlight targets and legible\ndetails, exceeding the state-of-the-art methods. Furthermore, our network has\nfewer weights and faster speed.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 16:15:56 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 12:04:44 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Zhao", "Zixiang", ""], ["Xu", "Shuang", ""], ["Zhang", "Jiangshe", ""], ["Liang", "Chengyang", ""], ["Zhang", "Chunxia", ""], ["Liu", "Junmin", ""]]}, {"id": "2005.05930", "submitter": "Mantas Luko\\v{s}evi\\v{c}ius", "authors": "Arnas Uselis, Mantas Luko\\v{s}evi\\v{c}ius, Lukas Stasytis", "title": "Localized convolutional neural networks for geospatial wind forecasting", "comments": null, "journal-ref": "Energies, 13 (13), pp. 3440, 2020", "doi": "10.3390/en13133440", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) possess many positive qualities when it\ncomes to spatial raster data. Translation invariance enables CNNs to detect\nfeatures regardless of their position in the scene. However, in some domains,\nlike geospatial, not all locations are exactly equal. In this work, we propose\nlocalized convolutional neural networks that enable convolutional architectures\nto learn local features in addition to the global ones. We investigate their\ninstantiations in the form of learnable inputs, local weights, and a more\ngeneral form. They can be added to any convolutional layers, easily end-to-end\ntrained, introduce minimal additional complexity, and let CNNs retain most of\ntheir benefits to the extent that they are needed. In this work we address\nspatio-temporal prediction: test the effectiveness of our methods on a\nsynthetic benchmark dataset and tackle three real-world wind prediction\ndatasets. For one of them, we propose a method to spatially order the unordered\ndata. We compare the recent state-of-the-art spatio-temporal prediction models\non the same data. Models that use convolutional layers can be and are extended\nwith our localizations. In all these cases our extensions improve the results,\nand thus often the state-of-the-art. We share all the code at a public\nrepository.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 17:14:49 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 14:56:21 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 16:13:17 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Uselis", "Arnas", ""], ["Luko\u0161evi\u010dius", "Mantas", ""], ["Stasytis", "Lukas", ""]]}, {"id": "2005.05960", "submitter": "Deepak Pathak", "authors": "Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar\n  Hafner, Deepak Pathak", "title": "Planning to Explore via Self-Supervised World Models", "comments": "Accepted at ICML 2020. Videos and code at\n  https://ramanans1.github.io/plan2explore/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning allows solving complex tasks, however, the learning\ntends to be task-specific and the sample efficiency remains a challenge. We\npresent Plan2Explore, a self-supervised reinforcement learning agent that\ntackles both these challenges through a new approach to self-supervised\nexploration and fast adaptation to new tasks, which need not be known during\nexploration. During exploration, unlike prior methods which retrospectively\ncompute the novelty of observations after the agent has already reached them,\nour agent acts efficiently by leveraging planning to seek out expected future\nnovelty. After exploration, the agent quickly adapts to multiple downstream\ntasks in a zero or a few-shot manner. We evaluate on challenging control tasks\nfrom high-dimensional image inputs. Without any training supervision or\ntask-specific interaction, Plan2Explore outperforms prior self-supervised\nexploration methods, and in fact, almost matches the performances oracle which\nhas access to rewards. Videos and code at\nhttps://ramanans1.github.io/plan2explore/\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 17:59:45 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 23:05:50 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Sekar", "Ramanan", ""], ["Rybkin", "Oleh", ""], ["Daniilidis", "Kostas", ""], ["Abbeel", "Pieter", ""], ["Hafner", "Danijar", ""], ["Pathak", "Deepak", ""]]}, {"id": "2005.05999", "submitter": "Sourya Dipta Das", "authors": "Sourya Dipta Das, Saikat Dutta", "title": "Fast Deep Multi-patch Hierarchical Network for Nonhomogeneous Image\n  Dehazing", "comments": "CVPR Workshops Proceedings 2020", "journal-ref": "2020 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW)", "doi": "10.1109/CVPRW50498.2020.00249", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, CNN based end-to-end deep learning methods achieve superiority in\nImage Dehazing but they tend to fail drastically in Non-homogeneous dehazing.\nApart from that, existing popular Multi-scale approaches are runtime intensive\nand memory inefficient. In this context, we proposed a fast Deep Multi-patch\nHierarchical Network to restore Non-homogeneous hazed images by aggregating\nfeatures from multiple image patches from different spatial sections of the\nhazed image with fewer number of network parameters. Our proposed method is\nquite robust for different environments with various density of the haze or fog\nin the scene and very lightweight as the total size of the model is around 21.7\nMB. It also provides faster runtime compared to current multi-scale methods\nwith an average runtime of 0.0145s to process 1200x1600 HD quality image.\nFinally, we show the superiority of this network on Dense Haze Removal to other\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 18:26:51 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Das", "Sourya Dipta", ""], ["Dutta", "Saikat", ""]]}, {"id": "2005.06023", "submitter": "Benedetta Tondi", "authors": "Wenjie Li, Benedetta Tondi, Rongrong Ni and Mauro Barni", "title": "Increased-confidence adversarial examples for improved transferability\n  of Counter-Forensic attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferability of adversarial examples is a key issue to study the security\nof multimedia forensics (MMF) techniques relying on Deep Learning (DL). The\ntransferability of the attacks, in fact, would open the way to the deployment\nof successful counter forensics attacks also in cases where the attacker does\nnot have a full knowledge of the to-be-attacked system. Some preliminary works\nhave shown that adversarial examples against CNN-based image forensics\ndetectors are in general non-transferrable, at least when the basic versions of\nthe attacks implemented in the most popular attack packages are adopted. In\nthis paper, we introduce a general strategy to increase the strength of the\nattacks and evaluate the transferability of the adversarial examples when such\na strength varies. We experimentally show that, in this way, attack\ntransferability can be improved to a large extent, at the expense of a larger\ndistortion. Our research confirms the security threats posed by the existence\nof adversarial examples even in multimedia forensics scenarios, thus calling\nfor new defense strategies to improve the security of DL-based MMF techniques.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 19:29:03 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Li", "Wenjie", ""], ["Tondi", "Benedetta", ""], ["Ni", "Rongrong", ""], ["Barni", "Mauro", ""]]}, {"id": "2005.06037", "submitter": "Aditya M. Deshpande", "authors": "Aditya M. Deshpande, Anil Kumar Telikicherla, Vinay Jakkali, David A.\n  Wickelhaus, Manish Kumar, and Sam Anand", "title": "Computer Vision Toolkit for Non-invasive Monitoring of Factory Floor\n  Artifacts", "comments": "Accepted for publication in 48th SME North American Manufacturing\n  Research Conference (NAMRC48)", "journal-ref": "Procedia Manufacturing 48 (2020) 1020-1028", "doi": "10.1016/j.promfg.2020.05.141", "report-no": null, "categories": "cs.CV cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digitization has led to smart, connected technologies be an integral part of\nbusinesses, governments and communities. For manufacturing digitization, there\nhas been active research and development with a focus on Cloud Manufacturing\n(CM) and the Industrial Internet of Things (IIoT). This work presents a\ncomputer vision toolkit (CV Toolkit) for non-invasive digitization of the\nfactory floor in line with Industry 4.0 requirements for factory data\ncollection. Currently, technical challenges persist towards digitization of\nlegacy systems due to the limitation for changes in their design and sensors.\nThis novel toolkit is developed to facilitate easy integration of legacy\nproduction machinery and factory floor artifacts with the digital and smart\nmanufacturing environment with no requirement of any physical changes in the\nmachines. The system developed is modular, and allows real-time monitoring of\nproduction machinery. Modularity aspect allows the incorporation of new\nsoftware applications in the current framework of CV Toolkit. To allow\nconnectivity of this toolkit with manufacturing floors in a simple, deployable\nand cost-effective manner, the toolkit is integrated with a known manufacturing\ndata standard, MTConnect, to \"translate\" the digital inputs into data streams\nthat can be read by commercial status tracking and reporting software\nsolutions. The proposed toolkit is demonstrated using a mock-panel environment\ndeveloped in house at the University of Cincinnati to highlight its usability.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 20:25:34 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Deshpande", "Aditya M.", ""], ["Telikicherla", "Anil Kumar", ""], ["Jakkali", "Vinay", ""], ["Wickelhaus", "David A.", ""], ["Kumar", "Manish", ""], ["Anand", "Sam", ""]]}, {"id": "2005.06038", "submitter": "Krishna Somandepalli", "authors": "Krishna Somandepalli and Shrikanth Narayanan", "title": "Generalized Multi-view Shared Subspace Learning using View Bootstrapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SD eess.AS eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key objective in multi-view learning is to model the information common to\nmultiple parallel views of a class of objects/events to improve downstream\nlearning tasks. In this context, two open research questions remain: How can we\nmodel hundreds of views per event? Can we learn robust multi-view embeddings\nwithout any knowledge of how these views are acquired? We present a neural\nmethod based on multi-view correlation to capture the information shared across\na large number of views by subsampling them in a view-agnostic manner during\ntraining. To provide an upper bound on the number of views to subsample for a\ngiven embedding dimension, we analyze the error of the bootstrapped multi-view\ncorrelation objective using matrix concentration theory. Our experiments on\nspoken word recognition, 3D object classification and pose-invariant face\nrecognition demonstrate the robustness of view bootstrapping to model a large\nnumber of views. Results underscore the applicability of our method for a\nview-agnostic learning setting.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 20:35:14 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Somandepalli", "Krishna", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2005.06040", "submitter": "Hui Ding", "authors": "Hui Ding, Peng Zhou, and Rama Chellappa", "title": "Occlusion-Adaptive Deep Network for Robust Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing the expressions of partially occluded faces is a challenging\ncomputer vision problem. Previous expression recognition methods, either\noverlooked this issue or resolved it using extreme assumptions. Motivated by\nthe fact that the human visual system is adept at ignoring the occlusion and\nfocus on non-occluded facial areas, we propose a landmark-guided attention\nbranch to find and discard corrupted features from occluded regions so that\nthey are not used for recognition. An attention map is first generated to\nindicate if a specific facial part is occluded and guide our model to attend to\nnon-occluded regions. To further improve robustness, we propose a facial region\nbranch to partition the feature maps into non-overlapping facial blocks and\ntask each block to predict the expression independently. This results in more\ndiverse and discriminative features, enabling the expression recognition system\nto recover even though the face is partially occluded. Depending on the\nsynergistic effects of the two branches, our occlusion-adaptive deep network\nsignificantly outperforms state-of-the-art methods on two challenging\nin-the-wild benchmark datasets and three real-world occluded expression\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 20:42:55 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Ding", "Hui", ""], ["Zhou", "Peng", ""], ["Chellappa", "Rama", ""]]}, {"id": "2005.06047", "submitter": "Yixiong Zou", "authors": "Yixiong Zou, Shanghang Zhang, Ke Chen, Yonghong Tian, Yaowei Wang,\n  Jos\\'e M. F. Moura", "title": "Compositional Few-Shot Recognition with Primitive Discovery and\n  Enhancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning (FSL) aims at recognizing novel classes given only few\ntraining samples, which still remains a great challenge for deep learning.\nHowever, humans can easily recognize novel classes with only few samples. A key\ncomponent of such ability is the compositional recognition that human can\nperform, which has been well studied in cognitive science but is not well\nexplored in FSL. Inspired by such capability of humans, to imitate humans'\nability of learning visual primitives and composing primitives to recognize\nnovel classes, we propose an approach to FSL to learn a feature representation\ncomposed of important primitives, which is jointly trained with two parts, i.e.\nprimitive discovery and primitive enhancing. In primitive discovery, we focus\non learning primitives related to object parts by self-supervision from the\norder of image splits, avoiding extra laborious annotations and alleviating the\neffect of semantic gaps. In primitive enhancing, inspired by current studies on\nthe interpretability of deep networks, we provide our composition view for the\nFSL baseline model. To modify this model for effective composition, inspired by\nboth mathematical deduction and biological studies (the Hebbian Learning rule\nand the Winner-Take-All mechanism), we propose a soft composition mechanism by\nenlarging the activation of important primitives while reducing that of others,\nso as to enhance the influence of important primitives and better utilize these\nprimitives to compose novel classes. Extensive experiments on public benchmarks\nare conducted on both the few-shot image classification and video recognition\ntasks. Our method achieves the state-of-the-art performance on all these\ndatasets and shows better interpretability.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 21:01:25 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 13:02:01 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 00:03:47 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Zou", "Yixiong", ""], ["Zhang", "Shanghang", ""], ["Chen", "Ke", ""], ["Tian", "Yonghong", ""], ["Wang", "Yaowei", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "2005.06050", "submitter": "Marvin Klingner", "authors": "Marvin Klingner, Andreas B\\\"ar, Philipp Donn and Tim Fingscheidt", "title": "Class-Incremental Learning for Semantic Segmentation Re-Using Neither\n  Old Data Nor Old Labels", "comments": "ITSC 2020 Conference Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While neural networks trained for semantic segmentation are essential for\nperception in autonomous driving, most current algorithms assume a fixed number\nof classes, presenting a major limitation when developing new autonomous\ndriving systems with the need of additional classes. In this paper we present a\ntechnique implementing class-incremental learning for semantic segmentation\nwithout using the labeled data the model was initially trained on. Previous\napproaches still either rely on labels for both old and new classes, or fail to\nproperly distinguish between them. We show how to overcome these problems with\na novel class-incremental learning technique, which nonetheless requires labels\nonly for the new classes. Specifically, (i) we introduce a new loss function\nthat neither relies on old data nor on old labels, (ii) we show how new classes\ncan be integrated in a modular fashion into pretrained semantic segmentation\nmodels, and finally (iii) we re-implement previous approaches in a unified\nsetting to compare them to ours. We evaluate our method on the Cityscapes\ndataset, where we exceed the mIoU performance of all baselines by 3.5% absolute\nreaching a result, which is only 2.2% absolute below the upper performance\nlimit of single-stage training, relying on all data and labels simultaneously.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 21:03:29 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Klingner", "Marvin", ""], ["B\u00e4r", "Andreas", ""], ["Donn", "Philipp", ""], ["Fingscheidt", "Tim", ""]]}, {"id": "2005.06089", "submitter": "Paolo Valdez", "authors": "Paolo Valdez", "title": "Apple Defect Detection Using Deep Learning Based Object Detection For\n  Better Post Harvest Handling", "comments": "Paper presented at the ICLR 2020 Workshop on Computer Vision for\n  Agriculture (CV4A)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inclusion of Computer Vision and Deep Learning technologies in\nAgriculture aims to increase the harvest quality, and productivity of farmers.\nDuring postharvest, the export market and quality evaluation are affected by\nassorting of fruits and vegetables. In particular, apples are susceptible to a\nwide range of defects that can occur during harvesting or/and during the\npost-harvesting period. This paper aims to help farmers with post-harvest\nhandling by exploring if recent computer vision and deep learning methods such\nas the YOLOv3 (Redmon & Farhadi (2018)) can help in detecting healthy apples\nfrom apples with defects.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 23:34:43 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Valdez", "Paolo", ""]]}, {"id": "2005.06107", "submitter": "Ali Borji", "authors": "Ali Borji", "title": "Adversarial examples are useful too!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has come a long way and has enjoyed an unprecedented success.\nDespite high accuracy, however, deep models are brittle and are easily fooled\nby imperceptible adversarial perturbations. In contrast to common\ninference-time attacks, Backdoor (\\aka Trojan) attacks target the training\nphase of model construction, and are extremely difficult to combat since a) the\nmodel behaves normally on a pristine testing set and b) the augmented\nperturbations can be minute and may only affect few training samples. Here, I\npropose a new method to tell whether a model has been subject to a backdoor\nattack. The idea is to generate adversarial examples, targeted or untargeted,\nusing conventional attacks such as FGSM and then feed them back to the\nclassifier. By computing the statistics (here simply mean maps) of the images\nin different categories and comparing them with the statistics of a reference\nmodel, it is possible to visually locate the perturbed regions and unveil the\nattack.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 01:38:56 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Borji", "Ali", ""]]}, {"id": "2005.06111", "submitter": "Yen-Chia Hsu", "authors": "Yen-Chia Hsu, Ting-Hao 'Kenneth' Huang, Ting-Yao Hu, Paul Dille, Sean\n  Prendi, Ryan Hoffman, Anastasia Tsuhlares, Jessica Pachuta, Randy Sargent,\n  Illah Nourbakhsh", "title": "Project RISE: Recognizing Industrial Smoke Emissions", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial smoke emissions pose a significant concern to human health. Prior\nworks have shown that using Computer Vision (CV) techniques to identify smoke\nas visual evidence can influence the attitude of regulators and empower\ncitizens to pursue environmental justice. However, existing datasets are not of\nsufficient quality nor quantity to train the robust CV models needed to support\nair quality advocacy. We introduce RISE, the first large-scale video dataset\nfor Recognizing Industrial Smoke Emissions. We adopted a citizen science\napproach to collaborate with local community members to annotate whether a\nvideo clip has smoke emissions. Our dataset contains 12,567 clips from 19\ndistinct views from cameras that monitored three industrial facilities. These\ndaytime clips span 30 days over two years, including all four seasons. We ran\nexperiments using deep neural networks to establish a strong performance\nbaseline and reveal smoke recognition challenges. Our survey study discussed\ncommunity feedback, and our data analysis displayed opportunities for\nintegrating citizen scientists and crowd workers into the application of\nArtificial Intelligence for Social Impact.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 01:51:15 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 01:17:12 GMT"}, {"version": "v3", "created": "Fri, 15 May 2020 20:42:47 GMT"}, {"version": "v4", "created": "Wed, 20 May 2020 03:22:31 GMT"}, {"version": "v5", "created": "Thu, 30 Jul 2020 20:24:00 GMT"}, {"version": "v6", "created": "Mon, 14 Sep 2020 17:29:40 GMT"}, {"version": "v7", "created": "Mon, 14 Dec 2020 06:36:54 GMT"}, {"version": "v8", "created": "Sun, 7 Mar 2021 06:59:54 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Hsu", "Yen-Chia", ""], ["Huang", "Ting-Hao 'Kenneth'", ""], ["Hu", "Ting-Yao", ""], ["Dille", "Paul", ""], ["Prendi", "Sean", ""], ["Hoffman", "Ryan", ""], ["Tsuhlares", "Anastasia", ""], ["Pachuta", "Jessica", ""], ["Sargent", "Randy", ""], ["Nourbakhsh", "Illah", ""]]}, {"id": "2005.06136", "submitter": "Shunkai Li", "authors": "Shunkai Li, Xin Wang, Yingdian Cao, Fei Xue, Zike Yan, Hongbin Zha", "title": "Self-Supervised Deep Visual Odometry with Online Adaptation", "comments": "Accepted by CVPR 2020 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised VO methods have shown great success in jointly estimating\ncamera pose and depth from videos. However, like most data-driven methods,\nexisting VO networks suffer from a notable decrease in performance when\nconfronted with scenes different from the training data, which makes them\nunsuitable for practical applications. In this paper, we propose an online\nmeta-learning algorithm to enable VO networks to continuously adapt to new\nenvironments in a self-supervised manner. The proposed method utilizes\nconvolutional long short-term memory (convLSTM) to aggregate rich\nspatial-temporal information in the past. The network is able to memorize and\nlearn from its past experience for better estimation and fast adaptation to the\ncurrent frame. When running VO in the open world, in order to deal with the\nchanging environment, we propose an online feature alignment method by aligning\nfeature distributions at different time. Our VO network is able to seamlessly\nadapt to different environments. Extensive experiments on unseen outdoor\nscenes, virtual to real world and outdoor to indoor environments demonstrate\nthat our method consistently outperforms state-of-the-art self-supervised VO\nbaselines considerably.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 03:39:29 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Li", "Shunkai", ""], ["Wang", "Xin", ""], ["Cao", "Yingdian", ""], ["Xue", "Fei", ""], ["Yan", "Zike", ""], ["Zha", "Hongbin", ""]]}, {"id": "2005.06147", "submitter": "Mi Tian", "authors": "Mi Tian, Qiong Nie, Hao Shen", "title": "3D Scene Geometry-Aware Constraint for Camera Localization with Deep\n  Learning", "comments": "Accepted for ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera localization is a fundamental and key component of autonomous driving\nvehicles and mobile robots to localize themselves globally for further\nenvironment perception, path planning and motion control. Recently end-to-end\napproaches based on convolutional neural network have been much studied to\nachieve or even exceed 3D-geometry based traditional methods. In this work, we\npropose a compact network for absolute camera pose regression. Inspired from\nthose traditional methods, a 3D scene geometry-aware constraint is also\nintroduced by exploiting all available information including motion, depth and\nimage contents. We add this constraint as a regularization term to our proposed\nnetwork by defining a pixel-level photometric loss and an image-level\nstructural similarity loss. To benchmark our method, different challenging\nscenes including indoor and outdoor environment are tested with our proposed\napproach and state-of-the-arts. And the experimental results demonstrate\nsignificant performance improvement of our method on both prediction accuracy\nand convergence efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 04:15:14 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Tian", "Mi", ""], ["Nie", "Qiong", ""], ["Shen", "Hao", ""]]}, {"id": "2005.06184", "submitter": "Chaoran Zhuge", "authors": "Chaoran Zhuge, Yujie Peng, Yadong Li, Jiangbo Ai, Junru Chen", "title": "Attribute-guided Feature Extraction and Augmentation Robust Learning for\n  Vehicle Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification is one of the core technologies of intelligent\ntransportation systems and smart cities, but large intra-class diversity and\ninter-class similarity poses great challenges for existing method. In this\npaper, we propose a multi-guided learning approach which utilizing the\ninformation of attributes and meanwhile introducing two novel random augments\nto improve the robustness during training. What's more, we propose an attribute\nconstraint method and group re-ranking strategy to refine matching results. Our\nmethod achieves mAP of 66.83% and rank-1 accuracy 76.05% in the CVPR 2020 AI\nCity Challenge.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 07:13:53 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Zhuge", "Chaoran", ""], ["Peng", "Yujie", ""], ["Li", "Yadong", ""], ["Ai", "Jiangbo", ""], ["Chen", "Junru", ""]]}, {"id": "2005.06189", "submitter": "Chuong Huynh", "authors": "Minh-Chuong Huynh, Trung-Hieu Nguyen, Minh-Triet Tran", "title": "Context Learning for Bone Shadow Exclusion in CheXNet Accuracy\n  Improvement", "comments": "KSE 2018 long paper", "journal-ref": null, "doi": "10.1109/KSE.2018.8573393", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Chest X-ray examination plays an important role in lung disease detection.\nThe more accuracy of this task, the more experienced radiologists are required.\nAfter ChestX-ray14 dataset containing over 100,000 frontal-view X-ray images of\n14 diseases was released, several models were proposed with high accuracy. In\nthis paper, we develop a work flow for lung disease diagnosis in chest X-ray\nimages, which can improve the average AUROC of the state-of-the-art model from\n0.8414 to 0.8445. We apply image preprocessing steps before feeding to the 14\ndiseases detection model. Our project includes three models: the first one is\nDenseNet-121 to predict whether a processed image has a better result, a\nconvolutional auto-encoder model for bone shadow exclusion is the second one,\nand the last is the original CheXNet.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 07:29:03 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Huynh", "Minh-Chuong", ""], ["Nguyen", "Trung-Hieu", ""], ["Tran", "Minh-Triet", ""]]}, {"id": "2005.06198", "submitter": "Carlos Arango Duque Dr", "authors": "Carlos Arango Duque, Olivier Alata, R\\'emi Emonet, Hubert Konik and\n  Anne-Claire Legrand", "title": "Mean Oriented Riesz Features for Micro Expression Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-expressions are brief and subtle facial expressions that go on and off\nthe face in a fraction of a second. This kind of facial expressions usually\noccurs in high stake situations and is considered to reflect a human's real\nintent. There has been some interest in micro-expression analysis, however, a\ngreat majority of the methods are based on classically established computer\nvision methods such as local binary patterns, histogram of gradients and\noptical flow. A novel methodology for micro-expression recognition using the\nRiesz pyramid, a multi-scale steerable Hilbert transform is presented. In fact,\nan image sequence is transformed with this tool, then the image phase\nvariations are extracted and filtered as proxies for motion. Furthermore, the\ndominant orientation constancy from the Riesz transform is exploited to average\nthe micro-expression sequence into an image pair. Based on that, the Mean\nOriented Riesz Feature description is introduced. Finally the performance of\nour methods are tested in two spontaneous micro-expressions databases and\ncompared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 08:23:34 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Duque", "Carlos Arango", ""], ["Alata", "Olivier", ""], ["Emonet", "R\u00e9mi", ""], ["Konik", "Hubert", ""], ["Legrand", "Anne-Claire", ""]]}, {"id": "2005.06209", "submitter": "Matteo Poggi", "authors": "Matteo Poggi, Filippo Aleotti, Fabio Tosi, Stefano Mattoccia", "title": "On the uncertainty of self-supervised monocular depth estimation", "comments": "CVPR 2020. Code will be available\n  https://github.com/mattpoggi/mono-uncertainty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised paradigms for monocular depth estimation are very appealing\nsince they do not require ground truth annotations at all. Despite the\nastonishing results yielded by such methodologies, learning to reason about the\nuncertainty of the estimated depth maps is of paramount importance for\npractical applications, yet uncharted in the literature. Purposely, we explore\nfor the first time how to estimate the uncertainty for this task and how this\naffects depth accuracy, proposing a novel peculiar technique specifically\ndesigned for self-supervised approaches. On the standard KITTI dataset, we\nexhaustively assess the performance of each method with different\nself-supervised paradigms. Such evaluation highlights that our proposal i)\nalways improves depth accuracy significantly and ii) yields state-of-the-art\nresults concerning uncertainty estimation when training on sequences and\ncompetitive results uniquely deploying stereo pairs.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 09:00:55 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Poggi", "Matteo", ""], ["Aleotti", "Filippo", ""], ["Tosi", "Fabio", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "2005.06216", "submitter": "Onur Tasar", "authors": "Onur Tasar, Alain Giros, Yuliya Tarabalka, Pierre Alliez, S\\'ebastien\n  Clerc", "title": "DAugNet: Unsupervised, Multi-source, Multi-target, and Life-long Domain\n  Adaptation for Semantic Segmentation of Satellite Images", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2020.3006161", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The domain adaptation of satellite images has recently gained an increasing\nattention to overcome the limited generalization abilities of machine learning\nmodels when segmenting large-scale satellite images. Most of the existing\napproaches seek for adapting the model from one domain to another. However,\nsuch single-source and single-target setting prevents the methods from being\nscalable solutions, since nowadays multiple source and target domains having\ndifferent data distributions are usually available. Besides, the continuous\nproliferation of satellite images necessitates the classifiers to adapt to\ncontinuously increasing data. We propose a novel approach, coined DAugNet, for\nunsupervised, multi-source, multi-target, and life-long domain adaptation of\nsatellite images. It consists of a classifier and a data augmentor. The data\naugmentor, which is a shallow network, is able to perform style transfer\nbetween multiple satellite images in an unsupervised manner, even when new data\nare added over the time. In each training iteration, it provides the classifier\nwith diversified data, which makes the classifier robust to large data\ndistribution difference between the domains. Our extensive experiments prove\nthat DAugNet significantly better generalizes to new geographic locations than\nthe existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 09:11:22 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 11:48:20 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Tasar", "Onur", ""], ["Giros", "Alain", ""], ["Tarabalka", "Yuliya", ""], ["Alliez", "Pierre", ""], ["Clerc", "S\u00e9bastien", ""]]}, {"id": "2005.06251", "submitter": "Tao Meng", "authors": "Shengyu Jia, Tao Meng, Jieyu Zhao and Kai-Wei Chang", "title": "Mitigating Gender Bias Amplification in Distribution by Posterior\n  Regularization", "comments": "7 pages, 3 figures, published in ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced machine learning techniques have boosted the performance of natural\nlanguage processing. Nevertheless, recent studies, e.g., Zhao et al. (2017)\nshow that these techniques inadvertently capture the societal bias hidden in\nthe corpus and further amplify it. However, their analysis is conducted only on\nmodels' top predictions. In this paper, we investigate the gender bias\namplification issue from the distribution perspective and demonstrate that the\nbias is amplified in the view of predicted probability distribution over\nlabels. We further propose a bias mitigation approach based on posterior\nregularization. With little performance loss, our method can almost remove the\nbias amplification in the distribution. Our study sheds the light on\nunderstanding the bias amplification.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 11:07:10 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Jia", "Shengyu", ""], ["Meng", "Tao", ""], ["Zhao", "Jieyu", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2005.06262", "submitter": "Lucas Brynte", "authors": "Lucas Brynte and Fredrik Kahl", "title": "Pose Proposal Critic: Robust Pose Refinement by Learning Reprojection\n  Errors", "comments": "Added acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, considerable progress has been made for the task of rigid\nobject pose estimation from a single RGB-image, but achieving robustness to\npartial occlusions remains a challenging problem. Pose refinement via rendering\nhas shown promise in order to achieve improved results, in particular, when\ndata is scarce.\n  In this paper we focus our attention on pose refinement, and show how to push\nthe state-of-the-art further in the case of partial occlusions. The proposed\npose refinement method leverages on a simplified learning task, where a CNN is\ntrained to estimate the reprojection error between an observed and a rendered\nimage. We experiment by training on purely synthetic data as well as a mixture\nof synthetic and real data. Current state-of-the-art results are outperformed\nfor two out of three metrics on the Occlusion LINEMOD benchmark, while\nperforming on-par for the final metric.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 11:46:04 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 10:41:36 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Brynte", "Lucas", ""], ["Kahl", "Fredrik", ""]]}, {"id": "2005.06278", "submitter": "Hadi Abdi Khojasteh", "authors": "Hadi Abdi Khojasteh", "title": "A Survey on Patch-based Synthesis: GPU Implementation and Optimization", "comments": "117 pages, 38 figures, in Persian", "journal-ref": null, "doi": "10.13140/RG.2.2.29490.86729/1", "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis surveys the research in patch-based synthesis and algorithms for\nfinding correspondences between small local regions of images. We additionally\nexplore a large kind of applications of this new fast randomized matching\ntechnique. One of the algorithms we have studied in particular is PatchMatch,\ncan find similar regions or \"patches\" of an image one to two orders of\nmagnitude faster than previous techniques. The algorithmic program is driven by\napplying mathematical properties of nearest neighbors in natural images. It is\nobserved that neighboring correspondences tend to be similar or \"coherent\" and\nuse this observation in algorithm in order to quickly converge to an\napproximate solution. The algorithm is the most general form can find k-nearest\nneighbor matching, using patches that translate, rotate, or scale, using\narbitrary descriptors, and between two or more images. Speed-ups are obtained\nover various techniques in an exceeding range of those areas. We have explored\nmany applications of PatchMatch matching algorithm. In computer graphics, we\nhave explored removing unwanted objects from images, seamlessly moving objects\nin images, changing image aspect ratios, and video summarization. In computer\nvision we have explored denoising images, object detection, detecting image\nforgeries, and detecting symmetries. We conclude by discussing the restrictions\nof our algorithmic program, GPU implementation and areas for future analysis.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 19:25:28 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Khojasteh", "Hadi Abdi", ""]]}, {"id": "2005.06305", "submitter": "Hai Phan", "authors": "Hai Phan, Zechun Liu, Dang Huynh, Marios Savvides, Kwang-Ting Cheng,\n  Zhiqiang Shen", "title": "Binarizing MobileNet via Evolution-based Searching", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Neural Networks (BNNs), known to be one among the effectively compact\nnetwork architectures, have achieved great outcomes in the visual tasks.\nDesigning efficient binary architectures is not trivial due to the binary\nnature of the network. In this paper, we propose a use of evolutionary search\nto facilitate the construction and training scheme when binarizing MobileNet, a\ncompact network with separable depth-wise convolution. Inspired by one-shot\narchitecture search frameworks, we manipulate the idea of group convolution to\ndesign efficient 1-Bit Convolutional Neural Networks (CNNs), assuming an\napproximately optimal trade-off between computational cost and model accuracy.\nOur objective is to come up with a tiny yet efficient binary neural\narchitecture by exploring the best candidates of the group convolution while\noptimizing the model performance in terms of complexity and latency. The\napproach is threefold. First, we train strong baseline binary networks with a\nwide range of random group combinations at each convolutional layer. This\nset-up gives the binary neural networks a capability of preserving essential\ninformation through layers. Second, to find a good set of hyperparameters for\ngroup convolutions we make use of the evolutionary search which leverages the\nexploration of efficient 1-bit models. Lastly, these binary models are trained\nfrom scratch in a usual manner to achieve the final binary model. Various\nexperiments on ImageNet are conducted to show that following our construction\nguideline, the final model achieves 60.09% Top-1 accuracy and outperforms the\nstate-of-the-art CI-BCNN with the same computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 13:25:51 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 15:48:58 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Phan", "Hai", ""], ["Liu", "Zechun", ""], ["Huynh", "Dang", ""], ["Savvides", "Marios", ""], ["Cheng", "Kwang-Ting", ""], ["Shen", "Zhiqiang", ""]]}, {"id": "2005.06331", "submitter": "Jacek Dabrowski", "authors": "Anna Wroblewska (1 and 2), Jacek Dabrowski (1), Michal Pastuszak (1),\n  Andrzej Michalowski (1), Michal Daniluk (1), Barbara Rychalska (1 and 2),\n  Mikolaj Wieczorek (1), Sylwia Sysko-Romanczuk (2) ((1) Synerise, (2) Warsaw\n  University of Technology)", "title": "Multi-modal Embedding Fusion-based Recommender", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems have lately been popularized globally, with primary\nuse cases in online interaction systems, with significant focus on e-commerce\nplatforms. We have developed a machine learning-based recommendation platform,\nwhich can be easily applied to almost any items and/or actions domain. Contrary\nto existing recommendation systems, our platform supports multiple types of\ninteraction data with multiple modalities of metadata natively. This is\nachieved through multi-modal fusion of various data representations. We\ndeployed the platform into multiple e-commerce stores of different kinds, e.g.\nfood and beverages, shoes, fashion items, telecom operators. Here, we present\nour system, its flexibility and performance. We also show benchmark results on\nopen datasets, that significantly outperform state-of-the-art prior work.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 14:13:35 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 11:45:22 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Wroblewska", "Anna", "", "1 and 2"], ["Dabrowski", "Jacek", "", "1 and 2"], ["Pastuszak", "Michal", "", "1 and 2"], ["Michalowski", "Andrzej", "", "1 and 2"], ["Daniluk", "Michal", "", "1 and 2"], ["Rychalska", "Barbara", "", "1 and 2"], ["Wieczorek", "Mikolaj", ""], ["Sysko-Romanczuk", "Sylwia", ""]]}, {"id": "2005.06338", "submitter": "Feifan Wang", "authors": "Feifan Wang", "title": "Neural Architecture Search for Gliomas Segmentation on Multimodal\n  Magnetic Resonance Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past few years have witnessed the artificial intelligence inspired evolution\nin various medical fields. The diagnosis and treatment of gliomas -- one of the\nmost commonly seen brain tumors with low survival rate -- rely heavily on the\ncomputer assisted segmentation process undertaken on the magnetic resonance\nimaging (MRI) scans. Although the encoder-decoder shaped deep learning networks\nhave been the de facto standard style for semantic segmentation tasks in\nmedical imaging analysis, enormous effort is still required to be spent on\ndesigning the detailed architecture of the down-sampling and up-sampling\nblocks. In this work, we propose a neural architecture search (NAS) based\nsolution to brain tumor segmentation tasks on multimodal volumetric MRI scans.\nThree sets of candidate operations are composed respectively for three kinds of\nbasic building blocks in which each operation is assigned with a specific\nprobabilistic parameter to be learned. Through alternately updating the weights\nof operations and the other parameters in the network, the searching mechanism\nends up with two optimal structures for the upward and downward blocks.\nMoreover, the developed solution also integrates normalization and patching\nstrategies tailored for brain MRI processing. Extensive comparative experiments\non the BraTS 2019 dataset demonstrate that the proposed algorithm not only\ncould relieve the pressure of fabricating block architectures but also\npossesses competitive feasibility and scalability.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 14:32:00 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 06:00:43 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Wang", "Feifan", ""]]}, {"id": "2005.06368", "submitter": "Wietske Bastiaansen", "authors": "Wietske A.P. Bastiaansen, Melek Rousian, R\\'egine P.M.\n  Steegers-Theunissen, Wiro J. Niessen, Anton Koning and Stefan Klein", "title": "Towards segmentation and spatial alignment of the human embryonic brain\n  using deep learning for atlas-based registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised deep learning method for atlas based registration\nto achieve segmentation and spatial alignment of the embryonic brain in a\nsingle framework. Our approach consists of two sequential networks with a\nspecifically designed loss function to address the challenges in 3D first\ntrimester ultrasound. The first part learns the affine transformation and the\nsecond part learns the voxelwise nonrigid deformation between the target image\nand the atlas. We trained this network end-to-end and validated it against a\nground truth on synthetic datasets designed to resemble the challenges present\nin 3D first trimester ultrasound. The method was tested on a dataset of human\nembryonic ultrasound volumes acquired at 9 weeks gestational age, which showed\nalignment of the brain in some cases and gave insight in open challenges for\nthe proposed method. We conclude that our method is a promising approach\ntowards fully automated spatial alignment and segmentation of embryonic brains\nin 3D ultrasound.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 15:23:44 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Bastiaansen", "Wietske A. P.", ""], ["Rousian", "Melek", ""], ["Steegers-Theunissen", "R\u00e9gine P. M.", ""], ["Niessen", "Wiro J.", ""], ["Koning", "Anton", ""], ["Klein", "Stefan", ""]]}, {"id": "2005.06382", "submitter": "Zhenjie Tang", "authors": "Zhenjie Tang, Bin Pan, Enhai Liu, Xia Xu, Tianyang Shi, Zhenwei Shi", "title": "SRDA-Net: Super-Resolution Domain Adaptation Networks for Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Unsupervised Domain Adaptation was proposed to address the domain\nshift problem in semantic segmentation task, but it may perform poor when\nsource and target domains belong to different resolutions. In this work, we\ndesign a novel end-to-end semantic segmentation network, Super-Resolution\nDomain Adaptation Network (SRDA-Net), which could simultaneously complete\nsuper-resolution and domain adaptation. Such characteristics exactly meet the\nrequirement of semantic segmentation for remote sensing images which usually\ninvolve various resolutions. Generally, SRDA-Net includes three deep neural\nnetworks: a Super-Resolution and Segmentation (SRS) model focuses on recovering\nhigh-resolution image and predicting segmentation map; a pixel-level domain\nclassifier (PDC) tries to distinguish the images from which domains; and\noutput-space domain classifier (ODC) discriminates pixel label distributions\nfrom which domains. PDC and ODC are considered as the discriminators, and SRS\nis treated as the generator. By the adversarial learning, SRS tries to align\nthe source with target domains on pixel-level visual appearance and\noutput-space. Experiments are conducted on the two remote sensing datasets with\ndifferent resolutions. SRDA-Net performs favorably against the state-of-the-art\nmethods in terms of accuracy and visual quality. Code and models are available\nat https://github.com/tangzhenjie/SRDA-Net.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 15:48:41 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 03:40:16 GMT"}, {"version": "v3", "created": "Thu, 21 May 2020 10:16:38 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Tang", "Zhenjie", ""], ["Pan", "Bin", ""], ["Liu", "Enhai", ""], ["Xu", "Xia", ""], ["Shi", "Tianyang", ""], ["Shi", "Zhenwei", ""]]}, {"id": "2005.06402", "submitter": "Hanxiang Hao", "authors": "Hanxiang Hao and Sriram Baireddy and Amy R. Reibman and Edward J. Delp", "title": "FaR-GAN for One-Shot Face Reenactment", "comments": "This paper has been accepted to the AI for content creation workshop\n  at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animating a static face image with target facial expressions and movements is\nimportant in the area of image editing and movie production. This face\nreenactment process is challenging due to the complex geometry and movement of\nhuman faces. Previous work usually requires a large set of images from the same\nperson to model the appearance. In this paper, we present a one-shot face\nreenactment model, FaR-GAN, that takes only one face image of any given source\nidentity and a target expression as input, and then produces a face image of\nthe same source identity but with the target expression. The proposed method\nmakes no assumptions about the source identity, facial expression, head pose,\nor even image background. We evaluate our method on the VoxCeleb1 dataset and\nshow that our method is able to generate a higher quality face image than the\ncompared methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 16:15:37 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Hao", "Hanxiang", ""], ["Baireddy", "Sriram", ""], ["Reibman", "Amy R.", ""], ["Delp", "Edward J.", ""]]}, {"id": "2005.06409", "submitter": "Hyounghun Kim", "authors": "Hyounghun Kim, Zineng Tang, Mohit Bansal", "title": "Dense-Caption Matching and Frame-Selection Gating for Temporal\n  Localization in VideoQA", "comments": "ACL 2020 (11 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos convey rich information. Dynamic spatio-temporal relationships between\npeople/objects, and diverse multimodal events are present in a video clip.\nHence, it is important to develop automated models that can accurately extract\nsuch information from videos. Answering questions on videos is one of the tasks\nwhich can evaluate such AI abilities. In this paper, we propose a video\nquestion answering model which effectively integrates multi-modal input sources\nand finds the temporally relevant information to answer questions.\nSpecifically, we first employ dense image captions to help identify objects and\ntheir detailed salient regions and actions, and hence give the model useful\nextra information (in explicit textual format to allow easier matching) for\nanswering questions. Moreover, our model is also comprised of dual-level\nattention (word/object and frame level), multi-head self/cross-integration for\ndifferent sources (video and dense captions), and gates which pass more\nrelevant information to the classifier. Finally, we also cast the frame\nselection problem as a multi-label classification task and introduce two loss\nfunctions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary\nCross-Entropy (BBCE), to better supervise the model with human importance\nannotations. We evaluate our model on the challenging TVQA dataset, where each\nof our model components provides significant gains, and our overall model\noutperforms the state-of-the-art by a large margin (74.09% versus 70.52%). We\nalso present several word, object, and frame level visualization studies. Our\ncode is publicly available at:\nhttps://github.com/hyounghk/VideoQADenseCapFrameGate-ACL2020\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 16:35:27 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Kim", "Hyounghun", ""], ["Tang", "Zineng", ""], ["Bansal", "Mohit", ""]]}, {"id": "2005.06421", "submitter": "Yuteng Zhu", "authors": "Yuteng Zhu, Graham D. Finlayson", "title": "Designing a Color Filter via Optimization of Vora-Value for Making a\n  Camera more Colorimetric", "comments": "6 pages, 6 figures, 1 table, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Luther condition states that if the spectral sensitivity responses of a\ncamera are a linear transform from the color matching functions of the human\nvisual system, the camera is colorimetric. Previous work proposed to solve for\na filter which, when placed in front of a camera, results in sensitivities that\nbest satisfy the Luther condition. By construction, the prior art solves for a\nfilter for a given set of human visual sensitivities, e.g. the XYZ color\nmatching functions or the cone response functions. However, depending on the\ntarget spectral sensitivity set, a different optimal filter is found.\n  This paper begins with the observation that the cone fundamentals, XYZ color\nmatching functions or any linear combination thereof span the same\n3-dimensional subspace. Thus, we set out to solve for a filter that makes the\nvector space spanned by the filtered camera sensitivities as similar as\npossible to the space spanned by human vision sensors. We argue that the\nVora-Value is a suitable way to measure subspace similarity and we develop an\noptimization method for finding a filter that maximizes the Vora-Value measure.\n  Experiments demonstrate that our new optimization leads to filtered camera\nsensitivities which have a significantly higher Vora-Value compared with\nantecedent methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 16:51:21 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Zhu", "Yuteng", ""], ["Finlayson", "Graham D.", ""]]}, {"id": "2005.06423", "submitter": "Yingxue Xu", "authors": "Yingxue Xu, Guihua Wen, Yang Hu, Mingnan Luo, Dan Dai, Yishan Zhuang\n  and Wendy Hall", "title": "Multiple Attentional Pyramid Networks for Chinese Herbal Recognition", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese herbs play a critical role in Traditional Chinese Medicine. Due to\ndifferent recognition granularity, they can be recognized accurately only by\nprofessionals with much experience. It is expected that they can be recognized\nautomatically using new techniques like machine learning. However, there is no\nChinese herbal image dataset available. Simultaneously, there is no machine\nlearning method which can deal with Chinese herbal image recognition well.\nTherefore, this paper begins with building a new standard Chinese-Herbs\ndataset. Subsequently, a new Attentional Pyramid Networks (APN) for Chinese\nherbal recognition is proposed, where both novel competitive attention and\nspatial collaborative attention are proposed and then applied. APN can\nadaptively model Chinese herbal images with different feature scales. Finally,\na new framework for Chinese herbal recognition is proposed as a new application\nof APN. Experiments are conducted on our constructed dataset and validate the\neffectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 16:55:01 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Xu", "Yingxue", ""], ["Wen", "Guihua", ""], ["Hu", "Yang", ""], ["Luo", "Mingnan", ""], ["Dai", "Dan", ""], ["Zhuang", "Yishan", ""], ["Hall", "Wendy", ""]]}, {"id": "2005.06431", "submitter": "Katja Schladitz", "authors": "Thomas Baranowski, Dascha Dobrovolskij, Kilian Dremel, Astrid\n  H\\\"olzing, G\\\"unter Lohfink, Katja Schladitz, Simon Zabler", "title": "Local Fiber Orientation from X-ray Region-of-Interest Computed\n  Tomography of large Fiber Reinforced Composite Components", "comments": null, "journal-ref": "Composites Science and Technology, 183: 107786, 2019", "doi": "10.1016/j.compscitech.2019.107786", "report-no": null, "categories": "eess.IV cs.CV physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The local fiber orientation is a micro-structural feature crucial for the\nmechanical properties of parts made from fiber reinforced polymers. It can be\ndetermined from micro-computed tomography data and subsequent quantitative\nanalysis of the resulting 3D images. However, although being by nature\nnon-destructive, this method so far has required to cut samples of a few\nmillimeter edge length in order to achieve the high lateral resolution needed\nfor the analysis. Here, we report on the successful combination of\nregion-of-interest scanning with structure texture orientation analysis\nrendering the above described approach truly non-destructive. Several regions\nof interest in a large bearing part from the automotive industry made of fiber\nreinforced polymer are scanned and analyzed. Differences of these regions with\nrespect to local fiber orientation are quantified. Moreover, consistency of the\nanalysis based on scans at varying lateral resolutions is proved. Finally,\nmeasured and numerically simulated orientation tensors are compared for one of\nthe regions.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 17:08:39 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Baranowski", "Thomas", ""], ["Dobrovolskij", "Dascha", ""], ["Dremel", "Kilian", ""], ["H\u00f6lzing", "Astrid", ""], ["Lohfink", "G\u00fcnter", ""], ["Schladitz", "Katja", ""], ["Zabler", "Simon", ""]]}, {"id": "2005.06508", "submitter": "Paramanand Chandramouli", "authors": "Paramanand Chandramouli, Kanchana Vaishnavi Gandikota, Andreas\n  Goerlitz, Andreas Kolb, Michael Moeller", "title": "A Generative Model for Generic Light Field Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep generative models have achieved impressive progress in modeling\nthe distribution of training data. In this work, we present for the first time\na generative model for 4D light field patches using variational autoencoders to\ncapture the data distribution of light field patches. We develop a generative\nmodel conditioned on the central view of the light field and incorporate this\nas a prior in an energy minimization framework to address diverse light field\nreconstruction tasks. While pure learning-based approaches do achieve excellent\nresults on each instance of such a problem, their applicability is limited to\nthe specific observation model they have been trained on. On the contrary, our\ntrained light field generative model can be incorporated as a prior into any\nmodel-based optimization approach and therefore extend to diverse\nreconstruction tasks including light field view synthesis, spatial-angular\nsuper resolution and reconstruction from coded projections. Our proposed method\ndemonstrates good reconstruction, with performance approaching end-to-end\ntrained networks, while outperforming traditional model-based approaches on\nboth synthetic and real scenes. Furthermore, we show that our approach enables\nreliable light field recovery despite distortions in the input.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 18:27:42 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 17:10:11 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Chandramouli", "Paramanand", ""], ["Gandikota", "Kanchana Vaishnavi", ""], ["Goerlitz", "Andreas", ""], ["Kolb", "Andreas", ""], ["Moeller", "Michael", ""]]}, {"id": "2005.06514", "submitter": "Shan Jia", "authors": "Shan Jia, Xin Li, Chuanbo Hu, Guodong Guo, Zhengquan Xu", "title": "3D Face Anti-spoofing with Factorized Bilinear Coding", "comments": "arXiv admin note: text overlap with arXiv:1910.05457", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have witnessed rapid advances in both face presentation attack models and\npresentation attack detection (PAD) in recent years. When compared with widely\nstudied 2D face presentation attacks, 3D face spoofing attacks are more\nchallenging because face recognition systems are more easily confused by the 3D\ncharacteristics of materials similar to real faces. In this work, we tackle the\nproblem of detecting these realistic 3D face presentation attacks, and propose\na novel anti-spoofing method from the perspective of fine-grained\nclassification. Our method, based on factorized bilinear coding of multiple\ncolor channels (namely MC\\_FBC), targets at learning subtle fine-grained\ndifferences between real and fake images. By extracting discriminative and\nfusing complementary information from RGB and YCbCr spaces, we have developed a\nprincipled solution to 3D face spoofing detection. A large-scale wax figure\nface database (WFFD) with both images and videos has also been collected as\nsuper-realistic attacks to facilitate the study of 3D face presentation attack\ndetection. Extensive experimental results show that our proposed method\nachieves the state-of-the-art performance on both our own WFFD and other face\nspoofing databases under various intra-database and inter-database testing\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 03:09:20 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 02:40:45 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2020 16:21:24 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Jia", "Shan", ""], ["Li", "Xin", ""], ["Hu", "Chuanbo", ""], ["Guo", "Guodong", ""], ["Xu", "Zhengquan", ""]]}, {"id": "2005.06536", "submitter": "Ning Zhang", "authors": "Ning Zhang, Jingen Liu, Ke Wang, Dan Zeng, Tao Mei", "title": "Robust Visual Object Tracking with Two-Stream Residual Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current deep learning based visual tracking approaches have been very\nsuccessful by learning the target classification and/or estimation model from a\nlarge amount of supervised training data in offline mode. However, most of them\ncan still fail in tracking objects due to some more challenging issues such as\ndense distractor objects, confusing background, motion blurs, and so on.\nInspired by the human \"visual tracking\" capability which leverages motion cues\nto distinguish the target from the background, we propose a Two-Stream Residual\nConvolutional Network (TS-RCN) for visual tracking, which successfully exploits\nboth appearance and motion features for model update. Our TS-RCN can be\nintegrated with existing deep learning based visual trackers. To further\nimprove the tracking performance, we adopt a \"wider\" residual network ResNeXt\nas its feature extraction backbone. To the best of our knowledge, TS-RCN is the\nfirst end-to-end trainable two-stream visual tracking system, which makes full\nuse of both appearance and motion features of the target. We have extensively\nevaluated the TS-RCN on most widely used benchmark datasets including VOT2018,\nVOT2019, and GOT-10K. The experiment results have successfully demonstrated\nthat our two-stream model can greatly outperform the appearance based tracker,\nand it also achieves state-of-the-art performance. The tracking system can run\nat up to 38.1 FPS.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 19:05:42 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Zhang", "Ning", ""], ["Liu", "Jingen", ""], ["Wang", "Ke", ""], ["Zeng", "Dan", ""], ["Mei", "Tao", ""]]}, {"id": "2005.06550", "submitter": "Shreshth Saini Mr.", "authors": "Shreshth Saini (1), Divij Gupta (1), Anil Kumar Tiwari (1) ((1) Indian\n  Institute of Technology Jodhpur)", "title": "Detector-SegMentor Network for Skin Lesion Localization and Segmentation", "comments": "9 pages, 7 figures, accepted at NCVPRIPG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Melanoma is a life-threatening form of skin cancer when left undiagnosed at\nthe early stages. Although there are more cases of non-melanoma cancer than\nmelanoma cancer, melanoma cancer is more deadly. Early detection of melanoma is\ncrucial for the timely diagnosis of melanoma cancer and prohibit its spread to\ndistant body parts. Segmentation of skin lesion is a crucial step in the\nclassification of melanoma cancer from the cancerous lesions in dermoscopic\nimages. Manual segmentation of dermoscopic skin images is very time consuming\nand error-prone resulting in an urgent need for an intelligent and accurate\nalgorithm. In this study, we propose a simple yet novel network-in-network\nconvolution neural network(CNN) based approach for segmentation of the skin\nlesion. A Faster Region-based CNN (Faster RCNN) is used for preprocessing to\npredict bounding boxes of the lesions in the whole image which are subsequently\ncropped and fed into the segmentation network to obtain the lesion mask. The\nsegmentation network is a combination of the UNet and Hourglass networks. We\ntrained and evaluated our models on ISIC 2018 dataset and also cross-validated\non PH\\textsuperscript{2} and ISBI 2017 datasets. Our proposed method surpassed\nthe state-of-the-art with Dice Similarity Coefficient of 0.915 and Accuracy\n0.959 on ISIC 2018 dataset and Dice Similarity Coefficient of 0.947 and\nAccuracy 0.971 on ISBI 2017 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 19:41:27 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Saini", "Shreshth", ""], ["Gupta", "Divij", ""], ["Tiwari", "Anil Kumar", ""]]}, {"id": "2005.06582", "submitter": "Amir Rasouli", "authors": "Amir Rasouli, Iuliia Kotseruba, John K. Tsotsos", "title": "Pedestrian Action Anticipation using Contextual Feature Fusion in\n  Stacked RNNs", "comments": "This paper was accepted and presented at British Machine Vision\n  Conference (BMVC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges for autonomous vehicles in urban environments is\nto understand and predict other road users' actions, in particular, pedestrians\nat the point of crossing. The common approach to solving this problem is to use\nthe motion history of the agents to predict their future trajectories. However,\npedestrians exhibit highly variable actions most of which cannot be understood\nwithout visual observation of the pedestrians themselves and their\nsurroundings. To this end, we propose a solution for the problem of pedestrian\naction anticipation at the point of crossing. Our approach uses a novel stacked\nRNN architecture in which information collected from various sources, both\nscene dynamics and visual features, is gradually fused into the network at\ndifferent levels of processing. We show, via extensive empirical evaluations,\nthat the proposed algorithm achieves a higher prediction accuracy compared to\nalternative recurrent network architectures. We conduct experiments to\ninvestigate the impact of the length of observation, time to event and types of\nfeatures on the performance of the proposed method. Finally, we demonstrate how\ndifferent data fusion strategies impact prediction accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 20:59:37 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Rasouli", "Amir", ""], ["Kotseruba", "Iuliia", ""], ["Tsotsos", "John K.", ""]]}, {"id": "2005.06583", "submitter": "Iuliia Kotseruba", "authors": "Iuliia Kotseruba, Calden Wloka, Amir Rasouli, John K. Tsotsos", "title": "Do Saliency Models Detect Odd-One-Out Targets? New Datasets and\n  Evaluations", "comments": "Published in BMVC 2019. 14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the field of saliency have concentrated on fixation\nprediction, with benchmarks reaching saturation. However, there is an extensive\nbody of works in psychology and neuroscience that describe aspects of human\nvisual attention that might not be adequately captured by current approaches.\nHere, we investigate singleton detection, which can be thought of as a\ncanonical example of salience. We introduce two novel datasets, one with\npsychophysical patterns and one with natural odd-one-out stimuli. Using these\ndatasets we demonstrate through extensive experimentation that nearly all\nsaliency algorithms do not adequately respond to singleton targets in synthetic\nand natural images. Furthermore, we investigate the effect of training\nstate-of-the-art CNN-based saliency models on these types of stimuli and\nconclude that the additional training data does not lead to a significant\nimprovement of their ability to find odd-one-out targets. Datasets are\navailable at http://data.nvision2.eecs.yorku.ca/P3O3/.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 20:59:53 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 17:27:23 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Kotseruba", "Iuliia", ""], ["Wloka", "Calden", ""], ["Rasouli", "Amir", ""], ["Tsotsos", "John K.", ""]]}, {"id": "2005.06653", "submitter": "Brigit Schroeder", "authors": "Brigit Schroeder, Subarna Tripathi", "title": "Structured Query-Based Image Retrieval Using Scene Graphs", "comments": "Accepted to Diagram Image Retrieval and Analysis (DIRA) Workshop at\n  CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A structured query can capture the complexity of object interactions (e.g.\n'woman rides motorcycle') unlike single objects (e.g. 'woman' or 'motorcycle').\nRetrieval using structured queries therefore is much more useful than single\nobject retrieval, but a much more challenging problem. In this paper we present\na method which uses scene graph embeddings as the basis for an approach to\nimage retrieval. We examine how visual relationships, derived from scene\ngraphs, can be used as structured queries. The visual relationships are\ndirected subgraphs of the scene graph with a subject and object as nodes\nconnected by a predicate relationship. Notably, we are able to achieve high\nrecall even on low to medium frequency objects found in the long-tailed\nCOCO-Stuff dataset, and find that adding a visual relationship-inspired loss\nboosts our recall by 10% in the best case.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 22:40:32 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Schroeder", "Brigit", ""], ["Tripathi", "Subarna", ""]]}, {"id": "2005.06654", "submitter": "Dario Kneubuehler", "authors": "Dario Kneubuehler, Shuhang Gu, Luc Van Gool, Radu Timofte", "title": "Flexible Example-based Image Enhancement with Task Adaptive Global\n  Feature Self-Guided Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first practical multitask image enhancement network, that is\nable to learn one-to-many and many-to-one image mappings. We show that our\nmodel outperforms the current state of the art in learning a single enhancement\nmapping, while having significantly fewer parameters than its competitors.\nFurthermore, the model achieves even higher performance on learning multiple\nmappings simultaneously, by taking advantage of shared representations. Our\nnetwork is based on the recently proposed SGN architecture, with modifications\ntargeted at incorporating global features and style adaption. Finally, we\npresent an unpaired learning method for multitask image enhancement, that is\nbased on generative adversarial networks (GANs).\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 22:45:07 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 12:59:50 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Kneubuehler", "Dario", ""], ["Gu", "Shuhang", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2005.06667", "submitter": "Frank Bieder", "authors": "Frank Bieder, Sascha Wirges, Johannes Janosovits, Sven Richter,\n  Zheyuan Wang, and Christoph Stiller", "title": "Exploiting Multi-Layer Grid Maps for Surround-View Semantic Segmentation\n  of Sparse LiDAR Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the transformation of laser range measurements\ninto a top-view grid map representation to approach the task of LiDAR-only\nsemantic segmentation. Since the recent publication of the SemanticKITTI data\nset, researchers are now able to study semantic segmentation of urban LiDAR\nsequences based on a reasonable amount of data. While other approaches propose\nto directly learn on the 3D point clouds, we are exploiting a grid map\nframework to extract relevant information and represent them by using\nmulti-layer grid maps. This representation allows us to use well-studied deep\nlearning architectures from the image domain to predict a dense semantic grid\nmap using only the sparse input data of a single LiDAR scan. We compare\nsingle-layer and multi-layer approaches and demonstrate the benefit of a\nmulti-layer grid map input. Since the grid map representation allows us to\npredict a dense, 360{\\deg} semantic environment representation, we further\ndevelop a method to combine the semantic information from multiple scans and\ncreate dense ground truth grids. This method allows us to evaluate and compare\nthe performance of our models not only based on grid cells with a detection,\nbut on the full visible measurement range.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 23:50:34 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Bieder", "Frank", ""], ["Wirges", "Sascha", ""], ["Janosovits", "Johannes", ""], ["Richter", "Sven", ""], ["Wang", "Zheyuan", ""], ["Stiller", "Christoph", ""]]}, {"id": "2005.06684", "submitter": "Abenezer Teklemariam", "authors": "Rohit Saha, Abenezer Teklemariam, Ian Hsu, Alan M. Moses", "title": "W-Cell-Net: Multi-frame Interpolation of Cellular Microscopy Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks are increasingly used in video frame interpolation tasks\nsuch as frame rate changes as well as generating fake face videos. Our project\naims to apply recent advances in Deep video interpolation to increase the\ntemporal resolution of fluorescent microscopy time-lapse movies. To our\nknowledge, there is no previous work that uses Convolutional Neural Networks\n(CNN) to generate frames between two consecutive microscopy images. We propose\na fully convolutional autoencoder network that takes as input two images and\ngenerates upto seven intermediate images. Our architecture has two encoders\neach with a skip connection to a single decoder. We evaluate the performance of\nseveral variants of our model that differ in network architecture and loss\nfunction. Our best model out-performs state of the art video frame\ninterpolation algorithms. We also show qualitative and quantitative comparisons\nwith state-of-the-art video frame interpolation algorithms. We believe deep\nvideo interpolation represents a new approach to improve the time-resolution of\nfluorescent microscopy.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 01:33:38 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Saha", "Rohit", ""], ["Teklemariam", "Abenezer", ""], ["Hsu", "Ian", ""], ["Moses", "Alan M.", ""]]}, {"id": "2005.06707", "submitter": "Shaoning Zeng", "authors": "Shaoning Zeng and Bob Zhang", "title": "Noise Homogenization via Multi-Channel Wavelet Filtering for\n  High-Fidelity Sample Generation in GANs", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the generator of typical Generative Adversarial Networks (GANs), a noise\nis inputted to generate fake samples via a series of convolutional operations.\nHowever, current noise generation models merely relies on the information from\nthe pixel space, which increases the difficulty to approach the target\ndistribution. Fortunately, the long proven wavelet transformation is able to\ndecompose multiple spectral information from the images. In this work, we\npropose a novel multi-channel wavelet-based filtering method for GANs, to cope\nwith this problem. When embedding a wavelet deconvolution layer in the\ngenerator, the resultant GAN, called WaveletGAN, takes advantage of the wavelet\ndeconvolution to learn a filtering with multiple channels, which can\nefficiently homogenize the generated noise via an averaging operation, so as to\ngenerate high-fidelity samples. We conducted benchmark experiments on the\nFashion-MNIST, KMNIST and SVHN datasets through an open GAN benchmark tool. The\nresults show that WaveletGAN has excellent performance in generating\nhigh-fidelity samples, thanks to the smallest FIDs obtained on these datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 03:40:11 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Zeng", "Shaoning", ""], ["Zhang", "Bob", ""]]}, {"id": "2005.06717", "submitter": "Qiuxia Lin", "authors": "Shuang Li, Chi Harold Liu, Qiuxia Lin, Binhui Xie, Zhengming Ding, Gao\n  Huang, Jian Tang", "title": "Domain Conditioned Adaptation Network", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous research efforts have been made to thrive deep domain adaptation\n(DA) by seeking domain-invariant features. Most existing deep DA models only\nfocus on aligning feature representations of task-specific layers across\ndomains while integrating a totally shared convolutional architecture for\nsource and target. However, we argue that such strongly-shared convolutional\nlayers might be harmful for domain-specific feature learning when source and\ntarget data distribution differs to a large extent. In this paper, we relax a\nshared-convnets assumption made by previous DA methods and propose a Domain\nConditioned Adaptation Network (DCAN), which aims to excite distinct\nconvolutional channels with a domain conditioned channel attention mechanism.\nAs a result, the critical low-level domain-dependent knowledge could be\nexplored appropriately. As far as we know, this is the first work to explore\nthe domain-wise convolutional channel activation for deep DA networks.\nMoreover, to effectively align high-level feature distributions across two\ndomains, we further deploy domain conditioned feature correction blocks after\ntask-specific layers, which will explicitly correct the domain discrepancy.\nExtensive experiments on three cross-domain benchmarks demonstrate the proposed\napproach outperforms existing methods by a large margin, especially on very\ntough cross-domain learning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 04:23:24 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Li", "Shuang", ""], ["Liu", "Chi Harold", ""], ["Lin", "Qiuxia", ""], ["Xie", "Binhui", ""], ["Ding", "Zhengming", ""], ["Huang", "Gao", ""], ["Tang", "Jian", ""]]}, {"id": "2005.06723", "submitter": "Przemek Gardias", "authors": "Przemek Gardias, Eric Arthur, Huaming Sun", "title": "Enhanced Residual Networks for Context-based Image Outpainting", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although humans perform well at predicting what exists beyond the boundaries\nof an image, deep models struggle to understand context and extrapolation\nthrough retained information. This task is known as image outpainting and\ninvolves generating realistic expansions of an image's boundaries. Current\nmodels use generative adversarial networks to generate results which lack\nlocalized image feature consistency and appear fake. We propose two methods to\nimprove this issue: the use of a local and global discriminator, and the\naddition of residual blocks within the encoding section of the network.\nComparisons of our model and the baseline's L1 loss, mean squared error (MSE)\nloss, and qualitative differences reveal our model is able to naturally extend\nobject boundaries and produce more internally consistent images compared to\ncurrent methods but produces lower fidelity images.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 05:14:26 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Gardias", "Przemek", ""], ["Arthur", "Eric", ""], ["Sun", "Huaming", ""]]}, {"id": "2005.06724", "submitter": "Siqi Li", "authors": "Siqi Li and Guobao Wang", "title": "Low-Dose CT Image Denoising Using Parallel-Clone Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have a great potential to improve image denoising in\nlow-dose computed tomography (LDCT). Popular ways to increase the network\ncapacity include adding more layers or repeating a modularized clone model in a\nsequence. In such sequential architectures, the noisy input image and end\noutput image are commonly used only once in the training model, which however\nlimits the overall learning performance. In this paper, we propose a\nparallel-clone neural network method that utilizes a modularized network model\nand exploits the benefit of parallel input, parallel-output loss, and\nclone-toclone feature transfer. The proposed model keeps a similar or less\nnumber of unknown network weights as compared to conventional models but can\naccelerate the learning process significantly. The method was evaluated using\nthe Mayo LDCT dataset and compared with existing deep learning models. The\nresults show that the use of parallel input, parallel-output loss, and\nclone-to-clone feature transfer all can contribute to an accelerated\nconvergence of deep learning and lead to improved image quality in testing. The\nparallel-clone network has been demonstrated promising for LDCT image\ndenoising.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 05:21:33 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Li", "Siqi", ""], ["Wang", "Guobao", ""]]}, {"id": "2005.06734", "submitter": "Shi Qiu", "authors": "Shi Qiu, Saeed Anwar, Nick Barnes", "title": "Dense-Resolution Network for Point Cloud Classification and Segmentation", "comments": "To appear in WACV2021. Codes and models are available at:\n  https://github.com/ShiQiu0419/DRNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud analysis is attracting attention from Artificial Intelligence\nresearch since it can be widely used in applications such as robotics,\nAugmented Reality, self-driving. However, it is always challenging due to\nirregularities, unorderedness, and sparsity. In this article, we propose a\nnovel network named Dense-Resolution Network (DRNet) for point cloud analysis.\nOur DRNet is designed to learn local point features from the point cloud in\ndifferent resolutions. In order to learn local point groups more effectively,\nwe present a novel grouping method for local neighborhood searching and an\nerror-minimizing module for capturing local features. In addition to validating\nthe network on widely used point cloud segmentation and classification\nbenchmarks, we also test and visualize the performance of the components.\nComparing with other state-of-the-art methods, our network shows superiority on\nModelNet40, ShapeNet synthetic and ScanObjectNN real point cloud datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 06:13:53 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 08:02:25 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Qiu", "Shi", ""], ["Anwar", "Saeed", ""], ["Barnes", "Nick", ""]]}, {"id": "2005.06739", "submitter": "Ali Khajegili Mirabadi", "authors": "Ali Khajegili Mirabadi and Stefano Rini", "title": "The Information & Mutual Information Ratio for Counting Image Features\n  and Their Matches", "comments": "8-th Iran Workshop on Communication and Information Theory, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction and description is an important topic of computer vision,\nas it is the starting point of a number of tasks such as image reconstruction,\nstitching, registration, and recognition among many others. In this paper, two\nnew image features are proposed: the Information Ratio (IR) and the Mutual\nInformation Ratio (MIR). The IR is a feature of a single image, while the MIR\ndescribes features common across two or more images.We begin by introducing the\nIR and the MIR and motivate these features in an information theoretical\ncontext as the ratio of the self-information of an intensity level over the\ninformation contained over the pixels of the same intensity. Notably, the\nrelationship of the IR and MIR with the image entropy and mutual information,\nclassic information measures, are discussed. Finally, the effectiveness of\nthese features is tested through feature extraction over INRIA Copydays\ndatasets and feature matching over the Oxfords Affine Covariant Regions. These\nnumerical evaluations validate the relevance of the IR and MIR in practical\ncomputer vision tasks\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 06:27:01 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Mirabadi", "Ali Khajegili", ""], ["Rini", "Stefano", ""]]}, {"id": "2005.06752", "submitter": "Atique Rehman", "authors": "Atique Ur Rehman, Sibt Ul Hussain", "title": "Large Scale Font Independent Urdu Text Recognition System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  OCR algorithms have received a significant improvement in performance\nrecently, mainly due to the increase in the capabilities of artificial\nintelligence algorithms. However, this advancement is not evenly distributed\nover all languages. Urdu is among the languages which did not receive much\nattention, especially in the font independent perspective. There exists no\nautomated system that can reliably recognize printed Urdu text in images and\nvideos across different fonts. To help bridge this gap, we have developed\nQaida, a large scale data set with 256 fonts, and a complete Urdu lexicon. We\nhave also developed a Convolutional Neural Network (CNN) based classification\nmodel which can recognize Urdu ligatures with 84.2% accuracy. Moreover, we\ndemonstrate that our recognition network can not only recognize the text in the\nfonts it is trained on but can also reliably recognize text in unseen (new)\nfonts. To this end, this paper makes following contributions: (i) we introduce\na large scale, multiple fonts based data set for printed Urdu text\nrecognition;(ii) we have designed, trained and evaluated a CNN based model for\nUrdu text recognition; (iii) we experiment with incremental learning methods to\nproduce state-of-the-art results for Urdu text recognition. All the experiment\nchoices were thoroughly validated via detailed empirical analysis. We believe\nthat this study can serve as the basis for further improvement in the\nperformance of font independent Urdu OCR systems.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 06:57:24 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Rehman", "Atique Ur", ""], ["Hussain", "Sibt Ul", ""]]}, {"id": "2005.06803", "submitter": "Limin Wang", "authors": "Zhaoyang Liu, Limin Wang, Wayne Wu, Chen Qian, Tong Lu", "title": "TAM: Temporal Adaptive Module for Video Recognition", "comments": "Code is available at\n  https://github.com/liu-zhy/temporal-adaptive-module", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video data is with complex temporal dynamics due to various factors such as\ncamera motion, speed variation, and different activities. To effectively\ncapture this diverse motion pattern, this paper presents a new temporal\nadaptive module (TAM) to generate video-specific temporal kernels based on its\nown feature maps. TAM proposes a unique two-level adaptive modeling scheme by\ndecoupling dynamic kernel into a location sensitive importance map and a\nlocation invariant aggregation weight. The importance map is learned in a local\ntemporal window to capture short term information, while the aggregation weight\nis generated from a global view with a focus on long-term structure. TAM is a\nprincipled module and could be integrated into 2D CNNs to yield a powerful\nvideo architecture (TANet) with a very small extra computational cost. The\nextensive experiments on Kinetics-400 and Something-Something datasets\ndemonstrate that the TAM outperforms other temporal modeling methods\nconsistently, and achieves the state-of-the-art performance under the similar\ncomplexity.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 08:22:45 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 02:00:40 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Liu", "Zhaoyang", ""], ["Wang", "Limin", ""], ["Wu", "Wayne", ""], ["Qian", "Chen", ""], ["Lu", "Tong", ""]]}, {"id": "2005.06821", "submitter": "Yehui Tang", "authors": "Yehui Tang, Yunhe Wang, Yixing Xu, Hanting Chen, Chunjing Xu, Boxin\n  Shi, Chao Xu, Qi Tian, Chang Xu", "title": "A Semi-Supervised Assessor of Neural Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) aims to automatically design deep neural\nnetworks of satisfactory performance. Wherein, architecture performance\npredictor is critical to efficiently value an intermediate neural architecture.\nBut for the training of this predictor, a number of neural architectures and\ntheir corresponding real performance often have to be collected. In contrast\nwith classical performance predictor optimized in a fully supervised way, this\npaper suggests a semi-supervised assessor of neural architectures. We employ an\nauto-encoder to discover meaningful representations of neural architectures.\nTaking each neural architecture as an individual instance in the search space,\nwe construct a graph to capture their intrinsic similarities, where both\nlabeled and unlabeled architectures are involved. A graph convolutional neural\nnetwork is introduced to predict the performance of architectures based on the\nlearned representations and their relation modeled by the graph. Extensive\nexperimental results on the NAS-Benchmark-101 dataset demonstrated that our\nmethod is able to make a significant reduction on the required fully trained\narchitectures for finding efficient architectures.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 09:02:33 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Tang", "Yehui", ""], ["Wang", "Yunhe", ""], ["Xu", "Yixing", ""], ["Chen", "Hanting", ""], ["Xu", "Chunjing", ""], ["Shi", "Boxin", ""], ["Xu", "Chao", ""], ["Tian", "Qi", ""], ["Xu", "Chang", ""]]}, {"id": "2005.06831", "submitter": "Philipp Oberdiek", "authors": "Philipp Oberdiek and Matthias Rottmann and Gernot A. Fink", "title": "Detection and Retrieval of Out-of-Distribution Objects in Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When deploying deep learning technology in self-driving cars, deep neural\nnetworks are constantly exposed to domain shifts. These include, e.g., changes\nin weather conditions, time of day, and long-term temporal shift. In this work\nwe utilize a deep neural network trained on the Cityscapes dataset containing\nurban street scenes and infer images from a different dataset, the A2D2\ndataset, containing also countryside and highway images. We present a novel\npipeline for semantic segmenation that detects out-of-distribution (OOD)\nsegments by means of the deep neural network's prediction and performs image\nretrieval after feature extraction and dimensionality reduction on image\npatches. In our experiments we demonstrate that the deployed OOD approach is\nsuitable for detecting out-of-distribution concepts. Furthermore, we evaluate\nthe image patch retrieval qualitatively as well as quantitatively by means of\nthe semi-compatible A2D2 ground truth and obtain mAP values of up to 52.2%.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 09:21:14 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Oberdiek", "Philipp", ""], ["Rottmann", "Matthias", ""], ["Fink", "Gernot A.", ""]]}, {"id": "2005.06833", "submitter": "Joao Antonio Alves Santinha", "authors": "Linda Bianchini, Joao Santinha, Nuno Lou\\c{c}\\~ao, Mario Figueiredo,\n  Francesca Botta, Daniela Origgi, Marta Cremonesi, Enrico Cassano, Nikolaos\n  Papanikolaou and Alessandro Lascialfari", "title": "A multicenter study on radiomic features from T$_2$-weighted images of a\n  customized MR pelvic phantom setting the basis for robust radiomic models in\n  clinics", "comments": "32 pages, 8 figures (7 + 1 supplemental), 8 tables (5 + 3\n  supplemental); Submitted to Magnetic Resonance in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we investigated the repeatability and reproducibility of\nradiomic features extracted from MRI images and provide a workflow to identify\nrobust features. 2D and 3D T$_2$-weighted images of a pelvic phantom were\nacquired on three scanners of two manufacturers and two magnetic field\nstrengths. The repeatability and reproducibility of the radiomic features were\nassessed respectively by intraclass correlation coefficient (ICC) and\nconcordance correlation coefficient (CCC), considering repeated acquisitions\nwith or without phantom repositioning, and with different scanner/acquisition\ntype, and acquisition parameters. The features showing ICC/CCC > 0.9 were\nselected, and their dependence on shape information (Spearman's $\\rho$> 0.8)\nwas analyzed. They were classified for their ability to distinguish textures,\nafter shuffling voxel intensities. From 944 2D features, 79.9% to 96.4% showed\nexcellent repeatability in fixed position across all scanners. Much lower range\n(11.2% to 85.4%) was obtained after phantom repositioning. 3D extraction did\nnot improve repeatability performance. Excellent reproducibility between\nscanners was observed in 4.6% to 15.6% of the features, at fixed imaging\nparameters. 82.4% to 94.9% of features showed excellent agreement when\nextracted from images acquired with TEs 5 ms apart (values decreased when\nincreasing TE intervals) and 90.7% of the features exhibited excellent\nreproducibility for changes in TR. 2.0% of non-shape features were identified\nas providing only shape information. This study demonstrates that radiomic\nfeatures are affected by specific MRI protocols. The use of our radiomic pelvic\nphantom allowed to identify unreliable features for radiomic analysis on\nT$_2$-weighted images. This paper proposes a general workflow to identify\nrepeatable, reproducible, and informative radiomic features, fundamental to\nensure robustness of clinical studies.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 09:24:48 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 12:46:35 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Bianchini", "Linda", ""], ["Santinha", "Joao", ""], ["Lou\u00e7\u00e3o", "Nuno", ""], ["Figueiredo", "Mario", ""], ["Botta", "Francesca", ""], ["Origgi", "Daniela", ""], ["Cremonesi", "Marta", ""], ["Cassano", "Enrico", ""], ["Papanikolaou", "Nikolaos", ""], ["Lascialfari", "Alessandro", ""]]}, {"id": "2005.06835", "submitter": "Baudouin Denis de Senneville PhD", "authors": "Baudouin Denis de Senneville, Jos\\'e V. Manjon, Pierrick Coup\\'e", "title": "RegQCNET: Deep Quality Control for Image-to-template Brain MRI Affine\n  Registration", "comments": null, "journal-ref": "Physics in Medicine and Biology, 2020", "doi": "10.1088/1361-6560/abb6be", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affine registration of one or several brain image(s) onto a common reference\nspace is a necessary prerequisite for many image processing tasks, such as\nbrain segmentation or functional analysis. Manual assessment of registration\nquality is a tedious and time-consuming task, especially in studies comprising\na large amount of data. An automated and reliable quality control (QC) becomes\nmandatory. Moreover, the computation time of the QC must be also compatible\nwith the processing of massive datasets. Therefore, an automated deep neural\nnetwork approaches appear as a method of choice to automatically assess\nregistration quality.\n  In the current study, a compact 3D convolutional neural network (CNN),\nreferred to as RegQCNET, is introduced to quantitatively predict the amplitude\nof an affine registration mismatch between a registered image and a reference\ntemplate. This quantitative estimation of registration error is expressed using\nmetric unit system. Therefore, a meaningful task-specific threshold can be\nmanually or automatically defined in order to distinguish usable and non-usable\nimages.\n  The robustness of the proposed RegQCNET is first analyzed on lifespan brain\nimages undergoing various simulated spatial transformations and intensity\nvariations between training and testing. Secondly, the potential of RegQCNET to\nclassify images as usable or non-usable is evaluated using both manual and\nautomatic thresholds. During our experiments, automatic thresholds are\nestimated using several computer-assisted classification models through\ncross-validation. To this end we used expert's visual quality control estimated\non a lifespan cohort of 3953 brains. Finally, the RegQCNET accuracy is compared\nto usual image features.\n  Results show that the proposed deep learning QC is robust, fast and accurate\nto estimate affine registration error in processing pipeline.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 09:27:24 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 16:58:46 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["de Senneville", "Baudouin Denis", ""], ["Manjon", "Jos\u00e9 V.", ""], ["Coup\u00e9", "Pierrick", ""]]}, {"id": "2005.06892", "submitter": "David Gschwend", "authors": "David Gschwend", "title": "ZynqNet: An FPGA-Accelerated Embedded Convolutional Neural Network", "comments": "85 pages, 26 figures. Code available at\n  http://github.com/dgschwend/zynqnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Understanding is becoming a vital feature in ever more applications\nranging from medical diagnostics to autonomous vehicles. Many applications\ndemand for embedded solutions that integrate into existing systems with tight\nreal-time and power constraints. Convolutional Neural Networks (CNNs) presently\nachieve record-breaking accuracies in all image understanding benchmarks, but\nhave a very high computational complexity. Embedded CNNs thus call for small\nand efficient, yet very powerful computing platforms. This master thesis\nexplores the potential of FPGA-based CNN acceleration and demonstrates a fully\nfunctional proof-of-concept CNN implementation on a Zynq System-on-Chip. The\nZynqNet Embedded CNN is designed for image classification on ImageNet and\nconsists of ZynqNet CNN, an optimized and customized CNN topology, and the\nZynqNet FPGA Accelerator, an FPGA-based architecture for its evaluation.\nZynqNet CNN is a highly efficient CNN topology. Detailed analysis and\noptimization of prior topologies using the custom-designed Netscope CNN\nAnalyzer have enabled a CNN with 84.5% top-5 accuracy at a computational\ncomplexity of only 530 million multiplyaccumulate operations. The topology is\nhighly regular and consists exclusively of convolutional layers, ReLU\nnonlinearities and one global pooling layer. The CNN fits ideally onto the FPGA\naccelerator. The ZynqNet FPGA Accelerator allows an efficient evaluation of\nZynqNet CNN. It accelerates the full network based on a nested-loop algorithm\nwhich minimizes the number of arithmetic operations and memory accesses. The\nFPGA accelerator has been synthesized using High-Level Synthesis for the Xilinx\nZynq XC-7Z045, and reaches a clock frequency of 200MHz with a device\nutilization of 80% to 90 %.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 11:54:04 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Gschwend", "David", ""]]}, {"id": "2005.06902", "submitter": "Muhammad Bilal", "authors": "Amin Ullah, Syed M. Anwar, Muhammad Bilal, and Raja M Mehmood", "title": "Classification of Arrhythmia by Using Deep Learning with 2-D ECG\n  Spectral Image Representation", "comments": "14 pages, 5 figures, accepted for future publication in Remote\n  Sensing MDPI Journal", "journal-ref": "Remote Sensing. 2020; 12(10):1685", "doi": "10.3390/rs12101685", "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electrocardiogram (ECG) is one of the most extensively employed signals\nused in the diagnosis and prediction of cardiovascular diseases (CVDs). The ECG\nsignals can capture the heart's rhythmic irregularities, commonly known as\narrhythmias. A careful study of ECG signals is crucial for precise diagnoses of\npatients' acute and chronic heart conditions. In this study, we propose a\ntwo-dimensional (2-D) convolutional neural network (CNN) model for the\nclassification of ECG signals into eight classes; namely, normal beat,\npremature ventricular contraction beat, paced beat, right bundle branch block\nbeat, left bundle branch block beat, atrial premature contraction beat,\nventricular flutter wave beat, and ventricular escape beat. The one-dimensional\nECG time series signals are transformed into 2-D spectrograms through\nshort-time Fourier transform. The 2-D CNN model consisting of four\nconvolutional layers and four pooling layers is designed for extracting robust\nfeatures from the input spectrograms. Our proposed methodology is evaluated on\na publicly available MIT-BIH arrhythmia dataset. We achieved a state-of-the-art\naverage classification accuracy of 99.11\\%, which is better than those of\nrecently reported results in classifying similar types of arrhythmias. The\nperformance is significant in other indices as well, including sensitivity and\nspecificity, which indicates the success of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 12:11:41 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 16:44:45 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ullah", "Amin", ""], ["Anwar", "Syed M.", ""], ["Bilal", "Muhammad", ""], ["Mehmood", "Raja M", ""]]}, {"id": "2005.06968", "submitter": "Xinsheng Wang", "authors": "Xinsheng Wang, Tingting Qiao, Jihua Zhu, Alan Hanjalic, Odette\n  Scharenborg", "title": "S2IGAN: Speech-to-Image Generation via Adversarial Learning", "comments": "Accepted to Interspeech2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An estimated half of the world's languages do not have a written form, making\nit impossible for these languages to benefit from any existing text-based\ntechnologies. In this paper, a speech-to-image generation (S2IG) framework is\nproposed which translates speech descriptions to photo-realistic images without\nusing any text information, thus allowing unwritten languages to potentially\nbenefit from this technology. The proposed S2IG framework, named S2IGAN,\nconsists of a speech embedding network (SEN) and a relation-supervised\ndensely-stacked generative model (RDG). SEN learns the speech embedding with\nthe supervision of the corresponding visual information. Conditioned on the\nspeech embedding produced by SEN, the proposed RDG synthesizes images that are\nsemantically consistent with the corresponding speech descriptions. Extensive\nexperiments on two public benchmark datasets CUB and Oxford-102 demonstrate the\neffectiveness of the proposed S2IGAN on synthesizing high-quality and\nsemantically-consistent images from the speech signal, yielding a good\nperformance and a solid baseline for the S2IG task.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 13:39:56 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 08:17:22 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Wang", "Xinsheng", ""], ["Qiao", "Tingting", ""], ["Zhu", "Jihua", ""], ["Hanjalic", "Alan", ""], ["Scharenborg", "Odette", ""]]}, {"id": "2005.07026", "submitter": "Fahad Shamshad", "authors": "Fahad Shamshad, Asif Hanif, Ali Ahmed", "title": "Subsampled Fourier Ptychography using Pretrained Invertible and\n  Untrained Network Priors", "comments": "Part of this work has been accepted in NeurIPS Deep Inverse Workshop,\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.IR cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently pretrained generative models have shown promising results for\nsubsampled Fourier Ptychography (FP) in terms of quality of reconstruction for\nextremely low sampling rate and high noise. However, one of the significant\ndrawbacks of these pretrained generative priors is their limited representation\ncapabilities. Moreover, training these generative models requires access to a\nlarge number of fully-observed clean samples of a particular class of images\nlike faces or digits that is prohibitive to obtain in the context of FP. In\nthis paper, we propose to leverage the power of pretrained invertible and\nuntrained generative models to mitigate the representation error issue and\nrequirement of a large number of example images (for training generative\nmodels) respectively. Through extensive experiments, we demonstrate the\neffectiveness of proposed approaches in the context of FP for low sampling\nrates and high noise levels.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 16:13:01 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Shamshad", "Fahad", ""], ["Hanif", "Asif", ""], ["Ahmed", "Ali", ""]]}, {"id": "2005.07058", "submitter": "Tuan Tran Anh", "authors": "Tuan Tran Anh, Khoa Nguyen-Tuan, Tran Minh Quan, and Won-Ki Jeong", "title": "Reinforced Coloring for End-to-End Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation is one of the actively studied research topics in\ncomputer vision in which many objects of interest should be separated\nindividually. While many feed-forward networks produce high-quality\nsegmentation on different types of images, their results often suffer from\ntopological errors (merging or splitting) for segmentation of many objects,\nrequiring post-processing. Existing iterative methods, on the other hand,\nextract a single object at a time using discriminative knowledge-based\nproperties (shapes, boundaries, etc.) without relying on post-processing, but\nthey do not scale well. To exploit the advantages of conventional\nsingle-object-per-step segmentation methods without impairing the scalability,\nwe propose a novel iterative deep reinforcement learning agent that learns how\nto differentiate multiple objects in parallel. Our reward function for the\ntrainable agent is designed to favor grouping pixels belonging to the same\nobject using a graph coloring algorithm. We demonstrate that the proposed\nmethod can efficiently perform instance segmentation of many objects without\nheavy post-processing.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 15:15:47 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 02:40:36 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Anh", "Tuan Tran", ""], ["Nguyen-Tuan", "Khoa", ""], ["Quan", "Tran Minh", ""], ["Jeong", "Won-Ki", ""]]}, {"id": "2005.07068", "submitter": "Manh Duong Phung", "authors": "Cong Hoang Quach, Minh Trien Pham, Anh Viet Dang, Dinh Tuan Pham,\n  Thuan Hoang Tran, Manh Duong Phung", "title": "Recognition of 26 Degrees of Freedom of Hands Using Model-based approach\n  and Depth-Color Images", "comments": "in Proceedings of the 2014 National Conference on Electronics,\n  Communications and Information Technology (REV-ECIT). in Vietnamese language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present an model-based approach to recognize full 26\ndegrees of freedom of a human hand. Input data include RGB-D images acquired\nfrom a Kinect camera and a 3D model of the hand constructed from its anatomy\nand graphical matrices. A cost function is then defined so that its minimum\nvalue is achieved when the model and observation images are matched. To solve\nthe optimization problem in 26 dimensional space, the particle swarm\noptimization algorimth with improvements are used. In addition, parallel\ncomputation in graphical processing units (GPU) is utilized to handle\ncomputationally expensive tasks. Simulation and experimental results show that\nthe system can recognize 26 degrees of freedom of hands with the processing\ntime of 0.8 seconds per frame. The algorithm is robust to noise and the\nhardware requirement is simple with a single camera.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 08:05:30 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Quach", "Cong Hoang", ""], ["Pham", "Minh Trien", ""], ["Dang", "Anh Viet", ""], ["Pham", "Dinh Tuan", ""], ["Tran", "Thuan Hoang", ""], ["Phung", "Manh Duong", ""]]}, {"id": "2005.07069", "submitter": "Sebastian Lunz", "authors": "Sebastian Lunz, Andreas Hauptmann, Tanja Tarvainen, Carola-Bibiane\n  Sch\\\"onlieb, Simon Arridge", "title": "On Learned Operator Correction in Inverse Problems", "comments": "28 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.LG cs.NA eess.IV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the possibility to learn a data-driven explicit model correction\nfor inverse problems and whether such a model correction can be used within a\nvariational framework to obtain regularised reconstructions. This paper\ndiscusses the conceptual difficulty to learn such a forward model correction\nand proceeds to present a possible solution as forward-adjoint correction that\nexplicitly corrects in both data and solution spaces. We then derive conditions\nunder which solutions to the variational problem with a learned correction\nconverge to solutions obtained with the correct operator. The proposed approach\nis evaluated on an application to limited view photoacoustic tomography and\ncompared to the established framework of Bayesian approximation error method.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 15:37:28 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 13:41:59 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Lunz", "Sebastian", ""], ["Hauptmann", "Andreas", ""], ["Tarvainen", "Tanja", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Arridge", "Simon", ""]]}, {"id": "2005.07074", "submitter": "Joon Son Chung", "authors": "Soo-Whan Chung, Soyeon Choe, Joon Son Chung, Hong-Goo Kang", "title": "FaceFilter: Audio-visual speech separation using still images", "comments": "Under submission as a conference paper. Video examples:\n  https://youtu.be/ku9xoLh62E", "journal-ref": null, "doi": "10.21437/Interspeech.2020-1065", "report-no": null, "categories": "cs.SD cs.CV cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to separate a target speaker's speech from a\nmixture of two speakers using a deep audio-visual speech separation network.\nUnlike previous works that used lip movement on video clips or pre-enrolled\nspeaker information as an auxiliary conditional feature, we use a single face\nimage of the target speaker. In this task, the conditional feature is obtained\nfrom facial appearance in cross-modal biometric task, where audio and visual\nidentity representations are shared in latent space. Learnt identities from\nfacial images enforce the network to isolate matched speakers and extract the\nvoices from mixed speech. It solves the permutation problem caused by swapped\nchannel outputs, frequently occurred in speech separation tasks. The proposed\nmethod is far more practical than video-based speech separation since user\nprofile images are readily available on many platforms. Also, unlike\nspeaker-aware separation methods, it is applicable on separation with unseen\nspeakers who have never been enrolled before. We show strong qualitative and\nquantitative results on challenging real-world examples.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 15:42:31 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Chung", "Soo-Whan", ""], ["Choe", "Soyeon", ""], ["Chung", "Joon Son", ""], ["Kang", "Hong-Goo", ""]]}, {"id": "2005.07093", "submitter": "Mart van Baalen", "authors": "Mart van Baalen and Christos Louizos and Markus Nagel and Rana Ali\n  Amjad and Ying Wang and Tijmen Blankevoort and Max Welling", "title": "Bayesian Bits: Unifying Quantization and Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Bayesian Bits, a practical method for joint mixed precision\nquantization and pruning through gradient based optimization. Bayesian Bits\nemploys a novel decomposition of the quantization operation, which sequentially\nconsiders doubling the bit width. At each new bit width, the residual error\nbetween the full precision value and the previously rounded value is quantized.\nWe then decide whether or not to add this quantized residual error for a higher\neffective bit width and lower quantization noise. By starting with a\npower-of-two bit width, this decomposition will always produce\nhardware-friendly configurations, and through an additional 0-bit option,\nserves as a unified view of pruning and quantization. Bayesian Bits then\nintroduces learnable stochastic gates, which collectively control the bit width\nof the given tensor. As a result, we can obtain low bit solutions by performing\napproximate inference over the gates, with prior distributions that encourage\nmost of them to be switched off. We experimentally validate our proposed method\non several benchmark datasets and show that we can learn pruned, mixed\nprecision networks that provide a better trade-off between accuracy and\nefficiency than their static bit width equivalents.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 16:00:34 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 10:10:46 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 11:27:24 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["van Baalen", "Mart", ""], ["Louizos", "Christos", ""], ["Nagel", "Markus", ""], ["Amjad", "Rana Ali", ""], ["Wang", "Ying", ""], ["Blankevoort", "Tijmen", ""], ["Welling", "Max", ""]]}, {"id": "2005.07097", "submitter": "Yuansheng Hua", "authors": "Di Hu, Lichao Mou, Qingzhong Wang, Junyu Gao, Yuansheng Hua, Dejing\n  Dou, Xiao Xiang Zhu", "title": "Ambient Sound Helps: Audiovisual Crowd Counting in Extreme Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual crowd counting has been recently studied as a way to enable people\ncounting in crowd scenes from images. Albeit successful, vision-based crowd\ncounting approaches could fail to capture informative features in extreme\nconditions, e.g., imaging at night and occlusion. In this work, we introduce a\nnovel task of audiovisual crowd counting, in which visual and auditory\ninformation are integrated for counting purposes. We collect a large-scale\nbenchmark, named auDiovISual Crowd cOunting (DISCO) dataset, consisting of\n1,935 images and the corresponding audio clips, and 170,270 annotated\ninstances. In order to fuse the two modalities, we make use of a linear\nfeature-wise fusion module that carries out an affine transformation on visual\nand auditory features. Finally, we conduct extensive experiments using the\nproposed dataset and approach. Experimental results show that introducing\nauditory information can benefit crowd counting under different illumination,\nnoise, and occlusion conditions. The dataset and code will be released. Code\nand data have been made available\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 16:05:47 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 20:56:26 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Hu", "Di", ""], ["Mou", "Lichao", ""], ["Wang", "Qingzhong", ""], ["Gao", "Junyu", ""], ["Hua", "Yuansheng", ""], ["Dou", "Dejing", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2005.07110", "submitter": "Duarte Rondao", "authors": "Duarte Rondao, Nabil Aouf, Mark A. Richardson, Vincent Dubanchet", "title": "Robust On-Manifold Optimization for Uncooperative Space Relative\n  Navigation with a Single Camera", "comments": "42 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical cameras are gaining popularity as the suitable sensor for relative\nnavigation in space due to their attractive sizing, power and cost properties\nwhen compared to conventional flight hardware or costly laser-based systems.\nHowever, a camera cannot infer depth information on its own, which is often\nsolved by introducing complementary sensors or a second camera. In this paper,\nan innovative model-based approach is instead demonstrated to estimate the\nsix-dimensional pose of a target object relative to the chaser spacecraft using\nsolely a monocular setup. The observed facet of the target is tackled as a\nclassification problem, where the three-dimensional shape is learned offline\nusing Gaussian mixture modeling. The estimate is refined by minimizing two\ndifferent robust loss functions based on local feature correspondences. The\nresulting pseudo-measurements are then processed and fused with an extended\nKalman filter. The entire optimization framework is designed to operate\ndirectly on the $SE\\text{(3)}$ manifold, uncoupling the process and measurement\nmodels from the global attitude state representation. It is validated on\nrealistic synthetic and laboratory datasets of a rendezvous trajectory with the\ncomplex spacecraft Envisat. It is demonstrated how it achieves an estimate of\nthe relative pose with high accuracy over its full tumbling motion.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 16:23:04 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Rondao", "Duarte", ""], ["Aouf", "Nabil", ""], ["Richardson", "Mark A.", ""], ["Dubanchet", "Vincent", ""]]}, {"id": "2005.07133", "submitter": "Shiyu Li", "authors": "Shiyu Li, Edward Hanson, Hai Li, Yiran Chen", "title": "PENNI: Pruned Kernel Sharing for Efficient CNN Inference", "comments": "9 pages, 5 figures, to appear on ICML2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Although state-of-the-art (SOTA) CNNs achieve outstanding performance on\nvarious tasks, their high computation demand and massive number of parameters\nmake it difficult to deploy these SOTA CNNs onto resource-constrained devices.\nPrevious works on CNN acceleration utilize low-rank approximation of the\noriginal convolution layers to reduce computation cost. However, these methods\nare very difficult to conduct upon sparse models, which limits execution\nspeedup since redundancies within the CNN model are not fully exploited. We\nargue that kernel granularity decomposition can be conducted with low-rank\nassumption while exploiting the redundancy within the remaining compact\ncoefficients. Based on this observation, we propose PENNI, a CNN model\ncompression framework that is able to achieve model compactness and hardware\nefficiency simultaneously by (1) implementing kernel sharing in convolution\nlayers via a small number of basis kernels and (2) alternately adjusting bases\nand coefficients with sparse constraints. Experiments show that we can prune\n97% parameters and 92% FLOPs on ResNet18 CIFAR10 with no accuracy loss, and\nachieve 44% reduction in run-time memory consumption and a 53% reduction in\ninference latency.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 16:57:41 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 02:28:00 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Li", "Shiyu", ""], ["Hanson", "Edward", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "2005.07151", "submitter": "Tianhang Zheng", "authors": "Tianhang Zheng, Sheng Liu, Changyou Chen, Junsong Yuan, Baochun Li,\n  Kui Ren", "title": "Towards Understanding the Adversarial Vulnerability of Skeleton-based\n  Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton-based action recognition has attracted increasing attention due to\nits strong adaptability to dynamic circumstances and potential for broad\napplications such as autonomous and anonymous surveillance. With the help of\ndeep learning techniques, it has also witnessed substantial progress and\ncurrently achieved around 90\\% accuracy in benign environment. On the other\nhand, research on the vulnerability of skeleton-based action recognition under\ndifferent adversarial settings remains scant, which may raise security concerns\nabout deploying such techniques into real-world systems. However, filling this\nresearch gap is challenging due to the unique physical constraints of skeletons\nand human actions. In this paper, we attempt to conduct a thorough study\ntowards understanding the adversarial vulnerability of skeleton-based action\nrecognition. We first formulate generation of adversarial skeleton actions as a\nconstrained optimization problem by representing or approximating the\nphysiological and physical constraints with mathematical formulations. Since\nthe primal optimization problem with equality constraints is intractable, we\npropose to solve it by optimizing its unconstrained dual problem using ADMM. We\nthen specify an efficient plug-in defense, inspired by recent theories and\nempirical observations, against the adversarial skeleton actions. Extensive\nevaluations demonstrate the effectiveness of the attack and defense method\nunder different settings.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 17:12:52 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 15:21:59 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Zheng", "Tianhang", ""], ["Liu", "Sheng", ""], ["Chen", "Changyou", ""], ["Yuan", "Junsong", ""], ["Li", "Baochun", ""], ["Ren", "Kui", ""]]}, {"id": "2005.07178", "submitter": "Kelvin Wong", "authors": "Lila Huang, Shenlong Wang, Kelvin Wong, Jerry Liu, Raquel Urtasun", "title": "OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression", "comments": "CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep compression algorithm to reduce the memory footprint\nof LiDAR point clouds. Our method exploits the sparsity and structural\nredundancy between points to reduce the bitrate. Towards this goal, we first\nencode the LiDAR points into an octree, a data-efficient structure suitable for\nsparse point clouds. We then design a tree-structured conditional entropy model\nthat models the probabilities of the octree symbols to encode the octree into a\ncompact bitstream. We validate the effectiveness of our method over two\nlarge-scale datasets. The results demonstrate that our approach reduces the\nbitrate by 10-20% at the same reconstruction quality, compared to the previous\nstate-of-the-art. Importantly, we also show that for the same bitrate, our\napproach outperforms other compression algorithms when performing downstream 3D\nsegmentation and detection tasks using compressed representations. Our\nalgorithm can be used to reduce the onboard and offboard storage of LiDAR\npoints for applications such as self-driving cars, where a single vehicle\ncaptures 84 billion points per day\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 17:48:49 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 22:27:07 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Huang", "Lila", ""], ["Wang", "Shenlong", ""], ["Wong", "Kelvin", ""], ["Liu", "Jerry", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2005.07225", "submitter": "Saisri Padmaja Jonnalagedda", "authors": "Padmaja Jonnalagedda, Brent Weinberg (MD, PhD), Jason Allen (MD, PhD),\n  Taejin L. Min (MD), Shiv Bhanu (MD), Bir Bhanu", "title": "SAGE: Sequential Attribute Generator for Analyzing Glioblastomas using\n  Limited Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning approaches have shown remarkable performance in many\nimaging tasks, most of these methods rely on availability of large quantities\nof data. Medical image data, however, is scarce and fragmented. Generative\nAdversarial Networks (GANs) have recently been very effective in handling such\ndatasets by generating more data. If the datasets are very small, however, GANs\ncannot learn the data distribution properly, resulting in less diverse or\nlow-quality results. One such limited dataset is that for the concurrent gain\nof 19 and 20 chromosomes (19/20 co-gain), a mutation with positive prognostic\nvalue in Glioblastomas (GBM). In this paper, we detect imaging biomarkers for\nthe mutation to streamline the extensive and invasive prognosis pipeline. Since\nthis mutation is relatively rare, i.e. small dataset, we propose a novel\ngenerative framework - the Sequential Attribute GEnerator (SAGE), that\ngenerates detailed tumor imaging features while learning from a limited\ndataset. Experiments show that not only does SAGE generate high quality tumors\nwhen compared to standard Deep Convolutional GAN (DC-GAN) and Wasserstein GAN\nwith Gradient Penalty (WGAN-GP), it also captures the imaging biomarkers\naccurately.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 19:14:28 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Jonnalagedda", "Padmaja", "", "MD, PhD"], ["Weinberg", "Brent", "", "MD, PhD"], ["Allen", "Jason", "", "MD, PhD"], ["Min", "Taejin L.", "", "MD"], ["Bhanu", "Shiv", "", "MD"], ["Bhanu", "Bir", ""]]}, {"id": "2005.07229", "submitter": "Iam Palatnik de Sousa", "authors": "Iam Palatnik de Sousa, Marley Maria Bernardes Rebuzzi Vellasco,\n  Eduardo Costa da Silva", "title": "Evolved Explainable Classifications for Lymph Node Metastases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel evolutionary approach for Explainable Artificial Intelligence is\npresented: the \"Evolved Explanations\" model (EvEx). This methodology consists\nin combining Local Interpretable Model Agnostic Explanations (LIME) with\nMulti-Objective Genetic Algorithms to allow for automated segmentation\nparameter tuning in image classification tasks. In this case, the dataset\nstudied is Patch-Camelyon, comprised of patches from pathology whole slide\nimages. A publicly available Convolutional Neural Network (CNN) was trained on\nthis dataset to provide a binary classification for presence/absence of lymph\nnode metastatic tissue. In turn, the classifications are explained by means of\nevolving segmentations, seeking to optimize three evaluation goals\nsimultaneously. The final explanation is computed as the mean of all\nexplanations generated by Pareto front individuals, evolved by the developed\ngenetic algorithm. To enhance reproducibility and traceability of the\nexplanations, each of them was generated from several different seeds, randomly\nchosen. The observed results show remarkable agreement between different seeds.\nDespite the stochastic nature of LIME explanations, regions of high explanation\nweights proved to have good agreement in the heat maps, as computed by\npixel-wise relative standard deviations. The found heat maps coincide with\nexpert medical segmentations, which demonstrates that this methodology can find\nhigh quality explanations (according to the evaluation metrics), with the novel\nadvantage of automated parameter fine tuning. These results give additional\ninsight into the inner workings of neural network black box decision making for\nmedical data.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 19:27:24 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["de Sousa", "Iam Palatnik", ""], ["Vellasco", "Marley Maria Bernardes Rebuzzi", ""], ["da Silva", "Eduardo Costa", ""]]}, {"id": "2005.07232", "submitter": "Lei Ding", "authors": "Lei Ding, Lorenzo Bruzzone", "title": "DiResNet: Direction-aware Residual Network for Road Extraction in VHR\n  Remote Sensing Images", "comments": "12 pages, 13 figures. IEEE Transactions on Geoscience and Remote\n  Sensing, 2020", "journal-ref": "Ding L, Bruzzone L. DiResNet: Direction-Aware Residual Network for\n  Road Extraction in VHR Remote Sensing Images[J]. IEEE Transactions on\n  Geoscience and Remote Sensing, 2020", "doi": "10.1109/TGRS.2020.3034011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The binary segmentation of roads in very high resolution (VHR) remote sensing\nimages (RSIs) has always been a challenging task due to factors such as\nocclusions (caused by shadows, trees, buildings, etc.) and the intra-class\nvariances of road surfaces. The wide use of convolutional neural networks\n(CNNs) has greatly improved the segmentation accuracy and made the task\nend-to-end trainable. However, there are still margins to improve in terms of\nthe completeness and connectivity of the results. In this paper, we consider\nthe specific context of road extraction and present a direction-aware residual\nnetwork (DiResNet) that includes three main contributions: 1) An asymmetric\nresidual segmentation network with deconvolutional layers and a structural\nsupervision to enhance the learning of road topology (DiResSeg); 2) A\npixel-level supervision of local directions to enhance the embedding of linear\nfeatures; 3) A refinement network to optimize the segmentation results\n(DiResRef). Ablation studies on two benchmark datasets (the Massachusetts\ndataset and the DeepGlobe dataset) have confirmed the effectiveness of the\npresented designs. Comparative experiments with other approaches show that the\nproposed method has advantages in both overall accuracy and F1-score. The code\nis available at: https://github.com/ggsDing/DiResNet.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 19:33:21 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 21:44:08 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ding", "Lei", ""], ["Bruzzone", "Lorenzo", ""]]}, {"id": "2005.07274", "submitter": "Abhishek Badki", "authors": "Abhishek Badki, Alejandro Troccoli, Kihwan Kim, Jan Kautz, Pradeep\n  Sen, Orazio Gallo", "title": "Bi3D: Stereo Depth Estimation via Binary Classifications", "comments": "To be presented at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo-based depth estimation is a cornerstone of computer vision, with\nstate-of-the-art methods delivering accurate results in real time. For several\napplications such as autonomous navigation, however, it may be useful to trade\naccuracy for lower latency. We present Bi3D, a method that estimates depth via\na series of binary classifications. Rather than testing if objects are at a\nparticular depth $D$, as existing stereo methods do, it classifies them as\nbeing closer or farther than $D$. This property offers a powerful mechanism to\nbalance accuracy and latency. Given a strict time budget, Bi3D can detect\nobjects closer than a given distance in as little as a few milliseconds, or\nestimate depth with arbitrarily coarse quantization, with complexity linear\nwith the number of quantization levels. Bi3D can also use the allotted\nquantization levels to get continuous depth, but in a specific depth range. For\nstandard stereo (i.e., continuous depth on the whole range), our method is\nclose to or on par with state-of-the-art, finely tuned stereo methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 21:33:00 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 16:44:34 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Badki", "Abhishek", ""], ["Troccoli", "Alejandro", ""], ["Kim", "Kihwan", ""], ["Kautz", "Jan", ""], ["Sen", "Pradeep", ""], ["Gallo", "Orazio", ""]]}, {"id": "2005.07277", "submitter": "Pingping Lu", "authors": "Pingping Lu, Chen Cui, Shaobing Xu, Huei Peng, Fan Wang", "title": "SUPER: A Novel Lane Detection System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI-based lane detection algorithms were actively studied over the last few\nyears. Many have demonstrated superior performance compared with traditional\nfeature-based methods. The accuracy, however, is still generally in the low 80%\nor high 90%, or even lower when challenging images are used. In this paper, we\npropose a real-time lane detection system, called Scene Understanding\nPhysics-Enhanced Real-time (SUPER) algorithm. The proposed method consists of\ntwo main modules: 1) a hierarchical semantic segmentation network as the scene\nfeature extractor and 2) a physics enhanced multi-lane parameter optimization\nmodule for lane inference. We train the proposed system using heterogeneous\ndata from Cityscapes, Vistas and Apollo, and evaluate the performance on four\ncompletely separate datasets (that were never seen before), including Tusimple,\nCaltech, URBAN KITTI-ROAD, and X-3000. The proposed approach performs the same\nor better than lane detection models already trained on the same dataset and\nperforms well even on datasets it was never trained on. Real-world vehicle\ntests were also conducted. Preliminary test results show promising real-time\nlane-detection performance compared with the Mobileye.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 21:40:39 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Lu", "Pingping", ""], ["Cui", "Chen", ""], ["Xu", "Shaobing", ""], ["Peng", "Huei", ""], ["Wang", "Fan", ""]]}, {"id": "2005.07289", "submitter": "Soren Pirk", "authors": "Yao Lu, S\\\"oren Pirk, Jan Dlabal, Anthony Brohan, Ankita Pasad, Zhao\n  Chen, Vincent Casser, Anelia Angelova, Ariel Gordon", "title": "Taskology: Utilizing Task Relations at Scale", "comments": "IEEE Conference on Computer Vision and Pattern Recognition, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision tasks address the problem of scene understanding and are\nnaturally interrelated e.g. object classification, detection, scene\nsegmentation, depth estimation, etc. We show that we can leverage the inherent\nrelationships among collections of tasks, as they are trained jointly,\nsupervising each other through their known relationships via consistency\nlosses. Furthermore, explicitly utilizing the relationships between tasks\nallows improving their performance while dramatically reducing the need for\nlabeled data, and allows training with additional unsupervised or simulated\ndata. We demonstrate a distributed joint training algorithm with task-level\nparallelism, which affords a high degree of asynchronicity and robustness. This\nallows learning across multiple tasks, or with large amounts of input data, at\nscale. We demonstrate our framework on subsets of the following collection of\ntasks: depth and normal prediction, semantic segmentation, 3D motion and\nego-motion estimation, and object tracking and 3D detection in point clouds. We\nobserve improved performance across these tasks, especially in the low-label\nregime.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 22:53:46 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 04:10:16 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Lu", "Yao", ""], ["Pirk", "S\u00f6ren", ""], ["Dlabal", "Jan", ""], ["Brohan", "Anthony", ""], ["Pasad", "Ankita", ""], ["Chen", "Zhao", ""], ["Casser", "Vincent", ""], ["Angelova", "Anelia", ""], ["Gordon", "Ariel", ""]]}, {"id": "2005.07298", "submitter": "Mohammad Rami Koujan", "authors": "Mohammad Rami Koujan, Anastasios Roussos, Stefanos Zafeiriou", "title": "DeepFaceFlow: In-the-wild Dense 3D Facial Motion Estimation", "comments": "to be published in the IEEE conference on Computer Vision and Pattern\n  Recognition (CVPR). 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense 3D facial motion capture from only monocular in-the-wild pairs of RGB\nimages is a highly challenging problem with numerous applications, ranging from\nfacial expression recognition to facial reenactment. In this work, we propose\nDeepFaceFlow, a robust, fast, and highly-accurate framework for the dense\nestimation of 3D non-rigid facial flow between pairs of monocular images. Our\nDeepFaceFlow framework was trained and tested on two very large-scale facial\nvideo datasets, one of them of our own collection and annotation, with the aid\nof occlusion-aware and 3D-based loss function. We conduct comprehensive\nexperiments probing different aspects of our approach and demonstrating its\nimproved performance against state-of-the-art flow and 3D reconstruction\nmethods. Furthermore, we incorporate our framework in a full-head\nstate-of-the-art facial video synthesis method and demonstrate the ability of\nour method in better representing and capturing the facial dynamics, resulting\nin a highly-realistic facial video synthesis. Given registered pairs of images,\nour framework generates 3D flow maps at ~60 fps.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 23:56:48 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Koujan", "Mohammad Rami", ""], ["Roussos", "Anastasios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2005.07302", "submitter": "Markos Georgopoulos", "authors": "Markos Georgopoulos, Yannis Panagakis, Maja Pantic", "title": "Investigating Bias in Deep Face Analysis: The KANFace Dataset and\n  Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based methods have pushed the limits of the state-of-the-art in\nface analysis. However, despite their success, these models have raised\nconcerns regarding their bias towards certain demographics. This bias is\ninflicted both by limited diversity across demographics in the training set, as\nwell as the design of the algorithms. In this work, we investigate the\ndemographic bias of deep learning models in face recognition, age estimation,\ngender recognition and kinship verification. To this end, we introduce the most\ncomprehensive, large-scale dataset of facial images and videos to date. It\nconsists of 40K still images and 44K sequences (14.5M video frames in total)\ncaptured in unconstrained, real-world conditions from 1,045 subjects. The data\nare manually annotated in terms of identity, exact age, gender and kinship. The\nperformance of state-of-the-art models is scrutinized and demographic bias is\nexposed by conducting a series of experiments. Lastly, a method to debias\nnetwork embeddings is introduced and tested on the proposed benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 00:14:39 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 02:00:26 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Georgopoulos", "Markos", ""], ["Panagakis", "Yannis", ""], ["Pantic", "Maja", ""]]}, {"id": "2005.07310", "submitter": "Jize Cao", "authors": "Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen and Jingjing\n  Liu", "title": "Behind the Scene: Revealing the Secrets of Pre-trained\n  Vision-and-Language Models", "comments": "Accepted by ECCV 2020 as Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent Transformer-based large-scale pre-trained models have revolutionized\nvision-and-language (V+L) research. Models such as ViLBERT, LXMERT and UNITER\nhave significantly lifted state of the art across a wide range of V+L\nbenchmarks with joint image-text pre-training. However, little is known about\nthe inner mechanisms that destine their impressive success. To reveal the\nsecrets behind the scene of these powerful models, we present VALUE\n(Vision-And-Language Understanding Evaluation), a set of meticulously designed\nprobing tasks (e.g., Visual Coreference Resolution, Visual Relation Detection,\nLinguistic Probing Tasks) generalizable to standard pre-trained V+L models,\naiming to decipher the inner workings of multimodal pre-training (e.g., the\nimplicit knowledge garnered in individual attention heads, the inherent\ncross-modal alignment learned through contextualized multimodal embeddings).\nThrough extensive analysis of each archetypal model architecture via these\nprobing tasks, our key observations are: (i) Pre-trained models exhibit a\npropensity for attending over text rather than images during inference. (ii)\nThere exists a subset of attention heads that are tailored for capturing\ncross-modal interactions. (iii) Learned attention matrix in pre-trained models\ndemonstrates patterns coherent with the latent alignment between image regions\nand textual words. (iv) Plotted attention patterns reveal\nvisually-interpretable relations among image regions. (v) Pure linguistic\nknowledge is also effectively encoded in the attention heads. These are\nvaluable insights serving to guide future work towards designing better model\narchitecture and objectives for multimodal pre-training.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 01:06:54 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 23:10:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Cao", "Jize", ""], ["Gan", "Zhe", ""], ["Cheng", "Yu", ""], ["Yu", "Licheng", ""], ["Chen", "Yen-Chun", ""], ["Liu", "Jingjing", ""]]}, {"id": "2005.07327", "submitter": "Zhe Wang", "authors": "Zhe Wang, Zhiyuan Fang, Jun Wang, Yezhou Yang", "title": "ViTAA: Visual-Textual Attributes Alignment in Person Search by Natural\n  Language", "comments": "ECCV2020, 18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person search by natural language aims at retrieving a specific person in a\nlarge-scale image pool that matches the given textual descriptions. While most\nof the current methods treat the task as a holistic visual and textual feature\nmatching one, we approach it from an attribute-aligning perspective that allows\ngrounding specific attribute phrases to the corresponding visual regions. We\nachieve success as well as the performance boosting by a robust feature\nlearning that the referred identity can be accurately bundled by multiple\nattribute visual cues. To be concrete, our Visual-Textual Attribute Alignment\nmodel (dubbed as ViTAA) learns to disentangle the feature space of a person\ninto subspaces corresponding to attributes using a light auxiliary attribute\nsegmentation computing branch. It then aligns these visual features with the\ntextual attributes parsed from the sentences by using a novel contrastive\nlearning loss. Upon that, we validate our ViTAA framework through extensive\nexperiments on tasks of person search by natural language and by\nattribute-phrase queries, on which our system achieves state-of-the-art\nperformances. Code will be publicly available upon publication.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 02:22:28 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 07:05:00 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Wang", "Zhe", ""], ["Fang", "Zhiyuan", ""], ["Wang", "Jun", ""], ["Yang", "Yezhou", ""]]}, {"id": "2005.07343", "submitter": "Maojing Shu", "authors": "Xiaoxiao Li, Xiaopeng Guo, Liye Mei, Mingyu Shang, Jie Gao, Maojing\n  Shu, and Xiang Wang", "title": "Visual Perception Model for Rapid and Adaptive Low-light Image\n  Enhancement", "comments": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract here is shorter than that in the PDF file", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-light image enhancement is a promising solution to tackle the problem of\ninsufficient sensitivity of human vision system (HVS) to perceive information\nin low light environments. Previous Retinex-based works always accomplish\nenhancement task by estimating light intensity. Unfortunately, single light\nintensity modelling is hard to accurately simulate visual perception\ninformation, leading to the problems of imbalanced visual photosensitivity and\nweak adaptivity. To solve these problems, we explore the precise relationship\nbetween light source and visual perception and then propose the visual\nperception (VP) model to acquire a precise mathematical description of visual\nperception. The core of VP model is to decompose the light source into light\nintensity and light spatial distribution to describe the perception process of\nHVS, offering refinement estimation of illumination and reflectance. To reduce\ncomplexity of the estimation process, we introduce the rapid and adaptive\n$\\mathbf{\\beta}$ and $\\mathbf{\\gamma}$ functions to build an illumination and\nreflectance estimation scheme. Finally, we present a optimal determination\nstrategy, consisting of a \\emph{cycle operation} and a \\emph{comparator}.\nSpecifically, the \\emph{comparator} is responsible for determining the optimal\nenhancement results from multiple enhanced results through implementing the\n\\emph{cycle operation}. By coordinating the proposed VP model, illumination and\nreflectance estimation scheme, and the optimal determination strategy, we\npropose a rapid and adaptive framework for low-light image enhancement.\nExtensive experiment results demenstrate that the proposed method achieves\nbetter performance in terms of visual comparison, quantitative assessment, and\ncomputational efficiency, compared with the currently state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 03:47:10 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Guo", "Xiaopeng", ""], ["Mei", "Liye", ""], ["Shang", "Mingyu", ""], ["Gao", "Jie", ""], ["Shu", "Maojing", ""], ["Wang", "Xiang", ""]]}, {"id": "2005.07344", "submitter": "Zhe Wang", "authors": "Zhe Wang, Jun Wang, Yezhou Yang", "title": "Resisting Crowd Occlusion and Hard Negatives for Pedestrian Detection in\n  the Wild", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection has been heavily studied in the last decade due to its\nwide application. Despite incremental progress, crowd occlusion and hard\nnegatives are still challenging current state-of-the-art pedestrian detectors.\nIn this paper, we offer two approaches based on the general region-based\ndetection framework to tackle these challenges. Specifically, to address the\nocclusion, we design a novel coulomb loss as a regulator on bounding box\nregression, in which proposals are attracted by their target instance and\nrepelled by the adjacent non-target instances. For hard negatives, we propose\nan efficient semantic-driven strategy for selecting anchor locations, which can\nsample informative negative examples at training phase for classification\nrefinement. It is worth noting that these methods can also be applied to\ngeneral object detection domain, and trainable in an end-to-end manner. We\nachieves consistently high performance on the Caltech-USA and CityPersons\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 03:47:32 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 01:28:31 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Wang", "Zhe", ""], ["Wang", "Jun", ""], ["Yang", "Yezhou", ""]]}, {"id": "2005.07356", "submitter": "Mohammed Belkhatir", "authors": "B. Tahayna, M. Belkhatir", "title": "Near-duplicate video detection featuring coupled temporal and perceptual\n  visual structures and logical inference based matching", "comments": null, "journal-ref": null, "doi": "10.1016/j.ipm.2011.03.003", "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper an architecture for near-duplicate video detection\nbased on: (i) index and query signature based structures integrating temporal\nand perceptual visual features and (ii) a matching framework computing the\nlogical inference between index and query documents. As far as indexing is\nconcerned, instead of concatenating low-level visual features in\nhigh-dimensional spaces which results in curse of dimensionality and redundancy\nissues, we adopt a perceptual symbolic representation based on color and\ntexture concepts. For matching, we propose to instantiate a retrieval model\nbased on logical inference through the coupling of an N-gram sliding window\nprocess and theoretically-sound lattice-based structures. The techniques we\ncover are robust and insensitive to general video editing and/or degradation,\nmaking it ideal for re-broadcasted video search. Experiments are carried out on\nlarge quantities of video data collected from the TRECVID 02, 03 and 04\ncollections and real-world video broadcasts recorded from two German TV\nstations. An empirical comparison over two state-of-the-art dynamic programming\ntechniques is encouraging and demonstrates the advantage and feasibility of our\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 04:45:52 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Tahayna", "B.", ""], ["Belkhatir", "M.", ""]]}, {"id": "2005.07377", "submitter": "Quande Liu", "authors": "Quande Liu, Lequan Yu, Luyang Luo, Qi Dou, Pheng Ann Heng", "title": "Semi-supervised Medical Image Classification with Relation-driven\n  Self-ensembling Model", "comments": "IEEE Transactions on Medical Imaging, 2020", "journal-ref": null, "doi": "10.1109/TMI.2020.2995518", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks usually requires a large amount of labeled data\nto obtain good performance. However, in medical image analysis, obtaining\nhigh-quality labels for the data is laborious and expensive, as accurately\nannotating medical images demands expertise knowledge of the clinicians. In\nthis paper, we present a novel relation-driven semi-supervised framework for\nmedical image classification. It is a consistency-based method which exploits\nthe unlabeled data by encouraging the prediction consistency of given input\nunder perturbations, and leverages a self-ensembling model to produce\nhigh-quality consistency targets for the unlabeled data. Considering that human\ndiagnosis often refers to previous analogous cases to make reliable decisions,\nwe introduce a novel sample relation consistency (SRC) paradigm to effectively\nexploit unlabeled data by modeling the relationship information among different\nsamples. Superior to existing consistency-based methods which simply enforce\nconsistency of individual predictions, our framework explicitly enforces the\nconsistency of semantic relation among different samples under perturbations,\nencouraging the model to explore extra semantic information from unlabeled\ndata. We have conducted extensive experiments to evaluate our method on two\npublic benchmark medical image classification datasets, i.e.,skin lesion\ndiagnosis with ISIC 2018 challenge and thorax disease classification with\nChestX-ray14. Our method outperforms many state-of-the-art semi-supervised\nlearning methods on both single-label and multi-label image classification\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 06:57:54 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Liu", "Quande", ""], ["Yu", "Lequan", ""], ["Luo", "Luyang", ""], ["Dou", "Qi", ""], ["Heng", "Pheng Ann", ""]]}, {"id": "2005.07424", "submitter": "Felix Nobis", "authors": "Felix Nobis, Fabian Brunhuber, Simon Janssen, Johannes Betz and Markus\n  Lienkamp", "title": "Exploring the Capabilities and Limits of 3D Monocular Object Detection\n  -- A Study on Simulation and Real World Data", "comments": "Accepted at The 23rd IEEE International Conference on Intelligent\n  Transportation Systems, September 20 - 23, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection based on monocular camera data is a key enabler for\nautonomous driving. The task however, is ill-posed due to lack of depth\ninformation in 2D images. Recent deep learning methods show promising results\nto recover depth information from single images by learning priors about the\nenvironment. Several competing strategies tackle this problem. In addition to\nthe network design, the major difference of these competing approaches lies in\nusing a supervised or self-supervised optimization loss function, which require\ndifferent data and ground truth information. In this paper, we evaluate the\nperformance of a 3D object detection pipeline which is parameterizable with\ndifferent depth estimation configurations. We implement a simple distance\ncalculation approach based on camera intrinsics and 2D bounding box size, a\nself-supervised, and a supervised learning approach for depth estimation.\n  Ground truth depth information cannot be recorded reliable in real world\nscenarios. This shifts our training focus to simulation data. In simulation,\nlabeling and ground truth generation can be automatized. We evaluate the\ndetection pipeline on simulator data and a real world sequence from an\nautonomous vehicle on a race track. The benefit of simulation training to real\nworld application is investigated. Advantages and drawbacks of the different\ndepth estimation strategies are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 09:05:17 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Nobis", "Felix", ""], ["Brunhuber", "Fabian", ""], ["Janssen", "Simon", ""], ["Betz", "Johannes", ""], ["Lienkamp", "Markus", ""]]}, {"id": "2005.07429", "submitter": "Felix Nobis", "authors": "Felix Nobis, Odysseas Papanikolaou, Johannes Betz and Markus Lienkamp", "title": "Persistent Map Saving for Visual Localization for Autonomous Vehicles:\n  An ORB-SLAM Extension", "comments": "Accepted at 2020 Fifteenth International Conference on Ecological\n  Vehicles and Renewable Energies (EVER)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electric vhicles and autonomous driving dominate current research efforts in\nthe automotive sector. The two topics go hand in hand in terms of enabling\nsafer and more environmentally friendly driving. One fundamental building block\nof an autonomous vehicle is the ability to build a map of the environment and\nlocalize itself on such a map. In this paper, we make use of a stereo camera\nsensor in order to perceive the environment and create the map. With live\nSimultaneous Localization and Mapping (SLAM), there is a risk of\nmislocalization, since no ground truth map is used as a reference and errors\naccumulate over time. Therefore, we first build up and save a map of visual\nfeatures of the environment at low driving speeds with our extension to the\nORB-SLAM\\,2 package. In a second run, we reload the map and then localize on\nthe previously built-up map. Loading and localizing on a previously built map\ncan improve the continuous localization accuracy for autonomous vehicles in\ncomparison to a full SLAM. This map saving feature is missing in the original\nORB-SLAM\\,2 implementation.\n  We evaluate the localization accuracy for scenes of the KITTI dataset against\nthe built up SLAM map. Furthermore, we test the localization on data recorded\nwith our own small scale electric model car. We show that the relative\ntranslation error of the localization stays under 1\\% for a vehicle travelling\nat an average longitudinal speed of 36 m/s in a feature-rich environment. The\nlocalization mode contributes to a better localization accuracy and lower\ncomputational load compared to a full SLAM. The source code of our contribution\nto the ORB-SLAM2 will be made public at:\nhttps://github.com/TUMFTM/orbslam-map-saving-extension.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 09:20:31 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Nobis", "Felix", ""], ["Papanikolaou", "Odysseas", ""], ["Betz", "Johannes", ""], ["Lienkamp", "Markus", ""]]}, {"id": "2005.07431", "submitter": "Felix Nobis", "authors": "Felix Nobis, Maximilian Geisslinger, Markus Weber, Johannes Betz and\n  Markus Lienkamp", "title": "A Deep Learning-based Radar and Camera Sensor Fusion Architecture for\n  Object Detection", "comments": "Accepted at 2019 Sensor Data Fusion: Trends, Solutions, Applications\n  (SDF)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection in camera images, using deep learning has been proven\nsuccessfully in recent years. Rising detection rates and computationally\nefficient network structures are pushing this technique towards application in\nproduction vehicles. Nevertheless, the sensor quality of the camera is limited\nin severe weather conditions and through increased sensor noise in sparsely lit\nareas and at night. Our approach enhances current 2D object detection networks\nby fusing camera data and projected sparse radar data in the network layers.\nThe proposed CameraRadarFusionNet (CRF-Net) automatically learns at which level\nthe fusion of the sensor data is most beneficial for the detection result.\nAdditionally, we introduce BlackIn, a training strategy inspired by Dropout,\nwhich focuses the learning on a specific sensor type. We show that the fusion\nnetwork is able to outperform a state-of-the-art image-only network for two\ndifferent datasets. The code for this research will be made available to the\npublic at: https://github.com/TUMFTM/CameraRadarFusionNet.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 09:28:01 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Nobis", "Felix", ""], ["Geisslinger", "Maximilian", ""], ["Weber", "Markus", ""], ["Betz", "Johannes", ""], ["Lienkamp", "Markus", ""]]}, {"id": "2005.07457", "submitter": "Christiane Sommer", "authors": "Christiane Sommer and Yumin Sun and Erik Bylow and Daniel Cremers", "title": "PrimiTect: Fast Continuous Hough Voting for Primitive Detection", "comments": "Accepted to IEEE International Conference on Robotics and Automation\n  (ICRA), 2020 | Code: https://github.com/c-sommer/primitect", "journal-ref": null, "doi": "10.1109/ICRA40945.2020.9196988", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of data abstraction in the context of 3D point\nsets. Our method classifies points into different geometric primitives, such as\nplanes and cones, leading to a compact representation of the data. Being based\non a semi-global Hough voting scheme, the method does not need initialization\nand is robust, accurate, and efficient. We use a local, low-dimensional\nparameterization of primitives to determine type, shape and pose of the object\nthat a point belongs to. This makes our algorithm suitable to run on devices\nwith low computational power, as often required in robotics applications. The\nevaluation shows that our method outperforms state-of-the-art methods both in\nterms of accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 10:16:07 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Sommer", "Christiane", ""], ["Sun", "Yumin", ""], ["Bylow", "Erik", ""], ["Cremers", "Daniel", ""]]}, {"id": "2005.07462", "submitter": "Kelei He", "authors": "Kelei He, Chunfeng Lian, Ehsan Adeli, Jing Huo, Yang Gao, Bing Zhang,\n  Junfeng Zhang, Dinggang Shen", "title": "MetricUNet: Synergistic Image- and Voxel-Level Learning for Precise CT\n  Prostate Segmentation via Online Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fully convolutional networks (FCNs), including UNet and VNet, are widely-used\nnetwork architectures for semantic segmentation in recent studies. However,\nconventional FCN is typically trained by the cross-entropy or Dice loss, which\nonly calculates the error between predictions and ground-truth labels for\npixels individually. This often results in non-smooth neighborhoods in the\npredicted segmentation. To address this problem, we propose a two-stage\nframework, with the first stage to quickly localize the prostate region and the\nsecond stage to precisely segment the prostate by a multi-task UNet\narchitecture. We introduce a novel online metric learning module through\nvoxel-wise sampling in the multi-task network. Therefore, the proposed network\nhas a dual-branch architecture that tackles two tasks: 1) a segmentation\nsub-network aiming to generate the prostate segmentation, and 2) a voxel-metric\nlearning sub-network aiming to improve the quality of the learned feature space\nsupervised by a metric loss. Specifically, the voxel-metric learning\nsub-network samples tuples (including triplets and pairs) in voxel-level\nthrough the intermediate feature maps. Unlike conventional deep metric learning\nmethods that generate triplets or pairs in image-level before the training\nphase, our proposed voxel-wise tuples are sampled in an online manner and\noperated in an end-to-end fashion via multi-task learning. To evaluate the\nproposed method, we implement extensive experiments on a real CT image dataset\nconsisting of 339 patients. The ablation studies show that our method can\neffectively learn more representative voxel-level features compared with the\nconventional learning methods with cross-entropy or Dice loss. And the\ncomparisons show that the proposed method outperforms the state-of-the-art\nmethods by a reasonable margin.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 10:37:02 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 13:05:03 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 08:19:15 GMT"}, {"version": "v4", "created": "Sat, 23 Jan 2021 17:18:35 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["He", "Kelei", ""], ["Lian", "Chunfeng", ""], ["Adeli", "Ehsan", ""], ["Huo", "Jing", ""], ["Gao", "Yang", ""], ["Zhang", "Bing", ""], ["Zhang", "Junfeng", ""], ["Shen", "Dinggang", ""]]}, {"id": "2005.07476", "submitter": "Jun Liu", "authors": "Jun Liu, Xue-Cheng Tai, and Shousheng Luo", "title": "Convex Shape Prior for Deep Neural Convolution Network based Eye Fundus\n  Images Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex Shapes (CS) are common priors for optic disc and cup segmentation in\neye fundus images. It is important to design proper techniques to represent\nconvex shapes. So far, it is still a problem to guarantee that the output\nobjects from a Deep Neural Convolution Networks (DCNN) are convex shapes. In\nthis work, we propose a technique which can be easily integrated into the\ncommonly used DCNNs for image segmentation and guarantee that outputs are\nconvex shapes. This method is flexible and it can handle multiple objects and\nallow some of the objects to be convex. Our method is based on the dual\nrepresentation of the sigmoid activation function in DCNNs. In the dual space,\nthe convex shape prior can be guaranteed by a simple quadratic constraint on a\nbinary representation of the shapes. Moreover, our method can also integrate\nspatial regularization and some other shape prior using a soft thresholding\ndynamics (STD) method. The regularization can make the boundary curves of the\nsegmentation objects to be simultaneously smooth and convex. We design a very\nstable active set projection algorithm to numerically solve our model. This\nalgorithm can form a new plug-and-play DCNN layer called CS-STD whose outputs\nmust be a nearly binary segmentation of convex objects. In the CS-STD block,\nthe convexity information can be propagated to guide the DCNN in both forward\nand backward propagation during training and prediction process. As an\napplication example, we apply the convexity prior layer to the retinal fundus\nimages segmentation by taking the popular DeepLabV3+ as a backbone network.\nExperimental results on several public datasets show that our method is\nefficient and outperforms the classical DCNN segmentation methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 11:36:04 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Liu", "Jun", ""], ["Tai", "Xue-Cheng", ""], ["Luo", "Shousheng", ""]]}, {"id": "2005.07493", "submitter": "Shubham Agarwal", "authors": "Shubham Agarwal, Trung Bui, Joon-Young Lee, Ioannis Konstas, Verena\n  Rieser", "title": "History for Visual Dialog: Do we really need it?", "comments": "ACL'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Dialog involves \"understanding\" the dialog history (what has been\ndiscussed previously) and the current question (what is asked), in addition to\ngrounding information in the image, to generate the correct response. In this\npaper, we show that co-attention models which explicitly encode dialog history\noutperform models that don't, achieving state-of-the-art performance (72 % NDCG\non val set). However, we also expose shortcomings of the crowd-sourcing dataset\ncollection procedure by showing that history is indeed only required for a\nsmall amount of the data and that the current evaluation metric encourages\ngeneric replies. To that end, we propose a challenging subset (VisDialConv) of\nthe VisDial val set and provide a benchmark of 63% NDCG.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 14:58:09 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Agarwal", "Shubham", ""], ["Bui", "Trung", ""], ["Lee", "Joon-Young", ""], ["Konstas", "Ioannis", ""], ["Rieser", "Verena", ""]]}, {"id": "2005.07502", "submitter": "Shirsendu Sukanta Halder", "authors": "Akella Ravi Tej, Shirsendu Sukanta Halder, Arunav Pratap Shandeelya,\n  Vinod Pankajakshan", "title": "Enhancing Perceptual Loss with Adversarial Feature Matching for\n  Super-Resolution", "comments": "Accepted for publication in the International Joint Conference on\n  Neural Networks (IJCNN) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SISR) is an ill-posed problem with an\nindeterminate number of valid solutions. Solving this problem with neural\nnetworks would require access to extensive experience, either presented as a\nlarge training set over natural images or a condensed representation from\nanother pre-trained network. Perceptual loss functions, which belong to the\nlatter category, have achieved breakthrough success in SISR and several other\ncomputer vision tasks. While perceptual loss plays a central role in the\ngeneration of photo-realistic images, it also produces undesired pattern\nartifacts in the super-resolved outputs. In this paper, we show that the root\ncause of these pattern artifacts can be traced back to a mismatch between the\npre-training objective of perceptual loss and the super-resolution objective.\nTo address this issue, we propose to augment the existing perceptual loss\nformulation with a novel content loss function that uses the latent features of\na discriminator network to filter the unwanted artifacts across several levels\nof adversarial similarity. Further, our modification has a stabilizing effect\non non-convex optimization in adversarial training. The proposed approach\noffers notable gains in perceptual quality based on an extensive human\nevaluation study and a competent reconstruction fidelity when tested on\nobjective evaluation metrics.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 12:36:54 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Tej", "Akella Ravi", ""], ["Halder", "Shirsendu Sukanta", ""], ["Shandeelya", "Arunav Pratap", ""], ["Pankajakshan", "Vinod", ""]]}, {"id": "2005.07518", "submitter": "Morten Goodwin Dr.", "authors": "Kristian Muri Knausg{\\aa}rd, Arne Wiklund, Tonje Knutsen S{\\o}rdalen,\n  Kim Halvorsen, Alf Ring Kleiven, Lei Jiao, Morten Goodwin", "title": "Temperate Fish Detection and Classification: a Deep Learning based\n  Approach", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.02768", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of applications in marine ecology extensively uses underwater\ncameras. Still, to efficiently process the vast amount of data generated, we\nneed to develop tools that can automatically detect and recognize species\ncaptured on film. Classifying fish species from videos and images in natural\nenvironments can be challenging because of noise and variation in illumination\nand the surrounding habitat. In this paper, we propose a two-step deep learning\napproach for the detection and classification of temperate fishes without\npre-filtering. The first step is to detect each single fish in an image,\nindependent of species and sex. For this purpose, we employ the You Only Look\nOnce (YOLO) object detection technique. In the second step, we adopt a\nConvolutional Neural Network (CNN) with the Squeeze-and-Excitation (SE)\narchitecture for classifying each fish in the image without pre-filtering. We\napply transfer learning to overcome the limited training samples of temperate\nfishes and to improve the accuracy of the classification. This is done by\ntraining the object detection model with ImageNet and the fish classifier via a\npublic dataset (Fish4Knowledge), whereupon both the object detection and\nclassifier are updated with temperate fishes of interest. The weights obtained\nfrom pre-training are applied to post-training as a priori. Our solution\nachieves the state-of-the-art accuracy of 99.27\\% on the pre-training. The\npercentage values for accuracy on the post-training are good; 83.68\\% and\n87.74\\% with and without image augmentation, respectively, indicating that the\nsolution is viable with a more extensive dataset.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 12:40:57 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Knausg\u00e5rd", "Kristian Muri", ""], ["Wiklund", "Arne", ""], ["S\u00f8rdalen", "Tonje Knutsen", ""], ["Halvorsen", "Kim", ""], ["Kleiven", "Alf Ring", ""], ["Jiao", "Lei", ""], ["Goodwin", "Morten", ""]]}, {"id": "2005.07545", "submitter": "Maureen Van Eijnatten", "authors": "Maureen van Eijnatten, Leonardo Rundo, K. Joost Batenburg, Felix\n  Lucka, Emma Beddowes, Carlos Caldas, Ferdia A. Gallagher, Evis Sala,\n  Carola-Bibiane Sch\\\"onlieb, Ramona Woitek", "title": "3D deformable registration of longitudinal abdominopelvic CT images\n  using unsupervised deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates the use of the unsupervised deep learning framework\nVoxelMorph for deformable registration of longitudinal abdominopelvic CT images\nacquired in patients with bone metastases from breast cancer. The CT images\nwere refined prior to registration by automatically removing the CT table and\nall other extra-corporeal components. To improve the learning capabilities of\nVoxelMorph when only a limited amount of training data is available, a novel\nincremental training strategy is proposed based on simulated deformations of\nconsecutive CT images. In a 4-fold cross-validation scheme, the incremental\ntraining strategy achieved significantly better registration performance\ncompared to training on a single volume. Although our deformable image\nregistration method did not outperform iterative registration using NiftyReg\n(considered as a benchmark) in terms of registration quality, the registrations\nwere approximately 300 times faster. This study showed the feasibility of deep\nlearning based deformable registration of longitudinal abdominopelvic CT images\nvia a novel incremental training strategy based on simulated deformations.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 13:49:13 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["van Eijnatten", "Maureen", ""], ["Rundo", "Leonardo", ""], ["Batenburg", "K. Joost", ""], ["Lucka", "Felix", ""], ["Beddowes", "Emma", ""], ["Caldas", "Carlos", ""], ["Gallagher", "Ferdia A.", ""], ["Sala", "Evis", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Woitek", "Ramona", ""]]}, {"id": "2005.07564", "submitter": "Xin Xia", "authors": "Xin Xia and Wenrui Ding", "title": "HNAS: Hierarchical Neural Architecture Search on Mobile Devices", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has attracted growing interest. To reduce\nthe search cost, recent work has explored weight sharing across models and made\nmajor progress in One-Shot NAS. However, it has been observed that a model with\nhigher one-shot model accuracy does not necessarily perform better when\nstand-alone trained. To address this issue, in this paper, we propose a new\nmethod, named Hierarchical Neural Architecture Search (HNAS). Unlike previous\napproaches where the same operation search space is shared by all the layers in\nthe supernet, we formulate a hierarchical search strategy based on operation\npruning and build a layer-wise operation search space. In this way, HNAS can\nautomatically select the operations for each layer. During the search, we also\ntake the hardware platform constraints into consideration for efficient neural\nnetwork model deployment. Extensive experiments on ImageNet show that under\nmobile latency constraint, our models consistently outperform state-of-the-art\nmodels both designed manually and generated automatically by NAS methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 14:21:07 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Xia", "Xin", ""], ["Ding", "Wenrui", ""]]}, {"id": "2005.07648", "submitter": "Pierre Sermanet", "authors": "Corey Lynch and Pierre Sermanet", "title": "Language Conditioned Imitation Learning over Unstructured Data", "comments": "Published at RSS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language is perhaps the most flexible and intuitive way for humans to\ncommunicate tasks to a robot. Prior work in imitation learning typically\nrequires each task be specified with a task id or goal image -- something that\nis often impractical in open-world environments. On the other hand, previous\napproaches in instruction following allow agent behavior to be guided by\nlanguage, but typically assume structure in the observations, actuators, or\nlanguage that limit their applicability to complex settings like robotics. In\nthis work, we present a method for incorporating free-form natural language\nconditioning into imitation learning. Our approach learns perception from\npixels, natural language understanding, and multitask continuous control\nend-to-end as a single neural network. Unlike prior work in imitation learning,\nour method is able to incorporate unlabeled and unstructured demonstration data\n(i.e. no task or language labels). We show this dramatically improves language\nconditioned performance, while reducing the cost of language annotation to less\nthan 1% of total data. At test time, a single language conditioned visuomotor\npolicy trained with our method can perform a wide variety of robotic\nmanipulation skills in a 3D environment, specified only with natural language\ndescriptions of each task (e.g. \"open the drawer...now pick up the block...now\npress the green button...\"). To scale up the number of instructions an agent\ncan follow, we propose combining text conditioned policies with large\npretrained neural language models. We find this allows a policy to be robust to\nmany out-of-distribution synonym instructions, without requiring new\ndemonstrations. See videos of a human typing live text commands to our agent at\nlanguage-play.github.io\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 17:08:50 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 23:43:24 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Lynch", "Corey", ""], ["Sermanet", "Pierre", ""]]}, {"id": "2005.07649", "submitter": "Hugo Mitre-Hernandez", "authors": "Rodolfo Ferro-P\\'erez, Hugo Mitre-Hernandez", "title": "ResMoNet: A Residual Mobile-based Network for Facial Emotion Recognition\n  in Resource-Limited Systems", "comments": "11 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Deep Neural Networks (DNNs) models have contributed a high accuracy for\nthe classification of human emotional states from facial expression recognition\ndata sets, where efficiency is an important factor for resource-limited systems\nas mobile devices and embedded systems. There are efficient Convolutional\nNeural Networks (CNN) models as MobileNet, PeleeNet, Extended Deep Neural\nNetwork (EDNN) and Inception-Based Deep Neural Network (IDNN) in terms of model\narchitecture results: parameters, Floating-point OPerations (FLOPs) and\naccuracy. Although these results are satisfactory, it is necessary to evaluate\nother computational resources related to the trained model such as main memory\nutilization and response time to complete the emotion recognition. In this\npaper, we compare our proposed model inspired in depthwise separable\nconvolutions and residual blocks with MobileNet, PeleeNet, EDNN and IDNN. The\ncomparative results of the CNN architectures and the trained models --with\nRadboud Faces Database (RaFD)-- installed in a resource-limited device are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 17:09:10 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Ferro-P\u00e9rez", "Rodolfo", ""], ["Mitre-Hernandez", "Hugo", ""]]}, {"id": "2005.07662", "submitter": "Stefan Hoehme", "authors": "Adrian Friebel, Tim Johann, Dirk Drasdo, Stefan Hoehme", "title": "Guided interactive image segmentation using machine learning and color\n  based data set clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach that combines machine learning based interactive\nimage segmentation using supervoxels with a clustering method for the automated\nidentification of similarly colored images in large data sets which enables a\nguided reuse of classifiers. Our approach solves the problem of significant\ncolor variability prevalent and often unavoidable in biological and medical\nimages which typically leads to deteriorated segmentation and quantification\naccuracy thereby greatly reducing the necessary training effort. This increase\nin efficiency facilitates the quantification of much larger numbers of images\nthereby enabling interactive image analysis for recent new technological\nadvances in high-throughput imaging. The presented methods are applicable for\nalmost any image type and represent a useful tool for image analysis tasks in\ngeneral.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 17:25:48 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 14:06:32 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 14:44:54 GMT"}, {"version": "v4", "created": "Fri, 18 Jun 2021 02:41:40 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Friebel", "Adrian", ""], ["Johann", "Tim", ""], ["Drasdo", "Dirk", ""], ["Hoehme", "Stefan", ""]]}, {"id": "2005.07669", "submitter": "Jeovane Hon\\'orio Alves", "authors": "Jeovane Honorio Alves, Lucas Ferrari de Oliveira", "title": "Optimizing Neural Architecture Search using Limited GPU Time in a\n  Dynamic Search Space: A Gene Expression Programming Approach", "comments": "Accepted for presentation at the IEEE Congress on Evolutionary\n  Computation (IEEE CEC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient identification of people and objects, segmentation of regions of\ninterest and extraction of relevant data in images, texts, audios and videos\nare evolving considerably in these past years, which deep learning methods,\ncombined with recent improvements in computational resources, contributed\ngreatly for this achievement. Although its outstanding potential, development\nof efficient architectures and modules requires expert knowledge and amount of\nresource time available. In this paper, we propose an evolutionary-based neural\narchitecture search approach for efficient discovery of convolutional models in\na dynamic search space, within only 24 GPU hours. With its efficient search\nenvironment and phenotype representation, Gene Expression Programming is\nadapted for network's cell generation. Despite having limited GPU resource time\nand broad search space, our proposal achieved similar state-of-the-art to\nmanually-designed convolutional networks and also NAS-generated ones, even\nbeating similar constrained evolutionary-based NAS works. The best cells in\ndifferent runs achieved stable results, with a mean error of 2.82% in CIFAR-10\ndataset (which the best model achieved an error of 2.67%) and 18.83% for\nCIFAR-100 (best model with 18.16%). For ImageNet in the mobile setting, our\nbest model achieved top-1 and top-5 errors of 29.51% and 10.37%, respectively.\nAlthough evolutionary-based NAS works were reported to require a considerable\namount of GPU time for architecture search, our approach obtained promising\nresults in little time, encouraging further experiments in evolutionary-based\nNAS, for search and network representation improvements.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 17:32:30 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Alves", "Jeovane Honorio", ""], ["de Oliveira", "Lucas Ferrari", ""]]}, {"id": "2005.07682", "submitter": "Luat Vuong T", "authors": "Baurzhan Muminov and Luat T. Vuong", "title": "Small-brain neural networks rapidly solve inverse problems with vortex\n  Fourier encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a vortex phase transform with a lenslet-array to accompany\nshallow, dense, ``small-brain'' neural networks for high-speed and low-light\nimaging. Our single-shot ptychographic approach exploits the coherent\ndiffraction, compact representation, and edge enhancement of Fourier-tranformed\nspiral-phase gradients. With vortex spatial encoding, a small brain is trained\nto deconvolve images at rates 5-20 times faster than those achieved with random\nencoding schemes, where greater advantages are gained in the presence of noise.\nOnce trained, the small brain reconstructs an object from intensity-only data,\nsolving an inverse mapping without performing iterations on each image and\nwithout deep-learning schemes. With this hybrid, optical-digital, vortex\nFourier encoded, small-brain scheme, we reconstruct MNIST Fashion objects\nilluminated with low-light flux (5 nJ/cm$^2$) at a rate of several thousand\nframes per second on a 15 W central processing unit, two orders of magnitude\nfaster than convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 17:53:32 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Muminov", "Baurzhan", ""], ["Vuong", "Luat T.", ""]]}, {"id": "2005.07727", "submitter": "David Bau iii", "authors": "David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou,\n  Jun-Yan Zhu, Antonio Torralba", "title": "Semantic Photo Manipulation with a Generative Image Prior", "comments": "SIGGRAPH 2019", "journal-ref": "ACM Transactions on Graphics (TOG) 38.4 (2019)", "doi": "10.1145/3306346.3323023", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of GANs in synthesizing images conditioned on\ninputs such as a user sketch, text, or semantic labels, manipulating the\nhigh-level attributes of an existing natural photograph with GANs is\nchallenging for two reasons. First, it is hard for GANs to precisely reproduce\nan input image. Second, after manipulation, the newly synthesized pixels often\ndo not fit the original image. In this paper, we address these issues by\nadapting the image prior learned by GANs to image statistics of an individual\nimage. Our method can accurately reconstruct the input image and synthesize new\ncontent, consistent with the appearance of the input image. We demonstrate our\ninteractive system on several semantic image editing tasks, including\nsynthesizing new objects consistent with background, removing unwanted objects,\nand changing the appearance of an object. Quantitative and qualitative\ncomparisons against several existing methods demonstrate the effectiveness of\nour method.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 18:22:05 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 19:53:55 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Bau", "David", ""], ["Strobelt", "Hendrik", ""], ["Peebles", "William", ""], ["Wulff", "Jonas", ""], ["Zhou", "Bolei", ""], ["Zhu", "Jun-Yan", ""], ["Torralba", "Antonio", ""]]}, {"id": "2005.07728", "submitter": "Yotam Nitzan", "authors": "Yotam Nitzan, Amit Bermano, Yangyan Li, Daniel Cohen-Or", "title": "Face Identity Disentanglement via Latent Space Mapping", "comments": "23 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning disentangled representations of data is a fundamental problem in\nartificial intelligence. Specifically, disentangled latent representations\nallow generative models to control and compose the disentangled factors in the\nsynthesis process. Current methods, however, require extensive supervision and\ntraining, or instead, noticeably compromise quality. In this paper, we present\na method that learns how to represent data in a disentangled way, with minimal\nsupervision, manifested solely using available pre-trained networks. Our key\ninsight is to decouple the processes of disentanglement and synthesis, by\nemploying a leading pre-trained unconditional image generator, such as\nStyleGAN. By learning to map into its latent space, we leverage both its\nstate-of-the-art quality, and its rich and expressive latent space, without the\nburden of training it. We demonstrate our approach on the complex and high\ndimensional domain of human heads. We evaluate our method qualitatively and\nquantitatively, and exhibit its success with de-identification operations and\nwith temporal identity coherency in image sequences. Through extensive\nexperimentation, we show that our method successfully disentangles identity\nfrom other facial attributes, surpassing existing methods, even though they\nrequire more training and supervision.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 18:24:49 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 16:24:06 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 12:24:42 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Nitzan", "Yotam", ""], ["Bermano", "Amit", ""], ["Li", "Yangyan", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2005.07771", "submitter": "Rajiv Ratn Shah", "authors": "Shagun Uppal, Anish Madan, Sarthak Bhagat, Yi Yu, Rajiv Ratn Shah", "title": "C3VQG: Category Consistent Cyclic Visual Question Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Generation (VQG) is the task of generating natural questions\nbased on an image. Popular methods in the past have explored image-to-sequence\narchitectures trained with maximum likelihood which have demonstrated\nmeaningful generated questions given an image and its associated ground-truth\nanswer. VQG becomes more challenging if the image contains rich contextual\ninformation describing its different semantic categories. In this paper, we try\nto exploit the different visual cues and concepts in an image to generate\nquestions using a variational autoencoder (VAE) without ground-truth answers.\nOur approach solves two major shortcomings of existing VQG systems: (i)\nminimize the level of supervision and (ii) replace generic questions with\ncategory relevant generations. Most importantly, by eliminating expensive\nanswer annotations, the required supervision is weakened. Using different\ncategories enables us to exploit different concepts as the inference requires\nonly the image and the category. Mutual information is maximized between the\nimage, question, and answer category in the latent space of our VAE. A novel\ncategory consistent cyclic loss is proposed to enable the model to generate\nconsistent predictions with respect to the answer category, reducing\nredundancies and irregularities. Additionally, we also impose supplementary\nconstraints on the latent space of our generative model to provide structure\nbased on categories and enhance generalization by encapsulating decorrelated\nfeatures within each dimension. Through extensive experiments, the proposed\nmodel, C3VQG outperforms state-of-the-art VQG methods with weak supervision.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 20:25:03 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 14:58:34 GMT"}, {"version": "v3", "created": "Sat, 30 May 2020 17:00:17 GMT"}, {"version": "v4", "created": "Sat, 13 Jun 2020 12:56:48 GMT"}, {"version": "v5", "created": "Sat, 9 Jan 2021 14:26:57 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Uppal", "Shagun", ""], ["Madan", "Anish", ""], ["Bhagat", "Sarthak", ""], ["Yu", "Yi", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2005.07779", "submitter": "Esteban Reyes", "authors": "Esteban Reyes, Pablo A. Est\\'evez", "title": "Transformation Based Deep Anomaly Detection in Astronomical Images", "comments": "8 pages, 6 figures, 4 tables. Accepted for publication in proceedings\n  of the IEEE World Congress on Computational Intelligence (IEEE WCCI),\n  Glasgow, UK, 19-24 July, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose several enhancements to a geometric transformation\nbased model for anomaly detection in images (GeoTranform). The model assumes\nthat the anomaly class is unknown and that only inlier samples are available\nfor training. We introduce new filter based transformations useful for\ndetecting anomalies in astronomical images, that highlight artifact properties\nto make them more easily distinguishable from real objects. In addition, we\npropose a transformation selection strategy that allows us to find\nindistinguishable pairs of transformations. This results in an improvement of\nthe area under the Receiver Operating Characteristic curve (AUROC) and accuracy\nperformance, as well as in a dimensionality reduction. The models were tested\non astronomical images from the High Cadence Transient Survey (HiTS) and Zwicky\nTransient Facility (ZTF) datasets. The best models obtained an average AUROC of\n99.20% for HiTS and 91.39% for ZTF. The improvement over the original\nGeoTransform algorithm and baseline methods such as One-Class Support Vector\nMachine, and deep learning based methods is significant both statistically and\nin practice.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 21:02:12 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Reyes", "Esteban", ""], ["Est\u00e9vez", "Pablo A.", ""]]}, {"id": "2005.07784", "submitter": "Ze Wang", "authors": "Danfeng Xie, Yiran Li, Hanlu Yang, Li Bai, Lei Zhang, Ze Wang", "title": "A Learning-from-noise Dilated Wide Activation Network for denoising\n  Arterial Spin Labeling (ASL) Perfusion Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arterial spin labeling (ASL) perfusion MRI provides a non-invasive way to\nquantify cerebral blood flow (CBF) but it still suffers from a low\nsignal-to-noise-ratio (SNR). Using deep machine learning (DL), several groups\nhave shown encouraging denoising results. Interestingly, the improvement was\nobtained when the deep neural network was trained using noise-contaminated\nsurrogate reference because of the lack of golden standard high quality ASL CBF\nimages. More strikingly, the output of these DL ASL networks (ASLDN) showed\neven higher SNR than the surrogate reference. This phenomenon indicates a\nlearning-from-noise capability of deep networks for ASL CBF image denoising,\nwhich can be further enhanced by network optimization. In this study, we\nproposed a new ASLDN to test whether similar or even better ASL CBF image\nquality can be achieved in the case of highly noisy training reference.\nDifferent experiments were performed to validate the learning-from-noise\nhypothesis. The results showed that the learning-from-noise strategy produced\nbetter output quality than ASLDN trained with relatively high SNR reference.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 21:05:56 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Xie", "Danfeng", ""], ["Li", "Yiran", ""], ["Yang", "Hanlu", ""], ["Bai", "Li", ""], ["Zhang", "Lei", ""], ["Wang", "Ze", ""]]}, {"id": "2005.07787", "submitter": "Mohammad Ebrahimpour", "authors": "Mohammad K. Ebrahimpour, J. Ben Falandays, Samuel Spevack, Ming-Hsuan\n  Yang, and David C. Noelle", "title": "WW-Nets: Dual Neural Networks for Object Detection", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new deep convolutional neural network framework that uses object\nlocation knowledge implicit in network connection weights to guide selective\nattention in object detection tasks. Our approach is called What-Where Nets\n(WW-Nets), and it is inspired by the structure of human visual pathways. In the\nbrain, vision incorporates two separate streams, one in the temporal lobe and\nthe other in the parietal lobe, called the ventral stream and the dorsal\nstream, respectively. The ventral pathway from primary visual cortex is\ndominated by \"what\" information, while the dorsal pathway is dominated by\n\"where\" information. Inspired by this structure, we have proposed an object\ndetection framework involving the integration of a \"What Network\" and a \"Where\nNetwork\". The aim of the What Network is to provide selective attention to the\nrelevant parts of the input image. The Where Network uses this information to\nlocate and classify objects of interest. In this paper, we compare this\napproach to state-of-the-art algorithms on the PASCAL VOC 2007 and 2012 and\nCOCO object detection challenge datasets. Also, we compare out approach to\nhuman \"ground-truth\" attention. We report the results of an eye-tracking\nexperiment on human subjects using images from PASCAL VOC 2007, and we\ndemonstrate interesting relationships between human overt attention and\ninformation processing in our WW-Nets. Finally, we provide evidence that our\nproposed method performs favorably in comparison to other object detection\napproaches, often by a large margin. The code and the eye-tracking ground-truth\ndataset can be found at: https://github.com/mkebrahimpour.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 21:16:22 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Ebrahimpour", "Mohammad K.", ""], ["Falandays", "J. Ben", ""], ["Spevack", "Samuel", ""], ["Yang", "Ming-Hsuan", ""], ["Noelle", "David C.", ""]]}, {"id": "2005.07796", "submitter": "Sohini Roychowdhury", "authors": "Francesco Piccoli, Rajarathnam Balakrishnan, Maria Jesus Perez,\n  Moraldeepsingh Sachdeo, Carlos Nunez, Matthew Tang, Kajsa Andreasson, Kalle\n  Bjurek, Ria Dass Raj, Ebba Davidsson, Colin Eriksson, Victor Hagman, Jonas\n  Sjoberg, Ying Li, L. Srikar Muppirisetty, Sohini Roychowdhury", "title": "FuSSI-Net: Fusion of Spatio-temporal Skeletons for Intention Prediction\n  Network", "comments": "5 pages, 6 figures, 5 tables, IEEE Asilomar SSC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian intention recognition is very important to develop robust and safe\nautonomous driving (AD) and advanced driver assistance systems (ADAS)\nfunctionalities for urban driving. In this work, we develop an end-to-end\npedestrian intention framework that performs well on day- and night- time\nscenarios. Our framework relies on objection detection bounding boxes combined\nwith skeletal features of human pose. We study early, late, and combined (early\nand late) fusion mechanisms to exploit the skeletal features and reduce false\npositives as well to improve the intention prediction performance. The early\nfusion mechanism results in AP of 0.89 and precision/recall of 0.79/0.89 for\npedestrian intention classification. Furthermore, we propose three new metrics\nto properly evaluate the pedestrian intention systems. Under these new\nevaluation metrics for the intention prediction, the proposed end-to-end\nnetwork offers accurate pedestrian intention up to half a second ahead of the\nactual risky maneuver.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 21:52:42 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Piccoli", "Francesco", ""], ["Balakrishnan", "Rajarathnam", ""], ["Perez", "Maria Jesus", ""], ["Sachdeo", "Moraldeepsingh", ""], ["Nunez", "Carlos", ""], ["Tang", "Matthew", ""], ["Andreasson", "Kajsa", ""], ["Bjurek", "Kalle", ""], ["Raj", "Ria Dass", ""], ["Davidsson", "Ebba", ""], ["Eriksson", "Colin", ""], ["Hagman", "Victor", ""], ["Sjoberg", "Jonas", ""], ["Li", "Ying", ""], ["Muppirisetty", "L. Srikar", ""], ["Roychowdhury", "Sohini", ""]]}, {"id": "2005.07839", "submitter": "Le Thanh Nguyen-Meidine", "authors": "Le Thanh Nguyen-Meidine, Eric Granger, Madhu Kiran, Jose Dolz,\n  Louis-Antoine Blais-Morin", "title": "Joint Progressive Knowledge Distillation and Unsupervised Domain\n  Adaptation", "comments": "Accepted to WCCI/IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, the divergence in distributions of design and operational data,\nand large computational complexity are limiting factors in the adoption of CNNs\nin real-world applications. For instance, person re-identification systems\ntypically rely on a distributed set of cameras, where each camera has different\ncapture conditions. This can translate to a considerable shift between source\n(e.g. lab setting) and target (e.g. operational camera) domains. Given the cost\nof annotating image data captured for fine-tuning in each target domain,\nunsupervised domain adaptation (UDA) has become a popular approach to adapt\nCNNs. Moreover, state-of-the-art deep learning models that provide a high level\nof accuracy often rely on architectures that are too complex for real-time\napplications. Although several compression and UDA approaches have recently\nbeen proposed to overcome these limitations, they do not allow optimizing a CNN\nto simultaneously address both. In this paper, we propose an unexplored\ndirection -- the joint optimization of CNNs to provide a compressed model that\nis adapted to perform well for a given target domain. In particular, the\nproposed approach performs unsupervised knowledge distillation (KD) from a\ncomplex teacher model to a compact student model, by leveraging both source and\ntarget data. It also improves upon existing UDA techniques by progressively\nteaching the student about domain-invariant features, instead of directly\nadapting a compact model on target domain data. Our method is compared against\nstate-of-the-art compression and UDA techniques, using two popular\nclassification datasets for UDA -- Office31 and ImageClef-DA. In both datasets,\nresults indicate that our method can achieve the highest level of accuracy\nwhile requiring a comparable or lower time complexity.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 01:07:03 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Nguyen-Meidine", "Le Thanh", ""], ["Granger", "Eric", ""], ["Kiran", "Madhu", ""], ["Dolz", "Jose", ""], ["Blais-Morin", "Louis-Antoine", ""]]}, {"id": "2005.07858", "submitter": "Seunghan Yang", "authors": "Seunghan Yang, Youngeun Kim, Dongki Jung, Changick Kim", "title": "Partial Domain Adaptation Using Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial domain adaptation (PDA), in which we assume the target label space is\nincluded in the source label space, is a general version of standard domain\nadaptation. Since the target label space is unknown, the main challenge of PDA\nis to reduce the learning impact of irrelevant source samples, named outliers,\nwhich do not belong to the target label space. Although existing partial domain\nadaptation methods effectively down-weigh outliers' importance, they do not\nconsider data structure of each domain and do not directly align the feature\ndistributions of the same class in the source and target domains, which may\nlead to misalignment of category-level distributions. To overcome these\nproblems, we propose a graph partial domain adaptation (GPDA) network, which\nexploits Graph Convolutional Networks for jointly considering data structure\nand the feature distribution of each class. Specifically, we propose a label\nrelational graph to align the distributions of the same category in two domains\nand introduce moving average centroid separation for learning networks from the\nlabel relational graph. We demonstrate that considering data structure and the\ndistribution of each category is effective for PDA and our GPDA network\nachieves state-of-the-art performance on the Digit and Office-31 datasets.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 03:37:38 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Yang", "Seunghan", ""], ["Kim", "Youngeun", ""], ["Jung", "Dongki", ""], ["Kim", "Changick", ""]]}, {"id": "2005.07862", "submitter": "Shijie Yu", "authors": "Shijie Yu and Shihua Li and Dapeng Chen and Rui Zhao and Junjie Yan\n  and Yu Qiao", "title": "COCAS: A Large-Scale Clothes Changing Person Dataset for\n  Re-identification", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed great progress in person re-identification\n(re-id). Several academic benchmarks such as Market1501, CUHK03 and DukeMTMC\nplay important roles to promote the re-id research. To our best knowledge, all\nthe existing benchmarks assume the same person will have the same clothes.\nWhile in real-world scenarios, it is very often for a person to change clothes.\nTo address the clothes changing person re-id problem, we construct a novel\nlarge-scale re-id benchmark named ClOthes ChAnging Person Set (COCAS), which\nprovides multiple images of the same identity with different clothes. COCAS\ntotally contains 62,382 body images from 5,266 persons. Based on COCAS, we\nintroduce a new person re-id setting for clothes changing problem, where the\nquery includes both a clothes template and a person image taking another\nclothes. Moreover, we propose a two-branch network named Biometric-Clothes\nNetwork (BC-Net) which can effectively integrate biometric and clothes feature\nfor re-id under our setting. Experiments show that it is feasible for clothes\nchanging re-id with clothes templates.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 03:50:08 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Yu", "Shijie", ""], ["Li", "Shihua", ""], ["Chen", "Dapeng", ""], ["Zhao", "Rui", ""], ["Yan", "Junjie", ""], ["Qiao", "Yu", ""]]}, {"id": "2005.07865", "submitter": "Yizhi Wang", "authors": "Yizhi Wang, Yue Gao, Zhouhui Lian", "title": "Attribute2Font: Creating Fonts You Want From Attributes", "comments": "SIGGRAPH 2020 techniqual paper; Wang and Gao contribute equally;\n  Code: https://hologerry.github.io/Attr2Font/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Font design is now still considered as an exclusive privilege of professional\ndesigners, whose creativity is not possessed by existing software systems.\nNevertheless, we also notice that most commercial font products are in fact\nmanually designed by following specific requirements on some attributes of\nglyphs, such as italic, serif, cursive, width, angularity, etc. Inspired by\nthis fact, we propose a novel model, Attribute2Font, to automatically create\nfonts by synthesizing visually-pleasing glyph images according to\nuser-specified attributes and their corresponding values. To the best of our\nknowledge, our model is the first one in the literature which is capable of\ngenerating glyph images in new font styles, instead of retrieving existing\nfonts, according to given values of specified font attributes. Specifically,\nAttribute2Font is trained to perform font style transfer between any two fonts\nconditioned on their attribute values. After training, our model can generate\nglyph images in accordance with an arbitrary set of font attribute values.\nFurthermore, a novel unit named Attribute Attention Module is designed to make\nthose generated glyph images better embody the prominent font attributes.\nConsidering that the annotations of font attribute values are extremely\nexpensive to obtain, a semi-supervised learning scheme is also introduced to\nexploit a large number of unlabeled fonts. Experimental results demonstrate\nthat our model achieves impressive performance on many tasks, such as creating\nglyph images in new font styles, editing existing fonts, interpolation among\ndifferent fonts, etc.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 04:06:53 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Wang", "Yizhi", ""], ["Gao", "Yue", ""], ["Lian", "Zhouhui", ""]]}, {"id": "2005.07896", "submitter": "Xin Li", "authors": "Xin Li, Simeng Sun, Zhizheng Zhang and Zhibo Chen", "title": "Multi-scale Grouped Dense Network for VVC Intra Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Versatile Video Coding (H.266/VVC) standard achieves better image quality\nwhen keeping the same bits than any other conventional image codec, such as\nBPG, JPEG, and etc. However, it is still attractive and challenging to improve\nthe image quality with high compression ratio on the basis of traditional\ncoding techniques. In this paper, we design the multi-scale grouped dense\nnetwork (MSGDN) to further reduce the compression artifacts by combining the\nmulti-scale and grouped dense block, which are integrated as the post-process\nnetwork of VVC intra coding. Besides, to improve the subjective quality of\ncompressed image, we also present a generative adversarial network (MSGDN-GAN)\nby utilizing our MSGDN as generator. Across the extensive experiments on\nvalidation set, our MSGDN trained by MSE losses yields the PSNR of 32.622 on\naverage with teams IMC at the bit-rate of 0.15 in Lowrate track. Moreover, our\nMSGDN-GAN could achieve the better subjective performance.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 08:08:44 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Li", "Xin", ""], ["Sun", "Simeng", ""], ["Zhang", "Zhizheng", ""], ["Chen", "Zhibo", ""]]}, {"id": "2005.07902", "submitter": "Zhiyuan Zha", "authors": "Zhiyuan Zha, Xin Yuan, Joey Tianyi Zhou, Jiantao Zhou, Bihan Wen and\n  Ce Zhu", "title": "The Power of Triply Complementary Priors for Image Compressive Sensing", "comments": null, "journal-ref": "2020 International Conference on Image Processing", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works that utilized deep models have achieved superior results in\nvarious image restoration applications. Such approach is typically supervised\nwhich requires a corpus of training images with distribution similar to the\nimages to be recovered. On the other hand, the shallow methods which are\nusually unsupervised remain promising performance in many inverse problems,\n\\eg, image compressive sensing (CS), as they can effectively leverage non-local\nself-similarity priors of natural images. However, most of such methods are\npatch-based leading to the restored images with various ringing artifacts due\nto naive patch aggregation. Using either approach alone usually limits\nperformance and generalizability in image restoration tasks. In this paper, we\npropose a joint low-rank and deep (LRD) image model, which contains a pair of\ntriply complementary priors, namely \\textit{external} and \\textit{internal},\n\\textit{deep} and \\textit{shallow}, and \\textit{local} and \\textit{non-local}\npriors. We then propose a novel hybrid plug-and-play (H-PnP) framework based on\nthe LRD model for image CS. To make the optimization tractable, a simple yet\neffective algorithm is proposed to solve the proposed H-PnP based image CS\nproblem. Extensive experimental results demonstrate that the proposed H-PnP\nalgorithm significantly outperforms the state-of-the-art techniques for image\nCS recovery such as SCSNet and WNNM.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 08:17:44 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zha", "Zhiyuan", ""], ["Yuan", "Xin", ""], ["Zhou", "Joey Tianyi", ""], ["Zhou", "Jiantao", ""], ["Wen", "Bihan", ""], ["Zhu", "Ce", ""]]}, {"id": "2005.07922", "submitter": "Vinay Kaushik", "authors": "Vinay Kaushik, Brejesh Lall", "title": "Deep feature fusion for self-supervised monocular depth prediction", "comments": "4 pages, 2 Tables, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in end-to-end unsupervised learning has significantly\nimproved the performance of monocular depth prediction and alleviated the\nrequirement of ground truth depth. Although a plethora of work has been done in\nenforcing various structural constraints by incorporating multiple losses\nutilising smoothness, left-right consistency, regularisation and matching\nsurface normals, a few of them take into consideration multi-scale structures\npresent in real world images. Most works utilise a VGG16 or ResNet50 model\npre-trained on ImageNet weights for predicting depth. We propose a deep feature\nfusion method utilising features at multiple scales for learning\nself-supervised depth from scratch. Our fusion network selects features from\nboth upper and lower levels at every level in the encoder network, thereby\ncreating multiple feature pyramid sub-networks that are fed to the decoder\nafter applying the CoordConv solution. We also propose a refinement module\nlearning higher scale residual depth from a combination of higher level deep\nfeatures and lower level residual depth using a pixel shuffling framework that\nsuper-resolves lower level residual depth. We select the KITTI dataset for\nevaluation and show that our proposed architecture can produce better or\ncomparable results in depth prediction.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 09:42:36 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Kaushik", "Vinay", ""], ["Lall", "Brejesh", ""]]}, {"id": "2005.07930", "submitter": "Lee Prangnell", "authors": "Lee Prangnell and Victor Sanchez", "title": "HVS-Based Perceptual Color Compression of Image Data", "comments": "Preprint: 2021 IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In perceptual image coding applications, the main objective is to decrease,\nas much as possible, Bits Per Pixel (BPP) while avoiding noticeable distortions\nin the reconstructed image. In this paper, we propose a novel perceptual image\ncoding technique, named Perceptual Color Compression (PCC). PCC is based on a\nnovel model related to Human Visual System (HVS) spectral sensitivity and\nCIELAB Just Noticeable Color Difference (JNCD). We utilize this modeling to\ncapitalize on the inability of the HVS to perceptually differentiate photons in\nvery similar wavelength bands (e.g., distinguishing very similar shades of a\nparticular color or different colors that look similar). The proposed PCC\ntechnique can be used with RGB (4:4:4) image data of various bit depths and\nspatial resolutions. In the evaluations, we compare the proposed PCC technique\nwith a set of reference methods including Versatile Video Coding (VVC) and High\nEfficiency Video Coding (HEVC) in addition to two other recently proposed\nalgorithms. Our PCC method attains considerable BPP reductions compared with\nall four reference techniques including, on average, 52.6% BPP reductions\ncompared with VVC (VVC in All Intra still image coding mode). Regarding image\nperceptual reconstruction quality, PCC achieves a score of SSIM = 0.99 in all\ntests in addition to a score of MS-SSIM = 0.99 in all but one test. Moreover,\nMOS = 5 is attained in 75% of subjective evaluation assessments conducted.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 10:05:20 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 15:41:55 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 17:15:53 GMT"}, {"version": "v4", "created": "Tue, 9 Feb 2021 17:02:45 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Prangnell", "Lee", ""], ["Sanchez", "Victor", ""]]}, {"id": "2005.07983", "submitter": "Chunping Qiu", "authors": "Chunping Qiu and Xiaochong Tong and Michael Schmitt and Benjamin\n  Bechtel and Xiao Xiang Zhu", "title": "Multi-level Feature Fusion-based CNN for Local Climate Zone\n  Classification from Sentinel-2 Images: Benchmark Results on the So2Sat LCZ42\n  Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a unique classification scheme for urban forms and functions, the local\nclimate zone (LCZ) system provides essential general information for any\nstudies related to urban environments, especially on a large scale. Remote\nsensing data-based classification approaches are the key to large-scale mapping\nand monitoring of LCZs. The potential of deep learning-based approaches is not\nyet fully explored, even though advanced convolutional neural networks (CNNs)\ncontinue to push the frontiers for various computer vision tasks. One reason is\nthat published studies are based on different datasets, usually at a regional\nscale, which makes it impossible to fairly and consistently compare the\npotential of different CNNs for real-world scenarios. This study is based on\nthe big So2Sat LCZ42 benchmark dataset dedicated to LCZ classification. Using\nthis dataset, we studied a range of CNNs of varying sizes. In addition, we\nproposed a CNN to classify LCZs from Sentinel-2 images, Sen2LCZ-Net. Using this\nbase network, we propose fusing multi-level features using the extended\nSen2LCZ-Net-MF. With this proposed simple network architecture and the highly\ncompetitive benchmark dataset, we obtain results that are better than those\nobtained by the state-of-the-art CNNs, while requiring less computation with\nfewer layers and parameters. Large-scale LCZ classification examples of\ncompletely unseen areas are presented, demonstrating the potential of our\nproposed Sen2LCZ-Net-MF as well as the So2Sat LCZ42 dataset. We also\nintensively investigated the influence of network depth and width and the\neffectiveness of the design choices made for Sen2LCZ-Net-MF. Our work will\nprovide important baselines for future CNN-based algorithm developments for\nboth LCZ classification and other urban land cover land use classification.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 13:16:47 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Qiu", "Chunping", ""], ["Tong", "Xiaochong", ""], ["Schmitt", "Michael", ""], ["Bechtel", "Benjamin", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2005.07991", "submitter": "Monu Verma", "authors": "Monu Verma, Santosh Kumar Vipparthi, Girdhari Singh", "title": "Non-Linearities Improve OrigiNet based on Active Imaging for Micro\n  Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Micro expression recognition (MER)is a very challenging task as the\nexpression lives very short in nature and demands feature modeling with the\ninvolvement of both spatial and temporal dynamics. Existing MER systems exploit\nCNN networks to spot the significant features of minor muscle movements and\nsubtle changes. However, existing networks fail to establish a relationship\nbetween spatial features of facial appearance and temporal variations of facial\ndynamics. Thus, these networks were not able to effectively capture minute\nvariations and subtle changes in expressive regions. To address these issues,\nwe introduce an active imaging concept to segregate active changes in\nexpressive regions of a video into a single frame while preserving facial\nappearance information. Moreover, we propose a shallow CNN network: hybrid\nlocal receptive field based augmented learning network (OrigiNet) that\nefficiently learns significant features of the micro-expressions in a video. In\nthis paper, we propose a new refined rectified linear unit (RReLU), which\novercome the problem of vanishing gradient and dying ReLU. RReLU extends the\nrange of derivatives as compared to existing activation functions. The RReLU\nnot only injects a nonlinearity but also captures the true edges by imposing\nadditive and multiplicative property. Furthermore, we present an augmented\nfeature learning block to improve the learning capabilities of the network by\nembedding two parallel fully connected layers. The performance of proposed\nOrigiNet is evaluated by conducting leave one subject out experiments on four\ncomprehensive ME datasets. The experimental results demonstrate that OrigiNet\noutperformed state-of-the-art techniques with less computational complexity.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 13:44:49 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Verma", "Monu", ""], ["Vipparthi", "Santosh Kumar", ""], ["Singh", "Girdhari", ""]]}, {"id": "2005.07995", "submitter": "Cesar H Comin Prof.", "authors": "Eric K. Tokuda, Cesar H. Comin and Luciano da F. Costa", "title": "Revisiting Agglomerative Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important issue in clustering concerns the avoidance of false positives\nwhile searching for clusters. This work addressed this problem considering\nagglomerative methods, namely single, average, median, complete, centroid and\nWard's approaches applied to unimodal and bimodal datasets obeying uniform,\ngaussian, exponential and power-law distributions. A model of clusters was also\nadopted, involving a higher density nucleus surrounded by a transition,\nfollowed by outliers. This paved the way to defining an objective means for\nidentifying the clusters from dendrograms. The adopted model also allowed the\nrelevance of the clusters to be quantified in terms of the height of their\nsubtrees. The obtained results include the verification that many methods\ndetect two clusters in unimodal data. The single-linkage method was found to be\nmore resilient to false positives. Also, several methods detected clusters not\ncorresponding directly to the nucleus. The possibility of identifying the type\nof distribution was also investigated.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 14:07:25 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 23:00:41 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Tokuda", "Eric K.", ""], ["Comin", "Cesar H.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "2005.08000", "submitter": "Nikolaos Zioulis Mr.", "authors": "Vasileios Gkitsas (1) and Nikolaos Zioulis (1 and 2) and Federico\n  Alvarez (2) and Dimitrios Zarpalas (1) and Petros Daras (1) ((1) Centre for\n  Research and Technology Hellas, (2) Universidad Politecnica de Madrid)", "title": "Deep Lighting Environment Map Estimation from Spherical Panoramas", "comments": "Code and models available at\n  https://vcl3d.github.io/DeepPanoramaLighting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a scene's lighting is a very important task when compositing\nsynthetic content within real environments, with applications in mixed reality\nand post-production. In this work we present a data-driven model that estimates\nan HDR lighting environment map from a single LDR monocular spherical panorama.\nIn addition to being a challenging and ill-posed problem, the lighting\nestimation task also suffers from a lack of facile illumination ground truth\ndata, a fact that hinders the applicability of data-driven methods. We approach\nthis problem differently, exploiting the availability of surface geometry to\nemploy image-based relighting as a data generator and supervision mechanism.\nThis relies on a global Lambertian assumption that helps us overcome issues\nrelated to pre-baked lighting. We relight our training data and complement the\nmodel's supervision with a photometric loss, enabled by a differentiable\nimage-based relighting technique. Finally, since we predict spherical spectral\ncoefficients, we show that by imposing a distribution prior on the predicted\ncoefficients, we can greatly boost performance. Code and models available at\nhttps://vcl3d.github.io/DeepPanoramaLighting.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 14:23:05 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Gkitsas", "Vasileios", "", "1 and 2"], ["Zioulis", "Nikolaos", "", "1 and 2"], ["Alvarez", "Federico", ""], ["Zarpalas", "Dimitrios", ""], ["Daras", "Petros", ""]]}, {"id": "2005.08001", "submitter": "Keqi Wang", "authors": "Keqi Wang, Peng Gao, Steven Hoi, Qian Guo, Yuhua Qian", "title": "Extreme Low-Light Imaging with Multi-granulation Cooperative Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-light imaging is challenging since images may appear to be dark and\nnoised due to low signal-to-noise ratio, complex image content, and the variety\nin shooting scenes in extreme low-light condition. Many methods have been\nproposed to enhance the imaging quality under extreme low-light conditions, but\nit remains difficult to obtain satisfactory results, especially when they\nattempt to retain high dynamic range (HDR). In this paper, we propose a novel\nmethod of multi-granulation cooperative networks (MCN) with bidirectional\ninformation flow to enhance extreme low-light images, and design an\nillumination map estimation function (IMEF) to preserve high dynamic range\n(HDR). To facilitate this research, we also contribute to create a new\nbenchmark dataset of real-world Dark High Dynamic Range (DHDR) images to\nevaluate the performance of high dynamic preservation in low light environment.\nExperimental results show that the proposed method outperforms the\nstate-of-the-art approaches in terms of both visual effects and quantitative\nanalysis.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 14:26:06 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Wang", "Keqi", ""], ["Gao", "Peng", ""], ["Hoi", "Steven", ""], ["Guo", "Qian", ""], ["Qian", "Yuhua", ""]]}, {"id": "2005.08009", "submitter": "Aibek Musaev", "authors": "Aibek Musaev, Jiangping Wang, Liang Zhu, Cheng Li, Yi Chen, Jialin\n  Liu, Wanqi Zhang, Juan Mei, De Wang", "title": "Towards in-store multi-person tracking using head detection and track\n  heatmaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision algorithms are being implemented across a breadth of\nindustries to enable technological innovations. In this paper, we study the\nproblem of computer vision based customer tracking in retail industry. To this\nend, we introduce a dataset collected from a camera in an office environment\nwhere participants mimic various behaviors of customers in a supermarket. In\naddition, we describe an illustrative example of the use of this dataset for\ntracking participants based on a head tracking model in an effort to minimize\nerrors due to occlusion. Furthermore, we propose a model for recognizing\ncustomers and staff based on their movement patterns. The model is evaluated\nusing a real-world dataset collected in a supermarket over a 24-hour period\nthat achieves 98% accuracy during training and 93% accuracy during evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 15:07:19 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 03:22:46 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Musaev", "Aibek", ""], ["Wang", "Jiangping", ""], ["Zhu", "Liang", ""], ["Li", "Cheng", ""], ["Chen", "Yi", ""], ["Liu", "Jialin", ""], ["Zhang", "Wanqi", ""], ["Mei", "Juan", ""], ["Wang", "De", ""]]}, {"id": "2005.08028", "submitter": "Xin Yuan", "authors": "Xin Yuan", "title": "Various Total Variation for Snapshot Video Compressive Imaging", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling high-dimensional images is challenging due to limited availability\nof sensors; scanning is usually necessary in these cases. To mitigate this\nchallenge, snapshot compressive imaging (SCI) was proposed to capture the\nhigh-dimensional (usually 3D) images using a 2D sensor (detector). Via novel\noptical design, the {\\em measurement} captured by the sensor is an encoded\nimage of multiple frames of the 3D desired signal. Following this,\nreconstruction algorithms are employed to retrieve the high-dimensional data.\nThough various algorithms have been proposed, the total variation (TV) based\nmethod is still the most efficient one due to a good trade-off between\ncomputational time and performance. This paper aims to answer the question of\nwhich TV penalty (anisotropic TV, isotropic TV and vectorized TV) works best\nfor video SCI reconstruction? Various TV denoising and projection algorithms\nare developed and tested for video SCI reconstruction on both simulation and\nreal datasets.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 16:20:56 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Yuan", "Xin", ""]]}, {"id": "2005.08045", "submitter": "Aniket Agarwal", "authors": "Aniket Agarwal, Ayush Mangal, Vipul", "title": "Visual Relationship Detection using Scene Graphs: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding a scene by decoding the visual relationships depicted in an\nimage has been a long studied problem. While the recent advances in deep\nlearning and the usage of deep neural networks have achieved near human\naccuracy on many tasks, there still exists a pretty big gap between human and\nmachine level performance when it comes to various visual relationship\ndetection tasks. Developing on earlier tasks like object recognition,\nsegmentation and captioning which focused on a relatively coarser image\nunderstanding, newer tasks have been introduced recently to deal with a finer\nlevel of image understanding. A Scene Graph is one such technique to better\nrepresent a scene and the various relationships present in it. With its wide\nnumber of applications in various tasks like Visual Question Answering,\nSemantic Image Retrieval, Image Generation, among many others, it has proved to\nbe a useful tool for deeper and better visual relationship understanding. In\nthis paper, we present a detailed survey on the various techniques for scene\ngraph generation, their efficacy to represent visual relationships and how it\nhas been used to solve various downstream tasks. We also attempt to analyze the\nvarious future directions in which the field might advance in the future. Being\none of the first papers to give a detailed survey on this topic, we also hope\nto give a succinct introduction to scene graphs, and guide practitioners while\ndeveloping approaches for their applications.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 17:06:06 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Agarwal", "Aniket", ""], ["Mangal", "Ayush", ""], ["Vipul", "", ""]]}, {"id": "2005.08076", "submitter": "Richard Jiang", "authors": "Fraser Young, L Zhang, Richard Jiang, Han Liu and Conor Wall", "title": "A Deep Learning based Wearable Healthcare IoT Device for AI-enabled\n  Hearing Assistance Automation", "comments": null, "journal-ref": "The 2020 International Conference on Machine Learning and\n  Cybernetics", "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the recent booming of artificial intelligence (AI), particularly deep\nlearning techniques, digital healthcare is one of the prevalent areas that\ncould gain benefits from AI-enabled functionality. This research presents a\nnovel AI-enabled Internet of Things (IoT) device operating from the ESP-8266\nplatform capable of assisting those who suffer from impairment of hearing or\ndeafness to communicate with others in conversations. In the proposed solution,\na server application is created that leverages Google's online speech\nrecognition service to convert the received conversations into texts, then\ndeployed to a micro-display attached to the glasses to display the conversation\ncontents to deaf people, to enable and assist conversation as normal with the\ngeneral population. Furthermore, in order to raise alert of traffic or\ndangerous scenarios, an 'urban-emergency' classifier is developed using a deep\nlearning model, Inception-v4, with transfer learning to detect/recognize\nalerting/alarming sounds, such as a horn sound or a fire alarm, with texts\ngenerated to alert the prospective user. The training of Inception-v4 was\ncarried out on a consumer desktop PC and then implemented into the AI based IoT\napplication. The empirical results indicate that the developed prototype system\nachieves an accuracy rate of 92% for sound recognition and classification with\nreal-time performance.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 19:42:16 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Young", "Fraser", ""], ["Zhang", "L", ""], ["Jiang", "Richard", ""], ["Liu", "Han", ""], ["Wall", "Conor", ""]]}, {"id": "2005.08087", "submitter": "Ashutosh Chaubey", "authors": "Ashutosh Chaubey, Nikhil Agrawal, Kavya Barnwal, Keerat K. Guliani,\n  Pramod Mehta", "title": "Universal Adversarial Perturbations: A Survey", "comments": "20 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, Deep Learning has emerged as a useful and efficient\ntool to solve a wide variety of complex learning problems ranging from image\nclassification to human pose estimation, which is challenging to solve using\nstatistical machine learning algorithms. However, despite their superior\nperformance, deep neural networks are susceptible to adversarial perturbations,\nwhich can cause the network's prediction to change without making perceptible\nchanges to the input image, thus creating severe security issues at the time of\ndeployment of such systems. Recent works have shown the existence of Universal\nAdversarial Perturbations, which, when added to any image in a dataset,\nmisclassifies it when passed through a target model. Such perturbations are\nmore practical to deploy since there is minimal computation done during the\nactual attack. Several techniques have also been proposed to defend the neural\nnetworks against these perturbations. In this paper, we attempt to provide a\ndetailed discussion on the various data-driven and data-independent methods for\ngenerating universal perturbations, along with measures to defend against such\nperturbations. We also cover the applications of such universal perturbations\nin various deep learning tasks.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 20:18:26 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Chaubey", "Ashutosh", ""], ["Agrawal", "Nikhil", ""], ["Barnwal", "Kavya", ""], ["Guliani", "Keerat K.", ""], ["Mehta", "Pramod", ""]]}, {"id": "2005.08094", "submitter": "Sharif Amit Kamran", "authors": "Sharif Amit Kamran, Alireza Tavakkoli, Stewart Lee Zuckerbrod", "title": "Improving Robustness using Joint Attention Network For Detecting Retinal\n  Degeneration From Optical Coherence Tomography Images", "comments": "\\c{opyright} 2020 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy data and the similarity in the ocular appearances caused by different\nophthalmic pathologies pose significant challenges for an automated expert\nsystem to accurately detect retinal diseases. In addition, the lack of\nknowledge transferability and the need for unreasonably large datasets limit\nclinical application of current machine learning systems. To increase\nrobustness, a better understanding of how the retinal subspace deformations\nlead to various levels of disease severity needs to be utilized for\nprioritizing disease-specific model details. In this paper we propose the use\nof disease-specific feature representation as a novel architecture comprised of\ntwo joint networks -- one for supervised encoding of disease model and the\nother for producing attention maps in an unsupervised manner to retain disease\nspecific spatial information. Our experimental results on publicly available\ndatasets show the proposed joint-network significantly improves the accuracy\nand robustness of state-of-the-art retinal disease classification networks on\nunseen datasets.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 20:32:49 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 01:16:42 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Kamran", "Sharif Amit", ""], ["Tavakkoli", "Alireza", ""], ["Zuckerbrod", "Stewart Lee", ""]]}, {"id": "2005.08104", "submitter": "Nikita Araslanov", "authors": "Nikita Araslanov and Stefan Roth", "title": "Single-Stage Semantic Segmentation from Image Labels", "comments": "To appear at CVPR 2020; minor corrections in Eq. (9). Code:\n  https://github.com/visinf/1-stage-wseg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a rapid growth in new approaches improving the\naccuracy of semantic segmentation in a weakly supervised setting, i.e. with\nonly image-level labels available for training. However, this has come at the\ncost of increased model complexity and sophisticated multi-stage training\nprocedures. This is in contrast to earlier work that used only a single stage\n$-$ training one segmentation network on image labels $-$ which was abandoned\ndue to inferior segmentation accuracy. In this work, we first define three\ndesirable properties of a weakly supervised method: local consistency, semantic\nfidelity, and completeness. Using these properties as guidelines, we then\ndevelop a segmentation-based network model and a self-supervised training\nscheme to train for semantic masks from image-level annotations in a single\nstage. We show that despite its simplicity, our method achieves results that\nare competitive with significantly more complex pipelines, substantially\noutperforming earlier single-stage methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 21:10:10 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Araslanov", "Nikita", ""], ["Roth", "Stefan", ""]]}, {"id": "2005.08108", "submitter": "Josef Bigun", "authors": "Josef Bigun and Fernando Alonso-Fernandez", "title": "Analytic Signal Phase in $N-D$ by Linear Symmetry Tensor--fingerprint\n  modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reveal that the Analytic Signal phase, and its gradient have a hitherto\nunstudied discontinuity in $2-D $ and higher dimensions. The shortcoming can\nresult in severe artifacts whereas the problem does not exist in $1-D $\nsignals. Direct use of Gabor phase, or its gradient, in computer vision and\nbiometric recognition e.g., as done in influential studies\n\\cite{fleet90,wiskott1997face}, may produce undesired results that will go\nunnoticed unless special images similar to ours reveal them. Instead of the\nAnalytic Signal phase, we suggest the use of Linear Symmetry phase, relying on\nmore than one set of Gabor filters, but with a negligible computational add-on,\nas a remedy. Gradient magnitudes of this phase are continuous in contrast to\nthat of the analytic signal whereas continuity of the gradient direction of the\nphase is guaranteed if Linear Symmetry Tensor replaces gradient vector. The\nsuggested phase has also a built-in automatic scale estimator, useful for\nrobust detection of patterns by multi-scale processing. We show crucial\nconcepts on synthesized fingerprint images, where ground truth regarding\ninstantaneous frequency, (scale \\& direction), and phase are known with\nfavorable results. A comparison to a baseline alternative is also reported. To\nthat end, a novel multi-scale minutia model where location, direction, and\nscale of minutia parameters are steerable, without the creation of\nuncontrollable minutia is also presented. This is a useful tool, to reduce\ndevelopment times of minutia detection methods with explainable behavior. A\nrevealed consequence is that minutia directions are not determined by the\nlinear phase alone, but also by each other and the influence must be corrected\nto obtain steerability and accurate ground truths. Essential conclusions are\nreadily transferable to $N-D $, and unrelated applications, e.g. optical flow\nor disparity estimation in stereo.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 21:17:26 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Bigun", "Josef", ""], ["Alonso-Fernandez", "Fernando", ""]]}, {"id": "2005.08114", "submitter": "Ignasi Clavera", "authors": "Yiming Ding, Ignasi Clavera, Pieter Abbeel", "title": "Mutual Information Maximization for Robust Plannable Representations", "comments": "Accepted at NeurIPS 2019 Workshop on Robot Learning: Control and\n  Interaction in the Real World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending the capabilities of robotics to real-world complex, unstructured\nenvironments requires the need of developing better perception systems while\nmaintaining low sample complexity. When dealing with high-dimensional state\nspaces, current methods are either model-free or model-based based on\nreconstruction objectives. The sample inefficiency of the former constitutes a\nmajor barrier for applying them to the real-world. The later, while they\npresent low sample complexity, they learn latent spaces that need to\nreconstruct every single detail of the scene. In real environments, the task\ntypically just represents a small fraction of the scene. Reconstruction\nobjectives suffer in such scenarios as they capture all the unnecessary\ncomponents. In this work, we present MIRO, an information theoretic\nrepresentational learning algorithm for model-based reinforcement learning. We\ndesign a latent space that maximizes the mutual information with the future\ninformation while being able to capture all the information needed for\nplanning. We show that our approach is more robust than reconstruction\nobjectives in the presence of distractors and cluttered scenes\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 21:58:47 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Ding", "Yiming", ""], ["Clavera", "Ignasi", ""], ["Abbeel", "Pieter", ""]]}, {"id": "2005.08116", "submitter": "Benjamin Kunsberg", "authors": "Benjamin Kunsberg and Steven W. Zucker", "title": "From Boundaries to Bumps: when closed (extremal) contours are critical", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invariants underlying shape inference are elusive: a variety of shapes can\ngive rise to the same image, and a variety of images can be rendered from the\nsame shape. The occluding contour is a rare exception: it has both image\nsalience, in terms of isophotes, and surface meaning, in terms of surface\nnormal. We relax the notion of occluding contour to define closed extremal\ncurves, a new shape invariant that exists at the topological level. They\nsurround bumps, a common but ill-specified interior shape component, and\nformalize the qualitative nature of bump perception. Extremal curves are\nbiologically computable, unify shape inferences from shading, texture, and\nspecular materials, and predict new phenomena in bump perception.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 22:27:12 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Kunsberg", "Benjamin", ""], ["Zucker", "Steven W.", ""]]}, {"id": "2005.08135", "submitter": "Mubariz Zaffar", "authors": "Mubariz Zaffar and Shoaib Ehsan and Michael Milford and David Flynn\n  and Klaus McDonald-Maier", "title": "VPR-Bench: An Open-Source Visual Place Recognition Evaluation Framework\n  with Quantifiable Viewpoint and Appearance Change", "comments": "Currently under-review, 25 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Place Recognition (VPR) is the process of recognising a previously\nvisited place using visual information, often under varying appearance\nconditions and viewpoint changes and with computational constraints. VPR is a\ncritical component of many autonomous navigation systems ranging from\nautonomous vehicles to drones. While the concept of place recognition has been\naround for many years, VPR research has grown rapidly as a field over the past\ndecade due to both improving camera hardware technologies and its suitability\nfor application of deep learning-based techniques. With this growth however has\ncome field fragmentation, lack of standardisation and a disconnect between\ncurrent performance metrics and the actual utility of a VPR technique at\napplication-deployment. In this paper we address these key challenges through a\nnew comprehensive open-source evaluation framework, dubbed 'VPR-Bench'.\nVPR-Bench introduces two much-needed capabilities for researchers: firstly,\nquantification of viewpoint and illumination variation, replacing what has\nlargely been assessed qualitatively in the past, and secondly, new metrics\n'Extended precision' (EP), 'Performance-Per-Compute-Unit' (PCU) and 'Number of\nProspective Place Matching Candidates' (NPPMC). These new metrics complement\nthe limitations of traditional Precision-Recall curves, by providing measures\nthat are more informative to the wide range of potential VPR applications.\nMechanistically, we develop new unified templates that facilitate the\nimplementation, deployment and evaluation of a wide range of VPR techniques and\ndatasets. We incorporate the most comprehensive combination of state-of-the-art\nVPR techniques and datasets to date into VPR-Bench and demonstrate how it\nprovides a rich range of previously inaccessible insights, such as the nuanced\nrelationship between viewpoint invariance, different types of VPR techniques\nand datasets.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 00:27:53 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zaffar", "Mubariz", ""], ["Ehsan", "Shoaib", ""], ["Milford", "Michael", ""], ["Flynn", "David", ""], ["McDonald-Maier", "Klaus", ""]]}, {"id": "2005.08139", "submitter": "Wei-Lun Chao", "authors": "Yan Wang, Xiangyu Chen, Yurong You, Li Erran, Bharath Hariharan, Mark\n  Campbell, Kilian Q. Weinberger, Wei-Lun Chao", "title": "Train in Germany, Test in The USA: Making 3D Object Detectors Generalize", "comments": "Accepted to 2020 Conference on Computer Vision and Pattern\n  Recognition (CVPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In the domain of autonomous driving, deep learning has substantially improved\nthe 3D object detection accuracy for LiDAR and stereo camera data alike. While\ndeep networks are great at generalization, they are also notorious to over-fit\nto all kinds of spurious artifacts, such as brightness, car sizes and models,\nthat may appear consistently throughout the data. In fact, most datasets for\nautonomous driving are collected within a narrow subset of cities within one\ncountry, typically under similar weather conditions. In this paper we consider\nthe task of adapting 3D object detectors from one dataset to another. We\nobserve that naively, this appears to be a very challenging task, resulting in\ndrastic drops in accuracy levels. We provide extensive experiments to\ninvestigate the true adaptation challenges and arrive at a surprising\nconclusion: the primary adaptation hurdle to overcome are differences in car\nsizes across geographic areas. A simple correction based on the average car\nsize yields a strong correction of the adaptation gap. Our proposed method is\nsimple and easily incorporated into most 3D object detection frameworks. It\nprovides a first baseline for 3D object detection adaptation across countries,\nand gives hope that the underlying problem may be more within grasp than one\nmay have hoped to believe. Our code is available at\nhttps://github.com/cxy1997/3D_adapt_auto_driving.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 00:56:18 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Wang", "Yan", ""], ["Chen", "Xiangyu", ""], ["You", "Yurong", ""], ["Erran", "Li", ""], ["Hariharan", "Bharath", ""], ["Campbell", "Mark", ""], ["Weinberger", "Kilian Q.", ""], ["Chao", "Wei-Lun", ""]]}, {"id": "2005.08144", "submitter": "Christopher Choy", "authors": "Christopher Choy, Junha Lee, Rene Ranftl, Jaesik Park, Vladlen Koltun", "title": "High-dimensional Convolutional Networks for Geometric Pattern\n  Recognition", "comments": "Accepted for CVPR 2020 oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many problems in science and engineering can be formulated in terms of\ngeometric patterns in high-dimensional spaces. We present high-dimensional\nconvolutional networks (ConvNets) for pattern recognition problems that arise\nin the context of geometric registration. We first study the effectiveness of\nconvolutional networks in detecting linear subspaces in high-dimensional spaces\nwith up to 32 dimensions: much higher dimensionality than prior applications of\nConvNets. We then apply high-dimensional ConvNets to 3D registration under\nrigid motions and image correspondence estimation. Experiments indicate that\nour high-dimensional ConvNets outperform prior approaches that relied on deep\nnetworks based on global pooling operators.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 01:46:12 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Choy", "Christopher", ""], ["Lee", "Junha", ""], ["Ranftl", "Rene", ""], ["Park", "Jaesik", ""], ["Koltun", "Vladlen", ""]]}, {"id": "2005.08165", "submitter": "Rui Fan", "authors": "Rui Fan, Hengli Wang, Bohuan Xue, Huaiyang Huang, Yuan Wang, Ming Liu,\n  Ioannis Pitas", "title": "Three-Filters-to-Normal: An Accurate and Ultrafast Surface Normal\n  Estimator", "comments": "webpage: sites.google.com/view/3f2n, accepted to IEEE RA-L and\n  ICRA'21", "journal-ref": null, "doi": "10.1109/LRA.2021.3067308", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes three-filters-to-normal (3F2N), an accurate and ultrafast\nsurface normal estimator (SNE), which is designed for structured range sensor\ndata, e.g., depth/disparity images. 3F2N SNE computes surface normals by simply\nperforming three filtering operations (two image gradient filters in horizontal\nand vertical directions, respectively, and a mean/median filter) on an inverse\ndepth image or a disparity image. Despite the simplicity of 3F2N SNE, no\nsimilar method already exists in the literature. To evaluate the performance of\nour proposed SNE, we created three large-scale synthetic datasets (easy, medium\nand hard) using 24 3D mesh models, each of which is used to generate 1800--2500\npairs of depth images (resolution: 480X640 pixels) and the corresponding\nground-truth surface normal maps from different views. 3F2N SNE demonstrates\nthe state-of-the-art performance, outperforming all other existing\ngeometry-based SNEs, where the average angular errors with respect to the easy,\nmedium and hard datasets are 1.66 degrees, 5.69 degrees and 15.31 degrees,\nrespectively. Furthermore, our C++ and CUDA implementations achieve a\nprocessing speed of over 260 Hz and 21 kHz, respectively. Our datasets and\nsource code are publicly available at sites.google.com/view/3f2n.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 04:46:24 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 20:45:12 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 23:06:18 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Fan", "Rui", ""], ["Wang", "Hengli", ""], ["Xue", "Bohuan", ""], ["Huang", "Huaiyang", ""], ["Wang", "Yuan", ""], ["Liu", "Ming", ""], ["Pitas", "Ioannis", ""]]}, {"id": "2005.08168", "submitter": "Jingwu He", "authors": "Jingwu He, Chuan Wang, Yang Zhang, Jie Guo, and Yanwen Guo", "title": "FA-GANs: Facial Attractiveness Enhancement with Generative Adversarial\n  Networks on Frontal Faces", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attractiveness enhancement has been an interesting application in\nComputer Vision and Graphics over these years. It aims to generate a more\nattractive face via manipulations on image and geometry structure while\npreserving face identity. In this paper, we propose the first Generative\nAdversarial Networks (GANs) for enhancing facial attractiveness in both\ngeometry and appearance aspects, which we call \"FA-GANs\". FA-GANs contain two\nbranches and enhance facial attractiveness in two perspectives: facial geometry\nand facial appearance. Each branch consists of individual GANs with the\nappearance branch adjusting the facial image and the geometry branch adjusting\nthe facial landmarks in appearance and geometry aspects, respectively. Unlike\nthe traditional facial manipulations learning from paired faces, which are\ninfeasible to collect before and after enhancement of the same individual, we\nachieve this by learning the features of attractiveness faces through\nunsupervised adversarial learning. The proposed FA-GANs are able to extract\nattractiveness features and impose them on the enhancement results. To better\nenhance faces, both the geometry and appearance networks are considered to\nrefine the facial attractiveness by adjusting the geometry layout of faces and\nthe appearance of faces independently. To the best of our knowledge, we are the\nfirst to enhance the facial attractiveness with GANs in both geometry and\nappearance aspects. The experimental results suggest that our FA-GANs can\ngenerate compelling perceptual results in both geometry structure and facial\nappearance and outperform current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 05:02:39 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 00:58:07 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["He", "Jingwu", ""], ["Wang", "Chuan", ""], ["Zhang", "Yang", ""], ["Guo", "Jie", ""], ["Guo", "Yanwen", ""]]}, {"id": "2005.08170", "submitter": "Shashi Kant", "authors": "Fengzi Li, Shashi Kant, Shunichi Araki, Sumer Bangera, Swapna Samir\n  Shukla", "title": "Neural Networks for Fashion Image Classification and Visual Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss two potentially challenging problems faced by the ecommerce\nindustry. One relates to the problem faced by sellers while uploading pictures\nof products on the platform for sale and the consequent manual tagging\ninvolved. It gives rise to misclassifications leading to its absence from\nsearch results. The other problem concerns with the potential bottleneck in\nplacing orders when a customer may not know the right keywords but has a visual\nimpression of an image. An image based search algorithm can unleash the true\npotential of ecommerce by enabling customers to click a picture of an object\nand search for similar products without the need for typing. In this paper, we\nexplore machine learning algorithms which can help us solve both these\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 05:25:41 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Li", "Fengzi", ""], ["Kant", "Shashi", ""], ["Araki", "Shunichi", ""], ["Bangera", "Sumer", ""], ["Shukla", "Swapna Samir", ""]]}, {"id": "2005.08186", "submitter": "Itai Lang", "authors": "Anna Darzi, Itai Lang, Ashutosh Taklikar, Hadar Averbuch-Elor, Shai\n  Avidan", "title": "Co-occurrence Based Texture Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As image generation techniques mature, there is a growing interest in\nexplainable representations that are easy to understand and intuitive to\nmanipulate. In this work, we turn to co-occurrence statistics, which have long\nbeen used for texture analysis, to learn a controllable texture synthesis\nmodel. We propose a fully convolutional generative adversarial network,\nconditioned locally on co-occurrence statistics, to generate arbitrarily large\nimages while having local, interpretable control over the texture appearance.\nTo encourage fidelity to the input condition, we introduce a novel\ndifferentiable co-occurrence loss that is integrated seamlessly into our\nframework in an end-to-end fashion. We demonstrate that our solution offers a\nstable, intuitive and interpretable latent representation for texture\nsynthesis, which can be used to generate a smooth texture morph between\ndifferent textures. We further show an interactive texture tool that allows a\nuser to adjust local characteristics of the synthesized texture image using the\nco-occurrence values directly.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 08:01:44 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 19:34:09 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Darzi", "Anna", ""], ["Lang", "Itai", ""], ["Taklikar", "Ashutosh", ""], ["Averbuch-Elor", "Hadar", ""], ["Avidan", "Shai", ""]]}, {"id": "2005.08191", "submitter": "Saeideh Ghanbari Azar", "authors": "Saeideh Ghanbari Azar and Saeed Meshgini and Tohid Yousefi Rezaii and\n  Soosan Beheshti", "title": "Hyperspectral Image Classification Based on Sparse Modeling of Spectral\n  Blocks", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2020.04.138", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images provide abundant spatial and spectral information that\nis very valuable for material detection in diverse areas of practical science.\nThe high-dimensions of data lead to many processing challenges that can be\naddressed via existent spatial and spectral redundancies. In this paper, a\nsparse modeling framework is proposed for hyperspectral image classification.\nSpectral blocks are introduced to be used along with spatial groups to jointly\nexploit spectral and spatial redundancies. To reduce the computational\ncomplexity of sparse modeling, spectral blocks are used to break the\nhigh-dimensional optimization problems into small-size sub-problems that are\nfaster to solve. Furthermore, the proposed sparse structure enables to extract\nthe most discriminative spectral blocks and further reduce the computational\nburden. Experiments on three benchmark datasets, i.e., Pavia University Image\nand Indian Pines Image verify that the proposed method leads to a robust sparse\nmodeling of hyperspectral images and improves the classification accuracy\ncompared to several state-of-the-art methods. Moreover, the experiments\ndemonstrate that the proposed method requires less processing time.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 08:18:13 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Azar", "Saeideh Ghanbari", ""], ["Meshgini", "Saeed", ""], ["Rezaii", "Tohid Yousefi", ""], ["Beheshti", "Soosan", ""]]}, {"id": "2005.08209", "submitter": "Rudrabha Mukhopadhyay", "authors": "K R Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, C V Jawahar", "title": "Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis", "comments": "10 pages (including references), 5 figures, Accepted in CVPR, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans involuntarily tend to infer parts of the conversation from lip\nmovements when the speech is absent or corrupted by external noise. In this\nwork, we explore the task of lip to speech synthesis, i.e., learning to\ngenerate natural speech given only the lip movements of a speaker.\nAcknowledging the importance of contextual and speaker-specific cues for\naccurate lip-reading, we take a different path from existing works. We focus on\nlearning accurate lip sequences to speech mappings for individual speakers in\nunconstrained, large vocabulary settings. To this end, we collect and release a\nlarge-scale benchmark dataset, the first of its kind, specifically to train and\nevaluate the single-speaker lip to speech task in natural settings. We propose\na novel approach with key design choices to achieve accurate, natural lip to\nspeech synthesis in such unconstrained scenarios for the first time. Extensive\nevaluation using quantitative, qualitative metrics and human evaluation shows\nthat our method is four times more intelligible than previous works in this\nspace. Please check out our demo video for a quick overview of the paper,\nmethod, and qualitative results.\nhttps://www.youtube.com/watch?v=HziA-jmlk_4&feature=youtu.be\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 10:29:19 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Prajwal", "K R", ""], ["Mukhopadhyay", "Rudrabha", ""], ["Namboodiri", "Vinay", ""], ["Jawahar", "C V", ""]]}, {"id": "2005.08230", "submitter": "Boris Knyazev", "authors": "Boris Knyazev, Harm de Vries, C\\u{a}t\\u{a}lina Cangea, Graham W.\n  Taylor, Aaron Courville, Eugene Belilovsky", "title": "Graph Density-Aware Losses for Novel Compositions in Scene Graph\n  Generation", "comments": "accepted at BMVC 2020, the code is available at\n  https://github.com/bknyaz/sgg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graph generation (SGG) aims to predict graph-structured descriptions of\ninput images, in the form of objects and relationships between them. This task\nis becoming increasingly useful for progress at the interface of vision and\nlanguage. Here, it is important - yet challenging - to perform well on novel\n(zero-shot) or rare (few-shot) compositions of objects and relationships. In\nthis paper, we identify two key issues that limit such generalization. Firstly,\nwe show that the standard loss used in this task is unintentionally a function\nof scene graph density. This leads to the neglect of individual edges in large\nsparse graphs during training, even though these contain diverse few-shot\nexamples that are important for generalization. Secondly, the frequency of\nrelationships can create a strong bias in this task, such that a blind model\npredicting the most frequent relationship achieves good performance.\nConsequently, some state-of-the-art models exploit this bias to improve\nresults. We show that such models can suffer the most in their ability to\ngeneralize to rare compositions, evaluating two different models on the Visual\nGenome dataset and its more recent, improved version, GQA. To address these\nissues, we introduce a density-normalized edge loss, which provides more than a\ntwo-fold improvement in certain generalization metrics. Compared to other works\nin this direction, our enhancements require only a few lines of code and no\nadded computational cost. We also highlight the difficulty of accurately\nevaluating models using existing metrics, especially on zero/few shots, and\nintroduce a novel weighted metric.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 11:45:29 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 00:47:17 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Knyazev", "Boris", ""], ["de Vries", "Harm", ""], ["Cangea", "C\u0103t\u0103lina", ""], ["Taylor", "Graham W.", ""], ["Courville", "Aaron", ""], ["Belilovsky", "Eugene", ""]]}, {"id": "2005.08235", "submitter": "Manuel Rey-Area", "authors": "Manuel Rey-Area, Emilio Guirado, Siham Tabik and Javier Ruiz-Hidalgo", "title": "FuCiTNet: Improving the generalization of deep learning networks by the\n  fusion of learned class-inherent transformations", "comments": null, "journal-ref": null, "doi": "10.1016/j.inffus.2020.06.015", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely known that very small datasets produce overfitting in Deep\nNeural Networks (DNNs), i.e., the network becomes highly biased to the data it\nhas been trained on. This issue is often alleviated using transfer learning,\nregularization techniques and/or data augmentation. This work presents a new\napproach, independent but complementary to the previous mentioned techniques,\nfor improving the generalization of DNNs on very small datasets in which the\ninvolved classes share many visual features. The proposed methodology, called\nFuCiTNet (Fusion Class inherent Transformations Network), inspired by GANs,\ncreates as many generators as classes in the problem. Each generator, $k$,\nlearns the transformations that bring the input image into the k-class domain.\nWe introduce a classification loss in the generators to drive the leaning of\nspecific k-class transformations. Our experiments demonstrate that the proposed\ntransformations improve the generalization of the classification model in three\ndiverse datasets.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 12:04:20 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Rey-Area", "Manuel", ""], ["Guirado", "Emilio", ""], ["Tabik", "Siham", ""], ["Ruiz-Hidalgo", "Javier", ""]]}, {"id": "2005.08271", "submitter": "Vladimir Iashin", "authors": "Vladimir Iashin and Esa Rahtu", "title": "A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal\n  Transformer", "comments": "Accepted by BMVC 2020. More experiments. Code:\n  https://github.com/v-iashin/bmt Project page: https://v-iashin.github.io/bmt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dense video captioning aims to localize and describe important events in\nuntrimmed videos. Existing methods mainly tackle this task by exploiting only\nvisual features, while completely neglecting the audio track. Only a few prior\nworks have utilized both modalities, yet they show poor results or demonstrate\nthe importance on a dataset with a specific domain. In this paper, we introduce\nBi-modal Transformer which generalizes the Transformer architecture for a\nbi-modal input. We show the effectiveness of the proposed model with audio and\nvisual modalities on the dense video captioning task, yet the module is capable\nof digesting any two modalities in a sequence-to-sequence task. We also show\nthat the pre-trained bi-modal encoder as a part of the bi-modal transformer can\nbe used as a feature extractor for a simple proposal generation module. The\nperformance is demonstrated on a challenging ActivityNet Captions dataset where\nour model achieves outstanding performance. The code is available:\nv-iashin.github.io/bmt\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 15:00:05 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 09:17:48 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Iashin", "Vladimir", ""], ["Rahtu", "Esa", ""]]}, {"id": "2005.08307", "submitter": "Alessia Bertugli", "authors": "Alessia Bertugli, Simone Calderara, Pasquale Coscia, Lamberto Ballan,\n  Rita Cucchiara", "title": "AC-VRNN: Attentive Conditional-VRNN for Multi-Future Trajectory\n  Prediction", "comments": "Accepted at Computer Vision and Image Understanding (CVIU)", "journal-ref": null, "doi": "10.1016/j.cviu.2021.103245", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anticipating human motion in crowded scenarios is essential for developing\nintelligent transportation systems, social-aware robots and advanced video\nsurveillance applications. A key component of this task is represented by the\ninherently multi-modal nature of human paths which makes socially acceptable\nmultiple futures when human interactions are involved. To this end, we propose\na generative architecture for multi-future trajectory predictions based on\nConditional Variational Recurrent Neural Networks (C-VRNNs). Conditioning\nmainly relies on prior belief maps, representing most likely moving directions\nand forcing the model to consider past observed dynamics in generating future\npositions. Human interactions are modeled with a graph-based attention\nmechanism enabling an online attentive hidden state refinement of the recurrent\nestimation. To corroborate our model, we perform extensive experiments on\npublicly-available datasets (e.g., ETH/UCY, Stanford Drone Dataset, STATS\nSportVU NBA, Intersection Drone Dataset and TrajNet++) and demonstrate its\neffectiveness in crowded scenes compared to several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 17:21:23 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 08:23:15 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Bertugli", "Alessia", ""], ["Calderara", "Simone", ""], ["Coscia", "Pasquale", ""], ["Ballan", "Lamberto", ""], ["Cucchiara", "Rita", ""]]}, {"id": "2005.08337", "submitter": "Jag Mohan Singh", "authors": "Jag Mohan Singh, Ahmed Madhun, Guoqiang Li, Raghavendra Ramachandra", "title": "A Survey on Unknown Presentation Attack Detection for Fingerprint", "comments": "Submitted to 3rd International Conference on Intelligent Technologies\n  and Applications INTAP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint recognition systems are widely deployed in various real-life\napplications as they have achieved high accuracy. The widely used applications\ninclude border control, automated teller machine (ATM), and attendance\nmonitoring systems. However, these critical systems are prone to spoofing\nattacks (a.k.a presentation attacks (PA)). PA for fingerprint can be performed\nby presenting gummy fingers made from different materials such as silicone,\ngelatine, play-doh, ecoflex, 2D printed paper, 3D printed material, or latex.\nBiometrics Researchers have developed Presentation Attack Detection (PAD)\nmethods as a countermeasure to PA. PAD is usually done by training a machine\nlearning classifier for known attacks for a given dataset, and they achieve\nhigh accuracy in this task. However, generalizing to unknown attacks is an\nessential problem from applicability to real-world systems, mainly because\nattacks cannot be exhaustively listed in advance. In this survey paper, we\npresent a comprehensive survey on existing PAD algorithms for fingerprint\nrecognition systems, specifically from the standpoint of detecting unknown PAD.\nWe categorize PAD algorithms, point out their advantages/disadvantages, and\nfuture directions for this area.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 18:46:23 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Singh", "Jag Mohan", ""], ["Madhun", "Ahmed", ""], ["Li", "Guoqiang", ""], ["Ramachandra", "Raghavendra", ""]]}, {"id": "2005.08339", "submitter": "Shaun Canavan", "authors": "Sk Rahatul Jannat, Diego Fabiano, Shaun Canavan, and Tempestt Neal", "title": "Subject Identification Across Large Expression Variations Using 3D\n  Facial Landmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landmark localization is an important first step towards geometric based\nvision research including subject identification. Considering this, we propose\nto use 3D facial landmarks for the task of subject identification, over a range\nof expressed emotion. Landmarks are detected, using a Temporal Deformable Shape\nModel and used to train a Support Vector Machine (SVM), Random Forest (RF), and\nLong Short-term Memory (LSTM) neural network for subject identification. As we\nare interested in subject identification with large variations in expression,\nwe conducted experiments on 3 emotion-based databases, namely the BU-4DFE,\nBP4D, and BP4D+ 3D/4D face databases. We show that our proposed method\noutperforms current state of the art methods for subject identification on\nBU-4DFE and BP4D. To the best of our knowledge, this is the first work to\ninvestigate subject identification on the BP4D+, resulting in a baseline for\nthe community.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 18:56:04 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Jannat", "Sk Rahatul", ""], ["Fabiano", "Diego", ""], ["Canavan", "Shaun", ""], ["Neal", "Tempestt", ""]]}, {"id": "2005.08341", "submitter": "Shaun Canavan", "authors": "Diego Fabiano, Manikandan Jaishanker, and Shaun Canavan", "title": "Impact of multiple modalities on emotion recognition: investigation into\n  3d facial landmarks, action units, and physiological data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To fully understand the complexities of human emotion, the integration of\nmultiple physical features from different modalities can be advantageous.\nConsidering this, we present an analysis of 3D facial data, action units, and\nphysiological data as it relates to their impact on emotion recognition. We\nanalyze each modality independently, as well as the fusion of each for\nrecognizing human emotion. This analysis includes which features are most\nimportant for specific emotions (e.g. happy). Our analysis indicates that both\n3D facial landmarks and physiological data are encouraging for\nexpression/emotion recognition. On the other hand, while action units can\npositively impact emotion recognition when fused with other modalities, the\nresults suggest it is difficult to detect emotion using them in a unimodal\nfashion.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 18:59:57 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Fabiano", "Diego", ""], ["Jaishanker", "Manikandan", ""], ["Canavan", "Shaun", ""]]}, {"id": "2005.08343", "submitter": "Shaun Canavan", "authors": "Saurabh Hinduja and Shaun Canavan", "title": "Facial Action Unit Detection using 3D Facial Landmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to detect facial action units (AU) using 3D facial\nlandmarks. Specifically, we train a 2D convolutional neural network (CNN) on 3D\nfacial landmarks, tracked using a shape index-based statistical shape model,\nfor binary and multi-class AU detection. We show that the proposed approach is\nable to accurately model AU occurrences, as the movement of the facial\nlandmarks corresponds directly to the movement of the AUs. By training a CNN on\n3D landmarks, we can achieve accurate AU detection on two state-of-the-art\nemotion datasets, namely BP4D and BP4D+. Using the proposed method, we detect\nmultiple AUs on over 330,000 frames, reporting improved results over\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 19:02:10 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Hinduja", "Saurabh", ""], ["Canavan", "Shaun", ""]]}, {"id": "2005.08344", "submitter": "Shaun Canavan", "authors": "Neilesh Sambhu and Shaun Canavan", "title": "Detecting Forged Facial Videos using convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to detect forged videos, of faces, in online\nvideos. To facilitate this detection, we propose to use smaller (fewer\nparameters to learn) convolutional neural networks (CNN), for a data-driven\napproach to forged video detection. To validate our approach, we investigate\nthe FaceForensics public dataset detailing both frame-based and video-based\nresults. The proposed method is shown to outperform current state of the art.\nWe also perform an ablation study, analyzing the impact of batch size, number\nof filters, and number of network layers on the accuracy of detecting forged\nvideos.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 19:04:59 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Sambhu", "Neilesh", ""], ["Canavan", "Shaun", ""]]}, {"id": "2005.08399", "submitter": "Muhammet Bastan", "authors": "Muhammet Bastan, Arnau Ramisa, Mehmet Tek", "title": "T-VSE: Transformer-Based Visual Semantic Embedding", "comments": "To appear: CVPR 2020 Workshop on Computer Vision for Fashion, Art and\n  Design (CVFAD 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer models have recently achieved impressive performance on NLP\ntasks, owing to new algorithms for self-supervised pre-training on very large\ntext corpora. In contrast, recent literature suggests that simple average word\nmodels outperform more complicated language models, e.g., RNNs and\nTransformers, on cross-modal image/text search tasks on standard benchmarks,\nlike MS COCO. In this paper, we show that dataset scale and training strategy\nare critical and demonstrate that transformer-based cross-modal embeddings\noutperform word average and RNN-based embeddings by a large margin, when\ntrained on a large dataset of e-commerce product image-title pairs.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 23:40:33 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Bastan", "Muhammet", ""], ["Ramisa", "Arnau", ""], ["Tek", "Mehmet", ""]]}, {"id": "2005.08424", "submitter": "Fabio Pinhelli", "authors": "Fabio Pinhelli, Alceu S. Britto Jr, Luiz S. Oliveira, Yandre M. G.\n  Costa, Diego Bertolini", "title": "Single-sample writers -- \"Document Filter\" and their impacts on writer\n  identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The writing can be used as an important biometric modality which allows to\nunequivocally identify an individual. It happens because the writing of two\ndifferent persons present differences that can be explored both in terms of\ngraphometric properties or even by addressing the manuscript as a digital\nimage, taking into account the use of image processing techniques that can\nproperly capture different visual attributes of the image (e.g. texture). In\nthis work, perform a detailed study in which we dissect whether or not the use\nof a database with only a single sample taken from some writers may skew the\nresults obtained in the experimental protocol. In this sense, we propose here\nwhat we call \"document filter\". The \"document filter\" protocol is supposed to\nbe used as a preprocessing technique, such a way that all the data taken from\nfragments of the same document must be placed either into the training or into\nthe test set. The rationale behind it, is that the classifier must capture the\nfeatures from the writer itself, and not features regarding other\nparticularities which could affect the writing in a specific document (i.e.\nemotional state of the writer, pen used, paper type, and etc.). By analyzing\nthe literature, one can find several works dealing the writer identification\nproblem. However, the performance of the writer identification systems must be\nevaluated also taking into account the occurrence of writer volunteers who\ncontributed with a single sample during the creation of the manuscript\ndatabases. To address the open issue investigated here, a comprehensive set of\nexperiments was performed on the IAM, BFL and CVL databases. They have shown\nthat, in the most extreme case, the recognition rate obtained using the\n\"document filter\" protocol drops from 81.80% to 50.37%.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 02:02:31 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Pinhelli", "Fabio", ""], ["Britto", "Alceu S.", "Jr"], ["Oliveira", "Luiz S.", ""], ["Costa", "Yandre M. G.", ""], ["Bertolini", "Diego", ""]]}, {"id": "2005.08431", "submitter": "Gengyan Zhao", "authors": "Gengyan Zhao, Gyujoon Hwang, Cole J. Cook, Fang Liu, Mary E. Meyerand\n  and Rasmus M. Birn", "title": "Deep Learning and Bayesian Deep Learning Based Gender Prediction in\n  Multi-Scale Brain Functional Connectivity", "comments": "40 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain gender differences have been known for a long time and are the possible\nreason for many psychological, psychiatric and behavioral differences between\nmales and females. Predicting genders from brain functional connectivity (FC)\ncan build the relationship between brain activities and gender, and extracting\nimportant gender related FC features from the prediction model offers a way to\ninvestigate the brain gender difference. Current predictive models applied to\ngender prediction demonstrate good accuracies, but usually extract individual\nfunctional connections instead of connectivity patterns in the whole\nconnectivity matrix as features. In addition, current models often omit the\neffect of the input brain FC scale on prediction and cannot give any model\nuncertainty information. Hence, in this study we propose to predict gender from\nmultiple scales of brain FC with deep learning, which can extract full FC\npatterns as features. We further develop the understanding of the feature\nextraction mechanism in deep neural network (DNN) and propose a DNN feature\nranking method to extract the highly important features based on their\ncontributions to the prediction. Moreover, we apply Bayesian deep learning to\nthe brain FC gender prediction, which as a probabilistic model can not only\nmake accurate predictions but also generate model uncertainty for each\nprediction. Experiments were done on the high-quality Human Connectome Project\nS1200 release dataset comprising the resting state functional MRI data of 1003\nhealthy adults. First, DNN reaches 83.0%, 87.6%, 92.0%, 93.5% and 94.1%\naccuracies respectively with the FC input derived from 25, 50, 100, 200, 300\nindependent component analysis (ICA) components. DNN outperforms the\nconventional machine learning methods on the 25-ICA-component scale FC, but the\nlinear machine learning method catches up as the number of ICA components\nincreases...\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 02:43:26 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zhao", "Gengyan", ""], ["Hwang", "Gyujoon", ""], ["Cook", "Cole J.", ""], ["Liu", "Fang", ""], ["Meyerand", "Mary E.", ""], ["Birn", "Rasmus M.", ""]]}, {"id": "2005.08448", "submitter": "Shuang Xu", "authors": "Shuang Xu, Zixiang Zhao, Yicheng Wang, Chunxia Zhang, Junmin Liu,\n  Jiangshe Zhang", "title": "Deep Convolutional Sparse Coding Networks for Image Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image fusion is a significant problem in many fields including digital\nphotography, computational imaging and remote sensing, to name but a few.\nRecently, deep learning has emerged as an important tool for image fusion. This\npaper presents three deep convolutional sparse coding (CSC) networks for three\nkinds of image fusion tasks (i.e., infrared and visible image fusion,\nmulti-exposure image fusion, and multi-modal image fusion). The CSC model and\nthe iterative shrinkage and thresholding algorithm are generalized into\ndictionary convolution units. As a result, all hyper-parameters are learned\nfrom data. Our extensive experiments and comprehensive comparisons reveal the\nsuperiority of the proposed networks with regard to quantitative evaluation and\nvisual inspection.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 04:12:01 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Xu", "Shuang", ""], ["Zhao", "Zixiang", ""], ["Wang", "Yicheng", ""], ["Zhang", "Chunxia", ""], ["Liu", "Junmin", ""], ["Zhang", "Jiangshe", ""]]}, {"id": "2005.08449", "submitter": "Di Hu", "authors": "Di Hu, Xuhong Li, Lichao Mou, Pu Jin, Dong Chen, Liping Jing,\n  Xiaoxiang Zhu, Dejing Dou", "title": "Cross-Task Transfer for Geotagged Audiovisual Aerial Scene Recognition", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial scene recognition is a fundamental task in remote sensing and has\nrecently received increased interest. While the visual information from\noverhead images with powerful models and efficient algorithms yields\nconsiderable performance on scene recognition, it still suffers from the\nvariation of ground objects, lighting conditions etc. Inspired by the\nmulti-channel perception theory in cognition science, in this paper, for\nimproving the performance on the aerial scene recognition, we explore a novel\naudiovisual aerial scene recognition task using both images and sounds as\ninput. Based on an observation that some specific sound events are more likely\nto be heard at a given geographic location, we propose to exploit the knowledge\nfrom the sound events to improve the performance on the aerial scene\nrecognition. For this purpose, we have constructed a new dataset named AuDio\nVisual Aerial sceNe reCognition datasEt (ADVANCE). With the help of this\ndataset, we evaluate three proposed approaches for transferring the sound event\nknowledge to the aerial scene recognition task in a multimodal learning\nframework, and show the benefit of exploiting the audio information for the\naerial scene recognition. The source code is publicly available for\nreproducibility purposes.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 04:14:16 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 03:33:17 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Hu", "Di", ""], ["Li", "Xuhong", ""], ["Mou", "Lichao", ""], ["Jin", "Pu", ""], ["Chen", "Dong", ""], ["Jing", "Liping", ""], ["Zhu", "Xiaoxiang", ""], ["Dou", "Dejing", ""]]}, {"id": "2005.08455", "submitter": "Xingyuan Bu", "authors": "Junran Peng, Xingyuan Bu, Ming Sun, Zhaoxiang Zhang, Tieniu Tan,\n  Junjie Yan", "title": "Large-Scale Object Detection in the Wild from Imbalanced Multi-Labels", "comments": "CVPR2020 oral. The first two authors contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training with more data has always been the most stable and effective way of\nimproving performance in deep learning era. As the largest object detection\ndataset so far, Open Images brings great opportunities and challenges for\nobject detection in general and sophisticated scenarios. However, owing to its\nsemi-automatic collecting and labeling pipeline to deal with the huge data\nscale, Open Images dataset suffers from label-related problems that objects may\nexplicitly or implicitly have multiple labels and the label distribution is\nextremely imbalanced. In this work, we quantitatively analyze these label\nproblems and provide a simple but effective solution. We design a concurrent\nsoftmax to handle the multi-label problems in object detection and propose a\nsoft-sampling methods with hybrid training scheduler to deal with the label\nimbalance. Overall, our method yields a dramatic improvement of 3.34 points,\nleading to the best single model with 60.90 mAP on the public object detection\ntest set of Open Images. And our ensembling result achieves 67.17 mAP, which is\n4.29 points higher than the best result of Open Images public test 2018.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 04:36:36 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Peng", "Junran", ""], ["Bu", "Xingyuan", ""], ["Sun", "Ming", ""], ["Zhang", "Zhaoxiang", ""], ["Tan", "Tieniu", ""], ["Yan", "Junjie", ""]]}, {"id": "2005.08460", "submitter": "Gengyan Zhao", "authors": "Gengyan Zhao, Fang Liu, Jonathan A. Oler, Mary E. Meyerand, Ned H.\n  Kalin and Rasmus M. Birn", "title": "Bayesian convolutional neural network based MRI brain extraction on\n  nonhuman primates", "comments": "37 pages, 14 figures", "journal-ref": "Neuroimage 175 (2018): 32-44", "doi": "10.1016/j.neuroimage.2018.03.065", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain extraction or skull stripping of magnetic resonance images (MRI) is an\nessential step in neuroimaging studies, the accuracy of which can severely\naffect subsequent image processing procedures. Current automatic brain\nextraction methods demonstrate good results on human brains, but are often far\nfrom satisfactory on nonhuman primates, which are a necessary part of\nneuroscience research. To overcome the challenges of brain extraction in\nnonhuman primates, we propose a fully-automated brain extraction pipeline\ncombining deep Bayesian convolutional neural network (CNN) and fully connected\nthree-dimensional (3D) conditional random field (CRF). The deep Bayesian CNN,\nBayesian SegNet, is used as the core segmentation engine. As a probabilistic\nnetwork, it is not only able to perform accurate high-resolution pixel-wise\nbrain segmentation, but also capable of measuring the model uncertainty by\nMonte Carlo sampling with dropout in the testing stage. Then, fully connected\n3D CRF is used to refine the probability result from Bayesian SegNet in the\nwhole 3D context of the brain volume. The proposed method was evaluated with a\nmanually brain-extracted dataset comprising T1w images of 100 nonhuman\nprimates. Our method outperforms six popular publicly available brain\nextraction packages and three well-established deep learning based methods with\na mean Dice coefficient of 0.985 and a mean average symmetric surface distance\nof 0.220 mm. A better performance against all the compared methods was verified\nby statistical tests (all p-values<10-4, two-sided, Bonferroni corrected). The\nmaximum uncertainty of the model on nonhuman primate brain extraction has a\nmean value of 0.116 across all the 100 subjects...\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 05:08:30 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zhao", "Gengyan", ""], ["Liu", "Fang", ""], ["Oler", "Jonathan A.", ""], ["Meyerand", "Mary E.", ""], ["Kalin", "Ned H.", ""], ["Birn", "Rasmus M.", ""]]}, {"id": "2005.08463", "submitter": "Yuhong Guo", "authors": "Bingyu Liu, Zhen Zhao, Zhenpeng Li, Jianan Jiang, Yuhong Guo, Jieping\n  Ye", "title": "Feature Transformation Ensemble Model with Batch Spectral Regularization\n  for Cross-Domain Few-Shot Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a feature transformation ensemble model with batch\nspectral regularization for the Cross-domain few-shot learning (CD-FSL)\nchallenge. Specifically, we proposes to construct an ensemble prediction model\nby performing diverse feature transformations after a feature extraction\nnetwork. On each branch prediction network of the model we use a batch spectral\nregularization term to suppress the singular values of the feature matrix\nduring pre-training to improve the generalization ability of the model. The\nproposed model can then be fine tuned in the target domain to address few-shot\nclassification. We also further apply label propagation, entropy minimization\nand data augmentation to mitigate the shortage of labeled data in target\ndomains. Experiments are conducted on a number of CD-FSL benchmark tasks with\nfour target domains and the results demonstrate the superiority of our proposed\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 05:31:04 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 12:53:18 GMT"}, {"version": "v3", "created": "Thu, 21 May 2020 02:44:03 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Liu", "Bingyu", ""], ["Zhao", "Zhen", ""], ["Li", "Zhenpeng", ""], ["Jiang", "Jianan", ""], ["Guo", "Yuhong", ""], ["Ye", "Jieping", ""]]}, {"id": "2005.08465", "submitter": "Huaidong Zhang", "authors": "Huaidong Zhang, Xuemiao Xu, Guoqiang Han, and Shengfeng He", "title": "Context-aware and Scale-insensitive Temporal Repetition Counting", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal repetition counting aims to estimate the number of cycles of a given\nrepetitive action. Existing deep learning methods assume repetitive actions are\nperformed in a fixed time-scale, which is invalid for the complex repetitive\nactions in real life. In this paper, we tailor a context-aware and\nscale-insensitive framework, to tackle the challenges in repetition counting\ncaused by the unknown and diverse cycle-lengths. Our approach combines two key\ninsights: (1) Cycle lengths from different actions are unpredictable that\nrequire large-scale searching, but, once a coarse cycle length is determined,\nthe variety between repetitions can be overcome by regression. (2) Determining\nthe cycle length cannot only rely on a short fragment of video but a contextual\nunderstanding. The first point is implemented by a coarse-to-fine cycle\nrefinement method. It avoids the heavy computation of exhaustively searching\nall the cycle lengths in the video, and, instead, it propagates the coarse\nprediction for further refinement in a hierarchical manner. We secondly propose\na bidirectional cycle length estimation method for a context-aware prediction.\nIt is a regression network that takes two consecutive coarse cycles as input,\nand predicts the locations of the previous and next repetitive cycles. To\nbenefit the training and evaluation of temporal repetition counting area, we\nconstruct a new and largest benchmark, which contains 526 videos with diverse\nrepetitive actions. Extensive experiments show that the proposed network\ntrained on a single dataset outperforms state-of-the-art methods on several\nbenchmarks, indicating that the proposed framework is general enough to capture\nrepetition patterns across domains.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 05:49:48 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zhang", "Huaidong", ""], ["Xu", "Xuemiao", ""], ["Han", "Guoqiang", ""], ["He", "Shengfeng", ""]]}, {"id": "2005.08501", "submitter": "Cheng Gong", "authors": "Cheng Gong, Yao Chen, Ye Lu, Tao Li, Cong Hao, Deming Chen", "title": "VecQ: Minimal Loss DNN Model Compression With Vectorized Weight\n  Quantization", "comments": "14 pages, 9 figures, Journal", "journal-ref": null, "doi": "10.1109/TC.2020.2995593", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization has been proven to be an effective method for reducing the\ncomputing and/or storage cost of DNNs. However, the trade-off between the\nquantization bitwidth and final accuracy is complex and non-convex, which makes\nit difficult to be optimized directly. Minimizing direct quantization loss\n(DQL) of the coefficient data is an effective local optimization method, but\nprevious works often neglect the accurate control of the DQL, resulting in a\nhigher loss of the final DNN model accuracy. In this paper, we propose a novel\nmetric called Vector Loss. Based on this new metric, we develop a new\nquantization solution called VecQ, which can guarantee minimal direct\nquantization loss and better model accuracy. In addition, in order to speed up\nthe proposed quantization process during model training, we accelerate the\nquantization process with a parameterized probability estimation method and\ntemplate-based derivation calculation. We evaluate our proposed algorithm on\nMNIST, CIFAR, ImageNet, IMDB movie review and THUCNews text data sets with\nnumerical DNN models. The results demonstrate that our proposed quantization\nsolution is more accurate and effective than the state-of-the-art approaches\nyet with more flexible bitwidth support. Moreover, the evaluation of our\nquantized models on Saliency Object Detection (SOD) tasks maintains comparable\nfeature extraction quality with up to 16$\\times$ weight size reduction.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 07:38:44 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 07:09:15 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Gong", "Cheng", ""], ["Chen", "Yao", ""], ["Lu", "Ye", ""], ["Li", "Tao", ""], ["Hao", "Cong", ""], ["Chen", "Deming", ""]]}, {"id": "2005.08514", "submitter": "Xiao Ma", "authors": "Cunjun Yu, Xiao Ma, Jiawei Ren, Haiyu Zhao, Shuai Yi", "title": "Spatio-Temporal Graph Transformer Networks for Pedestrian Trajectory\n  Prediction", "comments": "ECCV camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding crowd motion dynamics is critical to real-world applications,\ne.g., surveillance systems and autonomous driving. This is challenging because\nit requires effectively modeling the socially aware crowd spatial interaction\nand complex temporal dependencies. We believe attention is the most important\nfactor for trajectory prediction. In this paper, we present STAR, a\nSpatio-Temporal grAph tRansformer framework, which tackles trajectory\nprediction by only attention mechanisms. STAR models intra-graph crowd\ninteraction by TGConv, a novel Transformer-based graph convolution mechanism.\nThe inter-graph temporal dependencies are modeled by separate temporal\nTransformers. STAR captures complex spatio-temporal interactions by\ninterleaving between spatial and temporal Transformers. To calibrate the\ntemporal prediction for the long-lasting effect of disappeared pedestrians, we\nintroduce a read-writable external memory module, consistently being updated by\nthe temporal Transformer. We show that with only attention mechanism, STAR\nachieves state-of-the-art performance on 5 commonly used real-world pedestrian\nprediction datasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 08:08:09 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 03:32:07 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Yu", "Cunjun", ""], ["Ma", "Xiao", ""], ["Ren", "Jiawei", ""], ["Zhao", "Haiyu", ""], ["Yi", "Shuai", ""]]}, {"id": "2005.08551", "submitter": "Ping Liu", "authors": "Ping Liu, Yunchao Wei, Zibo Meng, Weihong Deng, Joey Tianyi Zhou, Yi\n  Yang", "title": "Omni-supervised Facial Expression Recognition: A Simple Baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we target on advancing the performance in facial expression\nrecognition (FER) by exploiting omni-supervised learning. The current state of\nthe art FER approaches usually aim to recognize facial expressions in a\ncontrolled environment by training models with a limited number of samples. To\nenhance the robustness of the learned models for various scenarios, we propose\nto perform omni-supervised learning by exploiting the labeled samples together\nwith a large number of unlabeled data. Particularly, we first employ\nMS-Celeb-1M as the facial-pool where around 5,822K unlabeled facial images are\nincluded. Then, a primitive model learned on a small number of labeled samples\nis adopted to select samples with high confidence from the facial-pool by\nconducting feature-based similarity comparison. We find the new dataset\nconstructed in such an omni-supervised manner can significantly improve the\ngeneralization ability of the learned FER model and boost the performance\nconsequently. However, as more training samples are used, more computation\nresources and training time are required, which is usually not affordable in\nmany circumstances. To relieve the requirement of computational resources, we\nfurther adopt a dataset distillation strategy to distill the target\ntask-related knowledge from the new mined samples and compressed them into a\nvery small set of images. This distilled dataset is capable of boosting the\nperformance of FER with few additional computational cost introduced. We\nperform extensive experiments on five popular benchmarks and a newly\nconstructed dataset, where consistent gains can be achieved under various\nsettings using the proposed framework. We hope this work will serve as a solid\nbaseline and help ease future research in FER.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 09:36:51 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 13:07:39 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Liu", "Ping", ""], ["Wei", "Yunchao", ""], ["Meng", "Zibo", ""], ["Deng", "Weihong", ""], ["Zhou", "Joey Tianyi", ""], ["Yang", "Yi", ""]]}, {"id": "2005.08562", "submitter": "Josue Page", "authors": "Josue Page, Paolo Favaro", "title": "Learning to Model and Calibrate Optics via a Differentiable Wave Optics\n  Simulator", "comments": "6 pages, 3 figures, for source code see\n  https://github.com/pvjosue/WaveBlocks, to be published in IEEE 2020\n  International Conference on Image Processing (ICIP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel learning-based method to build a differentiable\ncomputational model of a real fluorescence microscope. Our model can be used to\ncalibrate a real optical setup directly from data samples and to engineer point\nspread functions by specifying the desired input-output data. This approach is\npoised to drastically improve the design of microscopes, because the parameters\nof current models of optical setups cannot be easily fit to real data. Inspired\nby the recent progress in deep learning, our solution is to build a\ndifferentiable wave optics simulator as a composition of trainable modules,\neach computing light wave-front (WF) propagation due to a specific optical\nelement. We call our differentiable modules WaveBlocks and show reconstruction\nresults in the case of lenses, wave propagation in air, camera sensors and\ndiffractive elements (e.g., phase-masks).\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 10:23:04 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Page", "Josue", ""], ["Favaro", "Paolo", ""]]}, {"id": "2005.08605", "submitter": "Yuhuang Hu", "authors": "Yuhuang Hu, Jonathan Binas, Daniel Neil, Shih-Chii Liu and Tobi\n  Delbruck", "title": "DDD20 End-to-End Event Camera Driving Dataset: Fusing Frames and Events\n  with Deep Learning for Improved Steering Prediction", "comments": "Accepted in The 23rd IEEE International Conference on Intelligent\n  Transportation Systems (Special Session: Beyond Traditional Sensing for\n  Intelligent Transportation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic event cameras are useful for dynamic vision problems under\ndifficult lighting conditions. To enable studies of using event cameras in\nautomobile driving applications, this paper reports a new end-to-end driving\ndataset called DDD20. The dataset was captured with a DAVIS camera that\nconcurrently streams both dynamic vision sensor (DVS) brightness change events\nand active pixel sensor (APS) intensity frames. DDD20 is the longest event\ncamera end-to-end driving dataset to date with 51h of DAVIS event+frame camera\nand vehicle human control data collected from 4000km of highway and urban\ndriving under a variety of lighting conditions. Using DDD20, we report the\nfirst study of fusing brightness change events and intensity frame data using a\ndeep learning approach to predict the instantaneous human steering wheel angle.\nOver all day and night conditions, the explained variance for human steering\nprediction from a Resnet-32 is significantly better from the fused DVS+APS\nframes (0.88) than using either DVS (0.67) or APS (0.77) data alone.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 11:39:38 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Hu", "Yuhuang", ""], ["Binas", "Jonathan", ""], ["Neil", "Daniel", ""], ["Liu", "Shih-Chii", ""], ["Delbruck", "Tobi", ""]]}, {"id": "2005.08606", "submitter": "You Jin Kim", "authors": "You Jin Kim, Hee Soo Heo, Soo-Whan Chung and Bong-Jin Lee", "title": "End-to-End Lip Synchronisation Based on Pattern Classification", "comments": "slt 2021 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to synchronise audio and video of a talking face\nusing deep neural network models. Existing works have trained networks on proxy\ntasks such as cross-modal similarity learning, and then computed similarities\nbetween audio and video frames using a sliding window approach. While these\nmethods demonstrate satisfactory performance, the networks are not trained\ndirectly on the task. To this end, we propose an end-to-end trained network\nthat can directly predict the offset between an audio stream and the\ncorresponding video stream. The similarity matrix between the two modalities is\nfirst computed from the features, then the inference of the offset can be\nconsidered to be a pattern recognition problem where the matrix is considered\nequivalent to an image. The feature extractor and the classifier are trained\njointly. We demonstrate that the proposed approach outperforms the previous\nwork by a large margin on LRS2 and LRS3 datasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 11:42:32 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 06:55:05 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Kim", "You Jin", ""], ["Heo", "Hee Soo", ""], ["Chung", "Soo-Whan", ""], ["Lee", "Bong-Jin", ""]]}, {"id": "2005.08607", "submitter": "Dmitry Senushkin", "authors": "Dmitry Senushkin, Mikhail Romanov, Ilia Belikov, Anton Konushin,\n  Nikolay Patakin", "title": "Decoder Modulation for Indoor Depth Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth completion recovers a dense depth map from sensor measurements. Current\nmethods are mostly tailored for very sparse depth measurements from LiDARs in\noutdoor settings, while for indoor scenes Time-of-Flight (ToF) or structured\nlight sensors are mostly used. These sensors provide semi-dense maps, with\ndense measurements in some regions and almost empty in others. We propose a new\nmodel that takes into account the statistical difference between such regions.\nOur main contribution is a new decoder modulation branch added to the\nencoder-decoder architecture. The encoder extracts features from the\nconcatenated RGB image and raw depth. Given the mask of missing values as\ninput, the proposed modulation branch controls the decoding of a dense depth\nmap from these features differently for different regions. This is implemented\nby modifying the spatial distribution of output signals inside the decoder via\nSpatially-Adaptive Denormalization (SPADE) blocks. Our second contribution is a\nnovel training strategy that allows us to train on a semi-dense sensor data\nwhen the ground truth depth map is not available. Our model achieves the state\nof the art results on indoor Matterport3D dataset. Being designed for\nsemi-dense input depth, our model is still competitive with LiDAR-oriented\napproaches on the KITTI dataset. Our training strategy significantly improves\nprediction quality with no dense ground truth available, as validated on the\nNYUv2 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 11:42:42 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 08:20:51 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Senushkin", "Dmitry", ""], ["Romanov", "Mikhail", ""], ["Belikov", "Ilia", ""], ["Konushin", "Anton", ""], ["Patakin", "Nikolay", ""]]}, {"id": "2005.08622", "submitter": "Riccardo La Grassa", "authors": "Riccardo La Grassa, Ignazio Gallo, Nicola Landro", "title": "Learn Class Hierarchy using Convolutional Neural Networks", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A large amount of research on Convolutional Neural Networks has focused on\nflat Classification in the multi-class domain. In the real world, many problems\nare naturally expressed as problems of hierarchical classification, in which\nthe classes to be predicted are organized in a hierarchy of classes. In this\npaper, we propose a new architecture for hierarchical classification of images,\nintroducing a stack of deep linear layers with cross-entropy loss functions and\ncenter loss combined. The proposed architecture can extend any neural network\nmodel and simultaneously optimizes loss functions to discover local\nhierarchical class relationships and a loss function to discover global\ninformation from the whole class hierarchy while penalizing class hierarchy\nviolations. We experimentally show that our hierarchical classifier presents\nadvantages to the traditional classification approaches finding application in\ncomputer vision tasks.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 12:06:43 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["La Grassa", "Riccardo", ""], ["Gallo", "Ignazio", ""], ["Landro", "Nicola", ""]]}, {"id": "2005.08625", "submitter": "Na Li", "authors": "Na Li, Xinbo Zhao, Chong Ma", "title": "JointsGait:A model-based Gait Recognition Method based on Gait Graph\n  Convolutional Networks and Joints Relationship Pyramid Mapping", "comments": "The paper format was changed and experiments on other databases were\n  added. The format and page layout were changed greatly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait, as one of unique biometric features, has the advantage of being\nrecognized from a long distance away, can be widely used in public security.\nConsidering 3D pose estimation is more challenging than 2D pose estimation in\npractice , we research on using 2D joints to recognize gait in this paper, and\na new model-based gait recognition method JointsGait is put forward to extract\ngait information from 2D human body joints. Appearance-based gait recognition\nalgorithms are prevalent before. However, appearance features suffer from\nexternal factors which can cause drastic appearance variations, e.g. clothing.\nUnlike previous approaches, JointsGait firstly extracted spatio-temporal\nfeatures from 2D joints using gait graph convolutional networks, which are less\ninterfered by external factors. Secondly, Joints Relationship Pyramid Mapping\n(JRPM) are proposed to map spatio-temporal gait features into a discriminative\nfeature space with biological advantages according to the relationship of human\njoints when people are walking at various scales. Finally, we design a fusion\nloss strategy to help the joints features to be insensitive to cross-view. Our\nmethod is evaluated on two large datasets, Kinect Gait Biometry Dataset and\nCASIA-B. On Kinect Gait Biometry Dataset database, JointsGait only uses\ncorresponding 2D coordinates of joints, but achieves satisfactory recognition\naccuracy compared with those model-based algorithms using 3D joints. On CASIA-B\ndatabase, the proposed method greatly outperforms advanced model-based methods\nin all walking conditions, even performs superior to state-of-art\nappearance-based methods when clothing seriously affect people's appearance.\nThe experimental results demonstrate that JointsGait achieves the state-of-art\nperformance despite the low dimensional feature (2D body joints) and is less\naffected by the view variations and clothing variation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 08:30:37 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 09:12:03 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Li", "Na", ""], ["Zhao", "Xinbo", ""], ["Ma", "Chong", ""]]}, {"id": "2005.08626", "submitter": "Wenbo Li", "authors": "Wenbo Li, Yaodong Cui, Yintao Ma, Xingxin Chen, Guofa Li, Gang Guo and\n  Dongpu Cao", "title": "A Spontaneous Driver Emotion Facial Expression (DEFE) Dataset for\n  Intelligent Vehicles", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new dataset, the driver emotion facial\nexpression (DEFE) dataset, for driver spontaneous emotions analysis. The\ndataset includes facial expression recordings from 60 participants during\ndriving. After watching a selected video-audio clip to elicit a specific\nemotion, each participant completed the driving tasks in the same driving\nscenario and rated their emotional responses during the driving processes from\nthe aspects of dimensional emotion and discrete emotion. We also conducted\nclassification experiments to recognize the scales of arousal, valence,\ndominance, as well as the emotion category and intensity to establish baseline\nresults for the proposed dataset. Besides, this paper compared and discussed\nthe differences in facial expressions between driving and non-driving\nscenarios. The results show that there were significant differences in AUs\n(Action Units) presence of facial expressions between driving and non-driving\nscenarios, indicating that human emotional expressions in driving scenarios\nwere different from other life scenarios. Therefore, publishing a human emotion\ndataset specifically for the driver is necessary for traffic safety\nimprovement. The proposed dataset will be publicly available so that\nresearchers worldwide can use it to develop and examine their driver emotion\nanalysis methods. To the best of our knowledge, this is currently the only\npublic driver facial expression dataset.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 07:15:50 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Li", "Wenbo", ""], ["Cui", "Yaodong", ""], ["Ma", "Yintao", ""], ["Chen", "Xingxin", ""], ["Li", "Guofa", ""], ["Guo", "Gang", ""], ["Cao", "Dongpu", ""]]}, {"id": "2005.08628", "submitter": "Takato Yasuno", "authors": "Takato Yasuno, Michihiro Nakajima, Tomoharu Sekiguchi, Kazuhiro Noda,\n  Kiyoshi Aoyanagi, Sakura Kato", "title": "Synthetic Image Augmentation for Damage Region Segmentation using\n  Conditional GAN with Structure Edge", "comments": "4 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:2004.10126", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, social infrastructure is aging, and its predictive maintenance has\nbecome important issue. To monitor the state of infrastructures, bridge\ninspection is performed by human eye or bay drone. For diagnosis, primary\ndamage region are recognized for repair targets. But, the degradation at worse\nlevel has rarely occurred, and the damage regions of interest are often narrow,\nso their ratio per image is extremely small pixel count, as experienced 0.6 to\n1.5 percent. The both scarcity and imbalance property on the damage region of\ninterest influences limited performance to detect damage. If additional data\nset of damaged images can be generated, it may enable to improve accuracy in\ndamage region segmentation algorithm. We propose a synthetic augmentation\nprocedure to generate damaged images using the image-to-image translation\nmapping from the tri-categorical label that consists the both semantic label\nand structure edge to the real damage image. We use the Sobel gradient operator\nto enhance structure edge. Actually, in case of bridge inspection, we apply the\nRC concrete structure with the number of 208 eye-inspection photos that rebar\nexposure have occurred, which are prepared 840 block images with size 224 by\n224. We applied popular per-pixel segmentation algorithms such as the FCN-8s,\nSegNet, and DeepLabv3+Xception-v2. We demonstrates that re-training a data set\nadded with synthetic augmentation procedure make higher accuracy based on\nindices the mean IoU, damage region of interest IoU, precision, recall, BF\nscore when we predict test images.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 06:04:02 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Yasuno", "Takato", ""], ["Nakajima", "Michihiro", ""], ["Sekiguchi", "Tomoharu", ""], ["Noda", "Kazuhiro", ""], ["Aoyanagi", "Kiyoshi", ""], ["Kato", "Sakura", ""]]}, {"id": "2005.08629", "submitter": "Milad Sikaroudi", "authors": "Milad Sikaroudi, Amir Safarpoor, Benyamin Ghojogh, Sobhan Shafiei,\n  Mark Crowley, H.R. Tizhoosh", "title": "Supervision and Source Domain Impact on Representation Learning: A\n  Histopathology Case Study", "comments": "Accepted for presentation at the 42nd Annual International Conference\n  of the IEEE Engineering in Medicine and Biology Society (EMBC'20)", "journal-ref": "2020 42nd Annual International Conference of the IEEE Engineering\n  in Medicine & Biology Society (EMBC), pp. 1400-1403", "doi": "10.1109/EMBC44109.2020.9176279", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As many algorithms depend on a suitable representation of data, learning\nunique features is considered a crucial task. Although supervised techniques\nusing deep neural networks have boosted the performance of representation\nlearning, the need for a large set of labeled data limits the application of\nsuch methods. As an example, high-quality delineations of regions of interest\nin the field of pathology is a tedious and time-consuming task due to the large\nimage dimensions. In this work, we explored the performance of a deep neural\nnetwork and triplet loss in the area of representation learning. We\ninvestigated the notion of similarity and dissimilarity in pathology\nwhole-slide images and compared different setups from unsupervised and\nsemi-supervised to supervised learning in our experiments. Additionally,\ndifferent approaches were tested, applying few-shot learning on two publicly\navailable pathology image datasets. We achieved high accuracy and\ngeneralization when the learned representations were applied to two different\npathology datasets.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 21:27:38 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Sikaroudi", "Milad", ""], ["Safarpoor", "Amir", ""], ["Ghojogh", "Benyamin", ""], ["Shafiei", "Sobhan", ""], ["Crowley", "Mark", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "2005.08630", "submitter": "Sungrack Yun", "authors": "Seungwoo Yoo, Heeseok Lee, Heesoo Myeong, Sungrack Yun, Hyoungwoo\n  Park, Janghoon Cho, Duck Hoon Kim", "title": "End-to-End Lane Marker Detection via Row-wise Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In autonomous driving, detecting reliable and accurate lane marker positions\nis a crucial yet challenging task. The conventional approaches for the lane\nmarker detection problem perform a pixel-level dense prediction task followed\nby sophisticated post-processing that is inevitable since lane markers are\ntypically represented by a collection of line segments without thickness. In\nthis paper, we propose a method performing direct lane marker vertex prediction\nin an end-to-end manner, i.e., without any post-processing step that is\nrequired in the pixel-level dense prediction task. Specifically, we translate\nthe lane marker detection problem into a row-wise classification task, which\ntakes advantage of the innate shape of lane markers but, surprisingly, has not\nbeen explored well. In order to compactly extract sufficient information about\nlane markers which spread from the left to the right in an image, we devise a\nnovel layer, which is utilized to successively compress horizontal components\nso enables an end-to-end lane marker detection system where the final lane\nmarker positions are simply obtained via argmax operations in testing time.\nExperimental results demonstrate the effectiveness of the proposed method,\nwhich is on par or outperforms the state-of-the-art methods on two popular lane\nmarker detection benchmarks, i.e., TuSimple and CULane.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 12:48:46 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Yoo", "Seungwoo", ""], ["Lee", "Heeseok", ""], ["Myeong", "Heesoo", ""], ["Yun", "Sungrack", ""], ["Park", "Hyoungwoo", ""], ["Cho", "Janghoon", ""], ["Kim", "Duck Hoon", ""]]}, {"id": "2005.08632", "submitter": "Sandesh Kamath K", "authors": "Sandesh Kamath, Amit Deshpande, K V Subrahmanyam", "title": "Universalization of any adversarial attack using very few test examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning models are known to be vulnerable not only to input-dependent\nadversarial attacks but also to input-agnostic or universal adversarial\nattacks. Dezfooli et al. \\cite{Dezfooli17,Dezfooli17anal} construct universal\nadversarial attack on a given model by looking at a large number of training\ndata points and the geometry of the decision boundary near them. Subsequent\nwork \\cite{Khrulkov18} constructs universal attack by looking only at test\nexamples and intermediate layers of the given model. In this paper, we propose\na simple universalization technique to take any input-dependent adversarial\nattack and construct a universal attack by only looking at very few adversarial\ntest examples. We do not require details of the given model and have negligible\ncomputational overhead for universalization. We theoretically justify our\nuniversalization technique by a spectral property common to many\ninput-dependent adversarial perturbations, e.g., gradients, Fast Gradient Sign\nMethod (FGSM) and DeepFool. Using matrix concentration inequalities and\nspectral perturbation bounds, we show that the top singular vector of\ninput-dependent adversarial directions on a small test sample gives an\neffective and simple universal adversarial attack. For VGG16 and VGG19 models\ntrained on ImageNet, our simple universalization of Gradient, FGSM, and\nDeepFool perturbations using a test sample of 64 images gives fooling rates\ncomparable to state-of-the-art universal attacks \\cite{Dezfooli17,Khrulkov18}\nfor reasonable norms of perturbation.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 12:17:38 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Kamath", "Sandesh", ""], ["Deshpande", "Amit", ""], ["Subrahmanyam", "K V", ""]]}, {"id": "2005.08637", "submitter": "Zhentao Huang", "authors": "Xiangjun Peng, Zhentao Huang, Xu Sun", "title": "Building BROOK: A Multi-modal and Facial Video Database for\n  Human-Vehicle Interaction Research", "comments": "Conference: ACM CHI Conference on Human Factors in Computing Systems\n  Workshops (CHI'20 Workshops)At: Honolulu, Hawaii, USA\n  URL:https://emergentdatatrails.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing popularity of Autonomous Vehicles, more opportunities have\nbloomed in the context of Human-Vehicle Interactions. However, the lack of\ncomprehensive and concrete database support for such specific use case limits\nrelevant studies in the whole design spaces. In this paper, we present our\nwork-in-progress BROOK, a public multi-modal database with facial video\nrecords, which could be used to characterize drivers' affective states and\ndriving styles. We first explain how we over-engineer such database in details,\nand what we have gained through a ten-month study. Then we showcase a Neural\nNetwork-based predictor, leveraging BROOK, which supports multi-modal\nprediction (including physiological data of heart rate and skin conductance and\ndriving status data of speed)through facial videos. Finally, we discuss related\nissues when building such a database and our future directions in the context\nof BROOK. We believe BROOK is an essential building block for future\nHuman-Vehicle Interaction Research.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 12:20:17 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 14:42:30 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Peng", "Xiangjun", ""], ["Huang", "Zhentao", ""], ["Sun", "Xu", ""]]}, {"id": "2005.08641", "submitter": "Lalit Lakshmanan", "authors": "Lalit Lakshmanan, Yash Vora, Raj Ghate", "title": "Deep Learning Based Vehicle Tracking System Using License Plate\n  Detection And Recognition", "comments": "6 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle tracking is an integral part of intelligent traffic management\nsystems. Previous implementations of vehicle tracking used Global Positioning\nSystem(GPS) based systems that gave location of the vehicle of an individual on\ntheir smartphones.The proposed system uses a novel approach to vehicle tracking\nusing Vehicle License plate detection and recognition (VLPR) technique, which\ncan be integrated on a large scale with traffic management systems. Initial\nmethods of implementing VLPR used simple image processing techniques which were\nquite experimental and heuristic. With the onset of Deep learning and Computer\nVision, one can create robust VLPR systems that can produce results close to\nhuman efficiency. Previous implementations, based on deep learning, made use of\nobject detection and support vector machines for detection and a heuristic\nimage processing based approach for recognition. The proposed system makes use\nof scene text detection model architecture for License plate detection and for\nrecognition it uses the Optical character recognition engine (OCR) Tesseract.\nThe proposed system obtained extraordinary results when it was tested on a\nhighway video using NVIDIA Ge-force RTX 2080ti GPU, results were obtained at a\nspeed of 30 frames per second with accuracy close to human.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 14:03:33 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Lakshmanan", "Lalit", ""], ["Vora", "Yash", ""], ["Ghate", "Raj", ""]]}, {"id": "2005.08642", "submitter": "Ritam Guha Mr.", "authors": "Kushal Kanti Ghosh, Ritam Guha, Soulib Ghosh, Suman Kumar Bera, Ram\n  Sarkar", "title": "Atom Search Optimization with Simulated Annealing -- a Hybrid\n  Metaheuristic Approach for Feature Selection", "comments": "39 pages, submitted to Expert Systems with Applications, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  'Hybrid meta-heuristics' is one of the most interesting recent trends in the\nfield of optimization and feature selection (FS). In this paper, we have\nproposed a binary variant of Atom Search Optimization (ASO) and its hybrid with\nSimulated Annealing called ASO-SA techniques for FS. In order to map the real\nvalues used by ASO to the binary domain of FS, we have used two different\ntransfer functions: S-shaped and V-shaped. We have hybridized this technique\nwith a local search technique called, SA We have applied the proposed feature\nselection methods on 25 datasets from 4 different categories: UCI, Handwritten\ndigit recognition, Text, non-text separation, and Facial emotion recognition.\nWe have used 3 different classifiers (K-Nearest Neighbor, Multi-Layer\nPerceptron and Random Forest) for evaluating the strength of the selected\nfeatured by the binary ASO, ASO-SA and compared the results with some recent\nwrapper-based algorithms. The experimental results confirm the superiority of\nthe proposed method both in terms of classification accuracy and number of\nselected features.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 07:56:58 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Ghosh", "Kushal Kanti", ""], ["Guha", "Ritam", ""], ["Ghosh", "Soulib", ""], ["Bera", "Suman Kumar", ""], ["Sarkar", "Ram", ""]]}, {"id": "2005.08644", "submitter": "Utkarsh Srivastava", "authors": "Utkarsh Chandra Srivastava, Dhruv Upadhyay, Vinayak Sharma", "title": "Intracranial Hemorrhage Detection Using Neural Network Based Methods\n  With Federated Learning", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intracranial hemorrhage, bleeding that occurs inside the cranium, is a\nserious health problem requiring rapid and often intensive medical treatment.\nSuch a condition is traditionally diagnosed by highly-trained specialists\nanalyzing computed tomography (CT) scan of the patient and identifying the\nlocation and type of hemorrhage if one exists. We propose a neural network\napproach to find and classify the condition based upon the CT scan. The model\narchitecture implements a time distributed convolutional network. We observed\naccuracy above 92% from such an architecture, provided enough data. We propose\nfurther extensions to our approach involving the deployment of federated\nlearning. This would be helpful in pooling learned parameters without violating\nthe inherent privacy of the data involved.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 05:35:15 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Srivastava", "Utkarsh Chandra", ""], ["Upadhyay", "Dhruv", ""], ["Sharma", "Vinayak", ""]]}, {"id": "2005.08645", "submitter": "Jevgenij Gamper", "authors": "Jevgenij Gamper, Navid Alemi Kooohbanani, Nasir Rajpoot", "title": "Multi-Task Learning in Histo-pathology for Widely Generalizable Model", "comments": null, "journal-ref": "AI4CC ICLR 2020 workshop", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we show preliminary results of deep multi-task learning in the\narea of computational pathology. We combine 11 tasks ranging from patch-wise\noral cancer classification, one of the most prevalent cancers in the developing\nworld, to multi-tissue nuclei instance segmentation and classification.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 12:13:43 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Gamper", "Jevgenij", ""], ["Kooohbanani", "Navid Alemi", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "2005.08646", "submitter": "Peng Gao", "authors": "Shijie Geng, Ji Zhang, Zuohui Fu, Peng Gao, Hang Zhang, Gerard de Melo", "title": "Character Matters: Video Story Understanding with Character-Aware\n  Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from short videos and GIFs, video stories contain clear plots and\nlists of principal characters. Without identifying the connection between\nappearing people and character names, a model is not able to obtain a genuine\nunderstanding of the plots. Video Story Question Answering (VSQA) offers an\neffective way to benchmark higher-level comprehension abilities of a model.\nHowever, current VSQA methods merely extract generic visual features from a\nscene. With such an approach, they remain prone to learning just superficial\ncorrelations. In order to attain a genuine understanding of who did what to\nwhom, we propose a novel model that continuously refines character-aware\nrelations. This model specifically considers the characters in a video story,\nas well as the relations connecting different characters and objects. Based on\nthese signals, our framework enables weakly-supervised face naming through\nmulti-instance co-occurrence matching and supports high-level reasoning\nutilizing Transformer structures. We train and test our model on the six\ndiverse TV shows in the TVQA dataset, which is by far the largest and only\npublicly available dataset for VSQA. We validate our proposed approach over\nTVQA dataset through extensive ablation study.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 06:51:13 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Geng", "Shijie", ""], ["Zhang", "Ji", ""], ["Fu", "Zuohui", ""], ["Gao", "Peng", ""], ["Zhang", "Hang", ""], ["de Melo", "Gerard", ""]]}, {"id": "2005.08648", "submitter": "Sara Moccia", "authors": "Sara Moccia and Lucia Migliorelli and Virgilio Carnielli and Emanuele\n  Frontoni", "title": "Preterm infants' pose estimation with spatio-temporal features", "comments": "IEEE Transactions on Biomedical Engineering (2019)", "journal-ref": null, "doi": "10.1109/TBME.2019.2961448", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Preterm infants' limb monitoring in neonatal intensive care units\n(NICUs) is of primary importance for assessing infants' health status and\nmotor/cognitive development. Herein, we propose a new approach to preterm\ninfants' limb pose estimation that features spatio-temporal information to\ndetect and track limb joints from depth videos with high reliability. Methods:\nLimb-pose estimation is performed using a deep-learning framework consisting of\na detection and a regression convolutional neural network (CNN) for rough and\nprecise joint localization, respectively. The CNNs are implemented to encode\nconnectivity in the temporal direction through 3D convolution. Assessment of\nthe proposed framework is performed through a comprehensive study with sixteen\ndepth videos acquired in the actual clinical practice from sixteen preterm\ninfants (the babyPose dataset). Results: When applied to pose estimation, the\nmedian root mean squared distance, computed among all limbs, between the\nestimated and the ground-truth pose was 9.06 pixels, overcoming approaches\nbased on spatial features only (11.27pixels). Conclusion: Results showed that\nthe spatio-temporal features had a significant influence on the pose-estimation\nperformance, especially in challenging cases (e.g., homogeneous image\nintensity). Significance: This paper significantly enhances the state of art in\nautomatic assessment of preterm infants' health status by introducing the use\nof spatio-temporal features for limb detection and tracking, and by being the\nfirst study to use depth videos acquired in the actual clinical practice for\nlimb-pose estimation. The babyPose dataset has been released as the first\nannotated dataset for infants' pose estimation.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 09:51:22 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Moccia", "Sara", ""], ["Migliorelli", "Lucia", ""], ["Carnielli", "Virgilio", ""], ["Frontoni", "Emanuele", ""]]}, {"id": "2005.08649", "submitter": "Chih-Fan Hsu", "authors": "Chih-Fan Hsu, Chia-Ching Lin, Ting-Yang Hung, Chin-Laung Lei and\n  Kuan-Ta Chen", "title": "A Detailed Look At CNN-based Approaches In Facial Landmark Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial landmark detection has been studied over decades. Numerous neural\nnetwork (NN)-based approaches have been proposed for detecting landmarks,\nespecially the convolutional neural network (CNN)-based approaches. In general,\nCNN-based approaches can be divided into regression and heatmap approaches.\nHowever, no research systematically studies the characteristics of different\napproaches. In this paper, we investigate both CNN-based approaches, generalize\ntheir advantages and disadvantages, and introduce a variation of the heatmap\napproach, a pixel-wise classification (PWC) model. To the best of our\nknowledge, using the PWC model to detect facial landmarks have not been\ncomprehensively studied. We further design a hybrid loss function and a\ndiscrimination network for strengthening the landmarks' interrelationship\nimplied in the PWC model to improve the detection accuracy without modifying\nthe original model architecture. Six common facial landmark datasets, AFW,\nHelen, LFPW, 300-W, IBUG, and COFW are adopted to train or evaluate our model.\nA comprehensive evaluation is conducted and the result shows that the proposed\nmodel outperforms other models in all tested datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 16:17:42 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Hsu", "Chih-Fan", ""], ["Lin", "Chia-Ching", ""], ["Hung", "Ting-Yang", ""], ["Lei", "Chin-Laung", ""], ["Chen", "Kuan-Ta", ""]]}, {"id": "2005.08650", "submitter": "Marek Rychlik", "authors": "Marek Rychlik, and Dwight Nwaigwe and Yan Han and Dylan Murphy", "title": "Development of a New Image-to-text Conversion System for Pashto, Farsi\n  and Traditional Chinese", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report upon the results of a research and prototype building project\n\\emph{Worldly~OCR} dedicated to developing new, more accurate image-to-text\nconversion software for several languages and writing systems. These include\nthe cursive scripts Farsi and Pashto, and Latin cursive scripts. We also\ndescribe approaches geared towards Traditional Chinese, which is non-cursive,\nbut features an extremely large character set of 65,000 characters. Our\nmethodology is based on Machine Learning, especially Deep Learning, and Data\nScience, and is directed towards vast quantities of original documents,\nexceeding a billion pages. The target audience of this paper is a general\naudience with interest in Digital Humanities or in retrieval of accurate\nfull-text and metadata from digital images.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 17:58:48 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Rychlik", "Marek", ""], ["Nwaigwe", "Dwight", ""], ["Han", "Yan", ""], ["Murphy", "Dylan", ""]]}, {"id": "2005.08702", "submitter": "John Brandt", "authors": "John Brandt, Fred Stolle", "title": "A global method to identify trees outside of closed-canopy forests with\n  medium-resolution satellite imagery", "comments": null, "journal-ref": null, "doi": "10.1080/01431161.2020.1841324", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scattered trees outside of dense, closed-canopy forests are very important\nfor carbon sequestration, supporting livelihoods, maintaining ecosystem\nintegrity, and climate change adaptation and mitigation. In contrast to trees\ninside of closed-canopy forests, not much is known about the spatial extent and\ndistribution of scattered trees at a global scale. Due to the cost of\nhigh-resolution satellite imagery, global monitoring systems rely on\nmedium-resolution satellites to monitor land use. Here we present a globally\nconsistent method to identify trees with canopy diameters greater than three\nmeters with medium-resolution optical and radar imagery. Biweekly cloud-free,\npan-sharpened 10 meter Sentinel-2 optical imagery and Sentinel-1 radar imagery\nare used to train a fully convolutional network, consisting of a convolutional\ngated recurrent unit layer and a feature pyramid attention layer. Tested across\nmore than 215,000 Sentinel-1 and Sentinel-2 pixels distributed from -60 to +60\nlatitude, the proposed model exceeds 75% user's and producer's accuracy\nidentifying trees in hectares with a low to medium density (less than 40%) of\ntree cover, and 95% user's and producer's accuracy in hectares with dense\n(greater than 40%) tree cover. The proposed method increases the accuracy of\nmonitoring tree presence in areas with sparse and scattered tree cover (less\nthan 40%) by as much as 20%, and reduces commission and omission error in\nmountainous and very cloudy regions by nearly half. When applied across large,\nheterogeneous landscapes, the results demonstrate potential to map trees in\nhigh detail and accuracy over diverse landscapes across the globe. This\ninformation is important for understanding current land cover and can be used\nto detect changes in land cover such as agroforestry, buffer zones around\nbiological hotspots, and expansion or encroachment of forests.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 15:58:01 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 13:56:50 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Brandt", "John", ""], ["Stolle", "Fred", ""]]}, {"id": "2005.08704", "submitter": "Weipeng Cao", "authors": "Zhongwu Xie, Weipeng Cao, Xizhao Wang, Zhong Ming, Jingjing Zhang,\n  Jiyong Zhang", "title": "A Biologically Inspired Feature Enhancement Framework for Zero-Shot\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the Zero-Shot Learning (ZSL) algorithms currently use pre-trained\nmodels as their feature extractors, which are usually trained on the ImageNet\ndata set by using deep neural networks. The richness of the feature information\nembedded in the pre-trained models can help the ZSL model extract more useful\nfeatures from its limited training samples. However, sometimes the difference\nbetween the training data set of the current ZSL task and the ImageNet data set\nis too large, which may lead to the use of pre-trained models has no obvious\nhelp or even negative impact on the performance of the ZSL model. To solve this\nproblem, this paper proposes a biologically inspired feature enhancement\nframework for ZSL. Specifically, we design a dual-channel learning framework\nthat uses auxiliary data sets to enhance the feature extractor of the ZSL model\nand propose a novel method to guide the selection of the auxiliary data sets\nbased on the knowledge of biological taxonomy. Extensive experimental results\nshow that our proposed method can effectively improve the generalization\nability of the ZSL model and achieve state-of-the-art results on three\nbenchmark ZSL tasks. We also explained the experimental phenomena through the\nway of feature visualization.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 13:25:22 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Xie", "Zhongwu", ""], ["Cao", "Weipeng", ""], ["Wang", "Xizhao", ""], ["Ming", "Zhong", ""], ["Zhang", "Jingjing", ""], ["Zhang", "Jiyong", ""]]}, {"id": "2005.08706", "submitter": "Lu Zhang", "authors": "Lu Zhang, Jian Zhang, Zhibin Li, and Jingsong Xu", "title": "Towards Better Graph Representation: Two-Branch Collaborative Graph\n  Neural Networks for Multimodal Marketing Intention Detection", "comments": "Accepted by ICME2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the fact that spreading and collecting information through the\nInternet becomes the norm, more and more people choose to post for-profit\ncontents (images and texts) in social networks. Due to the difficulty of\nnetwork censors, malicious marketing may be capable of harming society.\nTherefore, it is meaningful to detect marketing intentions online\nautomatically. However, gaps between multimodal data make it difficult to fuse\nimages and texts for content marketing detection. To this end, this paper\nproposes Two-Branch Collaborative Graph Neural Networks to collaboratively\nrepresent multimodal data by Graph Convolution Networks (GCNs) in an end-to-end\nfashion. Experimental results demonstrate that our proposed method achieves\nsuperior graph classification performance for marketing intention detection.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 12:58:01 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 03:31:05 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 06:16:54 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zhang", "Lu", ""], ["Zhang", "Jian", ""], ["Li", "Zhibin", ""], ["Xu", "Jingsong", ""]]}, {"id": "2005.08746", "submitter": "Javier Velasco-Mata", "authors": "Mhd Wesam Al-Nabki, Francisco Ja\\~nez-Martino, Roberto A.\n  Vasco-Carofilis, Eduardo Fidalgo, Javier Velasco-Mata", "title": "Improving Named Entity Recognition in Tor Darknet with Local Distance\n  Neighbor Feature", "comments": "2 pages, 1 figure, to be published in conference JNIC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Name entity recognition in noisy user-generated texts is a difficult task\nusually enhanced by incorporating an external resource of information, such as\ngazetteers. However, gazetteers are task-specific, and they are expensive to\nbuild and maintain. This paper adopts and improves the approach of Aguilar et\nal. by presenting a novel feature, called Local Distance Neighbor, which\nsubstitutes gazetteers. We tested the new approach on the W-NUT-2017 dataset,\nobtaining state-of-the-art results for the Group, Person and Product categories\nof Named Entities. Next, we added 851 manually labeled samples to the\nW-NUT-2017 dataset to account for named entities in the Tor Darknet related to\nweapons and drug selling. Finally, our proposal achieved an entity and surface\nF1 scores of 52.96% and 50.57% on this extended dataset, demonstrating its\nusefulness for Law Enforcement Agencies to detect named entities in the Tor\nhidden services.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 14:21:22 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Al-Nabki", "Mhd Wesam", ""], ["Ja\u00f1ez-Martino", "Francisco", ""], ["Vasco-Carofilis", "Roberto A.", ""], ["Fidalgo", "Eduardo", ""], ["Velasco-Mata", "Javier", ""]]}, {"id": "2005.08752", "submitter": "Junjun Jiang", "authors": "Junjun Jiang, He Sun, Xianming Liu, and Jiayi Ma", "title": "Learning Spatial-Spectral Prior for Super-Resolution of Hyperspectral\n  Imagery", "comments": "Accepted for publication at IEEE Transactions on Computational\n  Imaging", "journal-ref": null, "doi": "10.1109/TCI.2020.2996075", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, single gray/RGB image super-resolution reconstruction task has been\nextensively studied and made significant progress by leveraging the advanced\nmachine learning techniques based on deep convolutional neural networks\n(DCNNs). However, there has been limited technical development focusing on\nsingle hyperspectral image super-resolution due to the high-dimensional and\ncomplex spectral patterns in hyperspectral image. In this paper, we make a step\nforward by investigating how to adapt state-of-the-art residual learning based\nsingle gray/RGB image super-resolution approaches for computationally efficient\nsingle hyperspectral image super-resolution, referred as SSPSR. Specifically,\nwe introduce a spatial-spectral prior network (SSPN) to fully exploit the\nspatial information and the correlation between the spectra of the\nhyperspectral data. Considering that the hyperspectral training samples are\nscarce and the spectral dimension of hyperspectral image data is very high, it\nis nontrivial to train a stable and effective deep network. Therefore, a group\nconvolution (with shared network parameters) and progressive upsampling\nframework is proposed. This will not only alleviate the difficulty in feature\nextraction due to high-dimension of the hyperspectral data, but also make the\ntraining process more stable. To exploit the spatial and spectral prior, we\ndesign a spatial-spectral block (SSB), which consists of a spatial residual\nmodule and a spectral attention residual module. Experimental results on some\nhyperspectral images demonstrate that the proposed SSPSR method enhances the\ndetails of the recovered high-resolution hyperspectral images, and outperforms\nstate-of-the-arts. The source code is available at\n\\url{https://github.com/junjun-jiang/SSPSR\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 14:25:50 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 03:26:38 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Jiang", "Junjun", ""], ["Sun", "He", ""], ["Liu", "Xianming", ""], ["Ma", "Jiayi", ""]]}, {"id": "2005.08766", "submitter": "Javier Velasco-Mata", "authors": "Mhd Wesam Al-Nabki, Eduardo Fidalgo, Roberto A. Vasco-Carofilis,\n  Francisco Ja\\~nez-Martino, Javier Velasco-Mata", "title": "Evaluating Performance of an Adult Pornography Classifier for Child\n  Sexual Abuse Detection", "comments": "4 pages, 8 figures, to be published in conference JNIC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The information technology revolution has facilitated reaching pornographic\nmaterial for everyone, including minors who are the most vulnerable in case\nthey were abused. Accuracy and time performance are features desired by\nforensic tools oriented to child sexual abuse detection, whose main components\nmay rely on image or video classifiers. In this paper, we identify which are\nthe hardware and software requirements that may affect the performance of a\nforensic tool. We evaluated the adult porn classifier proposed by Yahoo, based\non Deep Learning, into two different OS and four Hardware configurations, with\ntwo and four different CPU and GPU, respectively. The classification speed on\nUbuntu Operating System is $~5$ and $~2$ times faster than on Windows 10, when\na CPU and GPU are used, respectively. We demonstrate the superiority of a\nGPU-based machine rather than a CPU-based one, being $7$ to $8$ times faster.\nFinally, we prove that the upward and downward interpolation process conducted\nwhile resizing the input images do not influence the performance of the\nselected prediction model.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 14:32:33 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Al-Nabki", "Mhd Wesam", ""], ["Fidalgo", "Eduardo", ""], ["Vasco-Carofilis", "Roberto A.", ""], ["Ja\u00f1ez-Martino", "Francisco", ""], ["Velasco-Mata", "Javier", ""]]}, {"id": "2005.08772", "submitter": "Elad Hirsch", "authors": "Elad Hirsch, Ayellet Tal", "title": "Color Visual Illusions: A Statistics-based Computational Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual illusions may be explained by the likelihood of patches in real-world\nimages, as argued by input-driven paradigms in Neuro-Science. However, neither\nthe data nor the tools existed in the past to extensively support these\nexplanations. The era of big data opens a new opportunity to study input-driven\napproaches. We introduce a tool that computes the likelihood of patches, given\na large dataset to learn from. Given this tool, we present a model that\nsupports the approach and explains lightness and color visual illusions in a\nunified manner. Furthermore, our model generates visual illusions in natural\nimages, by applying the same tool, reversely.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 14:39:48 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 10:45:03 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Hirsch", "Elad", ""], ["Tal", "Ayellet", ""]]}, {"id": "2005.08806", "submitter": "Xuechen Zhang", "authors": "Yang Chen, Zongqing Lu, Xuechen Zhang, Lei Chen and Qingmin Liao", "title": "Noise-Sampling Cross Entropy Loss: Improving Disparity Regression Via\n  Cost Volume Aware Regularizer", "comments": "Accepted by IEEE ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent end-to-end deep neural networks for disparity regression have achieved\nthe state-of-the-art performance. However, many well-acknowledged specific\nproperties of disparity estimation are omitted in these deep learning\nalgorithms. Especially, matching cost volume, one of the most important\nprocedure, is treated as a normal intermediate feature for the following\nsoftargmin regression, lacking explicit constraints compared with those\ntraditional algorithms. In this paper, inspired by previous canonical\ndefinition of cost volume, we propose the noise-sampling cross entropy loss\nfunction to regularize the cost volume produced by deep neural networks to be\nunimodal and coherent. Extensive experiments validate that the proposed\nnoise-sampling cross entropy loss can not only help neural networks learn more\ninformative cost volume, but also lead to better stereo matching performance\ncompared with several representative algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 15:29:55 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 09:59:11 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Chen", "Yang", ""], ["Lu", "Zongqing", ""], ["Zhang", "Xuechen", ""], ["Chen", "Lei", ""], ["Liao", "Qingmin", ""]]}, {"id": "2005.08812", "submitter": "Jiangning Zhang", "authors": "Jiangning Zhang, Liang Liu, Chao Xu, Yong Liu", "title": "Hierarchical and Efficient Learning for Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works in the person re-identification task mainly focus on the model\naccuracy while ignore factors related to the efficiency, e.g. model size and\nlatency, which are critical for practical application. In this paper, we\npropose a novel Hierarchical and Efficient Network (HENet) that learns\nhierarchical global, partial, and recovery features ensemble under the\nsupervision of multiple loss combinations. To further improve the robustness\nagainst the irregular occlusion, we propose a new dataset augmentation\napproach, dubbed Random Polygon Erasing (RPE), to random erase irregular area\nof the input image for imitating the body part missing. We also propose an\nEfficiency Score (ES) metric to evaluate the model efficiency. Extensive\nexperiments on Market1501, DukeMTMC-ReID, and CUHK03 datasets shows the\nefficiency and superiority of our approach compared with epoch-making methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 15:45:25 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zhang", "Jiangning", ""], ["Liu", "Liang", ""], ["Xu", "Chao", ""], ["Liu", "Yong", ""]]}, {"id": "2005.08829", "submitter": "Chen Wang", "authors": "Chen Wang, Wenshan Wang, Yuheng Qiu, Yafei Hu, and Sebastian Scherer", "title": "Visual Memorability for Robotic Interestingness via Unsupervised Online\n  Learning", "comments": "Oral paper in ECCV 2020", "journal-ref": "2020 European Conference on Computer Vision (ECCV)", "doi": "10.1007/978-3-030-58536-5_4", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the problem of interesting scene prediction for\nmobile robots. This area is currently underexplored but is crucial for many\npractical applications such as autonomous exploration and decision making.\nInspired by industrial demands, we first propose a novel translation-invariant\nvisual memory for recalling and identifying interesting scenes, then design a\nthree-stage architecture of long-term, short-term, and online learning. This\nenables our system to learn human-like experience, environmental knowledge, and\nonline adaption, respectively. Our approach achieves much higher accuracy than\nthe state-of-the-art algorithms on challenging robotic interestingness\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 16:00:27 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 01:26:24 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 16:43:35 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wang", "Chen", ""], ["Wang", "Wenshan", ""], ["Qiu", "Yuheng", ""], ["Hu", "Yafei", ""], ["Scherer", "Sebastian", ""]]}, {"id": "2005.08847", "submitter": "Xin Liu", "authors": "Xin Liu, Jiancheng Li, Jiaqi Wang, Ziwei Liu", "title": "MMFashion: An Open-Source Toolbox for Visual Fashion Analysis", "comments": "Codes and models are available at:\n  https://github.com/open-mmlab/mmfashion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MMFashion, a comprehensive, flexible and user-friendly open-source\nvisual fashion analysis toolbox based on PyTorch. This toolbox supports a wide\nspectrum of fashion analysis tasks, including Fashion Attribute Prediction,\nFashion Recognition and Retrieval, Fashion Landmark Detection, Fashion Parsing\nand Segmentation and Fashion Compatibility and Recommendation. It covers almost\nall the mainstream tasks in fashion analysis community. MMFashion has several\nappealing properties. Firstly, MMFashion follows the principle of modular\ndesign. The framework is decomposed into different components so that it is\neasily extensible for diverse customized modules. In addition, detailed\ndocumentations, demo scripts and off-the-shelf models are available, which ease\nthe burden of layman users to leverage the recent advances in deep\nlearning-based fashion analysis. Our proposed MMFashion is currently the most\ncomplete platform for visual fashion analysis in deep learning era, with more\nfunctionalities to be added. This toolbox and the benchmark could serve the\nflourishing research community by providing a flexible toolkit to deploy\nexisting models and develop new ideas and approaches. We welcome all\ncontributions to this still-growing efforts towards open science:\nhttps://github.com/open-mmlab/mmfashion.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 16:19:00 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 02:33:36 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Liu", "Xin", ""], ["Li", "Jiancheng", ""], ["Wang", "Jiaqi", ""], ["Liu", "Ziwei", ""]]}, {"id": "2005.08877", "submitter": "Danhang Tang", "authors": "Danhang Tang and Saurabh Singh and Philip A. Chou and Christian Haene\n  and Mingsong Dou and Sean Fanello and Jonathan Taylor and Philip Davidson and\n  Onur G. Guleryuz and Yinda Zhang and Shahram Izadi and Andrea Tagliasacchi\n  and Sofien Bouaziz and Cem Keskin", "title": "Deep Implicit Volume Compression", "comments": "Danhang Tang and Saurabh Singh have equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe a novel approach for compressing truncated signed distance fields\n(TSDF) stored in 3D voxel grids, and their corresponding textures. To compress\nthe TSDF, our method relies on a block-based neural network architecture\ntrained end-to-end, achieving state-of-the-art rate-distortion trade-off. To\nprevent topological errors, we losslessly compress the signs of the TSDF, which\nalso upper bounds the reconstruction error by the voxel size. To compress the\ncorresponding texture, we designed a fast block-based UV parameterization,\ngenerating coherent texture maps that can be effectively compressed using\nexisting video compression algorithms. We demonstrate the performance of our\nalgorithms on two 4D performance capture datasets, reducing bitrate by 66% for\nthe same distortion, or alternatively reducing the distortion by 50% for the\nsame bitrate, compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 16:46:13 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Tang", "Danhang", ""], ["Singh", "Saurabh", ""], ["Chou", "Philip A.", ""], ["Haene", "Christian", ""], ["Dou", "Mingsong", ""], ["Fanello", "Sean", ""], ["Taylor", "Jonathan", ""], ["Davidson", "Philip", ""], ["Guleryuz", "Onur G.", ""], ["Zhang", "Yinda", ""], ["Izadi", "Shahram", ""], ["Tagliasacchi", "Andrea", ""], ["Bouaziz", "Sofien", ""], ["Keskin", "Cem", ""]]}, {"id": "2005.08891", "submitter": "Yi Zhou", "authors": "Yi Zhou, Jingwan Lu, Connelly Barnes, Jimei Yang, Sitao Xiang, Hao li", "title": "Generative Tweening: Long-term Inbetweening of 3D Human Motions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generate complex and realistic human body animations at scale,\nwhile following specific artistic constraints, has been a fundamental goal for\nthe game and animation industry for decades. Popular techniques include\nkey-framing, physics-based simulation, and database methods via motion graphs.\nRecently, motion generators based on deep learning have been introduced.\nAlthough these learning models can automatically generate highly intricate\nstylized motions of arbitrary length, they still lack user control. To this\nend, we introduce the problem of long-term inbetweening, which involves\nautomatically synthesizing complex motions over a long time interval given very\nsparse keyframes by users. We identify a number of challenges related to this\nproblem, including maintaining biomechanical and keyframe constraints,\npreserving natural motions, and designing the entire motion sequence\nholistically while considering all constraints. We introduce a biomechanically\nconstrained generative adversarial network that performs long-term inbetweening\nof human motions, conditioned on keyframe constraints. This network uses a\nnovel two-stage approach where it first predicts local motion in the form of\njoint angles, and then predicts global motion, i.e. the global path that the\ncharacter follows. Since there are typically a number of possible motions that\ncould satisfy the given user constraints, we also enable our network to\ngenerate a variety of outputs with a scheme that we call Motion DNA. This\napproach allows the user to manipulate and influence the output content by\nfeeding seed motions (DNA) to the network. Trained with 79 classes of captured\nmotion data, our network performs robustly on a variety of highly complex\nmotion styles.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 17:04:34 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 05:36:46 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Zhou", "Yi", ""], ["Lu", "Jingwan", ""], ["Barnes", "Connelly", ""], ["Yang", "Jimei", ""], ["Xiang", "Sitao", ""], ["li", "Hao", ""]]}, {"id": "2005.08892", "submitter": "Christopher Ren", "authors": "Christopher X. Ren, Amanda Ziemann, James Theiler, Alice M. S. Durieux", "title": "Deep Snow: Synthesizing Remote Sensing Imagery with Generative\n  Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we demonstrate that generative adversarial networks (GANs) can\nbe used to generate realistic pervasive changes in remote sensing imagery, even\nin an unpaired training setting. We investigate some transformation quality\nmetrics based on deep embedding of the generated and real images which enable\nvisualization and understanding of the training dynamics of the GAN, and may\nprovide a useful measure in terms of quantifying how distinguishable the\ngenerated images are from real images. We also identify some artifacts\nintroduced by the GAN in the generated images, which are likely to contribute\nto the differences seen between the real and generated samples in the deep\nembedding feature space even in cases where the real and generated samples\nappear perceptually similar.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 17:05:00 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Ren", "Christopher X.", ""], ["Ziemann", "Amanda", ""], ["Theiler", "James", ""], ["Durieux", "Alice M. S.", ""]]}, {"id": "2005.08925", "submitter": "Xuaner Zhang", "authors": "Xuaner Cecilia Zhang, Jonathan T. Barron, Yun-Ta Tsai, Rohit Pandey,\n  Xiuming Zhang, Ren Ng, David E. Jacobs", "title": "Portrait Shadow Manipulation", "comments": "(updated version); SIGGRAPH 2020;Project webpage:\n  https://people.eecs.berkeley.edu/~cecilia77/project-pages/portrait Video:\n  https://youtu.be/M_qYTXhzyac", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Casually-taken portrait photographs often suffer from unflattering lighting\nand shadowing because of suboptimal conditions in the environment. Aesthetic\nqualities such as the position and softness of shadows and the lighting ratio\nbetween the bright and dark parts of the face are frequently determined by the\nconstraints of the environment rather than by the photographer. Professionals\naddress this issue by adding light shaping tools such as scrims, bounce cards,\nand flashes. In this paper, we present a computational approach that gives\ncasual photographers some of this control, thereby allowing poorly-lit\nportraits to be relit post-capture in a realistic and easily-controllable way.\nOur approach relies on a pair of neural networks---one to remove foreign\nshadows cast by external objects, and another to soften facial shadows cast by\nthe features of the subject and to add a synthetic fill light to improve the\nlighting ratio. To train our first network we construct a dataset of real-world\nportraits wherein synthetic foreign shadows are rendered onto the face, and we\nshow that our network learns to remove those unwanted shadows. To train our\nsecond network we use a dataset of Light Stage scans of human subjects to\nconstruct input/output pairs of input images harshly lit by a small light\nsource, and variably softened and fill-lit output images of each face. We\npropose a way to explicitly encode facial symmetry and show that our dataset\nand training procedure enable the model to generalize to images taken in the\nwild. Together, these networks enable the realistic and aesthetically pleasing\nenhancement of shadows and lights in real-world portrait images\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 17:51:34 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 17:49:55 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Zhang", "Xuaner Cecilia", ""], ["Barron", "Jonathan T.", ""], ["Tsai", "Yun-Ta", ""], ["Pandey", "Rohit", ""], ["Zhang", "Xiuming", ""], ["Ng", "Ren", ""], ["Jacobs", "David E.", ""]]}, {"id": "2005.08931", "submitter": "Zechun Liu", "authors": "Zechun Liu and Xiangyu Zhang and Zhiqiang Shen and Zhe Li and Yichen\n  Wei and Kwang-Ting Cheng and Jian Sun", "title": "Joint Multi-Dimension Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present joint multi-dimension pruning (named as JointPruning), a new\nperspective of pruning a network on three crucial aspects: spatial, depth and\nchannel simultaneously. The joint strategy enables to search a better status\nthan previous studies that focused on individual dimension solely, as our\nmethod is optimized collaboratively across the three dimensions in a single\nend-to-end training. Moreover, each dimension that we consider can promote to\nget better performance through colluding with the other two. Our method is\nrealized by the adapted stochastic gradient estimation. Extensive experiments\non large-scale ImageNet dataset across a variety of network architectures\nMobileNet V1&V2 and ResNet demonstrate the effectiveness of our proposed\nmethod. For instance, we achieve significant margins of 2.5% and 2.6%\nimprovement over the state-of-the-art approach on the already compact MobileNet\nV1&V2 under an extremely large compression ratio.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 17:57:09 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Liu", "Zechun", ""], ["Zhang", "Xiangyu", ""], ["Shen", "Zhiqiang", ""], ["Li", "Zhe", ""], ["Wei", "Yichen", ""], ["Cheng", "Kwang-Ting", ""], ["Sun", "Jian", ""]]}, {"id": "2005.08944", "submitter": "Kevin Feng", "authors": "Kevin Feng", "title": "Saving the Sonorine: Photovisual Audio Recovery Using Image Processing\n  and Computer Vision Techniques", "comments": "This version has been removed by arXiv administrators because the\n  submitter did not have the right to agree to the license applied at the time\n  of submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.GR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel technique to recover audio from sonorines, an\nearly 20th century form of analogue sound storage. Our method uses high\nresolution photographs of sonorines under different lighting conditions to\nobserve the change in reflection behavior of the physical surface features and\ncreate a three-dimensional height map of the surface. Sound can then be\nextracted using height information within the surface's grooves, mimicking a\nphysical stylus on a phonograph. Unlike traditional playback methods, our\nmethod has the advantage of being contactless: the medium will not incur damage\nand wear from being played repeatedly. We compare the results of our technique\nto a previously successful contactless method using flatbed scans of the\nsonorines, and conclude with future research that can be applied to this\nphotovisual approach to audio recovery.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 00:45:26 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 00:51:35 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 20:08:01 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Feng", "Kevin", ""]]}, {"id": "2005.09007", "submitter": "Xuebin Qin", "authors": "Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R.\n  Zaiane and Martin Jagersand", "title": "U$^2$-Net: Going Deeper with Nested U-Structure for Salient Object\n  Detection", "comments": "Accepted in Pattern Recognition 2020", "journal-ref": null, "doi": "10.1016/j.patcog.2020.107404", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design a simple yet powerful deep network architecture,\nU$^2$-Net, for salient object detection (SOD). The architecture of our\nU$^2$-Net is a two-level nested U-structure. The design has the following\nadvantages: (1) it is able to capture more contextual information from\ndifferent scales thanks to the mixture of receptive fields of different sizes\nin our proposed ReSidual U-blocks (RSU), (2) it increases the depth of the\nwhole architecture without significantly increasing the computational cost\nbecause of the pooling operations used in these RSU blocks. This architecture\nenables us to train a deep network from scratch without using backbones from\nimage classification tasks. We instantiate two models of the proposed\narchitecture, U$^2$-Net (176.3 MB, 30 FPS on GTX 1080Ti GPU) and\nU$^2$-Net$^{\\dagger}$ (4.7 MB, 40 FPS), to facilitate the usage in different\nenvironments. Both models achieve competitive performance on six SOD datasets.\nThe code is available: https://github.com/NathanUA/U-2-Net.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 18:08:26 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 04:06:04 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Qin", "Xuebin", ""], ["Zhang", "Zichen", ""], ["Huang", "Chenyang", ""], ["Dehghan", "Masood", ""], ["Zaiane", "Osmar R.", ""], ["Jagersand", "Martin", ""]]}, {"id": "2005.09015", "submitter": "Hana Alghamdi", "authors": "Hana Alghamdi, Rozenn Dahyot", "title": "Patch based Colour Transfer using SIFT Flow", "comments": "8 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new colour transfer method with Optimal Transport (OT) to\ntransfer the colour of a sourceimage to match the colour of a target image of\nthe same scene that may exhibit large motion changes betweenimages. By\ndefinition OT does not take into account any available information about\ncorrespondences whencomputing the optimal solution. To tackle this problem we\npropose to encode overlapping neighborhoodsof pixels using both their colour\nand spatial correspondences estimated using motion estimation. We solvethe high\ndimensional problem in 1D space using an iterative projection approach. We\nfurther introducesmoothing as part of the iterative algorithms for solving\noptimal transport namely Iterative DistributionTransport (IDT) and its variant\nthe Sliced Wasserstein Distance (SWD). Experiments show quantitative\nandqualitative improvements over previous state of the art colour transfer\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 18:22:36 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Alghamdi", "Hana", ""], ["Dahyot", "Rozenn", ""]]}, {"id": "2005.09022", "submitter": "Nazifa Khan", "authors": "Nazifa Khan, Oliver A.S. Lyon, Mark Eramian and Ian McQuillan", "title": "A Novel Technique Combining Image Processing, Plant Development\n  Properties, and the Hungarian Algorithm, to Improve Leaf Detection in Maize", "comments": "to be published in the IEEE CVPR 2020 Workshop Proceedings, and\n  accepted by The 1st International Workshop and Prize Challenge on\n  Agriculture-Vision: Challenges & Opportunities for Computer Vision in\n  Agriculture in conjunction with IEEE/CVF CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual determination of plant phenotypic properties such as plant\narchitecture, growth, and health is very time consuming and sometimes\ndestructive. Automatic image analysis has become a popular approach. This\nresearch aims to identify the position (and number) of leaves from a temporal\nsequence of high-quality indoor images consisting of multiple views, focussing\nin particular of images of maize. The procedure used a segmentation on the\nimages, using the convex hull to pick the best view at each time step, followed\nby a skeletonization of the corresponding image. To remove skeleton spurs, a\ndiscrete skeleton evolution pruning process was applied. Pre-existing\nstatistics regarding maize development was incorporated to help differentiate\nbetween true leaves and false leaves. Furthermore, for each time step, leaves\nwere matched to those of the previous and next three days using the\ngraph-theoretic Hungarian algorithm. This matching algorithm can be used to\nboth remove false positives, and also to predict true leaves, even if they were\ncompletely occluded from the image itself. The algorithm was evaluated using an\nopen dataset consisting of 13 maize plants across 27 days from two different\nviews. The total number of true leaves from the dataset was 1843, and our\nproposed techniques detect a total of 1690 leaves including 1674 true leaves,\nand only 16 false leaves, giving a recall of 90.8%, and a precision of 99.0%.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 18:43:50 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Khan", "Nazifa", ""], ["Lyon", "Oliver A. S.", ""], ["Eramian", "Mark", ""], ["McQuillan", "Ian", ""]]}, {"id": "2005.09026", "submitter": "Youssef Skandarani", "authors": "Youssef Skandarani, Nathan Painchaud, Pierre-Marc Jodoin, Alain\n  Lalande", "title": "On the effectiveness of GAN generated cardiac MRIs for segmentation", "comments": "4 pages, Accepted for MIDL 2020", "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/f9Pl3Qj3_Q", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a Variational Autoencoder (VAE) - Generative\nAdversarial Networks (GAN) model that can produce highly realistic MRI together\nwith its pixel accurate groundtruth for the application of cine-MR image\ncardiac segmentation. On one side of our model is a Variational Autoencoder\n(VAE) trained to learn the latent representations of cardiac shapes. On the\nother side is a GAN that uses \"SPatially-Adaptive (DE)Normalization\" (SPADE)\nmodules to generate realistic MR images tailored to a given anatomical map. At\ntest time, the sampling of the VAE latent space allows to generate an arbitrary\nlarge number of cardiac shapes, which are fed to the GAN that subsequently\ngenerates MR images whose cardiac structure fits that of the cardiac shapes. In\nother words, our system can generate a large volume of realistic yet labeled\ncardiac MR images. We show that segmentation with CNNs trained with our\nsynthetic annotated images gets competitive results compared to traditional\ntechniques. We also show that combining data augmentation with our\nGAN-generated images lead to an improvement in the Dice score of up to 12\npercent while allowing for better generalization capabilities on other\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 18:48:38 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 09:28:39 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Skandarani", "Youssef", ""], ["Painchaud", "Nathan", ""], ["Jodoin", "Pierre-Marc", ""], ["Lalande", "Alain", ""]]}, {"id": "2005.09027", "submitter": "Pavel Levin", "authors": "Benjamin Gutelman and Pavel Levin", "title": "Efficient Image Gallery Representations at Scale Through Multi-Task\n  Learning", "comments": "Proceedings of the 43rd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval", "journal-ref": null, "doi": "10.1145/3397271.3401433", "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image galleries provide a rich source of diverse information about a product\nwhich can be leveraged across many recommendation and retrieval applications.\nWe study the problem of building a universal image gallery encoder through\nmulti-task learning (MTL) approach and demonstrate that it is indeed a\npractical way to achieve generalizability of learned representations to new\ndownstream tasks. Additionally, we analyze the relative predictive performance\nof MTL-trained solutions against optimal and substantially more expensive\nsolutions, and find signals that MTL can be a useful mechanism to address\nsparsity in low-resource binary tasks.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 18:49:22 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 05:50:53 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 10:24:02 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Gutelman", "Benjamin", ""], ["Levin", "Pavel", ""]]}, {"id": "2005.09034", "submitter": "Fuyuan Lyu", "authors": "Fuyuan Lyu, Shien Zhu, Weichen Liu", "title": "Cross-filter compression for CNN inference acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution neural network demonstrates great capability for multiple tasks,\nsuch as image classification and many others. However, much resource is\nrequired to train a network. Hence much effort has been made to accelerate\nneural network by reducing precision of weights, activation, and gradient.\nHowever, these filter-wise quantification methods exist a natural upper limit,\ncaused by the size of the kernel. Meanwhile, with the popularity of small\nkernel, the natural limit further decrease. To address this issue, we propose a\nnew cross-filter compression method that can provide $\\sim32\\times$ memory\nsavings and $122\\times$ speed up in convolution operations. In our method, all\nconvolution filters are quantized to given bits and spatially adjacent filters\nshare the same scaling factor. Our compression method, based on Binary-Weight\nand XNOR-Net separately, is evaluated on CIFAR-10 and ImageNet dataset with\nwidely used network structures, such as ResNet and VGG, and witness tolerable\naccuracy loss compared to state-of-the-art quantification methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 19:06:14 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Lyu", "Fuyuan", ""], ["Zhu", "Shien", ""], ["Liu", "Weichen", ""]]}, {"id": "2005.09057", "submitter": "Kevin Moran P", "authors": "Carlos Bernal-C\\'ardenas, Nathan Cooper, Kevin Moran, Oscar Chaparro,\n  Andrian Marcus and Denys Poshyvanyk", "title": "Translating Video Recordings of Mobile App Usages into Replayable\n  Scenarios", "comments": "In proceedings of the 42nd International Conference on Software\n  Engineering (ICSE'20), 13 pages", "journal-ref": null, "doi": "10.1145/3377811.3380328", "report-no": null, "categories": "cs.SE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Screen recordings of mobile applications are easy to obtain and capture a\nwealth of information pertinent to software developers (e.g., bugs or feature\nrequests), making them a popular mechanism for crowdsourced app feedback. Thus,\nthese videos are becoming a common artifact that developers must manage. In\nlight of unique mobile development constraints, including swift release cycles\nand rapidly evolving platforms, automated techniques for analyzing all types of\nrich software artifacts provide benefit to mobile developers. Unfortunately,\nautomatically analyzing screen recordings presents serious challenges, due to\ntheir graphical nature, compared to other types of (textual) artifacts. To\naddress these challenges, this paper introduces V2S, a lightweight, automated\napproach for translating video recordings of Android app usages into replayable\nscenarios. V2S is based primarily on computer vision techniques and adapts\nrecent solutions for object detection and image classification to detect and\nclassify user actions captured in a video, and convert these into a replayable\ntest scenario. We performed an extensive evaluation of V2S involving 175 videos\ndepicting 3,534 GUI-based actions collected from users exercising features and\nreproducing bugs from over 80 popular Android apps. Our results illustrate that\nV2S can accurately replay scenarios from screen recordings, and is capable of\nreproducing $\\approx$ 89% of our collected videos with minimal overhead. A case\nstudy with three industrial partners illustrates the potential usefulness of\nV2S from the viewpoint of developers.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 20:11:36 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Bernal-C\u00e1rdenas", "Carlos", ""], ["Cooper", "Nathan", ""], ["Moran", "Kevin", ""], ["Chaparro", "Oscar", ""], ["Marcus", "Andrian", ""], ["Poshyvanyk", "Denys", ""]]}, {"id": "2005.09084", "submitter": "Shaofan Li", "authors": "Chao Wang, Shaofan Li, Danielle Zeng, and Xinhai Zhu", "title": "An Artificial-intelligence/Statistics Solution to Quantify Material\n  Distortion for Thermal Compensation in Additive Manufacturing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a probabilistic statistics solution or artificial\nintelligence (AI) approach to identify and quantify permanent (non-zero strain)\ncontinuum/material deformation only based on the scanned material data in the\nspatial configuration and the shape of the initial design configuration or the\nmaterial configuration. The challenge of this problem is that we only know the\nscanned material data in the spatial configuration and the shape of the design\nconfiguration of three-dimensional (3D) printed products, whereas for a\nspecific scanned material point we do not know its corresponding material\ncoordinates in the initial or designed referential configuration, provided that\nwe do not know the detailed information on actual physical deformation process.\nDifferent from physics-based modeling, the method developed here is a\ndata-driven artificial intelligence method, which solves the problem with\nincomplete deformation data or with missing information of actual physical\ndeformation process. We coined the method is an AI-based material deformation\nfinding algorithm.\n  This method has practical significance and important applications in finding\nand designing thermal compensation configuration of a 3D printed product in\nadditive manufacturing, which is at the heart of the cutting edge 3D printing\ntechnology. In this paper, we demonstrate that the proposed AI\ncontinuum/material deformation finding approach can accurately find permanent\nthermal deformation configuration for a complex 3D printed structure component,\nand hence to identify the thermal compensation design configuration in order to\nminimizing the impact of temperature fluctuations on 3D printed structure\ncomponents that are sensitive to changes of temperature.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 20:02:47 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Wang", "Chao", ""], ["Li", "Shaofan", ""], ["Zeng", "Danielle", ""], ["Zhu", "Xinhai", ""]]}, {"id": "2005.09110", "submitter": "Alessandro Lameiras Koerich", "authors": "Voncarlos M. Araujo, Alceu S. Britto Jr., Luiz E. S. Oliveira and\n  Alessandro L. Koerich", "title": "Two-View Fine-grained Classification of Plant Species", "comments": "Submitted to Ecological Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic plant classification is a challenging problem due to the wide\nbiodiversity of the existing plant species in a fine-grained scenario. Powerful\ndeep learning architectures have been used to improve the classification\nperformance in such a fine-grained problem, but usually building models that\nare highly dependent on a large training dataset and which are not scalable. In\nthis paper, we propose a novel method based on a two-view leaf image\nrepresentation and a hierarchical classification strategy for fine-grained\nrecognition of plant species. It uses the botanical taxonomy as a basis for a\ncoarse-to-fine strategy applied to identify the plant genus and species. The\ntwo-view representation provides complementary global and local features of\nleaf images. A deep metric based on Siamese convolutional neural networks is\nused to reduce the dependence on a large number of training samples and make\nthe method scalable to new plant species. The experimental results on two\nchallenging fine-grained datasets of leaf images (i.e. LifeCLEF 2015 and\nLeafSnap) have shown the effectiveness of the proposed method, which achieved\nrecognition accuracy of 0.87 and 0.96 respectively.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 21:57:47 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Araujo", "Voncarlos M.", ""], ["Britto", "Alceu S.", "Jr."], ["Oliveira", "Luiz E. S.", ""], ["Koerich", "Alessandro L.", ""]]}, {"id": "2005.09120", "submitter": "Shuhao Fu", "authors": "Shuhao Fu, Yongyi Lu, Yan Wang, Yuyin Zhou, Wei Shen, Elliot Fishman,\n  Alan Yuille", "title": "Domain Adaptive Relational Reasoning for 3D Multi-Organ Segmentation", "comments": "Accepted at MICCAL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel unsupervised domain adaptation (UDA)\nmethod, named Domain Adaptive Relational Reasoning (DARR), to generalize 3D\nmulti-organ segmentation models to medical data collected from different\nscanners and/or protocols (domains). Our method is inspired by the fact that\nthe spatial relationship between internal structures in medical images is\nrelatively fixed, e.g., a spleen is always located at the tail of a pancreas,\nwhich serves as a latent variable to transfer the knowledge shared across\nmultiple domains. We formulate the spatial relationship by solving a jigsaw\npuzzle task, i.e., recovering a CT scan from its shuffled patches, and jointly\ntrain it with the organ segmentation task. To guarantee the transferability of\nthe learned spatial relationship to multiple domains, we additionally introduce\ntwo schemes: 1) Employing a super-resolution network also jointly trained with\nthe segmentation model to standardize medical images from different domain to a\ncertain spatial resolution; 2) Adapting the spatial relationship for a test\nimage by test-time jigsaw puzzle training. Experimental results show that our\nmethod improves the performance by 29.60% DSC on target datasets on average\nwithout using any data from the target domain during training.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 22:44:34 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 20:23:19 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Fu", "Shuhao", ""], ["Lu", "Yongyi", ""], ["Wang", "Yan", ""], ["Zhou", "Yuyin", ""], ["Shen", "Wei", ""], ["Fishman", "Elliot", ""], ["Yuille", "Alan", ""]]}, {"id": "2005.09134", "submitter": "Linhai Ma", "authors": "Linhai Ma, Liang Liang", "title": "Improve robustness of DNN for ECG signal classification:a\n  noise-to-signal ratio perspective", "comments": "This paper is accepted at ICLR 2020 workshop on Artificial\n  Intelligence for Affordable Healthcare. 14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electrocardiogram (ECG) is the most widely used diagnostic tool to monitor\nthe condition of the cardiovascular system. Deep neural networks (DNNs), have\nbeen developed in many research labs for automatic interpretation of ECG\nsignals to identify potential abnormalities in patient hearts. Studies have\nshown that given a sufficiently large amount of data, the classification\naccuracy of DNNs could reach human-expert cardiologist level. A DNN-based\nautomated ECG diagnostic system would be an affordable solution for patients in\ndeveloping countries where human-expert cardiologist are lacking. However,\ndespite of the excellent performance in classification accuracy, it has been\nshown that DNNs are highly vulnerable to adversarial attacks: subtle changes in\ninput of a DNN can lead to a wrong classification output with high confidence.\nThus, it is challenging and essential to improve adversarial robustness of DNNs\nfor ECG signal classification, a life-critical application. In this work, we\nproposed to improve DNN robustness from the perspective of noise-to-signal\nratio (NSR) and developed two methods to minimize NSR during training process.\nWe evaluated the proposed methods on PhysionNets MIT-BIH dataset, and the\nresults show that our proposed methods lead to an enhancement in robustness\nagainst PGD adversarial attack and SPSA attack, with a minimal change in\naccuracy on clean data.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 23:37:33 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 00:12:47 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 23:10:37 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Ma", "Linhai", ""], ["Liang", "Liang", ""]]}, {"id": "2005.09146", "submitter": "Brian Park", "authors": "Brian J. Park, Stephen J. Hunt, Gregory J. Nadolski, Terence P. Gade", "title": "3D Augmented Reality-Assisted CT-Guided Interventions: System Design and\n  Preclinical Trial on an Abdominal Phantom using HoloLens 2", "comments": "16 pages, 6 figures, 2 tables", "journal-ref": null, "doi": "10.1038/s41598-020-75676-4", "report-no": null, "categories": "physics.med-ph cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Out-of-plane lesions pose challenges for CT-guided interventions.\nAugmented reality (AR) headset devices have evolved and are readily capable to\nprovide virtual 3D guidance to improve CT-guided targeting.\n  Purpose: To describe the design of a three-dimensional (3D) AR-assisted\nnavigation system using HoloLens 2 and evaluate its performance through\nCT-guided simulations.\n  Materials and Methods: A prospective trial was performed assessing CT-guided\nneedle targeting on an abdominal phantom with and without AR guidance. A total\nof 8 operators with varying clinical experience were enrolled and performed a\ntotal of 86 needle passes. Procedure efficiency, radiation dose, and\ncomplication rates were compared with and without AR guidance. Vector analysis\nof the first needle pass was also performed.\n  Results: Average total number of needle passes to reach the target reduced\nfrom 7.4 passes without AR to 3.4 passes with AR (54.2% decrease, p=0.011).\nAverage dose-length product (DLP) decreased from 538 mGy-cm without AR to 318\nmGy-cm with AR (41.0% decrease, p=0.009). Complication rate of hitting a\nnon-targeted lesion decreased from 11.9% without AR (7/59 needle passes) to 0%\nwith AR (0/27 needle passes). First needle passes were more nearly aligned with\nthe ideal target trajectory with AR versus without AR (4.6{\\deg} vs 8.0{\\deg}\noffset, respectively, p=0.018). Medical students, residents, and attendings all\nperformed at the same level with AR guidance.\n  Conclusions: 3D AR guidance can provide significant improvements in\nprocedural efficiency and radiation dose savings for targeting challenging,\nout-of-plane lesions. AR guidance elevated the performance of all operators to\nthe same level irrespective of prior clinical experience.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 00:22:24 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Park", "Brian J.", ""], ["Hunt", "Stephen J.", ""], ["Nadolski", "Gregory J.", ""], ["Gade", "Terence P.", ""]]}, {"id": "2005.09147", "submitter": "Linhai Ma", "authors": "Linhai Ma, Liang Liang", "title": "Increasing-Margin Adversarial (IMA) Training to Improve Adversarial\n  Robustness of Neural Networks", "comments": "23 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) has surpassed traditional methods for\nmed-ical image classification. However, CNN is vulnerable to adversarial\nattacks which may lead to disastrous consequences in medical applications.\nAlthough adversarial noises are usually generated by attack algorithms,\nwhite-noise-induced adversarial samples can exist, and therefore the threats\nare real. In this study, we propose a novel training method, named IMA, to\nimprove the robust-ness of CNN against adversarial noises. During training, the\nIMA method in-creases the margins of training samples in the input space, i.e.,\nmoving CNN de-cision boundaries far away from the training samples to improve\nrobustness. The IMA method is evaluated on four publicly available datasets\nunder strong 100-PGD white-box adversarial attacks, and the results show that\nthe proposed meth-od significantly improved CNN classification accuracy on\nnoisy data while keep-ing a relatively high accuracy on clean data. We hope our\napproach may facilitate the development of robust applications in medical\nfield.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 00:26:52 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 20:31:20 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 20:58:21 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Ma", "Linhai", ""], ["Liang", "Liang", ""]]}, {"id": "2005.09153", "submitter": "Zihan Ye", "authors": "Zihan Ye, Fuyuan Hu, Yin Liu, Zhenping Xia, Fan Lyu, Pengqing Liu", "title": "Associating Multi-Scale Receptive Fields for Fine-grained Recognition", "comments": "Accepted by ICIP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting and fusing part features have become the key of fined-grained\nimage recognition. Recently, Non-local (NL) module has shown excellent\nimprovement in image recognition. However, it lacks the mechanism to model the\ninteractions between multi-scale part features, which is vital for fine-grained\nrecognition. In this paper, we propose a novel cross-layer non-local (CNL)\nmodule to associate multi-scale receptive fields by two operations. First, CNL\ncomputes correlations between features of a query layer and all response\nlayers. Second, all response features are weighted according to the\ncorrelations and are added to the query features. Due to the interactions of\ncross-layer features, our model builds spatial dependencies among multi-level\nlayers and learns more discriminative features. In addition, we can reduce the\naggregation cost if we set low-dimensional deep layer as query layer.\nExperiments are conducted to show our model achieves or surpasses\nstate-of-the-art results on three benchmark datasets of fine-grained\nclassification. Our codes can be found at github.com/FouriYe/CNL-ICIP2020.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 01:16:31 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Ye", "Zihan", ""], ["Hu", "Fuyuan", ""], ["Liu", "Yin", ""], ["Xia", "Zhenping", ""], ["Lyu", "Fan", ""], ["Liu", "Pengqing", ""]]}, {"id": "2005.09159", "submitter": "Hangyu Lin", "authors": "Hangyu Lin, Yanwei Fu, Yu-Gang Jiang, Xiangyang Xue", "title": "Sketch-BERT: Learning Sketch Bidirectional Encoder Representation from\n  Transformers by Self-supervised Learning of Sketch Gestalt", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous researches of sketches often considered sketches in pixel format and\nleveraged CNN based models in the sketch understanding. Fundamentally, a sketch\nis stored as a sequence of data points, a vector format representation, rather\nthan the photo-realistic image of pixels. SketchRNN studied a generative neural\nrepresentation for sketches of vector format by Long Short Term Memory networks\n(LSTM). Unfortunately, the representation learned by SketchRNN is primarily for\nthe generation tasks, rather than the other tasks of recognition and retrieval\nof sketches. To this end and inspired by the recent BERT model, we present a\nmodel of learning Sketch Bidirectional Encoder Representation from Transformer\n(Sketch-BERT). We generalize BERT to sketch domain, with the novel proposed\ncomponents and pre-training algorithms, including the newly designed sketch\nembedding networks, and the self-supervised learning of sketch gestalt.\nParticularly, towards the pre-training task, we present a novel Sketch Gestalt\nModel (SGM) to help train the Sketch-BERT. Experimentally, we show that the\nlearned representation of Sketch-BERT can help and improve the performance of\nthe downstream tasks of sketch recognition, sketch retrieval, and sketch\ngestalt.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 01:35:44 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Lin", "Hangyu", ""], ["Fu", "Yanwei", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""]]}, {"id": "2005.09161", "submitter": "Aishan Liu", "authors": "Aishan Liu, Tairan Huang, Xianglong Liu, Yitao Xu, Yuqing Ma, Xinyun\n  Chen, Stephen J. Maybank, Dacheng Tao", "title": "Spatiotemporal Attacks for Embodied Agents", "comments": "Accepted on ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks are valuable for providing insights into the blind-spots\nof deep learning models and help improve their robustness. Existing work on\nadversarial attacks have mainly focused on static scenes; however, it remains\nunclear whether such attacks are effective against embodied agents, which could\nnavigate and interact with a dynamic environment. In this work, we take the\nfirst step to study adversarial attacks for embodied agents. In particular, we\ngenerate spatiotemporal perturbations to form 3D adversarial examples, which\nexploit the interaction history in both the temporal and spatial dimensions.\nRegarding the temporal dimension, since agents make predictions based on\nhistorical observations, we develop a trajectory attention module to explore\nscene view contributions, which further help localize 3D objects appeared with\nthe highest stimuli. By conciliating with clues from the temporal dimension,\nalong the spatial dimension, we adversarially perturb the physical properties\n(e.g., texture and 3D shape) of the contextual objects that appeared in the\nmost important scene views. Extensive experiments on the EQA-v1 dataset for\nseveral embodied tasks in both the white-box and black-box settings have been\nconducted, which demonstrate that our perturbations have strong attack and\ngeneralization abilities.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 01:38:47 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 14:39:37 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 14:13:55 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Liu", "Aishan", ""], ["Huang", "Tairan", ""], ["Liu", "Xianglong", ""], ["Xu", "Yitao", ""], ["Ma", "Yuqing", ""], ["Chen", "Xinyun", ""], ["Maybank", "Stephen J.", ""], ["Tao", "Dacheng", ""]]}, {"id": "2005.09162", "submitter": "Shahabeddin Sotudian", "authors": "Mohammad Hossein Fazel Zarandi, Shahabeddin Sotudian, Oscar Castillo", "title": "A New Validity Index for Fuzzy-Possibilistic C-Means Clustering", "comments": "The following article has been accepted by Scientia Iranica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some complicated datasets, due to the presence of noisy data points and\noutliers, cluster validity indices can give conflicting results in determining\nthe optimal number of clusters. This paper presents a new validity index for\nfuzzy-possibilistic c-means clustering called Fuzzy-Possibilistic (FP) index,\nwhich works well in the presence of clusters that vary in shape and density.\nMoreover, FPCM like most of the clustering algorithms is susceptible to some\ninitial parameters. In this regard, in addition to the number of clusters, FPCM\nrequires a priori selection of the degree of fuzziness and the degree of\ntypicality. Therefore, we presented an efficient procedure for determining\ntheir optimal values. The proposed approach has been evaluated using several\nsynthetic and real-world datasets. Final computational results demonstrate the\ncapabilities and reliability of the proposed approach compared with several\nwell-known fuzzy validity indices in the literature. Furthermore, to clarify\nthe ability of the proposed method in real applications, the proposed method is\nimplemented in microarray gene expression data clustering and medical image\nsegmentation.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 01:48:13 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Zarandi", "Mohammad Hossein Fazel", ""], ["Sotudian", "Shahabeddin", ""], ["Castillo", "Oscar", ""]]}, {"id": "2005.09163", "submitter": "Yuang Liu", "authors": "Yuang Liu, Wei Zhang, Jun Wang", "title": "Learning from a Lightweight Teacher for Efficient Knowledge Distillation", "comments": "11 pages, 3 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Distillation (KD) is an effective framework for compressing deep\nlearning models, realized by a student-teacher paradigm requiring small student\nnetworks to mimic the soft target generated by well-trained teachers. However,\nthe teachers are commonly assumed to be complex and need to be trained on the\nsame datasets as students. This leads to a time-consuming training process. The\nrecent study shows vanilla KD plays a similar role as label smoothing and\ndevelops teacher-free KD, being efficient and mitigating the issue of learning\nfrom heavy teachers. But because teacher-free KD relies on manually-crafted\noutput distributions kept the same for all data instances belonging to the same\nclass, its flexibility and performance are relatively limited. To address the\nabove issues, this paper proposes en efficient knowledge distillation learning\nframework LW-KD, short for lightweight knowledge distillation. It firstly\ntrains a lightweight teacher network on a synthesized simple dataset, with an\nadjustable class number equal to that of a target dataset. The teacher then\ngenerates soft target whereby an enhanced KD loss could guide student learning,\nwhich is a combination of KD loss and adversarial loss for making student\noutput indistinguishable from the output of the teacher. Experiments on several\npublic datasets with different modalities demonstrate LWKD is effective and\nefficient, showing the rationality of its main design principles.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 01:54:15 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Liu", "Yuang", ""], ["Zhang", "Wei", ""], ["Wang", "Jun", ""]]}, {"id": "2005.09165", "submitter": "Minhyeok Lee", "authors": "Minhyeok Lee, Junhee Seok", "title": "Regularization Methods for Generative Adversarial Networks: An Overview\n  of Recent Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its short history, Generative Adversarial Network (GAN) has been\nextensively studied and used for various tasks, including its original purpose,\ni.e., synthetic sample generation. However, applying GAN to different data\ntypes with diverse neural network architectures has been hindered by its\nlimitation in training, where the model easily diverges. Such a notorious\ntraining of GANs is well known and has been addressed in numerous studies.\nConsequently, in order to make the training of GAN stable, numerous\nregularization methods have been proposed in recent years. This paper reviews\nthe regularization methods that have been recently introduced, most of which\nhave been published in the last three years. Specifically, we focus on general\nmethods that can be commonly used regardless of neural network architectures.\nTo explore the latest research trends in the regularization for GANs, the\nmethods are classified into several groups by their operation principles, and\nthe differences between the methods are analyzed. Furthermore, to provide\npractical knowledge of using these methods, we investigate popular methods that\nhave been frequently employed in state-of-the-art GANs. In addition, we discuss\nthe limitations in existing methods and propose future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 01:59:24 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Lee", "Minhyeok", ""], ["Seok", "Junhee", ""]]}, {"id": "2005.09167", "submitter": "Xixi Xu", "authors": "Xixi Xu, Chao Lu, Liang Zhu, Xiangyang Xue, Guanxian Chen, Qi Guo,\n  Yining Lin, Zhijian Zhao", "title": "MOTS: Multiple Object Tracking for General Categories Based On Few-Shot\n  Method", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern Multi-Object Tracking (MOT) systems typically apply REID-based\nparadigm to hold a balance between computational efficiency and performance. In\nthe past few years, numerous attempts have been made to perfect the systems.\nAlthough they presented favorable performance, they were constrained to track\nspecified category. Drawing on the ideas of few shot method, we pioneered a new\nmulti-target tracking system, named MOTS, which is based on metrics but not\nlimited to track specific category. It contains two stages in series: In the\nfirst stage, we design the self-Adaptive-matching module to perform simple\ntargets matching, which can complete 88.76% assignments without sacrificing\nperformance on MOT16 training set. In the second stage, a Fine-match Network\nwas carefully designed for unmatched targets. With a newly built TRACK-REID\ndata-set, the Fine-match Network can perform matching of 31 category targets,\neven generalizes to unseen categories.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 02:18:01 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Xu", "Xixi", ""], ["Lu", "Chao", ""], ["Zhu", "Liang", ""], ["Xue", "Xiangyang", ""], ["Chen", "Guanxian", ""], ["Guo", "Qi", ""], ["Lin", "Yining", ""], ["Zhao", "Zhijian", ""]]}, {"id": "2005.09183", "submitter": "Seito Kasai", "authors": "Seito Kasai, Yuchi Ishikawa, Masaki Hayashi, Yoshimitsu Aoki, Kensho\n  Hara, Hirokatsu Kataoka", "title": "Retrieving and Highlighting Action with Spatiotemporal Reference", "comments": "Accepted to ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a framework that jointly retrieves and\nspatiotemporally highlights actions in videos by enhancing current deep\ncross-modal retrieval methods. Our work takes on the novel task of action\nhighlighting, which visualizes where and when actions occur in an untrimmed\nvideo setting. Action highlighting is a fine-grained task, compared to\nconventional action recognition tasks which focus on classification or\nwindow-based localization. Leveraging weak supervision from annotated captions,\nour framework acquires spatiotemporal relevance maps and generates local\nembeddings which relate to the nouns and verbs in captions. Through\nexperiments, we show that our model generates various maps conditioned on\ndifferent actions, in which conventional visual reasoning methods only go as\nfar as to show a single deterministic saliency map. Also, our model improves\nretrieval recall over our baseline without alignment by 2-3% on the MSR-VTT\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 03:12:31 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Kasai", "Seito", ""], ["Ishikawa", "Yuchi", ""], ["Hayashi", "Masaki", ""], ["Aoki", "Yoshimitsu", ""], ["Hara", "Kensho", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "2005.09212", "submitter": "Jiayu Huo", "authors": "Jiayu Huo, Liping Si, Xi Ouyang, Kai Xuan, Weiwu Yao, Zhong Xue, Qian\n  Wang, Dinggang Shen, Lichi Zhang", "title": "A Self-ensembling Framework for Semi-supervised Knee Cartilage Defects\n  Assessment with Dual-Consistency", "comments": "accepted by International Workshop on PRedictive Intelligence In\n  MEdicine, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knee osteoarthritis (OA) is one of the most common musculoskeletal disorders\nand requires early-stage diagnosis. Nowadays, the deep convolutional neural\nnetworks have achieved greatly in the computer-aided diagnosis field. However,\nthe construction of the deep learning models usually requires great amounts of\nannotated data, which is generally high-cost. In this paper, we propose a novel\napproach for knee cartilage defects assessment, including severity\nclassification and lesion localization. This can be treated as a subtask of\nknee OA diagnosis. Particularly, we design a self-ensembling framework, which\nis composed of a student network and a teacher network with the same structure.\nThe student network learns from both labeled data and unlabeled data and the\nteacher network averages the student model weights through the training course.\nA novel attention loss function is developed to obtain accurate attention\nmasks. With dual-consistency checking of the attention in the lesion\nclassification and localization, the two networks can gradually optimize the\nattention distribution and improve the performance of each other, whereas the\ntraining relies on partially labeled data only and follows the semi-supervised\nmanner. Experiments show that the proposed method can significantly improve the\nself-ensembling performance in both knee cartilage defects classification and\nlocalization, and also greatly reduce the needs of annotated data.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 04:47:25 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 11:08:17 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Huo", "Jiayu", ""], ["Si", "Liping", ""], ["Ouyang", "Xi", ""], ["Xuan", "Kai", ""], ["Yao", "Weiwu", ""], ["Xue", "Zhong", ""], ["Wang", "Qian", ""], ["Shen", "Dinggang", ""], ["Zhang", "Lichi", ""]]}, {"id": "2005.09223", "submitter": "Zhixin Li", "authors": "Bo Xu, Xu Zhang, Zhixin Li, Matt Leotta, Shih-Fu Chang, Jie Shan", "title": "Deep Learning Guided Building Reconstruction from Satellite\n  Imagery-derived Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D urban reconstruction of buildings from remotely sensed imagery has drawn\nsignificant attention during the past two decades. While aerial imagery and\nLiDAR provide higher resolution, satellite imagery is cheaper and more\nefficient to acquire for large scale need. However, the high, orbital altitude\nof satellite observation brings intrinsic challenges, like unpredictable\natmospheric effect, multi view angles, significant radiometric differences due\nto the necessary multiple views, diverse land covers and urban structures in a\nscene, small base-height ratio or narrow field of view, all of which may\ndegrade 3D reconstruction quality. To address these major challenges, we\npresent a reliable and effective approach for building model reconstruction\nfrom the point clouds generated from multi-view satellite images. We utilize\nmultiple types of primitive shapes to fit the input point cloud. Specifically,\na deep-learning approach is adopted to distinguish the shape of building roofs\nin complex and yet noisy scenes. For points that belong to the same roof shape,\na multi-cue, hierarchical RANSAC approach is proposed for efficient and\nreliable segmenting and reconstructing the building point cloud. Experimental\nresults over four selected urban areas (0.34 to 2.04 sq km in size) demonstrate\nthe proposed method can generate detailed roof structures under noisy data\nenvironments. The average successful rate for building shape recognition is\n83.0%, while the overall completeness and correctness are over 70% with\nreference to ground truth created from airborne lidar. As the first effort to\naddress the public need of large scale city model generation, the development\nis deployed as open source software.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 05:38:06 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Xu", "Bo", ""], ["Zhang", "Xu", ""], ["Li", "Zhixin", ""], ["Leotta", "Matt", ""], ["Chang", "Shih-Fu", ""], ["Shan", "Jie", ""]]}, {"id": "2005.09226", "submitter": "Zhixin Li", "authors": "Zhixin Li, Wenyuan Zhang, Jie Shan", "title": "Holistic Parameteric Reconstruction of Building Models from Point Clouds", "comments": "Remote Sens. Spatial Inf. Sci., 2020", "journal-ref": null, "doi": "10.5194/isprs-archives-XLIII-B2-2020-689-2020", "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building models are conventionally reconstructed by building roof points\nplanar segmentation and then using a topology graph to group the planes\ntogether. Roof edges and vertices are then mathematically represented by\nintersecting segmented planes. Technically, such solution is based on\nsequential local fitting, i.e., the entire data of one building are not\nsimultaneously participating in determining the building model. As a\nconsequence, the solution is lack of topological integrity and geometric rigor.\nFundamentally different from this traditional approach, we propose a holistic\nparametric reconstruction method which means taking into consideration the\nentire point clouds of one building simultaneously. In our work, building\nmodels are reconstructed from predefined parametric (roof) primitives. We first\nuse a well-designed deep neural network to segment and identify primitives in\nthe given building point clouds. A holistic optimization strategy is then\nintroduced to simultaneously determine the parameters of a segmented primitive.\nIn the last step, the optimal parameters are used to generate a watertight\nbuilding model in CityGML format. The airborne LiDAR dataset RoofN3D with\npredefined roof types is used for our test. It is shown that PointNet++ applied\nto the entire dataset can achieve an accuracy of 83% for primitive\nclassification. For a subset of 910 buildings in RoofN3D, the holistic approach\nis then used to determine the parameters of primitives and reconstruct the\nbuildings. The achieved overall quality of reconstruction is 0.08 meters for\npoint-surface-distance or 0.7 times RMSE of the input LiDAR points. The study\ndemonstrates the efficiency and capability of the proposed approach and its\npotential to handle large scale urban point clouds.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 05:42:23 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Li", "Zhixin", ""], ["Zhang", "Wenyuan", ""], ["Shan", "Jie", ""]]}, {"id": "2005.09228", "submitter": "Hong Wang", "authors": "Hong Wang, Yichen Wu, Qi Xie, Qian Zhao, Yong Liang, Deyu Meng", "title": "Structural Residual Learning for Single Image Rain Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To alleviate the adverse effect of rain streaks in image processing tasks,\nCNN-based single image rain removal methods have been recently proposed.\nHowever, the performance of these deep learning methods largely relies on the\ncovering range of rain shapes contained in the pre-collected training\nrainy-clean image pairs. This makes them easily trapped into the\noverfitting-to-the-training-samples issue and cannot finely generalize to\npractical rainy images with complex and diverse rain streaks. Against this\ngeneralization issue, this study proposes a new network architecture by\nenforcing the output residual of the network possess intrinsic rain structures.\nSuch a structural residual setting guarantees the rain layer extracted by the\nnetwork finely comply with the prior knowledge of general rain streaks, and\nthus regulates sound rain shapes capable of being well extracted from rainy\nimages in both training and predicting stages. Such a general regularization\nfunction naturally leads to both its better training accuracy and testing\ngeneralization capability even for those non-seen rain configurations. Such\nsuperiority is comprehensively substantiated by experiments implemented on\nsynthetic and real datasets both visually and quantitatively as compared with\ncurrent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 05:52:13 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Wang", "Hong", ""], ["Wu", "Yichen", ""], ["Xie", "Qi", ""], ["Zhao", "Qian", ""], ["Liang", "Yong", ""], ["Meng", "Deyu", ""]]}, {"id": "2005.09230", "submitter": "Dongming Wei", "authors": "Dongming Wei, Sahar Ahmad, Yunzhi Huang, Lei Ma, Zhengwang Wu, Gang\n  Li, Li Wang, Qian Wang, Pew-Thian Yap, Dinggang Shen", "title": "An Auto-Context Deformable Registration Network for Infant Brain MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable image registration is fundamental to longitudinal and population\nanalysis. Geometric alignment of the infant brain MR images is challenging,\nowing to rapid changes in image appearance in association with brain\ndevelopment. In this paper, we propose an infant-dedicated deep registration\nnetwork that uses the auto-context strategy to gradually refine the deformation\nfields to obtain highly accurate correspondences. Instead of training multiple\nregistration networks, our method estimates the deformation fields by invoking\na single network multiple times for iterative deformation refinement. The final\ndeformation field is obtained by the incremental composition of the deformation\nfields. Experimental results in comparison with state-of-the-art registration\nmethods indicate that our method achieves higher accuracy while at the same\ntime preserves the smoothness of the deformation fields. Our implementation is\navailable online.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 06:00:13 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 08:56:54 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Wei", "Dongming", ""], ["Ahmad", "Sahar", ""], ["Huang", "Yunzhi", ""], ["Ma", "Lei", ""], ["Wu", "Zhengwang", ""], ["Li", "Gang", ""], ["Wang", "Li", ""], ["Wang", "Qian", ""], ["Yap", "Pew-Thian", ""], ["Shen", "Dinggang", ""]]}, {"id": "2005.09241", "submitter": "Damien Teney", "authors": "Damien Teney, Kushal Kafle, Robik Shrestha, Ehsan Abbasnejad,\n  Christopher Kanan, Anton van den Hengel", "title": "On the Value of Out-of-Distribution Testing: An Example of Goodhart's\n  Law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Out-of-distribution (OOD) testing is increasingly popular for evaluating a\nmachine learning system's ability to generalize beyond the biases of a training\nset. OOD benchmarks are designed to present a different joint distribution of\ndata and labels between training and test time. VQA-CP has become the standard\nOOD benchmark for visual question answering, but we discovered three troubling\npractices in its current use. First, most published methods rely on explicit\nknowledge of the construction of the OOD splits. They often rely on\n``inverting'' the distribution of labels, e.g. answering mostly 'yes' when the\ncommon training answer is 'no'. Second, the OOD test set is used for model\nselection. Third, a model's in-domain performance is assessed after retraining\nit on in-domain splits (VQA v2) that exhibit a more balanced distribution of\nlabels. These three practices defeat the objective of evaluating\ngeneralization, and put into question the value of methods specifically\ndesigned for this dataset. We show that embarrassingly-simple methods,\nincluding one that generates answers at random, surpass the state of the art on\nsome question types. We provide short- and long-term solutions to avoid these\npitfalls and realize the benefits of OOD evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 06:45:50 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Teney", "Damien", ""], ["Kafle", "Kushal", ""], ["Shrestha", "Robik", ""], ["Abbasnejad", "Ehsan", ""], ["Kanan", "Christopher", ""], ["Hengel", "Anton van den", ""]]}, {"id": "2005.09246", "submitter": "Rajeev Bhatt Ambati", "authors": "Rajeev Bhatt Ambati, Ahmed Ada Hanifi, Ramya Vunikili, Puneet Sharma,\n  and Oladimeji Farri", "title": "Assertion Detection in Multi-Label Clinical Text using Scope\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label sentences (text) in the clinical domain result from the rich\ndescription of scenarios during patient care. The state-of-theart methods for\nassertion detection mostly address this task in the setting of a single\nassertion label per sentence (text). In addition, few rules based and deep\nlearning methods perform negation/assertion scope detection on single-label\ntext. It is a significant challenge extending these methods to address\nmulti-label sentences without diminishing performance. Therefore, we developed\na convolutional neural network (CNN) architecture to localize multiple labels\nand their scopes in a single stage end-to-end fashion, and demonstrate that our\nmodel performs atleast 12% better than the state-of-the-art on multi-label\nclinical text.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 06:56:02 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Ambati", "Rajeev Bhatt", ""], ["Hanifi", "Ahmed Ada", ""], ["Vunikili", "Ramya", ""], ["Sharma", "Puneet", ""], ["Farri", "Oladimeji", ""]]}, {"id": "2005.09257", "submitter": "Jaikai Wang", "authors": "Aishan Liu, Jiakai Wang, Xianglong Liu, Bowen Cao, Chongzhi Zhang,\n  Hang Yu", "title": "Bias-based Universal Adversarial Patch Attack for Automatic Check-out", "comments": "This paper has been accepted on ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are inputs with imperceptible perturbations that easily\nmisleading deep neural networks(DNNs). Recently, adversarial patch, with noise\nconfined to a small and localized patch, has emerged for its easy feasibility\nin real-world scenarios. However, existing strategies failed to generate\nadversarial patches with strong generalization ability. In other words, the\nadversarial patches were input-specific and failed to attack images from all\nclasses, especially unseen ones during training. To address the problem, this\npaper proposes a bias-based framework to generate class-agnostic universal\nadversarial patches with strong generalization ability, which exploits both the\nperceptual and semantic bias of models. Regarding the perceptual bias, since\nDNNs are strongly biased towards textures, we exploit the hard examples which\nconvey strong model uncertainties and extract a textural patch prior from them\nby adopting the style similarities. The patch prior is more close to decision\nboundaries and would promote attacks. To further alleviate the heavy dependency\non large amounts of data in training universal attacks, we further exploit the\nsemantic bias. As the class-wise preference, prototypes are introduced and\npursued by maximizing the multi-class margin to help universal training. Taking\nAutomaticCheck-out (ACO) as the typical scenario, extensive experiments\nincluding white-box and black-box settings in both digital-world(RPC, the\nlargest ACO related dataset) and physical-world scenario(Taobao and JD, the\nworld' s largest online shopping platforms) are conducted. Experimental results\ndemonstrate that our proposed framework outperforms state-of-the-art\nadversarial patch attack methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 07:38:54 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 14:07:54 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 13:06:03 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Liu", "Aishan", ""], ["Wang", "Jiakai", ""], ["Liu", "Xianglong", ""], ["Cao", "Bowen", ""], ["Zhang", "Chongzhi", ""], ["Yu", "Hang", ""]]}, {"id": "2005.09294", "submitter": "Martin Kotuliak", "authors": "Martin Kotuliak, Sandro E. Schoenborn, Andrei Dan", "title": "Synthesizing Unrestricted False Positive Adversarial Objects Using\n  Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are data points misclassified by neural networks.\nOriginally, adversarial examples were limited to adding small perturbations to\na given image. Recent work introduced the generalized concept of unrestricted\nadversarial examples, without limits on the added perturbations. In this paper,\nwe introduce a new category of attacks that create unrestricted adversarial\nexamples for object detection. Our key idea is to generate adversarial objects\nthat are unrelated to the classes identified by the target object detector.\nDifferent from previous attacks, we use off-the-shelf Generative Adversarial\nNetworks (GAN), without requiring any further training or modification. Our\nmethod consists of searching over the latent normal space of the GAN for\nadversarial objects that are wrongly identified by the target object detector.\nWe evaluate this method on the commonly used Faster R-CNN ResNet-101, Inception\nv2 and SSD Mobilenet v1 object detectors using logo generative iWGAN-LC and\nSNGAN trained on CIFAR-10. The empirical results show that the generated\nadversarial objects are indistinguishable from non-adversarial objects\ngenerated by the GANs, transferable between the object detectors and robust in\nthe physical world. This is the first work to study unrestricted false positive\nadversarial examples for object detection.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 08:58:58 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Kotuliak", "Martin", ""], ["Schoenborn", "Sandro E.", ""], ["Dan", "Andrei", ""]]}, {"id": "2005.09305", "submitter": "Chaoxiong Wu", "authors": "Jiaojiao Li, Chaoxiong Wu, Rui Song, Yunsong Li, Fei Liu", "title": "AdaptiveWeighted Attention Network with Camera Spectral Sensitivity\n  Prior for Spectral Reconstruction from RGB Images", "comments": "The 1st ranking on the Clean track and the 3rd place only 1.59106e-4\n  more than the 1st on the Real World track of the NTIRE 2020 Spectral\n  Reconstruction Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent promising effort for spectral reconstruction (SR) focuses on learning\na complicated mapping through using a deeper and wider convolutional neural\nnetworks (CNNs). Nevertheless, most CNN-based SR algorithms neglect to explore\nthe camera spectral sensitivity (CSS) prior and interdependencies among\nintermediate features, thus limiting the representation ability of the network\nand performance of SR. To conquer these issues, we propose a novel adaptive\nweighted attention network (AWAN) for SR, whose backbone is stacked with\nmultiple dual residual attention blocks (DRAB) decorating with long and short\nskip connections to form the dual residual learning. Concretely, we investigate\nan adaptive weighted channel attention (AWCA) module to reallocate channel-wise\nfeature responses via integrating correlations between channels. Furthermore, a\npatch-level second-order non-local (PSNL) module is developed to capture\nlong-range spatial contextual information by second-order non-local operations\nfor more powerful feature representations. Based on the fact that the recovered\nRGB images can be projected by the reconstructed hyperspectral image (HSI) and\nthe given CSS function, we incorporate the discrepancies of the RGB images and\nHSIs as a finer constraint for more accurate reconstruction. Experimental\nresults demonstrate the effectiveness of our proposed AWAN network in terms of\nquantitative comparison and perceptual quality over other state-of-the-art SR\nmethods. In the NTIRE 2020 Spectral Reconstruction Challenge, our entries\nobtain the 1st ranking on the Clean track and the 3rd place on the Real World\ntrack. Codes are available at https://github.com/Deep-imagelab/AWAN.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 09:21:01 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Li", "Jiaojiao", ""], ["Wu", "Chaoxiong", ""], ["Song", "Rui", ""], ["Li", "Yunsong", ""], ["Liu", "Fei", ""]]}, {"id": "2005.09329", "submitter": "Abdul Basit", "authors": "Abdul Basit, Muhammad Akhtar Munir, Mohsen Ali, Arif Mahmood", "title": "Localizing Firearm Carriers by Identifying Human-Object Pairs", "comments": "5 pages, accepted in IEEE ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual identification of gunmen in a crowd is a challenging problem, that\nrequires resolving the association of a person with an object (firearm). We\npresent a novel approach to address this problem, by defining human-object\ninteraction (and non-interaction) bounding boxes. In a given image, human and\nfirearms are separately detected. Each detected human is paired with each\ndetected firearm, allowing us to create a paired bounding box that contains\nboth object and the human. A network is trained to classify these\npaired-bounding-boxes into human carrying the identified firearm or not.\nExtensive experiments were performed to evaluate effectiveness of the\nalgorithm, including exploiting full pose of the human, hand key-points, and\ntheir association with the firearm. The knowledge of spatially localized\nfeatures is key to success of our method by using multi-size proposals with\nadaptive average pooling. We have also extended a previously firearm detection\ndataset, by adding more images and tagging in extended dataset the\nhuman-firearm pairs (including bounding boxes for firearms and gunmen). The\nexperimental results ($AP_{hold} = 78.5$) demonstrate effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 09:50:23 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 09:49:30 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Basit", "Abdul", ""], ["Munir", "Muhammad Akhtar", ""], ["Ali", "Mohsen", ""], ["Mahmood", "Arif", ""]]}, {"id": "2005.09349", "submitter": "Lavsen Dahal", "authors": "Lavsen Dahal, Aayush Kafle, Bishesh Khanal", "title": "Uncertainty Estimation in Deep 2D Echocardiography Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  2D echocardiography is the most common imaging modality for cardiovascular\ndiseases. The portability and relatively low-cost nature of Ultrasound (US)\nenable the US devices needed for performing echocardiography to be made widely\navailable. However, acquiring and interpreting cardiac US images is operator\ndependent, limiting its use to only places where experts are present. Recently,\nDeep Learning (DL) has been used in 2D echocardiography for automated view\nclassification, and structure and function assessment. Although these recent\nworks show promise in developing computer-guided acquisition and automated\ninterpretation of echocardiograms, most of these methods do not model and\nestimate uncertainty which can be important when testing on data coming from a\ndistribution further away from that of the training data. Uncertainty estimates\ncan be beneficial both during the image acquisition phase (by providing\nreal-time feedback to the operator on acquired image's quality), and during\nautomated measurement and interpretation. The performance of uncertainty models\nand quantification metric may depend on the prediction task and the models\nbeing compared. Hence, to gain insight of uncertainty modelling for left\nventricular segmentation from US images, we compare three ensembling based\nuncertainty models quantified using four different metrics (one newly proposed)\non state-of-the-art baseline networks using two publicly available\nechocardiogram datasets. We further demonstrate how uncertainty estimation can\nbe used to automatically reject poor quality images and improve\nstate-of-the-art segmentation results.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 10:19:23 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Dahal", "Lavsen", ""], ["Kafle", "Aayush", ""], ["Khanal", "Bishesh", ""]]}, {"id": "2005.09372", "submitter": "Rituparna Sarkar", "authors": "Rituparna Sarkar, Suvadip Mukherjee, Elisabeth Labruy\\`ere and\n  Jean-Christophe Olivo-Marin", "title": "Learning to segment clustered amoeboid cells from brightfield microscopy\n  via multi-task learning with adaptive weight selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and segmenting individual cells from microscopy images is critical\nto various life science applications. Traditional cell segmentation tools are\noften ill-suited for applications in brightfield microscopy due to poor\ncontrast and intensity heterogeneity, and only a small subset are applicable to\nsegment cells in a cluster. In this regard, we introduce a novel supervised\ntechnique for cell segmentation in a multi-task learning paradigm. A\ncombination of a multi-task loss, based on the region and cell boundary\ndetection, is employed for an improved prediction efficiency of the network.\nThe learning problem is posed in a novel min-max framework which enables\nadaptive estimation of the hyper-parameters in an automatic fashion. The region\nand cell boundary predictions are combined via morphological operations and\nactive contour model to segment individual cells.\n  The proposed methodology is particularly suited to segment touching cells\nfrom brightfield microscopy images without manual interventions.\nQuantitatively, we observe an overall Dice score of 0.93 on the validation set,\nwhich is an improvement of over 15.9% on a recent unsupervised method, and\noutperforms the popular supervised U-net algorithm by at least $5.8\\%$ on\naverage.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 11:31:53 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Sarkar", "Rituparna", ""], ["Mukherjee", "Suvadip", ""], ["Labruy\u00e8re", "Elisabeth", ""], ["Olivo-Marin", "Jean-Christophe", ""]]}, {"id": "2005.09377", "submitter": "EL-Hachemi Guerrout", "authors": "EL-Hachemi Guerrout, Ramdane Mahiou, Dominique Michelucci, Boukabene\n  Randa and Ouali Assia", "title": "hidden markov random fields and cuckoo search method for medical image\n  segmentation", "comments": "5 pages, 2 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Segmentation of medical images is an essential part in the process of\ndiagnostics. Physicians require an automatic, robust and valid results. Hidden\nMarkov Random Fields (HMRF) provide powerful model. This latter models the\nsegmentation problem as the minimization of an energy function. Cuckoo search\n(CS) algorithm is one of the recent nature-inspired meta-heuristic algorithms.\nIt has shown its efficiency in many engineering optimization problems. In this\npaper, we use three cuckoo search algorithm to achieve medical image\nsegmentation.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 11:54:03 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Guerrout", "EL-Hachemi", ""], ["Mahiou", "Ramdane", ""], ["Michelucci", "Dominique", ""], ["Randa", "Boukabene", ""], ["Assia", "Ouali", ""]]}, {"id": "2005.09412", "submitter": "Dmitry Yashunin", "authors": "Dmitry Yashunin, Tamir Baydasov, Roman Vlasov", "title": "MaskFace: multi-task face and landmark detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently in the domain of facial analysis single task approaches for face\ndetection and landmark localization dominate. In this paper we draw attention\nto multi-task models solving both tasks simultaneously. We present a highly\naccurate model for face and landmark detection. The method, called MaskFace,\nextends previous face detection approaches by adding a keypoint prediction\nhead. The new keypoint head adopts ideas of Mask R-CNN by extracting facial\nfeatures with a RoIAlign layer. The keypoint head adds small computational\noverhead in the case of few faces in the image while improving the accuracy\ndramatically. We evaluate MaskFace's performance on a face detection task on\nthe AFW, PASCAL face, FDDB, WIDER FACE datasets and a landmark localization\ntask on the AFLW, 300W datasets. For both tasks MaskFace achieves\nstate-of-the-art results outperforming many of single-task and multi-task\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 13:09:28 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Yashunin", "Dmitry", ""], ["Baydasov", "Tamir", ""], ["Vlasov", "Roman", ""]]}, {"id": "2005.09448", "submitter": "Hans-J\\\"urgen Profitlich", "authors": "Daniel Sonntag, Fabrizio Nunnari, and Hans-J\\\"urgen Profitlich", "title": "The Skincare project, an interactive deep learning system for\n  differential diagnosis of malignant skin lesions. Technical Report", "comments": "20 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A shortage of dermatologists causes long wait times for patients who seek\ndermatologic care. In addition, the diagnostic accuracy of general\npractitioners has been reported to be lower than the accuracy of artificial\nintelligence software. This article describes the Skincare project (H2020, EIT\nDigital). Contributions include enabling technology for clinical decision\nsupport based on interactive machine learning (IML), a reference architecture\ntowards a Digital European Healthcare Infrastructure (also cf. EIT MCPS),\ntechnical components for aggregating digitised patient information, and the\nintegration of decision support technology into clinical test-bed environments.\nHowever, the main contribution is a diagnostic and decision support system in\ndermatology for patients and doctors, an interactive deep learning system for\ndifferential diagnosis of malignant skin lesions. In this article, we describe\nits functionalities and the user interfaces to facilitate machine learning from\nhuman input. The baseline deep learning system, which delivers state-of-the-art\nresults and the potential to augment general practitioners and even\ndermatologists, was developed and validated using de-identified cases from a\ndermatology image data base (ISIC), which has about 20000 cases for development\nand validation, provided by board-certified dermatologists defining the\nreference standard for every case. ISIC allows for differential diagnosis, a\nranked list of eight diagnoses, that is used to plan treatments in the common\nsetting of diagnostic ambiguity. We give an overall description of the outcome\nof the Skincare project, and we focus on the steps to support communication and\ncoordination between humans and machine in IML. This is an integral part of the\ndevelopment of future cognitive assistants in the medical domain, and we\ndescribe the necessary intelligent user interfaces.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 13:51:17 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Sonntag", "Daniel", ""], ["Nunnari", "Fabrizio", ""], ["Profitlich", "Hans-J\u00fcrgen", ""]]}, {"id": "2005.09459", "submitter": "Yihua Sun", "authors": "Yihua Sun", "title": "Review on Computer Vision in Gastric Cancer: Potential Efficient Tools\n  for Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid diagnosis of gastric cancer is a great challenge for clinical doctors.\nDramatic progress of computer vision on gastric cancer has been made recently\nand this review focuses on advances during the past five years. Different\nmethods for data generation and augmentation are presented, and various\napproaches to extract discriminative features compared and evaluated.\nClassification and segmentation techniques are carefully discussed for\nassisting more precise diagnosis and timely treatment. For classification,\nvarious methods have been developed to better proceed specific images, such as\nimages with rotation and estimated real-timely (endoscopy), high resolution\nimages (histopathology), low diagnostic accuracy images (X-ray), poor contrast\nimages of the soft-tissue with cavity (CT) or those images with insufficient\nannotation. For detection and segmentation, traditional methods and machine\nlearning methods are compared. Application of those methods will greatly reduce\nthe labor and time consumption for the diagnosis of gastric cancers.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 16:14:15 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 20:02:05 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Sun", "Yihua", ""]]}, {"id": "2005.09484", "submitter": "Andreas Eitel", "authors": "Andreas Eitel and Nico Hauff and Wolfram Burgard", "title": "Self-supervised Transfer Learning for Instance Segmentation through\n  Physical Interaction", "comments": "Extended version and code release of accepted IROS 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation of unknown objects from images is regarded as relevant\nfor several robot skills including grasping, tracking and object sorting.\nRecent results in computer vision have shown that large hand-labeled datasets\nenable high segmentation performance. To overcome the time-consuming process of\nmanually labeling data for new environments, we present a transfer learning\napproach for robots that learn to segment objects by interacting with their\nenvironment in a self-supervised manner. Our robot pushes unknown objects on a\ntable and uses information from optical flow to create training labels in the\nform of object masks. To achieve this, we fine-tune an existing DeepMask\nnetwork for instance segmentation on the self-labeled training data acquired by\nthe robot. We evaluate our trained network (SelfDeepMask) on a set of real\nimages showing challenging and cluttered scenes with novel objects. Here,\nSelfDeepMask outperforms the DeepMask network trained on the COCO dataset by\n9.5% in average precision. Furthermore, we combine our approach with recent\napproaches for training with noisy labels in order to better cope with induced\nlabel noise.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 14:31:24 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Eitel", "Andreas", ""], ["Hauff", "Nico", ""], ["Burgard", "Wolfram", ""]]}, {"id": "2005.09486", "submitter": "Manh Duong Phung", "authors": "Khai Ky Ly and Manh Duong Phung", "title": "Built Infrastructure Monitoring and Inspection Using UAVs and\n  Vision-based Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents an inspecting system using real-time control unmanned\naerial vehicles (UAVs) to investigate structural surfaces. The system operates\nunder favourable weather conditions to inspect a target structure, which is the\nWentworth light rail base structure in this study. The system includes a drone,\na GoPro HERO4 camera, a controller and a mobile phone. The drone takes off the\nground manually in the testing field to collect the data requiring for later\nanalysis. The images are taken through HERO 4 camera and then transferred in\nreal time to the remote processing unit such as a ground control station by the\nwireless connection established by a Wi-Fi router. An image processing method\nhas been proposed to detect defects or damages such as cracks. The method based\non intensity histogram algorithms to exploit the pixel group related to the\ncrack contained in the low intensity interval. Experiments, simulation and\ncomparisons have been conducted to evaluate the performance and validity of the\nproposed system.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 14:37:48 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Ly", "Khai Ky", ""], ["Phung", "Manh Duong", ""]]}, {"id": "2005.09496", "submitter": "Minesh Mathew", "authors": "Sangeeth Reddy, Minesh Mathew, Lluis Gomez, Marcal Rusinol,\n  Dimosthenis Karatzas. and C.V. Jawahar", "title": "RoadText-1K: Text Detection & Recognition Dataset for Driving Videos", "comments": "to be published in ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceiving text is crucial to understand semantics of outdoor scenes and\nhence is a critical requirement to build intelligent systems for driver\nassistance and self-driving. Most of the existing datasets for text detection\nand recognition comprise still images and are mostly compiled keeping text in\nmind. This paper introduces a new \"RoadText-1K\" dataset for text in driving\nvideos. The dataset is 20 times larger than the existing largest dataset for\ntext in videos. Our dataset comprises 1000 video clips of driving without any\nbias towards text and with annotations for text bounding boxes and\ntranscriptions in every frame. State of the art methods for text detection,\nrecognition and tracking are evaluated on the new dataset and the results\nsignify the challenges in unconstrained driving videos compared to existing\ndatasets. This suggests that RoadText-1K is suited for research and development\nof reading systems, robust enough to be incorporated into more complex\ndownstream tasks like driver assistance and self-driving. The dataset can be\nfound at http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtext-1k\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 14:51:25 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Reddy", "Sangeeth", ""], ["Mathew", "Minesh", ""], ["Gomez", "Lluis", ""], ["Rusinol", "Marcal", ""], ["Karatzas.", "Dimosthenis", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2005.09525", "submitter": "Jacob Whitehill", "authors": "Anand Ramakrishnan and Brian Zylich and Erin Ottmar and Jennifer\n  LoCasale-Crouch and Jacob Whitehill", "title": "Toward Automated Classroom Observation: Multimodal Machine Learning to\n  Estimate CLASS Positive Climate and Negative Climate", "comments": "The authors discovered that the results are not reproducible", "journal-ref": "IEEE Transactions on Affective Computing, 2021", "doi": "10.1109/TAFFC.2021.3059209", "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a multi-modal machine learning-based system, which we\ncall ACORN, to analyze videos of school classrooms for the Positive Climate\n(PC) and Negative Climate (NC) dimensions of the CLASS observation protocol\nthat is widely used in educational research. ACORN uses convolutional neural\nnetworks to analyze spectral audio features, the faces of teachers and\nstudents, and the pixels of each image frame, and then integrates this\ninformation over time using Temporal Convolutional Networks. The audiovisual\nACORN's PC and NC predictions have Pearson correlations of $0.55$ and $0.63$\nwith ground-truth scores provided by expert CLASS coders on the UVA Toddler\ndataset (cross-validation on $n=300$ 15-min video segments), and a purely\nauditory ACORN predicts PC and NC with correlations of $0.36$ and $0.41$ on the\nMET dataset (test set of $n=2000$ videos segments). These numbers are similar\nto inter-coder reliability of human coders. Finally, using Graph Convolutional\nNetworks we make early strides (AUC=$0.70$) toward predicting the specific\nmoments (45-90sec clips) when the PC is particularly weak/strong. Our findings\ninform the design of automatic classroom observation and also more general\nvideo activity recognition and summary recognition systems.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 15:36:32 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 23:02:07 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 15:24:49 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Ramakrishnan", "Anand", ""], ["Zylich", "Brian", ""], ["Ottmar", "Erin", ""], ["LoCasale-Crouch", "Jennifer", ""], ["Whitehill", "Jacob", ""]]}, {"id": "2005.09530", "submitter": "Peter Karkus", "authors": "Peter Karkus, Anelia Angelova, Vincent Vanhoucke, Rico Jonschkowski", "title": "Differentiable Mapping Networks: Learning Structured Map Representations\n  for Sparse Visual Localization", "comments": "ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping and localization, preferably from a small number of observations, are\nfundamental tasks in robotics. We address these tasks by combining spatial\nstructure (differentiable mapping) and end-to-end learning in a novel neural\nnetwork architecture: the Differentiable Mapping Network (DMN). The DMN\nconstructs a spatially structured view-embedding map and uses it for subsequent\nvisual localization with a particle filter. Since the DMN architecture is\nend-to-end differentiable, we can jointly learn the map representation and\nlocalization using gradient descent. We apply the DMN to sparse visual\nlocalization, where a robot needs to localize in a new environment with respect\nto a small number of images from known viewpoints. We evaluate the DMN using\nsimulated environments and a challenging real-world Street View dataset. We\nfind that the DMN learns effective map representations for visual localization.\nThe benefit of spatial structure increases with larger environments, more\nviewpoints for mapping, and when training data is scarce. Project website:\nhttp://sites.google.com/view/differentiable-mapping\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 15:43:39 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Karkus", "Peter", ""], ["Angelova", "Anelia", ""], ["Vanhoucke", "Vincent", ""], ["Jonschkowski", "Rico", ""]]}, {"id": "2005.09531", "submitter": "Tianrui Liu", "authors": "Tianrui Liu, Qingjie Meng, Athanasios Vlontzos, Jeremy Tan, Daniel\n  Rueckert and Bernhard Kainz", "title": "Ultrasound Video Summarization using Deep Reinforcement Learning", "comments": "Accepted by MICCAI'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video is an essential imaging modality for diagnostics, e.g. in ultrasound\nimaging, for endoscopy, or movement assessment. However, video hasn't received\na lot of attention in the medical image analysis community. In the clinical\npractice, it is challenging to utilise raw diagnostic video data efficiently as\nvideo data takes a long time to process, annotate or audit. In this paper we\nintroduce a novel, fully automatic video summarization method that is tailored\nto the needs of medical video data. Our approach is framed as reinforcement\nlearning problem and produces agents focusing on the preservation of important\ndiagnostic information. We evaluate our method on videos from fetal ultrasound\nscreening, where commonly only a small amount of the recorded data is used\ndiagnostically. We show that our method is superior to alternative video\nsummarization methods and that it preserves essential information required by\nclinical diagnostic standards.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 15:44:18 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Liu", "Tianrui", ""], ["Meng", "Qingjie", ""], ["Vlontzos", "Athanasios", ""], ["Tan", "Jeremy", ""], ["Rueckert", "Daniel", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2005.09544", "submitter": "Maxim Maximov", "authors": "Maxim Maximov, Ismail Elezi and Laura Leal-Taix\\'e", "title": "CIAGAN: Conditional Identity Anonymization Generative Adversarial\n  Networks", "comments": "CVPR 2020", "journal-ref": null, "doi": "10.1109/CVPR42600.2020.00549", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unprecedented increase in the usage of computer vision technology in\nsociety goes hand in hand with an increased concern in data privacy. In many\nreal-world scenarios like people tracking or action recognition, it is\nimportant to be able to process the data while taking careful consideration in\nprotecting people's identity. We propose and develop CIAGAN, a model for image\nand video anonymization based on conditional generative adversarial networks.\nOur model is able to remove the identifying characteristics of faces and bodies\nwhile producing high-quality images and videos that can be used for any\ncomputer vision task, such as detection or tracking. Unlike previous methods,\nwe have full control over the de-identification (anonymization) procedure,\nensuring both anonymization as well as diversity. We compare our method to\nseveral baselines and achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 15:56:08 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 17:12:44 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Maximov", "Maxim", ""], ["Elezi", "Ismail", ""], ["Leal-Taix\u00e9", "Laura", ""]]}, {"id": "2005.09619", "submitter": "Andrew Ilyas", "authors": "Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras,\n  Jacob Steinhardt, Aleksander Madry", "title": "Identifying Statistical Bias in Dataset Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dataset replication is a useful tool for assessing whether improvements in\ntest accuracy on a specific benchmark correspond to improvements in models'\nability to generalize reliably. In this work, we present unintuitive yet\nsignificant ways in which standard approaches to dataset replication introduce\nstatistical bias, skewing the resulting observations. We study ImageNet-v2, a\nreplication of the ImageNet dataset on which models exhibit a significant\n(11-14%) drop in accuracy, even after controlling for a standard\nhuman-in-the-loop measure of data quality. We show that after correcting for\nthe identified statistical bias, only an estimated $3.6\\% \\pm 1.5\\%$ of the\noriginal $11.7\\% \\pm 1.0\\%$ accuracy drop remains unaccounted for. We conclude\nwith concrete recommendations for recognizing and avoiding bias in dataset\nreplication. Code for our study is publicly available at\nhttp://github.com/MadryLab/dataset-replication-analysis .\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 17:48:32 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 06:38:04 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Engstrom", "Logan", ""], ["Ilyas", "Andrew", ""], ["Santurkar", "Shibani", ""], ["Tsipras", "Dimitris", ""], ["Steinhardt", "Jacob", ""], ["Madry", "Aleksander", ""]]}, {"id": "2005.09623", "submitter": "Maxim Maximov", "authors": "Maxim Maximov, Kevin Galim and Laura Leal-Taix\\'e", "title": "Focus on defocus: bridging the synthetic to real domain gap for depth\n  estimation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven depth estimation methods struggle with the generalization outside\ntheir training scenes due to the immense variability of the real-world scenes.\nThis problem can be partially addressed by utilising synthetically generated\nimages, but closing the synthetic-real domain gap is far from trivial. In this\npaper, we tackle this issue by using domain invariant defocus blur as direct\nsupervision. We leverage defocus cues by using a permutation invariant\nconvolutional neural network that encourages the network to learn from the\ndifferences between images with a different point of focus. Our proposed\nnetwork uses the defocus map as an intermediate supervisory signal. We are able\nto train our model completely on synthetic data and directly apply it to a wide\nrange of real-world images. We evaluate our model on synthetic and real\ndatasets, showing compelling generalization results and state-of-the-art depth\nprediction.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 17:52:37 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Maximov", "Maxim", ""], ["Galim", "Kevin", ""], ["Leal-Taix\u00e9", "Laura", ""]]}, {"id": "2005.09634", "submitter": "Andrew Loeb", "authors": "George S. Baggs, Paul Guerrier, Andrew Loeb, Jason C. Jones", "title": "Automated Copper Alloy Grain Size Evaluation Using a Deep-learning CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cond-mat.mtrl-sci cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moog Inc. has automated the evaluation of copper (Cu) alloy grain size using\na deep-learning convolutional neural network (CNN). The proof-of-concept\nautomated image acquisition and batch-wise image processing offers the\npotential for significantly reduced labor, improved accuracy of grain\nevaluation, and decreased overall turnaround times for approving Cu alloy bar\nstock for use in flight critical aircraft hardware. A classification accuracy\nof 91.1% on individual sub-images of the Cu alloy coupons was achieved. Process\ndevelopment included minimizing the variation in acquired image color,\nbrightness, and resolution to create a dataset with 12300 sub-images, and then\noptimizing the CNN hyperparameters on this dataset using statistical design of\nexperiments (DoE).\n  Over the development of the automated Cu alloy grain size evaluation, a\ndegree of \"explainability\" in the artificial intelligence (XAI) output was\nrealized, based on the decomposition of the large raw images into many smaller\ndataset sub-images, through the ability to explain the CNN ensemble image\noutput via inspection of the classification results from the individual smaller\nsub-images.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 13:13:38 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Baggs", "George S.", ""], ["Guerrier", "Paul", ""], ["Loeb", "Andrew", ""], ["Jones", "Jason C.", ""]]}, {"id": "2005.09635", "submitter": "Yujun Shen", "authors": "Yujun Shen, Ceyuan Yang, Xiaoou Tang, Bolei Zhou", "title": "InterFaceGAN: Interpreting the Disentangled Face Representation Learned\n  by GANs", "comments": "Accepted by TPAMI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Generative Adversarial Networks (GANs) have made significant\nprogress in face synthesis, there lacks enough understanding of what GANs have\nlearned in the latent representation to map a random code to a photo-realistic\nimage. In this work, we propose a framework called InterFaceGAN to interpret\nthe disentangled face representation learned by the state-of-the-art GAN models\nand study the properties of the facial semantics encoded in the latent space.\nWe first find that GANs learn various semantics in some linear subspaces of the\nlatent space. After identifying these subspaces, we can realistically\nmanipulate the corresponding facial attributes without retraining the model. We\nthen conduct a detailed study on the correlation between different semantics\nand manage to better disentangle them via subspace projection, resulting in\nmore precise control of the attribute manipulation. Besides manipulating the\ngender, age, expression, and presence of eyeglasses, we can even alter the face\npose and fix the artifacts accidentally made by GANs. Furthermore, we perform\nan in-depth face identity analysis and a layer-wise analysis to evaluate the\nediting results quantitatively. Finally, we apply our approach to real face\nediting by employing GAN inversion approaches and explicitly training\nfeed-forward models based on the synthetic data established by InterFaceGAN.\nExtensive experimental results suggest that learning to synthesize faces\nspontaneously brings a disentangled and controllable face representation.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 18:01:22 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 08:36:47 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Shen", "Yujun", ""], ["Yang", "Ceyuan", ""], ["Tang", "Xiaoou", ""], ["Zhou", "Bolei", ""]]}, {"id": "2005.09639", "submitter": "Mohammed Belkhatir", "authors": "F. Fauzi, H. J. Long, M. Belkhatir", "title": "Webpage Segmentation for Extracting Images and Their Surrounding\n  Contextual Information", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.02156", "journal-ref": null, "doi": "10.1145/1631272.1631379", "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web images come in hand with valuable contextual information. Although this\ninformation has long been mined for various uses such as image annotation,\nclustering of images, inference of image semantic content, etc., insufficient\nattention has been given to address issues in mining this contextual\ninformation. In this paper, we propose a webpage segmentation algorithm\ntargeting the extraction of web images and their contextual information based\non their characteristics as they appear on webpages. We conducted a user study\nto obtain a human-labeled dataset to validate the effectiveness of our method\nand experiments demonstrated that our method can achieve better results\ncompared to an existing segmentation algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 19:00:03 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Fauzi", "F.", ""], ["Long", "H. J.", ""], ["Belkhatir", "M.", ""]]}, {"id": "2005.09643", "submitter": "Zhongming Xiang", "authors": "Zhongming Xiang, Ge Ou, Abbas Rashidi", "title": "An Innovative Approach to Determine Rebar Depth and Size by Comparing\n  GPR Data with a Theoretical Database", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": "10", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground penetrating radar (GPR) is an efficient technique used for rapidly\nrecognizing embedded rebar in concrete structures. However, due to the\ndifficulty in extracting signals from GPR data and the intrinsic coupling\nbetween the rebar depth and size showing in the data, simultaneously\ndetermining rebar depth and size is challenging. This paper proposes an\ninnovative algorithm to address this issue. First, the hyperbola signal from\nthe GPR data is identified by direct wave removal, signal reconstruction and\nseparation. Subsequently, a database is developed from a series of theoretical\nhyperbolas and then compared with the extracted hyperbola outlines. Finally,\nthe rebar depth and size are determined by searching for the closest\ncounterpart in the database. The obtained results are very promising and\nindicate that: (1) implementing the method presented in this paper can\ncompletely remove the direct wave noise from the GPR data, and can successfully\nextract the outlines from the interlaced hyperbolas; and (2) the proposed\nmethod can simultaneously determine the rebar depth and size with the accuracy\nof 100% and 95.11%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 03:35:30 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Xiang", "Zhongming", ""], ["Ou", "Ge", ""], ["Rashidi", "Abbas", ""]]}, {"id": "2005.09674", "submitter": "Xi-Le Zhao", "authors": "Meng Ding, Ting-Zhu Huang, Xi-Le Zhao, Tian-Hui Ma", "title": "Tensor completion via nonconvex tensor ring rank minimization with\n  guaranteed convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent studies, the tensor ring (TR) rank has shown high effectiveness in\ntensor completion due to its ability of capturing the intrinsic structure\nwithin high-order tensors. A recently proposed TR rank minimization method is\nbased on the convex relaxation by penalizing the weighted sum of nuclear norm\nof TR unfolding matrices. However, this method treats each singular value\nequally and neglects their physical meanings, which usually leads to suboptimal\nsolutions in practice. In this paper, we propose to use the logdet-based\nfunction as a nonconvex smooth relaxation of the TR rank for tensor completion,\nwhich can more accurately approximate the TR rank and better promote the\nlow-rankness of the solution. To solve the proposed nonconvex model\nefficiently, we develop an alternating direction method of multipliers\nalgorithm and theoretically prove that, under some mild assumptions, our\nalgorithm converges to a stationary point. Extensive experiments on color\nimages, multispectral images, and color videos demonstrate that the proposed\nmethod outperforms several state-of-the-art competitors in both visual and\nquantitative comparison. Key words: nonconvex optimization, tensor ring rank,\nlogdet function, tensor completion, alternating direction method of\nmultipliers.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 03:13:17 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Ding", "Meng", ""], ["Huang", "Ting-Zhu", ""], ["Zhao", "Xi-Le", ""], ["Ma", "Tian-Hui", ""]]}, {"id": "2005.09681", "submitter": "Qi Qian", "authors": "Yuanhong Xu, Qi Qian, Hao Li, Rong Jin, Juhua Hu", "title": "Representation Learning with Fine-grained Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of computational power and techniques for data\ncollection, deep learning demonstrates a superior performance over most of\nexisting algorithms on benchmark data sets. Many efforts have been devoted to\nstudying the mechanism of deep learning. One important observation is that deep\nlearning can learn the discriminative patterns from raw materials directly in a\ntask-dependent manner. Therefore, the representations obtained by deep learning\noutperform hand-crafted features significantly. However, those patterns are\noften learned from super-class labels due to a limited availability of\nfine-grained labels, while fine-grained patterns are desired in many real-world\napplications such as visual search in online shopping. To mitigate the\nchallenge, we propose an algorithm to learn the fine-grained patterns\nsufficiently when only super-class labels are available. The effectiveness of\nour method can be guaranteed with the theoretical analysis. Extensive\nexperiments on real-world data sets demonstrate that the proposed method can\nsignificantly improve the performance on target tasks corresponding to\nfine-grained classes, when only super-class information is available for\ntraining.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 18:05:20 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 21:16:54 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Xu", "Yuanhong", ""], ["Qian", "Qi", ""], ["Li", "Hao", ""], ["Jin", "Rong", ""], ["Hu", "Juhua", ""]]}, {"id": "2005.09704", "submitter": "Zili Yi", "authors": "Zili Yi, Qiang Tang, Shekoofeh Azizi, Daesik Jang, Zhan Xu", "title": "Contextual Residual Aggregation for Ultra High-Resolution Image\n  Inpainting", "comments": "CVPR 2020 oral paper. 22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently data-driven image inpainting methods have made inspiring progress,\nimpacting fundamental image editing tasks such as object removal and damaged\nimage repairing. These methods are more effective than classic approaches,\nhowever, due to memory limitations they can only handle low-resolution inputs,\ntypically smaller than 1K. Meanwhile, the resolution of photos captured with\nmobile devices increases up to 8K. Naive up-sampling of the low-resolution\ninpainted result can merely yield a large yet blurry result. Whereas, adding a\nhigh-frequency residual image onto the large blurry image can generate a sharp\nresult, rich in details and textures. Motivated by this, we propose a\nContextual Residual Aggregation (CRA) mechanism that can produce high-frequency\nresiduals for missing contents by weighted aggregating residuals from\ncontextual patches, thus only requiring a low-resolution prediction from the\nnetwork. Since convolutional layers of the neural network only need to operate\non low-resolution inputs and outputs, the cost of memory and computing power is\nthus well suppressed. Moreover, the need for high-resolution training datasets\nis alleviated. In our experiments, we train the proposed model on small images\nwith resolutions 512x512 and perform inference on high-resolution images,\nachieving compelling inpainting quality. Our model can inpaint images as large\nas 8K with considerable hole sizes, which is intractable with previous\nlearning-based approaches. We further elaborate on the light-weight design of\nthe network architecture, achieving real-time performance on 2K images on a GTX\n1080 Ti GPU. Codes are available at: Atlas200dk/sample-imageinpainting-HiFill.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 18:55:32 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Yi", "Zili", ""], ["Tang", "Qiang", ""], ["Azizi", "Shekoofeh", ""], ["Jang", "Daesik", ""], ["Xu", "Zhan", ""]]}, {"id": "2005.09725", "submitter": "Tuomo Valkonen", "authors": "Kristian Bredies and Tuomo Valkonen", "title": "Inverse problems with second-order Total Generalized Variation\n  constraints", "comments": "Published in 2011 as a conference proceeding. Uploaded in 2020 on\n  arXiv to ensure availability: the original proceedings are no longer online", "journal-ref": "Proceedings of the 9th International Conference on Sampling Theory\n  and Applications (SampTA) 2011, Singapore (2011)", "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Total Generalized Variation (TGV) has recently been introduced as penalty\nfunctional for modelling images with edges as well as smooth variations. It can\nbe interpreted as a \"sparse\" penalization of optimal balancing from the first\nup to the $k$-th distributional derivative and leads to desirable results when\napplied to image denoising, i.e., $L^2$-fitting with TGV penalty. The present\npaper studies TGV of second order in the context of solving ill-posed linear\ninverse problems. Existence and stability for solutions of Tikhonov-functional\nminimization with respect to the data is shown and applied to the problem of\nrecovering an image from blurred and noisy data.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 19:48:28 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Bredies", "Kristian", ""], ["Valkonen", "Tuomo", ""]]}, {"id": "2005.09727", "submitter": "Mohammad Ebrahimpour", "authors": "Mohammad K. Ebrahimpour, Jiayun Li, Yen-Yun Yu, Jackson L. Reese,\n  Azadeh Moghtaderi, Ming-Hsuan Yang, David C. Noelle", "title": "Ventral-Dorsal Neural Networks: Object Detection via Selective Attention", "comments": "in Proceedings of WACV. arXiv admin note: substantial text overlap\n  with arXiv:2005.07787", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have been repeatedly proven to\nperform well on image classification tasks. Object detection methods, however,\nare still in need of significant improvements. In this paper, we propose a new\nframework called Ventral-Dorsal Networks (VDNets) which is inspired by the\nstructure of the human visual system. Roughly, the visual input signal is\nanalyzed along two separate neural streams, one in the temporal lobe and the\nother in the parietal lobe. The coarse functional distinction between these\nstreams is between object recognition -- the \"what\" of the signal -- and\nextracting location related information -- the \"where\" of the signal. The\nventral pathway from primary visual cortex, entering the temporal lobe, is\ndominated by \"what\" information, while the dorsal pathway, into the parietal\nlobe, is dominated by \"where\" information. Inspired by this structure, we\npropose the integration of a \"Ventral Network\" and a \"Dorsal Network\", which\nare complementary. Information about object identity can guide localization,\nand location information can guide attention to relevant image regions,\nimproving object recognition. This new dual network framework sharpens the\nfocus of object detection. Our experimental results reveal that the proposed\nmethod outperforms state-of-the-art object detection approaches on PASCAL VOC\n2007 by 8% (mAP) and PASCAL VOC 2012 by 3% (mAP). Moreover, a comparison of\ntechniques on Yearbook images displays substantial qualitative and quantitative\nbenefits of VDNet.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 23:57:36 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Ebrahimpour", "Mohammad K.", ""], ["Li", "Jiayun", ""], ["Yu", "Yen-Yun", ""], ["Reese", "Jackson L.", ""], ["Moghtaderi", "Azadeh", ""], ["Yang", "Ming-Hsuan", ""], ["Noelle", "David C.", ""]]}, {"id": "2005.09743", "submitter": "Yaser Souri", "authors": "Yaser Souri, Alexander Richard, Luca Minciullo, Juergen Gall", "title": "On Evaluating Weakly Supervised Action Segmentation Methods", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action segmentation is the task of temporally segmenting every frame of an\nuntrimmed video. Weakly supervised approaches to action segmentation,\nespecially from transcripts have been of considerable interest to the computer\nvision community. In this work, we focus on two aspects of the use and\nevaluation of weakly supervised action segmentation approaches that are often\noverlooked: the performance variance over multiple training runs and the impact\nof selecting feature extractors for this task. To tackle the first problem, we\ntrain each method on the Breakfast dataset 5 times and provide average and\nstandard deviation of the results. Our experiments show that the standard\ndeviation over these repetitions is between 1 and 2.5% and significantly\naffects the comparison between different approaches. Furthermore, our\ninvestigation on feature extraction shows that, for the studied\nweakly-supervised action segmentation methods, higher-level I3D features\nperform worse than classical IDT features.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 20:30:31 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 12:56:52 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Souri", "Yaser", ""], ["Richard", "Alexander", ""], ["Minciullo", "Luca", ""], ["Gall", "Juergen", ""]]}, {"id": "2005.09766", "submitter": "Dufan Wu", "authors": "Dufan Wu, Hui Ren, Quanzheng Li", "title": "Self-supervised Dynamic CT Perfusion Image Denoising with Deep Neural\n  Networks", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic computed tomography perfusion (CTP) imaging is a promising approach\nfor acute ischemic stroke diagnosis and evaluation. Hemodynamic parametric maps\nof cerebral parenchyma are calculated from repeated CT scans of the first pass\nof iodinated contrast through the brain. It is necessary to reduce the dose of\nCTP for routine applications due to the high radiation exposure from the\nrepeated scans, where image denoising is necessary to achieve a reliable\ndiagnosis. In this paper, we proposed a self-supervised deep learning method\nfor CTP denoising, which did not require any high-dose reference images for\ntraining. The network was trained by mapping each frame of CTP to an estimation\nfrom its adjacent frames. Because the noise in the source and target was\nindependent, this approach could effectively remove the noise. Being free from\nhigh-dose training images granted the proposed method easier adaptation to\ndifferent scanning protocols. The method was validated on both simulation and a\npublic real dataset. The proposed method achieved improved image quality\ncompared to conventional denoising methods. On the real data, the proposed\nmethod also had improved spatial resolution and contrast-to-noise ratio\ncompared to supervised learning which was trained on the simulation data\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 21:44:07 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Wu", "Dufan", ""], ["Ren", "Hui", ""], ["Li", "Quanzheng", ""]]}, {"id": "2005.09801", "submitter": "Dehong Gao", "authors": "Dehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng Li, Yi Wei, Yi Hu\n  and Hao Wang", "title": "FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal\n  Retrieval", "comments": "10 pages, to be published in SIGIR20 Industry Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the text and image matching in cross-modal\nretrieval of the fashion industry. Different from the matching in the general\ndomain, the fashion matching is required to pay much more attention to the\nfine-grained information in the fashion images and texts. Pioneer approaches\ndetect the region of interests (i.e., RoIs) from images and use the RoI\nembeddings as image representations. In general, RoIs tend to represent the\n\"object-level\" information in the fashion images, while fashion texts are prone\nto describe more detailed information, e.g. styles, attributes. RoIs are thus\nnot fine-grained enough for fashion text and image matching. To this end, we\npropose FashionBERT, which leverages patches as image features. With the\npre-trained BERT model as the backbone network, FashionBERT learns high level\nrepresentations of texts and images. Meanwhile, we propose an adaptive loss to\ntrade off multitask learning in the FashionBERT modeling. Two tasks (i.e., text\nand image matching and cross-modal retrieval) are incorporated to evaluate\nFashionBERT. On the public dataset, experiments demonstrate FashionBERT\nachieves significant improvements in performances than the baseline and\nstate-of-the-art approaches. In practice, FashionBERT is applied in a concrete\ncross-modal retrieval application. We provide the detailed matching performance\nand inference efficiency analysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 00:41:00 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 05:56:10 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Gao", "Dehong", ""], ["Jin", "Linbo", ""], ["Chen", "Ben", ""], ["Qiu", "Minghui", ""], ["Li", "Peng", ""], ["Wei", "Yi", ""], ["Hu", "Yi", ""], ["Wang", "Hao", ""]]}, {"id": "2005.09812", "submitter": "Juan Leon Alcazar", "authors": "Juan Leon Alcazar, Fabian Caba Heilbron, Long Mai, Federico Perazzi,\n  Joon-Young Lee, Pablo Arbelaez, and Bernard Ghanem", "title": "Active Speakers in Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for active speak er detection focus on modeling short-term\naudiovisual information from a single speaker. Although this strategy can be\nenough for addressing single-speaker scenarios, it prevents accurate detection\nwhen the task is to identify who of many candidate speakers are talking. This\npaper introduces the Active Speaker Context, a novel representation that models\nrelationships between multiple speakers over long time horizons. Our Active\nSpeaker Context is designed to learn pairwise and temporal relations from an\nstructured ensemble of audio-visual observations. Our experiments show that a\nstructured feature ensemble already benefits the active speaker detection\nperformance. Moreover, we find that the proposed Active Speaker Context\nimproves the state-of-the-art on the AVA-ActiveSpeaker dataset achieving a mAP\nof 87.1%. We present ablation studies that verify that this result is a direct\nconsequence of our long-term multi-speaker analysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 01:14:23 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Alcazar", "Juan Leon", ""], ["Heilbron", "Fabian Caba", ""], ["Mai", "Long", ""], ["Perazzi", "Federico", ""], ["Lee", "Joon-Young", ""], ["Arbelaez", "Pablo", ""], ["Ghanem", "Bernard", ""]]}, {"id": "2005.09816", "submitter": "Xinya Chen", "authors": "Xinya Chen, Yanrui Bin, Changxin Gao, Nong Sang, and Hao Tang", "title": "Relevant Region Prediction for Crowd Counting", "comments": "accepted by Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting is a concerned and challenging task in computer vision.\nExisting density map based methods excessively focus on the individuals'\nlocalization which harms the crowd counting performance in highly congested\nscenes. In addition, the dependency between the regions of different density is\nalso ignored. In this paper, we propose Relevant Region Prediction (RRP) for\ncrowd counting, which consists of the Count Map and the Region Relation-Aware\nModule (RRAM). Each pixel in the count map represents the number of heads\nfalling into the corresponding local area in the input image, which discards\nthe detailed spatial information and forces the network pay more attention to\ncounting rather than localizing individuals. Based on the Graph Convolutional\nNetwork (GCN), Region Relation-Aware Module is proposed to capture and exploit\nthe important region dependency. The module builds a fully connected directed\ngraph between the regions of different density where each node (region) is\nrepresented by weighted global pooled feature, and GCN is learned to map this\nregion graph to a set of relation-aware regions representations. Experimental\nresults on three datasets show that our method obviously outperforms other\nexisting state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 01:53:24 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Chen", "Xinya", ""], ["Bin", "Yanrui", ""], ["Gao", "Changxin", ""], ["Sang", "Nong", ""], ["Tang", "Hao", ""]]}, {"id": "2005.09829", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Qingsen Yan, Yu zhu, Xianjun Li, Jinqiu Sun, Yanning\n  Zhang", "title": "Attention-based network for low-light image enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The captured images under low light conditions often suffer insufficient\nbrightness and notorious noise. Hence, low-light image enhancement is a key\nchallenging task in computer vision. A variety of methods have been proposed\nfor this task, but these methods often failed in an extreme low-light\nenvironment and amplified the underlying noise in the input image. To address\nsuch a difficult problem, this paper presents a novel attention-based neural\nnetwork to generate high-quality enhanced low-light images from the raw sensor\ndata. Specifically, we first employ attention strategy (i.e. channel attention\nand spatial attention modules) to suppress undesired chromatic aberration and\nnoise. The channel attention module guides the network to refine redundant\ncolour features. The spatial attention module focuses on denoising by taking\nadvantage of the non-local correlation in the image. Furthermore, we propose a\nnew pooling layer, called inverted shuffle layer, which adaptively selects\nuseful information from previous features. Extensive experiments demonstrate\nthe superiority of the proposed network in terms of suppressing the chromatic\naberration and noise artifacts in enhancement, especially when the low-light\nimage has severe noise.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 02:43:02 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 01:55:38 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Zhang", "Cheng", ""], ["Yan", "Qingsen", ""], ["zhu", "Yu", ""], ["Li", "Xianjun", ""], ["Sun", "Jinqiu", ""], ["Zhang", "Yanning", ""]]}, {"id": "2005.09830", "submitter": "Ying Li", "authors": "Ying Li, Lingfei Ma, Zilong Zhong, Fei Liu, Dongpu Cao, Jonathan Li,\n  and Michael A. Chapman", "title": "Deep Learning for LiDAR Point Clouds in Autonomous Driving: A Review", "comments": "21 pages, submitted to IEEE Transactions on Neural Networks and\n  Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the advancement of deep learning in discriminative feature learning\nfrom 3D LiDAR data has led to rapid development in the field of autonomous\ndriving. However, automated processing uneven, unstructured, noisy, and massive\n3D point clouds is a challenging and tedious task. In this paper, we provide a\nsystematic review of existing compelling deep learning architectures applied in\nLiDAR point clouds, detailing for specific tasks in autonomous driving such as\nsegmentation, detection, and classification. Although several published\nresearch papers focus on specific topics in computer vision for autonomous\nvehicles, to date, no general survey on deep learning applied in LiDAR point\nclouds for autonomous vehicles exists. Thus, the goal of this paper is to\nnarrow the gap in this topic. More than 140 key contributions in the recent\nfive years are summarized in this survey, including the milestone 3D deep\narchitectures, the remarkable deep learning applications in 3D semantic\nsegmentation, object detection, and classification; specific datasets,\nevaluation metrics, and the state of the art performance. Finally, we conclude\nthe remaining challenges and future researches.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 03:01:40 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Li", "Ying", ""], ["Ma", "Lingfei", ""], ["Zhong", "Zilong", ""], ["Liu", "Fei", ""], ["Cao", "Dongpu", ""], ["Li", "Jonathan", ""], ["Chapman", "Michael A.", ""]]}, {"id": "2005.09890", "submitter": "Lars Ailo Bongo", "authors": "Tengel Ekrem Skar, Einar Holsb{\\o}, Kristian Svendsen, Lars Ailo Bongo", "title": "Interactive exploration of population scale pharmacoepidemiology\n  datasets", "comments": null, "journal-ref": null, "doi": "10.1145/3388440.3414862", "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population-scale drug prescription data linked with adverse drug reaction\n(ADR) data supports the fitting of models large enough to detect drug use and\nADR patterns that are not detectable using traditional methods on smaller\ndatasets. However, detecting ADR patterns in large datasets requires tools for\nscalable data processing, machine learning for data analysis, and interactive\nvisualization. To our knowledge no existing pharmacoepidemiology tool supports\nall three requirements. We have therefore created a tool for interactive\nexploration of patterns in prescription datasets with millions of samples. We\nuse Spark to preprocess the data for machine learning and for analyses using\nSQL queries. We have implemented models in Keras and the scikit-learn\nframework. The model results are visualized and interpreted using live Python\ncoding in Jupyter. We apply our tool to explore a 384 million prescription data\nset from the Norwegian Prescription Database combined with a 62 million\nprescriptions for elders that were hospitalized. We preprocess the data in two\nminutes, train models in seconds, and plot the results in milliseconds. Our\nresults show the power of combining computational power, short computation\ntimes, and ease of use for analysis of population scale pharmacoepidemiology\ndatasets. The code is open source and available at:\nhttps://github.com/uit-hdl/norpd_prescription_analyses\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 07:34:50 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Skar", "Tengel Ekrem", ""], ["Holsb\u00f8", "Einar", ""], ["Svendsen", "Kristian", ""], ["Bongo", "Lars Ailo", ""]]}, {"id": "2005.09917", "submitter": "Xiawu Zheng", "authors": "Xiawu Zheng, Rongrong Ji, Qiang Wang, Qixiang Ye, Zhenguo Li, Yonghong\n  Tian, Qi Tian", "title": "Rethinking Performance Estimation in Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) remains a challenging problem, which is\nattributed to the indispensable and time-consuming component of performance\nestimation (PE). In this paper, we provide a novel yet systematic rethinking of\nPE in a resource constrained regime, termed budgeted PE (BPE), which precisely\nand effectively estimates the performance of an architecture sampled from an\narchitecture space. Since searching an optimal BPE is extremely time-consuming\nas it requires to train a large number of networks for evaluation, we propose a\nMinimum Importance Pruning (MIP) approach. Given a dataset and a BPE search\nspace, MIP estimates the importance of hyper-parameters using random forest and\nsubsequently prunes the minimum one from the next iteration. In this way, MIP\neffectively prunes less important hyper-parameters to allocate more\ncomputational resource on more important ones, thus achieving an effective\nexploration. By combining BPE with various search algorithms including\nreinforcement learning, evolution algorithm, random search, and differentiable\narchitecture search, we achieve 1, 000x of NAS speed up with a negligible\nperformance drop comparing to the SOTA\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 09:01:44 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Zheng", "Xiawu", ""], ["Ji", "Rongrong", ""], ["Wang", "Qiang", ""], ["Ye", "Qixiang", ""], ["Li", "Zhenguo", ""], ["Tian", "Yonghong", ""], ["Tian", "Qi", ""]]}, {"id": "2005.09927", "submitter": "Alex Bewley", "authors": "Alex Bewley, Pei Sun, Thomas Mensink, Dragomir Anguelov, Cristian\n  Sminchisescu", "title": "Range Conditioned Dilated Convolutions for Scale Invariant 3D Object\n  Detection", "comments": "CoRL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel 3D object detection framework that processes\nLiDAR data directly on its native representation: range images. Benefiting from\nthe compactness of range images, 2D convolutions can efficiently process dense\nLiDAR data of a scene. To overcome scale sensitivity in this perspective view,\na novel range-conditioned dilation (RCD) layer is proposed to dynamically\nadjust a continuous dilation rate as a function of the measured range.\nFurthermore, localized soft range gating combined with a 3D box-refinement\nstage improves robustness in occluded areas, and produces overall more accurate\nbounding box predictions. On the public large-scale Waymo Open Dataset, our\nmethod sets a new baseline for range-based 3D detection, outperforming\nmultiview and voxel-based methods over all ranges with unparalleled performance\nat long range detection.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 09:24:43 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 08:06:26 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 14:52:57 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Bewley", "Alex", ""], ["Sun", "Pei", ""], ["Mensink", "Thomas", ""], ["Anguelov", "Dragomir", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "2005.09964", "submitter": "Yuqing Liu", "authors": "Yuqing Liu, Shiqi Wang, Jian Zhang, Shanshe Wang, Siwei Ma and Wen Gao", "title": "Iterative Network for Image Super-Resolution", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution (SISR), as a traditional ill-conditioned\ninverse problem, has been greatly revitalized by the recent development of\nconvolutional neural networks (CNN). These CNN-based methods generally map a\nlow-resolution image to its corresponding high-resolution version with\nsophisticated network structures and loss functions, showing impressive\nperformances. This paper proposes a substantially different approach relying on\nthe iterative optimization on HR space with an iterative super-resolution\nnetwork (ISRN). We first analyze the observation model of image SR problem,\ninspiring a feasible solution by mimicking and fusing each iteration in a more\ngeneral and efficient manner. Considering the drawbacks of batch normalization,\nwe propose a feature normalization (FNorm) method to regulate the features in\nnetwork. Furthermore, a novel block with F-Norm is developed to improve the\nnetwork representation, termed as FNB. Residual-in-residual structure is\nproposed to form a very deep network, which groups FNBs with a long skip\nconnection for better information delivery and stabling the training phase.\nExtensive experimental results on testing benchmarks with bicubic (BI)\ndegradation show our ISRN can not only recover more structural information, but\nalso achieve competitive or better PSNR/SSIM results with much fewer parameters\ncompared to other works. Besides BI, we simulate the real-world degradation\nwith blur-downscale (BD) and downscalenoise (DN). ISRN and its extension ISRN+\nboth achieve better performance than others with BD and DN degradation models.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 11:11:47 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 05:59:15 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Liu", "Yuqing", ""], ["Wang", "Shiqi", ""], ["Zhang", "Jian", ""], ["Wang", "Shanshe", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "2005.09973", "submitter": "XingJia Pan", "authors": "Xingjia Pan, Yuqiang Ren, Kekai Sheng, Weiming Dong, Haolei Yuan,\n  Xiaowei Guo, Chongyang Ma, Changsheng Xu", "title": "Dynamic Refinement Network for Oriented and Densely Packed Object\n  Detection", "comments": "Accepted by CVPR 2020 as Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection has achieved remarkable progress in the past decade.\nHowever, the detection of oriented and densely packed objects remains\nchallenging because of following inherent reasons: (1) receptive fields of\nneurons are all axis-aligned and of the same shape, whereas objects are usually\nof diverse shapes and align along various directions; (2) detection models are\ntypically trained with generic knowledge and may not generalize well to handle\nspecific objects at test time; (3) the limited dataset hinders the development\non this task. To resolve the first two issues, we present a dynamic refinement\nnetwork that consists of two novel components, i.e., a feature selection module\n(FSM) and a dynamic refinement head (DRH). Our FSM enables neurons to adjust\nreceptive fields in accordance with the shapes and orientations of target\nobjects, whereas the DRH empowers our model to refine the prediction\ndynamically in an object-aware manner. To address the limited availability of\nrelated benchmarks, we collect an extensive and fully annotated dataset,\nnamely, SKU110K-R, which is relabeled with oriented bounding boxes based on\nSKU110K. We perform quantitative evaluations on several publicly available\nbenchmarks including DOTA, HRSC2016, SKU110K, and our own SKU110K-R dataset.\nExperimental results show that our method achieves consistent and substantial\ngains compared with baseline approaches. The code and dataset are available at\nhttps://github.com/Anymake/DRN_CVPR2020.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 11:35:50 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 23:59:58 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Pan", "Xingjia", ""], ["Ren", "Yuqiang", ""], ["Sheng", "Kekai", ""], ["Dong", "Weiming", ""], ["Yuan", "Haolei", ""], ["Guo", "Xiaowei", ""], ["Ma", "Chongyang", ""], ["Xu", "Changsheng", ""]]}, {"id": "2005.09978", "submitter": "Oliver Rippel", "authors": "Oliver Rippel, Leon Weninger, Dorit Merhof", "title": "AutoML Segmentation for 3D Medical Image Data: Contribution to the MSD\n  Challenge 2018", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fueled by recent advances in machine learning, there has been tremendous\nprogress in the field of semantic segmentation for the medical image computing\ncommunity. However, developed algorithms are often optimized and validated by\nhand based on one task only. In combination with small datasets, interpreting\nthe generalizability of the results is often difficult. The Medical\nSegmentation Decathlon challenge addresses this problem, and aims to facilitate\ndevelopment of generalizable 3D semantic segmentation algorithms that require\nno manual parametrization. Such an algorithm was developed and is presented in\nthis paper. It consists of a 3D convolutional neural network with\nencoder-decoder architecture employing residual-connections, skip-connections\nand multi-level generation of predictions. It works on anisotropic\nvoxel-geometries and has anisotropic depth, i.e., the number of downsampling\nsteps is a task-specific parameter. These depths are automatically inferred for\neach task prior to training. By combining this flexible architecture with\non-the-fly data augmentation and little-to-no pre-- or postprocessing,\npromising results could be achieved. The code developed for this challenge will\nbe available online after the final deadline at:\nhttps://github.com/ORippler/MSD_2018\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 11:47:02 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Rippel", "Oliver", ""], ["Weninger", "Leon", ""], ["Merhof", "Dorit", ""]]}, {"id": "2005.09984", "submitter": "Sara Mandelli", "authors": "Sara Mandelli, Fabrizio Argenti, Paolo Bestagini, Massimo Iuliani,\n  Alessandro Piva, Stefano Tubaro", "title": "A Modified Fourier-Mellin Approach for Source Device Identification on\n  Stabilized Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To decide whether a digital video has been captured by a given device,\nmultimedia forensic tools usually exploit characteristic noise traces left by\nthe camera sensor on the acquired frames. This analysis requires that the noise\npattern characterizing the camera and the noise pattern extracted from video\nframes under analysis are geometrically aligned. However, in many practical\nscenarios this does not occur, thus a re-alignment or synchronization has to be\nperformed. Current solutions often require time consuming search of the\nrealignment transformation parameters. In this paper, we propose to overcome\nthis limitation by searching scaling and rotation parameters in the frequency\ndomain. The proposed algorithm tested on real videos from a well-known\nstate-of-the-art dataset shows promising results.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 12:06:40 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Mandelli", "Sara", ""], ["Argenti", "Fabrizio", ""], ["Bestagini", "Paolo", ""], ["Iuliani", "Massimo", ""], ["Piva", "Alessandro", ""], ["Tubaro", "Stefano", ""]]}, {"id": "2005.10033", "submitter": "Nils Gessert", "authors": "Nils Gessert, Marcel Bengs, Matthias Schl\\\"uter, and Alexander\n  Schlaefer", "title": "Deep learning with 4D spatio-temporal data representations for OCT-based\n  force estimation", "comments": "Accepted for publication in Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2020.101730", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the forces acting between instruments and tissue is a challenging\nproblem for robot-assisted minimally-invasive surgery. Recently, numerous\nvision-based methods have been proposed to replace electro-mechanical\napproaches. Moreover, optical coherence tomography (OCT) and deep learning have\nbeen used for estimating forces based on deformation observed in volumetric\nimage data. The method demonstrated the advantage of deep learning with 3D\nvolumetric data over 2D depth images for force estimation. In this work, we\nextend the problem of deep learning-based force estimation to 4D\nspatio-temporal data with streams of 3D OCT volumes. For this purpose, we\ndesign and evaluate several methods extending spatio-temporal deep learning to\n4D which is largely unexplored so far. Furthermore, we provide an in-depth\nanalysis of multi-dimensional image data representations for force estimation,\ncomparing our 4D approach to previous, lower-dimensional methods. Also, we\nanalyze the effect of temporal information and we study the prediction of\nshort-term future force values, which could facilitate safety features. For our\n4D force estimation architectures, we find that efficient decoupling of spatial\nand temporal processing is advantageous. We show that using 4D spatio-temporal\ndata outperforms all previously used data representations with a mean absolute\nerror of 10.7mN. We find that temporal information is valuable for force\nestimation and we demonstrate the feasibility of force prediction.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 13:30:36 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Gessert", "Nils", ""], ["Bengs", "Marcel", ""], ["Schl\u00fcter", "Matthias", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "2005.10034", "submitter": "Yixing Huang", "authors": "Yixing Huang, Alexander Preuhs, Michael Manhart, Guenter Lauritsch,\n  Andreas Maier", "title": "Data Consistent CT Reconstruction from Insufficient Data with Learned\n  Prior Images", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image reconstruction from insufficient data is common in computed tomography\n(CT), e.g., image reconstruction from truncated data, limited-angle data and\nsparse-view data. Deep learning has achieved impressive results in this field.\nHowever, the robustness of deep learning methods is still a concern for\nclinical applications due to the following two challenges: a) With limited\naccess to sufficient training data, a learned deep learning model may not\ngeneralize well to unseen data; b) Deep learning models are sensitive to noise.\nTherefore, the quality of images processed by neural networks only may be\ninadequate. In this work, we investigate the robustness of deep learning in CT\nimage reconstruction by showing false negative and false positive lesion cases.\nSince learning-based images with incorrect structures are likely not consistent\nwith measured projection data, we propose a data consistent reconstruction\n(DCR) method to improve their image quality, which combines the advantages of\ncompressed sensing and deep learning: First, a prior image is generated by deep\nlearning. Afterwards, unmeasured projection data are inpainted by forward\nprojection of the prior image. Finally, iterative reconstruction with\nreweighted total variation regularization is applied, integrating data\nconsistency for measured data and learned prior information for missing data.\nThe efficacy of the proposed method is demonstrated in cone-beam CT with\ntruncated data, limited-angle data and sparse-view data, respectively. For\nexample, for truncated data, DCR achieves a mean root-mean-square error of 24\nHU and a mean structure similarity index of 0.999 inside the field-of-view for\ndifferent patients in the noisy case, while the state-of-the-art U-Net method\nachieves 55 HU and 0.995 respectively for these two metrics.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 13:30:49 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Huang", "Yixing", ""], ["Preuhs", "Alexander", ""], ["Manhart", "Michael", ""], ["Lauritsch", "Guenter", ""], ["Maier", "Andreas", ""]]}, {"id": "2005.10052", "submitter": "Raghavendra Selvan", "authors": "Raghavendra Selvan, Erik B. Dam, Nicki S. Detlefsen, Sofus Rischel,\n  Kaining Sheng, Mads Nielsen, Akshay Pai", "title": "Lung Segmentation from Chest X-rays using Variational Data Imputation", "comments": "Accepted to be presented at the first Workshop on the Art of Learning\n  with Missing Values (Artemiss) hosted by the 37th International Conference on\n  Machine Learning (ICML). Source code, training data and the trained models\n  are available here: https://github.com/raghavian/lungVAE/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulmonary opacification is the inflammation in the lungs caused by many\nrespiratory ailments, including the novel corona virus disease 2019 (COVID-19).\nChest X-rays (CXRs) with such opacifications render regions of lungs\nimperceptible, making it difficult to perform automated image analysis on them.\nIn this work, we focus on segmenting lungs from such abnormal CXRs as part of a\npipeline aimed at automated risk scoring of COVID-19 from CXRs. We treat the\nhigh opacity regions as missing data and present a modified CNN-based image\nsegmentation network that utilizes a deep generative model for data imputation.\nWe train this model on normal CXRs with extensive data augmentation and\ndemonstrate the usefulness of this model to extend to cases with extreme\nabnormalities.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 13:52:03 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 06:12:42 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Selvan", "Raghavendra", ""], ["Dam", "Erik B.", ""], ["Detlefsen", "Nicki S.", ""], ["Rischel", "Sofus", ""], ["Sheng", "Kaining", ""], ["Nielsen", "Mads", ""], ["Pai", "Akshay", ""]]}, {"id": "2005.10053", "submitter": "Rui Zhang", "authors": "Rui Zhang, Conrad Albrecht, Wei Zhang, Xiaodong Cui, Ulrich Finkler,\n  David Kung, Siyuan Lu", "title": "Map Generation from Large Scale Incomplete and Inaccurate Data Labels", "comments": "This paper is accepted by KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately and globally mapping human infrastructure is an important and\nchallenging task with applications in routing, regulation compliance\nmonitoring, and natural disaster response management etc.. In this paper we\npresent progress in developing an algorithmic pipeline and distributed compute\nsystem that automates the process of map creation using high resolution aerial\nimages. Unlike previous studies, most of which use datasets that are available\nonly in a few cities across the world, we utilizes publicly available imagery\nand map data, both of which cover the contiguous United States (CONUS). We\napproach the technical challenge of inaccurate and incomplete training data\nadopting state-of-the-art convolutional neural network architectures such as\nthe U-Net and the CycleGAN to incrementally generate maps with increasingly\nmore accurate and more complete labels of man-made infrastructure such as roads\nand houses. Since scaling the mapping task to CONUS calls for parallelization,\nwe then adopted an asynchronous distributed stochastic parallel gradient\ndescent training scheme to distribute the computational workload onto a cluster\nof GPUs with nearly linear speed-up.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 13:59:43 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Zhang", "Rui", ""], ["Albrecht", "Conrad", ""], ["Zhang", "Wei", ""], ["Cui", "Xiaodong", ""], ["Finkler", "Ulrich", ""], ["Kung", "David", ""], ["Lu", "Siyuan", ""]]}, {"id": "2005.10086", "submitter": "Pablo Blanco-Medina", "authors": "Eduardo Fidalgo Fernandez, Roberto Andr\\'es Vasco Carofilis, Francisco\n  J\\'a\\~nez Martino and Pablo Blanco Medina", "title": "Classifying Suspicious Content in Tor Darknet", "comments": "To be published on the JNIC 2020 Conference. Summary of already\n  published research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the tasks of law enforcement agencies is to find evidence of criminal\nactivity in the Darknet. However, visiting thousands of domains to locate\nvisual information containing illegal acts manually requires a considerable\namount of time and resources. Furthermore, the background of the images can\npose a challenge when performing classification. To solve this problem, in this\npaper, we explore the automatic classification Tor Darknet images using\nSemantic Attention Keypoint Filtering, a strategy that filters non-significant\nfeatures at a pixel level that do not belong to the object of interest, by\ncombining saliency maps with Bag of Visual Words (BoVW). We evaluated SAKF on a\ncustom Tor image dataset against CNN features: MobileNet v1 and Resnet50, and\nBoVW using dense SIFT descriptors, achieving a result of 87.98% accuracy and\noutperforming all other approaches.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 14:49:02 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 15:45:54 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Fernandez", "Eduardo Fidalgo", ""], ["Carofilis", "Roberto Andr\u00e9s Vasco", ""], ["Martino", "Francisco J\u00e1\u00f1ez", ""], ["Medina", "Pablo Blanco", ""]]}, {"id": "2005.10090", "submitter": "Pablo Blanco-Medina", "authors": "Rubel Biswas, Roberto A. Vasco-Carofilis, Eduardo Fidalgo Fernandez,\n  Francisco J\\'a\\~nez Martino and Pablo Blanco Medina", "title": "Perceptual Hashing applied to Tor domains recognition", "comments": "To be published on the JNIC 2020 Conference. Already published\n  research summary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tor darknet hosts different types of illegal content, which are monitored\nby cybersecurity agencies. However, manually classifying Tor content can be\nslow and error-prone. To support this task, we introduce Frequency-Dominant\nNeighborhood Structure (F-DNS), a new perceptual hashing method for\nautomatically classifying domains by their screenshots. First, we evaluated\nF-DNS using images subject to various content preserving operations. We\ncompared them with their original images, achieving better correlation\ncoefficients than other state-of-the-art methods, especially in the case of\nrotation. Then, we applied F-DNS to categorize Tor domains using the Darknet\nUsage Service Images-2K (DUSI-2K), a dataset with screenshots of active Tor\nservice domains. Finally, we measured the performance of F-DNS against an image\nclassification approach and a state-of-the-art hashing method. Our proposal\nobtained 98.75% accuracy in Tor images, surpassing all other methods compared.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 14:53:36 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 15:35:25 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Biswas", "Rubel", ""], ["Vasco-Carofilis", "Roberto A.", ""], ["Fernandez", "Eduardo Fidalgo", ""], ["Martino", "Francisco J\u00e1\u00f1ez", ""], ["Medina", "Pablo Blanco", ""]]}, {"id": "2005.10091", "submitter": "Kashyap Chitta", "authors": "Aseem Behl, Kashyap Chitta, Aditya Prakash, Eshed Ohn-Bar, Andreas\n  Geiger", "title": "Label Efficient Visual Abstractions for Autonomous Driving", "comments": "International Conference on Intelligent Robots and Systems (IROS),\n  2020. First two authors contributed equally, listed in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that semantic segmentation can be used as an effective\nintermediate representation for learning driving policies. However, the task of\nstreet scene semantic segmentation requires expensive annotations. Furthermore,\nsegmentation algorithms are often trained irrespective of the actual driving\ntask, using auxiliary image-space loss functions which are not guaranteed to\nmaximize driving metrics such as safety or distance traveled per intervention.\nIn this work, we seek to quantify the impact of reducing segmentation\nannotation costs on learned behavior cloning agents. We analyze several\nsegmentation-based intermediate representations. We use these visual\nabstractions to systematically study the trade-off between annotation\nefficiency and driving performance, i.e., the types of classes labeled, the\nnumber of image samples used to learn the visual abstraction model, and their\ngranularity (e.g., object masks vs. 2D bounding boxes). Our analysis uncovers\nseveral practical insights into how segmentation-based visual abstractions can\nbe exploited in a more label efficient manner. Surprisingly, we find that\nstate-of-the-art driving performance can be achieved with orders of magnitude\nreduction in annotation cost. Beyond label efficiency, we find several\nadditional training benefits when leveraging visual abstractions, such as a\nsignificant reduction in the variance of the learned policy when compared to\nstate-of-the-art end-to-end driving models.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 14:53:54 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 16:57:52 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Behl", "Aseem", ""], ["Chitta", "Kashyap", ""], ["Prakash", "Aditya", ""], ["Ohn-Bar", "Eshed", ""], ["Geiger", "Andreas", ""]]}, {"id": "2005.10098", "submitter": "Pablo Blanco-Medina", "authors": "Pablo Blanco Medina, Eduardo Fidalgo Fernandez, Enrique Alegre,\n  Francisco J\\'a\\~nez Martino, Roberto A. Vasco-Carofilis and V\\'ictor Fidalgo\n  Villar", "title": "Classification of Industrial Control Systems screenshots using Transfer\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial Control Systems depend heavily on security and monitoring\nprotocols. Several tools are available for this purpose, which scout\nvulnerabilities and take screenshots from various control panels for later\nanalysis. However, they do not adequately classify images into specific control\ngroups, which can difficult operations performed by manual operators. In order\nto solve this problem, we use transfer learning with five CNN architectures,\npre-trained on Imagenet, to determine which one best classifies screenshots\nobtained from Industrial Controls Systems. Using 337 manually labeled images,\nwe train these architectures and study their performance both in accuracy and\nCPU and GPU time. We find out that MobilenetV1 is the best architecture based\non its 97,95% of F1-Score, and its speed on CPU with 0.47 seconds per image. In\nsystems where time is critical and GPU is available, VGG16 is preferable\nbecause it takes 0.04 seconds to process images, but dropping performance to\n87,67%.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 15:00:55 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 15:33:56 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2020 10:16:42 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Medina", "Pablo Blanco", ""], ["Fernandez", "Eduardo Fidalgo", ""], ["Alegre", "Enrique", ""], ["Martino", "Francisco J\u00e1\u00f1ez", ""], ["Vasco-Carofilis", "Roberto A.", ""], ["Villar", "V\u00edctor Fidalgo", ""]]}, {"id": "2005.10149", "submitter": "Abhinaba Roy", "authors": "Abhinaba Roy, Biplab Banerjee, Amir Hussain, Soujanya Poria", "title": "Discriminative Dictionary Design for Action Classification in Still\n  Images and Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of action recognition from still images\nand videos. Traditional local features such as SIFT, STIP etc. invariably pose\ntwo potential problems: 1) they are not evenly distributed in different\nentities of a given category and 2) many of such features are not exclusive of\nthe visual concept the entities represent. In order to generate a dictionary\ntaking the aforementioned issues into account, we propose a novel\ndiscriminative method for identifying robust and category specific local\nfeatures which maximize the class separability to a greater extent.\nSpecifically, we pose the selection of potent local descriptors as filtering\nbased feature selection problem which ranks the local features per category\nbased on a novel measure of distinctiveness. The underlying visual entities are\nsubsequently represented based on the learned dictionary and this stage is\nfollowed by action classification using the random forest model followed by\nlabel propagation refinement. The framework is validated on the action\nrecognition datasets based on still images (Stanford-40) as well as videos\n(UCF-50) and exhibits superior performances than the representative methods\nfrom the literature.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 15:56:41 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 17:36:11 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Roy", "Abhinaba", ""], ["Banerjee", "Biplab", ""], ["Hussain", "Amir", ""], ["Poria", "Soujanya", ""]]}, {"id": "2005.10220", "submitter": "Anush Sankaran", "authors": "Naveen Panwar, Tarun Tater, Anush Sankaran, Senthil Mani", "title": "Reducing Overlearning through Disentangled Representations by\n  Suppressing Unknown Tasks", "comments": "Added appendix with additional results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep learning approaches for learning visual features tend to\noverlearn and extract more information than what is required for the task at\nhand. From a privacy preservation perspective, the input visual information is\nnot protected from the model; enabling the model to become more intelligent\nthan it is trained to be. Current approaches for suppressing additional task\nlearning assume the presence of ground truth labels for the tasks to be\nsuppressed during training time. In this research, we propose a three-fold\nnovel contribution: (i) a model-agnostic solution for reducing model\noverlearning by suppressing all the unknown tasks, (ii) a novel metric to\nmeasure the trust score of a trained deep learning model, and (iii) a simulated\nbenchmark dataset, PreserveTask, having five different fundamental image\nclassification tasks to study the generalization nature of models. In the first\nset of experiments, we learn disentangled representations and suppress\noverlearning of five popular deep learning models: VGG16, VGG19, Inception-v1,\nMobileNet, and DenseNet on PreserverTask dataset. Additionally, we show results\nof our framework on color-MNIST dataset and practical applications of face\nattribute preservation in Diversity in Faces (DiF) and IMDB-Wiki dataset.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 17:31:44 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Panwar", "Naveen", ""], ["Tater", "Tarun", ""], ["Sankaran", "Anush", ""], ["Mani", "Senthil", ""]]}, {"id": "2005.10222", "submitter": "Jincheng Zhang", "authors": "Jincheng Zhang, Andrew R. Willis and Jamie Godwin", "title": "Compute-Bound and Low-Bandwidth Distributed 3D Graph-SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a new approach for distributed 3D SLAM map building.\nThe key contribution of this article is the creation of a distributed\ngraph-SLAM map-building architecture responsive to bandwidth and computational\nneeds of the robotic platform. Responsiveness is afforded by the integration of\na 3D point cloud to plane cloud compression algorithm that approximates dense\n3D point cloud using local planar patches. Compute bound platforms may restrict\nthe computational duration of the compression algorithm and low-bandwidth\nplatforms can restrict the size of the compression result. The backbone of the\napproach is an ultra-fast adaptive 3D compression algorithm that transforms\nswaths of 3D planar surface data into planar patches attributed with image\ntextures. Our approach uses DVO SLAM, a leading algorithm for 3D mapping, and\nextends it by computationally isolating map integration tasks from local\nGuidance, Navigation, and Control tasks and includes an addition of a network\nprotocol to share the compressed plane clouds. The joint effect of these\ncontributions allows agents with 3D sensing capabilities to calculate and\ncommunicate compressed map information commensurate with their onboard\ncomputational resources and communication channel capacities. This opens SLAM\nmapping to new categories of robotic platforms that may have computational and\nmemory limits that prohibit other SLAM solutions.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 17:39:55 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Zhang", "Jincheng", ""], ["Willis", "Andrew R.", ""], ["Godwin", "Jamie", ""]]}, {"id": "2005.10229", "submitter": "Yue Zhao", "authors": "Dian Shao, Yue Zhao, Bo Dai and Dahua Lin", "title": "Intra- and Inter-Action Understanding via Temporal Action Parsing", "comments": "CVPR 2020 Poster; Project page: https://sdolivia.github.io/TAPOS/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for action recognition primarily rely on deep convolutional\nnetworks to derive feature embeddings of visual and motion features. While\nthese methods have demonstrated remarkable performance on standard benchmarks,\nwe are still in need of a better understanding as to how the videos, in\nparticular their internal structures, relate to high-level semantics, which may\nlead to benefits in multiple aspects, e.g. interpretable predictions and even\nnew methods that can take the recognition performances to a next level. Towards\nthis goal, we construct TAPOS, a new dataset developed on sport videos with\nmanual annotations of sub-actions, and conduct a study on temporal action\nparsing on top. Our study shows that a sport activity usually consists of\nmultiple sub-actions and that the awareness of such temporal structures is\nbeneficial to action recognition. We also investigate a number of temporal\nparsing methods, and thereon devise an improved method that is capable of\nmining sub-actions from training data without knowing the labels of them. On\nthe constructed TAPOS, the proposed method is shown to reveal intra-action\ninformation, i.e. how action instances are made of sub-actions, and\ninter-action information, i.e. one specific sub-action may commonly appear in\nvarious actions.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 17:45:18 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Shao", "Dian", ""], ["Zhao", "Yue", ""], ["Dai", "Bo", ""], ["Lin", "Dahua", ""]]}, {"id": "2005.10242", "submitter": "Tongzhou Wang", "authors": "Tongzhou Wang, Phillip Isola", "title": "Understanding Contrastive Representation Learning through Alignment and\n  Uniformity on the Hypersphere", "comments": "International Conference on Machine Learning (ICML), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive representation learning has been outstandingly successful in\npractice. In this work, we identify two key properties related to the\ncontrastive loss: (1) alignment (closeness) of features from positive pairs,\nand (2) uniformity of the induced distribution of the (normalized) features on\nthe hypersphere. We prove that, asymptotically, the contrastive loss optimizes\nthese properties, and analyze their positive effects on downstream tasks.\nEmpirically, we introduce an optimizable metric to quantify each property.\nExtensive experiments on standard vision and language datasets confirm the\nstrong agreement between both metrics and downstream task performance.\nRemarkably, directly optimizing for these two metrics leads to representations\nwith comparable or better performance at downstream tasks than contrastive\nlearning.\n  Project Page: https://ssnl.github.io/hypersphere\n  Code: https://github.com/SsnL/align_uniform ,\nhttps://github.com/SsnL/moco_align_uniform\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 17:59:57 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 19:20:12 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 05:55:54 GMT"}, {"version": "v4", "created": "Mon, 1 Jun 2020 05:16:32 GMT"}, {"version": "v5", "created": "Wed, 17 Jun 2020 04:47:28 GMT"}, {"version": "v6", "created": "Mon, 24 Aug 2020 23:22:08 GMT"}, {"version": "v7", "created": "Sun, 1 Nov 2020 19:27:13 GMT"}, {"version": "v8", "created": "Sat, 7 Nov 2020 04:28:08 GMT"}, {"version": "v9", "created": "Tue, 10 Nov 2020 07:05:17 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Wang", "Tongzhou", ""], ["Isola", "Phillip", ""]]}, {"id": "2005.10243", "submitter": "Yonglong Tian", "authors": "Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid,\n  Phillip Isola", "title": "What Makes for Good Views for Contrastive Learning?", "comments": "NeurIPS 2020. Project page: https://hobbitlong.github.io/InfoMin/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning between multiple views of the data has recently achieved\nstate of the art performance in the field of self-supervised representation\nlearning. Despite its success, the influence of different view choices has been\nless studied. In this paper, we use theoretical and empirical analysis to\nbetter understand the importance of view selection, and argue that we should\nreduce the mutual information (MI) between views while keeping task-relevant\ninformation intact. To verify this hypothesis, we devise unsupervised and\nsemi-supervised frameworks that learn effective views by aiming to reduce their\nMI. We also consider data augmentation as a way to reduce MI, and show that\nincreasing data augmentation indeed leads to decreasing MI and improves\ndownstream classification accuracy. As a by-product, we achieve a new\nstate-of-the-art accuracy on unsupervised pre-training for ImageNet\nclassification ($73\\%$ top-1 linear readout with a ResNet-50). In addition,\ntransferring our models to PASCAL VOC object detection and COCO instance\nsegmentation consistently outperforms supervised pre-training.\nCode:http://github.com/HobbitLong/PyContrast\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 17:59:57 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 18:59:33 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 10:01:34 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Tian", "Yonglong", ""], ["Sun", "Chen", ""], ["Poole", "Ben", ""], ["Krishnan", "Dilip", ""], ["Schmid", "Cordelia", ""], ["Isola", "Phillip", ""]]}, {"id": "2005.10247", "submitter": "Alexander Robey", "authors": "Alexander Robey, Hamed Hassani, George J. Pappas", "title": "Model-Based Robust Deep Learning: Generalizing to Natural,\n  Out-of-Distribution Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has resulted in major breakthroughs in many application\ndomains, the frameworks commonly used in deep learning remain fragile to\nartificially-crafted and imperceptible changes in the data. In response to this\nfragility, adversarial training has emerged as a principled approach for\nenhancing the robustness of deep learning with respect to norm-bounded\nperturbations. However, there are other sources of fragility for deep learning\nthat are arguably more common and less thoroughly studied. Indeed, natural\nvariation such as lighting or weather conditions can significantly degrade the\naccuracy of trained neural networks, proving that such natural variation\npresents a significant challenge for deep learning.\n  In this paper, we propose a paradigm shift from perturbation-based\nadversarial robustness toward model-based robust deep learning. Our objective\nis to provide general training algorithms that can be used to train deep neural\nnetworks to be robust against natural variation in data. Critical to our\nparadigm is first obtaining a model of natural variation which can be used to\nvary data over a range of natural conditions. Such models may be either known a\npriori or else learned from data. In the latter case, we show that deep\ngenerative models can be used to learn models of natural variation that are\nconsistent with realistic conditions. We then exploit such models in three\nnovel model-based robust training algorithms in order to enhance the robustness\nof deep learning with respect to the given model. Our extensive experiments\nshow that across a variety of naturally-occurring conditions and across various\ndatasets, deep neural networks trained with our model-based algorithms\nsignificantly outperform both standard deep learning algorithms as well as\nnorm-bounded robust deep learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 13:46:31 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 13:20:37 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Robey", "Alexander", ""], ["Hassani", "Hamed", ""], ["Pappas", "George J.", ""]]}, {"id": "2005.10266", "submitter": "Liang-Chieh Chen", "authors": "Liang-Chieh Chen, Raphael Gontijo Lopes, Bowen Cheng, Maxwell D.\n  Collins, Ekin D. Cubuk, Barret Zoph, Hartwig Adam, Jonathon Shlens", "title": "Naive-Student: Leveraging Semi-Supervised Learning in Video Sequences\n  for Urban Scene Segmentation", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning in large discriminative models is a mainstay for modern\ncomputer vision. Such an approach necessitates investing in large-scale\nhuman-annotated datasets for achieving state-of-the-art results. In turn, the\nefficacy of supervised learning may be limited by the size of the human\nannotated dataset. This limitation is particularly notable for image\nsegmentation tasks, where the expense of human annotation is especially large,\nyet large amounts of unlabeled data may exist. In this work, we ask if we may\nleverage semi-supervised learning in unlabeled video sequences and extra images\nto improve the performance on urban scene segmentation, simultaneously tackling\nsemantic, instance, and panoptic segmentation. The goal of this work is to\navoid the construction of sophisticated, learned architectures specific to\nlabel propagation (e.g., patch matching and optical flow). Instead, we simply\npredict pseudo-labels for the unlabeled data and train subsequent models with\nboth human-annotated and pseudo-labeled data. The procedure is iterated for\nseveral times. As a result, our Naive-Student model, trained with such simple\nyet effective iterative semi-supervised learning, attains state-of-the-art\nresults at all three Cityscapes benchmarks, reaching the performance of 67.8%\nPQ, 42.6% AP, and 85.2% mIOU on the test set. We view this work as a notable\nstep towards building a simple procedure to harness unlabeled video sequences\nand extra images to surpass state-of-the-art performance on core computer\nvision tasks.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 18:00:05 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 04:38:50 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 16:29:10 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2020 03:40:38 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chen", "Liang-Chieh", ""], ["Lopes", "Raphael Gontijo", ""], ["Cheng", "Bowen", ""], ["Collins", "Maxwell D.", ""], ["Cubuk", "Ekin D.", ""], ["Zoph", "Barret", ""], ["Adam", "Hartwig", ""], ["Shlens", "Jonathon", ""]]}, {"id": "2005.10310", "submitter": "Jincheng Zhang", "authors": "Kevin M. Brink, Jincheng Zhang, Andrew R. Willis, Ryan E. Sherrill,\n  Jamie L. Godwin", "title": "Maplets: An Efficient Approach for Cooperative SLAM Map Building Under\n  Communication and Computation Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces an approach to facilitate cooperative exploration and\nmapping of large-scale, near-ground, underground, or indoor spaces via a novel\nintegration framework for locally-dense agent map data. The effort targets\nlimited Size, Weight, and Power (SWaP) agents with an emphasis on limiting\nrequired communications and redundant processing. The approach uses a unique\norganization of batch optimization engines to enable a highly efficient\ntwo-tier optimization structure. Tier I consist of agents that create and\npotentially share local maplets (local maps, limited in size) which are\ngenerated using Simultaneous Localization and Mapping (SLAM) map-building\nsoftware and then marginalized to a more compact parameterization. Maplets are\ngenerated in an overlapping manner and used to estimate the transform and\nuncertainty between those overlapping maplets, providing accurate and compact\nodometry or delta-pose representation between maplet's local frames. The delta\nposes can be shared between agents, and in cases where maplets have salient\nfeatures (for loop closures), the compact representation of the maplet can also\nbe shared.\n  The second optimization tier consists of a global optimizer that seeks to\noptimize those maplet-to-maplet transformations, including any loop closures\nidentified. This can provide an accurate global \"skeleton\"' of the traversed\nspace without operating on the high-density point cloud. This compact version\nof the map data allows for scalable, cooperative exploration with limited\ncommunication requirements where most of the individual maplets, or low\nfidelity renderings, are only shared if desired.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 18:49:31 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Brink", "Kevin M.", ""], ["Zhang", "Jincheng", ""], ["Willis", "Andrew R.", ""], ["Sherrill", "Ryan E.", ""], ["Godwin", "Jamie L.", ""]]}, {"id": "2005.10326", "submitter": "Apostolia Tsirikoglou", "authors": "Apostolia Tsirikoglou, Karin Stacke, Gabriel Eilertsen, Martin\n  Lindvall, Jonas Unger", "title": "A Study of Deep Learning Colon Cancer Detection in Limited Data Access\n  Scenarios", "comments": "Presented at the ICLR 2020 Workshop on AI for Overcoming Global\n  Disparities in Cancer Care (AI4CC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitization of histopathology slides has led to several advances, from easy\ndata sharing and collaborations to the development of digital diagnostic tools.\nDeep learning (DL) methods for classification and detection have shown great\npotential, but often require large amounts of training data that are hard to\ncollect, and annotate. For many cancer types, the scarceness of data creates\nbarriers for training DL models. One such scenario relates to detecting tumor\nmetastasis in lymph node tissue, where the low ratio of tumor to non-tumor\ncells makes the diagnostic task hard and time-consuming. DL-based tools can\nallow faster diagnosis, with potentially increased quality. Unfortunately, due\nto the sparsity of tumor cells, annotating this type of data demands a high\nlevel of effort from pathologists. Using weak annotations from slide-level\nimages have shown great potential, but demand access to a substantial amount of\ndata as well. In this study, we investigate mitigation strategies for limited\ndata access scenarios. Particularly, we address whether it is possible to\nexploit mutual structure between tissues to develop general techniques, wherein\ndata from one type of cancer in a particular tissue could have diagnostic value\nfor other cancers in other tissues. Our case is exemplified by a DL model for\nmetastatic colon cancer detection in lymph nodes. Could such a model be trained\nwith little or even no lymph node data? As alternative data sources, we\ninvestigate 1) tumor cells taken from the primary colon tumor tissue, and 2)\ncancer data from a different organ (breast), either as is or transformed to the\ntarget domain (colon) using Cycle-GANs. We show that the suggested approaches\nmake it possible to detect cancer metastasis with no or very little lymph node\ndata, opening up for the possibility that existing, annotated histopathology\ndata could generalize to other domains.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 19:28:07 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 09:03:54 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Tsirikoglou", "Apostolia", ""], ["Stacke", "Karin", ""], ["Eilertsen", "Gabriel", ""], ["Lindvall", "Martin", ""], ["Unger", "Jonas", ""]]}, {"id": "2005.10329", "submitter": "Hui-Po Wang", "authors": "Hui-Po Wang, Tribhuvanesh Orekondy, Mario Fritz", "title": "InfoScrub: Towards Attribute Privacy by Targeted Obfuscation", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal photos of individuals when shared online, apart from exhibiting a\nmyriad of memorable details, also reveals a wide range of private information\nand potentially entails privacy risks (e.g., online harassment, tracking). To\nmitigate such risks, it is crucial to study techniques that allow individuals\nto limit the private information leaked in visual data. We tackle this problem\nin a novel image obfuscation framework: to maximize entropy on inferences over\ntargeted privacy attributes, while retaining image fidelity. We approach the\nproblem based on an encoder-decoder style architecture, with two key novelties:\n(a) introducing a discriminator to perform bi-directional translation\nsimultaneously from multiple unpaired domains; (b) predicting an image\ninterpolation which maximizes uncertainty over a target set of attributes. We\nfind our approach generates obfuscated images faithful to the original input\nimages, and additionally increase uncertainty by 6.2$\\times$ (or up to 0.85\nbits) over the non-obfuscated counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 19:48:04 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 13:55:02 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wang", "Hui-Po", ""], ["Orekondy", "Tribhuvanesh", ""], ["Fritz", "Mario", ""]]}, {"id": "2005.10349", "submitter": "Benjamin Dutton", "authors": "Benjamin Dutton", "title": "Adversarial Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical Correlation Analysis (CCA) is a statistical technique used to\nextract common information from multiple data sources or views. It has been\nused in various representation learning problems, such as dimensionality\nreduction, word embedding, and clustering. Recent work has given CCA\nprobabilistic footing in a deep learning context and uses a variational lower\nbound for the data log likelihood to estimate model parameters. Alternatively,\nadversarial techniques have arisen in recent years as a powerful alternative to\nvariational Bayesian methods in autoencoders. In this work, we explore\nstraightforward adversarial alternatives to recent work in Deep Variational CCA\n(VCCA and VCCA-Private) we call ACCA and ACCA-Private and show how these\napproaches offer a stronger and more flexible way to match the approximate\nposteriors coming from encoders to much larger classes of priors than the VCCA\nand VCCA-Private models. This allows new priors for what constitutes a good\nrepresentation, such as disentangling underlying factors of variation, to be\nmore directly pursued. We offer further analysis on the multi-level\ndisentangling properties of VCCA-Private and ACCA-Private through the use of a\nnewly designed dataset we call Tangled MNIST. We also design a validation\ncriteria for these models that is theoretically grounded, task-agnostic, and\nworks well in practice. Lastly, we fill a minor research gap by deriving an\nadditional variational lower bound for VCCA that allows the representation to\nuse view-specific information from both input views.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 20:46:35 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 21:31:21 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Dutton", "Benjamin", ""]]}, {"id": "2005.10353", "submitter": "Yijun Zhou", "authors": "Yijun Zhou, James Gregson", "title": "WHENet: Real-time Fine-Grained Estimation for Wide Range Head Pose", "comments": "Accepted at BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end head-pose estimation network designed to predict\nEuler angles through the full range head yaws from a single RGB image. Existing\nmethods perform well for frontal views but few target head pose from all\nviewpoints. This has applications in autonomous driving and retail. Our network\nbuilds on multi-loss approaches with changes to loss functions and training\nstrategies adapted to wide range estimation. Additionally, we extract ground\ntruth labelings of anterior views from a current panoptic dataset for the first\ntime. The resulting Wide Headpose Estimation Network (WHENet) is the first\nfine-grained modern method applicable to the full-range of head yaws (hence\nwide) yet also meets or beats state-of-the-art methods for frontal head pose\nestimation. Our network is compact and efficient for mobile devices and\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 20:53:01 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 22:54:45 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Zhou", "Yijun", ""], ["Gregson", "James", ""]]}, {"id": "2005.10356", "submitter": "Achal Dave", "authors": "Achal Dave, Tarasha Khurana, Pavel Tokmakov, Cordelia Schmid, Deva\n  Ramanan", "title": "TAO: A Large-Scale Benchmark for Tracking Any Object", "comments": "Project page: http://taodataset.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many years, multi-object tracking benchmarks have focused on a handful of\ncategories. Motivated primarily by surveillance and self-driving applications,\nthese datasets provide tracks for people, vehicles, and animals, ignoring the\nvast majority of objects in the world. By contrast, in the related field of\nobject detection, the introduction of large-scale, diverse datasets (e.g.,\nCOCO) have fostered significant progress in developing highly robust solutions.\nTo bridge this gap, we introduce a similarly diverse dataset for Tracking Any\nObject (TAO). It consists of 2,907 high resolution videos, captured in diverse\nenvironments, which are half a minute long on average. Importantly, we adopt a\nbottom-up approach for discovering a large vocabulary of 833 categories, an\norder of magnitude more than prior tracking benchmarks. To this end, we ask\nannotators to label objects that move at any point in the video, and give names\nto them post factum. Our vocabulary is both significantly larger and\nqualitatively different from existing tracking datasets. To ensure scalability\nof annotation, we employ a federated approach that focuses manual effort on\nlabeling tracks for those relevant objects in a video (e.g., those that move).\nWe perform an extensive evaluation of state-of-the-art trackers and make a\nnumber of important discoveries regarding large-vocabulary tracking in an\nopen-world. In particular, we show that existing single- and multi-object\ntrackers struggle when applied to this scenario in the wild, and that\ndetection-based, multi-object trackers are in fact competitive with\nuser-initialized ones. We hope that our dataset and analysis will boost further\nprogress in the tracking community.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 21:07:28 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Dave", "Achal", ""], ["Khurana", "Tarasha", ""], ["Tokmakov", "Pavel", ""], ["Schmid", "Cordelia", ""], ["Ramanan", "Deva", ""]]}, {"id": "2005.10360", "submitter": "Gereon Fox", "authors": "Gereon Fox, Wentao Liu, Hyeongwoo Kim, Hans-Peter Seidel, Mohamed\n  Elgharib, Christian Theobalt", "title": "VideoForensicsHQ: Detecting High-quality Manipulated Face Videos", "comments": "ICME 2021 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are concerns that new approaches to the synthesis of high quality face\nvideos may be misused to manipulate videos with malicious intent. The research\ncommunity therefore developed methods for the detection of modified footage and\nassembled benchmark datasets for this task. In this paper, we examine how the\nperformance of forgery detectors depends on the presence of artefacts that the\nhuman eye can see. We introduce a new benchmark dataset for face video forgery\ndetection, of unprecedented quality. It allows us to demonstrate that existing\ndetection techniques have difficulties detecting fakes that reliably fool the\nhuman eye. We thus introduce a new family of detectors that examine\ncombinations of spatial and temporal features and outperform existing\napproaches both in terms of detection accuracy and generalization.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 21:17:43 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 12:00:26 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Fox", "Gereon", ""], ["Liu", "Wentao", ""], ["Kim", "Hyeongwoo", ""], ["Seidel", "Hans-Peter", ""], ["Elgharib", "Mohamed", ""], ["Theobalt", "Christian", ""]]}, {"id": "2005.10411", "submitter": "Zixuan Huang", "authors": "Zixuan Huang, Yin Li", "title": "Interpretable and Accurate Fine-grained Recognition via Region Grouping", "comments": "Accepted to CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interpretable deep model for fine-grained visual recognition.\nAt the core of our method lies the integration of region-based part discovery\nand attribution within a deep neural network. Our model is trained using\nimage-level object labels, and provides an interpretation of its results via\nthe segmentation of object parts and the identification of their contributions\ntowards classification. To facilitate the learning of object parts without\ndirect supervision, we explore a simple prior of the occurrence of object\nparts. We demonstrate that this prior, when combined with our region-based part\ndiscovery and attribution, leads to an interpretable model that remains highly\naccurate. Our model is evaluated on major fine-grained recognition datasets,\nincluding CUB-200, CelebA and iNaturalist. Our results compare favorably to\nstate-of-the-art methods on classification tasks, and our method outperforms\nprevious approaches on the localization of object parts.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 01:18:26 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Huang", "Zixuan", ""], ["Li", "Yin", ""]]}, {"id": "2005.10420", "submitter": "Mengtian Li", "authors": "Mengtian Li, Yu-Xiong Wang, Deva Ramanan", "title": "Towards Streaming Perception", "comments": "ECCV 2020 (Oral). Code and data can be found on the project page at\n  https://www.cs.cmu.edu/~mengtial/proj/streaming/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied perception refers to the ability of an autonomous agent to perceive\nits environment so that it can (re)act. The responsiveness of the agent is\nlargely governed by latency of its processing pipeline. While past work has\nstudied the algorithmic trade-off between latency and accuracy, there has not\nbeen a clear metric to compare different methods along the Pareto optimal\nlatency-accuracy curve. We point out a discrepancy between standard offline\nevaluation and real-time applications: by the time an algorithm finishes\nprocessing a particular frame, the surrounding world has changed. To these\nends, we present an approach that coherently integrates latency and accuracy\ninto a single metric for real-time online perception, which we refer to as\n\"streaming accuracy\". The key insight behind this metric is to jointly evaluate\nthe output of the entire perception stack at every time instant, forcing the\nstack to consider the amount of streaming data that should be ignored while\ncomputation is occurring. More broadly, building upon this metric, we introduce\na meta-benchmark that systematically converts any single-frame task into a\nstreaming perception task. We focus on the illustrative tasks of object\ndetection and instance segmentation in urban video streams, and contribute a\nnovel dataset with high-quality and temporally-dense annotations. Our proposed\nsolutions and their empirical analysis demonstrate a number of surprising\nconclusions: (1) there exists an optimal \"sweet spot\" that maximizes streaming\naccuracy along the Pareto optimal latency-accuracy curve, (2) asynchronous\ntracking and future forecasting naturally emerge as internal representations\nthat enable streaming perception, and (3) dynamic scheduling can be used to\novercome temporal aliasing, yielding the paradoxical result that latency is\nsometimes minimized by sitting idle and \"doing nothing\".\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 01:51:35 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 01:16:43 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Li", "Mengtian", ""], ["Wang", "Yu-Xiong", ""], ["Ramanan", "Deva", ""]]}, {"id": "2005.10430", "submitter": "Jungseock Joo", "authors": "Jungseock Joo, Kimmo K\\\"arkk\\\"ainen", "title": "Gender Slopes: Counterfactual Fairness for Computer Vision Models by\n  Attribute Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated computer vision systems have been applied in many domains including\nsecurity, law enforcement, and personal devices, but recent reports suggest\nthat these systems may produce biased results, discriminating against people in\ncertain demographic groups. Diagnosing and understanding the underlying true\ncauses of model biases, however, are challenging tasks because modern computer\nvision systems rely on complex black-box models whose behaviors are hard to\ndecode. We propose to use an encoder-decoder network developed for image\nattribute manipulation to synthesize facial images varying in the dimensions of\ngender and race while keeping other signals intact. We use these synthesized\nimages to measure counterfactual fairness of commercial computer vision\nclassifiers by examining the degree to which these classifiers are affected by\ngender and racial cues controlled in the images, e.g., feminine faces may\nelicit higher scores for the concept of nurse and lower scores for STEM-related\nconcepts. We also report the skewed gender representations in an online search\nservice on profession-related keywords, which may explain the origin of the\nbiases encoded in the models.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 02:33:28 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Joo", "Jungseock", ""], ["K\u00e4rkk\u00e4inen", "Kimmo", ""]]}, {"id": "2005.10434", "submitter": "Yu Song", "authors": "Yu Song, Zilong Huang, Chuanyue Shen, Humphrey Shi, and David A Lange", "title": "Deep Learning-Based Automated Image Segmentation for Concrete\n  Petrographic Analysis", "comments": "Accepted as a journal publication by Cement & Concrete Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard petrography test method for measuring air voids in concrete\n(ASTM C457) requires a meticulous and long examination of sample phase\ncomposition under a stereomicroscope. The high expertise and specialized\nequipment discourage this test for routine concrete quality control. Though the\ntask can be alleviated with the aid of color-based image segmentation,\nadditional surface color treatment is required. Recently, deep learning\nalgorithms using convolutional neural networks (CNN) have achieved\nunprecedented segmentation performance on image testing benchmarks. In this\nstudy, we investigated the feasibility of using CNN to conduct concrete\nsegmentation without the use of color treatment. The CNN demonstrated a strong\npotential to process a wide range of concretes, including those not involved in\nmodel training. The experimental results showed that CNN outperforms the\ncolor-based segmentation by a considerable margin, and has comparable accuracy\nto human experts. Furthermore, the segmentation time is reduced to mere\nseconds.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 02:46:29 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 06:01:27 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 20:16:26 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Song", "Yu", ""], ["Huang", "Zilong", ""], ["Shen", "Chuanyue", ""], ["Shi", "Humphrey", ""], ["Lange", "David A", ""]]}, {"id": "2005.10439", "submitter": "Kelei He", "authors": "Kelei He, Chunfeng Lian, Bing Zhang, Xin Zhang, Xiaohuan Cao, Dong\n  Nie, Yang Gao, Junfeng Zhang, Dinggang Shen", "title": "HF-UNet: Learning Hierarchically Inter-Task Relevance in Multi-Task\n  U-Net for Accurate Prostate Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of the prostate is a key step in external beam\nradiation therapy treatments. In this paper, we tackle the challenging task of\nprostate segmentation in CT images by a two-stage network with 1) the first\nstage to fast localize, and 2) the second stage to accurately segment the\nprostate. To precisely segment the prostate in the second stage, we formulate\nprostate segmentation into a multi-task learning framework, which includes a\nmain task to segment the prostate, and an auxiliary task to delineate the\nprostate boundary. Here, the second task is applied to provide additional\nguidance of unclear prostate boundary in CT images. Besides, the conventional\nmulti-task deep networks typically share most of the parameters (i.e., feature\nrepresentations) across all tasks, which may limit their data fitting ability,\nas the specificities of different tasks are inevitably ignored. By contrast, we\nsolve them by a hierarchically-fused U-Net structure, namely HF-UNet. The\nHF-UNet has two complementary branches for two tasks, with the novel proposed\nattention-based task consistency learning block to communicate at each level\nbetween the two decoding branches. Therefore, HF-UNet endows the ability to\nlearn hierarchically the shared representations for different tasks, and\npreserve the specificities of learned representations for different tasks\nsimultaneously. We did extensive evaluations of the proposed method on a large\nplanning CT image dataset, including images acquired from 339 patients. The\nexperimental results show HF-UNet outperforms the conventional multi-task\nnetwork architectures and the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 02:53:52 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 13:26:25 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["He", "Kelei", ""], ["Lian", "Chunfeng", ""], ["Zhang", "Bing", ""], ["Zhang", "Xin", ""], ["Cao", "Xiaohuan", ""], ["Nie", "Dong", ""], ["Gao", "Yang", ""], ["Zhang", "Junfeng", ""], ["Shen", "Dinggang", ""]]}, {"id": "2005.10451", "submitter": "Li Shen", "authors": "Yucong Shen, Li Shen, Hao-Zhi Huang, Xuan Wang, Wei Liu", "title": "CPOT: Channel Pruning via Optimal Transport", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep neural networks (DNNs) lead to tremendously growing\nnetwork parameters, making the deployments of DNNs on platforms with limited\nresources extremely difficult. Therefore, various pruning methods have been\ndeveloped to compress the deep network architectures and accelerate the\ninference process. Most of the existing channel pruning methods discard the\nless important filters according to well-designed filter ranking criteria.\nHowever, due to the limited interpretability of deep learning models, designing\nan appropriate ranking criterion to distinguish redundant filters is difficult.\nTo address such a challenging issue, we propose a new technique of Channel\nPruning via Optimal Transport, dubbed CPOT. Specifically, we locate the\nWasserstein barycenter for channels of each layer in the deep models, which is\nthe mean of a set of probability distributions under the optimal transport\nmetric. Then, we prune the redundant information located by Wasserstein\nbarycenters. At last, we empirically demonstrate that, for classification\ntasks, CPOT outperforms the state-of-the-art methods on pruning ResNet-20,\nResNet-32, ResNet-56, and ResNet-110. Furthermore, we show that the proposed\nCPOT technique is good at compressing the StarGAN models by pruning in the more\ndifficult case of image-to-image translation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 03:43:09 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Shen", "Yucong", ""], ["Shen", "Li", ""], ["Huang", "Hao-Zhi", ""], ["Wang", "Xuan", ""], ["Liu", "Wei", ""]]}, {"id": "2005.10455", "submitter": "Wenjie Ai", "authors": "Wenjie Ai, Xiaoguang Tu, Shilei Cheng, Mei Xie", "title": "Single Image Super-Resolution via Residual Neuron Attention Networks", "comments": "6 pages, 4 figures, Accepted by IEEE ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) have achieved impressive\nperformance in Single Image Super-Resolution (SISR). To further improve the\nperformance, existing CNN-based methods generally focus on designing deeper\narchitecture of the network. However, we argue blindly increasing network's\ndepth is not the most sensible way. In this paper, we propose a novel\nend-to-end Residual Neuron Attention Networks (RNAN) for more efficient and\neffective SISR. Structurally, our RNAN is a sequential integration of the\nwell-designed Global Context-enhanced Residual Groups (GCRGs), which extracts\nsuper-resolved features from coarse to fine. Our GCRG is designed with two\nnovelties. Firstly, the Residual Neuron Attention (RNA) mechanism is proposed\nin each block of GCRG to reveal the relevance of neurons for better feature\nrepresentation. Furthermore, the Global Context (GC) block is embedded into\nRNAN at the end of each GCRG for effectively modeling the global contextual\ninformation. Experiments results demonstrate that our RNAN achieves the\ncomparable results with state-of-the-art methods in terms of both quantitative\nmetrics and visual quality, however, with simplified network architecture.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 04:01:19 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Ai", "Wenjie", ""], ["Tu", "Xiaoguang", ""], ["Cheng", "Shilei", ""], ["Xie", "Mei", ""]]}, {"id": "2005.10481", "submitter": "Maxim Berman", "authors": "Maxim Berman, Leonid Pishchulin, Ning Xu, Matthew B. Blaschko, Gerard\n  Medioni", "title": "AOWS: Adaptive and optimal network width search with latency constraints", "comments": "Accepted to CVPR 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) approaches aim at automatically finding\nnovel CNN architectures that fit computational constraints while maintaining a\ngood performance on the target platform. We introduce a novel efficient\none-shot NAS approach to optimally search for channel numbers, given latency\nconstraints on a specific hardware. We first show that we can use a black-box\napproach to estimate a realistic latency model for a specific inference\nplatform, without the need for low-level access to the inference computation.\nThen, we design a pairwise MRF to score any channel configuration and use\ndynamic programming to efficiently decode the best performing configuration,\nyielding an optimal solution for the network width search. Finally, we propose\nan adaptive channel configuration sampling scheme to gradually specialize the\ntraining phase to the target computational constraints. Experiments on ImageNet\nclassification show that our approach can find networks fitting the resource\nconstraints on different target platforms while improving accuracy over the\nstate-of-the-art efficient networks.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 06:46:16 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Berman", "Maxim", ""], ["Pishchulin", "Leonid", ""], ["Xu", "Ning", ""], ["Blaschko", "Matthew B.", ""], ["Medioni", "Gerard", ""]]}, {"id": "2005.10497", "submitter": "Yonghyun Kim", "authors": "Yonghyun Kim, Wonpyo Park, Myung-Cheol Roh and Jongju Shin", "title": "GroupFace: Learning Latent Groups and Constructing Group-based\n  Representations for Face Recognition", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of face recognition, a model learns to distinguish millions of\nface images with fewer dimensional embedding features, and such vast\ninformation may not be properly encoded in the conventional model with a single\nbranch. We propose a novel face-recognition-specialized architecture called\nGroupFace that utilizes multiple group-aware representations, simultaneously,\nto improve the quality of the embedding feature. The proposed method provides\nself-distributed labels that balance the number of samples belonging to each\ngroup without additional human annotations, and learns the group-aware\nrepresentations that can narrow down the search space of the target identity.\nWe prove the effectiveness of the proposed method by showing extensive ablation\nstudies and visualizations. All the components of the proposed method can be\ntrained in an end-to-end manner with a marginal increase of computational\ncomplexity. Finally, the proposed method achieves the state-of-the-art results\nwith significant improvements in 1:1 face verification and 1:N face\nidentification tasks on the following public datasets: LFW, YTF, CALFW, CPLFW,\nCFP, AgeDB-30, MegaFace, IJB-B and IJB-C.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 07:30:34 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 04:51:33 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Kim", "Yonghyun", ""], ["Park", "Wonpyo", ""], ["Roh", "Myung-Cheol", ""], ["Shin", "Jongju", ""]]}, {"id": "2005.10499", "submitter": "Johannes Br\\\"unger", "authors": "Johannes Br\\\"unger, Maria Gentz, Imke Traulsen and Reinhard Koch", "title": "Panoptic Instance Segmentation on Pigs", "comments": "18 pages, 10 figures. Submitted to MDPI Sensors", "journal-ref": null, "doi": "10.3390/s20133710", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The behavioural research of pigs can be greatly simplified if automatic\nrecognition systems are used. Especially systems based on computer vision have\nthe advantage that they allow an evaluation without affecting the normal\nbehaviour of the animals. In recent years, methods based on deep learning have\nbeen introduced and have shown pleasingly good results. Especially object and\nkeypoint detectors have been used to detect the individual animals. Despite\ngood results, bounding boxes and sparse keypoints do not trace the contours of\nthe animals, resulting in a lot of information being lost. Therefore this work\nfollows the relatively new definition of a panoptic segmentation and aims at\nthe pixel accurate segmentation of the individual pigs. For this a framework of\na neural network for semantic segmentation, different network heads and\npostprocessing methods is presented. With the resulting instance segmentation\nmasks further information like the size or weight of the animals could be\nestimated. The method is tested on a specially created data set with 1000\nhand-labeled images and achieves detection rates of around 95% (F1 Score)\ndespite disturbances such as occlusions and dirty lenses.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 07:36:03 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Br\u00fcnger", "Johannes", ""], ["Gentz", "Maria", ""], ["Traulsen", "Imke", ""], ["Koch", "Reinhard", ""]]}, {"id": "2005.10510", "submitter": "Junbum Cha", "authors": "Junbum Cha, Sanghyuk Chun, Gayoung Lee, Bado Lee, Seonghyeon Kim, and\n  Hwalsuk Lee", "title": "Few-shot Compositional Font Generation with Dual Memory", "comments": "ECCV 2020 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating a new font library is a very labor-intensive and time-consuming\njob for glyph-rich scripts. Despite the remarkable success of existing font\ngeneration methods, they have significant drawbacks; they require a large\nnumber of reference images to generate a new font set, or they fail to capture\ndetailed styles with only a few samples. In this paper, we focus on\ncompositional scripts, a widely used letter system in the world, where each\nglyph can be decomposed by several components. By utilizing the\ncompositionality of compositional scripts, we propose a novel font generation\nframework, named Dual Memory-augmented Font Generation Network (DM-Font), which\nenables us to generate a high-quality font library with only a few samples. We\nemploy memory components and global-context awareness in the generator to take\nadvantage of the compositionality. In the experiments on Korean-handwriting\nfonts and Thai-printing fonts, we observe that our method generates a\nsignificantly better quality of samples with faithful stylization compared to\nthe state-of-the-art generation methods quantitatively and qualitatively.\nSource code is available at https://github.com/clovaai/dmfont.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 08:13:40 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 11:47:52 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Cha", "Junbum", ""], ["Chun", "Sanghyuk", ""], ["Lee", "Gayoung", ""], ["Lee", "Bado", ""], ["Kim", "Seonghyeon", ""], ["Lee", "Hwalsuk", ""]]}, {"id": "2005.10511", "submitter": "Ronghao Guo", "authors": "Ronghao Guo, Chen Lin, Chuming Li, Keyu Tian, Ming Sun, Lu Sheng,\n  Junjie Yan", "title": "Powering One-shot Topological NAS with Stabilized Share-parameter Proxy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot NAS method has attracted much interest from the research community\ndue to its remarkable training efficiency and capacity to discover high\nperformance models. However, the search spaces of previous one-shot based works\nusually relied on hand-craft design and were short for flexibility on the\nnetwork topology. In this work, we try to enhance the one-shot NAS by exploring\nhigh-performing network architectures in our large-scale Topology Augmented\nSearch Space (i.e., over 3.4*10^10 different topological structures).\nSpecifically, the difficulties for architecture searching in such a complex\nspace has been eliminated by the proposed stabilized share-parameter proxy,\nwhich employs Stochastic Gradient Langevin Dynamics to enable fast shared\nparameter sampling, so as to achieve stabilized measurement of architecture\nperformance even in search space with complex topological structures. The\nproposed method, namely Stablized Topological Neural Architecture Search\n(ST-NAS), achieves state-of-the-art performance under Multiply-Adds (MAdds)\nconstraint on ImageNet. Our lite model ST-NAS-A achieves 76.4% top-1 accuracy\nwith only 326M MAdds. Our moderate model ST-NAS-B achieves 77.9% top-1 accuracy\njust required 503M MAdds. Both of our models offer superior performances in\ncomparison to other concurrent works on one-shot NAS.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 08:18:55 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Guo", "Ronghao", ""], ["Lin", "Chen", ""], ["Li", "Chuming", ""], ["Tian", "Keyu", ""], ["Sun", "Ming", ""], ["Sheng", "Lu", ""], ["Yan", "Junjie", ""]]}, {"id": "2005.10513", "submitter": "Xi Li", "authors": "Xi Li, Huimin Ma, Hongbing Ma, Yidong Wang", "title": "Unsupervised segmentation via semantic-apparent feature fusion", "comments": "in Chinese. Accepted by NCIG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foreground segmentation is an essential task in the field of image\nunderstanding. Under unsupervised conditions, different images and instances\nalways have variable expressions, which make it difficult to achieve stable\nsegmentation performance based on fixed rules or single type of feature. In\norder to solve this problem, the research proposes an unsupervised foreground\nsegmentation method based on semantic-apparent feature fusion (SAFF). Here, we\nfound that key regions of foreground object can be accurately responded via\nsemantic features, while apparent features (represented by saliency and edge)\nprovide richer detailed expression. To combine the advantages of the two type\nof features, an encoding method for unary region features and binary context\nfeatures is established, which realizes a comprehensive description of the two\ntypes of expressions. Then, a method for adaptive parameter learning is put\nforward to calculate the most suitable feature weights and generate foreground\nconfidence score map. Furthermore, segmentation network is used to learn\nforeground common features from different instances. By fusing semantic and\napparent features, as well as cascading the modules of intra-image adaptive\nfeature weight learning and inter-image common feature learning, the research\nachieves performance that significantly exceeds baselines on the PASCAL VOC\n2012 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 08:28:49 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Li", "Xi", ""], ["Ma", "Huimin", ""], ["Ma", "Hongbing", ""], ["Wang", "Yidong", ""]]}, {"id": "2005.10524", "submitter": "Gaurav Mittal", "authors": "Gaurav Mittal, Chang Liu, Nikolaos Karianakis, Victor Fragoso, Mei\n  Chen, Yun Fu", "title": "HyperSTAR: Task-Aware Hyperparameters for Deep Networks", "comments": "Published at CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks excel in solving visual recognition tasks, they\nrequire significant effort to find hyperparameters that make them work\noptimally. Hyperparameter Optimization (HPO) approaches have automated the\nprocess of finding good hyperparameters but they do not adapt to a given task\n(task-agnostic), making them computationally inefficient. To reduce HPO time,\nwe present HyperSTAR (System for Task Aware Hyperparameter Recommendation), a\ntask-aware method to warm-start HPO for deep neural networks. HyperSTAR ranks\nand recommends hyperparameters by predicting their performance conditioned on a\njoint dataset-hyperparameter space. It learns a dataset (task) representation\nalong with the performance predictor directly from raw images in an end-to-end\nfashion. The recommendations, when integrated with an existing HPO method, make\nit task-aware and significantly reduce the time to achieve optimal performance.\nWe conduct extensive experiments on 10 publicly available large-scale image\nclassification datasets over two different network architectures, validating\nthat HyperSTAR evaluates 50% less configurations to achieve the best\nperformance compared to existing methods. We further demonstrate that HyperSTAR\nmakes Hyperband (HB) task-aware, achieving the optimal accuracy in just 25% of\nthe budget required by both vanilla HB and Bayesian Optimized HB~(BOHB).\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 08:56:50 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Mittal", "Gaurav", ""], ["Liu", "Chang", ""], ["Karianakis", "Nikolaos", ""], ["Fragoso", "Victor", ""], ["Chen", "Mei", ""], ["Fu", "Yun", ""]]}, {"id": "2005.10544", "submitter": "John Cai", "authors": "John Cai, Sheng Mei Shen", "title": "Cross-Domain Few-Shot Learning with Meta Fine-Tuning", "comments": "CVPR 2020 Workshop on Visual Learning with Limited Labels (VL3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the new Cross-Domain Few-Shot Learning benchmark\nproposed by the CVPR 2020 Challenge. To this end, we build upon\nstate-of-the-art methods in domain adaptation and few-shot learning to create a\nsystem that can be trained to perform both tasks. Inspired by the need to\ncreate models designed to be fine-tuned, we explore the integration of\ntransfer-learning (fine-tuning) with meta-learning algorithms, to train a\nnetwork that has specific layers that are designed to be adapted at a later\nfine-tuning stage. To do so, we modify the episodic training process to include\na first-order MAML-based meta-learning algorithm, and use a Graph Neural\nNetwork model as the subsequent meta-learning module. We find that our proposed\nmethod helps to boost accuracy significantly, especially when combined with\ndata augmentation. In our final results, we combine the novel method with the\nbaseline method in a simple ensemble, and achieve an average accuracy of 73.78%\non the benchmark. This is a 6.51% improvement over existing benchmarks that\nwere trained solely on miniImagenet.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 09:55:26 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 11:19:25 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 03:46:42 GMT"}, {"version": "v4", "created": "Tue, 25 Aug 2020 15:45:55 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Cai", "John", ""], ["Shen", "Sheng Mei", ""]]}, {"id": "2005.10547", "submitter": "Xiangjie Sui", "authors": "Xiangjie Sui, Kede Ma, Yiru Yao, Yuming Fang", "title": "Perceptual Quality Assessment of Omnidirectional Images as Moving Camera\n  Videos", "comments": "11 pages, 11 figure, 9 tables. This paper has been accepted by IEEE\n  Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omnidirectional images (also referred to as static 360{\\deg} panoramas)\nimpose viewing conditions much different from those of regular 2D images. How\ndo humans perceive image distortions in immersive virtual reality (VR)\nenvironments is an important problem which receives less attention. We argue\nthat, apart from the distorted panorama itself, two types of VR viewing\nconditions are crucial in determining the viewing behaviors of users and the\nperceived quality of the panorama: the starting point and the exploration time.\nWe first carry out a psychophysical experiment to investigate the interplay\namong the VR viewing conditions, the user viewing behaviors, and the perceived\nquality of 360{\\deg} images. Then, we provide a thorough analysis of the\ncollected human data, leading to several interesting findings. Moreover, we\npropose a computational framework for objective quality assessment of 360{\\deg}\nimages, embodying viewing conditions and behaviors in a delightful way.\nSpecifically, we first transform an omnidirectional image to several video\nrepresentations using different user viewing behaviors under different viewing\nconditions. We then leverage advanced 2D full-reference video quality models to\ncompute the perceived quality. We construct a set of specific quality measures\nwithin the proposed framework, and demonstrate their promises on three VR\nquality databases.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 10:03:40 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 03:23:16 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Sui", "Xiangjie", ""], ["Ma", "Kede", ""], ["Yao", "Yiru", ""], ["Fang", "Yuming", ""]]}, {"id": "2005.10550", "submitter": "Renato Hermoza Aragon\\'es", "authors": "Renato Hermoza, Gabriel Maicas, Jacinto C. Nascimento and Gustavo\n  Carneiro", "title": "Region Proposals for Saliency Map Refinement for Weakly-supervised\n  Disease Localisation and Classification", "comments": "Early accept at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of automated systems to diagnose diseases from medical images\nis challenged by the requirement to localise the diagnosed diseases to justify\nor explain the classification decision. This requirement is hard to fulfil\nbecause most of the training sets available to develop these systems only\ncontain global annotations, making the localisation of diseases a weakly\nsupervised approach. The main methods designed for weakly supervised disease\nclassification and localisation rely on saliency or attention maps that are not\nspecifically trained for localisation, or on region proposals that can not be\nrefined to produce accurate detections. In this paper, we introduce a new model\nthat combines region proposal and saliency detection to overcome both\nlimitations for weakly supervised disease classification and localisation.\nUsing the ChestX-ray14 data set, we show that our proposed model establishes\nthe new state-of-the-art for weakly-supervised disease diagnosis and\nlocalisation.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 10:07:43 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 01:15:47 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Hermoza", "Renato", ""], ["Maicas", "Gabriel", ""], ["Nascimento", "Jacinto C.", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2005.10582", "submitter": "Yiyang Shen", "authors": "Yiyang Shen, Yidan Feng, Sen Deng, Dong Liang, Jing Qin, Haoran Xie,\n  Mingqiang Wei", "title": "MBA-RainGAN: Multi-branch Attention Generative Adversarial Network for\n  Mixture of Rain Removal from Single Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain severely hampers the visibility of scene objects when images are\ncaptured through glass in heavily rainy days. We observe three intriguing\nphenomenons that, 1) rain is a mixture of raindrops, rain streaks and rainy\nhaze; 2) the depth from the camera determines the degrees of object visibility,\nwhere objects nearby and faraway are visually blocked by rain streaks and rainy\nhaze, respectively; and 3) raindrops on the glass randomly affect the object\nvisibility of the whole image space. We for the first time consider that, the\noverall visibility of objects is determined by the mixture of rain (MOR).\nHowever, existing solutions and established datasets lack full consideration of\nthe MOR. In this work, we first formulate a new rain imaging model; by then, we\nenrich the popular RainCityscapes by considering raindrops, named\nRainCityscapes++. Furthermore, we propose a multi-branch attention generative\nadversarial network (termed an MBA-RainGAN) to fully remove the MOR. The\nexperiment shows clear visual and numerical improvements of our approach over\nthe state-of-the-arts on RainCityscapes++. The code and dataset will be\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 11:44:21 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 00:21:00 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Shen", "Yiyang", ""], ["Feng", "Yidan", ""], ["Deng", "Sen", ""], ["Liang", "Dong", ""], ["Qin", "Jing", ""], ["Xie", "Haoran", ""], ["Wei", "Mingqiang", ""]]}, {"id": "2005.10589", "submitter": "Lia Morra", "authors": "Lia Morra, Luca Piano, Fabrizio Lamberti, Tatiana Tommasi", "title": "Bridging the gap between Natural and Medical Images through Deep\n  Colorization", "comments": "accepted for publication at ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has thrived by training on large-scale datasets. However, in\nmany applications, as for medical image diagnosis, getting massive amount of\ndata is still prohibitive due to privacy, lack of acquisition homogeneity and\nannotation cost. In this scenario, transfer learning from natural image\ncollections is a standard practice that attempts to tackle shape, texture and\ncolor discrepancies all at once through pretrained model fine-tuning. In this\nwork, we propose to disentangle those challenges and design a dedicated network\nmodule that focuses on color adaptation. We combine learning from scratch of\nthe color module with transfer learning of different classification backbones,\nobtaining an end-to-end, easy-to-train architecture for diagnostic image\nrecognition on X-ray images. Extensive experiments showed how our approach is\nparticularly efficient in case of data scarcity and provides a new path for\nfurther transferring the learned color information across multiple medical\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 12:03:14 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 21:47:58 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Morra", "Lia", ""], ["Piano", "Luca", ""], ["Lamberti", "Fabrizio", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "2005.10600", "submitter": "Steven Frank", "authors": "Steven J. Frank and Andrea M. Frank", "title": "A Neural Network Looks at Leonardo's(?) Salvator Mundi", "comments": "This is the author's final version. The article has been accepted for\n  publication in Leonardo (MIT Press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use convolutional neural networks (CNNs) to analyze authorship questions\nsurrounding the works of Leonardo da Vinci -- in particular, Salvator Mundi,\nthe world's most expensive painting and among the most controversial. Trained\non the works of an artist under study and visually comparable works of other\nartists, our system can identify likely forgeries and shed light on attribution\ncontroversies. Leonardo's few extant paintings test the limits of our system\nand require corroborative techniques of testing and analysis.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 12:27:40 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Frank", "Steven J.", ""], ["Frank", "Andrea M.", ""]]}, {"id": "2005.10626", "submitter": "Yu-Cheng Chang", "authors": "Jhih-Yuan Lin, Yu-Cheng Chang, Winston H. Hsu", "title": "Efficient and Phase-aware Video Super-resolution for Cardiac MRI", "comments": "MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac Magnetic Resonance Imaging (CMR) is widely used since it can\nillustrate the structure and function of heart in a non-invasive and painless\nway. However, it is time-consuming and high-cost to acquire the high-quality\nscans due to the hardware limitation. To this end, we propose a novel\nend-to-end trainable network to solve CMR video super-resolution problem\nwithout the hardware upgrade and the scanning protocol modifications. We\nincorporate the cardiac knowledge into our model to assist in utilizing the\ntemporal information. Specifically, we formulate the cardiac knowledge as the\nperiodic function, which is tailored to meet the cyclic characteristic of CMR.\nIn addition, the proposed residual of residual learning scheme facilitates the\nnetwork to learn the LR-HR mapping in a progressive refinement fashion. This\nmechanism enables the network to have the adaptive capability by adjusting\nrefinement iterations depending on the difficulty of the task. Extensive\nexperimental results on large-scale datasets demonstrate the superiority of the\nproposed method compared with numerous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 13:29:03 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 02:06:01 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 17:02:47 GMT"}, {"version": "v4", "created": "Wed, 8 Jul 2020 14:35:54 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Lin", "Jhih-Yuan", ""], ["Chang", "Yu-Cheng", ""], ["Hsu", "Winston H.", ""]]}, {"id": "2005.10635", "submitter": "Randall Balestriero", "authors": "Randall Balestriero", "title": "SymJAX: symbolic CPU/GPU/TPU programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SymJAX is a symbolic programming version of JAX simplifying graph\ninput/output/updates and providing additional functionalities for general\nmachine learning and deep learning applications. From an user perspective\nSymJAX provides a la Theano experience with fast graph optimization/compilation\nand broad hardware support, along with Lasagne-like deep learning\nfunctionalities.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 13:37:25 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Balestriero", "Randall", ""]]}, {"id": "2005.10663", "submitter": "Oran Gafni", "authors": "Oran Gafni, Lior Wolf", "title": "Wish You Were Here: Context-Aware Human Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for inserting objects, specifically humans, into\nexisting images, such that they blend in a photorealistic manner, while\nrespecting the semantic context of the scene. Our method involves three\nsubnetworks: the first generates the semantic map of the new person, given the\npose of the other persons in the scene and an optional bounding box\nspecification. The second network renders the pixels of the novel person and\nits blending mask, based on specifications in the form of multiple appearance\ncomponents. A third network refines the generated face in order to match those\nof the target person. Our experiments present convincing high-resolution\noutputs in this novel and challenging application domain. In addition, the\nthree networks are evaluated individually, demonstrating for example, state of\nthe art results in pose transfer benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 14:09:14 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Gafni", "Oran", ""], ["Wolf", "Lior", ""]]}, {"id": "2005.10686", "submitter": "Leixin Zhou", "authors": "Leixin Zhou, Wenxiang Deng, Xiaodong Wu", "title": "Unsupervised anomaly localization using VAE and beta-VAE", "comments": "arXiv admin note: substantial text overlap with arXiv:2002.03734 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variational Auto-Encoders (VAEs) have shown great potential in the\nunsupervised learning of data distributions. An VAE trained on normal images is\nexpected to only be able to reconstruct normal images, allowing the\nlocalization of anomalous pixels in an image via manipulating information\nwithin the VAE ELBO loss. The ELBO consists of KL divergence loss (image-wise)\nand reconstruction loss (pixel-wise). It is natural and straightforward to use\nthe later as the predictor. However, usually local anomaly added to a normal\nimage can deteriorate the whole reconstructed image, causing segmentation using\nonly naive pixel errors not accurate. Energy based projection was proposed to\nincrease the reconstruction accuracy of normal regions/pixels, which achieved\nthe state-of-the-art localization accuracy on simple natural images. Another\npossible predictors are ELBO and its components gradients with respect to each\npixels. Previous work claimed that KL gradient is a robust predictor. In this\npaper, we argue that the energy based projection in medical imaging is not as\nuseful as on natural images. Moreover, we observe that the robustness of KL\ngradient predictor totally depends on the setting of the VAE and dataset. We\nalso explored the effect of the weight of KL loss within beta-VAE and predictor\nensemble in anomaly localization.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 21:58:59 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Zhou", "Leixin", ""], ["Deng", "Wenxiang", ""], ["Wu", "Xiaodong", ""]]}, {"id": "2005.10687", "submitter": "Khalid Raza", "authors": "Nripendra Kumar Singh, Khalid Raza", "title": "Medical Image Generation using Generative Adversarial Networks", "comments": "19 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are unsupervised Deep Learning\napproach in the computer vision community which has gained significant\nattention from the last few years in identifying the internal structure of\nmultimodal medical imaging data. The adversarial network simultaneously\ngenerates realistic medical images and corresponding annotations, which proven\nto be useful in many cases such as image augmentation, image registration,\nmedical image generation, image reconstruction, and image-to-image translation.\nThese properties bring the attention of the researcher in the field of medical\nimage analysis and we are witness of rapid adaption in many novel and\ntraditional applications. This chapter provides state-of-the-art progress in\nGANs-based clinical application in medical image generation, and cross-modality\nsynthesis. The various framework of GANs which gained popularity in the\ninterpretation of medical images, such as Deep Convolutional GAN (DCGAN),\nLaplacian GAN (LAPGAN), pix2pix, CycleGAN, and unsupervised image-to-image\ntranslation model (UNIT), continue to improve their performance by\nincorporating additional hybrid architecture, has been discussed. Further, some\nof the recent applications of these frameworks for image reconstruction, and\nsynthesis, and future research directions in the area have been covered.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 20:31:57 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Singh", "Nripendra Kumar", ""], ["Raza", "Khalid", ""]]}, {"id": "2005.10745", "submitter": "Mohammed Yousefhussien", "authors": "Mohammed Yousefhussien, David J. Kelbe, and Carl Salvaggio", "title": "A Nearest Neighbor Network to Extract Digital Terrain Models from 3D\n  Point Clouds", "comments": "Preprint submitted to Science of Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When 3D-point clouds from overhead sensors are used as input to remote\nsensing data exploitation pipelines, a large amount of effort is devoted to\ndata preparation. Among the multiple stages of the preprocessing chain,\nestimating the Digital Terrain Model (DTM) model is considered to be of a high\nimportance; however, this remains a challenge, especially for raw point clouds\nderived from optical imagery. Current algorithms estimate the ground points\nusing either a set of geometrical rules that require tuning multiple parameters\nand human interaction, or cast the problem as a binary classification machine\nlearning task where ground and non-ground classes are found. In contrast, here\nwe present an algorithm that directly operates on 3D-point clouds and estimate\nthe underlying DTM for the scene using an end-to-end approach without the need\nto classify points into ground and non-ground cover types. Our model learns\nneighborhood information and seamlessly integrates this with point-wise and\nblock-wise global features. We validate our model using the ISPRS 3D Semantic\nLabeling Contest LiDAR data, as well as three scenes generated using dense\nstereo matching, representative of high-rise buildings, lower urban structures,\nand a dense old-city residential area. We compare our findings with two widely\nused software packages for DTM extraction, namely ENVI and LAStools. Our\npreliminary results show that the proposed method is able to achieve an overall\nMean Absolute Error of 11.5% compared to 29% and 16% for ENVI and LAStools.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 15:54:55 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 19:51:13 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Yousefhussien", "Mohammed", ""], ["Kelbe", "David J.", ""], ["Salvaggio", "Carl", ""]]}, {"id": "2005.10750", "submitter": "Yong Man Ro", "authors": "Byeong Cheon Kim, Jung Uk Kim, Hakmin Lee, Yong Man Ro", "title": "Revisiting Role of Autoencoders in Adversarial Settings", "comments": "Accepted at ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To combat against adversarial attacks, autoencoder structure is widely used\nto perform denoising which is regarded as gradient masking. In this paper, we\nrevisit the role of autoencoders in adversarial settings. Through the\ncomprehensive experimental results and analysis, this paper presents the\ninherent property of adversarial robustness in the autoencoders. We also found\nthat autoencoders may use robust features that cause inherent adversarial\nrobustness. We believe that our discovery of the adversarial robustness of the\nautoencoders can provide clues to the future research and applications for\nadversarial defense.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 16:01:23 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Kim", "Byeong Cheon", ""], ["Kim", "Jung Uk", ""], ["Lee", "Hakmin", ""], ["Ro", "Yong Man", ""]]}, {"id": "2005.10754", "submitter": "Yong Man Ro", "authors": "Hong Joo Lee, Seong Tae Kim, Hakmin Lee, Nassir Navab, Yong Man Ro", "title": "Efficient Ensemble Model Generation for Uncertainty Estimation with\n  Bayesian Approximation in Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that ensemble approaches could not only improve\naccuracy and but also estimate model uncertainty in deep learning. However, it\nrequires a large number of parameters according to the increase of ensemble\nmodels for better prediction and uncertainty estimation. To address this issue,\na generic and efficient segmentation framework to construct ensemble\nsegmentation models is devised in this paper. In the proposed method, ensemble\nmodels can be efficiently generated by using the stochastic layer selection\nmethod. The ensemble models are trained to estimate uncertainty through\nBayesian approximation. Moreover, to overcome its limitation from uncertain\ninstances, we devise a new pixel-wise uncertainty loss, which improves the\npredictive performance. To evaluate our method, comprehensive and comparative\nexperiments have been conducted on two datasets. Experimental results show that\nthe proposed method could provide useful uncertainty information by Bayesian\napproximation with the efficient ensemble model generation and improve the\npredictive performance.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 16:08:38 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 09:21:27 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Lee", "Hong Joo", ""], ["Kim", "Seong Tae", ""], ["Lee", "Hakmin", ""], ["Navab", "Nassir", ""], ["Ro", "Yong Man", ""]]}, {"id": "2005.10757", "submitter": "Yong Man Ro", "authors": "Hakmin Lee, Hong Joo Lee, Seong Tae Kim, Yong Man Ro", "title": "Robust Ensemble Model Training via Random Layer Sampling Against\n  Adversarial Attack", "comments": "Accepted at BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved substantial achievements in several\ncomputer vision areas, but have vulnerabilities that are often fooled by\nadversarial examples that are not recognized by humans. This is an important\nissue for security or medical applications. In this paper, we propose an\nensemble model training framework with random layer sampling to improve the\nrobustness of deep neural networks. In the proposed training framework, we\ngenerate various sampled model through the random layer sampling and update the\nweight of the sampled model. After the ensemble models are trained, it can hide\nthe gradient efficiently and avoid the gradient-based attack by the random\nlayer sampling method. To evaluate our proposed method, comprehensive and\ncomparative experiments have been conducted on three datasets. Experimental\nresults show that the proposed method improves the adversarial robustness.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 16:14:18 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 13:20:57 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Lee", "Hakmin", ""], ["Lee", "Hong Joo", ""], ["Kim", "Seong Tae", ""], ["Ro", "Yong Man", ""]]}, {"id": "2005.10766", "submitter": "Tianxin Shi", "authors": "Tianxin Shi, Hainan Cui, Zhuo Song, Shuhan Shen", "title": "Dense Semantic 3D Map Based Long-Term Visual Localization with Hybrid\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization plays an important role in many applications. However,\ndue to the large appearance variations such as season and illumination changes,\nas well as weather and day-night variations, it's still a big challenge for\nrobust long-term visual localization algorithms. In this paper, we present a\nnovel visual localization method using hybrid handcrafted and learned features\nwith dense semantic 3D map. Hybrid features help us to make full use of their\nstrengths in different imaging conditions, and the dense semantic map provide\nus reliable and complete geometric and semantic information for constructing\nsufficient 2D-3D matching pairs with semantic consistency scores. In our\npipeline, we retrieve and score each candidate database image through the\nsemantic consistency between the dense model and the query image. Then the\nsemantic consistency score is used as a soft constraint in the weighted\nRANSAC-based PnP pose solver. Experimental results on long-term visual\nlocalization benchmarks demonstrate the effectiveness of our method compared\nwith state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 16:36:37 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Shi", "Tianxin", ""], ["Cui", "Hainan", ""], ["Song", "Zhuo", ""], ["Shen", "Shuhan", ""]]}, {"id": "2005.10777", "submitter": "Jing Huo", "authors": "Jing Huo, Shiyin Jin, Wenbin Li, Jing Wu, Yu-Kun Lai, Yinghuan Shi,\n  Yang Gao", "title": "Manifold Alignment for Semantically Aligned Style Transfer", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a content image and a style image, the goal of style transfer is to\nsynthesize an output image by transferring the target style to the content\nimage. Currently, most of the methods address the problem with global style\ntransfer, assuming styles can be represented by global statistics, such as Gram\nmatrices or covariance matrices. In this paper, we make a different assumption\nthat local semantically aligned (or similar) regions between the content and\nstyle images should share similar style patterns. Based on this assumption,\ncontent features and style features are seen as two sets of manifolds and a\nmanifold alignment based style transfer (MAST) method is proposed. MAST is a\nsubspace learning method which learns a common subspace of the content and\nstyle features. In the common subspace, content and style features with larger\nfeature similarity or the same semantic meaning are forced to be close. The\nlearned projection matrices are added with orthogonality constraints so that\nthe mapping can be bidirectional, which allows us to project the content\nfeatures into the common subspace, and then into the original style space. By\nusing a pre-trained decoder, promising stylized images are obtained. The method\nis further extended to allow users to specify corresponding semantic regions\nbetween content and style images or using semantic segmentation maps as\nguidance. Extensive experiments show the proposed MAST achieves appealing\nresults in style transfer.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 16:52:37 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Huo", "Jing", ""], ["Jin", "Shiyin", ""], ["Li", "Wenbin", ""], ["Wu", "Jing", ""], ["Lai", "Yu-Kun", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""]]}, {"id": "2005.10821", "submitter": "Andrew Tao", "authors": "Andrew Tao, Karan Sapra, Bryan Catanzaro", "title": "Hierarchical Multi-Scale Attention for Semantic Segmentation", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-scale inference is commonly used to improve the results of semantic\nsegmentation. Multiple images scales are passed through a network and then the\nresults are combined with averaging or max pooling. In this work, we present an\nattention-based approach to combining multi-scale predictions. We show that\npredictions at certain scales are better at resolving particular failures\nmodes, and that the network learns to favor those scales for such cases in\norder to generate better predictions. Our attention mechanism is hierarchical,\nwhich enables it to be roughly 4x more memory efficient to train than other\nrecent approaches. In addition to enabling faster training, this allows us to\ntrain with larger crop sizes which leads to greater model accuracy. We\ndemonstrate the result of our method on two datasets: Cityscapes and Mapillary\nVistas. For Cityscapes, which has a large number of weakly labelled images, we\nalso leverage auto-labelling to improve generalization. Using our approach we\nachieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and\nCityscapes (85.1 IOU test).\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 17:55:59 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Tao", "Andrew", ""], ["Sapra", "Karan", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2005.10825", "submitter": "Jia-Bin Huang", "authors": "Jheng-Wei Su, Hung-Kuo Chu, Jia-Bin Huang", "title": "Instance-aware Image Colorization", "comments": "CVPR 2020. Project: https://ericsujw.github.io/InstColorization/\n  Code: https://github.com/ericsujw/InstColorization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image colorization is inherently an ill-posed problem with multi-modal\nuncertainty. Previous methods leverage the deep neural network to map input\ngrayscale images to plausible color outputs directly. Although these\nlearning-based methods have shown impressive performance, they usually fail on\nthe input images that contain multiple objects. The leading cause is that\nexisting models perform learning and colorization on the entire image. In the\nabsence of a clear figure-ground separation, these models cannot effectively\nlocate and learn meaningful object-level semantics. In this paper, we propose a\nmethod for achieving instance-aware colorization. Our network architecture\nleverages an off-the-shelf object detector to obtain cropped object images and\nuses an instance colorization network to extract object-level features. We use\na similar network to extract the full-image features and apply a fusion module\nto full object-level and image-level features to predict the final colors. Both\ncolorization networks and fusion modules are learned from a large-scale\ndataset. Experimental results show that our work outperforms existing methods\non different quality metrics and achieves state-of-the-art performance on image\ncolorization.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 17:59:23 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Su", "Jheng-Wei", ""], ["Chu", "Hung-Kuo", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "2005.10851", "submitter": "Yinghan Long", "authors": "Yinghan Long, Indranil Chakraborty, Kaushik Roy", "title": "Conditionally Deep Hybrid Neural Networks Across Edge and Cloud", "comments": "6 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pervasiveness of \"Internet-of-Things\" in our daily life has led to a\nrecent surge in fog computing, encompassing a collaboration of cloud computing\nand edge intelligence. To that effect, deep learning has been a major driving\nforce towards enabling such intelligent systems. However, growing model sizes\nin deep learning pose a significant challenge towards deployment in\nresource-constrained edge devices. Moreover, in a distributed intelligence\nenvironment, efficient workload distribution is necessary between edge and\ncloud systems. To address these challenges, we propose a conditionally deep\nhybrid neural network for enabling AI-based fog computing. The proposed network\ncan be deployed in a distributed manner, consisting of quantized layers and\nearly exits at the edge and full-precision layers on the cloud. During\ninference, if an early exit has high confidence in the classification results,\nit would allow samples to exit at the edge, and the deeper layers on the cloud\nare activated conditionally, which can lead to improved energy efficiency and\ninference latency. We perform an extensive design space exploration with the\ngoal of minimizing energy consumption at the edge while achieving\nstate-of-the-art classification accuracies on image classification tasks. We\nshow that with binarized layers at the edge, the proposed conditional hybrid\nnetwork can process 65% of inferences at the edge, leading to 5.5x\ncomputational energy reduction with minimal accuracy degradation on CIFAR-10\ndataset. For the more complex dataset CIFAR-100, we observe that the proposed\nnetwork with 4-bit quantization at the edge achieves 52% early classification\nat the edge with 4.8x energy reduction. The analysis gives us insights on\ndesigning efficient hybrid networks which achieve significantly higher energy\nefficiency than full-precision networks for edge-cloud based distributed\nintelligence systems.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 18:18:43 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Long", "Yinghan", ""], ["Chakraborty", "Indranil", ""], ["Roy", "Kaushik", ""]]}, {"id": "2005.10863", "submitter": "Ankit Laddha", "authors": "Ankit Laddha, Shivam Gautam, Gregory P. Meyer, Carlos\n  Vallespi-Gonzalez, Carl K. Wellington", "title": "RV-FuseNet: Range View Based Fusion of Time-Series LiDAR Data for Joint\n  3D Object Detection and Motion Forecasting", "comments": "Submitted to IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robust real-time detection and motion forecasting of traffic participants is\nnecessary for autonomous vehicles to safely navigate urban environments. In\nthis paper, we present RV-FuseNet, a novel end-to-end approach for joint\ndetection and trajectory estimation directly from time-series LiDAR data.\nInstead of the widely used bird's eye view (BEV) representation, we utilize the\nnative range view (RV) representation of LiDAR data. The RV preserves the full\nresolution of the sensor by avoiding the voxelization used in the BEV.\nFurthermore, RV can be processed efficiently due to its compactness. Previous\napproaches project time-series data to a common viewpoint for temporal fusion,\nand often this viewpoint is different from where it was captured. This is\nsufficient for BEV methods, but for RV methods, this can lead to loss of\ninformation and data distortion which has an adverse impact on performance. To\naddress this challenge we propose a simple yet effective novel architecture,\n\\textit{Incremental Fusion}, that minimizes the information loss by\nsequentially projecting each RV sweep into the viewpoint of the next sweep in\ntime. We show that our approach significantly improves motion forecasting\nperformance over the existing state-of-the-art. Furthermore, we demonstrate\nthat our sequential fusion approach is superior to alternative RV based fusion\nmethods on multiple datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 19:22:27 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 16:12:08 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 02:43:33 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Laddha", "Ankit", ""], ["Gautam", "Shivam", ""], ["Meyer", "Gregory P.", ""], ["Vallespi-Gonzalez", "Carlos", ""], ["Wellington", "Carl K.", ""]]}, {"id": "2005.10876", "submitter": "Marco Toldo", "authors": "Marco Toldo, Andrea Maracani, Umberto Michieli and Pietro Zanuttigh", "title": "Unsupervised Domain Adaptation in Semantic Segmentation: a Review", "comments": "34 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to give an overview of the recent advancements in\nthe Unsupervised Domain Adaptation (UDA) of deep networks for semantic\nsegmentation. This task is attracting a wide interest, since semantic\nsegmentation models require a huge amount of labeled data and the lack of data\nfitting specific requirements is the main limitation in the deployment of these\ntechniques. This problem has been recently explored and has rapidly grown with\na large number of ad-hoc approaches. This motivates us to build a comprehensive\noverview of the proposed methodologies and to provide a clear categorization.\nIn this paper, we start by introducing the problem, its formulation and the\nvarious scenarios that can be considered. Then, we introduce the different\nlevels at which adaptation strategies may be applied: namely, at the input\n(image) level, at the internal features representation and at the output level.\nFurthermore, we present a detailed overview of the literature in the field,\ndividing previous methods based on the following (non mutually exclusive)\ncategories: adversarial learning, generative-based, analysis of the classifier\ndiscrepancies, self-teaching, entropy minimization, curriculum learning and\nmulti-task learning. Novel research directions are also briefly introduced to\ngive a hint of interesting open problems in the field. Finally, a comparison of\nthe performance of the various methods in the widely used autonomous driving\nscenario is presented.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 20:10:38 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Toldo", "Marco", ""], ["Maracani", "Andrea", ""], ["Michieli", "Umberto", ""], ["Zanuttigh", "Pietro", ""]]}, {"id": "2005.10884", "submitter": "Chong Xiang", "authors": "Chong Xiang, Arjun Nitin Bhagoji, Vikash Sehwag, Prateek Mittal", "title": "PatchGuard: A Provably Robust Defense against Adversarial Patches via\n  Small Receptive Fields and Masking", "comments": "USENIX Security Symposium 2021; extended technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localized adversarial patches aim to induce misclassification in machine\nlearning models by arbitrarily modifying pixels within a restricted region of\nan image. Such attacks can be realized in the physical world by attaching the\nadversarial patch to the object to be misclassified, and defending against such\nattacks is an unsolved/open problem. In this paper, we propose a general\ndefense framework called PatchGuard that can achieve high provable robustness\nwhile maintaining high clean accuracy against localized adversarial patches.\nThe cornerstone of PatchGuard involves the use of CNNs with small receptive\nfields to impose a bound on the number of features corrupted by an adversarial\npatch. Given a bounded number of corrupted features, the problem of designing\nan adversarial patch defense reduces to that of designing a secure feature\naggregation mechanism. Towards this end, we present our robust masking defense\nthat robustly detects and masks corrupted features to recover the correct\nprediction. Notably, we can prove the robustness of our defense against any\nadversary within our threat model. Our extensive evaluation on ImageNet,\nImageNette (a 10-class subset of ImageNet), and CIFAR-10 datasets demonstrates\nthat our defense achieves state-of-the-art performance in terms of both\nprovable robust accuracy and clean accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 03:38:34 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 14:51:03 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2020 15:39:00 GMT"}, {"version": "v4", "created": "Sun, 18 Oct 2020 18:12:03 GMT"}, {"version": "v5", "created": "Wed, 31 Mar 2021 14:20:39 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Xiang", "Chong", ""], ["Bhagoji", "Arjun Nitin", ""], ["Sehwag", "Vikash", ""], ["Mittal", "Prateek", ""]]}, {"id": "2005.10903", "submitter": "Peratham Wiriyathammabhum Mr.", "authors": "Peratham Wiriyathammabhum", "title": "SpotFast Networks with Memory Augmented Lateral Transformers for\n  Lipreading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel deep learning architecture for word-level\nlipreading. Previous works suggest a potential for incorporating a pretrained\ndeep 3D Convolutional Neural Networks as a front-end feature extractor. We\nintroduce a SpotFast networks, a variant of the state-of-the-art SlowFast\nnetworks for action recognition, which utilizes a temporal window as a spot\npathway and all frames as a fast pathway. We further incorporate memory\naugmented lateral transformers to learn sequential features for classification.\nWe evaluate the proposed model on the LRW dataset. The experiments show that\nour proposed model outperforms various state-of-the-art models and\nincorporating the memory augmented lateral transformers makes a 3.7%\nimprovement to the SpotFast networks.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 21:04:12 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Wiriyathammabhum", "Peratham", ""]]}, {"id": "2005.10905", "submitter": "Bharti Munjal", "authors": "Bharti Munjal, Abdul Rafey Aftab, Sikandar Amin, Meltem D.\n  Brandlmaier, Federico Tombari, Fabio Galasso", "title": "Joint Detection and Tracking in Videos with Identification Features", "comments": "Accepted at Image and Vision Computing Journal", "journal-ref": null, "doi": "10.1016/j.imavis.2020.103932", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that combining object detection and tracking tasks,\nin the case of video data, results in higher performance for both tasks, but\nthey require a high frame-rate as a strict requirement for performance. This is\nassumption is often violated in real-world applications, when models run on\nembedded devices, often at only a few frames per second.\n  Videos at low frame-rate suffer from large object displacements. Here\nre-identification features may support to match large-displaced object\ndetections, but current joint detection and re-identification formulations\ndegrade the detector performance, as these two are contrasting tasks. In the\nreal-world application having separate detector and re-id models is often not\nfeasible, as both the memory and runtime effectively double.\n  Towards robust long-term tracking applicable to reduced-computational-power\ndevices, we propose the first joint optimization of detection, tracking and\nre-identification features for videos. Notably, our joint optimization\nmaintains the detector performance, a typical multi-task challenge. At\ninference time, we leverage detections for tracking (tracking-by-detection)\nwhen the objects are visible, detectable and slowly moving in the image. We\nleverage instead re-identification features to match objects which disappeared\n(e.g. due to occlusion) for several frames or were not tracked due to fast\nmotion (or low-frame-rate videos). Our proposed method reaches the\nstate-of-the-art on MOT, it ranks 1st in the UA-DETRAC'18 tracking challenge\namong online trackers, and 3rd overall.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 21:06:40 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 11:42:49 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Munjal", "Bharti", ""], ["Aftab", "Abdul Rafey", ""], ["Amin", "Sikandar", ""], ["Brandlmaier", "Meltem D.", ""], ["Tombari", "Federico", ""], ["Galasso", "Fabio", ""]]}, {"id": "2005.10912", "submitter": "Hadi Shafiee Dr.", "authors": "Prudhvi Thirumalaraju, Manoj Kumar Kanakasabapathy, Charles L Bormann,\n  Raghav Gupta, Rohan Pooniwala, Hemanth Kandula, Irene Souter, Irene\n  Dimitriadis, Hadi Shafiee", "title": "Evaluation of deep convolutional neural networks in classifying human\n  embryo images based on their morphological quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A critical factor that influences the success of an in-vitro fertilization\n(IVF) procedure is the quality of the transferred embryo. Embryo morphology\nassessments, conventionally performed through manual microscopic analysis\nsuffer from disparities in practice, selection criteria, and subjectivity due\nto the experience of the embryologist. Convolutional neural networks (CNNs) are\npowerful, promising algorithms with significant potential for accurate\nclassifications across many object categories. Network architectures and\nhyper-parameters affect the efficiency of CNNs for any given task. Here, we\nevaluate multi-layered CNNs developed from scratch and popular deep-learning\narchitectures such as Inception v3, ResNET, Inception-ResNET-v2, and Xception\nin differentiating between embryos based on their morphological quality at 113\nhours post insemination (hpi). Xception performed the best in differentiating\nbetween the embryos based on their morphological quality.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 21:21:22 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Thirumalaraju", "Prudhvi", ""], ["Kanakasabapathy", "Manoj Kumar", ""], ["Bormann", "Charles L", ""], ["Gupta", "Raghav", ""], ["Pooniwala", "Rohan", ""], ["Kandula", "Hemanth", ""], ["Souter", "Irene", ""], ["Dimitriadis", "Irene", ""], ["Shafiee", "Hadi", ""]]}, {"id": "2005.10915", "submitter": "Sourya Dipta Das", "authors": "Sourya Dipta Das, Soumil Mandal", "title": "Team Neuro at SemEval-2020 Task 8: Multi-Modal Fine Grain Emotion\n  Classification of Memes using Multitask Learning", "comments": "Proceedings of the International Workshop on Semantic Evaluation\n  (SemEval)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we describe the system that we used for the memotion\nanalysis challenge, which is Task 8 of SemEval-2020. This challenge had three\nsubtasks where affect based sentiment classification of the memes was required\nalong with intensities. The system we proposed combines the three tasks into a\nsingle one by representing it as multi-label hierarchical classification\nproblem.Here,Multi-Task learning or Joint learning Procedure is used to train\nour model.We have used dual channels to extract text and image based features\nfrom separate Deep Neural Network Backbone and aggregate them to create task\nspecific features. These task specific aggregated feature vectors ware then\npassed on to smaller networks with dense layers, each one assigned for\npredicting one type of fine grain sentiment label. Our Proposed method show the\nsuperiority of this system in few tasks to other best models from the\nchallenge.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 21:29:44 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Das", "Sourya Dipta", ""], ["Mandal", "Soumil", ""]]}, {"id": "2005.10929", "submitter": "Viet Anh Trinh", "authors": "Viet Anh Trinh, Michael I Mandel", "title": "Large scale evaluation of importance maps in automatic speech\n  recognition", "comments": "submitted to INTERSPEECH 2020", "journal-ref": "Proceedings of Interspeech 2020", "doi": "10.21437/Interspeech.2020-2883", "report-no": null, "categories": "cs.SD cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a metric that we call the structured saliency\nbenchmark (SSBM) to evaluate importance maps computed for automatic speech\nrecognizers on individual utterances. These maps indicate time-frequency points\nof the utterance that are most important for correct recognition of a target\nword. Our evaluation technique is not only suitable for standard classification\ntasks, but is also appropriate for structured prediction tasks like\nsequence-to-sequence models. Additionally, we use this approach to perform a\nlarge scale comparison of the importance maps created by our previously\nintroduced technique using \"bubble noise\" to identify important points through\ncorrelation with a baseline approach based on smoothed speech energy and forced\nalignment. Our results show that the bubble analysis approach is better at\nidentifying important speech regions than this baseline on 100 sentences from\nthe AMI corpus.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 22:39:51 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Trinh", "Viet Anh", ""], ["Mandel", "Michael I", ""]]}, {"id": "2005.10940", "submitter": "Hao Tang", "authors": "Hao Tang, Hong Liu, Wei Xiao, Nicu Sebe", "title": "When Dictionary Learning Meets Deep Learning: Deep Dictionary Learning\n  and Coding Network for Image Recognition with Limited Data", "comments": "Accepted to TNNLS, an extended version of a paper published in\n  WACV2019. arXiv admin note: substantial text overlap with arXiv:1809.04185", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Deep Dictionary Learning and Coding Network (DDLCN) for\nimage recognition tasks with limited data. The proposed DDLCN has most of the\nstandard deep learning layers (e.g., input/output, pooling, fully connected,\netc.), but the fundamental convolutional layers are replaced by our proposed\ncompound dictionary learning and coding layers. The dictionary learning learns\nan over-complete dictionary for input training data. At the deep coding layer,\na locality constraint is added to guarantee that the activated dictionary bases\nare close to each other. Then the activated dictionary atoms are assembled and\npassed to the compound dictionary learning and coding layers. In this way, the\nactivated atoms in the first layer can be represented by the deeper atoms in\nthe second dictionary. Intuitively, the second dictionary is designed to learn\nthe fine-grained components shared among the input dictionary atoms, thus a\nmore informative and discriminative low-level representation of the dictionary\natoms can be obtained. We empirically compare DDLCN with several leading\ndictionary learning methods and deep learning models. Experimental results on\nfive popular datasets show that DDLCN achieves competitive results compared\nwith state-of-the-art methods when the training data is limited. Code is\navailable at https://github.com/Ha0Tang/DDLCN.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 23:12:10 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Tang", "Hao", ""], ["Liu", "Hong", ""], ["Xiao", "Wei", ""], ["Sebe", "Nicu", ""]]}, {"id": "2005.10953", "submitter": "Xiaoxu Li", "authors": "Xiaoxu Li and Zhuo Sun and Jing-Hao Xue and Zhanyu Ma", "title": "A Concise Review of Recent Few-shot Meta-learning Methods", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot meta-learning has been recently reviving with expectations to mimic\nhumanity's fast adaption to new concepts based on prior knowledge. In this\nshort communication, we give a concise review on recent representative methods\nin few-shot meta-learning, which are categorized into four branches according\nto their technical characteristics. We conclude this review with some vital\ncurrent challenges and future prospects in few-shot meta-learning.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 00:39:14 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Li", "Xiaoxu", ""], ["Sun", "Zhuo", ""], ["Xue", "Jing-Hao", ""], ["Ma", "Zhanyu", ""]]}, {"id": "2005.10954", "submitter": "Mohammad Rami Koujan", "authors": "Mohammad Rami Koujan, Michail Christos Doukas, Anastasios Roussos,\n  Stefanos Zafeiriou", "title": "Head2Head: Video-based Neural Head Synthesis", "comments": "To be published in 15th IEEE International Conference on Automatic\n  Face and Gesture Recognition (FG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel machine learning architecture for facial\nreenactment. In particular, contrary to the model-based approaches or recent\nframe-based methods that use Deep Convolutional Neural Networks (DCNNs) to\ngenerate individual frames, we propose a novel method that (a) exploits the\nspecial structure of facial motion (paying particular attention to mouth\nmotion) and (b) enforces temporal consistency. We demonstrate that the proposed\nmethod can transfer facial expressions, pose and gaze of a source actor to a\ntarget video in a photo-realistic fashion more accurately than state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 00:44:43 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Koujan", "Mohammad Rami", ""], ["Doukas", "Michail Christos", ""], ["Roussos", "Anastasios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2005.10957", "submitter": "Yiping Wang", "authors": "Yiping Wang, David Farnell, Hossein Farahani, Mitchell Nursey, Basile\n  Tessier-Cloutier, Steven J.M. Jones, David G. Huntsman, C. Blake Gilks, Ali\n  Bashashati", "title": "Classification of Epithelial Ovarian Carcinoma Whole-Slide Pathology\n  Images Using Deep Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/VXdQD8B307", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ovarian cancer is the most lethal cancer of the female reproductive organs.\nThere are $5$ major histological subtypes of epithelial ovarian cancer, each\nwith distinct morphological, genetic, and clinical features. Currently, these\nhistotypes are determined by a pathologist's microscopic examination of tumor\nwhole-slide images (WSI). This process has been hampered by poor inter-observer\nagreement (Cohen's kappa $0.54$-$0.67$). We utilized a \\textit{two}-stage deep\ntransfer learning algorithm based on convolutional neural networks (CNN) and\nprogressive resizing for automatic classification of epithelial ovarian\ncarcinoma WSIs. The proposed algorithm achieved a mean accuracy of $87.54\\%$\nand Cohen's kappa of $0.8106$ in the slide-level classification of $305$ WSIs;\nperforming better than a standard CNN and pathologists without\ngynecology-specific training.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 01:14:05 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 03:03:56 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Wang", "Yiping", ""], ["Farnell", "David", ""], ["Farahani", "Hossein", ""], ["Nursey", "Mitchell", ""], ["Tessier-Cloutier", "Basile", ""], ["Jones", "Steven J. M.", ""], ["Huntsman", "David G.", ""], ["Gilks", "C. Blake", ""], ["Bashashati", "Ali", ""]]}, {"id": "2005.10977", "submitter": "Zhi Qiao", "authors": "Zhi Qiao, Yu Zhou, Dongbao Yang, Yucan Zhou, Weiping Wang", "title": "SEED: Semantics Enhanced Encoder-Decoder Framework for Scene Text\n  Recognition", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition is a hot research topic in computer vision. Recently,\nmany recognition methods based on the encoder-decoder framework have been\nproposed, and they can handle scene texts of perspective distortion and curve\nshape. Nevertheless, they still face lots of challenges like image blur, uneven\nillumination, and incomplete characters. We argue that most encoder-decoder\nmethods are based on local visual features without explicit global semantic\ninformation. In this work, we propose a semantics enhanced encoder-decoder\nframework to robustly recognize low-quality scene texts. The semantic\ninformation is used both in the encoder module for supervision and in the\ndecoder module for initializing. In particular, the state-of-the art ASTER\nmethod is integrated into the proposed framework as an exemplar. Extensive\nexperiments demonstrate that the proposed framework is more robust for\nlow-quality text images, and achieves state-of-the-art results on several\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 03:02:46 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Qiao", "Zhi", ""], ["Zhou", "Yu", ""], ["Yang", "Dongbao", ""], ["Zhou", "Yucan", ""], ["Wang", "Weiping", ""]]}, {"id": "2005.10979", "submitter": "Prateek Shroff", "authors": "Prateek Shroff, Tianlong Chen, Yunchao Wei, Zhangyang Wang", "title": "Focus Longer to See Better:Recursively Refined Attention for\n  Fine-Grained Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network has shown great strides in the coarse-grained image\nclassification task. It was in part due to its strong ability to extract\ndiscriminative feature representations from the images. However, the marginal\nvisual difference between different classes in fine-grained images makes this\nvery task harder. In this paper, we tried to focus on these marginal\ndifferences to extract more representative features. Similar to human vision,\nour network repetitively focuses on parts of images to spot small\ndiscriminative parts among the classes. Moreover, we show through\ninterpretability techniques how our network focus changes from coarse to fine\ndetails. Through our experiments, we also show that a simple attention model\ncan aggregate (weighted) these finer details to focus on the most dominant\ndiscriminative part of the image. Our network uses only image-level labels and\ndoes not need bounding box/part annotation information. Further, the simplicity\nof our network makes it an easy plug-n-play module. Apart from providing\ninterpretability, our network boosts the performance (up to 2%) when compared\nto its baseline counterparts. Our codebase is available at\nhttps://github.com/TAMU-VITA/Focus-Longer-to-See-Better\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 03:14:18 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Shroff", "Prateek", ""], ["Chen", "Tianlong", ""], ["Wei", "Yunchao", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2005.10984", "submitter": "Zhuojun Chen", "authors": "Donggen Dai, Wangkit Wong, Zhuojun Chen", "title": "RankPose: Learning Generalised Feature with Rank Supervision for Head\n  Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the challenging problem of RGB image-based head pose estimation.\nWe first reformulate head pose representation learning to constrain it to a\nbounded space. Head pose represented as vector projection or vector angles\nshows helpful to improving performance. Further, a ranking loss combined with\nMSE regression loss is proposed. The ranking loss supervises a neural network\nwith paired samples of the same person and penalises incorrect ordering of pose\nprediction. Analysis on this new loss function suggests it contributes to a\nbetter local feature extractor, where features are generalised to Abstract\nLandmarks which are pose-related features instead of pose-irrelevant\ninformation such as identity, age, and lighting. Extensive experiments show\nthat our method significantly outperforms the current state-of-the-art schemes\non public datasets: AFLW2000 and BIWI. Our model achieves significant\nimprovements over previous SOTA MAE on AFLW2000 and BIWI from 4.50 to 3.66 and\nfrom 4.0 to 3.71 respectively. Source code will be made available at:\nhttps://github.com/seathiefwang/RankHeadPose.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 03:27:30 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Dai", "Donggen", ""], ["Wong", "Wangkit", ""], ["Chen", "Zhuojun", ""]]}, {"id": "2005.10985", "submitter": "Seonwoo Lee", "authors": "SeonWoo Lee, HyeonTak Yu, HoJun Yang, JaeHeung Yang, GangMin Lim,\n  KyuSung Kim, ByeongKeun Choi, and JangWoo Kwon", "title": "Apply VGGNet-based deep learning model of vibration data for prediction\n  model of gravity acceleration equipment", "comments": "15 pages, 10 figures \"for associated publication of paper is as\n  follow: Journal of Mechanics in Medicine and Biology,\n  https://www.worldscientific.com/worldscinet/jmmb\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Hypergravity accelerators are a type of large machinery used for gravity\ntraining or medical research. A failure of such large equipment can be a\nserious problem in terms of safety or costs. This paper proposes a prediction\nmodel that can proactively prevent failures that may occur in a hypergravity\naccelerator. The method proposed in this paper was to convert vibration signals\nto spectograms and perform classification training using a deep learning model.\nAn experiment was conducted to evaluate the performance of the method proposed\nin this paper. A 4-channel accelerometer was attached to the bearing housing,\nwhich is a rotor, and time-amplitude data were obtained from the measured\nvalues by sampling. The data were converted to a two-dimensional spectrogram,\nand classification training was performed using a deep learning model for four\nconditions of the equipment: Unbalance, Misalignment, Shaft Rubbing, and\nNormal. The experimental results showed that the proposed method had a 99.5%\nF1-Score, which was up to 23% higher than the 76.25% for existing feature-based\nlearning models.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 03:36:06 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 02:49:31 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Lee", "SeonWoo", ""], ["Yu", "HyeonTak", ""], ["Yang", "HoJun", ""], ["Yang", "JaeHeung", ""], ["Lim", "GangMin", ""], ["Kim", "KyuSung", ""], ["Choi", "ByeongKeun", ""], ["Kwon", "JangWoo", ""]]}, {"id": "2005.10986", "submitter": "Jia-Wei Chen", "authors": "Jia-Wei Chen, Rongfang Wang, Fan Ding, Bo Liu, Licheng Jiao, Jie Zhang", "title": "A Convolutional Neural Network with Parallel Multi-Scale Spatial Pooling\n  to Detect Temporal Changes in SAR Images", "comments": null, "journal-ref": "Remote Sens. 2020, 12, 1619", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In synthetic aperture radar (SAR) image change detection, it is quite\nchallenging to exploit the changing information from the noisy difference image\nsubject to the speckle. In this paper, we propose a multi-scale spatial pooling\n(MSSP) network to exploit the changed information from the noisy difference\nimage. Being different from the traditional convolutional network with only\nmono-scale pooling kernels, in the proposed method, multi-scale pooling kernels\nare equipped in a convolutional network to exploit the spatial context\ninformation on changed regions from the difference image. Furthermore, to\nverify the generalization of the proposed method, we apply our proposed method\nto the cross-dataset bitemporal SAR image change detection, where the MSSP\nnetwork (MSSP-Net) is trained on a dataset and then applied to an unknown\ntesting dataset. We compare the proposed method with other state-of-arts and\nthe comparisons are performed on four challenging datasets of bitemporal SAR\nimages. Experimental results demonstrate that our proposed method obtains\ncomparable results with S-PCA-Net on YR-A and YR-B dataset and outperforms\nother state-of-art methods, especially on the Sendai-A and Sendai-B datasets\nwith more complex scenes. More important, MSSP-Net is more efficient than\nS-PCA-Net and convolutional neural networks (CNN) with less executing time in\nboth training and testing phases.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 03:37:30 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Chen", "Jia-Wei", ""], ["Wang", "Rongfang", ""], ["Ding", "Fan", ""], ["Liu", "Bo", ""], ["Jiao", "Licheng", ""], ["Zhang", "Jie", ""]]}, {"id": "2005.10987", "submitter": "Yong Man Ro", "authors": "Youngjoon Yu, Hong Joo Lee, Byeong Cheon Kim, Jung Uk Kim, Yong Man Ro", "title": "Investigating Vulnerability to Adversarial Examples on Multimodal Data\n  Fusion in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of multimodal data fusion in deep learning appears to be\nattributed to the use of complementary in-formation between multiple input\ndata. Compared to their predictive performance, relatively less attention has\nbeen devoted to the robustness of multimodal fusion models. In this paper, we\ninvestigated whether the current multimodal fusion model utilizes the\ncomplementary intelligence to defend against adversarial attacks. We applied\ngradient based white-box attacks such as FGSM and PGD on MFNet, which is a\nmajor multispectral (RGB, Thermal) fusion deep learning model for semantic\nsegmentation. We verified that the multimodal fusion model optimized for better\nprediction is still vulnerable to adversarial attack, even if only one of the\nsensors is attacked. Thus, it is hard to say that existing multimodal data\nfusion models are fully utilizing complementary relationships between multiple\nmodalities in terms of adversarial robustness. We believe that our observations\nopen a new horizon for adversarial attack research on multimodal data fusion.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 03:45:06 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Yu", "Youngjoon", ""], ["Lee", "Hong Joo", ""], ["Kim", "Byeong Cheon", ""], ["Kim", "Jung Uk", ""], ["Ro", "Yong Man", ""]]}, {"id": "2005.10992", "submitter": "Dat Tran Mr", "authors": "Nhan T. Nguyen, Dat Q. Tran, Nghia T. Nguyen, Ha Q. Nguyen", "title": "A CNN-LSTM Architecture for Detection of Intracranial Hemorrhage on CT\n  scans", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/1IoPbyuPFT", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method that combines a convolutional neural network (CNN)\nwith a long short-term memory (LSTM) mechanism for accurate prediction of\nintracranial hemorrhage on computed tomography (CT) scans. The CNN plays the\nrole of a slice-wise feature extractor while the LSTM is responsible for\nlinking the features across slices. The whole architecture is trained\nend-to-end with input being an RGB-like image formed by stacking 3 different\nviewing windows of a single slice. We validate the method on the recent RSNA\nIntracranial Hemorrhage Detection challenge and on the CQ500 dataset. For the\nRSNA challenge, our best single model achieves a weighted log loss of 0.0522 on\nthe leaderboard, which is comparable to the top 3% performances, almost all of\nwhich make use of ensemble learning. Importantly, our method generalizes very\nwell: the model trained on the RSNA dataset significantly outperforms the 2D\nmodel, which does not take into account the relationship between slices, on\nCQ500. Our codes and models is publicly avaiable at\nhttps://github.com/VinBDI-MedicalImagingTeam/midl2020-cnnlstm-ich.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 04:00:04 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 05:55:26 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 19:03:36 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Nguyen", "Nhan T.", ""], ["Tran", "Dat Q.", ""], ["Nguyen", "Nghia T.", ""], ["Nguyen", "Ha Q.", ""]]}, {"id": "2005.10999", "submitter": "Xuequan Lu", "authors": "Chengwei Chen, Wang Yuan, Xuequan Lu, Lizhuang Ma", "title": "Spoof Face Detection Via Semi-Supervised Adversarial Training", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face spoofing causes severe security threats in face recognition systems.\nPrevious anti-spoofing works focused on supervised techniques, typically with\neither binary or auxiliary supervision. Most of them suffer from limited\nrobustness and generalization, especially in the cross-dataset setting. In this\npaper, we propose a semi-supervised adversarial learning framework for spoof\nface detection, which largely relaxes the supervision condition. To capture the\nunderlying structure of live faces data in latent representation space, we\npropose to train the live face data only, with a convolutional Encoder-Decoder\nnetwork acting as a Generator. Meanwhile, we add a second convolutional network\nserving as a Discriminator. The generator and discriminator are trained by\ncompeting with each other while collaborating to understand the underlying\nconcept in the normal class(live faces). Since the spoof face detection is\nvideo based (i.e., temporal information), we intuitively take the optical flow\nmaps converted from consecutive video frames as input. Our approach is free of\nthe spoof faces, thus being robust and general to different types of spoof,\neven unknown spoof. Extensive experiments on intra- and cross-dataset tests\nshow that our semi-supervised method achieves better or comparable results to\nstate-of-the-art supervised techniques.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 04:32:33 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Chen", "Chengwei", ""], ["Yuan", "Wang", ""], ["Lu", "Xuequan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2005.11003", "submitter": "Jieli Zhou", "authors": "Jieli Zhou, Baoyu Jing, Zeya Wang", "title": "SODA: Detecting Covid-19 in Chest X-rays with Semi-supervised Open Set\n  Domain Adaptation", "comments": "BIOKDD 2020: 19th International Workshop on Data Mining in\n  Bioinformatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the shortage of COVID-19 viral testing kits and the long waiting time,\nradiology imaging is used to complement the screening process and triage\npatients into different risk levels. Deep learning based methods have taken an\nactive role in automatically detecting COVID-19 disease in chest x-ray images,\nas witnessed in many recent works in early 2020. Most of these works first\ntrain a Convolutional Neural Network (CNN) on an existing large-scale chest\nx-ray image dataset and then fine-tune it with a COVID-19 dataset at a much\nsmaller scale. However, direct transfer across datasets from different domains\nmay lead to poor performance for CNN due to two issues, the large domain shift\npresent in the biomedical imaging datasets and the extremely small scale of the\nCOVID-19 chest x-ray dataset. In an attempt to address these two important\nissues, we formulate the problem of COVID-19 chest x-ray image classification\nin a semi-supervised open set domain adaptation setting and propose a novel\ndomain adaptation method, Semi-supervised Open set Domain Adversarial network\n(SODA). SODA is able to align the data distributions across different domains\nin a general domain space and also in a common subspace of source and target\ndata. In our experiments, SODA achieves a leading classification performance\ncompared with recent state-of-the-art models in separating COVID-19 with common\npneumonia. We also present initial results showing that SODA can produce better\npathology localizations in the chest x-rays.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 04:58:28 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 05:17:57 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Zhou", "Jieli", ""], ["Jing", "Baoyu", ""], ["Wang", "Zeya", ""]]}, {"id": "2005.11031", "submitter": "Giulia Cisotto", "authors": "Giulia Cisotto, Martina Capuzzo, Anna V. Guglielmi, Andrea Zanella", "title": "Feature selection for gesture recognition in Internet-of-Things for\n  healthcare", "comments": null, "journal-ref": "ICC 2020 - 2020 IEEE International Conference on Communications\n  (ICC)", "doi": "10.1109/ICC40277.2020.9149381", "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things is rapidly spreading across several fields, including\nhealthcare, posing relevant questions related to communication capabilities,\nenergy efficiency and sensors unobtrusiveness. Particularly, in the context of\nrecognition of gestures, e.g., grasping of different objects, brain and\nmuscular activity could be simultaneously recorded via EEG and EMG,\nrespectively, and analyzed to identify the gesture that is being accomplished,\nand the quality of its performance. This paper proposes a new algorithm that\naims (i) to robustly extract the most relevant features to classify different\ngrasping tasks, and (ii) to retain the natural meaning of the selected\nfeatures. This, in turn, gives the opportunity to simplify the recording setup\nto minimize the data traffic over the communication network, including\nInternet, and provide physiologically significant features for medical\ninterpretation. The algorithm robustness is ensured both by consensus\nclustering as a feature selection strategy, and by nested cross-validation\nscheme to evaluate its classification performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 06:54:53 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Cisotto", "Giulia", ""], ["Capuzzo", "Martina", ""], ["Guglielmi", "Anna V.", ""], ["Zanella", "Andrea", ""]]}, {"id": "2005.11034", "submitter": "Yuan Zhou", "authors": "Shijie Hao and Yuan Zhou and Yanrong Guo and Richang Hong and Jun\n  Cheng and Meng Wang", "title": "Real-time Semantic Segmentation via Spatial-detail Guided Context\n  Propagation", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, vision-based computing tasks play an important role in various\nreal-world applications. However, many vision computing tasks, e.g. semantic\nsegmentation, are usually computationally expensive, posing a challenge to the\ncomputing systems that are resource-constrained but require fast response\nspeed. Therefore, it is valuable to develop accurate and real-time vision\nprocessing models that only require limited computational resources. To this\nend, we propose the Spatial-detail Guided Context Propagation Network (SGCPNet)\nfor achieving real-time semantic segmentation. In SGCPNet, we propose the\nstrategy of spatial-detail guided context propagation. It uses the spatial\ndetails of shallow layers to guide the propagation of the low-resolution global\ncontexts, in which the lost spatial information can be effectively\nreconstructed. In this way, the need for maintaining high-resolution features\nalong the network is freed, therefore largely improving the model efficiency.\nOn the other hand, due to the effective reconstruction of spatial details, the\nsegmentation accuracy can be still preserved. In the experiments, we validate\nthe effectiveness and efficiency of the proposed SGCPNet model. On the\nCitysacpes dataset, for example, our SGCPNet achieves 69.5 % mIoU segmentation\naccuracy, while its speed reaches 178.5 FPS on 768x1536 images on a GeForce GTX\n1080 Ti GPU card.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 07:07:26 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 08:05:14 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 04:50:39 GMT"}, {"version": "v4", "created": "Thu, 24 Jun 2021 13:09:41 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Hao", "Shijie", ""], ["Zhou", "Yuan", ""], ["Guo", "Yanrong", ""], ["Hong", "Richang", ""], ["Cheng", "Jun", ""], ["Wang", "Meng", ""]]}, {"id": "2005.11035", "submitter": "Jangho Kim", "authors": "Jangho Kim, KiYoon Yoo, Nojun Kwak", "title": "Position-based Scaled Gradient for Model Quantization and Pruning", "comments": "Advances in Neural Information Processing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the position-based scaled gradient (PSG) that scales the gradient\ndepending on the position of a weight vector to make it more\ncompression-friendly. First, we theoretically show that applying PSG to the\nstandard gradient descent (GD), which is called PSGD, is equivalent to the GD\nin the warped weight space, a space made by warping the original weight space\nvia an appropriately designed invertible function. Second, we empirically show\nthat PSG acting as a regularizer to a weight vector is favorable for model\ncompression domains such as quantization and pruning. PSG reduces the gap\nbetween the weight distributions of a full-precision model and its compressed\ncounterpart. This enables the versatile deployment of a model either as an\nuncompressed mode or as a compressed mode depending on the availability of\nresources. The experimental results on CIFAR-10/100 and ImageNet datasets show\nthe effectiveness of the proposed PSG in both domains of pruning and\nquantization even for extremely low bits. The code is released in Github.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 07:11:27 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 01:22:47 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2020 07:00:07 GMT"}, {"version": "v4", "created": "Wed, 11 Nov 2020 03:43:25 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Kim", "Jangho", ""], ["Yoo", "KiYoon", ""], ["Kwak", "Nojun", ""]]}, {"id": "2005.11037", "submitter": "Xin Jin", "authors": "Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, Li Zhang", "title": "Style Normalization and Restitution for Generalizable Person\n  Re-identification", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing fully-supervised person re-identification (ReID) methods usually\nsuffer from poor generalization capability caused by domain gaps. The key to\nsolving this problem lies in filtering out identity-irrelevant interference and\nlearning domain-invariant person representations. In this paper, we aim to\ndesign a generalizable person ReID framework which trains a model on source\ndomains yet is able to generalize/perform well on target domains. To achieve\nthis goal, we propose a simple yet effective Style Normalization and\nRestitution (SNR) module. Specifically, we filter out style variations (e.g.,\nillumination, color contrast) by Instance Normalization (IN). However, such a\nprocess inevitably removes discriminative information. We propose to distill\nidentity-relevant feature from the removed information and restitute it to the\nnetwork to ensure high discrimination. For better disentanglement, we enforce a\ndual causal loss constraint in SNR to encourage the separation of\nidentity-relevant features and identity-irrelevant features. Extensive\nexperiments demonstrate the strong generalization capability of our framework.\nOur models empowered by the SNR modules significantly outperform the\nstate-of-the-art domain generalization approaches on multiple widely-used\nperson ReID benchmarks, and also show superiority on unsupervised domain\nadaptation.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 07:15:10 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Jin", "Xin", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Chen", "Zhibo", ""], ["Zhang", "Li", ""]]}, {"id": "2005.11043", "submitter": "Hongyu Li", "authors": "Hongyu Li, Xiaogang Huang, Zhihui Fu, and Xiaolin Li", "title": "Arbitrary-sized Image Training and Residual Kernel Learning: Towards\n  Image Fraud Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preserving original noise residuals in images are critical to image fraud\nidentification. Since the resizing operation during deep learning will damage\nthe microstructures of image noise residuals, we propose a framework for\ndirectly training images of original input scales without resizing. Our\narbitrary-sized image training method mainly depends on the pseudo-batch\ngradient descent (PBGD), which bridges the gap between the input batch and the\nupdate batch to assure that model updates can normally run for arbitrary-sized\nimages.\n  In addition, a 3-phase alternate training strategy is designed to learn\noptimal residual kernels for image fraud identification. With the learnt\nresidual kernels and PBGD, the proposed framework achieved the state-of-the-art\nresults in image fraud identification, especially for images with small\ntampered regions or unseen images with different tampering distributions.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 07:57:24 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Li", "Hongyu", ""], ["Huang", "Xiaogang", ""], ["Fu", "Zhihui", ""], ["Li", "Xiaolin", ""]]}, {"id": "2005.11044", "submitter": "Marc Blanchon", "authors": "Marc Blanchon, Olivier Morel, Fabrice Meriaudeau, Ralph Seulin,\n  D\\'esir\\'e Sidib\\'e", "title": "Polarimetric image augmentation", "comments": "7 pages, submitted to ICPR2020 second round", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotics applications in urban environments are subject to obstacles that\nexhibit specular reflections hampering autonomous navigation. On the other\nhand, these reflections are highly polarized and this extra information can\nsuccessfully be used to segment the specular areas. In nature, polarized light\nis obtained by reflection or scattering. Deep Convolutional Neural Networks\n(DCNNs) have shown excellent segmentation results, but require a significant\namount of data to achieve best performances. The lack of data is usually\novercomed by using augmentation methods. However, unlike RGB images,\npolarization images are not only scalar (intensity) images and standard\naugmentation techniques cannot be applied straightforwardly. We propose to\nenhance deep learning models through a regularized augmentation procedure\napplied to polarimetric data in order to characterize scenes more effectively\nunder challenging conditions. We subsequently observe an average of 18.1%\nimprovement in IoU between non augmented and regularized training procedures on\nreal world data.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 08:00:12 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 15:12:40 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Blanchon", "Marc", ""], ["Morel", "Olivier", ""], ["Meriaudeau", "Fabrice", ""], ["Seulin", "Ralph", ""], ["Sidib\u00e9", "D\u00e9sir\u00e9", ""]]}, {"id": "2005.11061", "submitter": "Kazuhiro Takemoto", "authors": "Hokuto Hirano, Kazuki Koga, Kazuhiro Takemoto", "title": "Vulnerability of deep neural networks for detecting COVID-19 cases from\n  chest X-ray images to universal adversarial attacks", "comments": "17 pages, 5 figures, 3 tables", "journal-ref": "PLoS ONE 5(12), e0243963 (2020)", "doi": "10.1371/journal.pone.0243963", "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the epidemic of the novel coronavirus disease 2019 (COVID-19), chest\nX-ray computed tomography imaging is being used for effectively screening\nCOVID-19 patients. The development of computer-aided systems based on deep\nneural networks (DNNs) has been advanced, to rapidly and accurately detect\nCOVID-19 cases, because the need for expert radiologists, who are limited in\nnumber, forms a bottleneck for the screening. However, so far, the\nvulnerability of DNN-based systems has been poorly evaluated, although DNNs are\nvulnerable to a single perturbation, called universal adversarial perturbation\n(UAP), which can induce DNN failure in most classification tasks. Thus, we\nfocus on representative DNN models for detecting COVID-19 cases from chest\nX-ray images and evaluate their vulnerability to UAPs generated using simple\niterative algorithms. We consider nontargeted UAPs, which cause a task failure\nresulting in an input being assigned an incorrect label, and targeted UAPs,\nwhich cause the DNN to classify an input into a specific class. The results\ndemonstrate that the models are vulnerable to nontargeted and targeted UAPs,\neven in case of small UAPs. In particular, 2% norm of the UPAs to the average\nnorm of an image in the image dataset achieves >85% and >90% success rates for\nthe nontargeted and targeted attacks, respectively. Due to the nontargeted\nUAPs, the DNN models judge most chest X-ray images as COVID-19 cases. The\ntargeted UAPs make the DNN models classify most chest X-ray images into a given\ntarget class. The results indicate that careful consideration is required in\npractical applications of DNNs to COVID-19 diagnosis; in particular, they\nemphasize the need for strategies to address security concerns. As an example,\nwe show that iterative fine-tuning of the DNN models using UAPs improves the\nrobustness of the DNN models against UAPs.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 08:54:41 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Hirano", "Hokuto", ""], ["Koga", "Kazuki", ""], ["Takemoto", "Kazuhiro", ""]]}, {"id": "2005.11077", "submitter": "Zhezhang Ding", "authors": "Donghao Xu, Zhezhang Ding, Chenfeng Tu, Huijing Zhao, Mathieu Moze,\n  Fran\\c{c}ois Aioun, and Franck Guillemard", "title": "Driver Identification through Stochastic Multi-State Car-Following\n  Modeling", "comments": "13 pages, 4 figures. Submitted to T.ITS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intra-driver and inter-driver heterogeneity has been confirmed to exist in\nhuman driving behaviors by many studies. In this study, a joint model of the\ntwo types of heterogeneity in car-following behavior is proposed as an approach\nof driver profiling and identification. It is assumed that all drivers share a\npool of driver states; under each state a car-following data sequence obeys a\nspecific probability distribution in feature space; each driver has his/her own\nprobability distribution over the states, called driver profile, which\ncharacterize the intradriver heterogeneity, while the difference between the\ndriver profile of different drivers depict the inter-driver heterogeneity.\nThus, the driver profile can be used to distinguish a driver from others. Based\non the assumption, a stochastic car-following model is proposed to take both\nintra-driver and inter-driver heterogeneity into consideration, and a method is\nproposed to jointly learn parameters in behavioral feature extractor, driver\nstates and driver profiles. Experiments demonstrate the performance of the\nproposed method in driver identification on naturalistic car-following data:\naccuracy of 82.3% is achieved in an 8-driver experiment using 10 car-following\nsequences of duration 15 seconds for online inference. The potential of fast\nregistration of new drivers are demonstrated and discussed.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 09:39:00 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Xu", "Donghao", ""], ["Ding", "Zhezhang", ""], ["Tu", "Chenfeng", ""], ["Zhao", "Huijing", ""], ["Moze", "Mathieu", ""], ["Aioun", "Fran\u00e7ois", ""], ["Guillemard", "Franck", ""]]}, {"id": "2005.11084", "submitter": "Rana Hanocka", "authors": "Rana Hanocka, Gal Metzer, Raja Giryes, Daniel Cohen-Or", "title": "Point2Mesh: A Self-Prior for Deformable Meshes", "comments": "SIGGRAPH 2020; Project page:\n  https://ranahanocka.github.io/point2mesh/", "journal-ref": null, "doi": "10.1145/3386569.3392415", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Point2Mesh, a technique for reconstructing a\nsurface mesh from an input point cloud. Instead of explicitly specifying a\nprior that encodes the expected shape properties, the prior is defined\nautomatically using the input point cloud, which we refer to as a self-prior.\nThe self-prior encapsulates reoccurring geometric repetitions from a single\nshape within the weights of a deep neural network. We optimize the network\nweights to deform an initial mesh to shrink-wrap a single input point cloud.\nThis explicitly considers the entire reconstructed shape, since shared local\nkernels are calculated to fit the overall object. The convolutional kernels are\noptimized globally across the entire shape, which inherently encourages\nlocal-scale geometric self-similarity across the shape surface. We show that\nshrink-wrapping a point cloud with a self-prior converges to a desirable\nsolution; compared to a prescribed smoothness prior, which often becomes\ntrapped in undesirable local minima. While the performance of traditional\nreconstruction approaches degrades in non-ideal conditions that are often\npresent in real world scanning, i.e., unoriented normals, noise and missing\n(low density) parts, Point2Mesh is robust to non-ideal conditions. We\ndemonstrate the performance of Point2Mesh on a large variety of shapes with\nvarying complexity.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 10:01:04 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Hanocka", "Rana", ""], ["Metzer", "Gal", ""], ["Giryes", "Raja", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2005.11092", "submitter": "Yuanxin Ye", "authors": "Yuanxin Ye, Chao Yang, Bai Zhu, Youquan He, and Huarong Jia", "title": "Improving Co-registration for Sentinel-1 SAR and Sentinel-2 Optical\n  images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-registering the Sentinel-1 SAR and Sentinel-2 optical data of European\nSpace Agency (ESA) is of great importance for many remote sensing applications.\nHowever, we find that there are evident misregistration shifts between the\nSentinel-1 SAR and Sentinel-2 optical images that are directly downloaded from\nthe official website. To address that, this paper presents a fast and effective\nregistration method for the two types of images. In the proposed method, a\nblock-based scheme is first designed to extract evenly distributed interest\npoints. Then the correspondences are detected by using the similarity of\nstructural features between the SAR and optical images, where the three\ndimension (3D) phase correlation (PC) is used as the similarity measure for\naccelerating image matching. Finally, the obtained correspondences are employed\nto measure the misregistration shifts between the images. Moreover, to\neliminate the misregistration, we use some representative geometric\ntransformation models such as polynomial models, projective models, and\nrational function models for the co-registration of the two types of images,\nand compare and analyze their registration accuracy under different numbers of\ncontrol points and different terrains. Six pairs of the Sentinel-1 SAR L1 and\nSentinel-2 optical L1C images covering three different terrains are tested in\nour experiments. Experimental results show that the proposed method can achieve\nprecise correspondences between the images, and the 3rd. Order polynomial\nachieves the most satisfactory registration results. Its registration accuracy\nof the flat areas is less than 1.0 10m pixels, and that of the hilly areas is\nabout 1.5 10m pixels, and that of the mountainous areas is between 1.7 and 2.3\n10m pixels, which significantly improves the co-registration accuracy of the\nSentinel-1 SAR and Sentinel-2 optical images.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 10:35:11 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 07:52:42 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ye", "Yuanxin", ""], ["Yang", "Chao", ""], ["Zhu", "Bai", ""], ["He", "Youquan", ""], ["Jia", "Huarong", ""]]}, {"id": "2005.11098", "submitter": "Ziheng Duan", "authors": "Ziheng Duan, Daniel Montes, Yangsibo Huang, Dufan Wu, Javier M.\n  Romero, Ramon Gilberto Gonzalez, Quanzheng Li", "title": "Deep Learning Based Detection and Localization of Cerebal Aneurysms in\n  Computed Tomography Angiography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting cerebral aneurysms is an important clinical task of brain computed\ntomography angiography (CTA). However, human interpretation could be time\nconsuming due to the small size of some aneurysms. In this work, we proposed\nDeepBrain, a deep learning based cerebral aneurysm detection and localization\nalgorithm. The algorithm consisted of a 3D faster region-proposal convolution\nneural network for aneurysm detection and localization, and a 3D multi-scale\nfully convolutional neural network for false positive reduction. Furthermore, a\nnovel hierarchical non-maximum suppression algorithm was proposed to process\nthe detection results in 3D, which greatly reduced the time complexity by\neliminating unnecessary comparisons. DeepBrain was trained and tested on 550\nbrain CTA scans and achieved sensitivity of 93.3% with 0.3 false positives per\npatient on average.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 10:49:23 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Duan", "Ziheng", ""], ["Montes", "Daniel", ""], ["Huang", "Yangsibo", ""], ["Wu", "Dufan", ""], ["Romero", "Javier M.", ""], ["Gonzalez", "Ramon Gilberto", ""], ["Li", "Quanzheng", ""]]}, {"id": "2005.11101", "submitter": "Javier Hernandez-Ortega", "authors": "Javier Hernandez-Ortega, Julian Fierrez, Aythami Morales, David Diaz", "title": "A Comparative Evaluation of Heart Rate Estimation Methods using Face\n  Videos", "comments": "Accepted in \"IEEE International Workshop on Medical Computing\n  (MediComp) 2020\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a comparative evaluation of methods for remote heart rate\nestimation using face videos, i.e., given a video sequence of the face as\ninput, methods to process it to obtain a robust estimation of the subjects\nheart rate at each moment. Four alternatives from the literature are tested,\nthree based in hand crafted approaches and one based on deep learning. The\nmethods are compared using RGB videos from the COHFACE database. Experiments\nshow that the learning-based method achieves much better accuracy than the hand\ncrafted ones. The low error rate achieved by the learning based model makes\npossible its application in real scenarios, e.g. in medical or sports\nenvironments.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 10:54:49 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Hernandez-Ortega", "Javier", ""], ["Fierrez", "Julian", ""], ["Morales", "Aythami", ""], ["Diaz", "David", ""]]}, {"id": "2005.11194", "submitter": "Charlie Kirkwood", "authors": "Charlie Kirkwood", "title": "Deep covariate-learning: optimising information extraction from terrain\n  texture for geostatistical modelling applications", "comments": "14 pages, 8 figures, submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Where data is available, it is desirable in geostatistical modelling to make\nuse of additional covariates, for example terrain data, in order to improve\nprediction accuracy in the modelling task. While elevation itself may be\nimportant, additional explanatory power for any given problem can be sought\n(but not necessarily found) by filtering digital elevation models to extract\nhigher-order derivatives such as slope angles, curvatures, and roughness. In\nessence, it would be beneficial to extract as much task-relevant information as\npossible from the elevation grid. However, given the complexities of the\nnatural world, chance dictates that the use of 'off-the-shelf' filters is\nunlikely to derive covariates that provide strong explanatory power to the\ntarget variable at hand, and any attempt to manually design informative\ncovariates is likely to be a trial-and-error process -- not optimal. In this\npaper we present a solution to this problem in the form of a deep learning\napproach to automatically deriving optimal task-specific terrain texture\ncovariates from a standard SRTM 90m gridded digital elevation model (DEM). For\nour target variables we use point-sampled geochemical data from the British\nGeological Survey: concentrations of potassium, calcium and arsenic in stream\nsediments. We find that our deep learning approach produces covariates for\ngeostatistical modelling that have surprisingly strong explanatory power on\ntheir own, with R-squared values around 0.6 for all three elements (with\narsenic on the log scale). These results are achieved without the neural\nnetwork being provided with easting, northing, or absolute elevation as inputs,\nand purely reflect the capacity of our deep neural network to extract\ntask-specific information from terrain texture. We hope that these results will\ninspire further investigation into the capabilities of deep learning within\ngeostatistical applications.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 14:00:28 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 11:19:48 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Kirkwood", "Charlie", ""]]}, {"id": "2005.11212", "submitter": "Max Tegmark", "authors": "Silviu-Marian Udrescu (MIT), Max Tegmark (MIT)", "title": "Symbolic Pregression: Discovering Physical Laws from Distorted Video", "comments": "Expanded and improved physics discussion, additional method details.\n  9 pages, 7 figs", "journal-ref": "Phys. Rev. E 103, 043307 (2021)", "doi": "10.1103/PhysRevE.103.043307", "report-no": null, "categories": "cs.CV cs.AI cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for unsupervised learning of equations of motion for\nobjects in raw and optionally distorted unlabeled video. We first train an\nautoencoder that maps each video frame into a low-dimensional latent space\nwhere the laws of motion are as simple as possible, by minimizing a combination\nof non-linearity, acceleration and prediction error. Differential equations\ndescribing the motion are then discovered using Pareto-optimal symbolic\nregression. We find that our pre-regression (\"pregression\") step is able to\nrediscover Cartesian coordinates of unlabeled moving objects even when the\nvideo is distorted by a generalized lens. Using intuition from multidimensional\nknot-theory, we find that the pregression step is facilitated by first adding\nextra latent space dimensions to avoid topological problems during training and\nthen removing these extra dimensions via principal component analysis.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 18:00:52 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 17:52:02 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Udrescu", "Silviu-Marian", "", "MIT"], ["Tegmark", "Max", "", "MIT"]]}, {"id": "2005.11217", "submitter": "Prashnna Gyawali", "authors": "Prashnna Kumar Gyawali, Sandesh Ghimire, Pradeep Bajracharya, Zhiyuan\n  Li, Linwei Wang", "title": "Semi-supervised Medical Image Classification with Global Latent Mixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer-aided diagnosis via deep learning relies on large-scale annotated\ndata sets, which can be costly when involving expert knowledge. Semi-supervised\nlearning (SSL) mitigates this challenge by leveraging unlabeled data. One\neffective SSL approach is to regularize the local smoothness of neural\nfunctions via perturbations around single data points. In this work, we argue\nthat regularizing the global smoothness of neural functions by filling the void\nin between data points can further improve SSL. We present a novel SSL approach\nthat trains the neural network on linear mixing of labeled and unlabeled data,\nat both the input and latent space in order to regularize different portions of\nthe network. We evaluated the presented model on two distinct medical image\ndata sets for semi-supervised classification of thoracic disease and skin\nlesion, demonstrating its improved performance over SSL with local\nperturbations and SSL with global mixing but at the input space only. Our code\nis available at https://github.com/Prasanna1991/LatentMixing.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 14:49:13 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Gyawali", "Prashnna Kumar", ""], ["Ghimire", "Sandesh", ""], ["Bajracharya", "Pradeep", ""], ["Li", "Zhiyuan", ""], ["Wang", "Linwei", ""]]}, {"id": "2005.11220", "submitter": "Geonseok Seo", "authors": "Geonseok Seo, Jaeyoung Yoo, Jaeseok Choi, Nojun Kwak", "title": "KL-Divergence-Based Region Proposal Network for Object Detection", "comments": "5 pages, 3 figures, Accepted to ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning of the region proposal in object detection using the deep neural\nnetworks (DNN) is divided into two tasks: binary classification and bounding\nbox regression task. However, traditional RPN (Region Proposal Network) defines\nthese two tasks as different problems, and they are trained independently. In\nthis paper, we propose a new region proposal learning method that considers the\nbounding box offset's uncertainty in the objectness score. Our method redefines\nRPN to a problem of minimizing the KL-divergence, difference between the two\nprobability distributions. We applied KL-RPN, which performs region proposal\nusing KL-Divergence, to the existing two-stage object detection framework and\nshowed that it can improve the performance of the existing method. Experiments\nshow that it achieves 2.6% and 2.0% AP improvements on MS COCO test-dev in\nFaster R-CNN with VGG-16 and R-FCN with ResNet-101 backbone, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 14:59:26 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Seo", "Geonseok", ""], ["Yoo", "Jaeyoung", ""], ["Choi", "Jaeseok", ""], ["Kwak", "Nojun", ""]]}, {"id": "2005.11235", "submitter": "Gautam Krishna", "authors": "Gautam Krishna, Co Tran, Mason Carnahan, Ahmed Tewfik", "title": "Predicting Video features from EEG and Vice versa", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore predicting facial or lip video features from\nelectroencephalography (EEG) features and predicting EEG features from recorded\nfacial or lip video frames using deep learning models. The subjects were asked\nto read out loud English sentences shown to them on a computer screen and their\nsimultaneous EEG signals and facial video frames were recorded. Our model was\nable to generate very broad characteristics of the facial or lip video frame\nfrom input EEG features. Our results demonstrate the first step towards\nsynthesizing high quality facial or lip video from recorded EEG features. We\ndemonstrate results for a data set consisting of seven subjects.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 20:04:38 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Krishna", "Gautam", ""], ["Tran", "Co", ""], ["Carnahan", "Mason", ""], ["Tewfik", "Ahmed", ""]]}, {"id": "2005.11246", "submitter": "Quentin Paletta", "authors": "Quentin Paletta, Joan Lasenby", "title": "Convolutional Neural Networks applied to sky images for short-term solar\n  irradiance forecasting", "comments": "4 pages, 7 figures, 1 table, accepted for European PV Solar Energy\n  Conference and Exhibition (EU-PVSEC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the advances in the field of solar energy, improvements of solar\nforecasting techniques, addressing the intermittent electricity production,\nremain essential for securing its future integration into a wider energy\nsupply. A promising approach to anticipate irradiance changes consists of\nmodeling the cloud cover dynamics from ground taken or satellite images. This\nwork presents preliminary results on the application of deep Convolutional\nNeural Networks for 2 to 20 min irradiance forecasting using hemispherical sky\nimages and exogenous variables. We evaluate the models on a set of irradiance\nmeasurements and corresponding sky images collected in Palaiseau (France) over\n8 months with a temporal resolution of 2 min. To outline the learning of neural\nnetworks in the context of short-term irradiance forecasting, we implemented\nvisualisation techniques revealing the types of patterns recognised by trained\nalgorithms in sky images. In addition, we show that training models with past\nsamples of the same day improves their forecast skill, relative to the smart\npersistence model based on the Mean Square Error, by around 10% on a 10 min\nahead prediction. These results emphasise the benefit of integrating previous\nsame-day data in short-term forecasting. This, in turn, can be achieved through\nmodel fine tuning or using recurrent units to facilitate the extraction of\nrelevant temporal features from past data.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 15:57:39 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Paletta", "Quentin", ""], ["Lasenby", "Joan", ""]]}, {"id": "2005.11282", "submitter": "Ashish Khetan", "authors": "Ashish Khetan, Zohar Karnin", "title": "PruneNet: Channel Pruning via Global Importance", "comments": "12 pages, 3 figures, Published in ICLR 2020 NAS Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Channel pruning is one of the predominant approaches for accelerating deep\nneural networks. Most existing pruning methods either train from scratch with a\nsparsity inducing term such as group lasso, or prune redundant channels in a\npretrained network and then fine tune the network. Both strategies suffer from\nsome limitations: the use of group lasso is computationally expensive,\ndifficult to converge and often suffers from worse behavior due to the\nregularization bias. The methods that start with a pretrained network either\nprune channels uniformly across the layers or prune channels based on the basic\nstatistics of the network parameters. These approaches either ignore the fact\nthat some CNN layers are more redundant than others or fail to adequately\nidentify the level of redundancy in different layers. In this work, we\ninvestigate a simple-yet-effective method for pruning channels based on a\ncomputationally light-weight yet effective data driven optimization step that\ndiscovers the necessary width per layer. Experiments conducted on ILSVRC-$12$\nconfirm effectiveness of our approach. With non-uniform pruning across the\nlayers on ResNet-$50$, we are able to match the FLOP reduction of\nstate-of-the-art channel pruning results while achieving a $0.98\\%$ higher\naccuracy. Further, we show that our pruned ResNet-$50$ network outperforms\nResNet-$34$ and ResNet-$18$ networks, and that our pruned ResNet-$101$\noutperforms ResNet-$50$.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 17:09:56 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Khetan", "Ashish", ""], ["Karnin", "Zohar", ""]]}, {"id": "2005.11295", "submitter": "Dimitris Tsipras", "authors": "Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Andrew Ilyas,\n  Aleksander Madry", "title": "From ImageNet to Image Classification: Contextualizing Progress on\n  Benchmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building rich machine learning datasets in a scalable manner often\nnecessitates a crowd-sourced data collection pipeline. In this work, we use\nhuman studies to investigate the consequences of employing such a pipeline,\nfocusing on the popular ImageNet dataset. We study how specific design choices\nin the ImageNet creation process impact the fidelity of the resulting\ndataset---including the introduction of biases that state-of-the-art models\nexploit. Our analysis pinpoints how a noisy data collection pipeline can lead\nto a systematic misalignment between the resulting benchmark and the real-world\ntask it serves as a proxy for. Finally, our findings emphasize the need to\naugment our current model training and evaluation toolkit to take such\nmisalignments into account. To facilitate further research, we release our\nrefined ImageNet annotations at https://github.com/MadryLab/ImageNetMultiLabel.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 17:39:16 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Tsipras", "Dimitris", ""], ["Santurkar", "Shibani", ""], ["Engstrom", "Logan", ""], ["Ilyas", "Andrew", ""], ["Madry", "Aleksander", ""]]}, {"id": "2005.11341", "submitter": "Xavier Rafael-Palou", "authors": "Xavier Rafael-Palou, Anton Aubanell, Ilaria Bonavita, Mario Ceresa,\n  Gemma Piella, Vicent Ribas, Miguel A. Gonz\\'alez Ballester", "title": "Pulmonary Nodule Malignancy Classification Using its Temporal Evolution\n  with Two-Stream 3D Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/D1jTt_FOPY", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nodule malignancy assessment is a complex, time-consuming and error-prone\ntask. Current clinical practice requires measuring changes in size and density\nof the nodule at different time-points. State of the art solutions rely on 3D\nconvolutional neural networks built on pulmonary nodules obtained from single\nCT scan per patient. In this work, we propose a two-stream 3D convolutional\nneural network that predicts malignancy by jointly analyzing two pulmonary\nnodule volumes from the same patient taken at different time-points. Best\nresults achieve 77% of F1-score in test with an increment of 9% and 12% of\nF1-score with respect to the same network trained with images from a single\ntime-point.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 18:19:32 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Rafael-Palou", "Xavier", ""], ["Aubanell", "Anton", ""], ["Bonavita", "Ilaria", ""], ["Ceresa", "Mario", ""], ["Piella", "Gemma", ""], ["Ribas", "Vicent", ""], ["Ballester", "Miguel A. Gonz\u00e1lez", ""]]}, {"id": "2005.11362", "submitter": "Drew Linsley", "authors": "Drew Linsley, Alekh Karkada Ashok, Lakshmi Narasimhan Govindarajan,\n  Rex Liu, and Thomas Serre", "title": "Stable and expressive recurrent vision models", "comments": "Published at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Primate vision depends on recurrent processing for reliable perception. A\ngrowing body of literature also suggests that recurrent connections improve the\nlearning efficiency and generalization of vision models on classic computer\nvision challenges. Why then, are current large-scale challenges dominated by\nfeedforward networks? We posit that the effectiveness of recurrent vision\nmodels is bottlenecked by the standard algorithm used for training them,\n\"back-propagation through time\" (BPTT), which has O(N) memory-complexity for\ntraining an N step model. Thus, recurrent vision model design is bounded by\nmemory constraints, forcing a choice between rivaling the enormous capacity of\nleading feedforward models or trying to compensate for this deficit through\ngranular and complex dynamics. Here, we develop a new learning algorithm,\n\"contractor recurrent back-propagation\" (C-RBP), which alleviates these issues\nby achieving constant O(1) memory-complexity with steps of recurrent\nprocessing. We demonstrate that recurrent vision models trained with C-RBP can\ndetect long-range spatial dependencies in a synthetic contour tracing task that\nBPTT-trained models cannot. We further show that recurrent vision models\ntrained with C-RBP to solve the large-scale Panoptic Segmentation MS-COCO\nchallenge outperform the leading feedforward approach, with fewer free\nparameters. C-RBP is a general-purpose learning algorithm for any application\nthat can benefit from expansive recurrent dynamics. Code and data are available\nat https://github.com/c-rbp.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 19:31:28 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 23:15:14 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Linsley", "Drew", ""], ["Ashok", "Alekh Karkada", ""], ["Govindarajan", "Lakshmi Narasimhan", ""], ["Liu", "Rex", ""], ["Serre", "Thomas", ""]]}, {"id": "2005.11368", "submitter": "Valery Naranjo", "authors": "Amartya Kalapahar, Julio Silva-Rodr\\'iguez, Adri\\'an Colomer, Fernando\n  L\\'opez-Mir and Valery Naranjo", "title": "Gleason Grading of Histology Prostate Images through Semantic\n  Segmentation via Residual U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Worldwide, prostate cancer is one of the main cancers affecting men. The\nfinal diagnosis of prostate cancer is based on the visual detection of Gleason\npatterns in prostate biopsy by pathologists. Computer-aided-diagnosis systems\nallow to delineate and classify the cancerous patterns in the tissue via\ncomputer-vision algorithms in order to support the physicians' task. The\nmethodological core of this work is a U-Net convolutional neural network for\nimage segmentation modified with residual blocks able to segment cancerous\ntissue according to the full Gleason system. This model outperforms other\nwell-known architectures, and reaches a pixel-level Cohen's quadratic Kappa of\n0.52, at the level of previous image-level works in the literature, but\nproviding also a detailed localisation of the patterns.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 19:49:10 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Kalapahar", "Amartya", ""], ["Silva-Rodr\u00edguez", "Julio", ""], ["Colomer", "Adri\u00e1n", ""], ["L\u00f3pez-Mir", "Fernando", ""], ["Naranjo", "Valery", ""]]}, {"id": "2005.11384", "submitter": "Yingying Zhu", "authors": "Yingying Zhu, Daniel C. Elton, Sungwon Lee, Perry J. Pickhardt, Ronald\n  M. Summers", "title": "Image Translation by Latent Union of Subspaces for Cross-Domain Plaque\n  Detection", "comments": "accepted as a short paper in the 2020 Medical Imaging with Deep\n  Learning (MIDL) conference", "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/266", "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calcified plaque in the aorta and pelvic arteries is associated with coronary\nartery calcification and is a strong predictor of heart attack. Current\ncalcified plaque detection models show poor generalizability to different\ndomains (ie. pre-contrast vs. post-contrast CT scans). Many recent works have\nshown how cross domain object detection can be improved using an image\ntranslation model which translates between domains using a single shared latent\nspace. However, while current image translation models do a good job preserving\nglobal/intermediate level structures they often have trouble preserving tiny\nstructures. In medical imaging applications, preserving small structures is\nimportant since these structures can carry information which is highly relevant\nfor disease diagnosis. Recent works on image reconstruction show that complex\nreal-world images are better reconstructed using a union of subspaces approach.\nSince small image patches are used to train the image translation model, it\nmakes sense to enforce that each patch be represented by a linear combination\nof subspaces which may correspond to the different parts of the body present in\nthat patch. Motivated by this, we propose an image translation network using a\nshared union of subspaces constraint and show our approach preserves subtle\nstructures (plaques) better than the conventional method. We further applied\nour method to a cross domain plaque detection task and show significant\nimprovement compared to the state-of-the art method.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 20:35:34 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Zhu", "Yingying", ""], ["Elton", "Daniel C.", ""], ["Lee", "Sungwon", ""], ["Pickhardt", "Perry J.", ""], ["Summers", "Ronald M.", ""]]}, {"id": "2005.11387", "submitter": "Aydogan Ozcan", "authors": "Jingxi Li, Deniz Mengu, Nezih T. Yardimci, Yi Luo, Xurong Li, Muhammed\n  Veli, Yair Rivenson, Mona Jarrahi, Aydogan Ozcan", "title": "Spectrally-Encoded Single-Pixel Machine Vision Using Diffractive\n  Networks", "comments": "21 pages, 5 figures, 1 table", "journal-ref": "Science Advances (2021)", "doi": "10.1126/sciadv.abd7690", "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D engineering of matter has opened up new avenues for designing systems that\ncan perform various computational tasks through light-matter interaction. Here,\nwe demonstrate the design of optical networks in the form of multiple\ndiffractive layers that are trained using deep learning to transform and encode\nthe spatial information of objects into the power spectrum of the diffracted\nlight, which are used to perform optical classification of objects with a\nsingle-pixel spectroscopic detector. Using a time-domain spectroscopy setup\nwith a plasmonic nanoantenna-based detector, we experimentally validated this\nmachine vision framework at terahertz spectrum to optically classify the images\nof handwritten digits by detecting the spectral power of the diffracted light\nat ten distinct wavelengths, each representing one class/digit. We also report\nthe coupling of this spectral encoding achieved through a diffractive optical\nnetwork with a shallow electronic neural network, separately trained to\nreconstruct the images of handwritten digits based on solely the spectral\ninformation encoded in these ten distinct wavelengths within the diffracted\nlight. These reconstructed images demonstrate task-specific image decompression\nand can also be cycled back as new inputs to the same diffractive network to\nimprove its optical object classification. This unique machine vision framework\nmerges the power of deep learning with the spatial and spectral processing\ncapabilities of diffractive networks, and can also be extended to other\nspectral-domain measurement systems to enable new 3D imaging and sensing\nmodalities integrated with spectrally encoded classification tasks performed\nthrough diffractive optical networks.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 09:18:21 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 04:48:42 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Li", "Jingxi", ""], ["Mengu", "Deniz", ""], ["Yardimci", "Nezih T.", ""], ["Luo", "Yi", ""], ["Li", "Xurong", ""], ["Veli", "Muhammed", ""], ["Rivenson", "Yair", ""], ["Jarrahi", "Mona", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "2005.11405", "submitter": "Nathaniel Roth", "authors": "Nat Roth, Justin Wagle", "title": "One of these (Few) Things is Not Like the Others", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To perform well, most deep learning based image classification systems\nrequire large amounts of data and computing resources. These constraints make\nit difficult to quickly personalize to individual users or train models outside\nof fairly powerful machines. To deal with these problems, there has been a\nlarge body of research into teaching machines to learn to classify images based\non only a handful of training examples, a field known as few-shot learning.\nFew-shot learning research traditionally makes the simplifying assumption that\nall images belong to one of a fixed number of previously seen groups. However,\nmany image datasets, such as a camera roll on a phone, will be noisy and\ncontain images that may not be relevant or fit into any clear group. We propose\na model which can both classify new images based on a small number of examples\nand recognize images which do not belong to any previously seen group. We adapt\nprevious few-shot learning work to include a simple mechanism for learning a\ncutoff that determines whether an image should be excluded or classified. We\nexamine how well our method performs in a realistic setting, benchmarking the\napproach on a noisy and ambiguous dataset of images. We evaluate performance\nover a spectrum of model architectures, including setups small enough to be run\non low powered devices, such as mobile phones or web browsers. We find that\nthis task of excluding irrelevant images poses significant extra difficulty\nbeyond that of the traditional few-shot task. We decompose the sources of this\nerror, and suggest future improvements that might alleviate this difficulty.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 21:49:35 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Roth", "Nat", ""], ["Wagle", "Justin", ""]]}, {"id": "2005.11406", "submitter": "Yuhang Song", "authors": "Yuhang Song, Wenbo Li, Lei Zhang, Jianwei Yang, Emre Kiciman, Hamid\n  Palangi, Jianfeng Gao, C.-C. Jay Kuo, and Pengchuan Zhang", "title": "Novel Human-Object Interaction Detection via Adversarial Domain\n  Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper the problem of novel human-object interaction (HOI)\ndetection, aiming at improving the generalization ability of the model to\nunseen scenarios. The challenge mainly stems from the large compositional space\nof objects and predicates, which leads to the lack of sufficient training data\nfor all the object-predicate combinations. As a result, most existing HOI\nmethods heavily rely on object priors and can hardly generalize to unseen\ncombinations. To tackle this problem, we propose a unified framework of\nadversarial domain generalization to learn object-invariant features for\npredicate prediction. To measure the performance improvement, we create a new\nsplit of the HICO-DET dataset, where the HOIs in the test set are all unseen\ntriplet categories in the training set. Our experiments show that the proposed\nframework significantly increases the performance by up to 50% on the new split\nof HICO-DET dataset and up to 125% on the UnRel dataset for auxiliary\nevaluation in detecting novel HOIs.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 22:02:56 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Song", "Yuhang", ""], ["Li", "Wenbo", ""], ["Zhang", "Lei", ""], ["Yang", "Jianwei", ""], ["Kiciman", "Emre", ""], ["Palangi", "Hamid", ""], ["Gao", "Jianfeng", ""], ["Kuo", "C. -C. Jay", ""], ["Zhang", "Pengchuan", ""]]}, {"id": "2005.11417", "submitter": "Rishabh Malhotra", "authors": "Rishabh Malhotra, Dhron Joshi, Ku Young Shin", "title": "Approaching Bio Cellular Classification for Malaria Infected Cells Using\n  Machine Learning and then Deep Learning to compare & analyze K-Nearest\n  Neighbours and Deep CNNs", "comments": "7 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malaria is a deadly disease which claims the lives of hundreds of thousands\nof people every year. Computational methods have been proven to be useful in\nthe medical industry by providing effective means of classification of\ndiagnostic imaging and disease identification. This paper examines different\nmachine learning methods in the context of classifying the presence of malaria\nin cell images. Numerous machine learning methods can be applied to the same\nproblem; the question of whether one machine learning method is better suited\nto a problem relies heavily on the problem itself and the implementation of a\nmodel. In particular, convolutional neural networks and k nearest neighbours\nare both analyzed and contrasted in regards to their application to classifying\nthe presence of malaria and each models empirical performance. Here, we\nimplement two models of classification; a convolutional neural network, and the\nk nearest neighbours algorithm. These two algorithms are compared based on\nvalidation accuracy. For our implementation, CNN (95%) performed 25% better\nthan kNN (75%).\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 23:02:36 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Malhotra", "Rishabh", ""], ["Joshi", "Dhron", ""], ["Shin", "Ku Young", ""]]}, {"id": "2005.11423", "submitter": "Aviad Levis", "authors": "Aviad Levis, Yoav Y. Schechner, Anthony B. Davis and Jesse Loveridge", "title": "Multi-view polarimetric scattering cloud tomography and retrieval of\n  droplet size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tomography aims to recover a three-dimensional (3D) density map of a medium\nor an object. In medical imaging, it is extensively used for diagnostics via\nX-ray computed tomography (CT). Optical diffusion tomography is an alternative\nto X-ray CT that uses multiply scattered light to deliver coarse density maps\nfor soft tissues. We define and derive tomography of cloud droplet\ndistributions via passive remote sensing. We use multi-view polarimetric images\nto fit a 3D polarized radiative transfer (RT) forward model. Our motivation is\n3D volumetric probing of vertically-developed convectively-driven clouds that\nare ill-served by current methods in operational passive remote sensing. These\ntechniques are based on strictly 1D RT modeling and applied to a single cloudy\npixel, where cloud geometry is assumed to be that of a plane-parallel slab.\nIncident unpolarized sunlight, once scattered by cloud-droplets, changes its\npolarization state according to droplet size. Therefore, polarimetric\nmeasurements in the rainbow and glory angular regions can be used to infer the\ndroplet size distribution. This work defines and derives a framework for a full\n3D tomography of cloud droplets for both their mass concentration in space and\ntheir distribution across a range of sizes. This 3D retrieval of key\nmicrophysical properties is made tractable by our novel approach that involves\na restructuring and differentiation of an open-source polarized 3D RT code to\naccommodate a special two-step optimization technique. Physically-realistic\nsynthetic clouds are used to demonstrate the methodology with rigorous\nuncertainty quantification.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 23:39:21 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Levis", "Aviad", ""], ["Schechner", "Yoav Y.", ""], ["Davis", "Anthony B.", ""], ["Loveridge", "Jesse", ""]]}, {"id": "2005.11426", "submitter": "Jianfeng Wang", "authors": "Jianfeng Wang, Xi Yin, Lijuan Wang, Lei Zhang", "title": "Hashing-based Non-Maximum Suppression for Crowded Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an algorithm, named hashing-based non-maximum\nsuppression (HNMS) to efficiently suppress the non-maximum boxes for object\ndetection. Non-maximum suppression (NMS) is an essential component to suppress\nthe boxes at closely located locations with similar shapes. The time cost tends\nto be huge when the number of boxes becomes large, especially for crowded\nscenes. The basic idea of HNMS is to firstly map each box to a discrete code\n(hash cell) and then remove the boxes with lower confidences if they are in the\nsame cell. Considering the intersection-over-union (IoU) as the metric, we\npropose a simple yet effective hashing algorithm, named IoUHash, which\nguarantees that the boxes within the same cell are close enough by a lower IoU\nbound. For two-stage detectors, we replace NMS in region proposal network with\nHNMS, and observe significant speed-up with comparable accuracy. For one-stage\ndetectors, HNMS is used as a pre-filter to speed up the suppression with a\nlarge margin. Extensive experiments are conducted on CARPK, SKU-110K,\nCrowdHuman datasets to demonstrate the efficiency and effectiveness of HNMS.\nCode is released at \\url{https://github.com/microsoft/hnms.git}.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 23:45:59 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wang", "Jianfeng", ""], ["Yin", "Xi", ""], ["Wang", "Lijuan", ""], ["Zhang", "Lei", ""]]}, {"id": "2005.11437", "submitter": "Yizhe Zhu", "authors": "Yizhe Zhu, Martin Renqiang Min, Asim Kadav, Hans Peter Graf", "title": "S3VAE: Self-Supervised Sequential VAE for Representation Disentanglement\n  and Data Generation", "comments": "to appear in CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sequential variational autoencoder to learn disentangled\nrepresentations of sequential data (e.g., videos and audios) under\nself-supervision. Specifically, we exploit the benefits of some readily\naccessible supervisory signals from input data itself or some off-the-shelf\nfunctional models and accordingly design auxiliary tasks for our model to\nutilize these signals. With the supervision of the signals, our model can\neasily disentangle the representation of an input sequence into static factors\nand dynamic factors (i.e., time-invariant and time-varying parts).\nComprehensive experiments across videos and audios verify the effectiveness of\nour model on representation disentanglement and generation of sequential data,\nand demonstrate that, our model with self-supervision performs comparable to,\nif not better than, the fully-supervised model with ground truth labels, and\noutperforms state-of-the-art unsupervised models by a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 00:44:38 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Zhu", "Yizhe", ""], ["Min", "Martin Renqiang", ""], ["Kadav", "Asim", ""], ["Graf", "Hans Peter", ""]]}, {"id": "2005.11450", "submitter": "Alexander Lavin", "authors": "Bijan Haney and Alexander Lavin", "title": "Fine-Grain Few-Shot Vision via Domain Knowledge as Hyperspherical Priors", "comments": null, "journal-ref": "CVPR 2020 Workshop on Fine-Grained Visual Categorization", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prototypical networks have been shown to perform well at few-shot learning\ntasks in computer vision. Yet these networks struggle when classes are very\nsimilar to each other (fine-grain classification) and currently have no way of\ntaking into account prior knowledge (through the use of tabular data). Using a\nspherical latent space to encode prototypes, we can achieve few-shot fine-grain\nclassification by maximally separating the classes while incorporating domain\nknowledge as informative priors. We describe how to construct a hypersphere of\nprototypes that embed a-priori domain information, and demonstrate the\neffectiveness of the approach on challenging benchmark datasets for fine-grain\nclassification, with top results for one-shot classification and 5x speedups in\ntraining time.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 02:10:57 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Haney", "Bijan", ""], ["Lavin", "Alexander", ""]]}, {"id": "2005.11470", "submitter": "Zhezhang Ding", "authors": "Donghao Xu, Zhezhang Ding, Xu He, Huijing Zhao, Mathieu Moze,\n  Fran\\c{c}ois Aioun, and Franck Guillemard", "title": "Learning from Naturalistic Driving Data for Human-like Autonomous\n  Highway Driving", "comments": "14 pages, 9 figures. Submitted to T.ITS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving in a human-like manner is important for an autonomous vehicle to be a\nsmart and predictable traffic participant. To achieve this goal, parameters of\nthe motion planning module should be carefully tuned, which needs great effort\nand expert knowledge. In this study, a method of learning cost parameters of a\nmotion planner from naturalistic driving data is proposed. The learning is\nachieved by encouraging the selected trajectory to approximate the human\ndriving trajectory under the same traffic situation. The employed motion\nplanner follows a widely accepted methodology that first samples candidate\ntrajectories in the trajectory space, then select the one with minimal cost as\nthe planned trajectory. Moreover, in addition to traditional factors such as\ncomfort, efficiency and safety, the cost function is proposed to incorporate\nincentive of behavior decision like a human driver, so that both lane change\ndecision and motion planning are coupled into one framework. Two types of lane\nincentive cost -- heuristic and learning based -- are proposed and implemented.\nTo verify the validity of the proposed method, a data set is developed by using\nthe naturalistic trajectory data of human drivers collected on the motorways in\nBeijing, containing samples of lane changes to the left and right lanes, and\ncar followings. Experiments are conducted with respect to both lane change\ndecision and motion planning, and promising results are achieved.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 04:39:39 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Xu", "Donghao", ""], ["Ding", "Zhezhang", ""], ["He", "Xu", ""], ["Zhao", "Huijing", ""], ["Moze", "Mathieu", ""], ["Aioun", "Fran\u00e7ois", ""], ["Guillemard", "Franck", ""]]}, {"id": "2005.11472", "submitter": "Zheng Ge", "authors": "Zheng Ge, Zequn Jie, Xin Huang, Chengzheng Li, Osamu Yoshie", "title": "Delving into the Imbalance of Positive Proposals in Two-stage Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imbalance issue is a major yet unsolved bottleneck for the current object\ndetection models. In this work, we observe two crucial yet never discussed\nimbalance issues. The first imbalance lies in the large number of low-quality\nRPN proposals, which makes the R-CNN module (i.e., post-classification layers)\nbecome highly biased towards the negative proposals in the early training\nstage. The second imbalance stems from the unbalanced ground-truth numbers\nacross different testing images, resulting in the imbalance of the number of\npotentially existing positive proposals in testing phase. To tackle these two\nimbalance issues, we incorporates two innovations into Faster R-CNN: 1) an\nR-CNN Gradient Annealing (RGA) strategy to enhance the impact of positive\nproposals in the early training stage. 2) a set of Parallel R-CNN Modules (PRM)\nwith different positive/negative sampling ratios during training on one same\nbackbone. Our RGA and PRM can totally bring 2.0% improvements on AP on COCO\nminival. Experiments on CrowdHuman further validates the effectiveness of our\ninnovations across various kinds of object detection tasks.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 04:54:59 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ge", "Zheng", ""], ["Jie", "Zequn", ""], ["Huang", "Xin", ""], ["Li", "Chengzheng", ""], ["Yoshie", "Osamu", ""]]}, {"id": "2005.11475", "submitter": "Qi Chen", "authors": "Junxu Cao, Qi Chen, Jun Guo, and Ruichao Shi", "title": "Attention-guided Context Feature Pyramid Network for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For object detection, how to address the contradictory requirement between\nfeature map resolution and receptive field on high-resolution inputs still\nremains an open question. In this paper, to tackle this issue, we build a novel\narchitecture, called Attention-guided Context Feature Pyramid Network (AC-FPN),\nthat exploits discriminative information from various large receptive fields\nvia integrating attention-guided multi-path features. The model contains two\nmodules. The first one is Context Extraction Module (CEM) that explores large\ncontextual information from multiple receptive fields. As redundant contextual\nrelations may mislead localization and recognition, we also design the second\nmodule named Attention-guided Module (AM), which can adaptively capture the\nsalient dependencies over objects by using the attention mechanism. AM consists\nof two sub-modules, i.e., Context Attention Module (CxAM) and Content Attention\nModule (CnAM), which focus on capturing discriminative semantics and locating\nprecise positions, respectively. Most importantly, our AC-FPN can be readily\nplugged into existing FPN-based models. Extensive experiments on object\ndetection and instance segmentation show that existing models with our proposed\nCEM and AM significantly surpass their counterparts without them, and our model\nsuccessfully obtains state-of-the-art results. We have released the source code\nat https://github.com/Caojunxu/AC-FPN.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 05:24:50 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Cao", "Junxu", ""], ["Chen", "Qi", ""], ["Guo", "Jun", ""], ["Shi", "Ruichao", ""]]}, {"id": "2005.11487", "submitter": "Wei Wang", "authors": "Yudi Chen, Wei Wang, Yu Zhou, Fei Yang, Dongbao Yang, Weiping Wang", "title": "Self-Training for Domain Adaptive Scene Text Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though deep learning based scene text detection has achieved great progress,\nwell-trained detectors suffer from severe performance degradation for different\ndomains. In general, a tremendous amount of data is indispensable to train the\ndetector in the target domain. However, data collection and annotation are\nexpensive and time-consuming. To address this problem, we propose a\nself-training framework to automatically mine hard examples with pseudo-labels\nfrom unannotated videos or images. To reduce the noise of hard examples, a\nnovel text mining module is implemented based on the fusion of detection and\ntracking results. Then, an image-to-video generation method is designed for the\ntasks that videos are unavailable and only images can be used. Experimental\nresults on standard benchmarks, including ICDAR2015, MSRA-TD500, ICDAR2017 MLT,\ndemonstrate the effectiveness of our self-training method. The simple Mask\nR-CNN adapted with self-training and fine-tuned on real data can achieve\ncomparable or even superior results with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 07:36:23 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Chen", "Yudi", ""], ["Wang", "Wei", ""], ["Zhou", "Yu", ""], ["Yang", "Fei", ""], ["Yang", "Dongbao", ""], ["Wang", "Weiping", ""]]}, {"id": "2005.11489", "submitter": "Kourosh Meshgi", "authors": "Maryam Sadat Mirzaei, Kourosh Meshgi, Etienne Frigo, Toyoaki Nishida", "title": "AnimGAN: A Spatiotemporally-Conditioned Generative Adversarial Network\n  for Character Animation", "comments": "Submitted to ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Producing realistic character animations is one of the essential tasks in\nhuman-AI interactions. Considered as a sequence of poses of a humanoid, the\ntask can be considered as a sequence generation problem with spatiotemporal\nsmoothness and realism constraints. Additionally, we wish to control the\nbehavior of AI agents by giving them what to do and, more specifically, how to\ndo it. We proposed a spatiotemporally-conditioned GAN that generates a sequence\nthat is similar to a given sequence in terms of semantics and spatiotemporal\ndynamics. Using LSTM-based generator and graph ConvNet discriminator, this\nsystem is trained end-to-end on a large gathered dataset of gestures,\nexpressions, and actions. Experiments showed that compared to traditional\nconditional GAN, our method creates plausible, realistic, and semantically\nrelevant humanoid animation sequences that match user expectations.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 07:47:46 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Mirzaei", "Maryam Sadat", ""], ["Meshgi", "Kourosh", ""], ["Frigo", "Etienne", ""], ["Nishida", "Toyoaki", ""]]}, {"id": "2005.11524", "submitter": "Muhammad E. H. Chowdhury", "authors": "Anas Tahir, Yazan Qiblawey, Amith Khandakar, Tawsifur Rahman, Uzair\n  Khurshid, Farayi Musharavati, M. T. Islam, Serkan Kiranyaz, Muhammad E. H.\n  Chowdhury", "title": "Deep Learning for Reliable Classification of COVID-19, MERS, and SARS\n  from Chest X-Ray Images", "comments": "10 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel Coronavirus disease (COVID-19) is an extremely contagious and quickly\nspreading Coronavirus infestation. Severe Acute Respiratory Syndrome (SARS) and\nMiddle East Respiratory Syndrome (MERS), which outbreak in 2002 and 2011, and\nthe current COVID-19 pandemic are all from the same family of coronavirus. This\nwork aims to classify COVID-19, SARS, and MERS chest X-ray (CXR) images using\ndeep Convolutional Neural Networks (CNNs). A unique database was created,\nso-called QU-COVID-family, consisting of 423 COVID-19, 144 MERS, and 134 SARS\nCXR images. Besides, a robust COVID-19 recognition system was proposed to\nidentify lung regions using a CNN segmentation model (U-Net), and then classify\nthe segmented lung images as COVID-19, MERS, or SARS using a pre-trained CNN\nclassifier. Furthermore, the Score-CAM visualization method was utilized to\nvisualize classification output and understand the reasoning behind the\ndecision of deep CNNs. Several Deep Learning classifiers were trained and\ntested; four outperforming algorithms were reported. Original and preprocessed\nimages were used individually and all together as the input(s) to the networks.\nTwo recognition schemes were considered: plain CXR classification and segmented\nCXR classification. For plain CXRs, it was observed that InceptionV3\noutperforms other networks with a 3-channel scheme and achieves sensitivities\nof 99.5%, 93.1%, and 97% for classifying COVID-19, MERS, and SARS images,\nrespectively. In contrast, for segmented CXRs, InceptionV3 outperformed using\nthe original CXR dataset and achieved sensitivities of 96.94%, 79.68%, and\n90.26% for classifying COVID-19, MERS, and SARS images, respectively. All\nnetworks showed high COVID-19 detection sensitivity (>96%) with the segmented\nlung images. This indicates the unique radiographic signature of COVID-19 cases\nin the eyes of AI, which is often a challenging task for medical doctors.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 12:22:28 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 14:34:17 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 11:53:04 GMT"}, {"version": "v4", "created": "Mon, 8 Jun 2020 10:07:55 GMT"}, {"version": "v5", "created": "Thu, 18 Feb 2021 21:34:31 GMT"}, {"version": "v6", "created": "Tue, 1 Jun 2021 12:37:22 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Tahir", "Anas", ""], ["Qiblawey", "Yazan", ""], ["Khandakar", "Amith", ""], ["Rahman", "Tawsifur", ""], ["Khurshid", "Uzair", ""], ["Musharavati", "Farayi", ""], ["Islam", "M. T.", ""], ["Kiranyaz", "Serkan", ""], ["Chowdhury", "Muhammad E. H.", ""]]}, {"id": "2005.11546", "submitter": "Deepak Mittal", "authors": "VSR Veeravasarapu, Abhishek Goel, Deepak Mittal, Maneesh Singh", "title": "ProAlignNet : Unsupervised Learning for Progressively Aligning Noisy\n  Contours", "comments": "Accepted at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contour shape alignment is a fundamental but challenging problem in computer\nvision, especially when the observations are partial, noisy, and largely\nmisaligned. Recent ConvNet-based architectures that were proposed to align\nimage structures tend to fail with contour representation of shapes, mostly due\nto the use of proximity-insensitive pixel-wise similarity measures as loss\nfunctions in their training processes. This work presents a novel ConvNet,\n\"ProAlignNet\" that accounts for large scale misalignments and complex\ntransformations between the contour shapes. It infers the warp parameters in a\nmulti-scale fashion with progressively increasing complex transformations over\nincreasing scales. It learns --without supervision-- to align contours,\nagnostic to noise and missing parts, by training with a novel loss function\nwhich is derived an upperbound of a proximity-sensitive and local\nshape-dependent similarity metric that uses classical Morphological Chamfer\nDistance Transform. We evaluate the reliability of these proposals on a\nsimulated MNIST noisy contours dataset via some basic sanity check experiments.\nNext, we demonstrate the effectiveness of the proposed models in two real-world\napplications of (i) aligning geo-parcel data to aerial image maps and (ii)\nrefining coarsely annotated segmentation labels. In both applications, the\nproposed models consistently perform superior to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 14:56:14 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Veeravasarapu", "VSR", ""], ["Goel", "Abhishek", ""], ["Mittal", "Deepak", ""], ["Singh", "Maneesh", ""]]}, {"id": "2005.11549", "submitter": "Mahdieh Abbasi", "authors": "Mahdieh Abbasi, Denis Laurendeau, Christian Gagne", "title": "Self-supervised Robust Object Detectors from Partially Labelled Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the object detection task, merging various datasets from similar contexts\nbut with different sets of Objects of Interest (OoI) is an inexpensive way (in\nterms of labor cost) for crafting a large-scale dataset covering a wide range\nof objects. Moreover, merging datasets allows us to train one integrated object\ndetector, instead of training several ones, which in turn resulting in the\nreduction of computational and time costs. However, merging the datasets from\nsimilar contexts causes samples with partial labeling as each constituent\ndataset is originally annotated for its own set of OoI and ignores to annotate\nthose objects that are become interested after merging the datasets. With the\ngoal of training \\emph{one integrated robust object detector with high\ngeneralization performance}, we propose a training framework to overcome\nmissing-label challenge of the merged datasets. More specifically, we propose a\ncomputationally efficient self-supervised framework to create on-the-fly\npseudo-labels for the unlabeled positive instances in the merged dataset in\norder to train the object detector jointly on both ground truth and pseudo\nlabels. We evaluate our proposed framework for training Yolo on a simulated\nmerged dataset with missing rate $\\approx\\!48\\%$ using VOC2012 and VOC2007. We\nempirically show that generalization performance of Yolo trained on both ground\ntruth and the pseudo-labels created by our method is on average $4\\%$ higher\nthan the ones trained only with the ground truth labels of the merged dataset.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 15:18:20 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 03:56:12 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Abbasi", "Mahdieh", ""], ["Laurendeau", "Denis", ""], ["Gagne", "Christian", ""]]}, {"id": "2005.11552", "submitter": "Long Chen", "authors": "Long Chen, Zhihua Liu, Lei Tong, Zheheng Jiang, Shengke Wang, Junyu\n  Dong, Huiyu Zhou", "title": "Underwater object detection using Invert Multi-Class Adaboost with deep\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning based methods have achieved promising\nperformance in standard object detection. However, these methods lack\nsufficient capabilities to handle underwater object detection due to these\nchallenges: (1) Objects in real applications are usually small and their images\nare blurry, and (2) images in the underwater datasets and real applications\naccompany heterogeneous noise. To address these two problems, we first propose\na novel neural network architecture, namely Sample-WeIghted hyPEr Network\n(SWIPENet), for small object detection. SWIPENet consists of high resolution\nand semantic rich Hyper Feature Maps which can significantly improve small\nobject detection accuracy. In addition, we propose a novel sample-weighted loss\nfunction which can model sample weights for SWIPENet, which uses a novel sample\nre-weighting algorithm, namely Invert Multi-Class Adaboost (IMA), to reduce the\ninfluence of noise on the proposed SWIPENet. Experiments on two underwater\nrobot picking contest datasets URPC2017 and URPC2018 show that the proposed\nSWIPENet+IMA framework achieves better performance in detection accuracy\nagainst several state-of-the-art object detection approaches.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 15:30:38 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Chen", "Long", ""], ["Liu", "Zhihua", ""], ["Tong", "Lei", ""], ["Jiang", "Zheheng", ""], ["Wang", "Shengke", ""], ["Dong", "Junyu", ""], ["Zhou", "Huiyu", ""]]}, {"id": "2005.11558", "submitter": "Vasileios Petridis", "authors": "Vasileios Petridis (Dept. of Electrical and Computer Engineering,\n  Aristotle University, Thessaloniki, Greece)", "title": "Invariant 3D Shape Recognition using Predictive Modular Neural Networks", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper PREMONN (PREdictive MOdular Neural Networks) model/architecture\nis generalized to functions of two variables and to non-Euclidean spaces. It is\npresented in the context of 3D invariant shape recognition and texture\nrecognition. PREMONN uses local relation, it is modular and exhibits\nincremental learning. The recognition process can start at any point on a shape\nor texture, so a reference point is not needed. Its local relation\ncharacteristic enables it to recognize shape and texture even in presence of\nocclusion. The analysis is mainly mathematical. However, we present some\nexperimental results. The methods presented in this paper can be applied to\nmany problems such as gesture recognition, action recognition, dynamic texture\nrecognition etc.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 16:16:37 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Petridis", "Vasileios", "", "Dept. of Electrical and Computer Engineering,\n  Aristotle University, Thessaloniki, Greece"]]}, {"id": "2005.11576", "submitter": "Yiru Wang", "authors": "Jie Yang, Jiarou Fan, Yiru Wang, Yige Wang, Weihao Gan, Lin Liu, Wei\n  Wu", "title": "Hierarchical Feature Embedding for Attribute Recognition", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute recognition is a crucial but challenging task due to viewpoint\nchanges, illumination variations and appearance diversities, etc. Most of\nprevious work only consider the attribute-level feature embedding, which might\nperform poorly in complicated heterogeneous conditions. To address this\nproblem, we propose a hierarchical feature embedding (HFE) framework, which\nlearns a fine-grained feature embedding by combining attribute and ID\ninformation. In HFE, we maintain the inter-class and intra-class feature\nembedding simultaneously. Not only samples with the same attribute but also\nsamples with the same ID are gathered more closely, which could restrict the\nfeature embedding of visually hard samples with regard to attributes and\nimprove the robustness to variant conditions. We establish this hierarchical\nstructure by utilizing HFE loss consisted of attribute-level and ID-level\nconstraints. We also introduce an absolute boundary regularization and a\ndynamic loss weight as supplementary components to help build up the feature\nembedding. Experiments show that our method achieves the state-of-the-art\nresults on two pedestrian attribute datasets and a facial attribute dataset.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 17:52:41 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Yang", "Jie", ""], ["Fan", "Jiarou", ""], ["Wang", "Yiru", ""], ["Wang", "Yige", ""], ["Gan", "Weihao", ""], ["Liu", "Lin", ""], ["Wu", "Wei", ""]]}, {"id": "2005.11592", "submitter": "Chen Chen", "authors": "Sijie Zhu and Taojiannan Yang and Chen Chen", "title": "Revisiting Street-to-Aerial View Image Geo-localization and Orientation\n  Estimation", "comments": "WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Street-to-aerial image geo-localization, which matches a query street-view\nimage to the GPS-tagged aerial images in a reference set, has attracted\nincreasing attention recently. In this paper, we revisit this problem and point\nout the ignored issue about image alignment information. We show that the\nperformance of a simple Siamese network is highly dependent on the alignment\nsetting and the comparison of previous works can be unfair if they have\ndifferent assumptions. Instead of focusing on the feature extraction under the\nalignment assumption, we show that improvements in metric learning techniques\nsignificantly boost the performance regardless of the alignment. Without\nleveraging the alignment information, our pipeline outperforms previous works\non both panorama and cropped datasets. Furthermore, we conduct visualization to\nhelp understand the learned model and the effect of alignment information using\nGrad-CAM. With our discovery on the approximate rotation-invariant activation\nmaps, we propose a novel method to estimate the orientation/alignment between a\npair of cross-view images with unknown alignment information. It achieves\nstate-of-the-art results on the CVUSA dataset.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 19:52:24 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 02:15:30 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhu", "Sijie", ""], ["Yang", "Taojiannan", ""], ["Chen", "Chen", ""]]}, {"id": "2005.11610", "submitter": "Antonio D'Innocente", "authors": "Antonio D'Innocente, Francesco Cappio Borlino, Silvia Bucci, Barbara\n  Caputo, Tatiana Tommasi", "title": "One-Shot Unsupervised Cross-Domain Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite impressive progress in object detection over the last years, it is\nstill an open challenge to reliably detect objects across visual domains.\nAlthough the topic has attracted attention recently, current approaches all\nrely on the ability to access a sizable amount of target data for use at\ntraining time. This is a heavy assumption, as often it is not possible to\nanticipate the domain where a detector will be used, nor to access it in\nadvance for data acquisition. Consider for instance the task of monitoring\nimage feeds from social media: as every image is created and uploaded by a\ndifferent user it belongs to a different target domain that is impossible to\nforesee during training. This paper addresses this setting, presenting an\nobject detection algorithm able to perform unsupervised adaption across domains\nby using only one target sample, seen at test time. We achieve this by\nintroducing a multi-task architecture that one-shot adapts to any incoming\nsample by iteratively solving a self-supervised task on it. We further enhance\nthis auxiliary adaptation with cross-task pseudo-labeling. A thorough benchmark\nanalysis against the most recent cross-domain detection methods and a detailed\nablation study show the advantage of our method, which sets the\nstate-of-the-art in the defined one-shot scenario.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 22:12:20 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["D'Innocente", "Antonio", ""], ["Borlino", "Francesco Cappio", ""], ["Bucci", "Silvia", ""], ["Caputo", "Barbara", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "2005.11619", "submitter": "Himanshu Sharma", "authors": "Himanshu Sharma and Elise Jennings", "title": "Bayesian Neural Networks at Scale: A Performance Analysis and Pruning\n  Study", "comments": null, "journal-ref": "Journal of Super Computing (2020)", "doi": "10.1007/s11227-020-03401-z", "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian neural Networks (BNNs) are a promising method of obtaining\nstatistical uncertainties for neural network predictions but with a higher\ncomputational overhead which can limit their practical usage. This work\nexplores the use of high performance computing with distributed training to\naddress the challenges of training BNNs at scale. We present a performance and\nscalability comparison of training the VGG-16 and Resnet-18 models on a\nCray-XC40 cluster. We demonstrate that network pruning can speed up inference\nwithout accuracy loss and provide an open source software package,\n{\\it{BPrune}} to automate this pruning. For certain models we find that pruning\nup to 80\\% of the network results in only a 7.0\\% loss in accuracy. With the\ndevelopment of new hardware accelerators for Deep Learning, BNNs are of\nconsiderable interest for benchmarking performance. This analysis of training a\nBNN at scale outlines the limitations and benefits compared to a conventional\nneural network.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 23:15:34 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 23:18:54 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Sharma", "Himanshu", ""], ["Jennings", "Elise", ""]]}, {"id": "2005.11622", "submitter": "Norman Tatro", "authors": "N. Joseph Tatro, Stefan C. Schonsheck, Rongjie Lai", "title": "Unsupervised Geometric Disentanglement for Surfaces via CFAN-VAE", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric disentanglement, the separation of latent codes for intrinsic (i.e.\nidentity) and extrinsic(i.e. pose) geometry, is a prominent task for generative\nmodels of non-Euclidean data such as 3D deformable models. It provides greater\ninterpretability of the latent space, and leads to more control in generation.\nThis work introduces a mesh feature, the conformal factor and normal feature\n(CFAN),for use in mesh convolutional autoencoders. We further propose CFAN-VAE,\na novel architecture that disentangles identity and pose using the CFAN\nfeature. Requiring no label information on the identity or pose during\ntraining, CFAN-VAE achieves geometric disentanglement in an unsupervisedway.\nOur comprehensive experiments, including reconstruction, interpolation,\ngeneration, and identity/pose transfer, demonstrate CFAN-VAE achieves\nstate-of-the-art performance on unsupervised geometric disentanglement. We also\nsuccessfully detect a level of geometric disentanglement in mesh convolutional\nautoencoders that encode xyz-coordinates directly by registering its latent\nspace to that of CFAN-VAE.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 23:28:10 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 01:50:38 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Tatro", "N. Joseph", ""], ["Schonsheck", "Stefan C.", ""], ["Lai", "Rongjie", ""]]}, {"id": "2005.11623", "submitter": "Zhihao Duan", "authors": "Zhihao Duan, M. Ozan Tezcan, Hayato Nakamura, Prakash Ishwar, Janusz\n  Konrad", "title": "RAPiD: Rotation-Aware People Detection in Overhead Fisheye Images", "comments": "CVPR 2020 OmniCV Workshop paper extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methods for people detection in overhead, fisheye images either use\nradially-aligned bounding boxes to represent people, assuming people always\nappear along image radius or require significant pre-/post-processing which\nradically increases computational complexity. In this work, we develop an\nend-to-end rotation-aware people detection method, named RAPiD, that detects\npeople using arbitrarily-oriented bounding boxes. Our fully-convolutional\nneural network directly regresses the angle of each bounding box using a\nperiodic loss function, which accounts for angle periodicities. We have also\ncreated a new dataset with spatio-temporal annotations of rotated bounding\nboxes, for people detection as well as other vision tasks in overhead fisheye\nvideos. We show that our simple, yet effective method outperforms\nstate-of-the-art results on three fisheye-image datasets. Code and dataset are\navailable at http://vip.bu.edu/rapid .\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 23:47:18 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Duan", "Zhihao", ""], ["Tezcan", "M. Ozan", ""], ["Nakamura", "Hayato", ""], ["Ishwar", "Prakash", ""], ["Konrad", "Janusz", ""]]}, {"id": "2005.11626", "submitter": "Xinchen Yan", "authors": "Kibok Lee, Zhuoyuan Chen, Xinchen Yan, Raquel Urtasun, Ersin Yumer", "title": "ShapeAdv: Generating Shape-Aware Adversarial 3D Point Clouds", "comments": "3D Point Clouds, Adversarial Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce ShapeAdv, a novel framework to study shape-aware adversarial\nperturbations that reflect the underlying shape variations (e.g., geometric\ndeformations and structural differences) in the 3D point cloud space. We\ndevelop shape-aware adversarial 3D point cloud attacks by leveraging the\nlearned latent space of a point cloud auto-encoder where the adversarial noise\nis applied in the latent space. Specifically, we propose three different\nvariants including an exemplar-based one by guiding the shape deformation with\nauxiliary data, such that the generated point cloud resembles the shape\nmorphing between objects in the same category. Different from prior works, the\nresulting adversarial 3D point clouds reflect the shape variations in the 3D\npoint cloud space while still being close to the original one. In addition,\nexperimental evaluations on the ModelNet40 benchmark demonstrate that our\nadversaries are more difficult to defend with existing point cloud defense\nmethods and exhibit a higher attack transferability across classifiers. Our\nshape-aware adversarial attacks are orthogonal to existing point cloud based\nattacks and shed light on the vulnerability of 3D deep neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 00:03:27 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Lee", "Kibok", ""], ["Chen", "Zhuoyuan", ""], ["Yan", "Xinchen", ""], ["Urtasun", "Raquel", ""], ["Yumer", "Ersin", ""]]}, {"id": "2005.11630", "submitter": "Ang Li", "authors": "Ang Li, Chunpeng Wu, Yiran Chen, Bin Ni", "title": "MVStylizer: An Efficient Edge-Assisted Video Photorealistic Style\n  Transfer System for Mobile Phones", "comments": null, "journal-ref": null, "doi": "10.1145/3397166.3409140", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has made great progress in realizing neural style transfer of\nimages, which denotes transforming an image to a desired style. Many users\nstart to use their mobile phones to record their daily life, and then edit and\nshare the captured images and videos with other users. However, directly\napplying existing style transfer approaches on videos, i.e., transferring the\nstyle of a video frame by frame, requires an extremely large amount of\ncomputation resources. It is still technically unaffordable to perform style\ntransfer of videos on mobile phones. To address this challenge, we propose\nMVStylizer, an efficient edge-assisted photorealistic video style transfer\nsystem for mobile phones. Instead of performing stylization frame by frame,\nonly key frames in the original video are processed by a pre-trained deep\nneural network (DNN) on edge servers, while the rest of stylized intermediate\nframes are generated by our designed optical-flow-based frame interpolation\nalgorithm on mobile phones. A meta-smoothing module is also proposed to\nsimultaneously upscale a stylized frame to arbitrary resolution and remove\nstyle transfer related distortions in these upscaled frames. In addition, for\nthe sake of continuously enhancing the performance of the DNN model on the edge\nserver, we adopt a federated learning scheme to keep retraining each DNN model\non the edge server with collected data from mobile clients and syncing with a\nglobal DNN model on the cloud server. Such a scheme effectively leverages the\ndiversity of collected data from various mobile clients and efficiently\nimproves the system performance. Our experiments demonstrate that MVStylizer\ncan generate stylized videos with an even better visual quality compared to the\nstate-of-the-art method while achieving 75.5$\\times$ speedup for\n1920$\\times$1080 videos.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 00:54:27 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 19:16:08 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Li", "Ang", ""], ["Wu", "Chunpeng", ""], ["Chen", "Yiran", ""], ["Ni", "Bin", ""]]}, {"id": "2005.11634", "submitter": "Ang Li", "authors": "Ang Li, Wei Du, Qinghua Li", "title": "PoliteCamera: Respecting Strangers' Privacy in Mobile Photographing", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-01701-9_13", "report-no": null, "categories": "cs.CR cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera is a standard on-board sensor of modern mobile phones. It makes photo\ntaking popular due to its convenience and high resolution. However, when users\ntake a photo of a scenery, a building or a target person, a stranger may also\nbe unintentionally captured in the photo. Such photos expose the location and\nactivity of strangers, and hence may breach their privacy. In this paper, we\npropose a cooperative mobile photographing scheme called PoliteCamera to\nprotect strangers' privacy. Through the cooperation between a photographer and\na stranger, the stranger's face in a photo can be automatically blurred upon\nhis request when the photo is taken. Since multiple strangers nearby the\nphotographer might send out blurring requests but not all of them are in the\nphoto, an adapted balanced convolutional neural network (ABCNN) is proposed to\ndetermine whether the requesting stranger is in the photo based on facial\nattributes. Evaluations demonstrate that the ABCNN can accurately predict\nfacial attributes and PoliteCamera can provide accurate privacy protection for\nstrangers.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 01:18:13 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Li", "Ang", ""], ["Du", "Wei", ""], ["Li", "Qinghua", ""]]}, {"id": "2005.11643", "submitter": "Angtian Wang", "authors": "Angtian Wang, Yihong Sun, Adam Kortylewski, Alan Yuille", "title": "Robust Object Detection under Occlusion with Context-Aware\n  CompositionalNets", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting partially occluded objects is a difficult task. Our experimental\nresults show that deep learning approaches, such as Faster R-CNN, are not\nrobust at object detection under occlusion. Compositional convolutional neural\nnetworks (CompositionalNets) have been shown to be robust at classifying\noccluded objects by explicitly representing the object as a composition of\nparts. In this work, we propose to overcome two limitations of\nCompositionalNets which will enable them to detect partially occluded objects:\n1) CompositionalNets, as well as other DCNN architectures, do not explicitly\nseparate the representation of the context from the object itself. Under strong\nobject occlusion, the influence of the context is amplified which can have\nsevere negative effects for detection at test time. In order to overcome this,\nwe propose to segment the context during training via bounding box annotations.\nWe then use the segmentation to learn a context-aware CompositionalNet that\ndisentangles the representation of the context and the object. 2) We extend the\npart-based voting scheme in CompositionalNets to vote for the corners of the\nobject's bounding box, which enables the model to reliably estimate bounding\nboxes for partially occluded objects. Our extensive experiments show that our\nproposed model can detect objects robustly, increasing the detection\nperformance of strongly occluded vehicles from PASCAL3D+ and MS-COCO by 41% and\n35% respectively in absolute performance relative to Faster R-CNN.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 02:57:34 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 14:33:14 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Wang", "Angtian", ""], ["Sun", "Yihong", ""], ["Kortylewski", "Adam", ""], ["Yuille", "Alan", ""]]}, {"id": "2005.11645", "submitter": "Zhiguo Wang", "authors": "Zhiguo Wang, Zhongliang Yang, Yujin Zhang", "title": "Master-Auxiliary: an efficient aggregation strategy for video anomaly\n  detection", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of surveillance video anomaly detection is to detect events that\nrarely or never happened in a certain scene. Generally, different detectors can\ndetect different anomalies. This paper proposes an efficient strategy to\naggregate multiple detectors. First, the aggregation strategy chooses one\ndetector as master detector by experience, and sets the remaining detectors as\nauxiliary detectors. Then, the aggregation strategy extracts credible\ninformation from auxiliary detectors, including credible abnormal (Cred-a)\nframes and credible normal (Cred-n) frames. After that, the frequencies that\neach video frame being judged as Cred-a and Cred-n are counted. Applying the\nevents' time continuity property, more Cred-a and Cred-n frames can be\ninferred. Finally, the aggregation strategy utilizes the Cred-a and Cred-n\nfrequencies to vote to calculate soft weights, and uses the soft weights to\nassist the master detector. Experiments are carried out on multiple datasets.\nComparing with existing aggregation strategies, the proposed strategy achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 03:09:08 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 13:34:56 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Wang", "Zhiguo", ""], ["Yang", "Zhongliang", ""], ["Zhang", "Yujin", ""]]}, {"id": "2005.11670", "submitter": "Cristina Palmero", "authors": "Cristina Palmero, Oleg V. Komogortsev, Sachin S. Talathi", "title": "Benefits of temporal information for appearance-based gaze estimation", "comments": "In ACM Symposium on Eye Tracking Research & Applications (ETRA), 2020", "journal-ref": null, "doi": "10.1145/3379156.3391376", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art appearance-based gaze estimation methods, usually based on\ndeep learning techniques, mainly rely on static features. However, temporal\ntrace of eye gaze contains useful information for estimating a given gaze\npoint. For example, approaches leveraging sequential eye gaze information when\napplied to remote or low-resolution image scenarios with off-the-shelf cameras\nare showing promising results. The magnitude of contribution from temporal gaze\ntrace is yet unclear for higher resolution/frame rate imaging systems, in which\nmore detailed information about an eye is captured. In this paper, we\ninvestigate whether temporal sequences of eye images, captured using a\nhigh-resolution, high-frame rate head-mounted virtual reality system, can be\nleveraged to enhance the accuracy of an end-to-end appearance-based\ndeep-learning model for gaze estimation. Performance is compared against a\nstatic-only version of the model. Results demonstrate statistically-significant\nbenefits of temporal information, particularly for the vertical component of\ngaze.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 07:19:53 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Palmero", "Cristina", ""], ["Komogortsev", "Oleg V.", ""], ["Talathi", "Sachin S.", ""]]}, {"id": "2005.11679", "submitter": "Chichun Zhou", "authors": "Yang Liu, Hai-Long Tu, Chi-Chun Zhou, Yi Liua and Fu-Lin Zhang", "title": "Networks with pixels embedding: a method to improve noise resistance in\n  images classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the task of images classification, usually, the network is sensitive to\nnoises. For example, an image of cat with noises might be misclassified as an\nostrich. Conventionally, to overcome the problem of noises, one uses the\ntechnique of data enhancement, that is, to teach the network to distinguish\nnoises by adding more images with noises in the training dataset. In this work,\nwe provide a noise-resistance network in images classification by introducing a\ntechnique of pixels embedding. We test the network with pixels embedding, which\nis abbreviated as the network with PE, on the mnist database of handwritten\ndigits. It shows that the network with PE outperforms the conventional network\non images with noises. The technique of pixels embedding can be used in many\ntasks of images classification to improve noise resistance.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 07:55:08 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 09:02:01 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Liu", "Yang", ""], ["Tu", "Hai-Long", ""], ["Zhou", "Chi-Chun", ""], ["Liua", "Yi", ""], ["Zhang", "Fu-Lin", ""]]}, {"id": "2005.11711", "submitter": "Andrei Cramariuc", "authors": "Andrei Cramariuc, Aleksandar Petrov, Rohit Suri, Mayank Mittal, Roland\n  Siegwart, Cesar Cadena", "title": "Learning Camera Miscalibration Detection", "comments": "ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-diagnosis and self-repair are some of the key challenges in deploying\nrobotic platforms for long-term real-world applications. One of the issues that\ncan occur to a robot is miscalibration of its sensors due to aging,\nenvironmental transients, or external disturbances. Precise calibration lies at\nthe core of a variety of applications, due to the need to accurately perceive\nthe world. However, while a lot of work has focused on calibrating the sensors,\nnot much has been done towards identifying when a sensor needs to be\nrecalibrated. This paper focuses on a data-driven approach to learn the\ndetection of miscalibration in vision sensors, specifically RGB cameras. Our\ncontributions include a proposed miscalibration metric for RGB cameras and a\nnovel semi-synthetic dataset generation pipeline based on this metric.\nAdditionally, by training a deep convolutional neural network, we demonstrate\nthe effectiveness of our pipeline to identify whether a recalibration of the\ncamera's intrinsic parameters is required or not. The code is available at\nhttp://github.com/ethz-asl/camera_miscalib_detection.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 10:32:49 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Cramariuc", "Andrei", ""], ["Petrov", "Aleksandar", ""], ["Suri", "Rohit", ""], ["Mittal", "Mayank", ""], ["Siegwart", "Roland", ""], ["Cadena", "Cesar", ""]]}, {"id": "2005.11715", "submitter": "Neslihan Bayramoglu", "authors": "Neslihan Bayramoglu, Miika T. Nieminen and Simo Saarakkala", "title": "A Lightweight CNN and Joint Shape-Joint Space (JS2) Descriptor for\n  Radiological Osteoarthritis Detection", "comments": "MIUA2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knee osteoarthritis (OA) is very common progressive and degenerative\nmusculoskeletal disease worldwide creates a heavy burden on patients with\nreduced quality of life and also on society due to financial impact. Therefore,\nany attempt to reduce the burden of the disease could help both patients and\nsociety. In this study, we propose a fully automated novel method, based on\ncombination of joint shape and convolutional neural network (CNN) based bone\ntexture features, to distinguish between the knee radiographs with and without\nradiographic osteoarthritis. Moreover, we report the first attempt at\ndescribing the bone texture using CNN. Knee radiographs from Osteoarthritis\nInitiative (OAI) and Multicenter Osteoarthritis (MOST) studies were used in the\nexperiments. Our models were trained on 8953 knee radiographs from OAI and\nevaluated on 3445 knee radiographs from MOST. Our results demonstrate that\nfusing the proposed shape and texture parameters achieves the state-of-the art\nperformance in radiographic OA detection yielding area under the ROC curve\n(AUC) of 95.21%\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 10:48:38 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Bayramoglu", "Neslihan", ""], ["Nieminen", "Miika T.", ""], ["Saarakkala", "Simo", ""]]}, {"id": "2005.11716", "submitter": "Yaxin Shi", "authors": "Yaxin Shi, Yuangang Pan, Donna Xu and Ivor W. Tsang", "title": "Multi-view Alignment and Generation in CCA via Consistent Latent\n  Encoding", "comments": "37 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view alignment, achieving one-to-one correspondence of multi-view\ninputs, is critical in many real-world multi-view applications, especially for\ncross-view data analysis problems. Recently, an increasing number of works\nstudy this alignment problem with Canonical Correlation Analysis (CCA).\nHowever, existing CCA models are prone to misalign the multiple views due to\neither the neglect of uncertainty or the inconsistent encoding of the multiple\nviews. To tackle these two issues, this paper studies multi-view alignment from\nthe Bayesian perspective. Delving into the impairments of inconsistent\nencodings, we propose to recover correspondence of the multi-view inputs by\nmatching the marginalization of the joint distribution of multi-view random\nvariables under different forms of factorization. To realize our design, we\npresent Adversarial CCA (ACCA) which achieves consistent latent encodings by\nmatching the marginalized latent encodings through the adversarial training\nparadigm. Our analysis based on conditional mutual information reveals that\nACCA is flexible for handling implicit distributions. Extensive experiments on\ncorrelation analysis and cross-view generation under noisy input settings\ndemonstrate the superiority of our model.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 10:50:15 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Shi", "Yaxin", ""], ["Pan", "Yuangang", ""], ["Xu", "Donna", ""], ["Tsang", "Ivor W.", ""]]}, {"id": "2005.11742", "submitter": "Yu Zeng", "authors": "Yu Zeng, Zhe Lin, Jimei Yang, Jianming Zhang, Eli Shechtman, Huchuan\n  Lu", "title": "High-Resolution Image Inpainting with Iterative Confidence Feedback and\n  Guided Upsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing image inpainting methods often produce artifacts when dealing with\nlarge holes in real applications. To address this challenge, we propose an\niterative inpainting method with a feedback mechanism. Specifically, we\nintroduce a deep generative model which not only outputs an inpainting result\nbut also a corresponding confidence map. Using this map as feedback, it\nprogressively fills the hole by trusting only high-confidence pixels inside the\nhole at each iteration and focuses on the remaining pixels in the next\niteration. As it reuses partial predictions from the previous iterations as\nknown pixels, this process gradually improves the result. In addition, we\npropose a guided upsampling network to enable generation of high-resolution\ninpainting results. We achieve this by extending the Contextual Attention\nmodule to borrow high-resolution feature patches in the input image.\nFurthermore, to mimic real object removal scenarios, we collect a large object\nmask dataset and synthesize more realistic training data that better simulates\nuser inputs. Experiments show that our method significantly outperforms\nexisting methods in both quantitative and qualitative evaluations. More results\nand Web APP are available at https://zengxianyu.github.io/iic.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 13:23:45 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 05:52:30 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Zeng", "Yu", ""], ["Lin", "Zhe", ""], ["Yang", "Jimei", ""], ["Zhang", "Jianming", ""], ["Shechtman", "Eli", ""], ["Lu", "Huchuan", ""]]}, {"id": "2005.11746", "submitter": "Jitender Singh", "authors": "Jitender Singh Virk and Deepti R. Bathula", "title": "Domain Specific, Semi-Supervised Transfer Learning for Medical Imaging", "comments": "9 pages 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited availability of annotated medical imaging data poses a challenge for\ndeep learning algorithms. Although transfer learning minimizes this hurdle in\ngeneral, knowledge transfer across disparate domains is shown to be less\neffective. On the other hand, smaller architectures were found to be more\ncompelling in learning better features. Consequently, we propose a lightweight\narchitecture that uses mixed asymmetric kernels (MAKNet) to reduce the number\nof parameters significantly. Additionally, we train the proposed architecture\nusing semi-supervised learning to provide pseudo-labels for a large medical\ndataset to assist with transfer learning. The proposed MAKNet provides better\nclassification performance with $60 - 70\\%$ less parameters than popular\narchitectures. Experimental results also highlight the importance of\ndomain-specific knowledge for effective transfer learning.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 13:50:00 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Virk", "Jitender Singh", ""], ["Bathula", "Deepti R.", ""]]}, {"id": "2005.11772", "submitter": "Dawid Rymarczyk", "authors": "Bartosz Zieli\\'nski and Agnieszka Sroka-Oleksiak and Dawid Rymarczyk\n  and Adam Piekarczyk and Monika Brzychczy-W{\\l}och", "title": "Deep learning approach to describe and classify fungi microscopic images", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0234806", "report-no": "MIDL/2020/ExtendedAbstract/AEhp_Cqq-h", "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preliminary diagnosis of fungal infections can rely on microscopic\nexamination. However, in many cases, it does not allow unambiguous\nidentification of the species by microbiologist due to their visual similarity.\nTherefore, it is usually necessary to use additional biochemical tests. That\ninvolves additional costs and extends the identification process up to 10 days.\nSuch a delay in the implementation of targeted therapy may be grave in\nconsequence as the mortality rate for immunosuppressed patients is high. In\nthis paper, we apply a machine learning approach based on deep neural networks\nand Fisher Vector (advanced bag-of-words method) to classify microscopic images\nof various fungi species. Our approach has the potential to make the last stage\nof biochemical identification redundant, shortening the identification process\nby 2-3 days, and reducing the cost of the diagnosis.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 15:15:07 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Zieli\u0144ski", "Bartosz", ""], ["Sroka-Oleksiak", "Agnieszka", ""], ["Rymarczyk", "Dawid", ""], ["Piekarczyk", "Adam", ""], ["Brzychczy-W\u0142och", "Monika", ""]]}, {"id": "2005.11780", "submitter": "Chen Lv", "authors": "Zhongxu Hu, Yang Xing, Chen Lv, Peng Hang, Jie Liu", "title": "Deep Convolutional Neural Network-based Bernoulli Heatmap for Head Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head pose estimation is a crucial problem for many tasks, such as driver\nattention, fatigue detection, and human behaviour analysis. It is well known\nthat neural networks are better at handling classification problems than\nregression problems. It is an extremely nonlinear process to let the network\noutput the angle value directly for optimization learning, and the weight\nconstraint of the loss function will be relatively weak. This paper proposes a\nnovel Bernoulli heatmap for head pose estimation from a single RGB image. Our\nmethod can achieve the positioning of the head area while estimating the angles\nof the head. The Bernoulli heatmap makes it possible to construct fully\nconvolutional neural networks without fully connected layers and provides a new\nidea for the output form of head pose estimation. A deep convolutional neural\nnetwork (CNN) structure with multiscale representations is adopted to maintain\nhigh-resolution information and low-resolution information in parallel. This\nkind of structure can maintain rich, high-resolution representations. In\naddition, channelwise fusion is adopted to make the fusion weights learnable\ninstead of simple addition with equal weights. As a result, the estimation is\nspatially more precise and potentially more accurate. The effectiveness of the\nproposed method is empirically demonstrated by comparing it with other\nstate-of-the-art methods on public datasets.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 15:36:29 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Hu", "Zhongxu", ""], ["Xing", "Yang", ""], ["Lv", "Chen", ""], ["Hang", "Peng", ""], ["Liu", "Jie", ""]]}, {"id": "2005.11794", "submitter": "Geir Ole Tysse", "authors": "Geir Ole Tysse, Andrej Cibicik and Olav Egeland", "title": "Vision-based control of a knuckle boom crane with online cable length\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vision-based controller for a knuckle boom crane is presented. The\ncontroller is used to control the motion of the crane tip and at the same time\ncompensate for payload oscillations. The oscillations of the payload are\nmeasured with three cameras that are fixed to the crane king and are used to\ntrack two spherical markers fixed to the payload cable. Based on color and size\ninformation, each camera identifies the image points corresponding to the\nmarkers. The payload angles are then determined using linear triangulation of\nthe image points. An extended Kalman filter is used for estimation of payload\nangles and angular velocity. The length of the payload cable is also estimated\nusing a least squares technique with projection. The crane is controlled by a\nlinear cascade controller where the inner control loop is designed to damp out\nthe pendulum oscillation, and the crane tip is controlled by the outer loop.\nThe control variable of the controller is the commanded crane tip acceleration,\nwhich is converted to a velocity command using a velocity loop. The performance\nof the control system is studied experimentally using a scaled laboratory\nversion of a knuckle boom crane.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 16:38:14 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 07:26:30 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Tysse", "Geir Ole", ""], ["Cibicik", "Andrej", ""], ["Egeland", "Olav", ""]]}, {"id": "2005.11811", "submitter": "Tuan-Duy Nguyen", "authors": "Tuan-Duy H. Nguyen, Huu-Nghia H. Nguyen, Hieu Dao", "title": "Recognizing Families through Images with Pretrained Encoder", "comments": "Will appear as part of RFIW2020 in the Proceedings of 2020\n  International Conference on Automatic Face and Gesture Recognition (IEEE\n  AMFG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kinship verification and kinship retrieval are emerging tasks in computer\nvision. Kinship verification aims at determining whether two facial images are\nfrom related people or not, while kinship retrieval is the task of retrieving\npossible related facial images to a person from a gallery of images. They\nintroduce unique challenges because of the hidden relations and features that\ncarry inherent characteristics between the facial images. We employ 3 methods,\nFaceNet, Siamese VGG-Face, and a combination of FaceNet and VGG-Face models as\nfeature extractors, to achieve the 9th standing for kinship verification and\nthe 5th standing for kinship retrieval in the Recognizing Family in The Wild\n2020 competition. We then further experimented using StyleGAN2 as another\nencoder, with no improvement in the result.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 17:59:19 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Nguyen", "Tuan-Duy H.", ""], ["Nguyen", "Huu-Nghia H.", ""], ["Dao", "Hieu", ""]]}, {"id": "2005.11864", "submitter": "Dong Wang", "authors": "Dong Wang", "title": "An efficient iterative method for reconstructing surface from point\n  clouds", "comments": "23 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface reconstruction from point clouds is a fundamental step in many\napplications in computer vision. In this paper, we develop an efficient\niterative method on a variational model for the surface reconstruction from\npoint clouds. The surface is implicitly represented by indicator functions and\nthe energy functional is then approximated based on such representations using\nheat kernel convolutions. We then develop a novel iterative method to minimize\nthe approximate energy and prove the energy decaying property during each\niteration. We then use asymptotic expansion to give a connection between the\nproposed algorithm and active contour models. Extensive numerical experiments\nare performed in both 2- and 3- dimensional Euclidean spaces to show that the\nproposed method is simple, efficient, and accurate.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 00:01:53 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wang", "Dong", ""]]}, {"id": "2005.11875", "submitter": "Gengyan Zhao", "authors": "Gengyan Zhao, Mary E. Meyerand and Rasmus M. Birn", "title": "Bayesian Conditional GAN for MRI Brain Image Synthesis", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a powerful technique in medical imaging, image synthesis is widely used in\napplications such as denoising, super resolution and modality transformation\netc. Recently, the revival of deep neural networks made immense progress in the\nfield of medical imaging. Although many deep leaning based models have been\nproposed to improve the image synthesis accuracy, the evaluation of the model\nuncertainty, which is highly important for medical applications, has been a\nmissing part. In this work, we propose to use Bayesian conditional generative\nadversarial network (GAN) with concrete dropout to improve image synthesis\naccuracy. Meanwhile, an uncertainty calibration approach is involved in the\nwhole pipeline to make the uncertainty generated by Bayesian network\ninterpretable. The method is validated with the T1w to T2w MR image translation\nwith a brain tumor dataset of 102 subjects. Compared with the conventional\nBayesian neural network with Monte Carlo dropout, results of the proposed\nmethod reach a significant lower RMSE with a p-value of 0.0186. Improvement of\nthe calibration of the generated uncertainty by the uncertainty recalibration\nmethod is also illustrated.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 00:58:23 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 15:04:17 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zhao", "Gengyan", ""], ["Meyerand", "Mary E.", ""], ["Birn", "Rasmus M.", ""]]}, {"id": "2005.11904", "submitter": "Shangxi Wu", "authors": "Shangxi Wu and Jitao Sang and Kaiyuan Xu and Guanhua Zheng and\n  Changsheng Xu", "title": "Adaptive Adversarial Logits Pairing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples provide an opportunity as well as impose a challenge for\nunderstanding image classification systems. Based on the analysis of the\nadversarial training solution Adversarial Logits Pairing (ALP), we observed in\nthis work that: (1) The inference of adversarially robust model tends to rely\non fewer high-contribution features compared with vulnerable ones. (2) The\ntraining target of ALP doesn't fit well to a noticeable part of samples, where\nthe logits pairing loss is overemphasized and obstructs minimizing the\nclassification loss. Motivated by these observations, we design an Adaptive\nAdversarial Logits Pairing (AALP) solution by modifying the training process\nand training target of ALP. Specifically, AALP consists of an adaptive feature\noptimization module with Guided Dropout to systematically pursue fewer\nhigh-contribution features, and an adaptive sample weighting module by setting\nsample-specific training weights to balance between logits pairing loss and\nclassification loss. The proposed AALP solution demonstrates superior defense\nperformance on multiple datasets with extensive experiments.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 03:12:20 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 01:57:11 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Wu", "Shangxi", ""], ["Sang", "Jitao", ""], ["Xu", "Kaiyuan", ""], ["Zheng", "Guanhua", ""], ["Xu", "Changsheng", ""]]}, {"id": "2005.11909", "submitter": "Jian Jia", "authors": "Jian Jia, Houjing Huang, Wenjie Yang, Xiaotang Chen, Kaiqi Huang", "title": "Rethinking of Pedestrian Attribute Recognition: Realistic Datasets with\n  Efficient Method", "comments": "12 pages, 4 figures. Code is availabel at\n  https://github.com/valencebond/Strong_Baseline_of_Pedestrian_Attribute_Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite various methods are proposed to make progress in pedestrian attribute\nrecognition, a crucial problem on existing datasets is often neglected, namely,\na large number of identical pedestrian identities in train and test set, which\nis not consistent with practical application. Thus, images of the same\npedestrian identity in train set and test set are extremely similar, leading to\noverestimated performance of state-of-the-art methods on existing datasets. To\naddress this problem, we propose two realistic datasets\nPETA\\textsubscript{$zs$} and RAPv2\\textsubscript{$zs$} following zero-shot\nsetting of pedestrian identities based on PETA and RAPv2 datasets. Furthermore,\ncompared to our strong baseline method, we have observed that recent\nstate-of-the-art methods can not make performance improvement on PETA, RAPv2,\nPETA\\textsubscript{$zs$} and RAPv2\\textsubscript{$zs$}. Thus, through solving\nthe inherent attribute imbalance in pedestrian attribute recognition, an\nefficient method is proposed to further improve the performance. Experiments on\nexisting and proposed datasets verify the superiority of our method by\nachieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 03:30:15 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 01:13:23 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Jia", "Jian", ""], ["Huang", "Houjing", ""], ["Yang", "Wenjie", ""], ["Chen", "Xiaotang", ""], ["Huang", "Kaiqi", ""]]}, {"id": "2005.11922", "submitter": "Ang Li Ph.D.", "authors": "Huanhuan Fan, Yuhao Zhou, Ang Li, Shuang Gao, Jijunnan Li, Yandong Guo", "title": "Visual Localization Using Semantic Segmentation and Depth Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a monocular visual localization pipeline leveraging\nsemantic and depth cues. We apply semantic consistency evaluation to rank the\nimage retrieval results and a practical clustering technique to reject\nestimation outliers. In addition, we demonstrate a substantial performance\nboost achieved with a combination of multiple feature extractors. Furthermore,\nby using depth prediction with a deep neural network, we show that a\nsignificant amount of falsely matched keypoints are identified and eliminated.\nThe proposed pipeline outperforms most of the existing approaches at the\nLong-Term Visual Localization benchmark 2020.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 04:55:27 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Fan", "Huanhuan", ""], ["Zhou", "Yuhao", ""], ["Li", "Ang", ""], ["Gao", "Shuang", ""], ["Li", "Jijunnan", ""], ["Guo", "Yandong", ""]]}, {"id": "2005.11926", "submitter": "Sheng Wang", "authors": "Sheng Wang, Jiayu Huo, Xi Ouyang, Jifei Che, Xuhua Ren, Zhong Xue,\n  Qian Wang, Jie-Zhi Cheng", "title": "mr2NST: Multi-Resolution and Multi-Reference Neural Style Transfer for\n  Mammography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided diagnosis with deep learning techniques has been shown to be\nhelpful for the diagnosis of the mammography in many clinical studies. However,\nthe image styles of different vendors are very distinctive, and there may exist\ndomain gap among different vendors that could potentially compromise the\nuniversal applicability of one deep learning model. In this study, we\nexplicitly address style variety issue with the proposed multi-resolution and\nmulti-reference neural style transfer (mr2NST) network. The mr2NST can\nnormalize the styles from different vendors to the same style baseline with\nvery high resolution. We illustrate that the image quality of the transferred\nimages is comparable to the quality of original images of the target domain\n(vendor) in terms of NIMA scores. Meanwhile, the mr2NST results are also shown\nto be helpful for the lesion detection in mammograms.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 05:24:29 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wang", "Sheng", ""], ["Huo", "Jiayu", ""], ["Ouyang", "Xi", ""], ["Che", "Jifei", ""], ["Ren", "Xuhua", ""], ["Xue", "Zhong", ""], ["Wang", "Qian", ""], ["Cheng", "Jie-Zhi", ""]]}, {"id": "2005.11930", "submitter": "Benjamin Lucas", "authors": "Benjamin Lucas, Charlotte Pelletier, Daniel Schmidt, Geoffrey I. Webb,\n  and Fran\\c{c}ois Petitjean", "title": "A Bayesian-inspired, deep learning-based, semi-supervised domain\n  adaptation technique for land cover mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Land cover maps are a vital input variable to many types of environmental\nresearch and management. While they can be produced automatically by machine\nlearning techniques, these techniques require substantial training data to\nachieve high levels of accuracy, which are not always available. One technique\nresearchers use when labelled training data are scarce is domain adaptation\n(DA) -- where data from an alternate region, known as the source domain, are\nused to train a classifier and this model is adapted to map the study region,\nor target domain. The scenario we address in this paper is known as\nsemi-supervised DA, where some labelled samples are available in the target\ndomain. In this paper we present Sourcerer, a Bayesian-inspired, deep\nlearning-based, semi-supervised DA technique for producing land cover maps from\nSITS data. The technique takes a convolutional neural network trained on a\nsource domain and then trains further on the available target domain with a\nnovel regularizer applied to the model weights. The regularizer adjusts the\ndegree to which the model is modified to fit the target data, limiting the\ndegree of change when the target data are few in number and increasing it as\ntarget data quantity increases. Our experiments on Sentinel-2 time series\nimages compare Sourcerer with two state-of-the-art semi-supervised domain\nadaptation techniques and four baseline models. We show that on two different\nsource-target domain pairings Sourcerer outperforms all other methods for any\nquantity of labelled target data available. In fact, the results on the more\ndifficult target domain show that the starting accuracy of Sourcerer (when no\nlabelled target data are available), 74.2%, is greater than the next-best\nstate-of-the-art method trained on 20,000 labelled target instances.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 05:36:50 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 05:57:44 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Lucas", "Benjamin", ""], ["Pelletier", "Charlotte", ""], ["Schmidt", "Daniel", ""], ["Webb", "Geoffrey I.", ""], ["Petitjean", "Fran\u00e7ois", ""]]}, {"id": "2005.11943", "submitter": "Mingjie Wang", "authors": "Mingjie Wang and Hao Cai and Jun Zhou and Minglun Gong", "title": "Interlayer and Intralayer Scale Aggregation for Scale-invariant Crowd\n  Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting is an important vision task, which faces challenges on\ncontinuous scale variation within a given scene and huge density shift both\nwithin and across images. These challenges are typically addressed using\nmulti-column structures in existing methods. However, such an approach does not\nprovide consistent improvement and transferability due to limited ability in\ncapturing multi-scale features, sensitiveness to large density shift, and\ndifficulty in training multi-branch models. To overcome these limitations, a\nSingle-column Scale-invariant Network (ScSiNet) is presented in this paper,\nwhich extracts sophisticated scale-invariant features via the combination of\ninterlayer multi-scale integration and a novel intralayer scale-invariant\ntransformation (SiT). Furthermore, in order to enlarge the diversity of\ndensities, a randomly integrated loss is presented for training our\nsingle-branch method. Extensive experiments on public datasets demonstrate that\nthe proposed method consistently outperforms state-of-the-art approaches in\ncounting accuracy and achieves remarkable transferability and scale-invariant\nproperty.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 06:59:31 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wang", "Mingjie", ""], ["Cai", "Hao", ""], ["Zhou", "Jun", ""], ["Gong", "Minglun", ""]]}, {"id": "2005.11945", "submitter": "Bing Cao", "authors": "Bing Cao, Nannan Wang, Xinbo Gao, Jie Li, Zhifeng Li", "title": "Multi-Margin based Decorrelation Learning for Heterogeneous Face\n  Recognition", "comments": "IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous face recognition (HFR) refers to matching face images acquired\nfrom different domains with wide applications in security scenarios. This paper\npresents a deep neural network approach namely Multi-Margin based Decorrelation\nLearning (MMDL) to extract decorrelation representations in a hyperspherical\nspace for cross-domain face images. The proposed framework can be divided into\ntwo components: heterogeneous representation network and decorrelation\nrepresentation learning. First, we employ a large scale of accessible visual\nface images to train heterogeneous representation network. The decorrelation\nlayer projects the output of the first component into decorrelation latent\nsubspace and obtains decorrelation representation. In addition, we design a\nmulti-margin loss (MML), which consists of quadruplet margin loss (QML) and\nheterogeneous angular margin loss (HAML), to constrain the proposed framework.\nExperimental results on two challenging heterogeneous face databases show that\nour approach achieves superior performance on both verification and recognition\ntasks, comparing with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 07:01:12 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Cao", "Bing", ""], ["Wang", "Nannan", ""], ["Gao", "Xinbo", ""], ["Li", "Jie", ""], ["Li", "Zhifeng", ""]]}, {"id": "2005.11960", "submitter": "Mikhail Belyaev", "authors": "Maxim Pisov, Vladimir Kondratenko, Alexey Zakharov, Alexey Petraikin,\n  Victor Gombolevskiy, Sergey Morozov, Mikhail Belyaev", "title": "Keypoints Localization for Joint Vertebra Detection and Fracture\n  Severity Quantification", "comments": "Accepted to MICCAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertebral body compression fractures are reliable early signs of\nosteoporosis. Though these fractures are visible on Computed Tomography (CT)\nimages, they are frequently missed by radiologists in clinical settings. Prior\nresearch on automatic methods of vertebral fracture classification proves its\nreliable quality; however, existing methods provide hard-to-interpret outputs\nand sometimes fail to process cases with severe abnormalities such as highly\npathological vertebrae or scoliosis. We propose a new two-step algorithm to\nlocalize the vertebral column in 3D CT images and then to simultaneously detect\nindividual vertebrae and quantify fractures in 2D. We train neural networks for\nboth steps using a simple 6-keypoints based annotation scheme, which\ncorresponds precisely to current medical recommendation. Our algorithm has no\nexclusion criteria, processes 3D CT in 2 seconds on a single GPU, and provides\nan intuitive and verifiable output. The method approaches expert-level\nperformance and demonstrates state-of-the-art results in vertebrae 3D\nlocalization (the average error is 1 mm), vertebrae 2D detection (precision is\n0.99, recall is 1), and fracture identification (ROC AUC at the patient level\nis 0.93).\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 08:05:27 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 12:46:48 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Pisov", "Maxim", ""], ["Kondratenko", "Vladimir", ""], ["Zakharov", "Alexey", ""], ["Petraikin", "Alexey", ""], ["Gombolevskiy", "Victor", ""], ["Morozov", "Sergey", ""], ["Belyaev", "Mikhail", ""]]}, {"id": "2005.11977", "submitter": "Renlong Hang", "authors": "Renlong Hang, Zhu Li, Qingshan Liu, Pedram Ghamisi, and Shuvra S.\n  Bhattacharyya", "title": "Hyperspectral Image Classification with Attention Aided CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been widely used for hyperspectral\nimage classification. As a common process, small cubes are firstly cropped from\nthe hyperspectral image and then fed into CNNs to extract spectral and spatial\nfeatures. It is well known that different spectral bands and spatial positions\nin the cubes have different discriminative abilities. If fully explored, this\nprior information will help improve the learning capacity of CNNs. Along this\ndirection, we propose an attention aided CNN model for spectral-spatial\nclassification of hyperspectral images. Specifically, a spectral attention\nsub-network and a spatial attention sub-network are proposed for spectral and\nspatial classification, respectively. Both of them are based on the traditional\nCNN model, and incorporate attention modules to aid networks focus on more\ndiscriminative channels or positions. In the final classification phase, the\nspectral classification result and the spatial classification result are\ncombined together via an adaptively weighted summation method. To evaluate the\neffectiveness of the proposed model, we conduct experiments on three standard\nhyperspectral datasets. The experimental results show that the proposed model\ncan achieve superior performance compared to several state-of-the-art\nCNN-related models.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 08:40:56 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 14:25:00 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Hang", "Renlong", ""], ["Li", "Zhu", ""], ["Liu", "Qingshan", ""], ["Ghamisi", "Pedram", ""], ["Bhattacharyya", "Shuvra S.", ""]]}, {"id": "2005.11994", "submitter": "Pradipta Biswas", "authors": "Vinay Krishna Sharma, L.R.D. Murthy, KamalPreet Singh Saluja, Vimal\n  Mollyn, Gourav Sharma and Pradipta Biswas", "title": "Eye Gaze Controlled Robotic Arm for Persons with SSMI", "comments": "Citation: VK Sharma, KPS Saluja, LRD Murthy, G Sharma and P Biswas,\n  Webcam Controlled Robotic Arm for Persons with SSMI, Technology and\n  Disability 32 (3), IOS Press 2020 [Official journal of EU AAATE association]", "journal-ref": null, "doi": "10.3233/TAD-200264", "report-no": null, "categories": "cs.HC cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: People with severe speech and motor impairment (SSMI) often uses\na technique called eye pointing to communicate with outside world. One of their\nparents, caretakers or teachers hold a printed board in front of them and by\nanalyzing their eye gaze manually, their intentions are interpreted. This\ntechnique is often error prone and time consuming and depends on a single\ncaretaker.\n  Objective: We aimed to automate the eye tracking process electronically by\nusing commercially available tablet, computer or laptop and without requiring\nany dedicated hardware for eye gaze tracking. The eye gaze tracker is used to\ndevelop a video see through based AR (augmented reality) display that controls\na robotic device with eye gaze and deployed for a fabric printing task.\n  Methodology: We undertook a user centred design process and separately\nevaluated the web cam based gaze tracker and the video see through based human\nrobot interaction involving users with SSMI. We also reported a user study on\nmanipulating a robotic arm with webcam based eye gaze tracker.\n  Results: Using our bespoke eye gaze controlled interface, able bodied users\ncan select one of nine regions of screen at a median of less than 2 secs and\nusers with SSMI can do so at a median of 4 secs. Using the eye gaze controlled\nhuman-robot AR display, users with SSMI could undertake representative pick and\ndrop task at an average duration less than 15 secs and reach a randomly\ndesignated target within 60 secs using a COTS eye tracker and at an average\ntime of 2 mins using the webcam based eye gaze tracker.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 09:23:20 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Sharma", "Vinay Krishna", ""], ["Murthy", "L. R. D.", ""], ["Saluja", "KamalPreet Singh", ""], ["Mollyn", "Vimal", ""], ["Sharma", "Gourav", ""], ["Biswas", "Pradipta", ""]]}, {"id": "2005.12027", "submitter": "Kenta Yamamoto", "authors": "Kenta Yamamoto, Ryota Kawamura, Kazuki Takazawa, Hiroyuki Osone,\n  Yoichi Ochiai", "title": "A Preliminary Study for Identification of Additive Manufactured Objects\n  with Transmitted Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Additive manufacturing has the potential to become a standard method for\nmanufacturing products, and product information is indispensable for the item\ndistribution system. While most products are given barcodes to the exterior\nsurfaces, research on embedding barcodes inside products is underway. This is\nbecause additive manufacturing makes it possible to carry out manufacturing and\ninformation adding at the same time, and embedding information inside does not\nimpair the exterior appearance of the product. However, products that have not\nbeen embedded information can not be identified, and embedded information can\nnot be rewritten later. In this study, we have developed a product\nidentification system that does not require embedding barcodes inside. This\nsystem uses a transmission image of the product which contains information of\neach product such as different inner support structures and manufacturing\nerrors. We have shown through experiments that if datasets of transmission\nimages are available, objects can be identified with an accuracy of over 90%.\nThis result suggests that our approach can be useful for identifying objects\nwithout embedded information.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 11:04:04 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Yamamoto", "Kenta", ""], ["Kawamura", "Ryota", ""], ["Takazawa", "Kazuki", ""], ["Osone", "Hiroyuki", ""], ["Ochiai", "Yoichi", ""]]}, {"id": "2005.12044", "submitter": "Huikai Shao", "authors": "Huikai Shao and Dexing Zhong", "title": "A Joint Pixel and Feature Alignment Framework for Cross-dataset\n  Palmprint Recognition", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based palmprint recognition algorithms have shown great\npotential. Most of them are mainly focused on identifying samples from the same\ndataset. However, they may be not suitable for a more convenient case that the\nimages for training and test are from different datasets, such as collected by\nembedded terminals and smartphones. Therefore, we propose a novel Joint Pixel\nand Feature Alignment (JPFA) framework for such cross-dataset palmprint\nrecognition scenarios. Two stage-alignment is applied to obtain adaptive\nfeatures in source and target datasets. 1) Deep style transfer model is adopted\nto convert source images into fake images to reduce the dataset gaps and\nperform data augmentation on pixel level. 2) A new deep domain adaptation model\nis proposed to extract adaptive features by aligning the dataset-specific\ndistributions of target-source and target-fake pairs on feature level. Adequate\nexperiments are conducted on several benchmarks including constrained and\nunconstrained palmprint databases. The results demonstrate that our JPFA\noutperforms other models to achieve the state-of-the-arts. Compared with\nbaseline, the accuracy of cross-dataset identification is improved by up to\n28.10% and the Equal Error Rate (EER) of cross-dataset verification is reduced\nby up to 4.69%. To make our results reproducible, the codes are publicly\navailable at http://gr.xjtu.edu.cn/web/bell/resource.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 11:40:51 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Shao", "Huikai", ""], ["Zhong", "Dexing", ""]]}, {"id": "2005.12066", "submitter": "Uwe Schmidt", "authors": "Sarah Schmell and Falk Zakrzewski and Walter de Back and Martin\n  Weigert and Uwe Schmidt and Torsten Wenke and Silke Zeugner and Robert Mantey\n  and Christian Sperling and Ingo Roeder and Pia Hoenscheid and Daniela Aust\n  and Gustavo Baretton", "title": "An interpretable automated detection system for FISH-based HER2 oncogene\n  amplification testing in histo-pathological routine images of breast and\n  gastric cancer diagnostics", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/qDEYfzeK7k", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histo-pathological diagnostics are an inherent part of the everyday work but\nare particularly laborious and associated with time-consuming manual analysis\nof image data. In order to cope with the increasing diagnostic case numbers due\nto the current growth and demographic change of the global population and the\nprogress in personalized medicine, pathologists ask for assistance. Profiting\nfrom digital pathology and the use of artificial intelligence, individual\nsolutions can be offered (e.g. detect labeled cancer tissue sections). The\ntesting of the human epidermal growth factor receptor 2 (HER2) oncogene\namplification status via fluorescence in situ hybridization (FISH) is\nrecommended for breast and gastric cancer diagnostics and is regularly\nperformed at clinics. Here, we develop an interpretable, deep learning\n(DL)-based pipeline which automates the evaluation of FISH images with respect\nto HER2 gene amplification testing. It mimics the pathological assessment and\nrelies on the detection and localization of interphase nuclei based on instance\nsegmentation networks. Furthermore, it localizes and classifies fluorescence\nsignals within each nucleus with the help of image classification and object\ndetection convolutional neural networks (CNNs). Finally, the pipeline\nclassifies the whole image regarding its HER2 amplification status. The\nvisualization of pixels on which the networks' decision occurs, complements an\nessential part to enable interpretability by pathologists.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 12:14:38 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Schmell", "Sarah", ""], ["Zakrzewski", "Falk", ""], ["de Back", "Walter", ""], ["Weigert", "Martin", ""], ["Schmidt", "Uwe", ""], ["Wenke", "Torsten", ""], ["Zeugner", "Silke", ""], ["Mantey", "Robert", ""], ["Sperling", "Christian", ""], ["Roeder", "Ingo", ""], ["Hoenscheid", "Pia", ""], ["Aust", "Daniela", ""], ["Baretton", "Gustavo", ""]]}, {"id": "2005.12073", "submitter": "Matei Mancas", "authors": "Mancas Matei, Kong Phutphalla, Gosselin Bernard", "title": "Visual Attention: Deep Rare Features", "comments": "6 pages, double-colmun, accepted to IVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human visual system is modeled in engineering field providing\nfeature-engineered methods which detect contrasted/surprising/unusual data into\nimages. This data is \"interesting\" for humans and leads to numerous\napplications. Deep learning (DNNs) drastically improved the algorithms\nefficiency on the main benchmark datasets. However, DNN-based models are\ncounter-intuitive: surprising or unusual data is by definition difficult to\nlearn because of its low occurrence probability. In reality, DNNs models mainly\nlearn top-down features such as faces, text, people, or animals which usually\nattract human attention, but they have low efficiency in extracting surprising\nor unusual data in the images. In this paper, we propose a model called\nDeepRare2019 (DR) which uses the power of DNNs feature extraction and the\ngenericity of feature-engineered algorithms. DR 1) does not need any training,\n2) it takes less than a second per image on CPU only and 3) our tests on three\nvery different eye-tracking datasets show that DR is generic and is always in\nthe top-3 models on all datasets and metrics while no other model exhibits such\na regularity and genericity. DeepRare2019 code can be found at\nhttps://github.com/numediart/VisualAttention-RareFamily\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 12:28:08 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Matei", "Mancas", ""], ["Phutphalla", "Kong", ""], ["Bernard", "Gosselin", ""]]}, {"id": "2005.12074", "submitter": "Ester Gonzalez-Sosa", "authors": "Andrija Gajic and Ester Gonzalez-Sosa and Diego Gonzalez-Morin and\n  Marcos Escudero-Vi\\~nolo and Alvaro Villegas", "title": "Egocentric Human Segmentation for Mixed Reality", "comments": "Accepted for presentation at EPIC@CVPR2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to segment human body parts from egocentric\nvideo using semantic segmentation networks. Our contribution is two-fold: i) we\ncreate a semi-synthetic dataset composed of more than 15, 000 realistic images\nand associated pixel-wise labels of egocentric human body parts, such as arms\nor legs including different demographic factors; ii) building upon the\nThunderNet architecture, we implement a deep learning semantic segmentation\nalgorithm that is able to perform beyond real-time requirements (16 ms for 720\nx 720 images). It is believed that this method will enhance sense of presence\nof Virtual Environments and will constitute a more realistic solution to the\nstandard virtual avatars.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 12:34:47 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 14:58:07 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Gajic", "Andrija", ""], ["Gonzalez-Sosa", "Ester", ""], ["Gonzalez-Morin", "Diego", ""], ["Escudero-Vi\u00f1olo", "Marcos", ""], ["Villegas", "Alvaro", ""]]}, {"id": "2005.12110", "submitter": "Konstantin Dobratulin", "authors": "Konstantin Dobratulin, Andrey Gaidel, Irina Aupova, Anna Ivleva,\n  Aleksandr Kapishnikov, Pavel Zelter", "title": "The efficiency of deep learning algorithms for detecting anatomical\n  reference points on radiological images of the head profile", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we investigate the efficiency of deep learning algorithms in\nsolving the task of detecting anatomical reference points on radiological\nimages of the head in lateral projection using a fully convolutional neural\nnetwork and a fully convolutional neural network with an extended architecture\nfor biomedical image segmentation - U-Net. A comparison is made for the results\nof detection anatomical reference points for each of the selected neural\nnetwork architectures and their comparison with the results obtained when\northodontists detected anatomical reference points. Based on the obtained\nresults, it was concluded that a U-Net neural network allows performing the\ndetection of anatomical reference points more accurately than a fully\nconvolutional neural network. The results of the detection of anatomical\nreference points by the U-Net neural network are closer to the average results\nof the detection of reference points by a group of orthodontists.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 13:51:03 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 09:12:49 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Dobratulin", "Konstantin", ""], ["Gaidel", "Andrey", ""], ["Aupova", "Irina", ""], ["Ivleva", "Anna", ""], ["Kapishnikov", "Aleksandr", ""], ["Zelter", "Pavel", ""]]}, {"id": "2005.12126", "submitter": "Seung Wook Kim", "authors": "Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, Sanja\n  Fidler", "title": "Learning to Simulate Dynamic Environments with GameGAN", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation is a crucial component of any robotic system. In order to simulate\ncorrectly, we need to write complex rules of the environment: how dynamic\nagents behave, and how the actions of each of the agents affect the behavior of\nothers. In this paper, we aim to learn a simulator by simply watching an agent\ninteract with an environment. We focus on graphics games as a proxy of the real\nenvironment. We introduce GameGAN, a generative model that learns to visually\nimitate a desired game by ingesting screenplay and keyboard actions during\ntraining. Given a key pressed by the agent, GameGAN \"renders\" the next screen\nusing a carefully designed generative adversarial network. Our approach offers\nkey advantages over existing work: we design a memory module that builds an\ninternal map of the environment, allowing for the agent to return to previously\nvisited locations with high visual consistency. In addition, GameGAN is able to\ndisentangle static and dynamic components within an image making the behavior\nof the model more interpretable, and relevant for downstream tasks that require\nexplicit reasoning over dynamic elements. This enables many interesting\napplications such as swapping different components of the game to build new\ngames that do not exist.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 14:10:17 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Kim", "Seung Wook", ""], ["Zhou", "Yuhao", ""], ["Philion", "Jonah", ""], ["Torralba", "Antonio", ""], ["Fidler", "Sanja", ""]]}, {"id": "2005.12147", "submitter": "Mayank Singh", "authors": "Mayank Kumar Singh, Sayan Banerjee, Shubhasis Chaudhuri", "title": "NENET: An Edge Learnable Network for Link Prediction in Scene Text", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Text detection in scenes based on deep neural networks have shown promising\nresults. Instead of using word bounding box regression, recent state-of-the-art\nmethods have started focusing on character bounding box and pixel-level\nprediction. This necessitates the need to link adjacent characters, which we\npropose in this paper using a novel Graph Neural Network (GNN) architecture\nthat allows us to learn both node and edge features as opposed to only the node\nfeatures under the typical GNN. The main advantage of using GNN for link\nprediction lies in its ability to connect characters which are spatially\nseparated and have an arbitrary orientation. We show our concept on the well\nknown SynthText dataset, achieving top results as compared to state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 14:47:16 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Singh", "Mayank Kumar", ""], ["Banerjee", "Sayan", ""], ["Chaudhuri", "Shubhasis", ""]]}, {"id": "2005.12155", "submitter": "Xiaoli Liu", "authors": "Xiaoli Liu, Jianqin Yin, Huaping Liu, Jun Liu", "title": "DeepSSM: Deep State-Space Model for 3D Human Motion Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future human motion plays a significant role in human-machine\ninteractions for a variety of real-life applications. In this paper, we build a\ndeep state-space model, DeepSSM, to predict future human motion. Specifically,\nwe formulate the human motion system as the state-space model of a dynamic\nsystem and model the motion system by the state-space theory, offering a\nunified formulation for diverse human motion systems. Moreover, a novel deep\nnetwork is designed to build this system, enabling us to utilize both the\nadvantages of deep network and state-space model. The deep network jointly\nmodels the process of both the state-state transition and the state-observation\ntransition of the human motion system, and multiple future poses can be\ngenerated via the state-observation transition of the model recursively. To\nimprove the modeling ability of the system, a unique loss function, ATPL\n(Attention Temporal Prediction Loss), is introduced to optimize the model,\nencouraging the system to achieve more accurate predictions by paying\nincreasing attention to the early time-steps. The experiments on two benchmark\ndatasets (i.e., Human3.6M and 3DPW) confirm that our method achieves\nstate-of-the-art performance with improved effectiveness. The code will be\navailable if the paper is accepted.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 15:06:12 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 08:10:33 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2020 08:02:44 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Liu", "Xiaoli", ""], ["Yin", "Jianqin", ""], ["Liu", "Huaping", ""], ["Liu", "Jun", ""]]}, {"id": "2005.12188", "submitter": "Mona Minakshi", "authors": "Mona Minakshi, Pratool Bharti, Willie B. McClinton III, Jamshidbek\n  Mirzakhalov, Ryan M. Carney, Sriram Chellappan", "title": "Automating the Surveillance of Mosquito Vectors from Trapped Specimens\n  Using Computer Vision Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among all animals, mosquitoes are responsible for the most deaths worldwide.\nInterestingly, not all types of mosquitoes spread diseases, but rather, a\nselect few alone are competent enough to do so. In the case of any disease\noutbreak, an important first step is surveillance of vectors (i.e., those\nmosquitoes capable of spreading diseases). To do this today, public health\nworkers lay several mosquito traps in the area of interest. Hundreds of\nmosquitoes will get trapped. Naturally, among these hundreds, taxonomists have\nto identify only the vectors to gauge their density. This process today is\nmanual, requires complex expertise/ training, and is based on visual inspection\nof each trapped specimen under a microscope. It is long, stressful and\nself-limiting. This paper presents an innovative solution to this problem. Our\ntechnique assumes the presence of an embedded camera (similar to those in\nsmart-phones) that can take pictures of trapped mosquitoes. Our techniques\nproposed here will then process these images to automatically classify the\ngenus and species type. Our CNN model based on Inception-ResNet V2 and Transfer\nLearning yielded an overall accuracy of 80% in classifying mosquitoes when\ntrained on 25,867 images of 250 trapped mosquito vector specimens captured via\nmany smart-phone cameras. In particular, the accuracy of our model in\nclassifying Aedes aegypti and Anopheles stephensi mosquitoes (both of which are\ndeadly vectors) is amongst the highest. We present important lessons learned\nand practical impact of our techniques towards the end of the paper.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 15:58:27 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 19:58:45 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Minakshi", "Mona", ""], ["Bharti", "Pratool", ""], ["McClinton", "Willie B.", "III"], ["Mirzakhalov", "Jamshidbek", ""], ["Carney", "Ryan M.", ""], ["Chellappan", "Sriram", ""]]}, {"id": "2005.12209", "submitter": "Fengze Liu", "authors": "Fengze Liu and Jinzheng Cai and Yuankai Huo and Chi-Tung Cheng and\n  Ashwin Raju and Dakai Jin and Jing Xiao and Alan Yuille and Le Lu and\n  ChienHung Liao and Adam P Harrison", "title": "JSSR: A Joint Synthesis, Segmentation, and Registration System for 3D\n  Multi-Modal Image Alignment of Large-scale Pathological CT Scans", "comments": "accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal image registration is a challenging problem that is also an\nimportant clinical task for many real applications and scenarios. As a first\nstep in analysis, deformable registration among different image modalities is\noften required in order to provide complementary visual information. During\nregistration, semantic information is key to match homologous points and\npixels. Nevertheless, many conventional registration methods are incapable in\ncapturing high-level semantic anatomical dense correspondences. In this work,\nwe propose a novel multi-task learning system, JSSR, based on an end-to-end 3D\nconvolutional neural network that is composed of a generator, a registration\nand a segmentation component. The system is optimized to satisfy the implicit\nconstraints between different tasks in an unsupervised manner. It first\nsynthesizes the source domain images into the target domain, then an\nintra-modal registration is applied on the synthesized images and target\nimages. The segmentation module are then applied on the synthesized and target\nimages, providing additional cues based on semantic correspondences. The\nsupervision from another fully-annotated dataset is used to regularize the\nsegmentation. We extensively evaluate JSSR on a large-scale medical image\ndataset containing 1,485 patient CT imaging studies of four different contrast\nphases (i.e., 5,940 3D CT scans with pathological livers) on the registration,\nsegmentation and synthesis tasks. The performance is improved after joint\ntraining on the registration and segmentation tasks by 0.9% and 1.9%\nrespectively compared to a highly competitive and accurate deep learning\nbaseline. The registration also consistently outperforms conventional\nstate-of-the-art multi-modal registration methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 16:30:02 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 18:58:44 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 18:00:31 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Liu", "Fengze", ""], ["Cai", "Jinzheng", ""], ["Huo", "Yuankai", ""], ["Cheng", "Chi-Tung", ""], ["Raju", "Ashwin", ""], ["Jin", "Dakai", ""], ["Xiao", "Jing", ""], ["Yuille", "Alan", ""], ["Lu", "Le", ""], ["Liao", "ChienHung", ""], ["Harrison", "Adam P", ""]]}, {"id": "2005.12250", "submitter": "Dat Thanh Tran", "authors": "Dat Thanh Tran, Nikolaos Passalis, Anastasios Tefas, Moncef Gabbouj,\n  Alexandros Iosifidis", "title": "Attention-based Neural Bag-of-Features Learning for Sequence Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose 2D-Attention (2DA), a generic attention formulation\nfor sequence data, which acts as a complementary computation block that can\ndetect and focus on relevant sources of information for the given learning\nobjective. The proposed attention module is incorporated into the recently\nproposed Neural Bag of Feature (NBoF) model to enhance its learning capacity.\nSince 2DA acts as a plug-in layer, injecting it into different computation\nstages of the NBoF model results in different 2DA-NBoF architectures, each of\nwhich possesses a unique interpretation. We conducted extensive experiments in\nfinancial forecasting, audio analysis as well as medical diagnosis problems to\nbenchmark the proposed formulations in comparison with existing methods,\nincluding the widely used Gated Recurrent Units. Our empirical analysis shows\nthat the proposed attention formulations can not only improve performances of\nNBoF models but also make them resilient to noisy data.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 17:51:54 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Tran", "Dat Thanh", ""], ["Passalis", "Nikolaos", ""], ["Tefas", "Anastasios", ""], ["Gabbouj", "Moncef", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2005.12256", "submitter": "Devendra Singh Chaplot", "authors": "Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav Gupta, Saurabh\n  Gupta", "title": "Neural Topological SLAM for Visual Navigation", "comments": "Published in CVPR 2020. See the project webpage at\n  https://devendrachaplot.github.io/projects/Neural-Topological-SLAM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of image-goal navigation which involves\nnavigating to the location indicated by a goal image in a novel previously\nunseen environment. To tackle this problem, we design topological\nrepresentations for space that effectively leverage semantics and afford\napproximate geometric reasoning. At the heart of our representations are nodes\nwith associated semantic features, that are interconnected using coarse\ngeometric information. We describe supervised learning-based algorithms that\ncan build, maintain and use such representations under noisy actuation.\nExperimental study in visually and physically realistic simulation suggests\nthat our method builds effective representations that capture structural\nregularities and efficiently solve long-horizon navigation problems. We observe\na relative improvement of more than 50% over existing methods that study this\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 17:56:29 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 22:56:12 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Chaplot", "Devendra Singh", ""], ["Salakhutdinov", "Ruslan", ""], ["Gupta", "Abhinav", ""], ["Gupta", "Saurabh", ""]]}, {"id": "2005.12318", "submitter": "Sanjana Sinha", "authors": "Sanjana Sinha, Sandika Biswas and Brojeshwar Bhowmick", "title": "Identity-Preserving Realistic Talking Face Generation", "comments": "Accepted in IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech-driven facial animation is useful for a variety of applications such\nas telepresence, chatbots, etc. The necessary attributes of having a realistic\nface animation are 1) audio-visual synchronization (2) identity preservation of\nthe target individual (3) plausible mouth movements (4) presence of natural eye\nblinks. The existing methods mostly address the audio-visual lip\nsynchronization, and few recent works have addressed the synthesis of natural\neye blinks for overall video realism. In this paper, we propose a method for\nidentity-preserving realistic facial animation from speech. We first generate\nperson-independent facial landmarks from audio using DeepSpeech features for\ninvariance to different voices, accents, etc. To add realism, we impose eye\nblinks on facial landmarks using unsupervised learning and retargets the\nperson-independent landmarks to person-specific landmarks to preserve the\nidentity-related facial structure which helps in the generation of plausible\nmouth shapes of the target identity. Finally, we use LSGAN to generate the\nfacial texture from person-specific facial landmarks, using an attention\nmechanism that helps to preserve identity-related texture. An extensive\ncomparison of our proposed method with the current state-of-the-art methods\ndemonstrates a significant improvement in terms of lip synchronization\naccuracy, image reconstruction quality, sharpness, and identity-preservation. A\nuser study also reveals improved realism of our animation results over the\nstate-of-the-art methods. To the best of our knowledge, this is the first work\nin speech-driven 2D facial animation that simultaneously addresses all the\nabove-mentioned attributes of a realistic speech-driven face animation.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 18:08:28 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Sinha", "Sanjana", ""], ["Biswas", "Sandika", ""], ["Bhowmick", "Brojeshwar", ""]]}, {"id": "2005.12320", "submitter": "Wouter Van Gansbeke", "authors": "Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc\n  Proesmans, Luc Van Gool", "title": "SCAN: Learning to Classify Images without Labels", "comments": "Accepted at ECCV 2020. Includes supplementary. Code and pretrained\n  models at https://github.com/wvangansbeke/Unsupervised-Classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we automatically group images into semantically meaningful clusters when\nground-truth annotations are absent? The task of unsupervised image\nclassification remains an important, and open challenge in computer vision.\nSeveral recent approaches have tried to tackle this problem in an end-to-end\nfashion. In this paper, we deviate from recent works, and advocate a two-step\napproach where feature learning and clustering are decoupled. First, a\nself-supervised task from representation learning is employed to obtain\nsemantically meaningful features. Second, we use the obtained features as a\nprior in a learnable clustering approach. In doing so, we remove the ability\nfor cluster learning to depend on low-level features, which is present in\ncurrent end-to-end learning approaches. Experimental evaluation shows that we\noutperform state-of-the-art methods by large margins, in particular +26.6% on\nCIFAR10, +25.0% on CIFAR100-20 and +21.3% on STL10 in terms of classification\naccuracy. Furthermore, our method is the first to perform well on a large-scale\ndataset for image classification. In particular, we obtain promising results on\nImageNet, and outperform several semi-supervised learning methods in the\nlow-data regime without the use of any ground-truth annotations. The code is\nmade publicly available at\nhttps://github.com/wvangansbeke/Unsupervised-Classification.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 18:12:33 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 15:25:54 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Van Gansbeke", "Wouter", ""], ["Vandenhende", "Simon", ""], ["Georgoulis", "Stamatios", ""], ["Proesmans", "Marc", ""], ["Van Gool", "Luc", ""]]}, {"id": "2005.12420", "submitter": "Terence Broad", "authors": "Terence Broad, Frederic Fol Leymarie, Mick Grierson", "title": "Network Bending: Expressive Manipulation of Deep Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a new framework for manipulating and interacting with deep\ngenerative models that we call network bending. We present a comprehensive set\nof deterministic transformations that can be inserted as distinct layers into\nthe computational graph of a trained generative neural network and applied\nduring inference. In addition, we present a novel algorithm for analysing the\ndeep generative model and clustering features based on their spatial activation\nmaps. This allows features to be grouped together based on spatial similarity\nin an unsupervised fashion. This results in the meaningful manipulation of sets\nof features that correspond to the generation of a broad array of semantically\nsignificant features of the generated images. We outline this framework,\ndemonstrating our results on state-of-the-art deep generative models trained on\nseveral image datasets. We show how it allows for the direct manipulation of\nsemantically meaningful aspects of the generative process as well as allowing\nfor a broad range of expressive outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 21:48:45 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 15:06:56 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Broad", "Terence", ""], ["Leymarie", "Frederic Fol", ""], ["Grierson", "Mick", ""]]}, {"id": "2005.12439", "submitter": "Haitian Zheng", "authors": "Haitian Zheng, Kefei Wu, Jong-Hwi Park, Wei Zhu, Jiebo Luo", "title": "Personalized Fashion Recommendation from Personal Social Media Data: An\n  Item-to-Set Metric Learning Approach", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of online shopping for fashion products, accurate fashion\nrecommendation has become a critical problem. Meanwhile, social networks\nprovide an open and new data source for personalized fashion analysis. In this\nwork, we study the problem of personalized fashion recommendation from social\nmedia data, i.e. recommending new outfits to social media users that fit their\nfashion preferences. To this end, we present an item-to-set metric learning\nframework that learns to compute the similarity between a set of historical\nfashion items of a user to a new fashion item. To extract features from\nmulti-modal street-view fashion items, we propose an embedding module that\nperforms multi-modality feature extraction and cross-modality gated fusion. To\nvalidate the effectiveness of our approach, we collect a real-world social\nmedia dataset. Extensive experiments on the collected dataset show the superior\nperformance of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 23:24:24 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Zheng", "Haitian", ""], ["Wu", "Kefei", ""], ["Park", "Jong-Hwi", ""], ["Zhu", "Wei", ""], ["Luo", "Jiebo", ""]]}, {"id": "2005.12444", "submitter": "Yuchuan Gou", "authors": "Yuchuan Gou, Qiancheng Wu, Minghao Li, Bo Gong, Mei Han", "title": "SegAttnGAN: Text to Image Generation with Segmentation Attention", "comments": "Accepted to the AI for Content Creation Workshop at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel generative network (SegAttnGAN) that\nutilizes additional segmentation information for the text-to-image synthesis\ntask. As the segmentation data introduced to the model provides useful guidance\non the generator training, the proposed model can generate images with better\nrealism quality and higher quantitative measures compared with the previous\nstate-of-art methods. We achieved Inception Score of 4.84 on the CUB dataset\nand 3.52 on the Oxford-102 dataset. Besides, we tested the self-attention\nSegAttnGAN which uses generated segmentation data instead of masks from\ndatasets for attention and achieved similar high-quality results, suggesting\nthat our model can be adapted for the text-to-image synthesis task.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 23:56:41 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Gou", "Yuchuan", ""], ["Wu", "Qiancheng", ""], ["Li", "Minghao", ""], ["Gong", "Bo", ""], ["Han", "Mei", ""]]}, {"id": "2005.12466", "submitter": "Bukweon Kim", "authors": "Sihwan Kim and Taejang Park", "title": "Learning Robust Feature Representations for Scene Text Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text detection based on deep neural networks have progressed\nsubstantially over the past years. However, previous state-of-the-art methods\nmay still fall short when dealing with challenging public benchmarks because\nthe performances of algorithm are determined by the robust features extraction\nand components in network architecture. To address this issue, we will present\na network architecture derived from the loss to maximize conditional\nlog-likelihood by optimizing the lower bound with a proper approximate\nposterior that has shown impressive performance in several generative models.\nIn addition, by extending the layer of latent variables to multiple layers, the\nnetwork is able to learn robust features on scale with no task-specific\nregularization or data augmentation. We provide a detailed analysis and show\nthe results on three public benchmark datasets to confirm the efficiency and\nreliability of the proposed algorithm. In experiments, the proposed algorithm\nsignificantly outperforms state-of-the-art methods in terms of both recall and\nprecision. Specifically, it achieves an H-mean of 95.12 and 96.78 on ICDAR 2011\nand ICDAR 2013, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 01:06:47 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Kim", "Sihwan", ""], ["Park", "Taejang", ""]]}, {"id": "2005.12469", "submitter": "Mat\\'ias Mendieta", "authors": "Mat\\'ias Mendieta and Hamed Tabkhi", "title": "CARPe Posterum: A Convolutional Approach for Real-time Pedestrian Path\n  Prediction", "comments": "AAAI-21 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian path prediction is an essential topic in computer vision and video\nunderstanding. Having insight into the movement of pedestrians is crucial for\nensuring safe operation in a variety of applications including autonomous\nvehicles, social robots, and environmental monitoring. Current works in this\narea utilize complex generative or recurrent methods to capture many possible\nfutures. However, despite the inherent real-time nature of predicting future\npaths, little work has been done to explore accurate and computationally\nefficient approaches for this task. To this end, we propose a convolutional\napproach for real-time pedestrian path prediction, CARPe. It utilizes a\nvariation of Graph Isomorphism Networks in combination with an agile\nconvolutional neural network design to form a fast and accurate path prediction\napproach. Notable results in both inference speed and prediction accuracy are\nachieved, improving FPS considerably in comparison to current state-of-the-art\nmethods while delivering competitive accuracy on well-known path prediction\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 01:10:01 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 01:59:38 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 02:36:25 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Mendieta", "Mat\u00edas", ""], ["Tabkhi", "Hamed", ""]]}, {"id": "2005.12486", "submitter": "Lingbo Yang", "authors": "Lingbo Yang, Pan Wang, Xinfeng Zhang, Shanshe Wang, Zhanning Gao,\n  Peiran Ren, Xuansong Xie, Siwei Ma, Wen Gao", "title": "Region-adaptive Texture Enhancement for Detailed Person Image Synthesis", "comments": "Accepted in ICME 2020 oral, Recommended for TMM journal", "journal-ref": "2020 IEEE International Conference on Multimedia and Expo (ICME\n  2020)", "doi": "10.1109/ICME46284.2020.9102862", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The ability to produce convincing textural details is essential for the\nfidelity of synthesized person images. However, existing methods typically\nfollow a ``warping-based'' strategy that propagates appearance features through\nthe same pathway used for pose transfer. However, most fine-grained features\nwould be lost due to down-sampling, leading to over-smoothed clothes and\nmissing details in the output images. In this paper we presents RATE-Net, a\nnovel framework for synthesizing person images with sharp texture details. The\nproposed framework leverages an additional texture enhancing module to extract\nappearance information from the source image and estimate a fine-grained\nresidual texture map, which helps to refine the coarse estimation from the pose\ntransfer module. In addition, we design an effective alternate updating\nstrategy to promote mutual guidance between two modules for better shape and\nappearance consistency. Experiments conducted on DeepFashion benchmark dataset\nhave demonstrated the superiority of our framework compared with existing\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 02:33:21 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Yang", "Lingbo", ""], ["Wang", "Pan", ""], ["Zhang", "Xinfeng", ""], ["Wang", "Shanshe", ""], ["Gao", "Zhanning", ""], ["Ren", "Peiran", ""], ["Xie", "Xuansong", ""], ["Ma", "Siwei", ""], ["Gao", "Wen", ""]]}, {"id": "2005.12494", "submitter": "Lingbo Yang", "authors": "Lingbo Yang, Pan Wang, Chang Liu, Zhanning Gao, Peiran Ren, Xinfeng\n  Zhang, Shanshe Wang, Siwei Ma, Xiansheng Hua, Wen Gao", "title": "Towards Fine-grained Human Pose Transfer with Detail Replenishing\n  Network", "comments": "IEEE TIP accepted at 10.1109/TIP.2021.3052364", "journal-ref": "in IEEE Transactions on Image Processing, vol. 30, pp. 2422-2435,\n  2021", "doi": "10.1109/TIP.2021.3052364", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Human pose transfer (HPT) is an emerging research topic with huge potential\nin fashion design, media production, online advertising and virtual reality.\nFor these applications, the visual realism of fine-grained appearance details\nis crucial for production quality and user engagement. However, existing HPT\nmethods often suffer from three fundamental issues: detail deficiency, content\nambiguity and style inconsistency, which severely degrade the visual quality\nand realism of generated images. Aiming towards real-world applications, we\ndevelop a more challenging yet practical HPT setting, termed as Fine-grained\nHuman Pose Transfer (FHPT), with a higher focus on semantic fidelity and detail\nreplenishment. Concretely, we analyze the potential design flaws of existing\nmethods via an illustrative example, and establish the core FHPT methodology by\ncombing the idea of content synthesis and feature transfer together in a\nmutually-guided fashion. Thereafter, we substantiate the proposed methodology\nwith a Detail Replenishing Network (DRN) and a corresponding coarse-to-fine\nmodel training scheme. Moreover, we build up a complete suite of fine-grained\nevaluation protocols to address the challenges of FHPT in a comprehensive\nmanner, including semantic analysis, structural detection and perceptual\nquality assessment. Extensive experiments on the DeepFashion benchmark dataset\nhave verified the power of proposed benchmark against start-of-the-art works,\nwith 12\\%-14\\% gain on top-10 retrieval recall, 5\\% higher joint localization\naccuracy, and near 40\\% gain on face identity preservation. Moreover, the\nevaluation results offer further insights to the subject matter, which could\ninspire many promising future works along this direction.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 03:05:23 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 04:39:39 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Yang", "Lingbo", ""], ["Wang", "Pan", ""], ["Liu", "Chang", ""], ["Gao", "Zhanning", ""], ["Ren", "Peiran", ""], ["Zhang", "Xinfeng", ""], ["Wang", "Shanshe", ""], ["Ma", "Siwei", ""], ["Hua", "Xiansheng", ""], ["Gao", "Wen", ""]]}, {"id": "2005.12500", "submitter": "Shan-Jean Wu", "authors": "Shan-Jean Wu, Chih-Yuan Yang and Jane Yung-jen Hsu", "title": "CalliGAN: Style and Structure-aware Chinese Calligraphy Character\n  Generator", "comments": "the work has been accepted to the AI for content creation workshop at\n  CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese calligraphy is the writing of Chinese characters as an art form\nperformed with brushes so Chinese characters are rich of shapes and details.\nRecent studies show that Chinese characters can be generated through\nimage-to-image translation for multiple styles using a single model. We propose\na novel method of this approach by incorporating Chinese characters' component\ninformation into its model. We also propose an improved network to convert\ncharacters to their embedding space. Experiments show that the proposed method\ngenerates high-quality Chinese calligraphy characters over state-of-the-art\nmethods measured through numerical evaluations and human subject studies.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 03:15:03 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Wu", "Shan-Jean", ""], ["Yang", "Chih-Yuan", ""], ["Hsu", "Jane Yung-jen", ""]]}, {"id": "2005.12513", "submitter": "Fernanda Ribeiro", "authors": "Fernanda L. Ribeiro, Steffen Bollmann, Alexander M. Puckett", "title": "DeepRetinotopy: Predicting the Functional Organization of Human Visual\n  Cortex from Structural MRI Data using Geometric Deep Learning", "comments": null, "journal-ref": null, "doi": "10.1101/2020.02.11.934471", "report-no": "MIDL/2020/ExtendedAbstract/Nw_trRFjPE", "categories": "q-bio.NC cs.CV cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Whether it be in a man-made machine or a biological system, form and function\nare often directly related. In the latter, however, this particular\nrelationship is often unclear due to the intricate nature of biology. Here we\ndeveloped a geometric deep learning model capable of exploiting the actual\nstructure of the cortex to learn the complex relationship between brain\nfunction and anatomy from structural and functional MRI data. Our model was not\nonly able to predict the functional organization of human visual cortex from\nanatomical properties alone, but it was also able to predict nuanced variations\nacross individuals.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 04:54:31 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Ribeiro", "Fernanda L.", ""], ["Bollmann", "Steffen", ""], ["Puckett", "Alexander M.", ""]]}, {"id": "2005.12524", "submitter": "Sauradip Nag", "authors": "Sauradip Nag, Palaiahnakote Shivakumara, Umapada Pal, Tong Lu and\n  Michael Blumenstein", "title": "A New Unified Method for Detecting Text from Marathon Runners and Sports\n  Players in Video", "comments": "Accepted in Pattern Recognition, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting text located on the torsos of marathon runners and sports players\nin video is a challenging issue due to poor quality and adverse effects caused\nby flexible/colorful clothing, and different structures of human bodies or\nactions. This paper presents a new unified method for tackling the above\nchallenges. The proposed method fuses gradient magnitude and direction\ncoherence of text pixels in a new way for detecting candidate regions.\nCandidate regions are used for determining the number of temporal frame\nclusters obtained by K-means clustering on frame differences. This process in\nturn detects key frames. The proposed method explores Bayesian probability for\nskin portions using color values at both pixel and component levels of temporal\nframes, which provides fused images with skin components. Based on skin\ninformation, the proposed method then detects faces and torsos by finding\nstructural and spatial coherences between them. We further propose adaptive\npixels linking a deep learning model for text detection from torso regions. The\nproposed method is tested on our own dataset collected from marathon/sports\nvideo and three standard datasets, namely, RBNR, MMM and R-ID of marathon\nimages, to evaluate the performance. In addition, the proposed method is also\ntested on the standard natural scene datasets, namely, CTW1500 and MS-COCO text\ndatasets, to show the objectiveness of the proposed method. A comparative study\nwith the state-of-the-art methods on bib number/text detection of different\ndatasets shows that the proposed method outperforms the existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 05:54:28 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Nag", "Sauradip", ""], ["Shivakumara", "Palaiahnakote", ""], ["Pal", "Umapada", ""], ["Lu", "Tong", ""], ["Blumenstein", "Michael", ""]]}, {"id": "2005.12536", "submitter": "Zhouxia Wang", "authors": "Zhouxia Wang, Jiawei Zhang, Mude Lin, Jiong Wang, Ping Luo, and Jimmy\n  Ren", "title": "Learning a Reinforced Agent for Flexible Exposure Bracketing Selection", "comments": "to be published in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically selecting exposure bracketing (images exposed differently) is\nimportant to obtain a high dynamic range image by using multi-exposure fusion.\nUnlike previous methods that have many restrictions such as requiring camera\nresponse function, sensor noise model, and a stream of preview images with\ndifferent exposures (not accessible in some scenarios e.g. some mobile\napplications), we propose a novel deep neural network to automatically select\nexposure bracketing, named EBSNet, which is sufficiently flexible without\nhaving the above restrictions. EBSNet is formulated as a reinforced agent that\nis trained by maximizing rewards provided by a multi-exposure fusion network\n(MEFNet). By utilizing the illumination and semantic information extracted from\njust a single auto-exposure preview image, EBSNet can select an optimal\nexposure bracketing for multi-exposure fusion. EBSNet and MEFNet can be jointly\ntrained to produce favorable results against recent state-of-the-art\napproaches. To facilitate future research, we provide a new benchmark dataset\nfor multi-exposure selection and fusion.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 06:24:42 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Wang", "Zhouxia", ""], ["Zhang", "Jiawei", ""], ["Lin", "Mude", ""], ["Wang", "Jiong", ""], ["Luo", "Ping", ""], ["Ren", "Jimmy", ""]]}, {"id": "2005.12541", "submitter": "Xinhai Liu", "authors": "Xinhai Liu, Zhizhong Han, Yu-Shen Liu, Matthias Zwicker", "title": "Fine-Grained 3D Shape Classification with Hierarchical Part-View\n  Attentions", "comments": "Accepted by IEEE Transactions on Image Processing, 2020. The FG3D\n  dataset is available at https://github.com/liuxinhai/FG3D-Net", "journal-ref": null, "doi": "10.1109/TIP.2020.3048623", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained 3D shape classification is important for shape understanding and\nanalysis, which poses a challenging research problem. However, the studies on\nthe fine-grained 3D shape classification have rarely been explored, due to the\nlack of fine-grained 3D shape benchmarks. To address this issue, we first\nintroduce a new 3D shape dataset (named FG3D dataset) with fine-grained class\nlabels, which consists of three categories including airplane, car and chair.\nEach category consists of several subcategories at a fine-grained level.\nAccording to our experiments under this fine-grained dataset, we find that\nstate-of-the-art methods are significantly limited by the small variance among\nsubcategories in the same category. To resolve this problem, we further propose\na novel fine-grained 3D shape classification method named FG3D-Net to capture\nthe fine-grained local details of 3D shapes from multiple rendered views.\nSpecifically, we first train a Region Proposal Network (RPN) to detect the\ngenerally semantic parts inside multiple views under the benchmark of generally\nsemantic part detection. Then, we design a hierarchical part-view attention\naggregation module to learn a global shape representation by aggregating\ngenerally semantic part features, which preserves the local details of 3D\nshapes. The part-view attention module hierarchically leverages part-level and\nview-level attention to increase the discriminability of our features. The\npart-level attention highlights the important parts in each view while the\nview-level attention highlights the discriminative views among all the views of\nthe same object. In addition, we integrate a Recurrent Neural Network (RNN) to\ncapture the spatial relationships among sequential views from different\nviewpoints. Our results under the fine-grained 3D shape dataset show that our\nmethod outperforms other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 06:53:19 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 06:34:39 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Liu", "Xinhai", ""], ["Han", "Zhizhong", ""], ["Liu", "Yu-Shen", ""], ["Zwicker", "Matthias", ""]]}, {"id": "2005.12544", "submitter": "Jing Zhang", "authors": "Jing Zhang, Wanqing Li, Lu sheng, Chang Tang, Philip Ogunbona", "title": "Unsupervised Domain Expansion from Multiple Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an existing system learned from previous source domains, it is\ndesirable to adapt the system to new domains without accessing and forgetting\nall the previous domains in some applications. This problem is known as domain\nexpansion. Unlike traditional domain adaptation in which the target domain is\nthe domain defined by new data, in domain expansion the target domain is formed\njointly by the source domains and the new domain (hence, domain expansion) and\nthe label function to be learned must work for the expanded domain.\nSpecifically, this paper presents a method for unsupervised multi-source domain\nexpansion (UMSDE) where only the pre-learned models of the source domains and\nunlabelled new domain data are available. We propose to use the predicted class\nprobability of the unlabelled data in the new domain produced by different\nsource models to jointly mitigate the biases among domains, exploit the\ndiscriminative information in the new domain, and preserve the performance in\nthe source domains. Experimental results on the VLCS, ImageCLEF_DA and PACS\ndatasets have verified the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 07:02:35 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Zhang", "Jing", ""], ["Li", "Wanqing", ""], ["sheng", "Lu", ""], ["Tang", "Chang", ""], ["Ogunbona", "Philip", ""]]}, {"id": "2005.12548", "submitter": "Marie-Morgane Paumard", "authors": "Marie-Morgane Paumard, David Picard, Hedi Tabia", "title": "Deepzzle: Solving Visual Jigsaw Puzzles with Deep Learning andShortest\n  Path Optimization", "comments": null, "journal-ref": "IEEE Transactions on Image Processing (2020)", "doi": "10.1109/TIP.2019.2963378", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the image reassembly problem with wide space between the fragments,\nin such a way that the patterns and colors continuity is mostly unusable. The\nspacing emulates the erosion of which the archaeological fragments suffer. We\ncrop-square the fragments borders to compel our algorithm to learn from the\ncontent of the fragments. We also complicate the image reassembly by removing\nfragments and adding pieces from other sources. We use a two-step method to\nobtain the reassemblies: 1) a neural network predicts the positions of the\nfragments despite the gaps between them; 2) a graph that leads to the best\nreassemblies is made from these predictions. In this paper, we notably\ninvestigate the effect of branch-cut in the graph of reassemblies. We also\nprovide a comparison with the literature, solve complex images reassemblies,\nexplore at length the dataset, and propose a new metric that suits its\nspecificities.\n  Keywords: image reassembly, jigsaw puzzle, deep learning, graph, branch-cut,\ncultural heritage\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 07:19:54 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Paumard", "Marie-Morgane", ""], ["Picard", "David", ""], ["Tabia", "Hedi", ""]]}, {"id": "2005.12551", "submitter": "Alexey Abramov", "authors": "Alexey Abramov, Christopher Bayer, Claudio Heller", "title": "Keep it Simple: Image Statistics Matching for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying an object detector, which is neither trained nor fine-tuned on data\nclose to the final application, often leads to a substantial performance drop.\nIn order to overcome this problem, it is necessary to consider a shift between\nsource and target domains. Tackling the shift is known as Domain Adaptation\n(DA). In this work, we focus on unsupervised DA: maintaining the detection\naccuracy across different data distributions, when only unlabeled images are\navailable of the target domain. Recent state-of-the-art methods try to reduce\nthe domain gap using an adversarial training strategy which increases the\nperformance but at the same time the complexity of the training procedure. In\ncontrast, we look at the problem from a new perspective and keep it simple by\nsolely matching image statistics between source and target domain. We propose\nto align either color histograms or mean and covariance of the source images\ntowards the target domain. Hence, DA is accomplished without architectural\nadd-ons and additional hyper-parameters. The benefit of the approaches is\ndemonstrated by evaluating different domain shift scenarios on public data\nsets. In comparison to recent methods, we achieve state-of-the-art performance\nusing a much simpler procedure for the training. Additionally, we show that\napplying our techniques significantly reduces the amount of synthetic data\nneeded to learn a general model and thus increases the value of simulation.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 07:32:09 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Abramov", "Alexey", ""], ["Bayer", "Christopher", ""], ["Heller", "Claudio", ""]]}, {"id": "2005.12563", "submitter": "Max Blendowski", "authors": "Max Blendowski and Mattias P. Heinrich", "title": "Learning to map between ferns with differentiable binary embedding\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/EiT7GQAj-T", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning methods are based on the repeated, expensive\napplication of convolutions with parameter-intensive weight matrices. In this\nwork, we present a novel concept that enables the application of differentiable\nrandom ferns in end-to-end networks. It can then be used as multiplication-free\nconvolutional layer alternative in deep network architectures. Our experiments\non the binary classification task of the TUPAC'16 challenge demonstrate\nimproved results over the state-of-the-art binary XNOR net and only slightly\nworse performance than its 2x more parameter intensive floating point CNN\ncounterpart.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 08:13:23 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Blendowski", "Max", ""], ["Heinrich", "Mattias P.", ""]]}, {"id": "2005.12573", "submitter": "Kazuma Kobayashi", "authors": "Kazuma Kobayashi, Ryuichiro Hataya, Yusuke Kurose, Amina Bolatkan,\n  Mototaka Miyake, Hirokazu Watanabe, Masamichi Takahashi, Jun Itami, Tatsuya\n  Harada, Ryuji Hamamoto", "title": "Learning Global and Local Features of Normal Brain Anatomy for\n  Unsupervised Abnormality Detection", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world clinical practice, overlooking unanticipated findings can\nresult in serious consequences. However, supervised learning, which is the\nfoundation for the current success of deep learning, only encourages models to\nidentify abnormalities that are defined in datasets in advance. Therefore,\nabnormality detection must be implemented in medical images that are not\nlimited to a specific disease category. In this study, we demonstrate an\nunsupervised learning framework for pixel-wise abnormality detection in brain\nmagnetic resonance imaging captured from a patient population with metastatic\nbrain tumor. Our concept is as follows: If an image reconstruction network can\nfaithfully reproduce the global features of normal anatomy, then the abnormal\nlesions in unseen images can be identified based on the local difference from\nthose reconstructed as normal by a discriminative network. Both networks are\ntrained on a dataset comprising only normal images without labels. In addition,\nwe devise a metric to evaluate the anatomical fidelity of the reconstructed\nimages and confirm that the overall detection performance is improved when the\nimage reconstruction network achieves a higher score. For evaluation,\nclinically significant abnormalities are comprehensively segmented. The results\nshow that the area under the receiver operating characteristics curve values\nfor metastatic brain tumors, extracranial metastatic tumors, postoperative\ncavities, and structural changes are 0.78, 0.61, 0.91, and 0.60, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 08:46:32 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 01:49:08 GMT"}, {"version": "v3", "created": "Sat, 8 May 2021 11:45:24 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kobayashi", "Kazuma", ""], ["Hataya", "Ryuichiro", ""], ["Kurose", "Yusuke", ""], ["Bolatkan", "Amina", ""], ["Miyake", "Mototaka", ""], ["Watanabe", "Hirokazu", ""], ["Takahashi", "Masamichi", ""], ["Itami", "Jun", ""], ["Harada", "Tatsuya", ""], ["Hamamoto", "Ryuji", ""]]}, {"id": "2005.12597", "submitter": "Taizhang Shang", "authors": "Taizhang Shang, Qiuju Dai, Shengchen Zhu, Tong Yang, Yandong Guo", "title": "Perceptual Extreme Super Resolution Network with Receptive Field Block", "comments": "CVPRW 2020 accepted oral, 8 pages,45 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptual Extreme Super-Resolution for single image is extremely difficult,\nbecause the texture details of different images vary greatly. To tackle this\ndifficulty, we develop a super resolution network with receptive field block\nbased on Enhanced SRGAN. We call our network RFB-ESRGAN. The key contributions\nare listed as follows. First, for the purpose of extracting multi-scale\ninformation and enhance the feature discriminability, we applied receptive\nfield block (RFB) to super resolution. RFB has achieved competitive results in\nobject detection and classification. Second, instead of using large convolution\nkernels in multi-scale receptive field block, several small kernels are used in\nRFB, which makes us be able to extract detailed features and reduce the\ncomputation complexity. Third, we alternately use different upsampling methods\nin the upsampling stage to reduce the high computation complexity and still\nremain satisfactory performance. Fourth, we use the ensemble of 10 models of\ndifferent iteration to improve the robustness of model and reduce the noise\nintroduced by each individual model. Our experimental results show the superior\nperformance of RFB-ESRGAN. According to the preliminary results of NTIRE 2020\nPerceptual Extreme Super-Resolution Challenge, our solution ranks first among\nall the participants.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 09:38:33 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Shang", "Taizhang", ""], ["Dai", "Qiuju", ""], ["Zhu", "Shengchen", ""], ["Yang", "Tong", ""], ["Guo", "Yandong", ""]]}, {"id": "2005.12633", "submitter": "Xuelin Qian", "authors": "Xuelin Qian, Wenxuan Wang, Li Zhang, Fangrui Zhu, Yanwei Fu, Tao\n  Xiang, Yu-Gang Jiang, Xiangyang Xue", "title": "Long-Term Cloth-Changing Person Re-identification", "comments": "ACCV 2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) aims to match a target person across camera\nviews at different locations and times. Existing Re-ID studies focus on the\nshort-term cloth-consistent setting, under which a person re-appears in\ndifferent camera views with the same outfit. A discriminative feature\nrepresentation learned by existing deep Re-ID models is thus dominated by the\nvisual appearance of clothing. In this work, we focus on a much more difficult\nyet practical setting where person matching is conducted over long-duration,\ne.g., over days and months and therefore inevitably under the new challenge of\nchanging clothes. This problem, termed Long-Term Cloth-Changing (LTCC) Re-ID is\nmuch understudied due to the lack of large scale datasets. The first\ncontribution of this work is a new LTCC dataset containing people captured over\na long period of time with frequent clothing changes. As a second contribution,\nwe propose a novel Re-ID method specifically designed to address the\ncloth-changing challenge. Specifically, we consider that under cloth-changes,\nsoft-biometrics such as body shape would be more reliable. We, therefore,\nintroduce a shape embedding module as well as a cloth-elimination\nshape-distillation module aiming to eliminate the now unreliable clothing\nappearance features and focus on the body shape information. Extensive\nexperiments show that superior performance is achieved by the proposed model on\nthe new LTCC dataset. The code and dataset will be available at\nhttps://naiq.github.io/LTCC_Perosn_ReID.html.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 11:27:21 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 05:37:11 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 05:47:26 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Qian", "Xuelin", ""], ["Wang", "Wenxuan", ""], ["Zhang", "Li", ""], ["Zhu", "Fangrui", ""], ["Fu", "Yanwei", ""], ["Xiang", "Tao", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""]]}, {"id": "2005.12661", "submitter": "Alessio Monti", "authors": "Alessio Monti, Alessia Bertugli, Simone Calderara, Rita Cucchiara", "title": "DAG-Net: Double Attentive Graph Neural Network for Trajectory\n  Forecasting", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding human motion behaviour is a critical task for several possible\napplications like self-driving cars or social robots, and in general for all\nthose settings where an autonomous agent has to navigate inside a human-centric\nenvironment. This is non-trivial because human motion is inherently\nmulti-modal: given a history of human motion paths, there are many plausible\nways by which people could move in the future. Additionally, people activities\nare often driven by goals, e.g. reaching particular locations or interacting\nwith the environment. We address the aforementioned aspects by proposing a new\nrecurrent generative model that considers both single agents' future goals and\ninteractions between different agents. The model exploits a double\nattention-based graph neural network to collect information about the mutual\ninfluences among different agents and to integrate it with data about agents'\npossible future objectives. Our proposal is general enough to be applied to\ndifferent scenarios: the model achieves state-of-the-art results in both urban\nenvironments and also in sports applications.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 12:34:20 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 10:50:08 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Monti", "Alessio", ""], ["Bertugli", "Alessia", ""], ["Calderara", "Simone", ""], ["Cucchiara", "Rita", ""]]}, {"id": "2005.12662", "submitter": "Zihao Wang", "authors": "Zihao Wang, Clair Vandersteen, Thomas Demarcy, Dan Gnansia, Charles\n  Raffaelli, Nicolas Guevara, Herv\\'e Delingette", "title": "A Deep Learning based Fast Signed Distance Map Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/b2N5ZuEouu", "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signed distance map (SDM) is a common representation of surfaces in medical\nimage analysis and machine learning. The computational complexity of SDM for 3D\nparametric shapes is often a bottleneck in many applications, thus limiting\ntheir interest. In this paper, we propose a learning based SDM generation\nneural network which is demonstrated on a tridimensional cochlea shape model\nparameterized by 4 shape parameters. The proposed SDM Neural Network generates\na cochlea signed distance map depending on four input parameters and we show\nthat the deep learning approach leads to a 60 fold improvement in the time of\ncomputation compared to more classical SDM generation methods. Therefore, the\nproposed approach achieves a good trade-off between accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 12:36:19 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Wang", "Zihao", ""], ["Vandersteen", "Clair", ""], ["Demarcy", "Thomas", ""], ["Gnansia", "Dan", ""], ["Raffaelli", "Charles", ""], ["Guevara", "Nicolas", ""], ["Delingette", "Herv\u00e9", ""]]}, {"id": "2005.12690", "submitter": "Mengqi Ji", "authors": "Mengqi Ji, Jinzhi Zhang, Qionghai Dai, Lu Fang", "title": "SurfaceNet+: An End-to-end 3D Neural Network for Very Sparse Multi-view\n  Stereopsis", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI), May 2020", "journal-ref": "2020, IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "doi": "10.1109/TPAMI.2020.2996798", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view stereopsis (MVS) tries to recover the 3D model from 2D images. As\nthe observations become sparser, the significant 3D information loss makes the\nMVS problem more challenging. Instead of only focusing on densely sampled\nconditions, we investigate sparse-MVS with large baseline angles since the\nsparser sensation is more practical and more cost-efficient. By investigating\nvarious observation sparsities, we show that the classical depth-fusion\npipeline becomes powerless for the case with a larger baseline angle that\nworsens the photo-consistency check. As another line of the solution, we\npresent SurfaceNet+, a volumetric method to handle the 'incompleteness' and the\n'inaccuracy' problems induced by a very sparse MVS setup. Specifically, the\nformer problem is handled by a novel volume-wise view selection approach. It\nowns superiority in selecting valid views while discarding invalid occluded\nviews by considering the geometric prior. Furthermore, the latter problem is\nhandled via a multi-scale strategy that consequently refines the recovered\ngeometry around the region with the repeating pattern. The experiments\ndemonstrate the tremendous performance gap between SurfaceNet+ and\nstate-of-the-art methods in terms of precision and recall. Under the extreme\nsparse-MVS settings in two datasets, where existing methods can only return\nvery few points, SurfaceNet+ still works as well as in the dense MVS setting.\nThe benchmark and the implementation are publicly available at\nhttps://github.com/mjiUST/SurfaceNet-plus.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 13:13:02 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Ji", "Mengqi", ""], ["Zhang", "Jinzhi", ""], ["Dai", "Qionghai", ""], ["Fang", "Lu", ""]]}, {"id": "2005.12692", "submitter": "Shizuo Kaji", "authors": "Shizuo Kaji, Takeki Sudo, Kazushi Ahara", "title": "Cubical Ripser: Software for computing persistent homology of image and\n  volume data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Cubical Ripser for computing persistent homology of image and\nvolume data (more precisely, weighted cubical complexes). To our best\nknowledge, Cubical Ripser is currently the fastest and the most\nmemory-efficient program for computing persistent homology of weighted cubical\ncomplexes. We demonstrate our software with an example of image analysis in\nwhich persistent homology and convolutional neural networks are successfully\ncombined. Our open-source implementation is available online.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 08:25:49 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 06:44:01 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Kaji", "Shizuo", ""], ["Sudo", "Takeki", ""], ["Ahara", "Kazushi", ""]]}, {"id": "2005.12734", "submitter": "Huy Hieu Pham", "authors": "Hieu H. Pham, Tung T. Le, Dat T. Ngo, Dat Q. Tran, Ha Q. Nguyen", "title": "Interpreting Chest X-rays via CNNs that Exploit Hierarchical Disease\n  Dependencies and Uncertainty Labels", "comments": "MIDL 2020 Accepted Short Paper. arXiv admin note: substantial text\n  overlap with arXiv:1911.06475", "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/4o1GLIIHlh", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chest X-rays (CXRs) is one of the views most commonly ordered by\nradiologists (NHS),which is critical for diagnosis of many different thoracic\ndiseases. Accurately detecting thepresence of multiple diseases from CXRs is\nstill a challenging task. We present a multi-labelclassification framework\nbased on deep convolutional neural networks (CNNs) for diagnos-ing the presence\nof 14 common thoracic diseases and observations. Specifically, we trained\nastrong set of CNNs that exploit dependencies among abnormality labels and used\nthe labelsmoothing regularization (LSR) for a better handling of uncertain\nsamples. Our deep net-works were trained on over 200,000 CXRs of the recently\nreleased CheXpert dataset (Irvinandal., 2019) and the final model, which was an\nensemble of the best performing networks,achieved a mean area under the curve\n(AUC) of 0.940 in predicting 5 selected pathologiesfrom the validation set. To\nthe best of our knowledge, this is the highest AUC score yetreported to date.\nMore importantly, the proposed method was also evaluated on an inde-pendent\ntest set of the CheXpert competition, containing 500 CXR studies annotated by\napanel of 5 experienced radiologists. The reported performance was on average\nbetter than2.6 out of 3 other individual radiologists with a mean AUC of 0.930,\nwhich had led to thecurrent state-of-the-art performance on the CheXpert test\nset.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 11:07:53 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Pham", "Hieu H.", ""], ["Le", "Tung T.", ""], ["Ngo", "Dat T.", ""], ["Tran", "Dat Q.", ""], ["Nguyen", "Ha Q.", ""]]}, {"id": "2005.12739", "submitter": "ByungSoo Ko", "authors": "Yang-Ho Ji, HeeJae Jun, Insik Kim, Jongtack Kim, Youngjoon Kim,\n  Byungsoo Ko, Hyong-Keun Kook, Jingeun Lee, Sangwon Lee, Sanghyuk Park", "title": "An Effective Pipeline for a Real-world Clothes Retrieval System", "comments": "2nd place solution on DeepFashion2 clothes retrieval challenge in\n  CVPR2020 workshop (CVFAD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an effective pipeline for clothes retrieval system\nwhich has sturdiness on large-scale real-world fashion data. Our proposed\nmethod consists of three components: detection, retrieval, and post-processing.\nWe firstly conduct a detection task for precise retrieval on target clothes,\nthen retrieve the corresponding items with the metric learning-based model. To\nimprove the retrieval robustness against noise and misleading bounding boxes,\nwe apply post-processing methods such as weighted boxes fusion and feature\nconcatenation. With the proposed methodology, we achieved 2nd place in the\nDeepFashion2 Clothes Retrieval 2020 challenge.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 14:08:49 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Ji", "Yang-Ho", ""], ["Jun", "HeeJae", ""], ["Kim", "Insik", ""], ["Kim", "Jongtack", ""], ["Kim", "Youngjoon", ""], ["Ko", "Byungsoo", ""], ["Kook", "Hyong-Keun", ""], ["Lee", "Jingeun", ""], ["Lee", "Sangwon", ""], ["Park", "Sanghyuk", ""]]}, {"id": "2005.12741", "submitter": "Mengmi Zhang", "authors": "Mengmi Zhang, Gabriel Kreiman", "title": "What am I Searching for: Zero-shot Target Identity Inference in Visual\n  Search", "comments": "this was a mistaken new submission and a pointer to arXiv:1807.11926", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we infer intentions from a person's actions? As an example problem, here\nwe consider how to decipher what a person is searching for by decoding their\neye movement behavior. We conducted two psychophysics experiments where we\nmonitored eye movements while subjects searched for a target object. We defined\nthe fixations falling on \\textit{non-target} objects as \"error fixations\".\nUsing those error fixations, we developed a model (InferNet) to infer what the\ntarget was. InferNet uses a pre-trained convolutional neural network to extract\nfeatures from the error fixations and computes a similarity map between the\nerror fixations and all locations across the search image. The model\nconsolidates the similarity maps across layers and integrates these maps across\nall error fixations. InferNet successfully identifies the subject's goal and\noutperforms competitive null models, even without any object-specific training\non the inference task.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 04:53:32 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 19:49:41 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Zhang", "Mengmi", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "2005.12770", "submitter": "Deepanway Ghosal", "authors": "Deepanway Ghosal, Maheshkumar H. Kolekar", "title": "Visual Interest Prediction with Attentive Multi-Task Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Visual interest & affect prediction is a very interesting area of research in\nthe area of computer vision. In this paper, we propose a transfer learning and\nattention mechanism based neural network model to predict visual interest &\naffective dimensions in digital photos. Learning the multi-dimensional affects\nis addressed through a multi-task learning framework. With various experiments\nwe show the effectiveness of the proposed approach. Evaluation of our model on\nthe benchmark dataset shows large improvement over current state-of-the-art\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 14:49:34 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 10:05:58 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Ghosal", "Deepanway", ""], ["Kolekar", "Maheshkumar H.", ""]]}, {"id": "2005.12810", "submitter": "Shiyu Duan", "authors": "Shiyu Duan, Huaijin Chen, Jinwei Gu", "title": "JPAD-SE: High-Level Semantics for Joint Perception-Accuracy-Distortion\n  Enhancement in Image Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While humans can effortlessly transform complex visual scenes into simple\nwords and the other way around by leveraging their high-level understanding of\nthe content, conventional or the more recent learned image compression codecs\ndo not seem to utilize the semantic meanings of visual content to its full\npotential. Moreover, they focus mostly on rate-distortion and tend to\nunderperform in perception quality especially in low bitrate regime, and often\ndisregard the performance of downstream computer vision algorithms, which is a\nfast-growing consumer group of compressed images in addition to human viewers.\nIn this paper, we (1) present a generic framework that can enable any image\ncodec to leverage high-level semantics, and (2) study the joint optimization of\nperception quality, accuracy of downstream computer vision task, and\ndistortion. Our idea is that given any codec, we utilize high-level semantics\nto augment the low-level visual features extracted by it and produce\nessentially a new, semantic-aware codec. And we argue that semantic enhancement\nimplicitly optimizes rate-perception-accuracy-distortion (R-PAD) performance.\nTo validate our claim, we perform extensive empirical evaluations and provide\nboth quantitative and qualitative results.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 20:46:38 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 20:35:01 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Duan", "Shiyu", ""], ["Chen", "Huaijin", ""], ["Gu", "Jinwei", ""]]}, {"id": "2005.12813", "submitter": "Philipp Foehn", "authors": "Philipp Foehn, Dario Brescianini, Elia Kaufmann, Titus Cieslewski,\n  Mathias Gehrig, Manasi Muglikar and Davide Scaramuzza", "title": "AlphaPilot: Autonomous Drone Racing", "comments": "Accepted at Robotics: Science and Systems 2020, associated video at\n  https://youtu.be/DGjwm5PZQT8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel system for autonomous, vision-based drone racing\ncombining learned data abstraction, nonlinear filtering, and time-optimal\ntrajectory planning. The system has successfully been deployed at the first\nautonomous drone racing world championship: the 2019 AlphaPilot Challenge.\nContrary to traditional drone racing systems, which only detect the next gate,\nour approach makes use of any visible gate and takes advantage of multiple,\nsimultaneous gate detections to compensate for drift in the state estimate and\nbuild a global map of the gates. The global map and drift-compensated state\nestimate allow the drone to navigate through the race course even when the\ngates are not immediately visible and further enable to plan a near\ntime-optimal path through the race course in real time based on approximate\ndrone dynamics. The proposed system has been demonstrated to successfully guide\nthe drone through tight race courses reaching speeds up to 8m/s and ranked\nsecond at the 2019 AlphaPilot Challenge.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 15:45:05 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Foehn", "Philipp", ""], ["Brescianini", "Dario", ""], ["Kaufmann", "Elia", ""], ["Cieslewski", "Titus", ""], ["Gehrig", "Mathias", ""], ["Muglikar", "Manasi", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2005.12815", "submitter": "Diego Aghi", "authors": "Diego Aghi, Vittorio Mazzia, Marcello Chiaberge", "title": "Local Motion Planner for Autonomous Navigation in Vineyards with a RGB-D\n  Camera-Based Algorithm and Deep Learning Synergy", "comments": null, "journal-ref": null, "doi": "10.3390/machines8020027", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advent of agriculture 3.0 and 4.0, researchers are increasingly\nfocusing on the development of innovative smart farming and precision\nagriculture technologies by introducing automation and robotics into the\nagricultural processes. Autonomous agricultural field machines have been\ngaining significant attention from farmers and industries to reduce costs,\nhuman workload, and required resources. Nevertheless, achieving sufficient\nautonomous navigation capabilities requires the simultaneous cooperation of\ndifferent processes; localization, mapping, and path planning are just some of\nthe steps that aim at providing to the machine the right set of skills to\noperate in semi-structured and unstructured environments. In this context, this\nstudy presents a low-cost local motion planner for autonomous navigation in\nvineyards based only on an RGB-D camera, low range hardware, and a dual layer\ncontrol algorithm. The first algorithm exploits the disparity map and its depth\nrepresentation to generate a proportional control for the robotic platform.\nConcurrently, a second back-up algorithm, based on representations learning and\nresilient to illumination variations, can take control of the machine in case\nof a momentaneous failure of the first block. Moreover, due to the double\nnature of the system, after initial training of the deep learning model with an\ninitial dataset, the strict synergy between the two algorithms opens the\npossibility of exploiting new automatically labeled data, coming from the\nfield, to extend the existing model knowledge. The machine learning algorithm\nhas been trained and tested, using transfer learning, with acquired images\nduring different field surveys in the North region of Italy and then optimized\nfor on-device inference with model pruning and quantization. Finally, the\noverall system has been validated with a customized robot platform in the\nrelevant environment.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 15:47:42 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Aghi", "Diego", ""], ["Mazzia", "Vittorio", ""], ["Chiaberge", "Marcello", ""]]}, {"id": "2005.12855", "submitter": "Alexander Wong", "authors": "Alexander Wong, Zhong Qiu Lin, Linda Wang, Audrey G. Chung, Beiyi\n  Shen, Almas Abbasi, Mahsa Hoshmand-Kochi, and Timothy Q. Duong", "title": "COVID-Net S: Towards computer-aided severity assessment via training and\n  validation of deep neural networks for geographic extent and opacity extent\n  scoring of chest X-rays for SARS-CoV-2 lung disease severity", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: A critical step in effective care and treatment planning for\nsevere acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the cause of the\nCOVID-19 pandemic, is the assessment of the severity of disease progression.\nChest x-rays (CXRs) are often used to assess SARS-CoV-2 severity, with two\nimportant assessment metrics being extent of lung involvement and degree of\nopacity. In this proof-of-concept study, we assess the feasibility of\ncomputer-aided scoring of CXRs of SARS-CoV-2 lung disease severity using a deep\nlearning system.\n  Materials and Methods: Data consisted of 396 CXRs from SARS-CoV-2 positive\npatient cases. Geographic extent and opacity extent were scored by two\nboard-certified expert chest radiologists (with 20+ years of experience) and a\n2nd-year radiology resident. The deep neural networks used in this study, which\nwe name COVID-Net S, are based on a COVID-Net network architecture. 100\nversions of the network were independently learned (50 to perform geographic\nextent scoring and 50 to perform opacity extent scoring) using random subsets\nof CXRs from the study, and we evaluated the networks using stratified Monte\nCarlo cross-validation experiments.\n  Findings: The COVID-Net S deep neural networks yielded R$^2$ of 0.664 $\\pm$\n0.032 and 0.635 $\\pm$ 0.044 between predicted scores and radiologist scores for\ngeographic extent and opacity extent, respectively, in stratified Monte Carlo\ncross-validation experiments. The best performing networks achieved R$^2$ of\n0.739 and 0.741 between predicted scores and radiologist scores for geographic\nextent and opacity extent, respectively.\n  Interpretation: The results are promising and suggest that the use of deep\nneural networks on CXRs could be an effective tool for computer-aided\nassessment of SARS-CoV-2 lung disease severity, although additional studies are\nneeded before adoption for routine clinical use.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 16:33:52 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 19:32:33 GMT"}, {"version": "v3", "created": "Sat, 22 Aug 2020 04:57:32 GMT"}, {"version": "v4", "created": "Fri, 16 Apr 2021 13:48:28 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Wong", "Alexander", ""], ["Lin", "Zhong Qiu", ""], ["Wang", "Linda", ""], ["Chung", "Audrey G.", ""], ["Shen", "Beiyi", ""], ["Abbasi", "Almas", ""], ["Hoshmand-Kochi", "Mahsa", ""], ["Duong", "Timothy Q.", ""]]}, {"id": "2005.12872", "submitter": "Nicolas Carion", "authors": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier,\n  Alexander Kirillov, Sergey Zagoruyko", "title": "End-to-End Object Detection with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a new method that views object detection as a direct set\nprediction problem. Our approach streamlines the detection pipeline,\neffectively removing the need for many hand-designed components like a\nnon-maximum suppression procedure or anchor generation that explicitly encode\nour prior knowledge about the task. The main ingredients of the new framework,\ncalled DEtection TRansformer or DETR, are a set-based global loss that forces\nunique predictions via bipartite matching, and a transformer encoder-decoder\narchitecture. Given a fixed small set of learned object queries, DETR reasons\nabout the relations of the objects and the global image context to directly\noutput the final set of predictions in parallel. The new model is conceptually\nsimple and does not require a specialized library, unlike many other modern\ndetectors. DETR demonstrates accuracy and run-time performance on par with the\nwell-established and highly-optimized Faster RCNN baseline on the challenging\nCOCO object detection dataset. Moreover, DETR can be easily generalized to\nproduce panoptic segmentation in a unified manner. We show that it\nsignificantly outperforms competitive baselines. Training code and pretrained\nmodels are available at https://github.com/facebookresearch/detr.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 17:06:38 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 13:57:30 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 17:37:23 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Carion", "Nicolas", ""], ["Massa", "Francisco", ""], ["Synnaeve", "Gabriel", ""], ["Usunier", "Nicolas", ""], ["Kirillov", "Alexander", ""], ["Zagoruyko", "Sergey", ""]]}, {"id": "2005.12880", "submitter": "Hong Siyu", "authors": "Siyu Hong, Kunhong Li, Yongcong Zhang, Zhiheng Fu, Mengyi Liu and\n  Yulan Guo", "title": "Learning Local Features with Context Aggregation for Visual Localization", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keypoint detection and description is fundamental yet important in many\nvision applications. Most existing methods use detect-then-describe or\ndetect-and-describe strategy to learn local features without considering their\ncontext information. Consequently, it is challenging for these methods to learn\nrobust local features. In this paper, we focus on the fusion of low-level\ntextual information and high-level semantic context information to improve the\ndiscrimitiveness of local features. Specifically, we first estimate a score map\nto represent the distribution of potential keypoints according to the quality\nof descriptors of all pixels. Then, we extract and aggregate multi-scale\nhigh-level semantic features based by the guidance of the score map. Finally,\nthe low-level local features and high-level semantic features are fused and\nrefined using a residual module. Experiments on the challenging local feature\nbenchmark dataset demonstrate that our method achieves the state-of-the-art\nperformance in the local feature challenge of the visual localization\nbenchmark.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 17:19:06 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 16:55:28 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hong", "Siyu", ""], ["Li", "Kunhong", ""], ["Zhang", "Yongcong", ""], ["Fu", "Zhiheng", ""], ["Liu", "Mengyi", ""], ["Guo", "Yulan", ""]]}, {"id": "2005.12892", "submitter": "Munender Varshney Mr.", "authors": "Rajat, Munender Varshney, Pravendra Singh, Vinay P. Namboodiri", "title": "Minimizing Supervision in Multi-label Categorization", "comments": "Accepted in CVPR-W 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple categories of objects are present in most images. Treating this as a\nmulti-class classification is not justified. We treat this as a multi-label\nclassification problem. In this paper, we further aim to minimize the\nsupervision required for providing supervision in multi-label classification.\nSpecifically, we investigate an effective class of approaches that associate a\nweak localization with each category either in terms of the bounding box or\nsegmentation mask. Doing so improves the accuracy of multi-label\ncategorization. The approach we adopt is one of active learning, i.e.,\nincrementally selecting a set of samples that need supervision based on the\ncurrent model, obtaining supervision for these samples, retraining the model\nwith the additional set of supervised samples and proceeding again to select\nthe next set of samples. A crucial concern is the choice of the set of samples.\nIn doing so, we provide a novel insight, and no specific measure succeeds in\nobtaining a consistently improved selection criterion. We, therefore, provide a\nselection criterion that consistently improves the overall baseline criterion\nby choosing the top k set of samples for a varied set of criteria. Using this\ncriterion, we are able to show that we can retain more than 98% of the fully\nsupervised performance with just 20% of samples (and more than 96% using 10%)\nof the dataset on PASCAL VOC 2007 and 2012. Also, our proposed approach\nconsistently outperforms all other baseline metrics for all benchmark datasets\nand model combinations.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 17:35:47 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Rajat", "", ""], ["Varshney", "Munender", ""], ["Singh", "Pravendra", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2005.12945", "submitter": "Jie Feng", "authors": "XiangJi Wu, Ziwen Zhang, Jie Feng, Lei Zhou, Junmin Wu", "title": "End-to-end Optimized Video Compression with MV-Residual Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end trainable framework for P-frame compression in this\npaper. A joint motion vector (MV) and residual prediction network MV-Residual\nis designed to extract the ensembled features of motion representations and\nresidual information by treating the two successive frames as inputs. The prior\nprobability of the latent representations is modeled by a hyperprior\nautoencoder and trained jointly with the MV-Residual network. Specially, the\nspatially-displaced convolution is applied for video frame prediction, in which\na motion kernel for each pixel is learned to generate predicted pixel by\napplying the kernel at a displaced location in the source image. Finally, novel\nrate allocation and post-processing strategies are used to produce the final\ncompressed bits, considering the bits constraint of the challenge. The\nexperimental results on validation set show that the proposed optimized\nframework can generate the highest MS-SSIM for P-frame compression competition.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 18:09:59 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Wu", "XiangJi", ""], ["Zhang", "Ziwen", ""], ["Feng", "Jie", ""], ["Zhou", "Lei", ""], ["Wu", "Junmin", ""]]}, {"id": "2005.12951", "submitter": "Karan Ahuja", "authors": "Karan Ahuja, Abhishek Bose, Mohit Jain, Kuntal Dey, Anil Joshi,\n  Krishnaveni Achary, Blessin Varkey, Chris Harrison and Mayank Goel", "title": "Gaze-based Autism Detection for Adolescents and Young Adults using\n  Prosaic Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism often remains undiagnosed in adolescents and adults. Prior research\nhas indicated that an autistic individual often shows atypical fixation and\ngaze patterns. In this short paper, we demonstrate that by monitoring a user's\ngaze as they watch commonplace (i.e., not specialized, structured or coded)\nvideo, we can identify individuals with autism spectrum disorder. We recruited\n35 autistic and 25 non-autistic individuals, and captured their gaze using an\noff-the-shelf eye tracker connected to a laptop. Within 15 seconds, our\napproach was 92.5% accurate at identifying individuals with an autism\ndiagnosis. We envision such automatic detection being applied during e.g., the\nconsumption of web media, which could allow for passive screening and\nadaptation of user interfaces.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 18:14:31 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Ahuja", "Karan", ""], ["Bose", "Abhishek", ""], ["Jain", "Mohit", ""], ["Dey", "Kuntal", ""], ["Joshi", "Anil", ""], ["Achary", "Krishnaveni", ""], ["Varkey", "Blessin", ""], ["Harrison", "Chris", ""], ["Goel", "Mayank", ""]]}, {"id": "2005.12977", "submitter": "Laure Pr\\'etet", "authors": "Laure Pr\\'etet, Ga\\\"el Richard, Geoffroy Peeters", "title": "Learning to rank music tracks using triplet loss", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053135", "report-no": null, "categories": "cs.IR cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most music streaming services rely on automatic recommendation algorithms to\nexploit their large music catalogs. These algorithms aim at retrieving a ranked\nlist of music tracks based on their similarity with a target music track. In\nthis work, we propose a method for direct recommendation based on the audio\ncontent without explicitly tagging the music tracks. To that aim, we propose\nseveral strategies to perform triplet mining from ranked lists. We train a\nConvolutional Neural Network to learn the similarity via triplet loss. These\ndifferent strategies are compared and validated on a large-scale experiment\nagainst an auto-tagging based approach. The results obtained highlight the\nefficiency of our system, especially when associated with an Auto-pooling\nlayer.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 08:20:54 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Pr\u00e9tet", "Laure", ""], ["Richard", "Ga\u00ebl", ""], ["Peeters", "Geoffroy", ""]]}, {"id": "2005.12991", "submitter": "Dawid Rymarczyk", "authors": "Dawid Rymarczyk and Adriana Borowa and Jacek Tabor and Bartosz\n  Zieli\\'nski", "title": "Kernel Self-Attention in Deep Multiple Instance Learning", "comments": "https://openaccess.thecvf.com/content/WACV2021/papers/Rymarczyk_Kernel_Self-Attention_for_Weakly-Supervised_Image_Classification_Using_Deep_Multiple_Instance_WACV_2021_paper.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not all supervised learning problems are described by a pair of a fixed-size\ninput tensor and a label. In some cases, especially in medical image analysis,\na label corresponds to a bag of instances (e.g. image patches), and to classify\nsuch bag, aggregation of information from all of the instances is needed. There\nhave been several attempts to create a model working with a bag of instances,\nhowever, they are assuming that there are no dependencies within the bag and\nthe label is connected to at least one instance. In this work, we introduce\nSelf-Attention Attention-based MIL Pooling (SA-AbMILP) aggregation operation to\naccount for the dependencies between instances. We conduct several experiments\non MNIST, histological, microbiological, and retinal databases to show that\nSA-AbMILP performs better than other models. Additionally, we investigate\nkernel variations of Self-Attention and their influence on the results.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 14:59:13 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 12:36:50 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Rymarczyk", "Dawid", ""], ["Borowa", "Adriana", ""], ["Tabor", "Jacek", ""], ["Zieli\u0144ski", "Bartosz", ""]]}, {"id": "2005.13037", "submitter": "Naveen Sai Madiraju", "authors": "Naveen Madiraju, Homa Karimabadi", "title": "Instance Explainable Temporal Network For Multivariate Timeseries", "comments": "7 pages, 7 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep networks have been widely adopted, one of their shortcomings\nhas been their blackbox nature. One particularly difficult problem in machine\nlearning is multivariate time series (MVTS) classification. MVTS data arise in\nmany applications and are becoming ever more pervasive due to explosive growth\nof sensors and IoT devices. Here, we propose a novel network (IETNet) that\nidentifies the important channels in the classification decision for each\ninstance of inference. This feature also enables identification and removal of\nnon-predictive variables which would otherwise lead to overfit and/or\ninaccurate model. IETNet is an end-to-end network that combines temporal\nfeature extraction, variable selection, and joint variable interaction into a\nsingle learning framework. IETNet utilizes an 1D convolutions for temporal\nfeatures, a novel channel gate layer for variable-class assignment using an\nattention layer to perform cross channel reasoning and perform classification\nobjective. To gain insight into the learned temporal features and channels, we\nextract region of interest attention map along both time and channels. The\nviability of this network is demonstrated through a multivariate time series\ndata from N body simulations and spacecraft sensor data.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 20:55:24 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 22:56:10 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Madiraju", "Naveen", ""], ["Karimabadi", "Homa", ""]]}, {"id": "2005.13039", "submitter": "Shreyank N Gowda", "authors": "Shreyank N Gowda, Panagiotis Eustratiadis, Timothy Hospedales, Laura\n  Sevilla-Lara", "title": "ALBA : Reinforcement Learning for Video Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the challenging problem of zero-shot video object segmentation\n(VOS). That is, segmenting and tracking multiple moving objects within a video\nfully automatically, without any manual initialization. We treat this as a\ngrouping problem by exploiting object proposals and making a joint inference\nabout grouping over both space and time. We propose a network architecture for\ntractably performing proposal selection and joint grouping. Crucially, we then\nshow how to train this network with reinforcement learning so that it learns to\nperform the optimal non-myopic sequence of grouping decisions to segment the\nwhole video. Unlike standard supervised techniques, this also enables us to\ndirectly optimize for the non-differentiable overlap-based metrics used to\nevaluate VOS. We show that the proposed method, which we call ALBA outperforms\nthe previous stateof-the-art on three benchmarks: DAVIS 2017 [2], FBMS [20] and\nYoutube-VOS [27].\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 20:57:28 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 07:09:53 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Gowda", "Shreyank N", ""], ["Eustratiadis", "Panagiotis", ""], ["Hospedales", "Timothy", ""], ["Sevilla-Lara", "Laura", ""]]}, {"id": "2005.13044", "submitter": "Lei Kang", "authors": "Lei Kang, Pau Riba, Mar\\c{c}al Rusi\\~nol, Alicia Forn\\'es, Mauricio\n  Villegas", "title": "Pay Attention to What You Read: Non-recurrent Handwritten Text-Line\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The advent of recurrent neural networks for handwriting recognition marked an\nimportant milestone reaching impressive recognition accuracies despite the\ngreat variability that we observe across different writing styles. Sequential\narchitectures are a perfect fit to model text lines, not only because of the\ninherent temporal aspect of text, but also to learn probability distributions\nover sequences of characters and words. However, using such recurrent paradigms\ncomes at a cost at training stage, since their sequential pipelines prevent\nparallelization. In this work, we introduce a non-recurrent approach to\nrecognize handwritten text by the use of transformer models. We propose a novel\nmethod that bypasses any recurrence. By using multi-head self-attention layers\nboth at the visual and textual stages, we are able to tackle character\nrecognition as well as to learn language-related dependencies of the character\nsequences to be decoded. Our model is unconstrained to any predefined\nvocabulary, being able to recognize out-of-vocabulary words, i.e. words that do\nnot appear in the training vocabulary. We significantly advance over prior art\nand demonstrate that satisfactory recognition accuracies are yielded even in\nfew-shot learning scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 21:15:20 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Kang", "Lei", ""], ["Riba", "Pau", ""], ["Rusi\u00f1ol", "Mar\u00e7al", ""], ["Forn\u00e9s", "Alicia", ""], ["Villegas", "Mauricio", ""]]}, {"id": "2005.13053", "submitter": "Rihuan Ke", "authors": "Rihuan Ke, Aur\\'elie Bugeau, Nicolas Papadakis, Mark Kirkland, Peter\n  Schuetz, Carola-Bibiane Sch\\\"onlieb", "title": "Multi-task deep learning for image segmentation using recursive\n  approximation tasks", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3062726", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully supervised deep neural networks for segmentation usually require a\nmassive amount of pixel-level labels which are manually expensive to create. In\nthis work, we develop a multi-task learning method to relax this constraint. We\nregard the segmentation problem as a sequence of approximation subproblems that\nare recursively defined and in increasing levels of approximation accuracy. The\nsubproblems are handled by a framework that consists of 1) a segmentation task\nthat learns from pixel-level ground truth segmentation masks of a small\nfraction of the images, 2) a recursive approximation task that conducts partial\nobject regions learning and data-driven mask evolution starting from partial\nmasks of each object instance, and 3) other problem oriented auxiliary tasks\nthat are trained with sparse annotations and promote the learning of dedicated\nfeatures. Most training images are only labeled by (rough) partial masks, which\ndo not contain exact object boundaries, rather than by their full segmentation\nmasks. During the training phase, the approximation task learns the statistics\nof these partial masks, and the partial regions are recursively increased\ntowards object boundaries aided by the learned information from the\nsegmentation task in a fully data-driven fashion. The network is trained on an\nextremely small amount of precisely segmented images and a large set of coarse\nlabels. Annotations can thus be obtained in a cheap way. We demonstrate the\nefficiency of our approach in three applications with microscopy images and\nultrasound images.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 21:35:26 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ke", "Rihuan", ""], ["Bugeau", "Aur\u00e9lie", ""], ["Papadakis", "Nicolas", ""], ["Kirkland", "Mark", ""], ["Schuetz", "Peter", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "2005.13061", "submitter": "Zeynel Samak Mr", "authors": "Zeynel A. Samak, Philip Clatworthy and Majid Mirmehdi", "title": "Prediction of Thrombectomy Functional Outcomes using Multimodal Data", "comments": "Accepted at Medical Image Understanding and Analysis (MIUA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent randomised clinical trials have shown that patients with ischaemic\nstroke {due to occlusion of a large intracranial blood vessel} benefit from\nendovascular thrombectomy. However, predicting outcome of treatment in an\nindividual patient remains a challenge. We propose a novel deep learning\napproach to directly exploit multimodal data (clinical metadata information,\nimaging data, and imaging biomarkers extracted from images) to estimate the\nsuccess of endovascular treatment. We incorporate an attention mechanism in our\narchitecture to model global feature inter-dependencies, both channel-wise and\nspatially. We perform comparative experiments using unimodal and multimodal\ndata, to predict functional outcome (modified Rankin Scale score, mRS) and\nachieve 0.75 AUC for dichotomised mRS scores and 0.35 classification accuracy\nfor individual mRS scores.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 21:51:58 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 14:39:46 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Samak", "Zeynel A.", ""], ["Clatworthy", "Philip", ""], ["Mirmehdi", "Majid", ""]]}, {"id": "2005.13099", "submitter": "Sahib Singh", "authors": "Sahib Singh, Harshvardhan Sikka, Sasikanth Kotti, Andrew Trask", "title": "Benchmarking Differentially Private Residual Networks for Medical\n  Imagery", "comments": "5 Pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we measure the effectiveness of $\\epsilon$-Differential Privacy\n(DP) when applied to medical imaging. We compare two robust differential\nprivacy mechanisms: Local-DP and DP-SGD and benchmark their performance when\nanalyzing medical imagery records. We analyze the trade-off between the model's\naccuracy and the level of privacy it guarantees, and also take a closer look to\nevaluate how useful these theoretical privacy guarantees actually prove to be\nin the real world medical setting.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 00:29:56 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 09:47:09 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 03:15:27 GMT"}, {"version": "v4", "created": "Sun, 19 Jul 2020 03:27:18 GMT"}, {"version": "v5", "created": "Sat, 5 Sep 2020 02:25:06 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Singh", "Sahib", ""], ["Sikka", "Harshvardhan", ""], ["Kotti", "Sasikanth", ""], ["Trask", "Andrew", ""]]}, {"id": "2005.13102", "submitter": "Bangalore Ravi Kiran", "authors": "Leonardo Gigli, B Ravi Kiran, Thomas Paul, Andres Serna, Nagarjuna\n  Vemuri, Beatriz Marcotegui, Santiago Velasco-Forero", "title": "Road Segmentation on low resolution Lidar point clouds for autonomous\n  vehicles", "comments": "ISPRS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud datasets for perception tasks in the context of autonomous\ndriving often rely on high resolution 64-layer Light Detection and Ranging\n(LIDAR) scanners. They are expensive to deploy on real-world autonomous driving\nsensor architectures which usually employ 16/32 layer LIDARs. We evaluate the\neffect of subsampling image based representations of dense point clouds on the\naccuracy of the road segmentation task. In our experiments the low resolution\n16/32 layer LIDAR point clouds are simulated by subsampling the original 64\nlayer data, for subsequent transformation in to a feature map in the\nBird-Eye-View (BEV) and SphericalView (SV) representations of the point cloud.\nWe introduce the usage of the local normal vector with the LIDAR's spherical\ncoordinates as an input channel to existing LoDNN architectures. We demonstrate\nthat this local normal feature in conjunction with classical features not only\nimproves performance for binary road segmentation on full resolution point\nclouds, but it also reduces the negative impact on the accuracy when\nsubsampling dense point clouds as compared to the usage of classical features\nalone. We assess our method with several experiments on two datasets: KITTI\nRoad-segmentation benchmark and the recently released Semantic KITTI dataset.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 00:38:39 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Gigli", "Leonardo", ""], ["Kiran", "B Ravi", ""], ["Paul", "Thomas", ""], ["Serna", "Andres", ""], ["Vemuri", "Nagarjuna", ""], ["Marcotegui", "Beatriz", ""], ["Velasco-Forero", "Santiago", ""]]}, {"id": "2005.13110", "submitter": "Clifford Broni-Bediako", "authors": "Clifford Broni-Bediako, Yuki Murata, Luiz Henrique Mormille and\n  Masayasu Atsumi", "title": "Evolutionary NAS with Gene Expression Programming of Cellular Encoding", "comments": "Accepted at IEEE SSCI 2020 (7 pages, 3 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The renaissance of neural architecture search (NAS) has seen classical\nmethods such as genetic algorithms (GA) and genetic programming (GP) being\nexploited for convolutional neural network (CNN) architectures. While recent\nwork have achieved promising performance on visual perception tasks, the direct\nencoding scheme of both GA and GP has functional complexity deficiency and does\nnot scale well on large architectures like CNN. To address this, we present a\nnew generative encoding scheme -- $symbolic\\ linear\\ generative\\ encoding$\n(SLGE) -- simple, yet powerful scheme which embeds local graph transformations\nin chromosomes of linear fixed-length string to develop CNN architectures of\nvariant shapes and sizes via evolutionary process of gene expression\nprogramming. In experiments, the effectiveness of SLGE is shown in discovering\narchitectures that improve the performance of the state-of-the-art handcrafted\nCNN architectures on CIFAR-10 and CIFAR-100 image classification tasks; and\nachieves a competitive classification error rate with the existing NAS methods\nusing less GPU resources.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 01:19:32 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 15:41:20 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Broni-Bediako", "Clifford", ""], ["Murata", "Yuki", ""], ["Mormille", "Luiz Henrique", ""], ["Atsumi", "Masayasu", ""]]}, {"id": "2005.13116", "submitter": "Zhanzhan Cheng", "authors": "Jing Lu, Baorui Zou, Zhanzhan Cheng, Shiliang Pu, Shuigeng Zhou, Yi\n  Niu, Fei Wu", "title": "Object-QA: Towards High Reliable Object Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In object recognition applications, object images usually appear with\ndifferent quality levels. Practically, it is very important to indicate object\nimage qualities for better application performance, e.g. filtering out\nlow-quality object image frames to maintain robust video object recognition\nresults and speed up inference. However, no previous works are explicitly\nproposed for addressing the problem. In this paper, we define the problem of\nobject quality assessment for the first time and propose an effective approach\nnamed Object-QA to assess high-reliable quality scores for object images.\nConcretely, Object-QA first employs a well-designed relative quality assessing\nmodule that learns the intra-class-level quality scores by referring to the\ndifference between object images and their estimated templates. Then an\nabsolute quality assessing module is designed to generate the final quality\nscores by aligning the quality score distributions in inter-class. Besides,\nObject-QA can be implemented with only object-level annotations, and is also\neasily deployed to a variety of object recognition tasks. To our best knowledge\nthis is the first work to put forward the definition of this problem and\nconduct quantitative evaluations. Validations on 5 different datasets show that\nObject-QA can not only assess high-reliable quality scores according with human\ncognition, but also improve application performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 01:46:58 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Lu", "Jing", ""], ["Zou", "Baorui", ""], ["Cheng", "Zhanzhan", ""], ["Pu", "Shiliang", ""], ["Zhou", "Shuigeng", ""], ["Niu", "Yi", ""], ["Wu", "Fei", ""]]}, {"id": "2005.13117", "submitter": "Zhanzhan Cheng", "authors": "Chengwei Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Yi Niu, Fei Wu\n  and Futai Zou", "title": "SPIN: Structure-Preserving Inner Offset Network for Scene Text\n  Recognition", "comments": "Accepted to AAAI21. The code will be published soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Arbitrary text appearance poses a great challenge in scene text recognition\ntasks. Existing works mostly handle with the problem in consideration of the\nshape distortion, including perspective distortions, line curvature or other\nstyle variations. Therefore, methods based on spatial transformers are\nextensively studied. However, chromatic difficulties in complex scenes have not\nbeen paid much attention on. In this work, we introduce a new learnable\ngeometric-unrelated module, the Structure-Preserving Inner Offset Network\n(SPIN), which allows the color manipulation of source data within the network.\nThis differentiable module can be inserted before any recognition architecture\nto ease the downstream tasks, giving neural networks the ability to actively\ntransform input intensity rather than the existing spatial rectification. It\ncan also serve as a complementary module to known spatial transformations and\nwork in both independent and collaborative ways with them. Extensive\nexperiments show that the use of SPIN results in a significant improvement on\nmultiple text recognition benchmarks compared to the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 01:47:07 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 12:01:18 GMT"}, {"version": "v3", "created": "Fri, 25 Dec 2020 01:50:21 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhang", "Chengwei", ""], ["Xu", "Yunlu", ""], ["Cheng", "Zhanzhan", ""], ["Pu", "Shiliang", ""], ["Niu", "Yi", ""], ["Wu", "Fei", ""], ["Zou", "Futai", ""]]}, {"id": "2005.13118", "submitter": "Zhanzhan Cheng", "authors": "Peng Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Jing Lu, Liang\n  Qiao, Yi Niu, and Fei Wu", "title": "TRIE: End-to-End Text Reading and Information Extraction for Document\n  Understanding", "comments": "Accepted to ACM MM2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Since real-world ubiquitous documents (e.g., invoices, tickets, resumes and\nleaflets) contain rich information, automatic document image understanding has\nbecome a hot topic. Most existing works decouple the problem into two separate\ntasks, (1) text reading for detecting and recognizing texts in images and (2)\ninformation extraction for analyzing and extracting key elements from\npreviously extracted plain text. However, they mainly focus on improving\ninformation extraction task, while neglecting the fact that text reading and\ninformation extraction are mutually correlated. In this paper, we propose a\nunified end-to-end text reading and information extraction network, where the\ntwo tasks can reinforce each other. Specifically, the multimodal visual and\ntextual features of text reading are fused for information extraction and in\nturn, the semantics in information extraction contribute to the optimization of\ntext reading. On three real-world datasets with diverse document images (from\nfixed layout to variable layout, from structured text to semi-structured text),\nour proposed method significantly outperforms the state-of-the-art methods in\nboth efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 01:47:26 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 11:16:55 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Zhang", "Peng", ""], ["Xu", "Yunlu", ""], ["Cheng", "Zhanzhan", ""], ["Pu", "Shiliang", ""], ["Lu", "Jing", ""], ["Qiao", "Liang", ""], ["Niu", "Yi", ""], ["Wu", "Fei", ""]]}, {"id": "2005.13127", "submitter": "Xiaoying Ding", "authors": "Xiaoying Ding and Zhenzhong Chen", "title": "Towards Mesh Saliency Detection in 6 Degrees of Freedom", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional 3D mesh saliency detection algorithms and corresponding databases\nwere proposed under several constraints such as providing limited viewing\ndirections and not taking the subject's movement into consideration. In this\nwork, a novel 6DoF mesh saliency database is developed which provides both the\nsubject's 6DoF data and eye-movement data. Different from traditional\ndatabases, subjects in the experiment are allowed to move freely to observe 3D\nmeshes in a virtual reality environment. Based on the database, we first\nanalyze the inter-observer variation and the influence of viewing direction\ntowards subject's visual attention, then we provide further investigations\nabout the subject's visual attention bias during observation. Furthermore, we\npropose a 6DoF mesh saliency detection algorithm based on the uniqueness\nmeasure and the bias preference. To evaluate the proposed approach, we also\ndesign an evaluation metric accordingly which takes the 6DoF information into\nconsideration, and extend some state-of-the-art 3D saliency detection methods\nto make comparisons. The experimental results demonstrate the superior\nperformance of our approach for 6DoF mesh saliency detection, in addition to\nproviding benchmarks for the presented 6DoF mesh saliency database. The\ndatabase and the corresponding algorithms will be made publicly available for\nresearch purposes.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 02:04:33 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 01:07:33 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ding", "Xiaoying", ""], ["Chen", "Zhenzhong", ""]]}, {"id": "2005.13131", "submitter": "Guang Chen", "authors": "Guang Chen, Shiwen Shen, Longyin Wen, Si Luo, Liefeng Bo", "title": "Efficient Pig Counting in Crowds with Keypoints Tracking and\n  Spatial-aware Temporal Response Filtering", "comments": "IEEE International Conference on Robotics and Automation (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pig counting is a crucial task for large-scale pig farming, which is usually\ncompleted by human visually. But this process is very time-consuming and\nerror-prone. Few studies in literature developed automated pig counting method.\nExisting methods only focused on pig counting using single image, and its\naccuracy is challenged by several factors, including pig movements, occlusion\nand overlapping. Especially, the field of view of a single image is very\nlimited, and could not meet the requirements of pig counting for large pig\ngrouping houses. To that end, we presented a real-time automated pig counting\nsystem in crowds using only one monocular fisheye camera with an inspection\nrobot. Our system showed that it produces accurate results surpassing human.\nOur pipeline began with a novel bottom-up pig detection algorithm to avoid\nfalse negatives due to overlapping, occlusion and deformation of pigs. A deep\nconvolution neural network (CNN) is designed to detect keypoints of pig body\npart and associate the keypoints to identify individual pigs. After that, an\nefficient on-line tracking method is used to associate pigs across video\nframes. Finally, a novel spatial-aware temporal response filtering (STRF)\nmethod is proposed to predict the counts of pigs, which is effective to\nsuppress false positives caused by pig or camera movements or tracking\nfailures. The whole pipeline has been deployed in an edge computing device, and\ndemonstrated the effectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 02:17:54 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Chen", "Guang", ""], ["Shen", "Shiwen", ""], ["Wen", "Longyin", ""], ["Luo", "Si", ""], ["Bo", "Liefeng", ""]]}, {"id": "2005.13133", "submitter": "Deheng Qian", "authors": "Yanliang Zhu, Dongchun Ren, Mingyu Fan, Deheng Qian, Xin Li, Huaxia\n  Xia", "title": "Robust Trajectory Forecasting for Multiple Intelligent Agents in Dynamic\n  Scene", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory forecasting, or trajectory prediction, of multiple interacting\nagents in dynamic scenes, is an important problem for many applications, such\nas robotic systems and autonomous driving. The problem is a great challenge\nbecause of the complex interactions among the agents and their interactions\nwith the surrounding scenes. In this paper, we present a novel method for the\nrobust trajectory forecasting of multiple intelligent agents in dynamic scenes.\nThe proposed method consists of three major interrelated components: an\ninteraction net for global spatiotemporal interactive feature extraction, an\nenvironment net for decoding dynamic scenes (i.e., the surrounding road\ntopology of an agent), and a prediction net that combines the spatiotemporal\nfeature, the scene feature, the past trajectories of agents and some random\nnoise for the robust trajectory prediction of agents. Experiments on\npedestrian-walking and vehicle-pedestrian heterogeneous datasets demonstrate\nthat the proposed method outperforms the state-of-the-art prediction methods in\nterms of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 02:32:55 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Zhu", "Yanliang", ""], ["Ren", "Dongchun", ""], ["Fan", "Mingyu", ""], ["Qian", "Deheng", ""], ["Li", "Xin", ""], ["Xia", "Huaxia", ""]]}, {"id": "2005.13135", "submitter": "Zhongpai Gao", "authors": "Zhongpai Gao, Guangtao Zhai, Junchi Yan, Xiaokang Yang", "title": "Permutation Matters: Anisotropic Convolutional Layer for Learning on\n  Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has witnessed a growing demand for efficient representation learning on\npoint clouds in many 3D computer vision applications. Behind the success story\nof convolutional neural networks (CNNs) is that the data (e.g., images) are\nEuclidean structured. However, point clouds are irregular and unordered.\nVarious point neural networks have been developed with isotropic filters or\nusing weighting matrices to overcome the structure inconsistency on point\nclouds. However, isotropic filters or weighting matrices limit the\nrepresentation power. In this paper, we propose a permutable anisotropic\nconvolutional operation (PAI-Conv) that calculates soft-permutation matrices\nfor each point using dot-product attention according to a set of evenly\ndistributed kernel points on a sphere's surface and performs shared anisotropic\nfilters. In fact, dot product with kernel points is by analogy with the\ndot-product with keys in Transformer as widely used in natural language\nprocessing (NLP). From this perspective, PAI-Conv can be regarded as the\ntransformer for point clouds, which is physically meaningful and is robust to\ncooperate with the efficient random point sampling method. Comprehensive\nexperiments on point clouds demonstrate that PAI-Conv produces competitive\nresults in classification and semantic segmentation tasks compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 02:42:29 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 16:32:43 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Gao", "Zhongpai", ""], ["Zhai", "Guangtao", ""], ["Yan", "Junchi", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2005.13140", "submitter": "Shruti Jadon", "authors": "Shruti Jadon", "title": "SSM-Net for Plants Disease Identification in Low Data Regime", "comments": "5 pages, 7 Figures", "journal-ref": "Poster @CVPR workshop, Proceedings at IEEE / ITU International\n  Conference on Artificial Intelligence for Good 2020", "doi": "10.1109/AI4G50087.2020.9311073", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Plant disease detection is an essential factor in increasing agricultural\nproduction. Due to the difficulty of disease detection, farmers spray various\npesticides on their crops to protect them, causing great harm to crop growth\nand food standards. Deep learning can offer critical aid in detecting such\ndiseases. However, it is highly inconvenient to collect a large volume of data\non all forms of the diseases afflicting a specific plant species. In this\npaper, we propose a new metrics-based few-shot learning SSM net architecture,\nwhich consists of stacked siamese and matching network components to address\nthe problem of disease detection in low data regimes. We demonstrated our\nexperiments on two datasets: mini-leaves diseases and sugarcane diseases\ndataset. We have showcased that the SSM-Net approach can achieve better\ndecision boundaries with an accuracy of 92.7% on the mini-leaves dataset and\n94.3% on the sugarcane dataset. The accuracy increased by ~10% and ~5%\nrespectively, compared to the widely used VGG16 transfer learning approach.\nFurthermore, we attained F1 score of 0.90 using SSM Net on the sugarcane\ndataset and 0.91 on the mini-leaves dataset. Our code implementation is\navailable on Github: https://github.com/shruti-jadon/PlantsDiseaseDetection.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 03:43:38 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 05:52:17 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 22:02:02 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2020 20:16:31 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Jadon", "Shruti", ""]]}, {"id": "2005.13149", "submitter": "Mike Wu", "authors": "Mike Wu, Chengxu Zhuang, Milan Mosse, Daniel Yamins, Noah Goodman", "title": "On Mutual Information in Contrastive Learning for Visual Representations", "comments": "8 pages content; 15 pages supplement with proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, several unsupervised, \"contrastive\" learning algorithms in\nvision have been shown to learn representations that perform remarkably well on\ntransfer tasks. We show that this family of algorithms maximizes a lower bound\non the mutual information between two or more \"views\" of an image where typical\nviews come from a composition of image augmentations. Our bound generalizes the\nInfoNCE objective to support negative sampling from a restricted region of\n\"difficult\" contrasts. We find that the choice of negative samples and views\nare critical to the success of these algorithms. Reformulating previous\nlearning objectives in terms of mutual information also simplifies and\nstabilizes them. In practice, our new objectives yield representations that\noutperform those learned with previous approaches for transfer to\nclassification, bounding box detection, instance segmentation, and keypoint\ndetection. % experiments show that choosing more difficult negative samples\nresults in a stronger representation, outperforming those learned with IR, LA,\nand CMC in classification, bounding box detection, instance segmentation, and\nkeypoint detection. The mutual information framework provides a unifying\ncomparison of approaches to contrastive learning and uncovers the choices that\nimpact representation learning.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 04:21:53 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 16:39:20 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Wu", "Mike", ""], ["Zhuang", "Chengxu", ""], ["Mosse", "Milan", ""], ["Yamins", "Daniel", ""], ["Goodman", "Noah", ""]]}, {"id": "2005.13153", "submitter": "Sungmin Woo", "authors": "Sungmin Woo, Sangwon Hwang, Woojin Kim, Junhyeop Lee, Dogyoon Lee,\n  Sangyoun Lee", "title": "False Positive Removal for 3D Vehicle Detection with Penetrated Point\n  Classifier", "comments": "Accepted by ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, researchers have been leveraging LiDAR point cloud for higher\naccuracy in 3D vehicle detection. Most state-of-the-art methods are deep\nlearning based, but are easily affected by the number of points generated on\nthe object. This vulnerability leads to numerous false positive boxes at high\nrecall positions, where objects are occasionally predicted with few points. To\naddress the issue, we introduce Penetrated Point Classifier (PPC) based on the\nunderlying property of LiDAR that points cannot be generated behind vehicles.\nIt determines whether a point exists behind the vehicle of the predicted box,\nand if does, the box is distinguished as false positive. Our straightforward\nyet unprecedented approach is evaluated on KITTI dataset and achieved\nperformance improvement of PointRCNN, one of the state-of-the-art methods. The\nexperiment results show that precision at the highest recall position is\ndramatically increased by 15.46 percentage points and 14.63 percentage points\non the moderate and hard difficulty of car class, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 04:36:53 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 02:11:16 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Woo", "Sungmin", ""], ["Hwang", "Sangwon", ""], ["Kim", "Woojin", ""], ["Lee", "Junhyeop", ""], ["Lee", "Dogyoon", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2005.13178", "submitter": "Pegah Salehi", "authors": "Pegah Salehi, Abdolah Chalechale, Maryam Taghizadeh", "title": "Generative Adversarial Networks (GANs): An Overview of Theoretical\n  Model, Evaluation Metrics, and Recent Developments", "comments": "Submitted to a journal in the computer vision field", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  One of the most significant challenges in statistical signal processing and\nmachine learning is how to obtain a generative model that can produce samples\nof large-scale data distribution, such as images and speeches. Generative\nAdversarial Network (GAN) is an effective method to address this problem. The\nGANs provide an appropriate way to learn deep representations without\nwidespread use of labeled training data. This approach has attracted the\nattention of many researchers in computer vision since it can generate a large\namount of data without precise modeling of the probability density function\n(PDF). In GANs, the generative model is estimated via a competitive process\nwhere the generator and discriminator networks are trained simultaneously. The\ngenerator learns to generate plausible data, and the discriminator learns to\ndistinguish fake data created by the generator from real data samples. Given\nthe rapid growth of GANs over the last few years and their application in\nvarious fields, it is necessary to investigate these networks accurately. In\nthis paper, after introducing the main concepts and the theory of GAN, two new\ndeep generative models are compared, the evaluation metrics utilized in the\nliterature and challenges of GANs are also explained. Moreover, the most\nremarkable GAN architectures are categorized and discussed. Finally, the\nessential applications in computer vision are examined.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 05:56:53 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Salehi", "Pegah", ""], ["Chalechale", "Abdolah", ""], ["Taghizadeh", "Maryam", ""]]}, {"id": "2005.13180", "submitter": "Simone Fobi", "authors": "Simone Fobi, Terence Conlon, Jayant Taneja, Vijay Modi", "title": "Learning to segment from misaligned and partial labels", "comments": "This is the extended version of a paper to be published in ACM\n  COMPASS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To extract information at scale, researchers increasingly apply semantic\nsegmentation techniques to remotely-sensed imagery. While fully-supervised\nlearning enables accurate pixel-wise segmentation, compiling the exhaustive\ndatasets required is often prohibitively expensive. As a result, many non-urban\nsettings lack the ground-truth needed for accurate segmentation. Existing open\nsource infrastructure data for these regions can be inexact and non-exhaustive.\nOpen source infrastructure annotations like OpenStreetMaps (OSM) are\nrepresentative of this issue: while OSM labels provide global insights to road\nand building footprints, noisy and partial annotations limit the performance of\nsegmentation algorithms that learn from them. In this paper, we present a novel\nand generalizable two-stage framework that enables improved pixel-wise image\nsegmentation given misaligned and missing annotations. First, we introduce the\nAlignment Correction Network to rectify incorrectly registered open source\nlabels. Next, we demonstrate a segmentation model -- the Pointer Segmentation\nNetwork -- that uses corrected labels to predict infrastructure footprints\ndespite missing annotations. We test sequential performance on the AIRS\ndataset, achieving a mean intersection-over-union score of 0.79; more\nimportantly, model performance remains stable as we decrease the fraction of\nannotations present. We demonstrate the transferability of our method to lower\nquality data, by applying the Alignment Correction Network to OSM labels to\ncorrect building footprints; we also demonstrate the accuracy of the Pointer\nSegmentation Network in predicting cropland boundaries in California from\nmedium resolution data. Overall, our methodology is robust for multiple\napplications with varied amounts of training data present, thus offering a\nmethod to extract reliable information from noisy, partial data.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 06:02:58 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Fobi", "Simone", ""], ["Conlon", "Terence", ""], ["Taneja", "Jayant", ""], ["Modi", "Vijay", ""]]}, {"id": "2005.13192", "submitter": "Bingchen Liu", "authors": "Bingchen Liu, Kunpeng Song, Yizhe Zhu, Gerard de Melo, Ahmed Elgammal", "title": "TIME: Text and Image Mutual-Translation Adversarial Networks", "comments": "AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Focusing on text-to-image (T2I) generation, we propose Text and Image\nMutual-Translation Adversarial Networks (TIME), a lightweight but effective\nmodel that jointly learns a T2I generator G and an image captioning\ndiscriminator D under the Generative Adversarial Network framework. While\nprevious methods tackle the T2I problem as a uni-directional task and use\npre-trained language models to enforce the image--text consistency, TIME\nrequires neither extra modules nor pre-training. We show that the performance\nof G can be boosted substantially by training it jointly with D as a language\nmodel. Specifically, we adopt Transformers to model the cross-modal connections\nbetween the image features and word embeddings, and design an annealing\nconditional hinge loss that dynamically balances the adversarial learning. In\nour experiments, TIME achieves state-of-the-art (SOTA) performance on the CUB\nand MS-COCO dataset (Inception Score of 4.91 and Fr\\'echet Inception Distance\nof 14.3 on CUB), and shows promising performance on MS-COCO on image captioning\nand downstream vision-language tasks.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 06:40:12 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 20:46:36 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Liu", "Bingchen", ""], ["Song", "Kunpeng", ""], ["Zhu", "Yizhe", ""], ["de Melo", "Gerard", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "2005.13194", "submitter": "Sangjin Lee", "authors": "Sangjin Lee, Hyeongmin Lee, Taeoh Kim and Sangyoun Lee", "title": "Extrapolative-Interpolative Cycle-Consistency Learning for Video Frame\n  Extrapolation", "comments": "This paper has been accepted to 2020 IEEE International Conference on\n  Image Processing (ICIP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video frame extrapolation is a task to predict future frames when the past\nframes are given. Unlike previous studies that usually have been focused on the\ndesign of modules or construction of networks, we propose a novel\nExtrapolative-Interpolative Cycle (EIC) loss using pre-trained frame\ninterpolation module to improve extrapolation performance. Cycle-consistency\nloss has been used for stable prediction between two function spaces in many\nvisual tasks. We formulate this cycle-consistency using two mapping functions;\nframe extrapolation and interpolation. Since it is easier to predict\nintermediate frames than to predict future frames in terms of the object\nocclusion and motion uncertainty, interpolation module can give guidance signal\neffectively for training the extrapolation function. EIC loss can be applied to\nany existing extrapolation algorithms and guarantee consistent prediction in\nthe short future as well as long future frames. Experimental results show that\nsimply adding EIC loss to the existing baseline increases extrapolation\nperformance on both UCF101 and KITTI datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 06:42:21 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Lee", "Sangjin", ""], ["Lee", "Hyeongmin", ""], ["Kim", "Taeoh", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2005.13201", "submitter": "Ashwin Raju", "authors": "Ashwin Raju, Chi-Tung Cheng, Yunakai Huo, Jinzheng Cai, Junzhou Huang,\n  Jing Xiao, Le Lu, ChienHuang Liao and Adam P Harrison", "title": "Co-Heterogeneous and Adaptive Segmentation from Multi-Source and\n  Multi-Phase CT Imaging Data: A Study on Pathological Liver and Lesion\n  Segmentation", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical imaging, organ/pathology segmentation models trained on current\npublicly available and fully-annotated datasets usually do not well-represent\nthe heterogeneous modalities, phases, pathologies, and clinical scenarios\nencountered in real environments. On the other hand, there are tremendous\namounts of unlabelled patient imaging scans stored by many modern clinical\ncenters. In this work, we present a novel segmentation strategy,\nco-heterogenous and adaptive segmentation (CHASe), which only requires a small\nlabeled cohort of single phase imaging data to adapt to any unlabeled cohort of\nheterogenous multi-phase data with possibly new clinical scenarios and\npathologies. To do this, we propose a versatile framework that fuses appearance\nbased semi-supervision, mask based adversarial domain adaptation, and\npseudo-labeling. We also introduce co-heterogeneous training, which is a novel\nintegration of co-training and hetero modality learning. We have evaluated\nCHASe using a clinically comprehensive and challenging dataset of multi-phase\ncomputed tomography (CT) imaging studies (1147 patients and 4577 3D volumes).\nCompared to previous state-of-the-art baselines, CHASe can further improve\npathological liver mask Dice-Sorensen coefficients by ranges of $4.2\\% \\sim\n9.4\\%$, depending on the phase combinations: e.g., from $84.6\\%$ to $94.0\\%$ on\nnon-contrast CTs.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 06:58:39 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 19:12:49 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 07:39:31 GMT"}, {"version": "v4", "created": "Mon, 19 Jul 2021 18:54:43 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Raju", "Ashwin", ""], ["Cheng", "Chi-Tung", ""], ["Huo", "Yunakai", ""], ["Cai", "Jinzheng", ""], ["Huang", "Junzhou", ""], ["Xiao", "Jing", ""], ["Lu", "Le", ""], ["Liao", "ChienHuang", ""], ["Harrison", "Adam P", ""]]}, {"id": "2005.13215", "submitter": "Tugdual Ceillier", "authors": "Damien Grosgeorge (SAS), Maxime Arbelot (SAS), Alex Goupilleau (SAS),\n  Tugdual Ceillier (SAS), Renaud Allioux (SAS)", "title": "Concurrent Segmentation and Object Detection CNNs for Aircraft Detection\n  and Identification in Satellite Images", "comments": null, "journal-ref": "IEEE International Geoscience and Remote Sensing Symposium\n  (IGARSS), 2020, Waikoloa, Hawaii, United States", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and identifying objects in satellite images is a very challenging\ntask: objects of interest are often very small and features can be difficult to\nrecognize even using very high resolution imagery. For most applications, this\ntranslates into a trade-off between recall and precision. We present here a\ndedicated method to detect and identify aircraft, combining two very different\nconvolutional neural networks (CNNs): a segmentation model, based on a modified\nU-net architecture, and a detection model, based on the RetinaNet architecture.\nThe results we present show that this combination outperforms significantly\neach unitary model, reducing drastically the false negative rate.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 07:35:55 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Grosgeorge", "Damien", "", "SAS"], ["Arbelot", "Maxime", "", "SAS"], ["Goupilleau", "Alex", "", "SAS"], ["Ceillier", "Tugdual", "", "SAS"], ["Allioux", "Renaud", "", "SAS"]]}, {"id": "2005.13219", "submitter": "Yingying Deng", "authors": "Yingying Deng, Fan Tang, Weiming Dong, Wen Sun, Feiyue Huang,\n  Changsheng Xu", "title": "Arbitrary Style Transfer via Multi-Adaptation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary style transfer is a significant topic with research value and\napplication prospect. A desired style transfer, given a content image and\nreferenced style painting, would render the content image with the color tone\nand vivid stroke patterns of the style painting while synchronously maintaining\nthe detailed content structure information. Style transfer approaches would\ninitially learn content and style representations of the content and style\nreferences and then generate the stylized images guided by these\nrepresentations. In this paper, we propose the multi-adaptation network which\ninvolves two self-adaptation (SA) modules and one co-adaptation (CA) module:\nthe SA modules adaptively disentangle the content and style representations,\ni.e., content SA module uses position-wise self-attention to enhance content\nrepresentation and style SA module uses channel-wise self-attention to enhance\nstyle representation; the CA module rearranges the distribution of style\nrepresentation based on content representation distribution by calculating the\nlocal similarity between the disentangled content and style features in a\nnon-local fashion. Moreover, a new disentanglement loss function enables our\nnetwork to extract main style patterns and exact content structures to adapt to\nvarious input images, respectively. Various qualitative and quantitative\nexperiments demonstrate that the proposed multi-adaptation network leads to\nbetter results than the state-of-the-art style transfer methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 08:00:22 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 05:28:46 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Deng", "Yingying", ""], ["Tang", "Fan", ""], ["Dong", "Weiming", ""], ["Sun", "Wen", ""], ["Huang", "Feiyue", ""], ["Xu", "Changsheng", ""]]}, {"id": "2005.13222", "submitter": "Guanghan Li", "authors": "Guanghan Li, Yaping Zhao, Mengqi Ji, Xiaoyun Yuan, Lu Fang", "title": "Zoom in to the details of human-centric videos", "comments": "5 pages, 6 figures, accepted for presentation at IEEE ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presenting high-resolution (HR) human appearance is always critical for the\nhuman-centric videos. However, current imagery equipment can hardly capture HR\ndetails all the time. Existing super-resolution algorithms barely mitigate the\nproblem by only considering universal and low-level priors of im-age patches.\nIn contrast, our algorithm is under bias towards the human body\nsuper-resolution by taking advantage of high-level prior defined by HR human\nappearance. Firstly, a motion analysis module extracts inherent motion pattern\nfrom the HR reference video to refine the pose estimation of the low-resolution\n(LR) sequence. Furthermore, a human body reconstruction module maps the HR\ntexture in the reference frames onto a 3D mesh model. Consequently, the input\nLR videos get super-resolved HR human sequences are generated conditioned on\nthe original LR videos as well as few HR reference frames. Experiments on an\nexisting dataset and real-world data captured by hybrid cameras show that our\napproach generates superior visual quality of human body compared with the\ntraditional method.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 08:04:47 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Li", "Guanghan", ""], ["Zhao", "Yaping", ""], ["Ji", "Mengqi", ""], ["Yuan", "Xiaoyun", ""], ["Fang", "Lu", ""]]}, {"id": "2005.13243", "submitter": "Petr Hurtik", "authors": "Petr Hurtik, Vojtech Molek, Jan Hula, Marek Vajgl, Pavel Vlasanek,\n  Tomas Nejezchleba", "title": "Poly-YOLO: higher speed, more precise detection and instance\n  segmentation for YOLOv3", "comments": "18 pages, 15 figures, submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence (under review), Source code is available at\n  https://gitlab.com/irafm-ai/poly-yolo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new version of YOLO with better performance and extended with\ninstance segmentation called Poly-YOLO. Poly-YOLO builds on the original ideas\nof YOLOv3 and removes two of its weaknesses: a large amount of rewritten labels\nand inefficient distribution of anchors. Poly-YOLO reduces the issues by\naggregating features from a light SE-Darknet-53 backbone with a hypercolumn\ntechnique, using stairstep upsampling, and produces a single scale output with\nhigh resolution. In comparison with YOLOv3, Poly-YOLO has only 60% of its\ntrainable parameters but improves mAP by a relative 40%. We also present\nPoly-YOLO lite with fewer parameters and a lower output resolution. It has the\nsame precision as YOLOv3, but it is three times smaller and twice as fast, thus\nsuitable for embedded devices. Finally, Poly-YOLO performs instance\nsegmentation using bounding polygons. The network is trained to detect\nsize-independent polygons defined on a polar grid. Vertices of each polygon are\nbeing predicted with their confidence, and therefore Poly-YOLO produces\npolygons with a varying number of vertices.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 08:53:35 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 11:58:33 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Hurtik", "Petr", ""], ["Molek", "Vojtech", ""], ["Hula", "Jan", ""], ["Vajgl", "Marek", ""], ["Vlasanek", "Pavel", ""], ["Nejezchleba", "Tomas", ""]]}, {"id": "2005.13288", "submitter": "Jonas Wurst", "authors": "Jonas Wurst, Alberto Flores Fern\\'andez, Michael Botsch and Wolfgang\n  Utschick", "title": "An Entropy Based Outlier Score and its Application to Novelty Detection\n  for Road Infrastructure Images", "comments": "Copyright 2020 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "2020 IEEE Intelligent Vehicles Symposium (IV)", "doi": "10.1109/IV47402.2020.9304733", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel unsupervised outlier score, which can be embedded into graph based\ndimensionality reduction techniques, is presented in this work. The score uses\nthe directed nearest neighbor graphs of those techniques. Hence, the same\nmeasure of similarity that is used to project the data into lower dimensions,\nis also utilized to determine the outlier score. The outlier score is realized\nthrough a weighted normalized entropy of the similarities. This score is\napplied to road infrastructure images. The aim is to identify newly observed\ninfrastructures given a pre-collected base dataset. Detecting unknown scenarios\nis a key for accelerated validation of autonomous vehicles. The results show\nthe high potential of the proposed technique. To validate the generalization\ncapabilities of the outlier score, it is additionally applied to various real\nworld datasets. The overall average performance in identifying outliers using\nthe proposed methods is higher compared to state-of-the-art methods. In order\nto generate the infrastructure images, an openDRIVE parsing and plotting tool\nfor Matlab is developed as part of this work. This tool and the implementation\nof the entropy based outlier score in combination with Uniform Manifold\nApproximation and Projection are made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 11:34:42 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 08:40:47 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Wurst", "Jonas", ""], ["Fern\u00e1ndez", "Alberto Flores", ""], ["Botsch", "Michael", ""], ["Utschick", "Wolfgang", ""]]}, {"id": "2005.13291", "submitter": "Andrew Port", "authors": "Andrew Port, Chelhwon Kim, Mitesh Patel", "title": "Earballs: Neural Transmodal Translation", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As is expressed in the adage \"a picture is worth a thousand words\", when\nusing spoken language to communicate visual information, brevity can be a\nchallenge. This work describes a novel technique for leveraging machine learned\nfeature embeddings to translate visual (and other types of) information into a\nperceptual audio domain, allowing users to perceive this information using only\ntheir aural faculty. The system uses a pretrained image embedding network to\nextract visual features and embed them in a compact subset of Euclidean space\n-- this converts the images into feature vectors whose $L^2$ distances can be\nused as a meaningful measure of similarity. A generative adversarial network\n(GAN) is then used to find a distance preserving map from this metric space of\nfeature vectors into the metric space defined by a target audio dataset\nequipped with either the Euclidean metric or a mel-frequency cepstrum-based\npsychoacoustic distance metric. We demonstrate this technique by translating\nimages of faces into human speech-like audio. For both target audio metrics,\nthe GAN successfully found a metric preserving mapping, and in human subject\ntests, users were able to accurately classify audio translations of faces.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 11:41:48 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 20:48:18 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Port", "Andrew", ""], ["Kim", "Chelhwon", ""], ["Patel", "Mitesh", ""]]}, {"id": "2005.13297", "submitter": "Hongwei Xie", "authors": "Hongwei Xie, Shuo Zhang, Huanghao Ding, Yafei Song, Baitao Shao,\n  Conggang Hu, Ling Cai and Mingyang Li", "title": "Accelerating Neural Network Inference by Overflow Aware Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inherent heavy computation of deep neural networks prevents their\nwidespread applications. A widely used method for accelerating model inference\nis quantization, by replacing the input operands of a network using fixed-point\nvalues. Then the majority of computation costs focus on the integer matrix\nmultiplication accumulation. In fact, high-bit accumulator leads to partially\nwasted computation and low-bit one typically suffers from numerical overflow.\nTo address this problem, we propose an overflow aware quantization method by\ndesigning trainable adaptive fixed-point representation, to optimize the number\nof bits for each input tensor while prohibiting numeric overflow during the\ncomputation. With the proposed method, we are able to fully utilize the\ncomputing power to minimize the quantization loss and obtain optimized\ninference performance. To verify the effectiveness of our method, we conduct\nimage classification, object detection, and semantic segmentation tasks on\nImageNet, Pascal VOC, and COCO datasets, respectively. Experimental results\ndemonstrate that the proposed method can achieve comparable performance with\nstate-of-the-art quantization methods while accelerating the inference process\nby about 2 times.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 11:56:22 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Xie", "Hongwei", ""], ["Zhang", "Shuo", ""], ["Ding", "Huanghao", ""], ["Song", "Yafei", ""], ["Shao", "Baitao", ""], ["Hu", "Conggang", ""], ["Cai", "Ling", ""], ["Li", "Mingyang", ""]]}, {"id": "2005.13298", "submitter": "Sheng Huang", "authors": "Wenhao Tang and Sheng Huang and Qiming Zhao and Ren Li and Luwen\n  Huangfu", "title": "Iteratively Optimized Patch Label Inference Network for Automatic\n  Pavement Disease Detection", "comments": "Revision on IEEE Trans on ITS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a novel deep learning framework named the Iteratively Optimized\nPatch Label Inference Network (IOPLIN) for automatically detecting various\npavement diseases that are not solely limited to specific ones, such as cracks\nand potholes. IOPLIN can be iteratively trained with only the image label via\nthe Expectation-Maximization Inspired Patch Label Distillation (EMIPLD)\nstrategy, and accomplish this task well by inferring the labels of patches from\nthe pavement images. IOPLIN enjoys many desirable properties over the\nstate-of-the-art single branch CNN models such as GoogLeNet and EfficientNet.\nIt is able to handle images in different resolutions, and sufficiently utilize\nimage information particularly for the high-resolution ones, since IOPLIN\nextracts the visual features from unrevised image patches instead of the\nresized entire image. Moreover, it can roughly localize the pavement distress\nwithout using any prior localization information in the training phase. In\norder to better evaluate the effectiveness of our method in practice, we\nconstruct a large-scale Bituminous Pavement Disease Detection dataset named\nCQU-BPDD consisting of 60,059 high-resolution pavement images, which are\nacquired from different areas at different times. Extensive results on this\ndataset demonstrate the superiority of IOPLIN over the state-of-the-art image\nclassification approaches in automatic pavement disease detection. The source\ncodes of IOPLIN are released on \\url{https://github.com/DearCaat/ioplin}.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 11:56:38 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 08:13:07 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Tang", "Wenhao", ""], ["Huang", "Sheng", ""], ["Zhao", "Qiming", ""], ["Li", "Ren", ""], ["Huangfu", "Luwen", ""]]}, {"id": "2005.13312", "submitter": "Xin Chen", "authors": "Xin Chen, Yuwei Li, Xi Luo, Tianjia Shao, Jingyi Yu, Kun Zhou, Youyi\n  Zheng", "title": "AutoSweep: Recovering 3D Editable Objectsfrom a Single Photograph", "comments": "10 pages, 12 figures", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, vol. 26,\n  no. 3, pp. 1466-1475, 1 March 2020", "doi": "10.1109/TVCG.2018.2871190", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fully automatic framework for extracting editable 3D\nobjects directly from a single photograph. Unlike previous methods which\nrecover either depth maps, point clouds, or mesh surfaces, we aim to recover 3D\nobjects with semantic parts and can be directly edited. We base our work on the\nassumption that most human-made objects are constituted by parts and these\nparts can be well represented by generalized primitives. Our work makes an\nattempt towards recovering two types of primitive-shaped objects, namely,\ngeneralized cuboids and generalized cylinders. To this end, we build a novel\ninstance-aware segmentation network for accurate part separation. Our GeoNet\noutputs a set of smooth part-level masks labeled as profiles and bodies. Then\nin a key stage, we simultaneously identify profile-body relations and recover\n3D parts by sweeping the recognized profile along their body contour and\njointly optimize the geometry to align with the recovered masks. Qualitative\nand quantitative experiments show that our algorithm can recover high quality\n3D models and outperforms existing methods in both instance segmentation and 3D\nreconstruction. The dataset and code of AutoSweep are available at\nhttps://chenxin.tech/AutoSweep.html.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 12:16:24 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 01:08:27 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Chen", "Xin", ""], ["Li", "Yuwei", ""], ["Luo", "Xi", ""], ["Shao", "Tianjia", ""], ["Yu", "Jingyi", ""], ["Zhou", "Kun", ""], ["Zheng", "Youyi", ""]]}, {"id": "2005.13337", "submitter": "Liangzhi Li", "authors": "Liangzhi Li, Manisha Verma, Yuta Nakashima, Ryo Kawasaki, Hajime\n  Nagahara", "title": "Joint Learning of Vessel Segmentation and Artery/Vein Classification\n  with Post-processing", "comments": "Accepted in Medical Imaging with Deep Learning (MIDL) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal imaging serves as a valuable tool for diagnosis of various diseases.\nHowever, reading retinal images is a difficult and time-consuming task even for\nexperienced specialists. The fundamental step towards automated retinal image\nanalysis is vessel segmentation and artery/vein classification, which provide\nvarious information on potential disorders. To improve the performance of the\nexisting automated methods for retinal image analysis, we propose a two-step\nvessel classification. We adopt a UNet-based model, SeqNet, to accurately\nsegment vessels from the background and make prediction on the vessel type. Our\nmodel does segmentation and classification sequentially, which alleviates the\nproblem of label distribution bias and facilitates training. To further refine\nclassification results, we post-process them considering the structural\ninformation among vessels to propagate highly confident prediction to\nsurrounding vessels. Our experiments show that our method improves AUC to 0.98\nfor segmentation and the accuracy to 0.92 in classification over DRIVE dataset.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 13:06:16 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Li", "Liangzhi", ""], ["Verma", "Manisha", ""], ["Nakashima", "Yuta", ""], ["Kawasaki", "Ryo", ""], ["Nagahara", "Hajime", ""]]}, {"id": "2005.13338", "submitter": "Lasse Hansen", "authors": "Lasse Hansen, Mattias P. Heinrich", "title": "Tackling the Problem of Large Deformations in Deep Learning Based\n  Medical Image Registration Using Displacement Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/kPBUZluVq", "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though, deep learning based medical image registration is currently starting\nto show promising advances, often, it still fells behind conventional\nframeworks in terms of registration accuracy. This is especially true for\napplications where large deformations exist, such as registration of\ninterpatient abdominal MRI or inhale-to-exhale CT lung registration. Most\ncurrent works use U-Net-like architectures to predict dense displacement fields\nfrom the input images in different supervised and unsupervised settings. We\nbelieve that the U-Net architecture itself to some level limits the ability to\npredict large deformations (even when using multilevel strategies) and\ntherefore propose a novel approach, where the input images are mapped into a\ndisplacement space and final registrations are reconstructed from this\nembedding. Experiments on inhale-to-exhale CT lung registration demonstrate the\nability of our architecture to predict large deformations in a single forward\npath through our network (leading to errors below 2 mm).\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 13:06:24 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Hansen", "Lasse", ""], ["Heinrich", "Mattias P.", ""]]}, {"id": "2005.13358", "submitter": "Jong-Hoon Ahn", "authors": "Jong-Hoon Ahn", "title": "Data-Driven Continuum Dynamics via Transport-Teleport Duality", "comments": "11 pages, 10 figures (Added references, Added figures, Reorganized\n  sections)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, machine learning methods have been widely used to study\nphysical systems that are challenging to solve with governing equations.\nPhysicists and engineers are framing the data-driven paradigm as an alternative\napproach to physical sciences. In this paradigm change, the deep learning\napproach is playing a pivotal role. However, most learning architectures do not\ninherently incorporate conservation laws in the form of continuity equations,\nand they require dense data to learn the dynamics of conserved quantities. In\nthis study, we introduce a clever mathematical transform to represent the\nclassical dynamics as a point-wise process of disappearance and reappearance of\na quantity, which dramatically reduces model complexity and training data for\nmachine learning of transport phenomena. We demonstrate that just a few\nobservational data and a simple learning model can be enough to learn the\ndynamics of real-world objects. The approach does not require the explicit use\nof governing equations and only depends on observation data. Because the\ncontinuity equation is a general equation that any conserved quantity should\nobey, the applicability may range from physical to social and medical sciences\nor any field where data are conserved quantities.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 13:39:09 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 20:58:22 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Ahn", "Jong-Hoon", ""]]}, {"id": "2005.13359", "submitter": "Cameron Trotter", "authors": "Cameron Trotter and Georgia Atkinson and Matt Sharpe and Kirsten\n  Richardson and A. Stephen McGough and Nick Wright and Ben Burville and Per\n  Berggren", "title": "NDD20: A large-scale few-shot dolphin dataset for coarse and\n  fine-grained categorisation", "comments": "5 pages, 6 figures, download link, submitted to FGVC7 Workshop @\n  CVPR20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the Northumberland Dolphin Dataset 2020 (NDD20), a challenging\nimage dataset annotated for both coarse and fine-grained instance segmentation\nand categorisation. This dataset, the first release of the NDD, was created in\nresponse to the rapid expansion of computer vision into conservation research\nand the production of field-deployable systems suited to extreme environmental\nconditions -- an area with few open source datasets. NDD20 contains a large\ncollection of above and below water images of two different dolphin species for\ntraditional coarse and fine-grained segmentation. All data contained in NDD20\nwas obtained via manual collection in the North Sea around the Northumberland\ncoastline, UK. We present experimentation using standard deep learning network\narchitecture trained using NDD20 and report baselines results.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 13:41:39 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Trotter", "Cameron", ""], ["Atkinson", "Georgia", ""], ["Sharpe", "Matt", ""], ["Richardson", "Kirsten", ""], ["McGough", "A. Stephen", ""], ["Wright", "Nick", ""], ["Burville", "Ben", ""], ["Berggren", "Per", ""]]}, {"id": "2005.13362", "submitter": "Edison Marrese-Taylor", "authors": "Edison Marrese-Taylor, Cristian Rodriguez-Opazo, Jorge A. Balazs,\n  Stephen Gould, Yutaka Matsuo", "title": "A Multi-modal Approach to Fine-grained Opinion Mining on Video Reviews", "comments": "Second Grand Challenge and Workshop on Multimodal Language ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent advances in opinion mining for written reviews, few works\nhave tackled the problem on other sources of reviews. In light of this issue,\nwe propose a multi-modal approach for mining fine-grained opinions from video\nreviews that is able to determine the aspects of the item under review that are\nbeing discussed and the sentiment orientation towards them. Our approach works\nat the sentence level without the need for time annotations and uses features\nderived from the audio, video and language transcriptions of its contents. We\nevaluate our approach on two datasets and show that leveraging the video and\naudio modalities consistently provides increased performance over text-only\nbaselines, providing evidence these extra modalities are key in better\nunderstanding video reviews.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 13:46:11 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 03:13:49 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Marrese-Taylor", "Edison", ""], ["Rodriguez-Opazo", "Cristian", ""], ["Balazs", "Jorge A.", ""], ["Gould", "Stephen", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "2005.13363", "submitter": "Zhuoying Wang", "authors": "Zhuoying Wang and Yongtao Wang and Zhi Tang and Yangyan Li and Ying\n  Chen and Haibin Ling and Weisi Lin", "title": "GSTO: Gated Scale-Transfer Operation for Multi-Scale Feature Learning in\n  Pixel Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing CNN-based methods for pixel labeling heavily depend on multi-scale\nfeatures to meet the requirements of both semantic comprehension and detail\npreservation. State-of-the-art pixel labeling neural networks widely exploit\nconventional scale-transfer operations, i.e., up-sampling and down-sampling to\nlearn multi-scale features. In this work, we find that these operations lead to\nscale-confused features and suboptimal performance because they are\nspatial-invariant and directly transit all feature information cross scales\nwithout spatial selection. To address this issue, we propose the Gated\nScale-Transfer Operation (GSTO) to properly transit spatial-filtered features\nto another scale. Specifically, GSTO can work either with or without extra\nsupervision. Unsupervised GSTO is learned from the feature itself while the\nsupervised one is guided by the supervised probability matrix. Both forms of\nGSTO are lightweight and plug-and-play, which can be flexibly integrated into\nnetworks or modules for learning better multi-scale features. In particular, by\nplugging GSTO into HRNet, we get a more powerful backbone (namely GSTO-HRNet)\nfor pixel labeling, and it achieves new state-of-the-art results on the COCO\nbenchmark for human pose estimation and other benchmarks for semantic\nsegmentation including Cityscapes, LIP and Pascal Context, with negligible\nextra computational cost. Moreover, experiment results demonstrate that GSTO\ncan also significantly boost the performance of multi-scale feature aggregation\nmodules like PPM and ASPP. Code will be made available at\nhttps://github.com/VDIGPKU/GSTO.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 13:46:58 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 13:51:04 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Wang", "Zhuoying", ""], ["Wang", "Yongtao", ""], ["Tang", "Zhi", ""], ["Li", "Yangyan", ""], ["Chen", "Ying", ""], ["Ling", "Haibin", ""], ["Lin", "Weisi", ""]]}, {"id": "2005.13366", "submitter": "Jingyang Zhang", "authors": "Jingyang Zhang, Guotai Wang, Hongzhi Xie, Shuyang Zhang, Ning Huang,\n  Shaoting Zhang, Lixu Gu", "title": "Weakly Supervised Vessel Segmentation in X-ray Angiograms by Self-Paced\n  Learning from Noisy Labels with Suggestive Annotation", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2020.06.122", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of coronary arteries in X-ray angiograms by convolutional\nneural networks (CNNs) is promising yet limited by the requirement of precisely\nannotating all pixels in a large number of training images, which is extremely\nlabor-intensive especially for complex coronary trees. To alleviate the burden\non the annotator, we propose a novel weakly supervised training framework that\nlearns from noisy pseudo labels generated from automatic vessel enhancement,\nrather than accurate labels obtained by fully manual annotation. A typical\nself-paced learning scheme is used to make the training process robust against\nlabel noise while challenged by the systematic biases in pseudo labels, thus\nleading to the decreased performance of CNNs at test time. To solve this\nproblem, we propose an annotation-refining self-paced learning framework\n(AR-SPL) to correct the potential errors using suggestive annotation. An\nelaborate model-vesselness uncertainty estimation is also proposed to enable\nthe minimal annotation cost for suggestive annotation, based on not only the\nCNNs in training but also the geometric features of coronary arteries derived\ndirectly from raw data. Experiments show that our proposed framework achieves\n1) comparable accuracy to fully supervised learning, which also significantly\noutperforms other weakly supervised learning frameworks; 2) largely reduced\nannotation cost, i.e., 75.18% of annotation time is saved, and only 3.46% of\nimage regions are required to be annotated; and 3) an efficient intervention\nprocess, leading to superior performance with even fewer manual interactions.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 13:55:33 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Zhang", "Jingyang", ""], ["Wang", "Guotai", ""], ["Xie", "Hongzhi", ""], ["Zhang", "Shuyang", ""], ["Huang", "Ning", ""], ["Zhang", "Shaoting", ""], ["Gu", "Lixu", ""]]}, {"id": "2005.13402", "submitter": "Pratik Mazumder", "authors": "Pratik Mazumder, Pravendra Singh, Kranti Kumar Parida, Vinay P.\n  Namboodiri", "title": "AVGZSLNet: Audio-Visual Generalized Zero-Shot Learning by Reconstructing\n  Label Features from Multi-Modal Embeddings", "comments": "Accepted in WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for generalized zero-shot learning\nin a multi-modal setting, where we have novel classes of audio/video during\ntesting that are not seen during training. We use the semantic relatedness of\ntext embeddings as a means for zero-shot learning by aligning audio and video\nembeddings with the corresponding class label text feature space. Our approach\nuses a cross-modal decoder and a composite triplet loss. The cross-modal\ndecoder enforces a constraint that the class label text features can be\nreconstructed from the audio and video embeddings of data points. This helps\nthe audio and video embeddings to move closer to the class label text\nembedding. The composite triplet loss makes use of the audio, video, and text\nembeddings. It helps bring the embeddings from the same class closer and push\naway the embeddings from different classes in a multi-modal setting. This helps\nthe network to perform better on the multi-modal zero-shot learning task.\nImportantly, our multi-modal zero-shot learning approach works even if a\nmodality is missing at test time. We test our approach on the generalized\nzero-shot classification and retrieval tasks and show that our approach\noutperforms other models in the presence of a single modality as well as in the\npresence of multiple modalities. We validate our approach by comparing it with\nprevious approaches and using various ablations.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 14:58:34 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 17:15:52 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 06:13:16 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Mazumder", "Pratik", ""], ["Singh", "Pravendra", ""], ["Parida", "Kranti Kumar", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2005.13423", "submitter": "Sebastian Dorn", "authors": "Yunlei Tang, Sebastian Dorn and Chiragkumar Savani", "title": "Center3D: Center-based Monocular 3D Object Detection with Joint Depth\n  Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing objects in 3D space and understanding their associated 3D\nproperties is challenging given only monocular RGB images. The situation is\ncompounded by the loss of depth information during perspective projection. We\npresent Center3D, a one-stage anchor-free approach, to efficiently estimate 3D\nlocation and depth using only monocular RGB images. By exploiting the\ndifference between 2D and 3D centers, we are able to estimate depth\nconsistently. Center3D uses a combination of classification and regression to\nunderstand the hidden depth information more robustly than each method alone.\nOur method employs two joint approaches: (1) LID: a classification-dominated\napproach with sequential Linear Increasing Discretization. (2) DepJoint: a\nregression-dominated approach with multiple Eigen's transformations for depth\nestimation. Evaluating on KITTI dataset for moderate objects, Center3D improved\nthe AP in BEV from $29.7\\%$ to $42.8\\%$, and the AP in 3D from $18.6\\%$ to\n$39.1\\%$. Compared with state-of-the-art detectors, Center3D has achieved the\nbest speed-accuracy trade-off in realtime monocular object detection.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 15:29:09 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Tang", "Yunlei", ""], ["Dorn", "Sebastian", ""], ["Savani", "Chiragkumar", ""]]}, {"id": "2005.13449", "submitter": "Jun Ma", "authors": "Jun Ma", "title": "Segmentation Loss Odyssey", "comments": "Educational Materials\n  (https://miccai-sb.github.io/materials/Ma2019.pdf)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Loss functions are one of the crucial ingredients in deep learning-based\nmedical image segmentation methods. Many loss functions have been proposed in\nexisting literature, but are studied separately or only investigated with few\nother losses. In this paper, we present a systematic taxonomy to sort existing\nloss functions into four meaningful categories. This helps to reveal links and\nfundamental similarities between them. Moreover, we explore the relationship\nbetween the traditional region-based and the more recent boundary-based loss\nfunctions. The PyTorch implementations of these loss functions are publicly\navailable at \\url{https://github.com/JunMa11/SegLoss}.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 16:00:55 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Ma", "Jun", ""]]}, {"id": "2005.13452", "submitter": "Dong Wang", "authors": "Dong Wang, Kexin Zhang, Jia Ding and Liwei Wang", "title": "Improve bone age assessment by learning from anatomical local regions", "comments": "Early accepted to MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeletal bone age assessment (BAA), as an essential imaging examination, aims\nat evaluating the biological and structural maturation of human bones. In the\nclinical practice, Tanner and Whitehouse (TW2) method is a widely-used method\nfor radiologists to perform BAA. The TW2 method splits the hands into Region Of\nInterests (ROI) and analyzes each of the anatomical ROI separately to estimate\nthe bone age. Because of considering the analysis of local information, the TW2\nmethod shows accurate results in practice. Following the spirit of TW2, we\npropose a novel model called Anatomical Local-Aware Network (ALA-Net) for\nautomatic bone age assessment. In ALA-Net, anatomical local extraction module\nis introduced to learn the hand structure and extract local information.\nMoreover, we design an anatomical patch training strategy to provide extra\nregularization during the training process. Our model can detect the anatomical\nROIs and estimate bone age jointly in an end-to-end manner. The experimental\nresults show that our ALA-Net achieves a new state-of-the-art single model\nperformance of 3.91 mean absolute error (MAE) on the public available RSNA\ndataset. Since the design of our model is well consistent with the well\nrecognized TW2 method, it is interpretable and reliable for clinical usage.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 16:08:30 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Wang", "Dong", ""], ["Zhang", "Kexin", ""], ["Ding", "Jia", ""], ["Wang", "Liwei", ""]]}, {"id": "2005.13471", "submitter": "Ziyu Shu", "authors": "Ziyu Shu, Alireza Entezari", "title": "Gram filtering and sinogram interpolation for pixel-basis in\n  parallel-beam X-ray CT reconstruction", "comments": null, "journal-ref": "ISBI 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key aspect of parallel-beam X-ray CT is forward and back projection, but\nits computational burden continues to be an obstacle for applications. We\npropose a method to improve the performance of related algorithms by\ncalculating the Gram filter exactly and interpolating the sinogram signal\noptimally. In addition, the detector blur effect can be included in our model\nefficiently. The improvements in speed and quality for back projection and\niterative reconstruction are shown in our experiments on both analytical\nphantoms and real CT images.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 16:26:19 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Shu", "Ziyu", ""], ["Entezari", "Alireza", ""]]}, {"id": "2005.13483", "submitter": "Pradeep Reddy Raamana", "authors": "Pradeep Reddy Raamana", "title": "Kernel methods library for pattern analysis and machine learning in\n  python", "comments": "6 pages, 3 code examples, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kernel methods have proven to be powerful techniques for pattern analysis and\nmachine learning (ML) in a variety of domains. However, many of their original\nor advanced implementations remain in Matlab. With the incredible rise and\nadoption of Python in the ML and data science world, there is a clear need for\na well-defined library that enables not only the use of popular kernels, but\nalso allows easy definition of customized kernels to fine-tune them for diverse\napplications. The kernelmethods library fills that important void in the python\nML ecosystem in a domain-agnostic fashion, allowing the sample data type to be\nanything from numerical, categorical, graphs or a combination of them. In\naddition, this library provides a number of well-defined classes to make\nvarious kernel-based operations efficient (for large scale datasets), modular\n(for ease of domain adaptation), and inter-operable (across different\necosystems). The library is available at\nhttps://github.com/raamana/kernelmethods.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 16:44:42 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Raamana", "Pradeep Reddy", ""]]}, {"id": "2005.13531", "submitter": "Michael Kellman", "authors": "Michael Kellman, Michael Lustig, Laura Waller", "title": "How to do Physics-based Learning", "comments": "3 pages, 2 figures, linked repository\n  https://github.com/kellman/physics_based_learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this tutorial is to explain step-by-step how to implement\nphysics-based learning for the rapid prototyping of a computational imaging\nsystem. We provide a basic overview of physics-based learning, the construction\nof a physics-based network, and its reduction to practice. Specifically, we\nadvocate exploiting the auto-differentiation functionality twice, once to build\na physics-based network and again to perform physics-based learning. Thus, the\nuser need only implement the forward model process for their system, speeding\nup prototyping time. We provide an open-source Pytorch implementation of a\nphysics-based network and training procedure for a generic sparse recovery\nproblem\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 17:54:45 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 17:08:12 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Kellman", "Michael", ""], ["Lustig", "Michael", ""], ["Waller", "Laura", ""]]}, {"id": "2005.13532", "submitter": "Aayush Bansal", "authors": "Aayush Bansal, Minh Vo, Yaser Sheikh, Deva Ramanan, Srinivasa\n  Narasimhan", "title": "4D Visualization of Dynamic Events from Unconstrained Multi-View Videos", "comments": "Project Page - http://www.cs.cmu.edu/~aayushb/Open4D/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-driven approach for 4D space-time visualization of dynamic\nevents from videos captured by hand-held multiple cameras. Key to our approach\nis the use of self-supervised neural networks specific to the scene to compose\nstatic and dynamic aspects of an event. Though captured from discrete\nviewpoints, this model enables us to move around the space-time of the event\ncontinuously. This model allows us to create virtual cameras that facilitate:\n(1) freezing the time and exploring views; (2) freezing a view and moving\nthrough time; and (3) simultaneously changing both time and view. We can also\nedit the videos and reveal occluded objects for a given view if it is visible\nin any of the other views. We validate our approach on challenging in-the-wild\nevents captured using up to 15 mobile cameras.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 17:57:19 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Bansal", "Aayush", ""], ["Vo", "Minh", ""], ["Sheikh", "Yaser", ""], ["Ramanan", "Deva", ""], ["Narasimhan", "Srinivasa", ""]]}, {"id": "2005.13580", "submitter": "Patrick Esser", "authors": "Robin Rombach and Patrick Esser and Bj\\\"orn Ommer", "title": "Network-to-Network Translation with Conditional Invertible Neural\n  Networks", "comments": "NeurIPS 2020 (oral). Code at https://github.com/CompVis/net2net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the ever-increasing computational costs of modern machine learning\nmodels, we need to find new ways to reuse such expert models and thus tap into\nthe resources that have been invested in their creation. Recent work suggests\nthat the power of these massive models is captured by the representations they\nlearn. Therefore, we seek a model that can relate between different existing\nrepresentations and propose to solve this task with a conditionally invertible\nnetwork. This network demonstrates its capability by (i) providing generic\ntransfer between diverse domains, (ii) enabling controlled content synthesis by\nallowing modification in other domains, and (iii) facilitating diagnosis of\nexisting representations by translating them into interpretable domains such as\nimages. Our domain transfer network can translate between fixed representations\nwithout having to learn or finetune them. This allows users to utilize various\nexisting domain-specific expert models from the literature that had been\ntrained with extensive computational resources. Experiments on diverse\nconditional image synthesis tasks, competitive image modification results and\nexperiments on image-to-image and text-to-image generation demonstrate the\ngeneric applicability of our approach. For example, we translate between BERT\nand BigGAN, state-of-the-art text and image models to provide text-to-image\ngeneration, which neither of both experts can perform on their own.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 18:14:22 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 20:34:36 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Rombach", "Robin", ""], ["Esser", "Patrick", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2005.13605", "submitter": "Yurun Tian", "authors": "Yurun Tian, Vassileios Balntas, Tony Ng, Axel Barroso-Laguna, Yiannis\n  Demiris, Krystian Mikolajczyk", "title": "D2D: Keypoint Extraction with Describe to Detect Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach that exploits the information\nwithin the descriptor space to propose keypoint locations. Detect then\ndescribe, or detect and describe jointly are two typical strategies for\nextracting local descriptors. In contrast, we propose an approach that inverts\nthis process by first describing and then detecting the keypoint locations. %\nDescribe-to-Detect (D2D) leverages successful descriptor models without the\nneed for any additional training. Our method selects keypoints as salient\nlocations with high information content which is defined by the descriptors\nrather than some independent operators. We perform experiments on multiple\nbenchmarks including image matching, camera localisation, and 3D\nreconstruction. The results indicate that our method improves the matching\nperformance of various descriptors and that it generalises across methods and\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 19:27:46 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Tian", "Yurun", ""], ["Balntas", "Vassileios", ""], ["Ng", "Tony", ""], ["Barroso-Laguna", "Axel", ""], ["Demiris", "Yiannis", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "2005.13638", "submitter": "Sebastian Raschka", "authors": "Zhongjie Yu and Sebastian Raschka", "title": "Looking back to lower-level information in few-shot learning", "comments": "13 pages, 2 figures; fixed typographic errors and added journal ref", "journal-ref": "Information 2020, 11, 345", "doi": "10.3390/info11070345", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are capable of learning new concepts from small numbers of examples.\nIn contrast, supervised deep learning models usually lack the ability to\nextract reliable predictive rules from limited data scenarios when attempting\nto classify new examples. This challenging scenario is commonly known as\nfew-shot learning. Few-shot learning has garnered increased attention in recent\nyears due to its significance for many real-world problems. Recently, new\nmethods relying on meta-learning paradigms combined with graph-based\nstructures, which model the relationship between examples, have shown promising\nresults on a variety of few-shot classification tasks. However, existing work\non few-shot learning is only focused on the feature embeddings produced by the\nlast layer of the neural network. In this work, we propose the utilization of\nlower-level, supporting information, namely the feature embeddings of the\nhidden neural network layers, to improve classifier accuracy. Based on a\ngraph-based meta-learning framework, we develop a method called Looking-Back,\nwhere such lower-level information is used to construct additional graphs for\nlabel propagation in limited data settings. Our experiments on two popular\nfew-shot learning datasets, miniImageNet and tieredImageNet, show that our\nmethod can utilize the lower-level information in the network to improve\nstate-of-the-art classification performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 20:32:13 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 02:32:21 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Yu", "Zhongjie", ""], ["Raschka", "Sebastian", ""]]}, {"id": "2005.13643", "submitter": "Abdul Qayyum", "authors": "Abdul Qayyum, Alain Lalande, Thomas Decourselle, Thibaut Pommier,\n  Alexandre Cochet, Fabrice Meriaudeau", "title": "Segmentation of the Myocardium on Late-Gadolinium Enhanced MRI based on\n  2.5 D Residual Squeeze and Excitation Deep Learning Model", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/4v2lR3Zvsw", "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Cardiac left ventricular (LV) segmentation from short-axis MRI acquired 10\nminutes after the injection of a contrast agent (LGE-MRI) is a necessary step\nin the processing allowing the identification and diagnosis of cardiac diseases\nsuch as myocardial infarction. However, this segmentation is challenging due to\nhigh variability across subjects and the potential lack of contrast between\nstructures. Then, the main objective of this work is to develop an accurate\nautomatic segmentation method based on deep learning models for the myocardial\nborders on LGE-MRI. To this end, 2.5 D residual neural network integrated with\na squeeze and excitation blocks in encoder side with specialized convolutional\nhas been proposed. Late fusion has been used to merge the output of the best\ntrained proposed models from a different set of hyperparameters. A total number\nof 320 exams (with a mean number of 6 slices per exam) were used for training\nand 28 exams used for testing. The performance analysis of the proposed\nensemble model in the basal and middle slices was similar as compared to\nintra-observer study and slightly lower at apical slices. The overall Dice\nscore was 82.01% by our proposed method as compared to Dice score of 83.22%\nobtained from the intra observer study. The proposed model could be used for\nthe automatic segmentation of myocardial border that is a very important step\nfor accurate quantification of no-reflow, myocardial infarction, myocarditis,\nand hypertrophic cardiomyopathy, among others.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 20:44:38 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Qayyum", "Abdul", ""], ["Lalande", "Alain", ""], ["Decourselle", "Thomas", ""], ["Pommier", "Thibaut", ""], ["Cochet", "Alexandre", ""], ["Meriaudeau", "Fabrice", ""]]}, {"id": "2005.13690", "submitter": "Jue Jiang Dr.", "authors": "Hyemin Um, Jue Jiang, Maria Thor, Andreas Rimner, Leo Luo, Joseph O.\n  Deasy, and Harini Veeraraghavan", "title": "Multiple resolution residual network for automatic thoracic\n  organs-at-risk segmentation from CT", "comments": "MIDL 2020 short paper", "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/h3Miqa_jqN", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implemented and evaluated a multiple resolution residual network (MRRN)\nfor multiple normal organs-at-risk (OAR) segmentation from computed tomography\n(CT) images for thoracic radiotherapy treatment (RT) planning. Our approach\nsimultaneously combines feature streams computed at multiple image resolutions\nand feature levels through residual connections. The feature streams at each\nlevel are updated as the images are passed through various feature levels. We\ntrained our approach using 206 thoracic CT scans of lung cancer patients with\n35 scans held out for validation to segment the left and right lungs, heart,\nesophagus, and spinal cord. This approach was tested on 60 CT scans from the\nopen-source AAPM Thoracic Auto-Segmentation Challenge dataset. Performance was\nmeasured using the Dice Similarity Coefficient (DSC). Our approach outperformed\nthe best-performing method in the grand challenge for hard-to-segment\nstructures like the esophagus and achieved comparable results for all other\nstructures. Median DSC using our method was 0.97 (interquartile range [IQR]:\n0.97-0.98) for the left and right lungs, 0.93 (IQR: 0.93-0.95) for the heart,\n0.78 (IQR: 0.76-0.80) for the esophagus, and 0.88 (IQR: 0.86-0.89) for the\nspinal cord.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 22:39:09 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 22:50:43 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Um", "Hyemin", ""], ["Jiang", "Jue", ""], ["Thor", "Maria", ""], ["Rimner", "Andreas", ""], ["Luo", "Leo", ""], ["Deasy", "Joseph O.", ""], ["Veeraraghavan", "Harini", ""]]}, {"id": "2005.13695", "submitter": "Mohammed Ahmed", "authors": "Mohammed Ahmed, Hongbo Du, Alaa AlZoubi", "title": "An ENAS Based Approach for Constructing Deep Learning Models for Breast\n  Cancer Recognition from Ultrasound Images", "comments": "6 pages, 3 figures, Conference: Medical Imaging with Deep Learning\n  2020", "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/GxYt8XnZHM", "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNN) provides an \"end-to-end\" solution\nfor image pattern recognition with impressive performance in many areas of\napplication including medical imaging. Most CNN models of high performance use\nhand-crafted network architectures that require expertise in CNNs to utilise\ntheir potentials. In this paper, we applied the Efficient Neural Architecture\nSearch (ENAS) method to find optimal CNN architectures for classifying breast\nlesions from ultrasound (US) images. Our empirical study with a dataset of 524\nUS images shows that the optimal models generated by using ENAS achieve an\naverage accuracy of 89.3%, surpassing other hand-crafted alternatives.\nFurthermore, the models are simpler in complexity and more efficient. Our study\ndemonstrates that the ENAS approach to CNN model design is a promising\ndirection for classifying ultrasound images of breast lesions.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 22:49:45 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Ahmed", "Mohammed", ""], ["Du", "Hongbo", ""], ["AlZoubi", "Alaa", ""]]}, {"id": "2005.13702", "submitter": "Shahbaz Rezaei", "authors": "Shahbaz Rezaei and Xin Liu", "title": "On the Difficulty of Membership Inference Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies propose membership inference (MI) attacks on deep models,\nwhere the goal is to infer if a sample has been used in the training process.\nDespite their apparent success, these studies only report accuracy, precision,\nand recall of the positive class (member class). Hence, the performance of\nthese attacks have not been clearly reported on negative class (non-member\nclass). In this paper, we show that the way the MI attack performance has been\nreported is often misleading because they suffer from high false positive rate\nor false alarm rate (FAR) that has not been reported. FAR shows how often the\nattack model mislabel non-training samples (non-member) as training (member)\nones. The high FAR makes MI attacks fundamentally impractical, which is\nparticularly more significant for tasks such as membership inference where the\nmajority of samples in reality belong to the negative (non-training) class.\nMoreover, we show that the current MI attack models can only identify the\nmembership of misclassified samples with mediocre accuracy at best, which only\nconstitute a very small portion of training samples.\n  We analyze several new features that have not been comprehensively explored\nfor membership inference before, including distance to the decision boundary\nand gradient norms, and conclude that deep models' responses are mostly similar\namong train and non-train samples. We conduct several experiments on image\nclassification tasks, including MNIST, CIFAR-10, CIFAR-100, and ImageNet, using\nvarious model architecture, including LeNet, AlexNet, ResNet, etc. We show that\nthe current state-of-the-art MI attacks cannot achieve high accuracy and low\nFAR at the same time, even when the attacker is given several advantages.\n  The source code is available at https://github.com/shrezaei/MI-Attack.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 23:09:17 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 00:18:48 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 20:17:22 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Rezaei", "Shahbaz", ""], ["Liu", "Xin", ""]]}, {"id": "2005.13704", "submitter": "Hsin-Min Cheng", "authors": "Hsin-Min Cheng and Dezhen Song", "title": "Graph-based Proprioceptive Localization Using a Discrete Heading-Length\n  Feature Sequence Matching Approach", "comments": "13 pages, 32 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proprioceptive localization refers to a new class of robot egocentric\nlocalization methods that do not rely on the perception and recognition of\nexternal landmarks. These methods are naturally immune to bad weather, poor\nlighting conditions, or other extreme environmental conditions that may hinder\nexteroceptive sensors such as a camera or a laser ranger finder. These methods\ndepend on proprioceptive sensors such as inertial measurement units (IMUs)\nand/or wheel encoders. Assisted by magnetoreception, the sensors can provide a\nrudimentary estimation of vehicle trajectory which is used to query a prior\nknown map to obtain location. Named as graph-based proprioceptive localization\n(GBPL), we provide a low cost fallback solution for localization under\nchallenging environmental conditions. As a robot/vehicle travels, we extract a\nsequence of heading-length values for straight segments from the trajectory and\nmatch the sequence with a pre-processed heading-length graph (HLG) abstracted\nfrom the prior known map to localize the robot under a graph-matching approach.\nUsing the information from HLG, our location alignment and verification module\ncompensates for trajectory drift, wheel slip, or tire inflation level. We have\nimplemented our algorithm and tested it in both simulated and physical\nexperiments. The algorithm runs successfully in finding robot location\ncontinuously and achieves localization accurate at the level that the prior map\nallows (less than 10m).\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 23:10:15 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Cheng", "Hsin-Min", ""], ["Song", "Dezhen", ""]]}, {"id": "2005.13705", "submitter": "Zhuotun Zhu", "authors": "Zhuotun Zhu, Ke Yan, Dakai Jin, Jinzheng Cai, Tsung-Ying Ho, Adam P\n  Harrison, Dazhou Guo, Chun-Hung Chao, Xianghua Ye, Jing Xiao, Alan Yuille,\n  and Le Lu", "title": "Detecting Scatteredly-Distributed, Small, andCritically Important\n  Objects in 3D OncologyImaging via Decision Stratification", "comments": "14 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding and identifying scatteredly-distributed, small, and critically\nimportant objects in 3D oncology images is very challenging. We focus on the\ndetection and segmentation of oncology-significant (or suspicious cancer\nmetastasized) lymph nodes (OSLNs), which has not been studied before as a\ncomputational task. Determining and delineating the spread of OSLNs is\nessential in defining the corresponding resection/irradiating regions for the\ndownstream workflows of surgical resection and radiotherapy of various cancers.\nFor patients who are treated with radiotherapy, this task is performed by\nexperienced radiation oncologists that involves high-level reasoning on whether\nLNs are metastasized, which is subject to high inter-observer variations. In\nthis work, we propose a divide-and-conquer decision stratification approach\nthat divides OSLNs into tumor-proximal and tumor-distal categories. This is\nmotivated by the observation that each category has its own different\nunderlying distributions in appearance, size and other characteristics. Two\nseparate detection-by-segmentation networks are trained per category and fused.\nTo further reduce false positives (FP), we present a novel global-local network\n(GLNet) that combines high-level lesion characteristics with features learned\nfrom localized 3D image patches. Our method is evaluated on a dataset of 141\nesophageal cancer patients with PET and CT modalities (the largest to-date).\nOur results significantly improve the recall from $45\\%$ to $67\\%$ at $3$ FPs\nper patient as compared to previous state-of-the-art methods. The highest\nachieved OSLN recall of $0.828$ is clinically relevant and valuable.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 23:12:11 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Zhu", "Zhuotun", ""], ["Yan", "Ke", ""], ["Jin", "Dakai", ""], ["Cai", "Jinzheng", ""], ["Ho", "Tsung-Ying", ""], ["Harrison", "Adam P", ""], ["Guo", "Dazhou", ""], ["Chao", "Chun-Hung", ""], ["Ye", "Xianghua", ""], ["Xiao", "Jing", ""], ["Yuille", "Alan", ""], ["Lu", "Le", ""]]}, {"id": "2005.13708", "submitter": "Tianyang Xu", "authors": "Tianyang Xu, Zhen-Hua Feng, Xiao-Jun Wu, Josef Kittler", "title": "AFAT: Adaptive Failure-Aware Tracker for Robust Visual Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Siamese approaches have achieved promising performance in visual object\ntracking recently. The key to the success of Siamese trackers is to learn\nappearance-invariant feature embedding functions via pair-wise offline training\non large-scale video datasets. However, the Siamese paradigm uses one-shot\nlearning to model the online tracking task, which impedes online adaptation in\nthe tracking process. Additionally, the uncertainty of an online tracking\nresponse is not measured, leading to the problem of ignoring potential\nfailures. In this paper, we advocate online adaptation in the tracking stage.\nTo this end, we propose a failure-aware system, realised by a Quality\nPrediction Network (QPN), based on convolutional and LSTM modules in the\ndecision stage, enabling online reporting of potential tracking failures.\nSpecifically, sequential response maps from previous successive frames as well\nas current frame are collected to predict the tracking confidence, realising\nspatio-temporal fusion in the decision level. In addition, we further provide\nan Adaptive Failure-Aware Tracker (AFAT) by combing the state-of-the-art\nSiamese trackers with our system. The experimental results obtained on standard\nbenchmarking datasets demonstrate the effectiveness of the proposed\nfailure-aware system and the merits of our AFAT tracker, with outstanding and\nbalanced performance in both accuracy and speed.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 23:21:12 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Xu", "Tianyang", ""], ["Feng", "Zhen-Hua", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "2005.13713", "submitter": "Bo Liu", "authors": "Bo Liu, Hao Kang, Haoxiang Li, Gang Hua, Nuno Vasconcelos", "title": "Few-Shot Open-Set Recognition using Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of open-set recognition is considered. While previous approaches\nonly consider this problem in the context of large-scale classifier training,\nwe seek a unified solution for this and the low-shot classification setting. It\nis argued that the classic softmax classifier is a poor solution for open-set\nrecognition, since it tends to overfit on the training classes. Randomization\nis then proposed as a solution to this problem. This suggests the use of\nmeta-learning techniques, commonly used for few-shot classification, for the\nsolution of open-set recognition. A new oPen sEt mEta LEaRning (PEELER)\nalgorithm is then introduced. This combines the random selection of a set of\nnovel classes per episode, a loss that maximizes the posterior entropy for\nexamples of those classes, and a new metric learning formulation based on the\nMahalanobis distance. Experimental results show that PEELER achieves state of\nthe art open set recognition performance for both few-shot and large-scale\nrecognition. On CIFAR and miniImageNet, it achieves substantial gains in\nseen/unseen class detection AUROC for a given seen-class classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 23:49:26 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 19:15:41 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Liu", "Bo", ""], ["Kang", "Hao", ""], ["Li", "Haoxiang", ""], ["Hua", "Gang", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2005.13734", "submitter": "Satoshi Hashimoto", "authors": "Satoshi Hashimoto, Yonghoon Ji, Kenichi Kudo, Takayuki Takahashi, and\n  Kazunori Umeda", "title": "Anomaly Detection Based on Deep Learning Using Video for Prevention of\n  Industrial Accidents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an anomaly detection method for the prevention of\nindustrial accidents using machine learning technology.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 01:53:57 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Hashimoto", "Satoshi", ""], ["Ji", "Yonghoon", ""], ["Kudo", "Kenichi", ""], ["Takahashi", "Takayuki", ""], ["Umeda", "Kazunori", ""]]}, {"id": "2005.13736", "submitter": "Tunai Porto Marques", "authors": "Tunai Porto Marques, Alexandra Branzan Albu", "title": "L^2UWE: A Framework for the Efficient Enhancement of Low-Light\n  Underwater Images Using Local Contrast and Multi-Scale Fusion", "comments": "Presented on the 2020 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR) Workshop NTIRE: New Trends in Image Restoration\n  and Enhancement. Code and dataset available at:\n  https://github.com/tunai/l2uwe", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition Workshops, 2020, pp. 538-539", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Images captured underwater often suffer from suboptimal illumination settings\nthat can hide important visual features, reducing their quality. We present a\nnovel single-image low-light underwater image enhancer, L^2UWE, that builds on\nour observation that an efficient model of atmospheric lighting can be derived\nfrom local contrast information. We create two distinct models and generate two\nenhanced images from them: one that highlights finer details, the other focused\non darkness removal. A multi-scale fusion process is employed to combine these\nimages while emphasizing regions of higher luminance, saliency and local\ncontrast. We demonstrate the performance of L^2UWE by using seven metrics to\ntest it against seven state-of-the-art enhancement methods specific to\nunderwater and low-light scenes. Code available at:\nhttps://github.com/tunai/l2uwe.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 01:57:32 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 21:26:23 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Marques", "Tunai Porto", ""], ["Albu", "Alexandra Branzan", ""]]}, {"id": "2005.13753", "submitter": "Ke Yan", "authors": "Ke Yan, Jinzheng Cai, Adam P. Harrison, Dakai Jin, Jing Xiao, Le Lu", "title": "Universal Lesion Detection by Learning from Multiple Heterogeneously\n  Labeled Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lesion detection is an important problem within medical imaging analysis.\nMost previous work focuses on detecting and segmenting a specialized category\nof lesions (e.g., lung nodules). However, in clinical practice, radiologists\nare responsible for finding all possible types of anomalies. The task of\nuniversal lesion detection (ULD) was proposed to address this challenge by\ndetecting a large variety of lesions from the whole body. There are multiple\nheterogeneously labeled datasets with varying label completeness: DeepLesion,\nthe largest dataset of 32,735 annotated lesions of various types, but with even\nmore missing annotation instances; and several fully-labeled single-type lesion\ndatasets, such as LUNA for lung nodules and LiTS for liver tumors. In this\nwork, we propose a novel framework to leverage all these datasets together to\nimprove the performance of ULD. First, we learn a multi-head multi-task lesion\ndetector using all datasets and generate lesion proposals on DeepLesion.\nSecond, missing annotations in DeepLesion are retrieved by a new method of\nembedding matching that exploits clinical prior knowledge. Last, we discover\nsuspicious but unannotated lesions using knowledge transfer from single-type\nlesion detectors. In this way, reliable positive and negative regions are\nobtained from partially-labeled and unlabeled images, which are effectively\nutilized to train ULD. To assess the clinically realistic protocol of 3D\nvolumetric ULD, we fully annotated 1071 CT sub-volumes in DeepLesion. Our\nmethod outperforms the current state-of-the-art approach by 29% in the metric\nof average sensitivity.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 02:56:00 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Yan", "Ke", ""], ["Cai", "Jinzheng", ""], ["Harrison", "Adam P.", ""], ["Jin", "Dakai", ""], ["Xiao", "Jing", ""], ["Lu", "Le", ""]]}, {"id": "2005.13759", "submitter": "Yoshihiro Nakano", "authors": "Yoshihiro Nakano", "title": "Stereo Vision Based Single-Shot 6D Object Pose Estimation for\n  Bin-Picking by a Robot Manipulator", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast and accurate method of 6D object pose estimation for\nbin-picking of mechanical parts by a robot manipulator. We extend the\nsingle-shot approach to stereo vision by application of attention architecture.\nOur convolutional neural network model regresses to object locations and\nrotations from either a left image or a right image without depth information.\nThen, a stereo feature matching module, designated as Stereo Grid Attention,\ngenerates stereo grid matching maps. The important point of our method is only\nto calculate disparity of the objects found by the attention from stereo\nimages, instead of calculating a point cloud over the entire image. The\ndisparity value is then used to calculate the depth to the objects by the\nprinciple of triangulation. Our method also achieves a rapid processing speed\nof pose estimation by the single-shot architecture and it is possible to\nprocess a 1024 x 1024 pixels image in 75 milliseconds on the Jetson AGX Xavier\nimplemented with half-float model. Weakly textured mechanical parts are used to\nexemplify the method. First, we create original synthetic datasets for training\nand evaluating of the proposed model. This dataset is created by capturing and\nrendering numerous 3D models of several types of mechanical parts in virtual\nspace. Finally, we use a robotic manipulator with an electromagnetic gripper to\npick up the mechanical parts in a cluttered state to verify the validity of our\nmethod in an actual scene. When a raw stereo image is used by the proposed\nmethod from our stereo camera to detect black steel screws, stainless screws,\nand DC motor parts, i.e., cases, rotor cores and commutator caps, the\nbin-picking tasks are successful with 76.3%, 64.0%, 50.5%, 89.1% and 64.2%\nprobability, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 03:15:20 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Nakano", "Yoshihiro", ""]]}, {"id": "2005.13796", "submitter": "Zejiang Hou", "authors": "Zejiang Hou and Sun-Yuan Kung", "title": "A Feature-map Discriminant Perspective for Pruning Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network pruning has become the de facto tool to accelerate deep neural\nnetworks for mobile and edge applications. Recently, feature-map discriminant\nbased channel pruning has shown promising results, as it aligns well with the\nCNN objective of differentiating multiple classes and offers better\ninterpretability of the pruning decision. However, existing discriminant-based\nmethods are challenged by computation inefficiency, as there is a lack of\ntheoretical guidance on quantifying the feature-map discriminant power. In this\npaper, we present a new mathematical formulation to accurately and efficiently\nquantify the feature-map discriminativeness, which gives rise to a novel\ncriterion,Discriminant Information(DI). We analyze the theoretical property of\nDI, specifically the non-decreasing property, that makes DI a valid selection\ncriterion. DI-based pruning removes channels with minimum influence to DI\nvalue, as they contain little information regarding to the discriminant power.\nThe versatility of DI criterion also enables an intra-layer mixed precision\nquantization to further compress the network. Moreover, we propose a DI-based\ngreedy pruning algorithm and structure distillation technique to automatically\ndecide the pruned structure that satisfies certain resource budget, which is a\ncommon requirement in reality. Extensive experiments demonstratethe\neffectiveness of our method: our pruned ResNet50 on ImageNet achieves 44% FLOPs\nreduction without any Top-1 accuracy loss compared to unpruned model\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 06:25:22 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Hou", "Zejiang", ""], ["Kung", "Sun-Yuan", ""]]}, {"id": "2005.13797", "submitter": "Michael Shin", "authors": "Michael Shin, Eduardo Castillo, Irene Font Peradejordi, Shobhna\n  Jayaraman", "title": "3D human pose estimation with adaptive receptive fields and dilated\n  temporal convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this work, we demonstrate that receptive fields in 3D pose estimation can\nbe effectively specified using optical flow. We introduce adaptive receptive\nfields, a simple and effective method to aid receptive field selection in pose\nestimation models based on optical flow inference. We contrast the performance\nof a benchmark state-of-the-art model running on fixed receptive fields with\ntheir adaptive field counterparts. By using a reduced receptive field, our\nmodel can process slow-motion sequences (10x longer) 23% faster than the\nbenchmark model running at regular speed. The reduction in computational cost\nis achieved while producing a pose prediction accuracy to within 0.36% of the\nbenchmark model.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 06:25:41 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Shin", "Michael", ""], ["Castillo", "Eduardo", ""], ["Peradejordi", "Irene Font", ""], ["Jayaraman", "Shobhna", ""]]}, {"id": "2005.13799", "submitter": "Amitojdeep Singh", "authors": "Amitojdeep Singh, Sourya Sengupta, Vasudevan Lakshminarayanan", "title": "Explainable deep learning models in medical image analysis", "comments": "Preprint submitted to J.Imaging, MDPI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning methods have been very effective for a variety of medical\ndiagnostic tasks and has even beaten human experts on some of those. However,\nthe black-box nature of the algorithms has restricted clinical use. Recent\nexplainability studies aim to show the features that influence the decision of\na model the most. The majority of literature reviews of this area have focused\non taxonomy, ethics, and the need for explanations. A review of the current\napplications of explainable deep learning for different medical imaging tasks\nis presented here. The various approaches, challenges for clinical deployment,\nand the areas requiring further research are discussed here from a practical\nstandpoint of a deep learning researcher designing a system for the clinical\nend-users.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 06:31:05 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Singh", "Amitojdeep", ""], ["Sengupta", "Sourya", ""], ["Lakshminarayanan", "Vasudevan", ""]]}, {"id": "2005.13820", "submitter": "Huaxi Huang", "authors": "Huaxi Huang, Junjie Zhang, Jian Zhang, Qiang Wu, Chang Xu", "title": "TOAN: Target-Oriented Alignment Network for Fine-Grained Image\n  Categorization with Few Labeled Samples", "comments": "T-CSVT Accepted", "journal-ref": null, "doi": "10.1109/TCSVT.2021.3065693", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The challenges of high intra-class variance yet low inter-class fluctuations\nin fine-grained visual categorization are more severe with few labeled samples,\n\\textit{i.e.,} Fine-Grained categorization problems under the Few-Shot setting\n(FGFS). High-order features are usually developed to uncover subtle differences\nbetween sub-categories in FGFS, but they are less effective in handling the\nhigh intra-class variance. In this paper, we propose a Target-Oriented\nAlignment Network (TOAN) to investigate the fine-grained relation between the\ntarget query image and support classes. The feature of each support image is\ntransformed to match the query ones in the embedding feature space, which\nreduces the disparity explicitly within each category. Moreover, different from\nexisting FGFS approaches devise the high-order features over the global image\nwith less explicit consideration of discriminative parts, we generate\ndiscriminative fine-grained features by integrating compositional concept\nrepresentations to global second-order pooling. Extensive experiments are\nconducted on four fine-grained benchmarks to demonstrate the effectiveness of\nTOAN compared with the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 07:48:44 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 05:40:46 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Huang", "Huaxi", ""], ["Zhang", "Junjie", ""], ["Zhang", "Jian", ""], ["Wu", "Qiang", ""], ["Xu", "Chang", ""]]}, {"id": "2005.13826", "submitter": "Weiran Huang", "authors": "Aoxue Li and Weiran Huang and Xu Lan and Jiashi Feng and Zhenguo Li\n  and Liwei Wang", "title": "Boosting Few-Shot Learning With Adaptive Margin Loss", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning (FSL) has attracted increasing attention in recent years\nbut remains challenging, due to the intrinsic difficulty in learning to\ngeneralize from a few examples. This paper proposes an adaptive margin\nprinciple to improve the generalization ability of metric-based meta-learning\napproaches for few-shot learning problems. Specifically, we first develop a\nclass-relevant additive margin loss, where semantic similarity between each\npair of classes is considered to separate samples in the feature embedding\nspace from similar classes. Further, we incorporate the semantic context among\nall classes in a sampled training task and develop a task-relevant additive\nmargin loss to better distinguish samples from different classes. Our adaptive\nmargin method can be easily extended to a more realistic generalized FSL\nsetting. Extensive experiments demonstrate that the proposed method can boost\nthe performance of current metric-based meta-learning approaches, under both\nthe standard FSL and generalized FSL settings.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 07:58:41 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Li", "Aoxue", ""], ["Huang", "Weiran", ""], ["Lan", "Xu", ""], ["Feng", "Jiashi", ""], ["Li", "Zhenguo", ""], ["Wang", "Liwei", ""]]}, {"id": "2005.13862", "submitter": "Jan Kristanto Wibisono", "authors": "Jan Kristanto Wibisono and Hsueh-Ming Hang", "title": "Traditional Method Inspired Deep Neural Network for Edge Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Deep-Neural-Network (DNN) based edge prediction is progressing\nfast. Although the DNN based schemes outperform the traditional edge detectors,\nthey have much higher computational complexity. It could be that the DNN based\nedge detectors often adopt the neural net structures designed for high-level\ncomputer vision tasks, such as image segmentation and object recognition. Edge\ndetection is a rather local and simple job, the over-complicated architecture\nand massive parameters may be unnecessary. Therefore, we propose a traditional\nmethod inspired framework to produce good edges with minimal complexity. We\nsimplify the network architecture to include Feature Extractor, Enrichment, and\nSummarizer, which roughly correspond to gradient, low pass filter, and pixel\nconnection in the traditional edge detection schemes. The proposed structure\ncan effectively reduce the complexity and retain the edge prediction quality.\nOur TIN2 (Traditional Inspired Network) model has an accuracy higher than the\nrecent BDCN2 (Bi-Directional Cascade Network) but with a smaller model.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 09:20:37 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Wibisono", "Jan Kristanto", ""], ["Hang", "Hsueh-Ming", ""]]}, {"id": "2005.13867", "submitter": "Mao Ye", "authors": "Chenpeng Zhang (1), Shuai Li (2), Mao Ye (1), Ce Zhu (2), Xue Li (3)\n  ((1) School of Computer Science and Engineering, University of Electronic\n  Science and Technology of China, (2) School of Information and Communication\n  Engineering, University of Electronic Science and Technology of China, (3)\n  School of Information Technology and Electronic Engineering, The University\n  of Queensland)", "title": "Learning Various Length Dependence by Dual Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are widely used as a memory model for\nsequence-related problems. Many variants of RNN have been proposed to solve the\ngradient problems of training RNNs and process long sequences. Although some\nclassical models have been proposed, capturing long-term dependence while\nresponding to short-term changes remains a challenge. To this problem, we\npropose a new model named Dual Recurrent Neural Networks (DuRNN). The DuRNN\nconsists of two parts to learn the short-term dependence and progressively\nlearn the long-term dependence. The first part is a recurrent neural network\nwith constrained full recurrent connections to deal with short-term dependence\nin sequence and generate short-term memory. Another part is a recurrent neural\nnetwork with independent recurrent connections which helps to learn long-term\ndependence and generate long-term memory. A selection mechanism is added\nbetween two parts to help the needed long-term information transfer to the\nindependent neurons. Multiple modules can be stacked to form a multi-layer\nmodel for better performance. Our contributions are: 1) a new recurrent model\ndeveloped based on the divide-and-conquer strategy to learn long and short-term\ndependence separately, and 2) a selection mechanism to enhance the separating\nand learning of different temporal scales of dependence. Both theoretical\nanalysis and extensive experiments are conducted to validate the performance of\nour model, and we also conduct simple visualization experiments and ablation\nanalyses for the model interpretability. Experimental results indicate that the\nproposed DuRNN model can handle not only very long sequences (over 5000 time\nsteps), but also short sequences very well. Compared with many state-of-the-art\nRNN models, our model has demonstrated efficient and better performance.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 09:30:01 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Zhang", "Chenpeng", ""], ["Li", "Shuai", ""], ["Ye", "Mao", ""], ["Zhu", "Ce", ""], ["Li", "Xue", ""]]}, {"id": "2005.13884", "submitter": "Zhenghao Shi", "authors": "Zhaorun Zhou, Zhenghao Shi, Mingtao Guo, Yaning Feng, Minghua Zhao", "title": "CGGAN: A Context Guided Generative Adversarial Network For Single Image\n  Dehazing", "comments": "12 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image haze removal is highly desired for the application of computer vision.\nThis paper proposes a novel Context Guided Generative Adversarial Network\n(CGGAN) for single image dehazing. Of which, an novel new encoder-decoder is\nemployed as the generator. And it consists of a feature-extraction-net, a\ncontext-extractionnet, and a fusion-net in sequence. The feature extraction-net\nacts as a encoder, and is used for extracting haze features. The\ncontext-extraction net is a multi-scale parallel pyramid decoder, and is used\nfor extracting the deep features of the encoder and generating coarse dehazing\nimage. The fusion-net is a decoder, and is used for obtaining the final\nhaze-free image. To obtain more better results, multi-scale information\nobtained during the decoding process of the context extraction decoder is used\nfor guiding the fusion decoder. By introducing an extra coarse decoder to the\noriginal encoder-decoder, the CGGAN can make better use of the deep feature\ninformation extracted by the encoder. To ensure our CGGAN work effectively for\ndifferent haze scenarios, different loss functions are employed for the two\ndecoders. Experiments results show the advantage and the effectiveness of our\nproposed CGGAN, evidential improvements over existing state-of-the-art methods\nare obtained.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 10:14:30 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Zhou", "Zhaorun", ""], ["Shi", "Zhenghao", ""], ["Guo", "Mingtao", ""], ["Feng", "Yaning", ""], ["Zhao", "Minghua", ""]]}, {"id": "2005.13888", "submitter": "Haozhe Qi", "authors": "Haozhe Qi, Chen Feng, Zhiguo Cao, Feng Zhao, and Yang Xiao", "title": "P2B: Point-to-Box Network for 3D Object Tracking in Point Clouds", "comments": "Accepted by CVPR 2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Towards 3D object tracking in point clouds, a novel point-to-box network\ntermed P2B is proposed in an end-to-end learning manner. Our main idea is to\nfirst localize potential target centers in 3D search area embedded with target\ninformation. Then point-driven 3D target proposal and verification are executed\njointly. In this way, the time-consuming 3D exhaustive search can be avoided.\nSpecifically, we first sample seeds from the point clouds in template and\nsearch area respectively. Then, we execute permutation-invariant feature\naugmentation to embed target clues from template into search area seeds and\nrepresent them with target-specific features. Consequently, the augmented\nsearch area seeds regress the potential target centers via Hough voting. The\ncenters are further strengthened with seed-wise targetness scores. Finally,\neach center clusters its neighbors to leverage the ensemble power for joint 3D\ntarget proposal and verification. We apply PointNet++ as our backbone and\nexperiments on KITTI tracking dataset demonstrate P2B's superiority (~10%'s\nimprovement over state-of-the-art). Note that P2B can run with 40FPS on a\nsingle NVIDIA 1080Ti GPU. Our code and model are available at\nhttps://github.com/HaozheQi/P2B.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 10:25:12 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Qi", "Haozhe", ""], ["Feng", "Chen", ""], ["Cao", "Zhiguo", ""], ["Zhao", "Feng", ""], ["Xiao", "Yang", ""]]}, {"id": "2005.13899", "submitter": "Alexandr A. Kalinin", "authors": "Tatiana Gabruseva, Dmytro Poplavskiy, Alexandr A. Kalinin", "title": "Deep Learning for Automatic Pneumonia Detection", "comments": "to appear in CVPR 2020 Workshops proceedings", "journal-ref": "2020 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW), Seattle, WA, USA, 2020, pp. 1436-1443", "doi": "10.1109/CVPRW50498.2020.00183", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pneumonia is the leading cause of death among young children and one of the\ntop mortality causes worldwide. The pneumonia detection is usually performed\nthrough examine of chest X-ray radiograph by highly-trained specialists. This\nprocess is tedious and often leads to a disagreement between radiologists.\nComputer-aided diagnosis systems showed the potential for improving diagnostic\naccuracy. In this work, we develop the computational approach for pneumonia\nregions detection based on single-shot detectors, squeeze-and-excitation deep\nconvolution neural networks, augmentations and multi-task learning. The\nproposed approach was evaluated in the context of the Radiological Society of\nNorth America Pneumonia Detection Challenge, achieving one of the best results\nin the challenge.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 10:54:34 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Gabruseva", "Tatiana", ""], ["Poplavskiy", "Dmytro", ""], ["Kalinin", "Alexandr A.", ""]]}, {"id": "2005.13924", "submitter": "Ferdaous Idlahcen", "authors": "Ferdaous Idlahcen, Mohammed Majid Himmi, Abdelhak Mahmoudi", "title": "CNN-based Approach for Cervical Cancer Classification in Whole-Slide\n  Histopathology Images", "comments": "Presented at the ICLR 2020 Workshop on AI for Overcoming Global\n  Disparities in Cancer Care (AI4CC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cervical cancer will cause 460 000 deaths per year by 2040, approximately 90%\nare Sub-Saharan African women. A constantly increasing incidence in Africa\nmaking cervical cancer a priority by the World Health Organization (WHO) in\nterms of screening, diagnosis, and treatment. Conventionally, cancer diagnosis\nrelies primarily on histopathological assessment, a deeply error-prone\nprocedure requiring intelligent computer-aided systems as low-cost patient\nsafety mechanisms but lack of labeled data in digital pathology limits their\napplicability. In this study, few cervical tissue digital slides from TCGA data\nportal were pre-processed to overcome whole-slide images obstacles and included\nin our proposed VGG16-CNN classification approach. Our results achieved an\naccuracy of 98,26% and an F1-score of 97,9%, which confirm the potential of\ntransfer learning on this weakly-supervised task.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 11:45:23 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Idlahcen", "Ferdaous", ""], ["Himmi", "Mohammed Majid", ""], ["Mahmoudi", "Abdelhak", ""]]}, {"id": "2005.13928", "submitter": "Carles Sanchez Ramos", "authors": "D. Gil, K. D\\'iaz-Chito, C. S\\'anchez, A. Hern\\'andez-Sabat\\'e", "title": "Early Screening of SARS-CoV-2 by Intelligent Analysis of X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future SARS-CoV-2 virus outbreak COVID-XX might possibly occur during the\nnext years. However the pathology in humans is so recent that many clinical\naspects, like early detection of complications, side effects after recovery or\nearly screening, are currently unknown. In spite of the number of cases of\nCOVID-19, its rapid spread putting many sanitary systems in the edge of\ncollapse has hindered proper collection and analysis of the data related to\nCOVID-19 clinical aspects. We describe an interdisciplinary initiative that\nintegrates clinical research, with image diagnostics and the use of new\ntechnologies such as artificial intelligence and radiomics with the aim of\nclarifying some of SARS-CoV-2 open questions. The whole initiative addresses 3\nmain points: 1) collection of standardize data including images, clinical data\nand analytics; 2) COVID-19 screening for its early diagnosis at primary care\ncenters; 3) define radiomic signatures of COVID-19 evolution and associated\npathologies for the early treatment of complications. In particular, in this\npaper we present a general overview of the project, the experimental design and\nfirst results of X-ray COVID-19 detection using a classic approach based on HoG\nand feature selection. Our experiments include a comparison to some recent\nmethods for COVID-19 screening in X-Ray and an exploratory analysis of the\nfeasibility of X-Ray COVID-19 screening. Results show that classic approaches\ncan outperform deep-learning methods in this experimental setting, indicate the\nfeasibility of early COVID-19 screening and that non-COVID infiltration is the\ngroup of patients most similar to COVID-19 in terms of radiological description\nof X-ray. Therefore, an efficient COVID-19 screening should be complemented\nwith other clinical data to better discriminate these cases.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 11:46:31 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Gil", "D.", ""], ["D\u00edaz-Chito", "K.", ""], ["S\u00e1nchez", "C.", ""], ["Hern\u00e1ndez-Sabat\u00e9", "A.", ""]]}, {"id": "2005.13934", "submitter": "Ronny Hug", "authors": "Ronny Hug, Stefan Becker, Wolfgang H\\\"ubner, Michael Arens", "title": "Quantifying the Complexity of Standard Benchmarking Datasets for\n  Long-Term Human Trajectory Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods to quantify the complexity of trajectory datasets are still a missing\npiece in benchmarking human trajectory prediction models. In order to gain a\nbetter understanding of the complexity of trajectory prediction tasks and\nfollowing the intuition, that more complex datasets contain more information,\nan approach for quantifying the amount of information contained in a dataset\nfrom a prototype-based dataset representation is proposed. The dataset\nrepresentation is obtained by first employing a non-trivial spatial sequence\nalignment, which enables a subsequent learning vector quantization (LVQ) stage.\nA large-scale complexity analysis is conducted on several human trajectory\nprediction benchmarking datasets, followed by a brief discussion on indications\nfor human trajectory prediction and benchmarking.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 12:00:41 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 12:20:58 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2021 12:47:53 GMT"}, {"version": "v4", "created": "Thu, 20 May 2021 08:17:40 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Hug", "Ronny", ""], ["Becker", "Stefan", ""], ["H\u00fcbner", "Wolfgang", ""], ["Arens", "Michael", ""]]}, {"id": "2005.13947", "submitter": "Mao Ye", "authors": "Lihua Zhou, Mao Ye, Xinpeng Li, Ce Zhu, Yiguang Liu, and Xue Li", "title": "Disentanglement Then Reconstruction: Learning Compact Features for\n  Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works in domain adaptation always learn domain invariant features to\nmitigate the gap between the source and target domains by adversarial methods.\nThe category information are not sufficiently used which causes the learned\ndomain invariant features are not enough discriminative. We propose a new\ndomain adaptation method based on prototype construction which likes capturing\ndata cluster centers. Specifically, it consists of two parts: disentanglement\nand reconstruction. First, the domain specific features and domain invariant\nfeatures are disentangled from the original features. At the same time, the\ndomain prototypes and class prototypes of both domains are estimated. Then, a\nreconstructor is trained by reconstructing the original features from the\ndisentangled domain invariant features and domain specific features. By this\nreconstructor, we can construct prototypes for the original features using\nclass prototypes and domain prototypes correspondingly. In the end, the feature\nextraction network is forced to extract features close to these prototypes. Our\ncontribution lies in the technical use of the reconstructor to obtain the\noriginal feature prototypes which helps to learn compact and discriminant\nfeatures. As far as we know, this idea is proposed for the first time.\nExperiment results on several public datasets confirm the state-of-the-art\nperformance of our method.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 12:30:12 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Zhou", "Lihua", ""], ["Ye", "Mao", ""], ["Li", "Xinpeng", ""], ["Zhu", "Ce", ""], ["Liu", "Yiguang", ""], ["Li", "Xue", ""]]}, {"id": "2005.13956", "submitter": "Mao Ye", "authors": "Xinpeng Li", "title": "Improving Generalized Zero-Shot Learning by Semantic Discriminator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a recognized fact that the classification accuracy of unseen classes in\nthe setting of Generalized Zero-Shot Learning (GZSL) is much lower than that of\ntraditional Zero-Shot Leaning (ZSL). One of the reasons is that an instance is\nalways misclassified to the wrong domain. Here we refer to the seen and unseen\nclasses as two domains respectively. We propose a new approach to distinguish\nwhether the instances come from the seen or unseen classes. First the visual\nfeature of instance is projected into the semantic space. Then the absolute\nnorm difference between the projected semantic vector and the class semantic\nembedding vector, and the minimum distance between the projected semantic\nvectors and the semantic embedding vectors of the seen classes are used as\ndiscrimination basis. This approach is termed as SD (Semantic Discriminator)\nbecause domain judgement of instance is performed in the semantic space. Our\napproach can be combined with any existing ZSL method and fully supervision\nclassification model to form a new GZSL method. Furthermore, our approach is\nvery simple and does not need any fixed parameters.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 12:48:38 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 14:43:10 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Li", "Xinpeng", ""]]}, {"id": "2005.13982", "submitter": "AKMMahbubur Rahman", "authors": "AKMMahbubur Rahman, ASM Iftekhar Anam, and Mohammed Yeasin", "title": "Robust Modeling of Epistemic Mental States", "comments": "Accepted for Publication in Multimedia Tools and Application, Special\n  Issue: Socio-Affective Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work identifies and advances some research challenges in the analysis of\nfacial features and their temporal dynamics with epistemic mental states in\ndyadic conversations. Epistemic states are: Agreement, Concentration,\nThoughtful, Certain, and Interest. In this paper, we perform a number of\nstatistical analyses and simulations to identify the relationship between\nfacial features and epistemic states. Non-linear relations are found to be more\nprevalent, while temporal features derived from original facial features have\ndemonstrated a strong correlation with intensity changes. Then, we propose a\nnovel prediction framework that takes facial features and their nonlinear\nrelation scores as input and predict different epistemic states in videos. The\nprediction of epistemic states is boosted when the classification of emotion\nchanging regions such as rising, falling, or steady-state are incorporated with\nthe temporal features. The proposed predictive models can predict the epistemic\nstates with significantly improved accuracy: correlation coefficient (CoERR)\nfor Agreement is 0.827, for Concentration 0.901, for Thoughtful 0.794, for\nCertain 0.854, and for Interest 0.913.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 13:34:45 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Rahman", "AKMMahbubur", ""], ["Anam", "ASM Iftekhar", ""], ["Yeasin", "Mohammed", ""]]}, {"id": "2005.13983", "submitter": "Weixia Zhang", "authors": "Weixia Zhang and Kede Ma and Guangtao Zhai and Xiaokang Yang", "title": "Uncertainty-Aware Blind Image Quality Assessment in the Laboratory and\n  Wild", "comments": "Accepted to IEEE TIP. The implementations are available at\n  https://github.com/zwx8981/UNIQUE", "journal-ref": null, "doi": "10.1109/TIP.2021.3061932", "report-no": null, "categories": "cs.CV cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of blind image quality assessment (BIQA) models has been\nsignificantly boosted by end-to-end optimization of feature engineering and\nquality regression. Nevertheless, due to the distributional shift between\nimages simulated in the laboratory and captured in the wild, models trained on\ndatabases with synthetic distortions remain particularly weak at handling\nrealistic distortions (and vice versa). To confront the\ncross-distortion-scenario challenge, we develop a \\textit{unified} BIQA model\nand an approach of training it for both synthetic and realistic distortions. We\nfirst sample pairs of images from individual IQA databases, and compute a\nprobability that the first image of each pair is of higher quality. We then\nemploy the fidelity loss to optimize a deep neural network for BIQA over a\nlarge number of such image pairs. We also explicitly enforce a hinge constraint\nto regularize uncertainty estimation during optimization. Extensive experiments\non six IQA databases show the promise of the learned method in blindly\nassessing image quality in the laboratory and wild. In addition, we demonstrate\nthe universality of the proposed training strategy by using it to improve\nexisting BIQA models.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 13:35:23 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 05:12:39 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2020 09:32:54 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2020 08:27:23 GMT"}, {"version": "v5", "created": "Mon, 22 Feb 2021 15:46:10 GMT"}, {"version": "v6", "created": "Tue, 23 Feb 2021 09:45:41 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhang", "Weixia", ""], ["Ma", "Kede", ""], ["Zhai", "Guangtao", ""], ["Yang", "Xiaokang", ""]]}, {"id": "2005.13986", "submitter": "Varun Murali", "authors": "Igor Spasojevic, Varun Murali, and Sertac Karaman", "title": "Perception-aware time optimal path parameterization for quadrotors", "comments": "Accepted to appear at ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of quadrotors has given rise to a class of\npredominantly vision-driven vehicles. This paper addresses the problem of\nperception-aware time optimal path parametrization for quadrotors. Although\nmany different choices of perceptual modalities are available, the low weight\nand power budgets of quadrotor systems makes a camera ideal for on-board\nnavigation and estimation algorithms. However, this does come with a set of\nchallenges. The limited field of view of the camera can restrict the visibility\nof salient regions in the environment, which dictates the necessity to consider\nperception and planning jointly. The main contribution of this paper is an\nefficient time optimal path parametrization algorithm for quadrotors with\nlimited field of view constraints. We show in a simulation study that a\nstate-of-the-art controller can track planned trajectories, and we validate the\nproposed algorithm on a quadrotor platform in experiments.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 13:40:07 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Spasojevic", "Igor", ""], ["Murali", "Varun", ""], ["Karaman", "Sertac", ""]]}, {"id": "2005.14017", "submitter": "William Le", "authors": "William Le, Francisco Perdig\\'on Romero, Samuel Kadoury", "title": "A Normalized Fully Convolutional Approach to Head and Neck Cancer\n  Outcome Prediction", "comments": "6 pages, 1 figure, 1 table, Medical Imaging with Deep Learning 2020\n  conference", "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/JojEzQ3E5n", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical imaging, radiological scans of different modalities serve to\nenhance different sets of features for clinical diagnosis and treatment\nplanning. This variety enriches the source information that could be used for\noutcome prediction. Deep learning methods are particularly well-suited for\nfeature extraction from high-dimensional inputs such as images. In this work,\nwe apply a CNN classification network augmented with a FCN preprocessor\nsub-network to a public TCIA head and neck cancer dataset. The training goal is\nsurvival prediction of radiotherapy cases based on pre-treatment FDG PET-CT\nscans, acquired across 4 different hospitals. We show that the preprocessor\nsub-network in conjunction with aggregated residual connection leads to\nimprovements over state-of-the-art results when combining both CT and PET input\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 14:00:47 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 14:31:06 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Le", "William", ""], ["Romero", "Francisco Perdig\u00f3n", ""], ["Kadoury", "Samuel", ""]]}, {"id": "2005.14036", "submitter": "Kalliopi Basioti", "authors": "Kalliopi Basioti, George V. Moustakides", "title": "Image Restoration from Parametric Transformations using Generative\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When images are statistically described by a generative model we can use this\ninformation to develop optimum techniques for various image restoration\nproblems as inpainting, super-resolution, image coloring, generative model\ninversion, etc. With the help of the generative model it is possible to\nformulate, in a natural way, these restoration problems as Statistical\nestimation problems. Our approach, by combining maximum a-posteriori\nprobability with maximum likelihood estimation, is capable of restoring images\nthat are distorted by transformations even when the latter contain unknown\nparameters. The resulting optimization is completely defined with no parameters\nrequiring tuning. This must be compared with the current state of the art which\nrequires exact knowledge of the transformations and contains regularizer terms\nwith weights that must be properly defined. Finally, we must mention that we\nextend our method to accommodate mixtures of multiple images where each image\nis described by its own generative model and we are able of successfully\nseparating each participating image from a single mixture.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 01:14:40 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 12:09:41 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Basioti", "Kalliopi", ""], ["Moustakides", "George V.", ""]]}, {"id": "2005.14107", "submitter": "Mattias Heinrich", "authors": "Mattias P Heinrich and Lasse Hansen", "title": "Unsupervised learning of multimodal image registration using domain\n  adaptation with projected Earth Move's discrepancies", "comments": "Medical Imaging with Deep Learning (accepted short paper)\n  https://openreview.net/forum?id=wbZM-DcJB9", "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/wbZM-DcJB9", "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal image registration is a very challenging problem for deep learning\napproaches. Most current work focuses on either supervised learning that\nrequires labelled training scans and may yield models that bias towards\nannotated structures or unsupervised approaches that are based on hand-crafted\nsimilarity metrics and may therefore not outperform their classical non-trained\ncounterparts. We believe that unsupervised domain adaptation can be beneficial\nin overcoming the current limitations for multimodal registration, where good\nmetrics are hard to define. Domain adaptation has so far been mainly limited to\nclassification problems. We propose the first use of unsupervised domain\nadaptation for discrete multimodal registration. Based on a source domain for\nwhich quantised displacement labels are available as supervision, we transfer\nthe output distribution of the network to better resemble the target domain\n(other modality) using classifier discrepancies. To improve upon the sliced\nWasserstein metric for 2D histograms, we present a novel approximation that\nprojects predictions into 1D and computes the L1 distance of their cumulative\nsums. Our proof-of-concept demonstrates the applicability of domain transfer\nfrom mono- to multimodal (multi-contrast) 2D registration of canine MRI scans\nand improves the registration accuracy from 33% (using sliced Wasserstein) to\n44%.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 15:57:21 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Heinrich", "Mattias P", ""], ["Hansen", "Lasse", ""]]}, {"id": "2005.14136", "submitter": "Antonyo Musabini", "authors": "Antonyo Musabini, Mounsif Chetitah", "title": "Heatmap-Based Method for Estimating Drivers' Cognitive Distraction", "comments": "Accepted at IEEE ICCI*CC 2020 (matching camera-ready version)", "journal-ref": null, "doi": "10.1109/ICCICC50026.2020.9450216", "report-no": null, "categories": "cs.HC cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to increase road safety, among the visual and manual distractions,\nmodern intelligent vehicles need also to detect cognitive distracted driving\n(i.e., the drivers mind wandering). In this study, the influence of cognitive\nprocesses on the drivers gaze behavior is explored. A novel image-based\nrepresentation of the driver's eye-gaze dispersion is proposed to estimate\ncognitive distraction. Data are collected on open highway roads, with a\ntailored protocol to create cognitive distraction. The visual difference of\ncreated shapes shows that a driver explores a wider area in neutral driving\ncompared to distracted driving. Thus, support vector machine (SVM)-based\nclassifiers are trained, and 85.2% of accuracy is achieved for a two-class\nproblem, even with a small dataset. Thus, the proposed method has the\ndiscriminative power to recognize cognitive distraction using gaze information.\nFinally, this work details how this image-based representation could be useful\nfor other cases of distracted driving detection.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 16:37:30 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 16:47:18 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Musabini", "Antonyo", ""], ["Chetitah", "Mounsif", ""]]}, {"id": "2005.14137", "submitter": "Huichen Li", "authors": "Huichen Li, Xiaojun Xu, Xiaolu Zhang, Shuang Yang, Bo Li", "title": "QEBA: Query-Efficient Boundary-Based Blackbox Attack", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML), especially deep neural networks (DNNs) have been\nwidely used in various applications, including several safety-critical ones\n(e.g. autonomous driving). As a result, recent research about adversarial\nexamples has raised great concerns. Such adversarial attacks can be achieved by\nadding a small magnitude of perturbation to the input to mislead model\nprediction. While several whitebox attacks have demonstrated their\neffectiveness, which assume that the attackers have full access to the machine\nlearning models; blackbox attacks are more realistic in practice. In this\npaper, we propose a Query-Efficient Boundary-based blackbox Attack (QEBA) based\nonly on model's final prediction labels. We theoretically show why previous\nboundary-based attack with gradient estimation on the whole gradient space is\nnot efficient in terms of query numbers, and provide optimality analysis for\nour dimension reduction-based gradient estimation. On the other hand, we\nconducted extensive experiments on ImageNet and CelebA datasets to evaluate\nQEBA. We show that compared with the state-of-the-art blackbox attacks, QEBA is\nable to use a smaller number of queries to achieve a lower magnitude of\nperturbation with 100% attack success rate. We also show case studies of\nattacks on real-world APIs including MEGVII Face++ and Microsoft Azure.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 16:41:12 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Li", "Huichen", ""], ["Xu", "Xiaojun", ""], ["Zhang", "Xiaolu", ""], ["Yang", "Shuang", ""], ["Li", "Bo", ""]]}, {"id": "2005.14140", "submitter": "Oliver Rippel", "authors": "Oliver Rippel, Patrick Mertens, Dorit Merhof", "title": "Modeling the Distribution of Normal Data in Pre-Trained Deep Features\n  for Anomaly Detection", "comments": "Camera-ready for ICPR2020 (8 pages + 4 pages appendix). First two\n  authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly Detection (AD) in images is a fundamental computer vision problem and\nrefers to identifying images and image substructures that deviate significantly\nfrom the norm. Popular AD algorithms commonly try to learn a model of normality\nfrom scratch using task specific datasets, but are limited to semi-supervised\napproaches employing mostly normal data due to the inaccessibility of anomalies\non a large scale combined with the ambiguous nature of anomaly appearance.\n  We follow an alternative approach and demonstrate that deep feature\nrepresentations learned by discriminative models on large natural image\ndatasets are well suited to describe normality and detect even subtle anomalies\nin a transfer learning setting. Our model of normality is established by\nfitting a multivariate Gaussian (MVG) to deep feature representations of\nclassification networks trained on ImageNet using normal data only. By\nsubsequently applying the Mahalanobis distance as the anomaly score we\noutperform the current state of the art on the public MVTec AD dataset,\nachieving an AUROC value of $95.8 \\pm 1.2$ (mean $\\pm$ SEM) over all 15\nclasses. We further investigate why the learned representations are\ndiscriminative to the AD task using Principal Component Analysis. We find that\nthe principal components containing little variance in normal data are the ones\ncrucial for discriminating between normal and anomalous instances. This gives a\npossible explanation to the often sub-par performance of AD approaches trained\nfrom scratch using normal data only. By selectively fitting a MVG to these most\nrelevant components only, we are able to further reduce model complexity while\nretaining AD performance. We also investigate setting the working point by\nselecting acceptable False Positive Rate thresholds based on the MVG\nassumption.\n  Code available at https://github.com/ORippler/gaussian-ad-mvtec\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 16:43:41 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 13:51:36 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Rippel", "Oliver", ""], ["Mertens", "Patrick", ""], ["Merhof", "Dorit", ""]]}, {"id": "2005.14169", "submitter": "Longlong Jing", "authors": "Longlong Jing, Yucheng Chen, Ling Zhang, Mingyi He, Yingli Tian", "title": "Self-supervised Modal and View Invariant Feature Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing self-supervised feature learning methods for 3D data\neither learn 3D features from point cloud data or from multi-view images. By\nexploring the inherent multi-modality attributes of 3D objects, in this paper,\nwe propose to jointly learn modal-invariant and view-invariant features from\ndifferent modalities including image, point cloud, and mesh with heterogeneous\nnetworks for 3D data. In order to learn modal- and view-invariant features, we\npropose two types of constraints: cross-modal invariance constraint and\ncross-view invariant constraint. Cross-modal invariance constraint forces the\nnetwork to maximum the agreement of features from different modalities for same\nobjects, while the cross-view invariance constraint forces the network to\nmaximum agreement of features from different views of images for same objects.\nThe quality of learned features has been tested on different downstream tasks\nwith three modalities of data including point cloud, multi-view images, and\nmesh. Furthermore, the invariance cross different modalities and views are\nevaluated with the cross-modal retrieval task. Extensive evaluation results\ndemonstrate that the learned features are robust and have strong\ngeneralizability across different tasks.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 17:35:14 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Jing", "Longlong", ""], ["Chen", "Yucheng", ""], ["Zhang", "Ling", ""], ["He", "Mingyi", ""], ["Tian", "Yingli", ""]]}, {"id": "2005.14214", "submitter": "Saikat Dutta", "authors": "Saikat Dutta", "title": "Depth-aware Blending of Smoothed Images for Bokeh Effect Generation", "comments": null, "journal-ref": "Journal of Visual Communication and Image Representation 2021", "doi": "10.1016/j.jvcir.2021.103089", "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bokeh effect is used in photography to capture images where the closer\nobjects look sharp and every-thing else stays out-of-focus. Bokeh photos are\ngenerally captured using Single Lens Reflex cameras using shallow\ndepth-of-field. Most of the modern smartphones can take bokeh images by\nleveraging dual rear cameras or a good auto-focus hardware. However, for\nsmartphones with single-rear camera without a good auto-focus hardware, we have\nto rely on software to generate bokeh images. This kind of system is also\nuseful to generate bokeh effect in already captured images. In this paper, an\nend-to-end deep learning framework is proposed to generate high-quality bokeh\neffect from images. The original image and different versions of smoothed\nimages are blended to generate Bokeh effect with the help of a monocular depth\nestimation network. The proposed approach is compared against a saliency\ndetection based baseline and a number of approaches proposed in AIM 2019\nChallenge on Bokeh Effect Synthesis. Extensive experiments are shown in order\nto understand different parts of the proposed algorithm. The network is\nlightweight and can process an HD image in 0.03 seconds. This approach ranked\nsecond in AIM 2019 Bokeh effect challenge-Perceptual Track.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 18:11:05 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dutta", "Saikat", ""]]}, {"id": "2005.14229", "submitter": "Celso A M Lopes Junior", "authors": "Celso A. M. Lopes Junior, Matheus Henrique M. da Silva, Byron Leite\n  Dantas Bezerra, Bruno Jose Torres Fernandes, and Donato Impedovo", "title": "FCN+RL: A Fully Convolutional Network followed by Refinement Layers to\n  Offline Handwritten Signature Segmentation", "comments": "7 pages, 6 figures, Accepted at IJCNN 2020: International Joint\n  Conference on Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although secular, handwritten signature is one of the most reliable biometric\nmethods used by most countries. In the last ten years, the application of\ntechnology for verification of handwritten signatures has evolved strongly,\nincluding forensic aspects. Some factors, such as the complexity of the\nbackground and the small size of the region of interest - signature pixels -\nincrease the difficulty of the targeting task. Other factors that make it\nchallenging are the various variations present in handwritten signatures such\nas location, type of ink, color and type of pen, and the type of stroke. In\nthis work, we propose an approach to locate and extract the pixels of\nhandwritten signatures on identification documents, without any prior\ninformation on the location of the signatures. The technique used is based on a\nfully convolutional encoder-decoder network combined with a block of refinement\nlayers for the alpha channel of the predicted image. The experimental results\ndemonstrate that the technique outputs a clean signature with higher fidelity\nin the lines than the traditional approaches and preservation of the pertinent\ncharacteristics to the signer's spelling. To evaluate the quality of our\nproposal, we use the following image similarity metrics: SSIM, SIFT, and Dice\nCoefficient. The qualitative and quantitative results show a significant\nimprovement in comparison with the baseline system.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 18:47:10 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Junior", "Celso A. M. Lopes", ""], ["da Silva", "Matheus Henrique M.", ""], ["Bezerra", "Byron Leite Dantas", ""], ["Fernandes", "Bruno Jose Torres", ""], ["Impedovo", "Donato", ""]]}, {"id": "2005.14236", "submitter": "Muhammad Ahmad", "authors": "Muhammad Ahmad", "title": "Fuzziness-based Spatial-Spectral Class Discriminant Information\n  Preserving Active Learning for Hyperspectral Image Classification", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Active/Self/Interactive Learning for Hyperspectral Image\nClassification (HSIC) increases the size of the training set without\nconsidering the class scatters and randomness among the existing and new\nsamples. Second, very limited research has been carried out on joint\nspectral-spatial information and finally, a minor but still worth mentioning is\nthe stopping criteria which not being much considered by the community.\nTherefore, this work proposes a novel fuzziness-based spatial-spectral within\nand between for both local and global class discriminant information preserving\n(FLG) method. We first investigate a spatial prior fuzziness-based\nmisclassified sample information. We then compute the total local and global\nfor both within and between class information and formulate it in a\nfine-grained manner. Later this information is fed to a discriminative\nobjective function to query the heterogeneous samples which eliminate the\nrandomness among the training samples. Experimental results on benchmark HSI\ndatasets demonstrate the effectiveness of the FLG method on Generative, Extreme\nLearning Machine and Sparse Multinomial Logistic Regression (SMLR)-LORSAL\nclassifiers.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 18:58:11 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Ahmad", "Muhammad", ""]]}, {"id": "2005.14238", "submitter": "Jiuwen Zhu", "authors": "Jiuwen Zhu, Hu Han, and S. Kevin Zhou", "title": "Human Recognition Using Face in Computed Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the mushrooming use of computed tomography (CT) images in clinical\ndecision making, management of CT data becomes increasingly difficult. From the\npatient identification perspective, using the standard DICOM tag to track\npatient information is challenged by issues such as misspelling, lost file,\nsite variation, etc. In this paper, we explore the feasibility of leveraging\nthe faces in 3D CT images as biometric features. Specifically, we propose an\nautomatic processing pipeline that first detects facial landmarks in 3D for ROI\nextraction and then generates aligned 2D depth images, which are used for\nautomatic recognition. To boost the recognition performance, we employ transfer\nlearning to reduce the data sparsity issue and to introduce a group sampling\nstrategy to increase inter-class discrimination when training the recognition\nnetwork. Our proposed method is capable of capturing underlying identity\ncharacteristics in medical images while reducing memory consumption. To test\nits effectiveness, we curate 600 3D CT images of 280 patients from multiple\nsources for performance evaluation. Experimental results demonstrate that our\nmethod achieves a 1:56 identification accuracy of 92.53% and a 1:1 verification\naccuracy of 96.12%, outperforming other competing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 18:59:59 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Zhu", "Jiuwen", ""], ["Han", "Hu", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2005.14247", "submitter": "Ya\\\"el Balbastre", "authors": "Ya\\\"el Balbastre, Mikael Brudfors, Michela Azzarito, Christian\n  Lambert, Martina F. Callaghan, John Ashburner", "title": "Joint Total Variation ESTATICS for Robust Multi-Parameter Mapping", "comments": "11 pages, 2 figures, 1 table, conference paper, accepted at MICCAI\n  2020", "journal-ref": null, "doi": "10.1007/978-3-030-59713-9_6", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative magnetic resonance imaging (qMRI) derives tissue-specific\nparameters -- such as the apparent transverse relaxation rate R2*, the\nlongitudinal relaxation rate R1 and the magnetisation transfer saturation --\nthat can be compared across sites and scanners and carry important information\nabout the underlying microstructure. The multi-parameter mapping (MPM) protocol\ntakes advantage of multi-echo acquisitions with variable flip angles to extract\nthese parameters in a clinically acceptable scan time. In this context,\nESTATICS performs a joint loglinear fit of multiple echo series to extract R2*\nand multiple extrapolated intercepts, thereby improving robustness to motion\nand decreasing the variance of the estimators. In this paper, we extend this\nmodel in two ways: (1) by introducing a joint total variation (JTV) prior on\nthe intercepts and decay, and (2) by deriving a nonlinear maximum \\emph{a\nposteriori} estimate. We evaluated the proposed algorithm by predicting\nleft-out echoes in a rich single-subject dataset. In this validation, we\noutperformed other state-of-the-art methods and additionally showed that the\nproposed approach greatly reduces the variance of the estimated maps, without\nintroducing bias.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 19:08:42 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Balbastre", "Ya\u00ebl", ""], ["Brudfors", "Mikael", ""], ["Azzarito", "Michela", ""], ["Lambert", "Christian", ""], ["Callaghan", "Martina F.", ""], ["Ashburner", "John", ""]]}, {"id": "2005.14260", "submitter": "Elizabeth Holm", "authors": "Elizabeth A. Holm, Ryan Cohn, Nan Gao, Andrew R. Kitahara, Thomas P.\n  Matson, Bo Lei, Srujana Rao Yarasi", "title": "Overview: Computer vision and machine learning for microstructural\n  characterization and analysis", "comments": "submitted to Materials and Metallurgical Transactions A", "journal-ref": null, "doi": "10.1007/s11661-020-06008-4", "report-no": null, "categories": "cs.CV cond-mat.mtrl-sci", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The characterization and analysis of microstructure is the foundation of\nmicrostructural science, connecting the materials structure to its composition,\nprocess history, and properties. Microstructural quantification traditionally\ninvolves a human deciding a priori what to measure and then devising a\npurpose-built method for doing so. However, recent advances in data science,\nincluding computer vision (CV) and machine learning (ML) offer new approaches\nto extracting information from microstructural images. This overview surveys CV\napproaches to numerically encode the visual information contained in a\nmicrostructural image, which then provides input to supervised or unsupervised\nML algorithms that find associations and trends in the high-dimensional image\nrepresentation. CV/ML systems for microstructural characterization and analysis\nspan the taxonomy of image analysis tasks, including image classification,\nsemantic segmentation, object detection, and instance segmentation. These tools\nenable new approaches to microstructural analysis, including the development of\nnew, rich visual metrics and the discovery of\nprocessing-microstructure-property relationships.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 19:51:23 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Holm", "Elizabeth A.", ""], ["Cohn", "Ryan", ""], ["Gao", "Nan", ""], ["Kitahara", "Andrew R.", ""], ["Matson", "Thomas P.", ""], ["Lei", "Bo", ""], ["Yarasi", "Srujana Rao", ""]]}, {"id": "2005.14262", "submitter": "Raghav Mehta", "authors": "Raghav Mehta, Angelos Filos, Yarin Gal, Tal Arbel", "title": "Uncertainty Evaluation Metric for Brain Tumour Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIDL/2019/ExtendedAbstract/H-PvDNIex", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a metric designed to assess and rank uncertainty\nmeasures for the task of brain tumour sub-tissue segmentation in the BraTS 2019\nsub-challenge on uncertainty quantification. The metric is designed to: (1)\nreward uncertainty measures where high confidence is assigned to correct\nassertions, and where incorrect assertions are assigned low confidence and (2)\npenalize measures that have higher percentages of under-confident correct\nassertions. Here, the workings of the components of the metric are explored\nbased on a number of popular uncertainty measures evaluated on the BraTS 2019\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 19:53:32 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Mehta", "Raghav", ""], ["Filos", "Angelos", ""], ["Gal", "Yarin", ""], ["Arbel", "Tal", ""]]}, {"id": "2005.14264", "submitter": "Wentong Liao", "authors": "Wentong Liao, Xiang Chen, Jingfeng Yang, Stefan Roth, Michael Goesele,\n  Michael Ying Yang, Bodo Rosenhahn", "title": "LR-CNN: Local-aware Region CNN for Vehicle Detection in Aerial Imagery", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art object detection approaches such as Fast/Faster R-CNN, SSD,\nor YOLO have difficulties detecting dense, small targets with arbitrary\norientation in large aerial images. The main reason is that using interpolation\nto align RoI features can result in a lack of accuracy or even loss of location\ninformation. We present the Local-aware Region Convolutional Neural Network\n(LR-CNN), a novel two-stage approach for vehicle detection in aerial imagery.\nWe enhance translation invariance to detect dense vehicles and address the\nboundary quantization issue amongst dense vehicles by aggregating the\nhigh-precision RoIs' features. Moreover, we resample high-level semantic pooled\nfeatures, making them regain location information from the features of a\nshallower convolutional block. This strengthens the local feature invariance\nfor the resampled features and enables detecting vehicles in an arbitrary\norientation. The local feature invariance enhances the learning ability of the\nfocal loss function, and the focal loss further helps to focus on the hard\nexamples. Taken together, our method better addresses the challenges of aerial\nimagery. We evaluate our approach on several challenging datasets (VEDAI,\nDOTA), demonstrating a significant improvement over state-of-the-art methods.\nWe demonstrate the good generalization ability of our approach on the DLR 3K\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 19:57:34 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Liao", "Wentong", ""], ["Chen", "Xiang", ""], ["Yang", "Jingfeng", ""], ["Roth", "Stefan", ""], ["Goesele", "Michael", ""], ["Yang", "Michael Ying", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "2005.14284", "submitter": "Muhammad Naseer Bajwa", "authors": "Muhammad Naseer Bajwa, Muhammad Imran Malik, Shoaib Ahmed Siddiqui,\n  Andreas Dengel, Faisal Shafait, Wolfgang Neumeier, Sheraz Ahmed", "title": "Two-stage framework for optic disc localization and glaucoma\n  classification in retinal fundus images using deep learning", "comments": "16 Pages, 10 Figures", "journal-ref": "BMC medical informatics and decision making 19.1 (2019): 136", "doi": "10.1186/s12911-019-0842-8", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement of powerful image processing and machine learning\ntechniques, CAD has become ever more prevalent in all fields of medicine\nincluding ophthalmology. Since optic disc is the most important part of retinal\nfundus image for glaucoma detection, this paper proposes a two-stage framework\nthat first detects and localizes optic disc and then classifies it into healthy\nor glaucomatous. The first stage is based on RCNN and is responsible for\nlocalizing and extracting optic disc from a retinal fundus image while the\nsecond stage uses Deep CNN to classify the extracted disc into healthy or\nglaucomatous. In addition to the proposed solution, we also developed a\nrule-based semi-automatic ground truth generation method that provides\nnecessary annotations for training RCNN based model for automated disc\nlocalization. The proposed method is evaluated on seven publicly available\ndatasets for disc localization and on ORIGA dataset, which is the largest\npublicly available dataset for glaucoma classification. The results of\nautomatic localization mark new state-of-the-art on six datasets with accuracy\nreaching 100% on four of them. For glaucoma classification we achieved AUC\nequal to 0.874 which is 2.7% relative improvement over the state-of-the-art\nresults previously obtained for classification on ORIGA. Once trained on\ncarefully annotated data, Deep Learning based methods for optic disc detection\nand localization are not only robust, accurate and fully automated but also\neliminates the need for dataset-dependent heuristic algorithms. Our empirical\nevaluation of glaucoma classification on ORIGA reveals that reporting only AUC,\nfor datasets with class imbalance and without pre-defined train and test\nsplits, does not portray true picture of the classifier's performance and calls\nfor additional performance metrics to substantiate the results.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 20:40:19 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Bajwa", "Muhammad Naseer", ""], ["Malik", "Muhammad Imran", ""], ["Siddiqui", "Shoaib Ahmed", ""], ["Dengel", "Andreas", ""], ["Shafait", "Faisal", ""], ["Neumeier", "Wolfgang", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "2005.14288", "submitter": "Naoto Usuyama", "authors": "Naoto Usuyama, Natalia Larios Delgado, Amanda K. Hall, Jessica Lundin", "title": "ePillID Dataset: A Low-Shot Fine-Grained Benchmark for Pill\n  Identification", "comments": "CVPR 2020 VL3. Project Page:\n  https://github.com/usuyama/ePillID-benchmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying prescription medications is a frequent task for patients and\nmedical professionals; however, this is an error-prone task as many pills have\nsimilar appearances (e.g. white round pills), which increases the risk of\nmedication errors. In this paper, we introduce ePillID, the largest public\nbenchmark on pill image recognition, composed of 13k images representing 9804\nappearance classes (two sides for 4902 pill types). For most of the appearance\nclasses, there exists only one reference image, making it a challenging\nlow-shot recognition setting. We present our experimental setup and evaluation\nresults of various baseline models on the benchmark. The best baseline using a\nmulti-head metric-learning approach with bilinear features performed remarkably\nwell; however, our error analysis suggests that they still fail to distinguish\nparticularly confusing classes. The code and data are available at\nhttps://github.com/usuyama/ePillID-benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 20:53:36 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 22:29:24 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Usuyama", "Naoto", ""], ["Delgado", "Natalia Larios", ""], ["Hall", "Amanda K.", ""], ["Lundin", "Jessica", ""]]}, {"id": "2005.14302", "submitter": "Alwyn Mathew", "authors": "Alwyn Mathew, Aditya Prakash Patra, Jimson Mathew", "title": "Monocular Depth Estimators: Vulnerabilities and Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements of neural networks lead to reliable monocular depth\nestimation. Monocular depth estimated techniques have the upper hand over\ntraditional depth estimation techniques as it only needs one image during\ninference. Depth estimation is one of the essential tasks in robotics, and\nmonocular depth estimation has a wide variety of safety-critical applications\nlike in self-driving cars and surgical devices. Thus, the robustness of such\ntechniques is very crucial. It has been shown in recent works that these deep\nneural networks are highly vulnerable to adversarial samples for tasks like\nclassification, detection and segmentation. These adversarial samples can\ncompletely ruin the output of the system, making their credibility in real-time\ndeployment questionable. In this paper, we investigate the robustness of the\nmost state-of-the-art monocular depth estimation networks against adversarial\nattacks. Our experiments show that tiny perturbations on an image that are\ninvisible to the naked eye (perturbation attack) and corruption less than about\n1% of an image (patch attack) can affect the depth estimation drastically. We\nintroduce a novel deep feature annihilation loss that corrupts the hidden\nfeature space representation forcing the decoder of the network to output poor\ndepth maps. The white-box and black-box test compliments the effectiveness of\nthe proposed attack. We also perform adversarial example transferability tests,\nmainly cross-data transferability.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 21:25:21 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Mathew", "Alwyn", ""], ["Patra", "Aditya Prakash", ""], ["Mathew", "Jimson", ""]]}, {"id": "2005.14308", "submitter": "Muhammad Naseer Bajwa", "authors": "Muhammad Naseer Bajwa, Yoshinobu Taniguchi, Muhammad Imran Malik,\n  Wolfgang Neumeier, Andreas Dengel, Sheraz Ahmed", "title": "Combining Fine- and Coarse-Grained Classifiers for Diabetic Retinopathy\n  Detection", "comments": "Pages 12, Figures 5", "journal-ref": null, "doi": "10.1007/978-3-030-39343-4_21", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual artefacts of early diabetic retinopathy in retinal fundus images are\nusually small in size, inconspicuous, and scattered all over retina. Detecting\ndiabetic retinopathy requires physicians to look at the whole image and fixate\non some specific regions to locate potential biomarkers of the disease.\nTherefore, getting inspiration from ophthalmologist, we propose to combine\ncoarse-grained classifiers that detect discriminating features from the whole\nimages, with a recent breed of fine-grained classifiers that discover and pay\nparticular attention to pathologically significant regions. To evaluate the\nperformance of this proposed ensemble, we used publicly available EyePACS and\nMessidor datasets. Extensive experimentation for binary, ternary and quaternary\nclassification shows that this ensemble largely outperforms individual image\nclassifiers as well as most of the published works in most training setups for\ndiabetic retinopathy detection. Furthermore, the performance of fine-grained\nclassifiers is found notably superior than coarse-grained image classifiers\nencouraging the development of task-oriented fine-grained classifiers modelled\nafter specialist ophthalmologists.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 21:37:36 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Bajwa", "Muhammad Naseer", ""], ["Taniguchi", "Yoshinobu", ""], ["Malik", "Muhammad Imran", ""], ["Neumeier", "Wolfgang", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "2005.14310", "submitter": "Zhibo Yang", "authors": "Zhibo Yang, Lihan Huang, Yupei Chen, Zijun Wei, Seoyoung Ahn, Gregory\n  Zelinsky, Dimitris Samaras, Minh Hoai", "title": "Predicting Goal-directed Human Attention Using Inverse Reinforcement\n  Learning", "comments": "16 pages, 13 figures, CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to predict human gaze behavior has obvious importance for\nbehavioral vision and for computer vision applications. Most models have mainly\nfocused on predicting free-viewing behavior using saliency maps, but these\npredictions do not generalize to goal-directed behavior, such as when a person\nsearches for a visual target object. We propose the first inverse reinforcement\nlearning (IRL) model to learn the internal reward function and policy used by\nhumans during visual search. The viewer's internal belief states were modeled\nas dynamic contextual belief maps of object locations. These maps were learned\nby IRL and then used to predict behavioral scanpaths for multiple target\ncategories. To train and evaluate our IRL model we created COCO-Search18, which\nis now the largest dataset of high-quality search fixations in existence.\nCOCO-Search18 has 10 participants searching for each of 18 target-object\ncategories in 6202 images, making about 300,000 goal-directed fixations. When\ntrained and evaluated on COCO-Search18, the IRL model outperformed baseline\nmodels in predicting search fixation scanpaths, both in terms of similarity to\nhuman search behavior and search efficiency. Finally, reward maps recovered by\nthe IRL model reveal distinctive target-dependent patterns of object\nprioritization, which we interpret as a learned object context.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 21:46:27 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 10:56:15 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Yang", "Zhibo", ""], ["Huang", "Lihan", ""], ["Chen", "Yupei", ""], ["Wei", "Zijun", ""], ["Ahn", "Seoyoung", ""], ["Zelinsky", "Gregory", ""], ["Samaras", "Dimitris", ""], ["Hoai", "Minh", ""]]}, {"id": "2005.14313", "submitter": "Alwyn Mathew", "authors": "Alwyn Mathew, Aditya Prakash Patra, Jimson Mathew", "title": "Self-Attention Dense Depth Estimation Network for Unrectified Video\n  Sequences", "comments": null, "journal-ref": null, "doi": "10.1109/ICIP40778.2020.9190764", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dense depth estimation of a 3D scene has numerous applications, mainly in\nrobotics and surveillance. LiDAR and radar sensors are the hardware solution\nfor real-time depth estimation, but these sensors produce sparse depth maps and\nare sometimes unreliable. In recent years research aimed at tackling depth\nestimation using single 2D image has received a lot of attention. The deep\nlearning based self-supervised depth estimation methods from the rectified\nstereo and monocular video frames have shown promising results. We propose a\nself-attention based depth and ego-motion network for unrectified images. We\nalso introduce non-differentiable distortion of the camera into the training\npipeline. Our approach performs competitively when compared to other\nestablished approaches that used rectified images for depth estimation.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 21:53:53 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Mathew", "Alwyn", ""], ["Patra", "Aditya Prakash", ""], ["Mathew", "Jimson", ""]]}, {"id": "2005.14330", "submitter": "Abdullah-Al-Zubaer Imran", "authors": "Abdullah-Al-Zubaer Imran, Chao Huang, Hui Tang, Wei Fan, Kenneth M.C.\n  Cheung, Michael To, Zhen Qian, Demetri Terzopoulos", "title": "Bipartite Distance for Shape-Aware Landmark Detection in Spinal X-Ray\n  Images", "comments": "Presented at Med-NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Scoliosis is a congenital disease that causes lateral curvature in the spine.\nIts assessment relies on the identification and localization of vertebrae in\nspinal X-ray images, conventionally via tedious and time-consuming manual\nradiographic procedures that are prone to subjectivity and observational\nvariability. Reliability can be improved through the automatic detection and\nlocalization of spinal landmarks. To guide a CNN in the learning of spinal\nshape while detecting landmarks in X-ray images, we propose a novel loss based\non a bipartite distance (BPD) measure, and show that it consistently improves\nlandmark detection performance.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 22:34:24 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Imran", "Abdullah-Al-Zubaer", ""], ["Huang", "Chao", ""], ["Tang", "Hui", ""], ["Fan", "Wei", ""], ["Cheung", "Kenneth M. C.", ""], ["To", "Michael", ""], ["Qian", "Zhen", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "2005.14354", "submitter": "Zhengzhong Tu", "authors": "Zhengzhong Tu, Yilin Wang, Neil Birkbeck, Balu Adsumilli, and Alan C.\n  Bovik", "title": "UGC-VQA: Benchmarking Blind Video Quality Assessment for User Generated\n  Content", "comments": "IEEE Transactions on Image Processing 2021", "journal-ref": null, "doi": "10.1109/TIP.2021.3072221", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent years have witnessed an explosion of user-generated content (UGC)\nvideos shared and streamed over the Internet, thanks to the evolution of\naffordable and reliable consumer capture devices, and the tremendous popularity\nof social media platforms. Accordingly, there is a great need for accurate\nvideo quality assessment (VQA) models for UGC/consumer videos to monitor,\ncontrol, and optimize this vast content. Blind quality prediction of\nin-the-wild videos is quite challenging, since the quality degradations of UGC\ncontent are unpredictable, complicated, and often commingled. Here we\ncontribute to advancing the UGC-VQA problem by conducting a comprehensive\nevaluation of leading no-reference/blind VQA (BVQA) features and models on a\nfixed evaluation architecture, yielding new empirical insights on both\nsubjective video quality studies and VQA model design. By employing a feature\nselection strategy on top of leading VQA model features, we are able to extract\n60 of the 763 statistical features used by the leading models to create a new\nfusion-based BVQA model, which we dub the \\textbf{VID}eo quality\n\\textbf{EVAL}uator (VIDEVAL), that effectively balances the trade-off between\nVQA performance and efficiency. Our experimental results show that VIDEVAL\nachieves state-of-the-art performance at considerably lower computational cost\nthan other leading models. Our study protocol also defines a reliable benchmark\nfor the UGC-VQA problem, which we believe will facilitate further research on\ndeep learning-based VQA modeling, as well as perceptually-optimized efficient\nUGC video processing, transcoding, and streaming. To promote reproducible\nresearch and public evaluation, an implementation of VIDEVAL has been made\navailable online: \\url{https://github.com/tu184044109/VIDEVAL_release}.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 00:39:20 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 04:40:56 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Tu", "Zhengzhong", ""], ["Wang", "Yilin", ""], ["Birkbeck", "Neil", ""], ["Adsumilli", "Balu", ""], ["Bovik", "Alan C.", ""]]}, {"id": "2005.14355", "submitter": "Dong Yang", "authors": "Dong Yang, Holger Roth, Xiaosong Wang, Ziyue Xu, Andriy Myronenko,\n  Daguang Xu", "title": "Enhancing Foreground Boundaries for Medical Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/PAlQnIVKLY", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object segmentation plays an important role in the modern medical image\nanalysis, which benefits clinical study, disease diagnosis, and surgery\nplanning. Given the various modalities of medical images, the automated or\nsemi-automated segmentation approaches have been used to identify and parse\norgans, bones, tumors, and other regions-of-interest (ROI). However, these\ncontemporary segmentation approaches tend to fail to predict the boundary areas\nof ROI, because of the fuzzy appearance contrast caused during the imaging\nprocedure. To further improve the segmentation quality of boundary areas, we\npropose a boundary enhancement loss to enforce additional constraints on\noptimizing machine learning models. The proposed loss function is\nlight-weighted and easy to implement without any pre- or post-processing. Our\nexperimental results validate that our loss function are better than, or at\nleast comparable to, other state-of-the-art loss functions in terms of\nsegmentation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 00:50:08 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Yang", "Dong", ""], ["Roth", "Holger", ""], ["Wang", "Xiaosong", ""], ["Xu", "Ziyue", ""], ["Myronenko", "Andriy", ""], ["Xu", "Daguang", ""]]}, {"id": "2005.14363", "submitter": "Aditi Jha", "authors": "Aditi Jha, Joshua Peterson, Thomas L. Griffiths", "title": "Extracting low-dimensional psychological representations from\n  convolutional neural networks", "comments": "Accepted to CogSci 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are increasingly being used in cognitive modeling as a\nmeans of deriving representations for complex stimuli such as images. While the\npredictive power of these networks is high, it is often not clear whether they\nalso offer useful explanations of the task at hand. Convolutional neural\nnetwork representations have been shown to be predictive of human similarity\njudgments for images after appropriate adaptation. However, these\nhigh-dimensional representations are difficult to interpret. Here we present a\nmethod for reducing these representations to a low-dimensional space which is\nstill predictive of similarity judgments. We show that these low-dimensional\nrepresentations also provide insightful explanations of factors underlying\nhuman similarity judgments.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 01:29:39 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Jha", "Aditi", ""], ["Peterson", "Joshua", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "2005.14376", "submitter": "Jia-Wei Chen Dr.", "authors": "Rongfang Wang, Fan Ding, Licheng Jiao, Jia-Wei Chen, Bo Liu, Wenping\n  Ma, Mi Wang", "title": "A Light-Weighted Convolutional Neural Network for Bitemporal SAR Image\n  Change Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, many Convolution Neural Networks (CNN) have been successfully\nemployed in bitemporal SAR image change detection. However, most of the\nexisting networks are too heavy and occupy a large volume of memory for storage\nand calculation. Motivated by this, in this paper, we propose a lightweight\nneural network to reduce the computational and spatial complexity and\nfacilitate the change detection on an edge device. In the proposed network, we\nreplace normal convolutional layers with bottleneck layers that keep the same\nnumber of channels between input and output. Next, we employ dilated\nconvolutional kernels with a few non-zero entries that reduce the running time\nin convolutional operators. Comparing with the conventional convolutional\nneural network, our light-weighted neural network will be more efficient with\nfewer parameters. We verify our light-weighted neural network on four sets of\nbitemporal SAR images. The experimental results show that the proposed network\ncan obtain better performance than the conventional CNN and has better model\ngeneralization, especially on the challenging datasets with complex scenes.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 04:01:32 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2020 02:40:04 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Wang", "Rongfang", ""], ["Ding", "Fan", ""], ["Jiao", "Licheng", ""], ["Chen", "Jia-Wei", ""], ["Liu", "Bo", ""], ["Ma", "Wenping", ""], ["Wang", "Mi", ""]]}, {"id": "2005.14386", "submitter": "Ruotian Luo", "authors": "Ruotian Luo and Greg Shakhnarovich", "title": "Controlling Length in Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and evaluate captioning models that allow control of caption\nlength. Our models can leverage this control to generate captions of different\nstyle and descriptiveness.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 05:03:15 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Luo", "Ruotian", ""], ["Shakhnarovich", "Greg", ""]]}, {"id": "2005.14390", "submitter": "Harim Lee", "authors": "Harim Lee, Myeung Un Kim, Yeongjun Kim, Hyeonsu Lyu, and Hyun Jong\n  Yang", "title": "Privacy-Protection Drone Patrol System based on Face Anonymization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robot market has been growing significantly and is expected to become 1.5\ntimes larger in 2024 than what it was in 2019. Robots have attracted attention\nof security companies thanks to their mobility. These days, for security\nrobots, unmanned aerial vehicles (UAVs) have quickly emerged by highlighting\ntheir advantage: they can even go to any hazardous place that humans cannot\naccess. For UAVs, Drone has been a representative model and has several merits\nto consist of various sensors such as high-resolution cameras. Therefore, Drone\nis the most suitable as a mobile surveillance robot. These attractive\nadvantages such as high-resolution cameras and mobility can be a double-edged\nsword, i.e., privacy infringement. Surveillance drones take videos with\nhigh-resolution to fulfill their role, however, those contain a lot of privacy\nsensitive information. The indiscriminate shooting is a critical issue for\nthose who are very reluctant to be exposed. To tackle the privacy infringement,\nthis work proposes face-anonymizing drone patrol system. In this system, one\nperson's face in a video is transformed into a different face with facial\ncomponents maintained. To construct our privacy-preserving system, we have\nadopted the latest generative adversarial networks frameworks and have some\nmodifications on losses of those frameworks. Our face-anonymzing approach is\nevaluated with various public face-image and video dataset. Moreover, our\nsystem is evaluated with a customized drone consisting of a high-resolution\ncamera, a companion computer, and a drone control computer. Finally, we confirm\nthat our system can protect privacy sensitive information with our\nface-anonymzing algorithm while preserving the performance of robot perception,\ni.e., simultaneous localization and mapping.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 05:14:18 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Lee", "Harim", ""], ["Kim", "Myeung Un", ""], ["Kim", "Yeongjun", ""], ["Lyu", "Hyeonsu", ""], ["Yang", "Hyun Jong", ""]]}, {"id": "2005.14400", "submitter": "Liang-Jian Deng", "authors": "Jin-Fan Hu, Ting-Zhu Huang, Liang-Jian Deng, Tai-Xiang Jiang, Gemine\n  Vivone and Jocelyn Chanussot", "title": "Hyperspectral Image Super-resolution via Deep Spatio-spectral\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images are of crucial importance in order to better understand\nfeatures of different materials. To reach this goal, they leverage on a high\nnumber of spectral bands. However, this interesting characteristic is often\npaid by a reduced spatial resolution compared with traditional multispectral\nimage systems. In order to alleviate this issue, in this work, we propose a\nsimple and efficient architecture for deep convolutional neural networks to\nfuse a low-resolution hyperspectral image (LR-HSI) and a high-resolution\nmultispectral image (HR-MSI), yielding a high-resolution hyperspectral image\n(HR-HSI). The network is designed to preserve both spatial and spectral\ninformation thanks to an architecture from two folds: one is to utilize the\nHR-HSI at a different scale to get an output with a satisfied spectral\npreservation; another one is to apply concepts of multi-resolution analysis to\nextract high-frequency information, aiming to output high quality spatial\ndetails. Finally, a plain mean squared error loss function is used to measure\nthe performance during the training. Extensive experiments demonstrate that the\nproposed network architecture achieves best performance (both qualitatively and\nquantitatively) compared with recent state-of-the-art hyperspectral image\nsuper-resolution approaches. Moreover, other significant advantages can be\npointed out by the use of the proposed approach, such as, a better network\ngeneralization ability, a limited computational burden, and a robustness with\nrespect to the number of training samples.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 05:56:50 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Hu", "Jin-Fan", ""], ["Huang", "Ting-Zhu", ""], ["Deng", "Liang-Jian", ""], ["Jiang", "Tai-Xiang", ""], ["Vivone", "Gemine", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "2005.14403", "submitter": "Guangfeng Lin", "authors": "Guangfeng Lin, Xiaobing Kang, Kaiyang Liao, Fan Zhao, Yajun Chen", "title": "Deep graph learning for semi-supervised classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Graph learning (GL) can dynamically capture the distribution structure (graph\nstructure) of data based on graph convolutional networks (GCN), and the\nlearning quality of the graph structure directly influences GCN for\nsemi-supervised classification. Existing methods mostly combine the\ncomputational layer and the related losses into GCN for exploring the global\ngraph(measuring graph structure from all data samples) or local graph\n(measuring graph structure from local data samples). Global graph emphasises on\nthe whole structure description of the inter-class data, while local graph\ntrend to the neighborhood structure representation of intra-class data.\nHowever, it is difficult to simultaneously balance these graphs of the learning\nprocess for semi-supervised classification because of the interdependence of\nthese graphs. To simulate the interdependence, deep graph learning(DGL) is\nproposed to find the better graph representation for semi-supervised\nclassification. DGL can not only learn the global structure by the previous\nlayer metric computation updating, but also mine the local structure by next\nlayer local weight reassignment. Furthermore, DGL can fuse the different\nstructures by dynamically encoding the interdependence of these structures, and\ndeeply mine the relationship of the different structures by the hierarchical\nprogressive learning for improving the performance of semi-supervised\nclassification. Experiments demonstrate the DGL outperforms state-of-the-art\nmethods on three benchmark datasets (Citeseer,Cora, and Pubmed) for citation\nnetworks and two benchmark datasets (MNIST and Cifar10) for images.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 05:59:45 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Lin", "Guangfeng", ""], ["Kang", "Xiaobing", ""], ["Liao", "Kaiyang", ""], ["Zhao", "Fan", ""], ["Chen", "Yajun", ""]]}, {"id": "2005.14405", "submitter": "Parul Gupta", "authors": "Komal Chugh, Parul Gupta, Abhinav Dhall and Ramanathan Subramanian", "title": "Not made for each other- Audio-Visual Dissonance-based Deepfake\n  Detection and Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose detection of deepfake videos based on the dissimilarity between\nthe audio and visual modalities, termed as the Modality Dissonance Score (MDS).\nWe hypothesize that manipulation of either modality will lead to dis-harmony\nbetween the two modalities, eg, loss of lip-sync, unnatural facial and lip\nmovements, etc. MDS is computed as an aggregate of dissimilarity scores between\naudio and visual segments in a video. Discriminative features are learnt for\nthe audio and visual channels in a chunk-wise manner, employing the\ncross-entropy loss for individual modalities, and a contrastive loss that\nmodels inter-modality similarity. Extensive experiments on the DFDC and\nDeepFake-TIMIT Datasets show that our approach outperforms the state-of-the-art\nby up to 7%. We also demonstrate temporal forgery localization, and show how\nour technique identifies the manipulated video segments.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 06:09:33 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 03:13:38 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 15:09:49 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chugh", "Komal", ""], ["Gupta", "Parul", ""], ["Dhall", "Abhinav", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "2005.14415", "submitter": "Guangfeng Lin", "authors": "Guangfeng Lin, Ying Yang, Yindi Fan, Xiaobing Kang, Kaiyang Liao, and\n  Fan Zhao", "title": "High-order structure preserving graph neural network for few-shot\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Few-shot learning can find the latent structure information between the prior\nknowledge and the queried data by the similarity metric of meta-learning to\nconstruct the discriminative model for recognizing the new categories with the\nrare labeled samples. Most existing methods try to model the similarity\nrelationship of the samples in the intra tasks, and generalize the model to\nidentify the new categories. However, the relationship of samples between the\nseparated tasks is difficultly considered because of the different metric\ncriterion in the respective tasks. In contrast, the proposed high-order\nstructure preserving graph neural network(HOSP-GNN) can further explore the\nrich structure of the samples to predict the label of the queried data on graph\nthat enables the structure evolution to explicitly discriminate the categories\nby iteratively updating the high-order structure relationship (the relative\nmetric in multi-samples,instead of pairwise sample metric) with the manifold\nstructure constraints. HOSP-GNN can not only mine the high-order structure for\ncomplementing the relevance between samples that may be divided into the\ndifferent task in meta-learning, and but also generate the rule of the\nstructure updating by manifold constraint. Furthermore, HOSP-GNN doesn't need\nretrain the learning model for recognizing the new classes, and HOSP-GNN has\nthe well-generalizable high-order structure for model adaptability. Experiments\nshow that HOSP-GNN outperforms the state-of-the-art methods on supervised and\nsemi-supervised few-shot learning in three benchmark datasets that are\nminiImageNet, tieredImageNet and FC100.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 06:38:51 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Lin", "Guangfeng", ""], ["Yang", "Ying", ""], ["Fan", "Yindi", ""], ["Kang", "Xiaobing", ""], ["Liao", "Kaiyang", ""], ["Zhao", "Fan", ""]]}, {"id": "2005.14439", "submitter": "Xi Li", "authors": "Huanyu Wang, Zequn Qin, Songyuan Li, and Xi Li", "title": "CoDiNet: Path Distribution Modeling with Consistency and Diversity for\n  Dynamic Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic routing networks, aimed at finding the best routing paths in the\nnetworks, have achieved significant improvements to neural networks in terms of\naccuracy and efficiency. In this paper, we see dynamic routing networks in a\nfresh light, formulating a routing method as a mapping from a sample space to a\nrouting space. From the perspective of space mapping, prevalent methods of\ndynamic routing didn't consider how inference paths would be distributed in the\nrouting space. Thus, we propose a novel method, termed CoDiNet, to model the\nrelationship between a sample space and a routing space by regularizing the\ndistribution of routing paths with the properties of consistency and diversity.\nSpecifically, samples with similar semantics should be mapped into the same\narea in routing space, while those with dissimilar semantics should be mapped\ninto different areas. Moreover, we design a customizable dynamic routing\nmodule, which can strike a balance between accuracy and efficiency. When\ndeployed upon ResNet models, our method achieves higher performance and\neffectively reduces average computational cost on four widely used datasets.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 08:09:21 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 06:29:22 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 08:21:06 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Wang", "Huanyu", ""], ["Qin", "Zequn", ""], ["Li", "Songyuan", ""], ["Li", "Xi", ""]]}, {"id": "2005.14446", "submitter": "Zhaohui Yang", "authors": "Zhaohui Yang, Yunhe Wang, Xinghao Chen, Jianyuan Guo, Wei Zhang, Chao\n  Xu, Chunjing Xu, Dacheng Tao, Chang Xu", "title": "HourNAS: Extremely Fast Neural Architecture Search Through an Hourglass\n  Lens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) refers to automatically design the\narchitecture. We propose an hourglass-inspired approach (HourNAS) for this\nproblem that is motivated by the fact that the effects of the architecture\noften proceed from the vital few blocks. Acting like the narrow neck of an\nhourglass, vital blocks in the guaranteed path from the input to the output of\na deep neural network restrict the information flow and influence the network\naccuracy. The other blocks occupy the major volume of the network and determine\nthe overall network complexity, corresponding to the bulbs of an hourglass. To\nachieve an extremely fast NAS while preserving the high accuracy, we propose to\nidentify the vital blocks and make them the priority in the architecture\nsearch. The search space of those non-vital blocks is further shrunk to only\ncover the candidates that are affordable under the computational resource\nconstraints. Experimental results on the ImageNet show that only using 3 hours\n(0.1 days) with one GPU, our HourNAS can search an architecture that achieves a\n77.0% Top-1 accuracy, which outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 08:35:32 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 11:40:05 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 00:43:12 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Yang", "Zhaohui", ""], ["Wang", "Yunhe", ""], ["Chen", "Xinghao", ""], ["Guo", "Jianyuan", ""], ["Zhang", "Wei", ""], ["Xu", "Chao", ""], ["Xu", "Chunjing", ""], ["Tao", "Dacheng", ""], ["Xu", "Chang", ""]]}, {"id": "2005.14461", "submitter": "Qiufu Li", "authors": "Qiufu Li and Linlin Shen", "title": "WaveSNet: Wavelet Integrated Deep Networks for Image Segmentation", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep networks, the lost data details significantly degrade the\nperformances of image segmentation. In this paper, we propose to apply Discrete\nWavelet Transform (DWT) to extract the data details during feature map\ndown-sampling, and adopt Inverse DWT (IDWT) with the extracted details during\nthe up-sampling to recover the details. We firstly transform DWT/IDWT as\ngeneral network layers, which are applicable to 1D/2D/3D data and various\nwavelets like Haar, Cohen, and Daubechies, etc. Then, we design wavelet\nintegrated deep networks for image segmentation (WaveSNets) based on various\narchitectures, including U-Net, SegNet, and DeepLabv3+. Due to the\neffectiveness of the DWT/IDWT in processing data details, experimental results\non CamVid, Pascal VOC, and Cityscapes show that our WaveSNets achieve better\nsegmentation performances than their vanilla versions.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 09:17:37 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Li", "Qiufu", ""], ["Shen", "Linlin", ""]]}, {"id": "2005.14480", "submitter": "Yi Li Dr.", "authors": "Wenwu Ye, Jin Yao, Hui Xue, Yi Li", "title": "Weakly Supervised Lesion Localization With Probabilistic-CAM Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing thoracic diseases on chest X-ray plays a critical role in clinical\npractices such as diagnosis and treatment planning. However, current deep\nlearning based approaches often require strong supervision, e.g. annotated\nbounding boxes, for training such systems, which is infeasible to harvest in\nlarge-scale. We present Probabilistic Class Activation Map (PCAM) pooling, a\nnovel global pooling operation for lesion localization with only image-level\nsupervision. PCAM pooling explicitly leverages the excellent localization\nability of CAM during training in a probabilistic fashion. Experiments on the\nChestX-ray14 dataset show a ResNet-34 model trained with PCAM pooling\noutperforms state-of-the-art baselines on both the classification task and the\nlocalization task. Visual examination on the probability maps generated by PCAM\npooling shows clear and sharp boundaries around lesion regions compared to the\nlocalization heatmaps generated by CAM. PCAM pooling is open sourced at\nhttps://github.com/jfhealthcare/Chexpert.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 09:57:17 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Ye", "Wenwu", ""], ["Yao", "Jin", ""], ["Xue", "Hui", ""], ["Li", "Yi", ""]]}, {"id": "2005.14502", "submitter": "Uzair Nadeem", "authors": "Uzair Nadeem, Mohammed Bennamoun, Roberto Togneri, Ferdous Sohel", "title": "Unconstrained Matching of 2D and 3D Descriptors for 6-DOF Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel concept to directly match feature descriptors\nextracted from 2D images with feature descriptors extracted from 3D point\nclouds. We use this concept to directly localize images in a 3D point cloud. We\ngenerate a dataset of matching 2D and 3D points and their corresponding feature\ndescriptors, which is used to learn a Descriptor-Matcher classifier. To\nlocalize the pose of an image at test time, we extract keypoints and feature\ndescriptors from the query image. The trained Descriptor-Matcher is then used\nto match the features from the image and the point cloud. The locations of the\nmatched features are used in a robust pose estimation algorithm to predict the\nlocation and orientation of the query image. We carried out an extensive\nevaluation of the proposed method for indoor and outdoor scenarios and with\ndifferent types of point clouds to verify the feasibility of our approach.\nExperimental results demonstrate that direct matching of feature descriptors\nfrom images and point clouds is not only a viable idea but can also be reliably\nused to estimate the 6-DOF poses of query cameras in any type of 3D point cloud\nin an unconstrained manner with high precision.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 11:17:32 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Nadeem", "Uzair", ""], ["Bennamoun", "Mohammed", ""], ["Togneri", "Roberto", ""], ["Sohel", "Ferdous", ""]]}, {"id": "2005.14511", "submitter": "Navid Alemi Koohbanani", "authors": "Navid Alemi Koohbanani, Mostafa Jahanifar, Neda Zamani Tajadin, and\n  Nasir Rajpoot", "title": "NuClick: A Deep Learning Framework for Interactive Segmentation of\n  Microscopy Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object segmentation is an important step in the workflow of computational\npathology. Deep learning based models generally require large amount of labeled\ndata for precise and reliable prediction. However, collecting labeled data is\nexpensive because it often requires expert knowledge, particularly in medical\nimaging domain where labels are the result of a time-consuming analysis made by\none or more human experts. As nuclei, cells and glands are fundamental objects\nfor downstream analysis in computational pathology/cytology, in this paper we\npropose a simple CNN-based approach to speed up collecting annotations for\nthese objects which requires minimum interaction from the annotator. We show\nthat for nuclei and cells in histology and cytology images, one click inside\neach object is enough for NuClick to yield a precise annotation. For\nmulticellular structures such as glands, we propose a novel approach to provide\nthe NuClick with a squiggle as a guiding signal, enabling it to segment the\nglandular boundaries. These supervisory signals are fed to the network as\nauxiliary inputs along with RGB channels. With detailed experiments, we show\nthat NuClick is adaptable to the object scale, robust against variations in the\nuser input, adaptable to new domains, and delivers reliable annotations. An\ninstance segmentation model trained on masks generated by NuClick achieved the\nfirst rank in LYON19 challenge. As exemplar outputs of our framework, we are\nreleasing two datasets: 1) a dataset of lymphocyte annotations within IHC\nimages, and 2) a dataset of segmented WBCs in blood smear images.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 11:51:27 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 15:27:41 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Koohbanani", "Navid Alemi", ""], ["Jahanifar", "Mostafa", ""], ["Tajadin", "Neda Zamani", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "2005.14521", "submitter": "Xiaozhen Xie", "authors": "Haijin Zeng, Xiaozhen Xie, Jifeng Ning", "title": "Enhanced nonconvex low-rank approximation of tensor multi-modes for\n  tensor completion", "comments": "arXiv admin note: substantial text overlap with arXiv:2004.08747", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order low-rank tensor arises in many data processing applications and\nhas attracted great interests. Inspired by low-rank approximation theory,\nresearchers have proposed a series of effective tensor completion methods.\nHowever, most of these methods directly consider the global low-rankness of\nunderlying tensors, which is not sufficient for a low sampling rate; in\naddition, the single nuclear norm or its relaxation is usually adopted to\napproximate the rank function, which would lead to suboptimal solution deviated\nfrom the original one. To alleviate the above problems, in this paper, we\npropose a novel low-rank approximation of tensor multi-modes (LRATM), in which\na double nonconvex $L_{\\gamma}$ norm is designed to represent the underlying\njoint-manifold drawn from the modal factorization factors of the underlying\ntensor. A block successive upper-bound minimization method-based algorithm is\ndesigned to efficiently solve the proposed model, and it can be demonstrated\nthat our numerical scheme converges to the coordinatewise minimizers. Numerical\nresults on three types of public multi-dimensional datasets have tested and\nshown that our algorithm can recover a variety of low-rank tensors with\nsignificantly fewer samples than the compared methods.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 08:53:54 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 08:58:14 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zeng", "Haijin", ""], ["Xie", "Xiaozhen", ""], ["Ning", "Jifeng", ""]]}, {"id": "2005.14553", "submitter": "Christos Sakaridis", "authors": "Christos Sakaridis, Dengxin Dai, Luc Van Gool", "title": "Map-Guided Curriculum Domain Adaptation and Uncertainty-Aware Evaluation\n  for Semantic Nighttime Image Segmentation", "comments": "IEEE T-PAMI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of semantic nighttime image segmentation and improve\nthe state-of-the-art, by adapting daytime models to nighttime without using\nnighttime annotations. Moreover, we design a new evaluation framework to\naddress the substantial uncertainty of semantics in nighttime images. Our\ncentral contributions are: 1) a curriculum framework to gradually adapt\nsemantic segmentation models from day to night through progressively darker\ntimes of day, exploiting cross-time-of-day correspondences between daytime\nimages from a reference map and dark images to guide the label inference in the\ndark domains; 2) a novel uncertainty-aware annotation and evaluation framework\nand metric for semantic segmentation, including image regions beyond human\nrecognition capability in the evaluation in a principled fashion; 3) the Dark\nZurich dataset, comprising 2416 unlabeled nighttime and 2920 unlabeled twilight\nimages with correspondences to their daytime counterparts plus a set of 201\nnighttime images with fine pixel-level annotations created with our protocol,\nwhich serves as a first benchmark for our novel evaluation. Experiments show\nthat our map-guided curriculum adaptation significantly outperforms\nstate-of-the-art methods on nighttime sets both for standard metrics and our\nuncertainty-aware metric. Furthermore, our uncertainty-aware evaluation reveals\nthat selective invalidation of predictions can improve results on data with\nambiguous content such as our benchmark and profit safety-oriented applications\ninvolving invalid inputs.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 16:54:38 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 15:26:43 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Sakaridis", "Christos", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "2005.14565", "submitter": "Cristiano Premebida", "authors": "G. Melotti, C. Premebida, J.J. Bird, D.R. Faria, N. Gon\\c{c}alves", "title": "Probabilistic Object Classification using CNN ML-MAP layers", "comments": "Accepted at the Workshop on Perception for Autonomous Driving (PAD),\n  European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks are currently the state-of-the-art for sensory perception in\nautonomous driving and robotics. However, deep models often generate\noverconfident predictions precluding proper probabilistic interpretation which\nwe argue is due to the nature of the SoftMax layer. To reduce the\noverconfidence without compromising the classification performance, we\nintroduce a CNN probabilistic approach based on distributions calculated in the\nnetwork's Logit layer. The approach enables Bayesian inference by means of ML\nand MAP layers. Experiments with calibrated and the proposed prediction layers\nare carried out on object classification using data from the KITTI database.\nResults are reported for camera ($RGB$) and LiDAR (range-view) modalities,\nwhere the new approach shows promising performance compared to SoftMax.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 13:34:15 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 11:37:53 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Melotti", "G.", ""], ["Premebida", "C.", ""], ["Bird", "J. J.", ""], ["Faria", "D. R.", ""], ["Gon\u00e7alves", "N.", ""]]}, {"id": "2005.14600", "submitter": "Hengyue Pan", "authors": "Hengyue Pan, Xin Niu, Rongchun Li, Siqi Shen, Yong Dou", "title": "Fixed-size Objects Encoding for Visual Relationship Detection", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a fixed-size object encoding method (FOE-VRD) to\nimprove performance of visual relationship detection tasks. Comparing with\nprevious methods, FOE-VRD has an important feature, i.e., it uses one\nfixed-size vector to encoding all objects in each input image to assist the\nprocess of relationship detection. Firstly, we use a regular convolution neural\nnetwork as a feature extractor to generate high-level features of input images.\nThen, for each relationship triplet in input images, i.e.,\n$<$subject-predicate-object$>$, we apply ROI-pooling to get feature vectors of\ntwo regions on the feature maps that corresponding to bounding boxes of the\nsubject and object. Besides the subject and object, our analysis implies that\nthe results of predicate classification may also related to the rest objects in\ninput images (we call them background objects). Due to the variable number of\nbackground objects in different images and computational costs, we cannot\ngenerate feature vectors for them one-by-one by using ROI pooling technique.\nInstead, we propose a novel method to encode all background objects in each\nimage by using one fixed-size vector (i.e., FBE vector). By concatenating the 3\nvectors we generate above, we successfully encode the objects using one\nfixed-size vector. The generated feature vector is then feed into a fully\nconnected neural network to get predicate classification results. Experimental\nresults on VRD database (entire set and zero-shot tests) show that the proposed\nmethod works well on both predicate classification and relationship detection.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 14:36:25 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Pan", "Hengyue", ""], ["Niu", "Xin", ""], ["Li", "Rongchun", ""], ["Shen", "Siqi", ""], ["Dou", "Yong", ""]]}, {"id": "2005.14638", "submitter": "Rui Shao", "authors": "Rui Shao, Pramuditha Perera, Pong C. Yuen, Vishal M. Patel", "title": "Federated Face Presentation Attack Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face presentation attack detection (fPAD) plays a critical role in the modern\nface recognition pipeline. A face presentation attack detection model with good\ngeneralization can be obtained when it is trained with face images from\ndifferent input distributions and different types of spoof attacks. In reality,\ntraining data (both real face images and spoof images) are not directly shared\nbetween data owners due to legal and privacy issues. In this paper, with the\nmotivation of circumventing this challenge, we propose Federated Face\nPresentation Attack Detection (FedPAD) framework. FedPAD simultaneously takes\nadvantage of rich fPAD information available at different data owners while\npreserving data privacy. In the proposed framework, each data owner (referred\nto as \\textit{data centers}) locally trains its own fPAD model. A server learns\na global fPAD model by iteratively aggregating model updates from all data\ncenters without accessing private data in each of them. Once the learned global\nmodel converges, it is used for fPAD inference. We introduce the experimental\nsetting to evaluate the proposed FedPAD framework and carry out extensive\nexperiments to provide various insights about federated learning for fPAD.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 15:56:01 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 03:01:14 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Shao", "Rui", ""], ["Perera", "Pramuditha", ""], ["Yuen", "Pong C.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2005.14684", "submitter": "Fei Shen", "authors": "Fei Shen, Jianqing Zhu, Xiaobin Zhu, Yi Xie, and Jingchang Huang", "title": "Exploring Spatial Significance via Hybrid Pyramidal Graph Network for\n  Vehicle Re-identification", "comments": null, "journal-ref": "IEEE Transactions on Intelligent Transportation Systems, 2021", "doi": "10.1109/TITS.2021.3086142", "report-no": "1-12", "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing vehicle re-identification methods commonly use spatial pooling\noperations to aggregate feature maps extracted via off-the-shelf backbone\nnetworks. They ignore exploring the spatial significance of feature maps,\neventually degrading the vehicle re-identification performance. In this paper,\nfirstly, an innovative spatial graph network (SGN) is proposed to elaborately\nexplore the spatial significance of feature maps. The SGN stacks multiple\nspatial graphs (SGs). Each SG assigns feature map's elements as nodes and\nutilizes spatial neighborhood relationships to determine edges among nodes.\nDuring the SGN's propagation, each node and its spatial neighbors on an SG are\naggregated to the next SG. On the next SG, each aggregated node is re-weighted\nwith a learnable parameter to find the significance at the corresponding\nlocation. Secondly, a novel pyramidal graph network (PGN) is designed to\ncomprehensively explore the spatial significance of feature maps at multiple\nscales. The PGN organizes multiple SGNs in a pyramidal manner and makes each\nSGN handles feature maps of a specific scale. Finally, a hybrid pyramidal graph\nnetwork (HPGN) is developed by embedding the PGN behind a ResNet-50 based\nbackbone network. Extensive experiments on three large scale vehicle databases\n(i.e., VeRi776, VehicleID, and VeRi-Wild) demonstrate that the proposed HPGN is\nsuperior to state-of-the-art vehicle re-identification approaches.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 17:21:12 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 02:23:37 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Shen", "Fei", ""], ["Zhu", "Jianqing", ""], ["Zhu", "Xiaobin", ""], ["Xie", "Yi", ""], ["Huang", "Jingchang", ""]]}, {"id": "2005.14711", "submitter": "Bin Yang", "authors": "Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu, Sergio Casas,\n  Raquel Urtasun", "title": "PnPNet: End-to-End Perception and Prediction with Tracking in the Loop", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We tackle the problem of joint perception and motion forecasting in the\ncontext of self-driving vehicles. Towards this goal we propose PnPNet, an\nend-to-end model that takes as input sequential sensor data, and outputs at\neach time step object tracks and their future trajectories. The key component\nis a novel tracking module that generates object tracks online from detections\nand exploits trajectory level features for motion forecasting. Specifically,\nthe object tracks get updated at each time step by solving both the data\nassociation problem and the trajectory estimation problem. Importantly, the\nwhole model is end-to-end trainable and benefits from joint optimization of all\ntasks. We validate PnPNet on two large-scale driving datasets, and show\nsignificant improvements over the state-of-the-art with better occlusion\nrecovery and more accurate future prediction.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 17:57:25 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 21:32:07 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Liang", "Ming", ""], ["Yang", "Bin", ""], ["Zeng", "Wenyuan", ""], ["Chen", "Yun", ""], ["Hu", "Rui", ""], ["Casas", "Sergio", ""], ["Urtasun", "Raquel", ""]]}]