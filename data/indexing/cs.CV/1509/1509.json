[{"id": "1509.00083", "submitter": "Samuel Kadoury", "authors": "Samuel Kadoury, Eugene Vorontsov, An Tang", "title": "Metastatic liver tumour segmentation from discriminant Grassmannian\n  manifolds", "comments": null, "journal-ref": "Physics in Medicine and Biology 60 (2015)", "doi": "10.1088/0031-9155/60/16/6459", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The early detection, diagnosis and monitoring of liver cancer progression can\nbe achieved with the precise delineation of metastatic tumours. However,\naccurate automated segmentation remains challenging due to the presence of\nnoise, inhomogeneity and the high appearance variability of malignant tissue.\nIn this paper, we propose an unsupervised metastatic liver tumour segmentation\nframework using a machine learning approach based on discriminant Grassmannian\nmanifolds which learns the appearance of tumours with respect to normal tissue.\nFirst, the framework learns within-class and between-class similarity\ndistributions from a training set of images to discover the optimal manifold\ndiscrimination between normal and pathological tissue in the liver. Second, a\nconditional optimisation scheme computes nonlocal pairwise as well as\npattern-based clique potentials from the manifold subspace to recognise regions\nwith similar labelings and to incorporate global consistency in the\nsegmentation process. The proposed framework was validated on a clinical\ndatabase of 43 CT images from patients with metastatic liver cancer. Compared\nto state-of-the-art methods, our method achieves a better performance on two\nseparate datasets of metastatic liver tumours from different clinical sites,\nyielding an overall mean Dice similarity coefficient of 90.7 +/- 2.4 in over 50\ntumours with an average volume of 27.3 mm3.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 21:45:40 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Kadoury", "Samuel", ""], ["Vorontsov", "Eugene", ""], ["Tang", "An", ""]]}, {"id": "1509.00111", "submitter": "Alexander Wong", "authors": "Audrey G. Chung, Mohammad Javad Shafiee, Devinder Kumar, Farzad\n  Khalvati, Masoom A. Haider, Alexander Wong", "title": "Discovery Radiomics for Multi-Parametric MRI Prostate Cancer Detection", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prostate cancer is the most diagnosed form of cancer in Canadian men, and is\nthe third leading cause of cancer death. Despite these statistics, prognosis is\nrelatively good with a sufficiently early diagnosis, making fast and reliable\nprostate cancer detection crucial. As imaging-based prostate cancer screening,\nsuch as magnetic resonance imaging (MRI), requires an experienced medical\nprofessional to extensively review the data and perform a diagnosis,\nradiomics-driven methods help streamline the process and has the potential to\nsignificantly improve diagnostic accuracy and efficiency, and thus improving\npatient survival rates. These radiomics-driven methods currently rely on\nhand-crafted sets of quantitative imaging-based features, which are selected\nmanually and can limit their ability to fully characterize unique prostate\ncancer tumour phenotype. In this study, we propose a novel \\textit{discovery\nradiomics} framework for generating custom radiomic sequences tailored for\nprostate cancer detection. Discovery radiomics aims to uncover abstract\nimaging-based features that capture highly unique tumour traits and\ncharacteristics beyond what can be captured using predefined feature models. In\nthis paper, we discover new custom radiomic sequencers for generating new\nprostate radiomic sequences using multi-parametric MRI data. We evaluated the\nperformance of the discovered radiomic sequencer against a state-of-the-art\nhand-crafted radiomic sequencer for computer-aided prostate cancer detection\nwith a feedforward neural network using real clinical prostate multi-parametric\nMRI data. Results for the discovered radiomic sequencer demonstrate good\nperformance in prostate cancer detection and clinical decision support relative\nto the hand-crafted radiomic sequencer. The use of discovery radiomics shows\npotential for more efficient and reliable automatic prostate cancer detection.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 01:40:23 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 01:17:25 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2015 01:49:05 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Chung", "Audrey G.", ""], ["Shafiee", "Mohammad Javad", ""], ["Kumar", "Devinder", ""], ["Khalvati", "Farzad", ""], ["Haider", "Masoom A.", ""], ["Wong", "Alexander", ""]]}, {"id": "1509.00116", "submitter": "M. Salman Asif", "authors": "M. Salman Asif, Ali Ayremlou, Aswin Sankaranarayanan, Ashok\n  Veeraraghavan, and Richard Baraniuk", "title": "FlatCam: Thin, Bare-Sensor Cameras using Coded Aperture and Computation", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FlatCam is a thin form-factor lensless camera that consists of a coded mask\nplaced on top of a bare, conventional sensor array. Unlike a traditional,\nlens-based camera where an image of the scene is directly recorded on the\nsensor pixels, each pixel in FlatCam records a linear combination of light from\nmultiple scene elements. A computational algorithm is then used to demultiplex\nthe recorded measurements and reconstruct an image of the scene. FlatCam is an\ninstance of a coded aperture imaging system; however, unlike the vast majority\nof related work, we place the coded mask extremely close to the image sensor\nthat can enable a thin system. We employ a separable mask to ensure that both\ncalibration and image reconstruction are scalable in terms of memory\nrequirements and computational complexity. We demonstrate the potential of the\nFlatCam design using two prototypes: one at visible wavelengths and one at\ninfrared wavelengths.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 01:59:59 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 20:09:57 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Asif", "M. Salman", ""], ["Ayremlou", "Ali", ""], ["Sankaranarayanan", "Aswin", ""], ["Veeraraghavan", "Ashok", ""], ["Baraniuk", "Richard", ""]]}, {"id": "1509.00117", "submitter": "Alexander Wong", "authors": "Devinder Kumar, Mohammad Javad Shafiee, Audrey G. Chung, Farzad\n  Khalvati, Masoom A. Haider, Alexander Wong", "title": "Discovery Radiomics for Pathologically-Proven Computed Tomography Lung\n  Cancer Prediction", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung cancer is the leading cause for cancer related deaths. As such, there is\nan urgent need for a streamlined process that can allow radiologists to provide\ndiagnosis with greater efficiency and accuracy. A powerful tool to do this is\nradiomics: a high-dimension imaging feature set. In this study, we take the\nidea of radiomics one step further by introducing the concept of discovery\nradiomics for lung cancer prediction using CT imaging data. In this study, we\nrealize these custom radiomic sequencers as deep convolutional sequencers using\na deep convolutional neural network learning architecture. To illustrate the\nprognostic power and effectiveness of the radiomic sequences produced by the\ndiscovered sequencer, we perform cancer prediction between malignant and benign\nlesions from 97 patients using the pathologically-proven diagnostic data from\nthe LIDC-IDRI dataset. Using the clinically provided pathologically-proven data\nas ground truth, the proposed framework provided an average accuracy of 77.52%\nvia 10-fold cross-validation with a sensitivity of 79.06% and specificity of\n76.11%, surpassing the state-of-the art method.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 02:00:56 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 19:10:18 GMT"}, {"version": "v3", "created": "Tue, 28 Mar 2017 02:01:31 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Kumar", "Devinder", ""], ["Shafiee", "Mohammad Javad", ""], ["Chung", "Audrey G.", ""], ["Khalvati", "Farzad", ""], ["Haider", "Masoom A.", ""], ["Wong", "Alexander", ""]]}, {"id": "1509.00151", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Shiyu Chang, Jiayu Zhou, Meng Wang, Thomas S. Huang", "title": "Learning A Task-Specific Deep Architecture For Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While sparse coding-based clustering methods have shown to be successful,\ntheir bottlenecks in both efficiency and scalability limit the practical usage.\nIn recent years, deep learning has been proved to be a highly effective,\nefficient and scalable feature learning tool. In this paper, we propose to\nemulate the sparse coding-based clustering pipeline in the context of deep\nlearning, leading to a carefully crafted deep model benefiting from both. A\nfeed-forward network structure, named TAGnet, is constructed based on a\ngraph-regularized sparse coding algorithm. It is then trained with\ntask-specific loss functions from end to end. We discover that connecting deep\nlearning to sparse coding benefits not only the model performance, but also its\ninitialization and interpretation. Moreover, by introducing auxiliary\nclustering tasks to the intermediate feature hierarchy, we formulate DTAGnet\nand obtain a further performance boost. Extensive experiments demonstrate that\nthe proposed model gains remarkable margins over several state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 06:12:29 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2015 18:32:27 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2015 06:38:37 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Wang", "Zhangyang", ""], ["Chang", "Shiyu", ""], ["Zhou", "Jiayu", ""], ["Wang", "Meng", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1509.00154", "submitter": "Jalil Rasekhi", "authors": "Jalil Rasekhi", "title": "Tumor Motion Tracking in Liver Ultrasound Images Using Mean Shift and\n  Active Contour", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equation 5,6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new method for motion tracking of tumors in liver\nultrasound image sequences. Our algorithm has two main steps. In the first\nstep, we apply mean shift algorithm with multiple features to estimate the\ncenter of the target in each frame. Target in the first frame is defined using\nan ellipse. Edge, texture, and intensity features are extracted from the first\nframe, and then mean shift algorithm is applied to each feature separately to\nfind the center of ellipse related to that feature in the next frame. The\ncenter of ellipse will be the weighted average of these centers. By using mean\nshift actually we estimate the target movement between two consecutive frames.\nOnce the correct ellipsoid in each frame is known, in the second step we apply\nthe Dynamic Directional Gradient Vector Flow (DDGVF) version of active contour\nmodels, in order to find the correct boundary of tumors. We sample a few points\non the boundary of active contour then translate those points based on the\ntranslation of the center of ellipsoid in two consecutive frames to determine\nthe target movement. We use these translated sample points as an initial guess\nfor active contour in the next frame. Our experimental results show that, the\nsuggested method provides a reliable performance for liver tumor tracking in\nultrasound image sequences.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 06:21:44 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2015 11:22:42 GMT"}, {"version": "v3", "created": "Wed, 30 Sep 2015 21:09:05 GMT"}, {"version": "v4", "created": "Wed, 14 Oct 2015 23:15:38 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Rasekhi", "Jalil", ""]]}, {"id": "1509.00244", "submitter": "Dacheng Tao", "authors": "Changxing Ding, Dacheng Tao", "title": "Robust Face Recognition via Multimodal Deep Face Representation", "comments": "To appear in IEEE Trans. Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2015.2477042", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face images appeared in multimedia applications, e.g., social networks and\ndigital entertainment, usually exhibit dramatic pose, illumination, and\nexpression variations, resulting in considerable performance degradation for\ntraditional face recognition algorithms. This paper proposes a comprehensive\ndeep learning framework to jointly learn face representation using multimodal\ninformation. The proposed deep learning structure is composed of a set of\nelaborately designed convolutional neural networks (CNNs) and a three-layer\nstacked auto-encoder (SAE). The set of CNNs extracts complementary facial\nfeatures from multimodal data. Then, the extracted features are concatenated to\nform a high-dimensional feature vector, whose dimension is compressed by SAE.\nAll the CNNs are trained using a subset of 9,000 subjects from the publicly\navailable CASIA-WebFace database, which ensures the reproducibility of this\nwork. Using the proposed single CNN architecture and limited training data,\n98.43% verification rate is achieved on the LFW database. Benefited from the\ncomplementary information contained in multimodal data, our small ensemble\nsystem achieves higher than 99.0% recognition rate on LFW using publicly\navailable training set.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 12:09:06 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Ding", "Changxing", ""], ["Tao", "Dacheng", ""]]}, {"id": "1509.00296", "submitter": "Tae-Hyun Oh", "authors": "Tae-Hyun Oh, Yasuyuki Matsushita, Yu-Wing Tai, In So Kweon", "title": "Fast Randomized Singular Value Thresholding for Low-rank Optimization", "comments": "Appeared in CVPR 2015, and under major revision of TPAMI. Source code\n  is available on http://thoh.kaist.ac.kr", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank minimization can be converted into tractable surrogate problems, such as\nNuclear Norm Minimization (NNM) and Weighted NNM (WNNM). The problems related\nto NNM, or WNNM, can be solved iteratively by applying a closed-form proximal\noperator, called Singular Value Thresholding (SVT), or Weighted SVT, but they\nsuffer from high computational cost of Singular Value Decomposition (SVD) at\neach iteration. We propose a fast and accurate approximation method for SVT,\nthat we call fast randomized SVT (FRSVT), with which we avoid direct\ncomputation of SVD. The key idea is to extract an approximate basis for the\nrange of the matrix from its compressed matrix. Given the basis, we compute\npartial singular values of the original matrix from the small factored matrix.\nIn addition, by developping a range propagation method, our method further\nspeeds up the extraction of approximate basis at each iteration. Our\ntheoretical analysis shows the relationship between the approximation bound of\nSVD and its effect to NNM via SVT. Along with the analysis, our empirical\nresults quantitatively and qualitatively show that our approximation rarely\nharms the convergence of the host algorithms. We assess the efficiency and\naccuracy of the proposed method on various computer vision problems, e.g.,\nsubspace clustering, weather artifact removal, and simultaneous multi-image\nalignment and rectification.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 13:49:11 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 17:43:05 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Oh", "Tae-Hyun", ""], ["Matsushita", "Yasuyuki", ""], ["Tai", "Yu-Wing", ""], ["Kweon", "In So", ""]]}, {"id": "1509.00313", "submitter": "Amit K.C.", "authors": "Amit Kumar K.C., Damien Delannay and Christophe De Vleeschouwer", "title": "Iterative hypothesis testing for multi-object tracking in presence of\n  features with variable reliability", "comments": "21 pages, 8 figures, submitted to CVIU: Special Issue on Visual\n  Tracking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper assumes prior detections of multiple targets at each time instant,\nand uses a graph-based approach to connect those detections across time, based\non their position and appearance estimates. In contrast to most earlier works\nin the field, our framework has been designed to exploit the appearance\nfeatures, even when they are only sporadically available, or affected by a\nnon-stationary noise, along the sequence of detections. This is done by\nimplementing an iterative hypothesis testing strategy to progressively\naggregate the detections into short trajectories, named tracklets.\nSpecifically, each iteration considers a node, named key-node, and investigates\nhow to link this key-node with other nodes in its neighborhood, under the\nassumption that the target appearance is defined by the key-node appearance\nestimate. This is done through shortest path computation in a temporal\nneighborhood of the key-node. The approach is conservative in that it only\naggregates the shortest paths that are sufficiently better compared to\nalternative paths. It is also multi-scale in that the size of the investigated\nneighborhood is increased proportionally to the number of detections already\naggregated into the key-node. The multi-scale nature of the process and the\nprogressive relaxation of its conservativeness makes it both computationally\nefficient and effective.\n  Experimental validations are performed extensively on a toy example, a 15\nminutes long multi-view basketball dataset, and other monocular pedestrian\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 14:27:50 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["C.", "Amit Kumar K.", ""], ["Delannay", "Damien", ""], ["De Vleeschouwer", "Christophe", ""]]}, {"id": "1509.00552", "submitter": "Bing Shuai", "authors": "Bing Shuai, Zhen Zuo, Gang Wang, Bing Wang", "title": "DAG-Recurrent Neural Networks For Scene Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image labeling, local representations for image units are usually\ngenerated from their surrounding image patches, thus long-range contextual\ninformation is not effectively encoded. In this paper, we introduce recurrent\nneural networks (RNNs) to address this issue. Specifically, directed acyclic\ngraph RNNs (DAG-RNNs) are proposed to process DAG-structured images, which\nenables the network to model long-range semantic dependencies among image\nunits. Our DAG-RNNs are capable of tremendously enhancing the discriminative\npower of local representations, which significantly benefits the local\nclassification. Meanwhile, we propose a novel class weighting function that\nattends to rare classes, which phenomenally boosts the recognition accuracy for\nnon-frequent classes. Integrating with convolution and deconvolution layers,\nour DAG-RNNs achieve new state-of-the-art results on the challenging SiftFlow,\nCamVid and Barcelona benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 03:09:23 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 12:27:52 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Shuai", "Bing", ""], ["Zuo", "Zhen", ""], ["Wang", "Gang", ""], ["Wang", "Bing", ""]]}, {"id": "1509.00568", "submitter": "Michael (Micky) Fire", "authors": "Michael Fire and Jonathan Schler", "title": "Exploring Online Ad Images Using a Deep Convolutional Neural Network\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online advertising is a huge, rapidly growing advertising market in today's\nworld. One common form of online advertising is using image ads. A decision is\nmade (often in real time) every time a user sees an ad, and the advertiser is\neager to determine the best ad to display. Consequently, many algorithms have\nbeen developed that calculate the optimal ad to show to the current user at the\npresent time. Typically, these algorithms focus on variations of the ad,\noptimizing among different properties such as background color, image size, or\nset of images. However, there is a more fundamental layer. Our study looks at\nnew qualities of ads that can be determined before an ad is shown (rather than\nonline optimization) and defines which ads are most likely to be successful.\n  We present a set of novel algorithms that utilize deep-learning image\nprocessing, machine learning, and graph theory to investigate online\nadvertising and to construct prediction models which can foresee an image ad's\nsuccess. We evaluated our algorithms on a dataset with over 260,000 ad images,\nas well as a smaller dataset specifically related to the automotive industry,\nand we succeeded in constructing regression models for ad image click rate\nprediction. The obtained results emphasize the great potential of using\ndeep-learning algorithms to effectively and efficiently analyze image ads and\nto create better and more innovative online ads. Moreover, the algorithms\npresented in this paper can help predict ad success and can be applied to\nanalyze other large-scale image corpora.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 06:18:27 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Fire", "Michael", ""], ["Schler", "Jonathan", ""]]}, {"id": "1509.00651", "submitter": "Changzhi Luo Mr.", "authors": "Changzhi Luo, Bingbing Ni, Jun Yuan, Jianfeng Wang, Shuicheng Yan,\n  Meng Wang", "title": "Manipulated Object Proposal: A Discriminative Object Extraction and\n  Feature Fusion Framework for First-Person Daily Activity Recognition", "comments": "This paper has been withdrawn by the author due to incomplete\n  experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and recognizing objects interacting with humans lie in the center\nof first-person (egocentric) daily activity recognition. However, due to noisy\ncamera motion and frequent changes in viewpoint and scale, most of the previous\negocentric action recognition methods fail to capture and model highly\ndiscriminative object features. In this work, we propose a novel pipeline for\nfirst-person daily activity recognition, aiming at more discriminative object\nfeature representation and object-motion feature fusion. Our object feature\nextraction and representation pipeline is inspired by the recent success of\nobject hypotheses and deep convolutional neural network based detection\nframeworks. Our key contribution is a simple yet effective manipulated object\nproposal generation scheme. This scheme leverages motion cues such as motion\nboundary and motion magnitude (in contrast, camera motion is usually considered\nas \"noise\" for most previous methods) to generate a more compact and\ndiscriminative set of object proposals, which are more closely related to the\nobjects which are being manipulated. Then, we learn more discriminative object\ndetectors from these manipulated object proposals based on region-based\nconvolutional neural network (R-CNN). Meanwhile, we develop a network based\nfeature fusion scheme which better combines object and motion features. We show\nin experiments that the proposed framework significantly outperforms the\nstate-of-the-art recognition performance on a challenging first-person daily\nactivity benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 11:51:48 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 03:11:41 GMT"}, {"version": "v3", "created": "Tue, 31 May 2016 12:26:08 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Luo", "Changzhi", ""], ["Ni", "Bingbing", ""], ["Yuan", "Jun", ""], ["Wang", "Jianfeng", ""], ["Yan", "Shuicheng", ""], ["Wang", "Meng", ""]]}, {"id": "1509.00714", "submitter": "Nitish Chandra", "authors": "Nitish Chandra and Kedar Khare", "title": "Dictionary based Approach to Edge Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge detection is a very essential part of image processing, as quality and\naccuracy of detection determines the success of further processing. We have\ndeveloped a new self learning technique for edge detection using dictionary\ncomprised of eigenfilters constructed using features of the input image. The\ndictionary based method eliminates the need of pre or post processing of the\nimage and accounts for noise, blurriness, class of image and variation of\nillumination during the detection process itself. Since, this method depends on\nthe characteristics of the image, the new technique can detect edges more\naccurately and capture greater detail than existing algorithms such as Sobel,\nPrewitt Laplacian of Gaussian, Canny method etc which use generic filters and\noperators. We have demonstrated its application on various classes of images\nsuch as text, face, barcodes, traffic and cell images. An application of this\ntechnique to cell counting in a microscopic image is also presented.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 14:09:13 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Chandra", "Nitish", ""], ["Khare", "Kedar", ""]]}, {"id": "1509.00728", "submitter": "Florian Bernard", "authors": "Johan Thunberg, Florian Bernard, Jorge Goncalves", "title": "On Transitive Consistency for Linear Invertible Transformations between\n  Euclidean Coordinate Systems", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.MA cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transitive consistency is an intrinsic property for collections of linear\ninvertible transformations between Euclidean coordinate frames. In practice,\nwhen the transformations are estimated from data, this property is lacking.\nThis work addresses the problem of synchronizing transformations that are not\ntransitively consistent. Once the transformations have been synchronized, they\nsatisfy the transitive consistency condition - a transformation from frame $A$\nto frame $C$ is equal to the composite transformation of first transforming A\nto B and then transforming B to C. The coordinate frames correspond to nodes in\na graph and the transformations correspond to edges in the same graph. Two\ndirect or centralized synchronization methods are presented for different graph\ntopologies; the first one for quasi-strongly connected graphs, and the second\none for connected graphs. As an extension of the second method, an iterative\nGauss-Newton method is presented, which is later adapted to the case of affine\nand Euclidean transformations. Two distributed synchronization methods are also\npresented for orthogonal matrices, which can be seen as distributed versions of\nthe two direct or centralized methods; they are similar in nature to standard\nconsensus protocols used for distributed averaging. When the transformations\nare orthogonal matrices, a bound on the optimality gap can be computed.\nSimulations show that the gap is almost right, even for noise large in\nmagnitude. This work also contributes on a theoretical level by providing\nlinear algebraic relationships for transitively consistent transformations. One\nof the benefits of the proposed methods is their simplicity - basic linear\nalgebraic methods are used, e.g., the Singular Value Decomposition (SVD). For a\nwide range of parameter settings, the methods are numerically validated.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 14:57:16 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Thunberg", "Johan", ""], ["Bernard", "Florian", ""], ["Goncalves", "Jorge", ""]]}, {"id": "1509.00816", "submitter": "Suren Jayasuriya", "authors": "Suren Jayasuriya, Adithya Pediredla, Sriram Sivaramakrishnan, Alyosha\n  Molnar, Ashok Veeraraghavan", "title": "Depth Fields: Extending Light Field Techniques to Time-of-Flight Imaging", "comments": "9 pages, 8 figures, Accepted to 3DV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of techniques such as light field, structured illumination, and\ntime-of-flight (TOF) are commonly used for depth acquisition in consumer\nimaging, robotics and many other applications. Unfortunately, each technique\nsuffers from its individual limitations preventing robust depth sensing. In\nthis paper, we explore the strengths and weaknesses of combining light field\nand time-of-flight imaging, particularly the feasibility of an on-chip\nimplementation as a single hybrid depth sensor. We refer to this combination as\ndepth field imaging. Depth fields combine light field advantages such as\nsynthetic aperture refocusing with TOF imaging advantages such as high depth\nresolution and coded signal processing to resolve multipath interference. We\nshow applications including synthesizing virtual apertures for TOF imaging,\nimproved depth mapping through partial and scattering occluders, and single\nfrequency TOF phase unwrapping. Utilizing space, angle, and temporal coding,\ndepth fields can improve depth sensing in the wild and generate new insights\ninto the dimensions of light's plenoptic function.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 18:49:18 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Jayasuriya", "Suren", ""], ["Pediredla", "Adithya", ""], ["Sivaramakrishnan", "Sriram", ""], ["Molnar", "Alyosha", ""], ["Veeraraghavan", "Ashok", ""]]}, {"id": "1509.01074", "submitter": "Ahmed Mohamed", "authors": "Ahmed Nabil Mohamed", "title": "A Novice Guide towards Human Motion Analysis and Understanding", "comments": "35 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion analysis and understanding has been, and is still, the focus of\nattention of many disciplines which is considered an obvious indicator of the\nwide and massive importance of the subject. The purpose of this article is to\nshed some light on this very important subject, so it can be a good insight for\na novice computer vision researcher in this field by providing him/her with a\nwealth of knowledge about the subject covering many directions. There are two\nmain contributions of this article. The first one investigates various aspects\nof some disciplines (e.g., arts, philosophy, psychology, and neuroscience) that\nare interested in the subject and review some of their contributions stressing\non those that can be useful for computer vision researchers. Moreover, many\nexamples are illustrated to indicate the benefits of integrating concepts and\nresults among different disciplines. The second contribution is concerned with\nthe subject from the computer vision aspect where we discuss the following\nissues. First, we explore many demanding and promising applications to reveal\nthe wide and massive importance of the field. Second, we list various types of\nsensors that may be used for acquiring various data. Third, we review different\ntaxonomies used for classifying motions. Fourth, we review various processes\ninvolved in motion analysis. Fifth, we exhibit how different surveys are\nstructured. Sixth, we examine many of the most cited and recent reviews in the\nfield that have been published during the past two decades to reveal various\napproaches used for implementing different stages of the problem and refer to\nvarious algorithms and their suitability for different situations. Moreover, we\nprovide a long list of public datasets and discuss briefly some examples of\nthese datasets. Finally, we provide a general discussion of the subject from\nthe aspect of computer vision.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 13:25:37 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Mohamed", "Ahmed Nabil", ""]]}, {"id": "1509.01122", "submitter": "Caio C\\'esar Teodoro Mendes", "authors": "Caio C\\'esar Teodoro Mendes, Vincent Fr\\'emont and Denis Fernando Wolf", "title": "Vision-Based Road Detection using Contextual Blocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road detection is a fundamental task in autonomous navigation systems. In\nthis paper, we consider the case of monocular road detection, where images are\nsegmented into road and non-road regions. Our starting point is the well-known\nmachine learning approach, in which a classifier is trained to distinguish road\nand non-road regions based on hand-labeled images. We proceed by introducing\nthe use of \"contextual blocks\" as an efficient way of providing contextual\ninformation to the classifier. Overall, the proposed methodology, including its\nimage feature selection and classifier, was conceived with computational cost\nin mind, leaving room for optimized implementations. Regarding experiments, we\nperform a sensible evaluation of each phase and feature subset that composes\nour system. The results show a great benefit from using contextual blocks and\ndemonstrate their computational efficiency. Finally, we submit our results to\nthe KITTI road detection benchmark achieving scores comparable with state of\nthe art methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 15:24:06 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Mendes", "Caio C\u00e9sar Teodoro", ""], ["Fr\u00e9mont", "Vincent", ""], ["Wolf", "Denis Fernando", ""]]}, {"id": "1509.01220", "submitter": "Moshe Ben-Ezra", "authors": "Moshe Ben-Ezra", "title": "Light Efficient Flutter Shutter", "comments": "This documnet and the code listing in it are submitted under the\n  permissive MIT License in hope it will be useful. In case anyone is\n  interesting in 2012 date confirmation - the documnet was notarized at MIT on\n  5 Dec 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flutter shutter is a technique in which the exposure is chopped into segments\nand light is only integrated part of the time. By carefully selecting the\nchopping sequence it is possible to better condition the data for\nreconstruction problems such as motion deblurring, focal sweeping, and\ncompressed sensing. The partial exposure trades better conditioning for less\nenergy. In problems such as motion deblurring the available energy is what\ncaused the problem in the first place (as strong illumination allows short\nexposure thus eliminates motion blur). It is still beneficial because the\nbenefit from the better conditioning outweighs the cost in energy.\n  This documents is focused on light efficient flutter shutter that provides\nbetter conditioning and better energy utilization than conventional flutter\nshutter.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 00:37:47 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Ben-Ezra", "Moshe", ""]]}, {"id": "1509.01277", "submitter": "Colin Rennie", "authors": "Colin Rennie, Rahul Shome, Kostas E. Bekris, Alberto F. De Souza", "title": "A Dataset for Improved RGBD-based Object Detection and Pose Estimation\n  for Warehouse Pick-and-Place", "comments": "To appear in RA-L", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important logistics application of robotics involves manipulators that\npick-and-place objects placed in warehouse shelves. A critical aspect of this\ntask corre- sponds to detecting the pose of a known object in the shelf using\nvisual data. Solving this problem can be assisted by the use of an RGB-D\nsensor, which also provides depth information beyond visual data. Nevertheless,\nit remains a challenging problem since multiple issues need to be addressed,\nsuch as low illumination inside shelves, clutter, texture-less and reflective\nobjects as well as the limitations of depth sensors. This paper provides a new\nrich data set for advancing the state-of-the-art in RGBD- based 3D object pose\nestimation, which is focused on the challenges that arise when solving\nwarehouse pick- and-place tasks. The publicly available data set includes\nthousands of images and corresponding ground truth data for the objects used\nduring the first Amazon Picking Challenge at different poses and clutter\nconditions. Each image is accompanied with ground truth information to assist\nin the evaluation of algorithms for object detection. To show the utility of\nthe data set, a recent algorithm for RGBD-based pose estimation is evaluated in\nthis paper. Based on the measured performance of the algorithm on the data set,\nvarious modifications and improvements are applied to increase the accuracy of\ndetection. These steps can be easily applied to a variety of different\nmethodologies for object pose detection and improve performance in the domain\nof warehouse pick-and-place.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 20:53:06 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2016 20:05:18 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Rennie", "Colin", ""], ["Shome", "Rahul", ""], ["Bekris", "Kostas E.", ""], ["De Souza", "Alberto F.", ""]]}, {"id": "1509.01287", "submitter": "Filipe Condessa", "authors": "Filipe Condessa, Jos\\'e Bioucas-Dias, Carlos Castro, John Ozolek,\n  Jelena Kova\\v{c}evi\\'c", "title": "Image Classification with Rejection using Contextual Information", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new supervised algorithm for image classification with\nrejection using multiscale contextual information. Rejection is desired in\nimage-classification applications that require a robust classifier but not the\nclassification of the entire image. The proposed algorithm combines local and\nmultiscale contextual information with rejection, improving the classification\nperformance. As a probabilistic model for classification, we adopt a\nmultinomial logistic regression. The concept of rejection with contextual\ninformation is implemented by modeling the classification problem as an energy\nminimization problem over a graph representing local and multiscale\nsimilarities of the image. The rejection is introduced through an energy data\nterm associated with the classification risk and the contextual information\nthrough an energy smoothness term associated with the local and multiscale\nsimilarities within the image. We illustrate the proposed method on the\nclassification of images of H&E-stained teratoma tissues.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 22:04:37 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Condessa", "Filipe", ""], ["Bioucas-Dias", "Jos\u00e9", ""], ["Castro", "Carlos", ""], ["Ozolek", "John", ""], ["Kova\u010devi\u0107", "Jelena", ""]]}, {"id": "1509.01329", "submitter": "Piotr Doll\\'ar", "authors": "Yan Zhu and Yuandong Tian and Dimitris Mexatas and Piotr Doll\\'ar", "title": "Semantic Amodal Segmentation", "comments": "major update including new COCO data, metrics, and baselines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common visual recognition tasks such as classification, object detection, and\nsemantic segmentation are rapidly reaching maturity, and given the recent rate\nof progress, it is not unreasonable to conjecture that techniques for many of\nthese problems will approach human levels of performance in the next few years.\nIn this paper we look to the future: what is the next frontier in visual\nrecognition?\n  We offer one possible answer to this question. We propose a detailed image\nannotation that captures information beyond the visible pixels and requires\ncomplex reasoning about full scene structure. Specifically, we create an amodal\nsegmentation of each image: the full extent of each region is marked, not just\nthe visible pixels. Annotators outline and name all salient regions in the\nimage and specify a partial depth order. The result is a rich scene structure,\nincluding visible and occluded portions of each region, figure-ground edge\ninformation, semantic labels, and object overlap.\n  We create two datasets for semantic amodal segmentation. First, we label 500\nimages in the BSDS dataset with multiple annotators per image, allowing us to\nstudy the statistics of human annotations. We show that the proposed full scene\nannotation is surprisingly consistent between annotators, including for regions\nand edges. Second, we annotate 5000 images from COCO. This larger dataset\nallows us to explore a number of algorithmic ideas for amodal segmentation and\ndepth ordering. We introduce novel metrics for these tasks, and along with our\nstrong baselines, define concrete new challenges for the community.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 02:20:13 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 19:49:24 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Zhu", "Yan", ""], ["Tian", "Yuandong", ""], ["Mexatas", "Dimitris", ""], ["Doll\u00e1r", "Piotr", ""]]}, {"id": "1509.01343", "submitter": "Iman Abbasnejad", "authors": "Iman Abbasnejad, Sridha Sridharan, Simon Denman, Clinton Fookes, Simon\n  Lucey", "title": "Learning Temporal Alignment Uncertainty for Efficient Event Detection", "comments": "Appeared in DICTA 2015, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the problem of efficient video event detection. We\nargue that linear detection functions should be preferred in this regard due to\ntheir scalability and efficiency during estimation and evaluation. A popular\napproach in this regard is to represent a sequence using a bag of words (BOW)\nrepresentation due to its: (i) fixed dimensionality irrespective of the\nsequence length, and (ii) its ability to compactly model the statistics in the\nsequence. A drawback to the BOW representation, however, is the intrinsic\ndestruction of the temporal ordering information. In this paper we propose a\nnew representation that leverages the uncertainty in relative temporal\nalignments between pairs of sequences while not destroying temporal ordering.\nOur representation, like BOW, is of a fixed dimensionality making it easily\nintegrated with a linear detection function. Extensive experiments on CK+,\n6DMG, and UvA-NEMO databases show significant performance improvements across\nboth isolated and continuous event detection tasks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 05:33:42 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Abbasnejad", "Iman", ""], ["Sridharan", "Sridha", ""], ["Denman", "Simon", ""], ["Fookes", "Clinton", ""], ["Lucey", "Simon", ""]]}, {"id": "1509.01354", "submitter": "Jinma Guo", "authors": "Jinma Guo and Jianmin Li", "title": "CNN Based Hashing for Image Retrieval", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Along with data on the web increasing dramatically, hashing is becoming more\nand more popular as a method of approximate nearest neighbor search. Previous\nsupervised hashing methods utilized similarity/dissimilarity matrix to get\nsemantic information. But the matrix is not easy to construct for a new\ndataset. Rather than to reconstruct the matrix, we proposed a straightforward\nCNN-based hashing method, i.e. binarilizing the activations of a fully\nconnected layer with threshold 0 and taking the binary result as hash codes.\nThis method achieved the best performance on CIFAR-10 and was comparable with\nthe state-of-the-art on MNIST. And our experiments on CIFAR-10 suggested that\nthe signs of activations may carry more information than the relative values of\nactivations between samples, and that the co-adaption between feature extractor\nand hash functions is important for hashing.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 07:08:44 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Guo", "Jinma", ""], ["Li", "Jianmin", ""]]}, {"id": "1509.01404", "submitter": "Nicolas Gillis", "authors": "Arnaud Vandaele, Nicolas Gillis, Qi Lei, Kai Zhong, Inderjit Dhillon", "title": "Coordinate Descent Methods for Symmetric Nonnegative Matrix\n  Factorization", "comments": "25 pages, 5 figures, 7 tables. Main changes: comparison with another\n  symNMF algorithm (namely, BetaSNMF), and correction of an error in the\n  convergence proof", "journal-ref": "IEEE Transactions on Signal Processing 64 (21), pp. 5571-5584,\n  2016", "doi": "10.1109/TSP.2016.2591510", "report-no": null, "categories": "cs.NA cs.CV cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a symmetric nonnegative matrix $A$, symmetric nonnegative matrix\nfactorization (symNMF) is the problem of finding a nonnegative matrix $H$,\nusually with much fewer columns than $A$, such that $A \\approx HH^T$. SymNMF\ncan be used for data analysis and in particular for various clustering tasks.\nIn this paper, we propose simple and very efficient coordinate descent schemes\nto solve this problem, and that can handle large and sparse input matrices. The\neffectiveness of our methods is illustrated on synthetic and real-world data\nsets, and we show that they perform favorably compared to recent\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 11:19:35 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 12:50:38 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Vandaele", "Arnaud", ""], ["Gillis", "Nicolas", ""], ["Lei", "Qi", ""], ["Zhong", "Kai", ""], ["Dhillon", "Inderjit", ""]]}, {"id": "1509.01509", "submitter": "Radu Horaud P", "authors": "Israel D. Gebru, Xavier Alameda-Pineda, Florence Forbes and Radu\n  Horaud", "title": "EM Algorithms for Weighted-Data Clustering with Application to\n  Audio-Visual Scene Analysis", "comments": "14 pages, 4 figures, 4 tables", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  volume 38, number 12, 2402 - 2415, 2016", "doi": "10.1109/TPAMI.2016.2522425", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data clustering has received a lot of attention and numerous methods,\nalgorithms and software packages are available. Among these techniques,\nparametric finite-mixture models play a central role due to their interesting\nmathematical properties and to the existence of maximum-likelihood estimators\nbased on expectation-maximization (EM). In this paper we propose a new mixture\nmodel that associates a weight with each observed point. We introduce the\nweighted-data Gaussian mixture and we derive two EM algorithms. The first one\nconsiders a fixed weight for each observation. The second one treats each\nweight as a random variable following a gamma distribution. We propose a model\nselection method based on a minimum message length criterion, provide a weight\ninitialization strategy, and validate the proposed algorithms by comparing them\nwith several state of the art parametric and non-parametric clustering\ntechniques. We also demonstrate the effectiveness and robustness of the\nproposed clustering technique in the presence of heterogeneous data, namely\naudio-visual scene analysis.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 15:51:17 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 11:17:13 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Gebru", "Israel D.", ""], ["Alameda-Pineda", "Xavier", ""], ["Forbes", "Florence", ""], ["Horaud", "Radu", ""]]}, {"id": "1509.01514", "submitter": "Alexander Malyshev", "authors": "Andrew Knyazev and Alexander Malyshev", "title": "Conjugate Gradient Acceleration of Non-Linear Smoothing Filters", "comments": "5 pages, 5 figures, IEEE Conference GlobalSIP 2015", "journal-ref": "2015 IEEE Global Conference on Signal and Information Processing\n  (GlobalSIP), Orlando, FL, 2015, pp. 245-249", "doi": "10.1109/GlobalSIP.2015.7418194", "report-no": "MERL TR2015-143", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most efficient signal edge-preserving smoothing filters, e.g., for\ndenoising, are non-linear. Thus, their acceleration is challenging and is often\nperformed in practice by tuning filter parameters, such as by increasing the\nwidth of the local smoothing neighborhood, resulting in more aggressive\nsmoothing of a single sweep at the cost of increased edge blurring. We propose\nan alternative technology, accelerating the original filters without tuning, by\nrunning them through a special conjugate gradient method, not affecting their\nquality. The filter non-linearity is dealt with by careful freezing and\nrestarting. Our initial numerical experiments on toy one-dimensional signals\ndemonstrate 20x acceleration of the classical bilateral filter and 3-5x\nacceleration of the recently developed guided filter.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 16:06:38 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Knyazev", "Andrew", ""], ["Malyshev", "Alexander", ""]]}, {"id": "1509.01520", "submitter": "Radu Horaud P", "authors": "Sileye Ba, Xavier Alameda-Pineda, Alessio Xompero and Radu Horaud", "title": "An On-line Variational Bayesian Model for Multi-Person Tracking from\n  Cluttered Scenes", "comments": "21 pages, 9 figures, 4 tables", "journal-ref": "Computer Vision and Image Understanding, volume 153, December\n  2016, pages 64-76", "doi": "10.1016/j.cviu.2016.07.006", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking is an ubiquitous problem that appears in many applications\nsuch as remote sensing, audio processing, computer vision, human-machine\ninterfaces, human-robot interaction, etc. Although thoroughly investigated in\ncomputer vision, tracking a time-varying number of persons remains a\nchallenging open problem. In this paper, we propose an on-line variational\nBayesian model for multi-person tracking from cluttered visual observations\nprovided by person detectors. The contributions of this paper are the\nfollowings. First, we propose a variational Bayesian framework for tracking an\nunknown and varying number of persons. Second, our model results in a\nvariational expectation-maximization (VEM) algorithm with closed-form\nexpressions for the posterior distributions of the latent variables and for the\nestimation of the model parameters. Third, the proposed model exploits\nobservations from multiple detectors, and it is therefore multimodal by nature.\nFinally, we propose to embed both object-birth and object-visibility processes\nin an effort to robustly handle person appearances and disappearances over\ntime. Evaluated on classical multiple person tracking datasets, our method\nshows competitive results with respect to state-of-the-art multiple-object\ntracking models, such as the probability hypothesis density (PHD) filter among\nothers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 16:16:42 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 13:06:59 GMT"}, {"version": "v3", "created": "Thu, 30 Jun 2016 08:50:42 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Ba", "Sileye", ""], ["Alameda-Pineda", "Xavier", ""], ["Xompero", "Alessio", ""], ["Horaud", "Radu", ""]]}, {"id": "1509.01602", "submitter": "Ivan Bogun", "authors": "Ivan Bogun, Anelia Angelova and Navdeep Jaitly", "title": "Object Recognition from Short Videos for Robotic Perception", "comments": "7 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become the primary learning technique for object\nrecognition. Videos, unlike still images, are temporally coherent which makes\nthe application of deep networks non-trivial. Here, we investigate how motion\ncan aid object recognition in short videos. Our approach is based on Long\nShort-Term Memory (LSTM) deep networks. Unlike previous applications of LSTMs,\nwe implement each gate as a convolution. We show that convolutional-based LSTM\nmodels are capable of learning motion dependencies and are able to improve the\nrecognition accuracy when more frames in a sequence are available. We evaluate\nour approach on the Washington RGBD Object dataset and on the Washington RGBD\nScenes dataset. Our approach outperforms deep nets applied to still images and\nsets a new state-of-the-art in this domain.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 20:48:23 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Bogun", "Ivan", ""], ["Angelova", "Anelia", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1509.01624", "submitter": "Andrew Knyazev", "authors": "Dong Tian, Hassan Mansour, Andrew Knyazev, Anthony Vetro", "title": "Chebyshev and Conjugate Gradient Filters for Graph Image Denoising", "comments": "6 pages, 6 figures, accepted to 2014 IEEE International Conference on\n  Multimedia and Expo Workshops (ICMEW)", "journal-ref": "Multimedia and Expo Workshops (ICMEW), 2014 IEEE International\n  Conference on, vol., no., pp.1-6, 14-18 July 2014", "doi": "10.1109/ICMEW.2014.6890711", "report-no": "MERL TR2014-062", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 3D image/video acquisition, different views are often captured with\nvarying noise levels across the views. In this paper, we propose a graph-based\nimage enhancement technique that uses a higher quality view to enhance a\ndegraded view. A depth map is utilized as auxiliary information to match the\nperspectives of the two views. Our method performs graph-based filtering of the\nnoisy image by directly computing a projection of the image to be filtered onto\na lower dimensional Krylov subspace of the graph Laplacian. We discuss two\ngraph spectral denoising methods: first using Chebyshev polynomials, and second\nusing iterations of the conjugate gradient algorithm. Our framework generalizes\npreviously known polynomial graph filters, and we demonstrate through numerical\nsimulations that our proposed technique produces subjectively cleaner images\nwith about 1-3 dB improvement in PSNR over existing polynomial graph filters.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 22:22:25 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Tian", "Dong", ""], ["Mansour", "Hassan", ""], ["Knyazev", "Andrew", ""], ["Vetro", "Anthony", ""]]}, {"id": "1509.01654", "submitter": "Yuewei Lin", "authors": "Yuewei Lin, Kareem Ezzeldeen, Youjie Zhou, Xiaochuan Fan, Hongkai Yu,\n  Hui Qian, Song Wang", "title": "Co-interest Person Detection from Multiple Wearable Camera Videos", "comments": "ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable cameras, such as Google Glass and Go Pro, enable video data\ncollection over larger areas and from different views. In this paper, we tackle\na new problem of locating the co-interest person (CIP), i.e., the one who draws\nattention from most camera wearers, from temporally synchronized videos taken\nby multiple wearable cameras. Our basic idea is to exploit the motion patterns\nof people and use them to correlate the persons across different videos,\ninstead of performing appearance-based matching as in traditional video\nco-segmentation/localization. This way, we can identify CIP even if a group of\npeople with similar appearance are present in the view. More specifically, we\ndetect a set of persons on each frame as the candidates of the CIP and then\nbuild a Conditional Random Field (CRF) model to select the one with consistent\nmotion patterns in different videos and high spacial-temporal consistency in\neach video. We collect three sets of wearable-camera videos for testing the\nproposed algorithm. All the involved people have similar appearances in the\ncollected videos and the experiments demonstrate the effectiveness of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2015 01:48:00 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Lin", "Yuewei", ""], ["Ezzeldeen", "Kareem", ""], ["Zhou", "Youjie", ""], ["Fan", "Xiaochuan", ""], ["Yu", "Hongkai", ""], ["Qian", "Hui", ""], ["Wang", "Song", ""]]}, {"id": "1509.01719", "submitter": "Yuewei Lin", "authors": "Yuewei Lin, Jing Chen, Yu Cao, Youjie Zhou, Lingfeng Zhang, Yuan Yan\n  Tang, Song Wang", "title": "Unsupervised Cross-Domain Recognition by Identifying Compact Joint\n  Subspaces", "comments": "ICIP 2015 Top 10% paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new method to solve the cross-domain recognition\nproblem. Different from the traditional domain adaption methods which rely on a\nglobal domain shift for all classes between source and target domain, the\nproposed method is more flexible to capture individual class variations across\ndomains. By adopting a natural and widely used assumption -- \"the data samples\nfrom the same class should lay on a low-dimensional subspace, even if they come\nfrom different domains\", the proposed method circumvents the limitation of the\nglobal domain shift, and solves the cross-domain recognition by finding the\ncompact joint subspaces of source and target domain. Specifically, given\nlabeled samples in source domain, we construct subspaces for each of the\nclasses. Then we construct subspaces in the target domain, called anchor\nsubspaces, by collecting unlabeled samples that are close to each other and\nhighly likely all fall into the same class. The corresponding class label is\nthen assigned by minimizing a cost function which reflects the overlap and\ntopological structure consistency between subspaces across source and target\ndomains, and within anchor subspaces, respectively.We further combine the\nanchor subspaces to corresponding source subspaces to construct the compact\njoint subspaces. Subsequently, one-vs-rest SVM classifiers are trained in the\ncompact joint subspaces and applied to unlabeled data in the target domain. We\nevaluate the proposed method on two widely used datasets: object recognition\ndataset for computer vision tasks, and sentiment classification dataset for\nnatural language processing tasks. Comparison results demonstrate that the\nproposed method outperforms the comparison methods on both datasets.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2015 17:12:21 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Lin", "Yuewei", ""], ["Chen", "Jing", ""], ["Cao", "Yu", ""], ["Zhou", "Youjie", ""], ["Zhang", "Lingfeng", ""], ["Tang", "Yuan Yan", ""], ["Wang", "Song", ""]]}, {"id": "1509.01788", "submitter": "Md Abul Hasnat", "authors": "Md. Abul Hasnat, Olivier Alata and Alain Tr\\'emeau", "title": "Joint Color-Spatial-Directional clustering and Region Merging (JCSD-RM)\n  for unsupervised RGB-D image segmentation", "comments": "submitted to the IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2513407", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in depth imaging sensors provide easy access to the\nsynchronized depth with color, called RGB-D image. In this paper, we propose an\nunsupervised method for indoor RGB-D image segmentation and analysis. We\nconsider a statistical image generation model based on the color and geometry\nof the scene. Our method consists of a joint color-spatial-directional\nclustering method followed by a statistical planar region merging method. We\nevaluate our method on the NYU depth database and compare it with existing\nunsupervised RGB-D segmentation methods. Results show that, it is comparable\nwith the state of the art methods and it needs less computation time. Moreover,\nit opens interesting perspectives to fuse color and geometry in an unsupervised\nmanner.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 09:02:58 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Hasnat", "Md. Abul", ""], ["Alata", "Olivier", ""], ["Tr\u00e9meau", "Alain", ""]]}, {"id": "1509.01947", "submitter": "Hilde Kuehne", "authors": "Hilde Kuehne and Juergen Gall and Thomas Serre", "title": "An end-to-end generative framework for video segmentation and\n  recognition", "comments": "Proc. of IEEE Winter Conference on Applications of Computer Vision\n  (WACV), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an end-to-end generative approach for the segmentation and\nrecognition of human activities. In this approach, a visual representation\nbased on reduced Fisher Vectors is combined with a structured temporal model\nfor recognition. We show that the statistical properties of Fisher Vectors make\nthem an especially suitable front-end for generative models such as Gaussian\nmixtures. The system is evaluated for both the recognition of complex\nactivities as well as their parsing into action units. Using a variety of video\ndatasets ranging from human cooking activities to animal behaviors, our\nexperiments demonstrate that the resulting architecture outperforms\nstate-of-the-art approaches for larger datasets, i.e. when sufficient amount of\ndata is available for training structured generative models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 08:35:48 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 09:43:10 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Kuehne", "Hilde", ""], ["Gall", "Juergen", ""], ["Serre", "Thomas", ""]]}, {"id": "1509.01951", "submitter": "Atul Katole", "authors": "Atul Laxman Katole, Krishna Prasad Yellapragada, Amish Kumar Bedi,\n  Sehaj Singh Kalra and Mynepalli Siva Chaitanya", "title": "Hierarchical Deep Learning Architecture For 10K Objects Classification", "comments": "As appeared in proceedings for CS & IT 2015 - Second International\n  Conference on Computer Science & Engineering (CSEN 2015)", "journal-ref": "Computer Science & Information Technology (CS & IT) (2015) 77-93", "doi": "10.5121/csit.2015.51408", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolution of visual object recognition architectures based on Convolutional\nNeural Networks & Convolutional Deep Belief Networks paradigms has\nrevolutionized artificial Vision Science. These architectures extract & learn\nthe real world hierarchical visual features utilizing supervised & unsupervised\nlearning approaches respectively. Both the approaches yet cannot scale up\nrealistically to provide recognition for a very large number of objects as high\nas 10K. We propose a two level hierarchical deep learning architecture inspired\nby divide & conquer principle that decomposes the large scale recognition\narchitecture into root & leaf level model architectures. Each of the root &\nleaf level models is trained exclusively to provide superior results than\npossible by any 1-level deep learning architecture prevalent today. The\nproposed architecture classifies objects in two steps. In the first step the\nroot level model classifies the object in a high level category. In the second\nstep, the leaf level recognition model for the recognized high level category\nis selected among all the leaf models. This leaf level model is presented with\nthe same input object image which classifies it in a specific category. Also we\npropose a blend of leaf level models trained with either supervised or\nunsupervised learning approaches. Unsupervised learning is suitable whenever\nlabelled data is scarce for the specific leaf level models. Currently the\ntraining of leaf level models is in progress; where we have trained 25 out of\nthe total 47 leaf level models as of now. We have trained the leaf models with\nthe best case top-5 error rate of 3.2% on the validation data set for the\nparticular leaf models. Also we demonstrate that the validation error of the\nleaf level models saturates towards the above mentioned accuracy as the number\nof epochs are increased to more than sixty.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 08:49:39 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Katole", "Atul Laxman", ""], ["Yellapragada", "Krishna Prasad", ""], ["Bedi", "Amish Kumar", ""], ["Kalra", "Sehaj Singh", ""], ["Chaitanya", "Mynepalli Siva", ""]]}, {"id": "1509.01978", "submitter": "Alessia Amelio Dr.", "authors": "Darko Brodic, Alessia Amelio, Zoran N. Milivojevic", "title": "An Approach to the Analysis of the South Slavic Medieval Labels Using\n  Image Texture", "comments": "15 pages, 9 figures, 3rd Workshop on Recognition and Action for Scene\n  Understanding (REACTS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a new script classification method for the discrimination\nof the South Slavic medieval labels. It consists in the textural analysis of\nthe script types. In the first step, each letter is coded by the equivalent\nscript type, which is defined by its typographical features. Obtained coded\ntext is subjected to the run-length statistical analysis and to the adjacent\nlocal binary pattern analysis in order to extract the features. The result\nshows a diversity between the extracted features of the scripts, which makes\nthe feature classification more effective. It is the basis for the\nclassification process of the script identification by using an extension of a\nstate-of-the-art approach for document clustering. The proposed method is\nevaluated on an example of hand-engraved in stone and hand-printed in paper\nlabels in old Cyrillic, angular and round Glagolitic. Experiments demonstrate\nvery positive results, which prove the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 10:39:20 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Brodic", "Darko", ""], ["Amelio", "Alessia", ""], ["Milivojevic", "Zoran N.", ""]]}, {"id": "1509.02027", "submitter": "Wenrui Hu", "authors": "Wenrui Hu, Dacheng Tao, Wensheng Zhang, Yuan Xie, Yehui Yang", "title": "A New Low-Rank Tensor Model for Video Completion", "comments": "8 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new low-rank tensor model based on the circulant\nalgebra, namely, twist tensor nuclear norm or t-TNN for short. The twist tensor\ndenotes a 3-way tensor representation to laterally store 2D data slices in\norder. On one hand, t-TNN convexly relaxes the tensor multi-rank of the twist\ntensor in the Fourier domain, which allows an efficient computation using FFT.\nOn the other, t-TNN is equal to the nuclear norm of block circulant\nmatricization of the twist tensor in the original domain, which extends the\ntraditional matrix nuclear norm in a block circulant way. We test the t-TNN\nmodel on a video completion application that aims to fill missing values and\nthe experiment results validate its effectiveness, especially when dealing with\nvideo recorded by a non-stationary panning camera. The block circulant\nmatricization of the twist tensor can be transformed into a circulant block\nrepresentation with nuclear norm invariance. This representation, after\ntransformation, exploits the horizontal translation relationship between the\nframes in a video, and endows the t-TNN model with a more powerful ability to\nreconstruct panning videos than the existing state-of-the-art low-rank models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 13:19:40 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Hu", "Wenrui", ""], ["Tao", "Dacheng", ""], ["Zhang", "Wensheng", ""], ["Xie", "Yuan", ""], ["Yang", "Yehui", ""]]}, {"id": "1509.02094", "submitter": "Hyun Soo Park", "authors": "Hyun Soo Park, Yedong Niu, Jianbo Shi", "title": "Future Localization from an Egocentric Depth Image", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for future localization: to predict a set of\nplausible trajectories of ego-motion given a depth image. We predict paths\navoiding obstacles, between objects, even paths turning around a corner into\nspace behind objects. As a byproduct of the predicted trajectories of\nego-motion, we discover in the image the empty space occluded by foreground\nobjects. We use no image based features such as semantic labeling/segmentation\nor object detection/recognition for this algorithm. Inspired by proxemics, we\nrepresent the space around a person using an EgoSpace map, akin to an\nillustrated tourist map, that measures a likelihood of occlusion at the\negocentric coordinate system. A future trajectory of ego-motion is modeled by a\nlinear combination of compact trajectory bases allowing us to constrain the\npredicted trajectory. We learn the relationship between the EgoSpace map and\ntrajectory from the EgoMotion dataset providing in-situ measurements of the\nfuture trajectory. A cost function that takes into account partial occlusion\ndue to foreground objects is minimized to predict a trajectory. This cost\nfunction generates a trajectory that passes through the occluded space, which\nallows us to discover the empty space behind the foreground objects. We\nquantitatively evaluate our method to show predictive validity and apply to\nvarious real world scenes including walking, shopping, and social interactions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 15:51:11 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Park", "Hyun Soo", ""], ["Niu", "Yedong", ""], ["Shi", "Jianbo", ""]]}, {"id": "1509.02122", "submitter": "Dagmar Kainmueller", "authors": "Loic A. Royer, David L. Richmond, Carsten Rother, Bjoern Andres,\n  Dagmar Kainmueller", "title": "Convexity Shape Constraints for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting an image into multiple components is a central task in computer\nvision. In many practical scenarios, prior knowledge about plausible components\nis available. Incorporating such prior knowledge into models and algorithms for\nimage segmentation is highly desirable, yet can be non-trivial. In this work,\nwe introduce a new approach that allows, for the first time, to constrain some\nor all components of a segmentation to have convex shapes. Specifically, we\nextend the Minimum Cost Multicut Problem by a class of constraints that enforce\nconvexity. To solve instances of this APX-hard integer linear program to\noptimality, we separate the proposed constraints in the branch-and-cut loop of\na state-of-the-art ILP solver. Results on natural and biological images\ndemonstrate the effectiveness of the approach as well as its advantage over the\nstate-of-the-art heuristic.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 16:58:51 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Royer", "Loic A.", ""], ["Richmond", "David L.", ""], ["Rother", "Carsten", ""], ["Andres", "Bjoern", ""], ["Kainmueller", "Dagmar", ""]]}, {"id": "1509.02130", "submitter": "Arnau Ramisa", "authors": "Ariadna Quattoni, Arnau Ramisa, Pranava Swaroop Madhyastha, Edgar\n  Simo-Serra and Francesc Moreno-Noguer", "title": "Structured Prediction with Output Embeddings for Semantic Image\n  Annotation", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of annotating images with semantic tuples. Solving this\nproblem requires an algorithm which is able to deal with hundreds of classes\nfor each argument of the tuple. In such contexts, data sparsity becomes a key\nchallenge, as there will be a large number of classes for which only a few\nexamples are available. We propose handling this by incorporating feature\nrepresentations of both the inputs (images) and outputs (argument classes) into\na factorized log-linear model, and exploiting the flexibility of scoring\nfunctions based on bilinear forms. Experiments show that integrating feature\nrepresentations of the outputs in the structured prediction model leads to\nbetter overall predictions. We also conclude that the best output\nrepresentation is specific for each type of argument.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 17:23:05 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Quattoni", "Ariadna", ""], ["Ramisa", "Arnau", ""], ["Madhyastha", "Pranava Swaroop", ""], ["Simo-Serra", "Edgar", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "1509.02223", "submitter": "Tuomo Valkonen", "authors": "Artur Gorokh, Yury Korolev, Tuomo Valkonen", "title": "Diffusion tensor imaging with deterministic error bounds", "comments": null, "journal-ref": null, "doi": "10.1007/s10851-016-0639-7", "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Errors in the data and the forward operator of an inverse problem can be\nhandily modelled using partial order in Banach lattices. We present some\nexisting results of the theory of regularisation in this novel framework, where\nerrors are represented as bounds by means of the appropriate partial order.\n  We apply the theory to Diffusion Tensor Imaging, where correct noise\nmodelling is challenging: it involves the Rician distribution and the nonlinear\nStejskal-Tanner equation. Linearisation of the latter in the statistical\nframework would complicate the noise model even further. We avoid this using\nthe error bounds approach, which preserves simple error structure under\nmonotone transformations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 23:15:51 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 10:13:27 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Gorokh", "Artur", ""], ["Korolev", "Yury", ""], ["Valkonen", "Tuomo", ""]]}, {"id": "1509.02317", "submitter": "Lluis Gomez", "authors": "Lluis Gomez and Dimosthenis Karatzas", "title": "Object Proposals for Text Extraction in the Wild", "comments": "13th International Conference on Document Analysis and Recognition\n  (ICDAR 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Proposals is a recent computer vision technique receiving increasing\ninterest from the research community. Its main objective is to generate a\nrelatively small set of bounding box proposals that are most likely to contain\nobjects of interest. The use of Object Proposals techniques in the scene text\nunderstanding field is innovative. Motivated by the success of powerful while\nexpensive techniques to recognize words in a holistic way, Object Proposals\ntechniques emerge as an alternative to the traditional text detectors.\n  In this paper we study to what extent the existing generic Object Proposals\nmethods may be useful for scene text understanding. Also, we propose a new\nObject Proposals algorithm that is specifically designed for text and compare\nit with other generic methods in the state of the art. Experiments show that\nour proposal is superior in its ability of producing good quality word\nproposals in an efficient way. The source code of our method is made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 10:44:09 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Gomez", "Lluis", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1509.02320", "submitter": "Xianbiao Qi", "authors": "Xianbiao Qi, Guoying Zhao, Jie Chen, Matti Pietik\\\"ainen", "title": "HEp-2 Cell Classification: The Role of Gaussian Scale Space Theory as A\n  Pre-processing Approach", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\textit{Indirect Immunofluorescence Imaging of Human Epithelial Type 2}\n(HEp-2) cells is an effective way to identify the presence of Anti-Nuclear\nAntibody (ANA). Most existing works on HEp-2 cell classification mainly focus\non feature extraction, feature encoding and classifier design. Very few efforts\nhave been devoted to study the importance of the pre-processing techniques. In\nthis paper, we analyze the importance of the pre-processing, and investigate\nthe role of Gaussian Scale Space (GSS) theory as a pre-processing approach for\nthe HEp-2 cell classification task. We validate the GSS pre-processing under\nthe Local Binary Pattern (LBP) and the Bag-of-Words (BoW) frameworks. Under the\nBoW framework, the introduced pre-processing approach, using only one Local\nOrientation Adaptive Descriptor (LOAD), achieved superior performance on the\nExecutable Thematic on Pattern Recognition Techniques for Indirect\nImmunofluorescence (ET-PRT-IIF) image analysis. Our system, using only one\nfeature, outperformed the winner of the ICPR 2014 contest that combined four\ntypes of features. Meanwhile, the proposed pre-processing method is not\nrestricted to this work; it can be generalized to many existing works.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 10:59:21 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Qi", "Xianbiao", ""], ["Zhao", "Guoying", ""], ["Chen", "Jie", ""], ["Pietik\u00e4inen", "Matti", ""]]}, {"id": "1509.02441", "submitter": "Subarna Tripathi", "authors": "Subarna Tripathi, Serge Belongie, Youngbae Hwang, Truong Nguyen", "title": "Semantic Video Segmentation : Exploring Inference Efficiency", "comments": "To appear in proc of ISOCC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the efficiency of the CRF inference beyond image level semantic\nsegmentation and perform joint inference in video frames. The key idea is to\ncombine best of two worlds: semantic co-labeling and more expressive models.\nOur formulation enables us to perform inference over ten thousand images within\nseconds and makes the system amenable to perform video semantic segmentation\nmost effectively. On CamVid dataset, with TextonBoost unaries, our proposed\nmethod achieves up to 8% improvement in accuracy over individual semantic image\nsegmentation without additional time overhead. The source code is available at\nhttps://github.com/subtri/video_inference\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 22:03:40 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Tripathi", "Subarna", ""], ["Belongie", "Serge", ""], ["Hwang", "Youngbae", ""], ["Nguyen", "Truong", ""]]}, {"id": "1509.02468", "submitter": "Alexander Malyshev", "authors": "Andrew Knyazev and Alexander Malyshev", "title": "Accelerated graph-based spectral polynomial filters", "comments": "6 pages, 6 figures. Accepted to the 2015 IEEE International Workshop\n  on Machine Learning for Signal Processing", "journal-ref": "Machine Learning for Signal Processing (MLSP), 2015 IEEE 25th\n  International Workshop on , pp.1-6, 17-20 Sept. 2015", "doi": "10.1109/MLSP.2015.7324315", "report-no": "MERL-TR2015-106", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based spectral denoising is a low-pass filtering using the\neigendecomposition of the graph Laplacian matrix of a noisy signal. Polynomial\nfiltering avoids costly computation of the eigendecomposition by projections\nonto suitable Krylov subspaces. Polynomial filters can be based, e.g., on the\nbilateral and guided filters. We propose constructing accelerated polynomial\nfilters by running flexible Krylov subspace based linear and eigenvalue solvers\nsuch as the Block Locally Optimal Preconditioned Conjugate Gradient (LOBPCG)\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 17:52:03 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Knyazev", "Andrew", ""], ["Malyshev", "Alexander", ""]]}, {"id": "1509.02470", "submitter": "Jianguo Li", "authors": "Jianwei Luo and Jianguo Li and Jun Wang and Zhiguo Jiang and Yurong\n  Chen", "title": "Deep Attributes from Context-Aware Regional Neural Codes", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many researches employ middle-layer output of convolutional neural\nnetwork models (CNN) as features for different visual recognition tasks.\nAlthough promising results have been achieved in some empirical studies, such\ntype of representations still suffer from the well-known issue of semantic gap.\nThis paper proposes so-called deep attribute framework to alleviate this issue\nfrom three aspects. First, we introduce object region proposals as intermedia\nto represent target images, and extract features from region proposals. Second,\nwe study aggregating features from different CNN layers for all region\nproposals. The aggregation yields a holistic yet compact representation of\ninput images. Results show that cross-region max-pooling of soft-max layer\noutput outperform all other layers. As soft-max layer directly corresponds to\nsemantic concepts, this representation is named \"deep attributes\". Third, we\nobserve that only a small portion of generated regions by object proposals\nalgorithm are correlated to classification target. Therefore, we introduce\ncontext-aware region refining algorithm to pick out contextual regions and\nbuild context-aware classifiers.\n  We apply the proposed deep attributes framework for various vision tasks.\nExtensive experiments are conducted on standard benchmarks for three visual\nrecognition tasks, i.e., image classification, fine-grained recognition and\nvisual instance retrieval. Results show that deep attribute approaches achieve\nstate-of-the-art results, and outperforms existing peer methods with a\nsignificant margin, even though some benchmarks have little overlap of concepts\nwith the pre-trained CNN models.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 17:53:54 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Luo", "Jianwei", ""], ["Li", "Jianguo", ""], ["Wang", "Jun", ""], ["Jiang", "Zhiguo", ""], ["Chen", "Yurong", ""]]}, {"id": "1509.02491", "submitter": "Andrew Knyazev", "authors": "Andrew Knyazev", "title": "Edge-enhancing Filters with Negative Weights", "comments": "5 pages; 6 figures. Accepted to IEEE GlobalSIP 2015 conference", "journal-ref": "2015 IEEE Global Conference on Signal and Information Processing\n  (GlobalSIP), Orlando, FL, 14-16 Dec.2015, pp. 260 - 264", "doi": "10.1109/GlobalSIP.2015.7418197", "report-no": "MERL-TR2015-142", "categories": "cs.CV cs.IT math.CO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [DOI:10.1109/ICMEW.2014.6890711], a graph-based denoising is performed by\nprojecting the noisy image to a lower dimensional Krylov subspace of the graph\nLaplacian, constructed using nonnegative weights determined by distances\nbetween image data corresponding to image pixels. We~extend the construction of\nthe graph Laplacian to the case, where some graph weights can be negative.\nRemoving the positivity constraint provides a more accurate inference of a\ngraph model behind the data, and thus can improve quality of filters for\ngraph-based signal processing, e.g., denoising, compared to the standard\nconstruction, without affecting the costs.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 18:36:53 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Knyazev", "Andrew", ""]]}, {"id": "1509.02587", "submitter": "Bardia Yousefi", "authors": "Bardia Yousefi, C.K. Loo", "title": "A Dual Fast and Slow Feature Interaction in Biologically Inspired Visual\n  Recognition of Human Action", "comments": "This paper has been withdrawn by the author due to a mistake in file", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational neuroscience studies that have examined human visual system\nthrough functional magnetic resonance imaging (fMRI) have identified a model\nwhere the mammalian brain pursues two distinct pathways (for recognition of\nbiological movement tasks). In the brain, dorsal stream analyzes the\ninformation of motion (optical flow), which is the fast features, and ventral\nstream (form pathway) analyzes form information (through active basis model\nbased incremental slow feature analysis ) as slow features. The proposed\napproach suggests the motion perception of the human visual system composes of\nfast and slow feature interactions that identifies biological movements. Form\nfeatures in the visual system biologically follows the application of active\nbasis model with incremental slow feature analysis for the extraction of the\nslowest form features of human objects movements in the ventral stream.\nApplying incremental slow feature analysis provides an opportunity to use the\naction prototypes. To extract the slowest features episodic observation is\nrequired but the fast features updates the processing of motion information in\nevery frames. Experimental results have shown promising accuracy for the\nproposed model and good performance with two datasets (KTH and Weizmann).\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 00:31:53 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2015 23:44:42 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Yousefi", "Bardia", ""], ["Loo", "C. K.", ""]]}, {"id": "1509.02634", "submitter": "Ziwei Liu", "authors": "Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, Xiaoou Tang", "title": "Semantic Image Segmentation via Deep Parsing Network", "comments": "To appear in International Conference on Computer Vision (ICCV) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses semantic image segmentation by incorporating rich\ninformation into Markov Random Field (MRF), including high-order relations and\nmixture of label contexts. Unlike previous works that optimized MRFs using\niterative algorithm, we solve MRF by proposing a Convolutional Neural Network\n(CNN), namely Deep Parsing Network (DPN), which enables deterministic\nend-to-end computation in a single forward pass. Specifically, DPN extends a\ncontemporary CNN architecture to model unary terms and additional layers are\ncarefully devised to approximate the mean field algorithm (MF) for pairwise\nterms. It has several appealing properties. First, different from the recent\nworks that combined CNN and MRF, where many iterations of MF were required for\neach training image during back-propagation, DPN is able to achieve high\nperformance by approximating one iteration of MF. Second, DPN represents\nvarious types of pairwise terms, making many existing works as its special\ncases. Third, DPN makes MF easier to be parallelized and speeded up in\nGraphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC\n2012 dataset, where a single DPN model yields a new state-of-the-art\nsegmentation accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 04:39:34 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2015 14:15:17 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Liu", "Ziwei", ""], ["Li", "Xiaoxiao", ""], ["Luo", "Ping", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1509.02636", "submitter": "Yunchao Wei", "authors": "Xiaodan Liang and Yunchao Wei and Xiaohui Shen and Jianchao Yang and\n  Liang Lin and Shuicheng Yan", "title": "Proposal-free Network for Instance-level Object Segmentation", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance-level object segmentation is an important yet under-explored task.\nThe few existing studies are almost all based on region proposal methods to\nextract candidate segments and then utilize object classification to produce\nfinal results. Nonetheless, generating accurate region proposals itself is\nquite challenging. In this work, we propose a Proposal-Free Network (PFN ) to\naddress the instance-level object segmentation problem, which outputs the\ninstance numbers of different categories and the pixel-level information on 1)\nthe coordinates of the instance bounding box each pixel belongs to, and 2) the\nconfidences of different categories for each pixel, based on pixel-to-pixel\ndeep convolutional neural network. All the outputs together, by using any\noff-the-shelf clustering method for simple post-processing, can naturally\ngenerate the ultimate instance-level object segmentation results. The whole PFN\ncan be easily trained in an end-to-end way without the requirement of a\nproposal generation stage. Extensive evaluations on the challenging PASCAL VOC\n2012 semantic segmentation benchmark demonstrate that the proposed PFN solution\nwell beats the state-of-the-arts for instance-level object segmentation. In\nparticular, the $AP^r$ over 20 classes at 0.5 IoU reaches 58.7% by PFN,\nsignificantly higher than 43.8% and 46.3% by the state-of-the-art algorithms,\nSDS [9] and [16], respectively.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 05:05:53 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 01:12:03 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Liang", "Xiaodan", ""], ["Wei", "Yunchao", ""], ["Shen", "Xiaohui", ""], ["Yang", "Jianchao", ""], ["Lin", "Liang", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1509.02649", "submitter": "Pan Ji", "authors": "Pan Ji, Mathieu Salzmann, Hongdong Li", "title": "Shape Interaction Matrix Revisited and Robustified: Efficient Subspace\n  Clustering with Corrupted and Incomplete Data", "comments": "This is an extended version of our iccv15 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shape Interaction Matrix (SIM) is one of the earliest approaches to\nperforming subspace clustering (i.e., separating points drawn from a union of\nsubspaces). In this paper, we revisit the SIM and reveal its connections to\nseveral recent subspace clustering methods. Our analysis lets us derive a\nsimple, yet effective algorithm to robustify the SIM and make it applicable to\nrealistic scenarios where the data is corrupted by noise. We justify our method\nby intuitive examples and the matrix perturbation theory. We then show how this\napproach can be extended to handle missing data, thus yielding an efficient and\ngeneral subspace clustering algorithm. We demonstrate the benefits of our\napproach over state-of-the-art subspace clustering methods on several\nchallenging motion segmentation and face clustering problems, where the data\nincludes corrupted and missing measurements.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 06:35:12 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 01:34:12 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Ji", "Pan", ""], ["Salzmann", "Mathieu", ""], ["Li", "Hongdong", ""]]}, {"id": "1509.02805", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Clustering by Hierarchical Nearest Neighbor Descent (H-NND)", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Previously in 2014, we proposed the Nearest Descent (ND) method, capable of\ngenerating an efficient Graph, called the in-tree (IT). Due to some beautiful\nand effective features, this IT structure proves well suited for data\nclustering. Although there exist some redundant edges in IT, they usually have\nsalient features and thus it is not hard to remove them.\n  Subsequently, in order to prevent the seemingly redundant edges from\noccurring, we proposed the Nearest Neighbor Descent (NND) by adding the\n\"Neighborhood\" constraint on ND. Consequently, clusters automatically emerged,\nwithout the additional requirement of removing the redundant edges. However,\nNND proved still not perfect, since it brought in a new yet worse problem, the\n\"over-partitioning\" problem.\n  Now, in this paper, we propose a method, called the Hierarchical Nearest\nNeighbor Descent (H-NND), which overcomes the over-partitioning problem of NND\nvia using the hierarchical strategy. Specifically, H-NND uses ND to effectively\nmerge the over-segmented sub-graphs or clusters that NND produces. Like ND,\nH-NND also generates the IT structure, in which the redundant edges once again\nappear. This seemingly comes back to the situation that ND faces. However,\ncompared with ND, the redundant edges in the IT structure generated by H-NND\ngenerally become more salient, thus being much easier and more reliable to be\nidentified even by the simplest edge-removing method which takes the edge\nlength as the only measure. In other words, the IT structure constructed by\nH-NND becomes more fitted for data clustering. We prove this on several\nclustering datasets of varying shapes, dimensions and attributes. Besides,\ncompared with ND, H-NND generally takes less computation time to construct the\nIT data structure for the input data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 15:15:44 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 15:43:25 GMT"}, {"version": "v3", "created": "Fri, 4 Mar 2016 15:50:58 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1509.02970", "submitter": "Piotr Koniusz", "authors": "Piotr Koniusz and Anoop Cherian", "title": "Dictionary Learning and Sparse Coding for Third-order Super-symmetric\n  Tensors", "comments": "13 pages, NIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-symmetric tensors - a higher-order extension of scatter matrices - are\nbecoming increasingly popular in machine learning and computer vision for\nmodelling data statistics, co-occurrences, or even as visual descriptors.\nHowever, the size of these tensors are exponential in the data dimensionality,\nwhich is a significant concern. In this paper, we study third-order\nsuper-symmetric tensor descriptors in the context of dictionary learning and\nsparse coding. Our goal is to approximate these tensors as sparse conic\ncombinations of atoms from a learned dictionary, where each atom is a symmetric\npositive semi-definite matrix. Apart from the significant benefits to tensor\ncompression that this framework provides, our experiments demonstrate that the\nsparse coefficients produced by the scheme lead to better aggregation of\nhigh-dimensional data, and showcases superior performance on two common\ncomputer vision tasks compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 22:30:01 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Koniusz", "Piotr", ""], ["Cherian", "Anoop", ""]]}, {"id": "1509.03001", "submitter": "Byeongkeun Kang", "authors": "Byeongkeun Kang, Subarna Tripathi, Truong Q. Nguyen", "title": "Real-time Sign Language Fingerspelling Recognition using Convolutional\n  Neural Networks from Depth map", "comments": "2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language recognition is important for natural and convenient\ncommunication between deaf community and hearing majority. We take the highly\nefficient initial step of automatic fingerspelling recognition system using\nconvolutional neural networks (CNNs) from depth maps. In this work, we consider\nrelatively larger number of classes compared with the previous literature. We\ntrain CNNs for the classification of 31 alphabets and numbers using a subset of\ncollected depth data from multiple subjects. While using different learning\nconfigurations, such as hyper-parameter selection with and without validation,\nwe achieve 99.99% accuracy for observed signers and 83.58% to 85.49% accuracy\nfor new signers. The result shows that accuracy improves as we include more\ndata from different subjects during training. The processing time is 3 ms for\nthe prediction of a single image. To the best of our knowledge, the system\nachieves the highest accuracy and speed. The trained model and dataset is\navailable on our repository.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 03:58:56 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2015 17:07:56 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2015 19:15:41 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Kang", "Byeongkeun", ""], ["Tripathi", "Subarna", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1509.03150", "submitter": "Yunchao Wei", "authors": "Yunchao Wei and Xiaodan Liang and Yunpeng Chen and Xiaohui Shen and\n  Ming-Ming Cheng and Jiashi Feng and Yao Zhao and Shuicheng Yan", "title": "STC: A Simple to Complex Framework for Weakly-supervised Semantic\n  Segmentation", "comments": "To Appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2016", "doi": "10.1109/TPAMI.2016.2636150", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, significant improvement has been made on semantic object\nsegmentation due to the development of deep convolutional neural networks\n(DCNNs). Training such a DCNN usually relies on a large number of images with\npixel-level segmentation masks, and annotating these images is very costly in\nterms of both finance and human effort. In this paper, we propose a simple to\ncomplex (STC) framework in which only image-level annotations are utilized to\nlearn DCNNs for semantic segmentation. Specifically, we first train an initial\nsegmentation network called Initial-DCNN with the saliency maps of simple\nimages (i.e., those with a single category of major object(s) and clean\nbackground). These saliency maps can be automatically obtained by existing\nbottom-up salient object detection techniques, where no supervision information\nis needed. Then, a better network called Enhanced-DCNN is learned with\nsupervision from the predicted segmentation masks of simple images based on the\nInitial-DCNN as well as the image-level annotations. Finally, more pixel-level\nsegmentation masks of complex images (two or more categories of objects with\ncluttered background), which are inferred by using Enhanced-DCNN and\nimage-level annotations, are utilized as the supervision information to learn\nthe Powerful-DCNN for semantic segmentation. Our method utilizes $40$K simple\nimages from Flickr.com and 10K complex images from PASCAL VOC for step-wisely\nboosting the segmentation network. Extensive experimental results on PASCAL VOC\n2012 segmentation benchmark well demonstrate the superiority of the proposed\nSTC framework compared with other state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 13:45:01 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 10:59:12 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Wei", "Yunchao", ""], ["Liang", "Xiaodan", ""], ["Chen", "Yunpeng", ""], ["Shen", "Xiaohui", ""], ["Cheng", "Ming-Ming", ""], ["Feng", "Jiashi", ""], ["Zhao", "Yao", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1509.03248", "submitter": "George Trigeorgis", "authors": "George Trigeorgis, Konstantinos Bousmalis, Stefanos Zafeiriou, Bjoern\n  W.Schuller", "title": "A deep matrix factorization method for learning attribute\n  representations", "comments": "Submitted to TPAMI (16-Mar-2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-Non-negative Matrix Factorization is a technique that learns a\nlow-dimensional representation of a dataset that lends itself to a clustering\ninterpretation. It is possible that the mapping between this new representation\nand our original data matrix contains rather complex hierarchical information\nwith implicit lower-level hidden attributes, that classical one level\nclustering methodologies can not interpret. In this work we propose a novel\nmodel, Deep Semi-NMF, that is able to learn such hidden representations that\nallow themselves to an interpretation of clustering according to different,\nunknown attributes of a given dataset. We also present a semi-supervised\nversion of the algorithm, named Deep WSF, that allows the use of (partial)\nprior information for each of the known attributes of a dataset, that allows\nthe model to be used on datasets with mixed attribute knowledge. Finally, we\nshow that our models are able to learn low-dimensional representations that are\nbetter suited for clustering, but also classification, outperforming\nSemi-Non-negative Matrix Factorization, but also other state-of-the-art\nmethodologies variants.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 17:57:03 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Trigeorgis", "George", ""], ["Bousmalis", "Konstantinos", ""], ["Zafeiriou", "Stefanos", ""], ["Schuller", "Bjoern W.", ""]]}, {"id": "1509.03257", "submitter": "Andre Wagner", "authors": "Michael Joswig, Joe Kileel, Bernd Sturmfels, Andr\\'e Wagner", "title": "Rigid Multiview Varieties", "comments": "12 pages, 1 figure", "journal-ref": "Int. J. Algebra Comput. 26 (2016) 775-788", "doi": "10.1142/S021819671650034X", "report-no": null, "categories": "math.AG cs.CV math.AC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multiview variety from computer vision is generalized to images by $n$\ncameras of points linked by a distance constraint. The resulting\nfive-dimensional variety lives in a product of $2n$ projective planes. We\ndetermine defining polynomial equations, and we explore generalizations of this\nvariety to scenarios of interest in applications.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 18:33:10 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 09:37:09 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Joswig", "Michael", ""], ["Kileel", "Joe", ""], ["Sturmfels", "Bernd", ""], ["Wagner", "Andr\u00e9", ""]]}, {"id": "1509.03371", "submitter": "Fabian Tschopp", "authors": "Fabian Tschopp", "title": "Efficient Convolutional Neural Networks for Pixelwise Classification on\n  Heterogeneous Hardware Systems", "comments": "92 pages, project source code available at\n  https://github.com/naibaf7/, technical report written at ETH Z\\\"urich, in\n  collaboration with AMD, UZH INI and HHMI Janelia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents and analyzes three convolutional neural network (CNN)\nmodels for efficient pixelwise classification of images. When using\nconvolutional neural networks to classify single pixels in patches of a whole\nimage, a lot of redundant computations are carried out when using sliding\nwindow networks. This set of new architectures solve this issue by either\nremoving redundant computations or using fully convolutional architectures that\ninherently predict many pixels at once.\n  The implementations of the three models are accessible through a new utility\non top of the Caffe library. The utility provides support for a wide range of\nimage input and output formats, pre-processing parameters and methods to\nequalize the label histogram during training. The Caffe library has been\nextended by new layers and a new backend for availability on a wider range of\nhardware such as CPUs and GPUs through OpenCL.\n  On AMD GPUs, speedups of $54\\times$ (SK-Net), $437\\times$ (U-Net) and\n$320\\times$ (USK-Net) have been observed, taking the SK equivalent SW (sliding\nwindow) network as the baseline. The label throughput is up to one megapixel\nper second.\n  The analyzed neural networks have distinctive characteristics that apply\nduring training or processing, and not every data set is suitable to every\narchitecture. The quality of the predictions is assessed on two neural tissue\ndata sets, of which one is the ISBI 2012 challenge data set. Two different loss\nfunctions, Malis loss and Softmax loss, were used during training.\n  The whole pipeline, consisting of models, interface and modified Caffe\nlibrary, is available as Open Source software under the working title Project\nGreentea.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 01:20:46 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Tschopp", "Fabian", ""]]}, {"id": "1509.03413", "submitter": "Saikat Basu", "authors": "Saikat Basu, Manohar Karki, Sangram Ganguly, Robert DiBiano, Supratik\n  Mukhopadhyay and Ramakrishna Nemani", "title": "Learning Sparse Feature Representations using Probabilistic Quadtrees\n  and Deep Belief Nets", "comments": "Published in the European Symposium on Artificial Neural Networks,\n  ESANN 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning sparse feature representations is a useful instrument for solving an\nunsupervised learning problem. In this paper, we present three labeled\nhandwritten digit datasets, collectively called n-MNIST. Then, we propose a\nnovel framework for the classification of handwritten digits that learns sparse\nrepresentations using probabilistic quadtrees and Deep Belief Nets. On the\nMNIST and n-MNIST datasets, our framework shows promising results and\nsignificantly outperforms traditional Deep Belief Networks.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 08:13:35 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Basu", "Saikat", ""], ["Karki", "Manohar", ""], ["Ganguly", "Sangram", ""], ["DiBiano", "Robert", ""], ["Mukhopadhyay", "Supratik", ""], ["Nemani", "Ramakrishna", ""]]}, {"id": "1509.03453", "submitter": "Giovanni Poggi", "authors": "Luisa Verdoliva, Davide Cozzolino, Giovanni Poggi", "title": "A reliable order-statistics-based approximate nearest neighbor search\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for fast approximate nearest neighbor search based\non the properties of ordered vectors. Data vectors are classified based on the\nindex and sign of their largest components, thereby partitioning the space in a\nnumber of cones centered in the origin. The query is itself classified, and the\nsearch starts from the selected cone and proceeds to neighboring ones. Overall,\nthe proposed algorithm corresponds to locality sensitive hashing in the space\nof directions, with hashing based on the order of components. Thanks to the\nstatistical features emerging through ordering, it deals very well with the\nchallenging case of unstructured data, and is a valuable building block for\nmore complex techniques dealing with structured data. Experiments on both\nsimulated and real-world data prove the proposed algorithm to provide a\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 10:33:34 GMT"}, {"version": "v2", "created": "Sat, 29 Oct 2016 10:16:09 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Verdoliva", "Luisa", ""], ["Cozzolino", "Davide", ""], ["Poggi", "Giovanni", ""]]}, {"id": "1509.03456", "submitter": "Abdeslam El Harraj", "authors": "Abdeslam El Harraj, Naoufal Raissouni", "title": "OCR accuracy improvement on document images through a novel\n  pre-processing approach", "comments": null, "journal-ref": "Signal & Image Processing : An International Journal (SIPIJ)\n  Vol.6, No.4, August 2015", "doi": "10.5121/sipij.2015.6401", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital camera and mobile document image acquisition are new trends arising\nin the world of Optical Character Recognition and text detection. In some\ncases, such process integrates many distortions and produces poorly scanned\ntext or text-photo images and natural images, leading to an unreliable OCR\ndigitization. In this paper, we present a novel nonparametric and unsupervised\nmethod to compensate for undesirable document image distortions aiming to\noptimally improve OCR accuracy. Our approach relies on a very efficient stack\nof document image enhancing techniques to recover deformation of the entire\ndocument image. First, we propose a local brightness and contrast adjustment\nmethod to effectively handle lighting variations and the irregular distribution\nof image illumination. Second, we use an optimized greyscale conversion\nalgorithm to transform our document image to greyscale level. Third, we sharpen\nthe useful information in the resulting greyscale image using Un-sharp Masking\nmethod. Finally, an optimal global binarization approach is used to prepare the\nfinal document image to OCR recognition. The proposed approach can\nsignificantly improve text detection rate and optical character recognition\naccuracy. To demonstrate the efficiency of our approach, an exhaustive\nexperimentation on a standard dataset is presented.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 10:52:52 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Harraj", "Abdeslam El", ""], ["Raissouni", "Naoufal", ""]]}, {"id": "1509.03502", "submitter": "Seong Joon Oh", "authors": "Seong Joon Oh, Rodrigo Benenson, Mario Fritz, Bernt Schiele", "title": "Person Recognition in Personal Photo Collections", "comments": "Accepted to ICCV 2015, revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognising persons in everyday photos presents major challenges (occluded\nfaces, different clothing, locations, etc.) for machine vision. We propose a\nconvnet based person recognition system on which we provide an in-depth\nanalysis of informativeness of different body cues, impact of training data,\nand the common failure modes of the system. In addition, we discuss the\nlimitations of existing benchmarks and propose more challenging ones. Our\nmethod is simple and is built on open source and open data, yet it improves the\nstate of the art results on a large dataset of social media photos (PIPA).\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 13:34:45 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 19:58:34 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Oh", "Seong Joon", ""], ["Benenson", "Rodrigo", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1509.03503", "submitter": "Marco Winkler", "authors": "Marco Winkler", "title": "NoSPaM Manual - A Tool for Node-Specific Triad Pattern Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.DS physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of triadic subgraph motifs is a common methodology in\ncomplex-networks research. The procedure usually applied in order to detect\nmotifs evaluates whether a certain subgraph pattern is overrepresented in a\nnetwork as a whole. However, motifs do not necessarily appear frequently in\nevery region of a graph. For this reason, we recently introduced the framework\nof Node-Specific Pattern Mining (NoSPaM). This work is a manual for an\nimplementation of NoSPaM which can be downloaded from www.mwinkler.eu.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 14:57:08 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Winkler", "Marco", ""]]}, {"id": "1509.03542", "submitter": "Shervin Minaee", "authors": "Shervin Minaee and Yao Wang", "title": "Fingerprint Recognition Using Translation Invariant Scattering Network", "comments": "IEEE Signal Processing in Medicine and Biology Symposium, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint recognition has drawn a lot of attention during last decades.\nDifferent features and algorithms have been used for fingerprint recognition in\nthe past. In this paper, a powerful image representation called scattering\ntransform/network, is used for recognition. Scattering network is a\nconvolutional network where its architecture and filters are predefined wavelet\ntransforms. The first layer of scattering representation is similar to sift\ndescriptors and the higher layers capture higher frequency content of the\nsignal. After extraction of scattering features, their dimensionality is\nreduced by applying principal component analysis (PCA). At the end, multi-class\nSVM is used to perform template matching for the recognition task. The proposed\nscheme is tested on a well-known fingerprint database and has shown promising\nresults with the best accuracy rate of 98\\%.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 15:04:35 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 14:51:06 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2015 01:48:13 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Minaee", "Shervin", ""], ["Wang", "Yao", ""]]}, {"id": "1509.03557", "submitter": "Martin Uecker", "authors": "Martin Uecker and Michael Lustig", "title": "Estimating Absolute-Phase Maps Using ESPIRiT and Virtual Conjugate Coils", "comments": "15 pages, 5 figures", "journal-ref": "Magnetic Resonance in Medicine 77 (2017) 1201-1207", "doi": "10.1002/mrm.26191", "report-no": null, "categories": "cs.CV cs.CE physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To develop an ESPIRiT-based method to estimate coil sensitivities\nwith image phase as a building block for efficient and robust image\nreconstruction with phase constraints. Theory and Methods: ESPIRiT is a new\nframework for calibration of the coil sensitivities and reconstruction in\nparallel Magnetic Resonance Imaging (MRI). Applying ESPIRiT to a combined set\nof physical and virtual conjugate coils (VCC-ESPIRiT) implicitly exploits\nconjugate symmetry in k-space similar to VCC-GRAPPA. Based on this method, a\nnew post-processing step is proposed for the explicit computation of coil\nsensitivities that include the absolute phase of the image. The accuracy of the\ncomputed maps is directly validated using a test based on projection onto fully\nsampled coil images and also indirectly in phase-constrained parallel-imaging\nreconstructions. Results: The proposed method can estimate accurate\nsensitivities which include low-resolution image phase. In case of\nhigh-frequency phase variations VCC-ESPIRiT yields an additional set of maps\nthat indicates the existence of a high-frequency phase component. Taking this\nadditional set of maps into account can improve the robustness of\nphase-constrained parallel imaging. Conclusion: The extended VCC-ESPIRiT is a\nuseful tool for phase-constrained imaging.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2015 22:35:14 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 09:02:37 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Uecker", "Martin", ""], ["Lustig", "Michael", ""]]}, {"id": "1509.03602", "submitter": "Saikat Basu", "authors": "Saikat Basu, Sangram Ganguly, Supratik Mukhopadhyay, Robert DiBiano,\n  Manohar Karki and Ramakrishna Nemani", "title": "DeepSat - A Learning framework for Satellite Imagery", "comments": "Paper was accepted at ACM SIGSPATIAL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite image classification is a challenging problem that lies at the\ncrossroads of remote sensing, computer vision, and machine learning. Due to the\nhigh variability inherent in satellite data, most of the current object\nclassification approaches are not suitable for handling satellite datasets. The\nprogress of satellite image analytics has also been inhibited by the lack of a\nsingle labeled high-resolution dataset with multiple class labels. The\ncontributions of this paper are twofold - (1) first, we present two new\nsatellite datasets called SAT-4 and SAT-6, and (2) then, we propose a\nclassification framework that extracts features from an input image, normalizes\nthem and feeds the normalized feature vectors to a Deep Belief Network for\nclassification. On the SAT-4 dataset, our best network produces a\nclassification accuracy of 97.95% and outperforms three state-of-the-art object\nrecognition algorithms, namely - Deep Belief Networks, Convolutional Neural\nNetworks and Stacked Denoising Autoencoders by ~11%. On SAT-6, it produces a\nclassification accuracy of 93.9% and outperforms the other algorithms by ~15%.\nComparative studies with a Random Forest classifier show the advantage of an\nunsupervised learning approach over traditional supervised learning techniques.\nA statistical analysis based on Distribution Separability Criterion and\nIntrinsic Dimensionality Estimation substantiates the effectiveness of our\napproach in learning better representations for satellite imagery.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 18:32:51 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Basu", "Saikat", ""], ["Ganguly", "Sangram", ""], ["Mukhopadhyay", "Supratik", ""], ["DiBiano", "Robert", ""], ["Karki", "Manohar", ""], ["Nemani", "Ramakrishna", ""]]}, {"id": "1509.03660", "submitter": "Jordi Pont-Tuset", "authors": "Jordi Pont-Tuset, Pablo Arbel\\'aez, Luc Van Gool", "title": "Oracle MCG: A first peek into COCO Detection Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently presented COCO detection challenge will most probably be the\nreference benchmark in object detection in the next years. COCO is two orders\nof magnitude larger than Pascal and has four times the number of categories; so\nin all likelihood researchers will be faced with a number of new challenges. At\nthis point, without any finished round of the competition, it is difficult for\nresearchers to put their techniques in context, or in other words, to know how\ngood their results are. In order to give a little context, this note evaluates\na hypothetical object detector consisting in an oracle picking the best object\nproposal from a state-of-the-art technique. This oracle achieves a AP=0.292 in\nsegmented objects and AP=0.317 in bounding boxes, showing that indeed the\ndatabase is challenging, given that this value is the best one can expect if\nworking on object proposals without refinement.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 08:13:34 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Pont-Tuset", "Jordi", ""], ["Arbel\u00e1ez", "Pablo", ""], ["Van Gool", "Luc", ""]]}, {"id": "1509.03789", "submitter": "Bardia Yousefi", "authors": "Bardia Yousefi, Chu Kiong Loo", "title": "Bio-Inspired Human Action Recognition using Hybrid Max-Product\n  Neuro-Fuzzy Classifier and Quantum-Behaved PSO", "comments": "author's version, SWJ 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies on computational neuroscience through functional magnetic resonance\nimaging (fMRI) and following biological inspired system stated that human\naction recognition in the brain of mammalian leads two distinct pathways in the\nmodel, which are specialized for analysis of motion (optic flow) and form\ninformation. Principally, we have defined a novel and robust form features\napplying active basis model as form extractor in form pathway in the biological\ninspired model. An unbalanced synergetic neural net-work classifies shapes and\nstructures of human objects along with tuning its attention parameter by\nquantum particle swarm optimization (QPSO) via initiation of Centroidal Voronoi\nTessellations. These tools utilized and justified as strong tools for following\nbiological system model in form pathway. But the final decision has done by\ncombination of ultimate outcomes of both pathways via fuzzy inference which\nincreases novality of proposed model. Combination of these two brain pathways\nis done by considering each feature sets in Gaussian membership functions with\nfuzzy product inference method. Two configurations have been proposed for form\npathway: applying multi-prototype human action templates using two time\nsynergetic neural network for obtaining uniform template regarding each\nactions, and second scenario that it uses abstracting human action in four\nkey-frames. Experimental results showed promising accuracy performance on\ndifferent datasets (KTH and Weizmann).\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2015 00:34:18 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2016 00:04:24 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Yousefi", "Bardia", ""], ["Loo", "Chu Kiong", ""]]}, {"id": "1509.03844", "submitter": "Yiannis Andreopoulos", "authors": "Alhabib Abbas, Nikos Deligiannis and Yiannis Andreopoulos", "title": "Vectors of Locally Aggregated Centers for Compact Video Representation", "comments": "Proc. IEEE International Conference on Multimedia and Expo, ICME\n  2015, Torino, Italy", "journal-ref": null, "doi": "10.1109/ICME.2015.7177501", "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel vector aggregation technique for compact video\nrepresentation, with application in accurate similarity detection within large\nvideo datasets. The current state-of-the-art in visual search is formed by the\nvector of locally aggregated descriptors (VLAD) of Jegou et. al. VLAD generates\ncompact video representations based on scale-invariant feature transform (SIFT)\nvectors (extracted per frame) and local feature centers computed over a\ntraining set. With the aim to increase robustness to visual distortions, we\npropose a new approach that operates at a coarser level in the feature\nrepresentation. We create vectors of locally aggregated centers (VLAC) by first\nclustering SIFT features to obtain local feature centers (LFCs) and then\nencoding the latter with respect to given centers of local feature centers\n(CLFCs), extracted from a training set. The sum-of-differences between the LFCs\nand the CLFCs are aggregated to generate an extremely-compact video description\nused for accurate video segment similarity detection. Experimentation using a\nvideo dataset, comprising more than 1000 minutes of content from the Open Video\nProject, shows that VLAC obtains substantial gains in terms of mean Average\nPrecision (mAP) against VLAD and the hyper-pooling method of Douze et. al.,\nunder the same compaction factor and the same set of distortions.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2015 13:06:36 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Abbas", "Alhabib", ""], ["Deligiannis", "Nikos", ""], ["Andreopoulos", "Yiannis", ""]]}, {"id": "1509.03877", "submitter": "Zhen Zuo PHD", "authors": "Zhen Zuo, Bing Shuai, Gang Wang, Xiao Liu, Xingxing Wang, and Bing\n  Wang", "title": "Learning Contextual Dependencies with Convolutional Hierarchical\n  Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2548241", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep convolutional neural networks (CNNs) have shown their great\nsuccess on image classification. CNNs mainly consist of convolutional and\npooling layers, both of which are performed on local image areas without\nconsidering the dependencies among different image regions. However, such\ndependencies are very important for generating explicit image representation.\nIn contrast, recurrent neural networks (RNNs) are well known for their ability\nof encoding contextual information among sequential data, and they only require\na limited number of network parameters. General RNNs can hardly be directly\napplied on non-sequential data. Thus, we proposed the hierarchical RNNs\n(HRNNs). In HRNNs, each RNN layer focuses on modeling spatial dependencies\namong image regions from the same scale but different locations. While the\ncross RNN scale connections target on modeling scale dependencies among regions\nfrom the same location but different scales. Specifically, we propose two\nrecurrent neural network models: 1) hierarchical simple recurrent network\n(HSRN), which is fast and has low computational cost; and 2) hierarchical\nlong-short term memory recurrent network (HLSTM), which performs better than\nHSRN with the price of more computational cost.\n  In this manuscript, we integrate CNNs with HRNNs, and develop end-to-end\nconvolutional hierarchical recurrent neural networks (C-HRNNs). C-HRNNs not\nonly make use of the representation power of CNNs, but also efficiently encodes\nspatial and scale dependencies among different image regions. On four of the\nmost challenging object/scene image classification benchmarks, our C-HRNNs\nachieve state-of-the-art results on Places 205, SUN 397, MIT indoor, and\ncompetitive results on ILSVRC 2012.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2015 18:33:15 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 09:31:36 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Zuo", "Zhen", ""], ["Shuai", "Bing", ""], ["Wang", "Gang", ""], ["Liu", "Xiao", ""], ["Wang", "Xingxing", ""], ["Wang", "Bing", ""]]}, {"id": "1509.03891", "submitter": "Soroush Mehri", "authors": "Soroush Mehri", "title": "On Binary Classification with Single-Layer Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are becoming standard tools for solving object\nrecognition and visual tasks. However, most of the design and implementation of\nthese complex models are based on trail-and-error. In this report, the main\nfocus is to consider some of the important factors in designing convolutional\nnetworks to perform better. Specifically, classification with wide single-layer\nnetworks with large kernels as a general framework is considered. Particularly,\nwe will show that pre-training using unsupervised schemes is vital, reasonable\nregularization is beneficial and applying of strong regularizers like dropout\ncould be devastating. Pool size is also could be as important as learning\nprocedure itself. In addition, it has been presented that using such a simple\nand relatively fast model for classifying cats and dogs, performance is close\nto state-of-the-art achievable by a combination of SVM models on color and\ntexture features.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2015 20:14:35 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Mehri", "Soroush", ""]]}, {"id": "1509.03936", "submitter": "Zhanpeng Zhang", "authors": "Zhanpeng Zhang, Ping Luo, Chen Change Loy, Xiaoou Tang", "title": "Learning Social Relation Traits from Face Images", "comments": "To appear in International Conference on Computer Vision (ICCV) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social relation defines the association, e.g, warm, friendliness, and\ndominance, between two or more people. Motivated by psychological studies, we\ninvestigate if such fine-grained and high-level relation traits can be\ncharacterised and quantified from face images in the wild. To address this\nchallenging problem we propose a deep model that learns a rich face\nrepresentation to capture gender, expression, head pose, and age-related\nattributes, and then performs pairwise-face reasoning for relation prediction.\nTo learn from heterogeneous attribute sources, we formulate a new network\narchitecture with a bridging layer to leverage the inherent correspondences\namong these datasets. It can also cope with missing target attribute labels.\nExtensive experiments show that our approach is effective for fine-grained\nsocial relation learning in images and videos.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 03:02:36 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Zhang", "Zhanpeng", ""], ["Luo", "Ping", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1509.03942", "submitter": "Davide Barbieri", "authors": "Davide Barbieri", "title": "Geometry and dimensionality reduction of feature spaces in primary\n  visual cortex", "comments": null, "journal-ref": null, "doi": "10.1117/12.2187026", "report-no": null, "categories": "q-bio.NC cs.CV math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some geometric properties of the wavelet analysis performed by visual neurons\nare discussed and compared with experimental data. In particular, several\nrelationships between the cortical morphologies and the parametric dependencies\nof extracted features are formalized and considered from a harmonic analysis\npoint of view.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 03:21:54 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Barbieri", "Davide", ""]]}, {"id": "1509.03956", "submitter": "Francesco Solera", "authors": "Francesco Solera, Simone Calderara and Rita Cucchiara", "title": "Learning to Divide and Conquer for Online Multi-Target Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Multiple Target Tracking (MTT) is often addressed within the\ntracking-by-detection paradigm. Detections are previously extracted\nindependently in each frame and then objects trajectories are built by\nmaximizing specifically designed coherence functions. Nevertheless, ambiguities\narise in presence of occlusions or detection errors. In this paper we claim\nthat the ambiguities in tracking could be solved by a selective use of the\nfeatures, by working with more reliable features if possible and exploiting a\ndeeper representation of the target only if necessary. To this end, we propose\nan online divide and conquer tracker for static camera scenes, which partitions\nthe assignment problem in local subproblems and solves them by selectively\nchoosing and combining the best features. The complete framework is cast as a\nstructural learning task that unifies these phases and learns tracker\nparameters from examples. Experiments on two different datasets highlights a\nsignificant improvement of tracking performances (MOTA +10%) over the state of\nthe art.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 05:25:52 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Solera", "Francesco", ""], ["Calderara", "Simone", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1509.03970", "submitter": "Fernando Soler-Toscano", "authors": "Nicolas Gauvrit, Fernando Soler-Toscano and Hector Zenil", "title": "Natural scene statistics mediate the perception of image complexity", "comments": null, "journal-ref": "Visual Cognition 22 (8), 2014, pages 1084-1091", "doi": "10.1080/13506285.2014.950365", "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Humans are sensitive to complexity and regularity in patterns. The subjective\nperception of pattern complexity is correlated to algorithmic\n(Kolmogorov-Chaitin) complexity as defined in computer science, but also to the\nfrequency of naturally occurring patterns. However, the possible mediational\nrole of natural frequencies in the perception of algorithmic complexity remains\nunclear. Here we reanalyze Hsu et al. (2010) through a mediational analysis,\nand complement their results in a new experiment. We conclude that human\nperception of complexity seems partly shaped by natural scenes statistics,\nthereby establishing a link between the perception of complexity and the effect\nof natural scene statistics.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 07:34:46 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Gauvrit", "Nicolas", ""], ["Soler-Toscano", "Fernando", ""], ["Zenil", "Hector", ""]]}, {"id": "1509.04115", "submitter": "Changsoo Je", "authors": "Changsoo Je, Sang Wook Lee, Rae-Hong Park", "title": "Color-Phase Analysis for Sinusoidal Structured Light in Rapid Range\n  Imaging", "comments": "6 pages, 12 figures. 6th Asian Conference on Computer Vision (ACCV\n  2004)", "journal-ref": "Proc. 6th Asian Conference on Computer Vision (ACCV 2004), vol. 1,\n  pp. 270-275, Jeju Island, Korea, January 27, 2004", "doi": null, "report-no": null, "categories": "cs.CV cs.GR physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active range sensing using structured-light is the most accurate and reliable\nmethod for obtaining 3D information. However, most of the work has been limited\nto range sensing of static objects, and range sensing of dynamic (moving or\ndeforming) objects has been investigated recently only by a few researchers.\nSinusoidal structured-light is one of the well-known optical methods for 3D\nmeasurement. In this paper, we present a novel method for rapid high-resolution\nrange imaging using color sinusoidal pattern. We consider the real-world\nproblem of nonlinearity and color-band crosstalk in the color light projector\nand color camera, and present methods for accurate recovery of color-phase. For\nhigh-resolution ranging, we use high-frequency patterns and describe new\nunwrapping algorithms for reliable range recovery. The experimental results\ndemonstrate the effectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 14:35:20 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Je", "Changsoo", ""], ["Lee", "Sang Wook", ""], ["Park", "Rae-Hong", ""]]}, {"id": "1509.04186", "submitter": "Gaurav Sharma", "authors": "Gaurav Sharma, Frederic Jurie and Cordelia Schmid", "title": "Expanded Parts Model for Semantic Description of Humans in Still Images", "comments": "Accepted for publication in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2537325", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an Expanded Parts Model (EPM) for recognizing human attributes\n(e.g. young, short hair, wearing suit) and actions (e.g. running, jumping) in\nstill images. An EPM is a collection of part templates which are learnt\ndiscriminatively to explain specific scale-space regions in the images (in\nhuman centric coordinates). This is in contrast to current models which consist\nof a relatively few (i.e. a mixture of) 'average' templates. EPM uses only a\nsubset of the parts to score an image and scores the image sparsely in space,\ni.e. it ignores redundant and random background in an image. To learn our\nmodel, we propose an algorithm which automatically mines parts and learns\ncorresponding discriminative templates together with their respective locations\nfrom a large number of candidate parts. We validate our method on three recent\nchallenging datasets of human attributes and actions. We obtain convincing\nqualitative and state-of-the-art quantitative results on the three datasets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 16:33:04 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2016 12:14:05 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Sharma", "Gaurav", ""], ["Jurie", "Frederic", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1509.04232", "submitter": "Victor Adrian Prisacariu", "authors": "Carl Yuheng Ren and Victor Adrian Prisacariu and Ian D Reid", "title": "gSLICr: SLIC superpixels at over 250Hz", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a parallel GPU implementation of the Simple Linear Iterative\nClustering (SLIC) superpixel segmentation. Using a single graphic card, our\nimplementation achieves speedups of up to $83\\times$ from the standard\nsequential implementation. Our implementation is fully compatible with the\nstandard sequential implementation and the software is now available online and\nis open source.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 18:30:05 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Ren", "Carl Yuheng", ""], ["Prisacariu", "Victor Adrian", ""], ["Reid", "Ian D", ""]]}, {"id": "1509.04237", "submitter": "Ke Chen", "authors": "Jianping Zhang and Ke Chen", "title": "A Total Fractional-Order Variation Model for Image Restoration with\n  Non-homogeneous Boundary Conditions and its Numerical Solution", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To overcome the weakness of a total variation based model for image\nrestoration, various high order (typically second order) regularization models\nhave been proposed and studied recently. In this paper we analyze and test a\nfractional-order derivative based total $\\alpha$-order variation model, which\ncan outperform the currently popular high order regularization models. There\nexist several previous works using total $\\alpha$-order variations for image\nrestoration; however first no analysis is done yet and second all tested\nformulations, differing from each other, utilize the zero Dirichlet boundary\nconditions which are not realistic (while non-zero boundary conditions violate\ndefinitions of fractional-order derivatives). This paper first reviews some\nresults of fractional-order derivatives and then analyzes the theoretical\nproperties of the proposed total $\\alpha$-order variational model rigorously.\nIt then develops four algorithms for solving the variational problem, one based\non the variational Split-Bregman idea and three based on direct solution of the\ndiscretise-optimization problem. Numerical experiments show that, in terms of\nrestoration quality and solution efficiency, the proposed model can produce\nhighly competitive results, for smooth images, to two established high order\nmodels: the mean curvature and the total generalized variation.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 12:42:53 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Zhang", "Jianping", ""], ["Chen", "Ke", ""]]}, {"id": "1509.04309", "submitter": "Xiaowei Zhou", "authors": "Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Kostas Daniilidis", "title": "Sparse Representation for 3D Shape Estimation: A Convex Relaxation\n  Approach", "comments": "Extended version of the paper: 3D Shape Estimation from 2D Landmarks:\n  A Convex Relaxation Approach. X. Zhou et al., CVPR, 2015. arXiv admin note:\n  substantial text overlap with arXiv:1411.2942", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2017", "doi": "10.1109/TPAMI.2016.2605097", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of estimating the 3D shape of an object defined by\na set of 3D landmarks, given their 2D correspondences in a single image. A\nsuccessful approach to alleviating the reconstruction ambiguity is the 3D\ndeformable shape model and a sparse representation is often used to capture\ncomplex shape variability. But the model inference is still a challenge due to\nthe nonconvexity in optimization resulted from joint estimation of shape and\nviewpoint. In contrast to prior work that relies on a alternating scheme with\nsolutions depending on initialization, we propose a convex approach to\naddressing this challenge and develop an efficient algorithm to solve the\nproposed convex program. Moreover, we propose a robust model to handle gross\nerrors in the 2D correspondences. We demonstrate the exact recovery property of\nthe proposed method, the advantage compared to the nonconvex baseline methods\nand the applicability to recover 3D human poses and car models from single\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 20:35:11 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 15:18:35 GMT"}, {"version": "v3", "created": "Tue, 10 Jan 2017 19:16:44 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Zhou", "Xiaowei", ""], ["Zhu", "Menglong", ""], ["Leonardos", "Spyridon", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1509.04399", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Ravi Kiran Sarvadevabhatla, Venkatesh Babu R", "title": "Analyzing structural characteristics of object category representations\n  from their semantic-part distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies from neuroscience show that part-mapping computations are employed by\nhuman visual system in the process of object recognition. In this work, we\npresent an approach for analyzing semantic-part characteristics of object\ncategory representations. For our experiments, we use category-epitome, a\nrecently proposed sketch-based spatial representation for objects. To enable\npart-importance analysis, we first obtain semantic-part annotations of\nhand-drawn sketches originally used to construct the corresponding epitomes. We\nthen examine the extent to which the semantic-parts are present in the epitomes\nof a category and visualize the relative importance of parts as a word cloud.\nFinally, we show how such word cloud visualizations provide an intuitive\nunderstanding of category-level structural trends that exist in the\ncategory-epitome object representations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 04:58:03 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Sarvadevabhatla", "Ravi Kiran", ""], ["R", "Venkatesh Babu", ""]]}, {"id": "1509.04420", "submitter": "Jonathan Heras", "authors": "J\\'onathan Heras and Gadea Mata and Germ\\'an Cuesto and Julio Rubio\n  and Miguel Morales", "title": "Neuron detection in stack images: a persistent homology interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automation and reliability are the two main requirements when computers are\napplied in Life Sciences. In this paper we report on an application to neuron\nrecognition, an important step in our long-term project of providing software\nsystems to the study of neural morphology and functionality from biomedical\nimages. Our algorithms have been implemented in an ImageJ plugin called\nNeuronPersistentJ, which has been validated experimentally. The soundness and\nreliability of our approach are based on the interpretation of our processing\nmethods with respect to persistent homology, a well-known tool in computational\nmathematics.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 07:06:32 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Heras", "J\u00f3nathan", ""], ["Mata", "Gadea", ""], ["Cuesto", "Germ\u00e1n", ""], ["Rubio", "Julio", ""], ["Morales", "Miguel", ""]]}, {"id": "1509.04581", "submitter": "Zhen Liu", "authors": "Zhen Liu", "title": "Kernelized Deep Convolutional Neural Network for Describing Complex\n  Images", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the impressive capability to capture visual content, deep convolutional\nneural networks (CNN) have demon- strated promising performance in various\nvision-based ap- plications, such as classification, recognition, and objec- t\ndetection. However, due to the intrinsic structure design of CNN, for images\nwith complex content, it achieves lim- ited capability on invariance to\ntranslation, rotation, and re-sizing changes, which is strongly emphasized in\nthe s- cenario of content-based image retrieval. In this paper, to address this\nproblem, we proposed a new kernelized deep convolutional neural network. We\nfirst discuss our motiva- tion by an experimental study to demonstrate the\nsensitivi- ty of the global CNN feature to the basic geometric trans-\nformations. Then, we propose to represent visual content with approximate\ninvariance to the above geometric trans- formations from a kernelized\nperspective. We extract CNN features on the detected object-like patches and\naggregate these patch-level CNN features to form a vectorial repre- sentation\nwith the Fisher vector model. The effectiveness of our proposed algorithm is\ndemonstrated on image search application with three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 14:35:11 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Liu", "Zhen", ""]]}, {"id": "1509.04612", "submitter": "Alan Mosca", "authors": "Alan Mosca and George D. Magoulas", "title": "Adapting Resilient Propagation for Deep Learning", "comments": "Published in the proceedings of the UK workshop on Computational\n  Intelligence 2015 (UKCI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Resilient Propagation (Rprop) algorithm has been very popular for\nbackpropagation training of multilayer feed-forward neural networks in various\napplications. The standard Rprop however encounters difficulties in the context\nof deep neural networks as typically happens with gradient-based learning\nalgorithms. In this paper, we propose a modification of the Rprop that combines\nstandard Rprop steps with a special drop out technique. We apply the method for\ntraining Deep Neural Networks as standalone components and in ensemble\nformulations. Results on the MNIST dataset show that the proposed modification\nalleviates standard Rprop's problems demonstrating improved learning speed and\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 15:55:29 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 11:45:48 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Mosca", "Alan", ""], ["Magoulas", "George D.", ""]]}, {"id": "1509.04619", "submitter": "Hamid Tizhoosh", "authors": "Zehra Camlica, H.R. Tizhoosh, Farzad Khalvati", "title": "Medical Image Classification via SVM using LBP Features from\n  Saliency-Based Folded Data", "comments": "To appear in proceedings of The 14th International Conference on\n  Machine Learning and Applications (IEEE ICMLA 2015), Miami, Florida, USA,\n  2015", "journal-ref": null, "doi": "10.1109/ICMLA.2015.131", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good results on image classification and retrieval using support vector\nmachines (SVM) with local binary patterns (LBPs) as features have been\nextensively reported in the literature where an entire image is retrieved or\nclassified. In contrast, in medical imaging, not all parts of the image may be\nequally significant or relevant to the image retrieval application at hand. For\ninstance, in lung x-ray image, the lung region may contain a tumour, hence\nbeing highly significant whereas the surrounding area does not contain\nsignificant information from medical diagnosis perspective. In this paper, we\npropose to detect salient regions of images during training and fold the data\nto reduce the effect of irrelevant regions. As a result, smaller image areas\nwill be used for LBP features calculation and consequently classification by\nSVM. We use IRMA 2009 dataset with 14,410 x-ray images to verify the\nperformance of the proposed approach. The results demonstrate the benefits of\nsaliency-based folding approach that delivers comparable classification\naccuracies with state-of-the-art but exhibits lower computational cost and\nstorage requirements, factors highly important for big data analytics.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 16:08:08 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Camlica", "Zehra", ""], ["Tizhoosh", "H. R.", ""], ["Khalvati", "Farzad", ""]]}, {"id": "1509.04648", "submitter": "M. Zeeshan Zia", "authors": "M. Zeeshan Zia, Luigi Nardi, Andrew Jack, Emanuele Vespa, Bruno Bodin,\n  Paul H.J. Kelly, Andrew J. Davison", "title": "Comparative Design Space Exploration of Dense and Semi-Dense SLAM", "comments": "IEEE International Conference on Robotics and Automation 2016", "journal-ref": null, "doi": "10.1109/ICRA.2016.7487261", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SLAM has matured significantly over the past few years, and is beginning to\nappear in serious commercial products. While new SLAM systems are being\nproposed at every conference, evaluation is often restricted to qualitative\nvisualizations or accuracy estimation against a ground truth. This is due to\nthe lack of benchmarking methodologies which can holistically and\nquantitatively evaluate these systems. Further investigation at the level of\nindividual kernels and parameter spaces of SLAM pipelines is non-existent,\nwhich is absolutely essential for systems research and integration. We extend\nthe recently introduced SLAMBench framework to allow comparing two\nstate-of-the-art SLAM pipelines, namely KinectFusion and LSD-SLAM, along the\nmetrics of accuracy, energy consumption, and processing frame rate on two\ndifferent hardware platforms, namely a desktop and an embedded device. We also\nanalyze the pipelines at the level of individual kernels and explore their\nalgorithmic and hardware design spaces for the first time, yielding valuable\ninsights.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 17:12:13 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 01:50:40 GMT"}, {"version": "v3", "created": "Thu, 3 Mar 2016 18:19:22 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zia", "M. Zeeshan", ""], ["Nardi", "Luigi", ""], ["Jack", "Andrew", ""], ["Vespa", "Emanuele", ""], ["Bodin", "Bruno", ""], ["Kelly", "Paul H. J.", ""], ["Davison", "Andrew J.", ""]]}, {"id": "1509.04664", "submitter": "Hamid Tizhoosh", "authors": "A. Othman, H.R. Tizhoosh, F. Khalvati", "title": "Self-Configuring and Evolving Fuzzy Image Thresholding", "comments": "To appear in proceedings of The 14th International Conference on\n  Machine Learning and Applications (IEEE ICMLA 2015), Miami, Florida, USA,\n  2015", "journal-ref": null, "doi": "10.1109/ICMLA.2015.130", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every segmentation algorithm has parameters that need to be adjusted in order\nto achieve good results. Evolving fuzzy systems for adjustment of segmentation\nparameters have been proposed recently (Evolving fuzzy image segmentation --\nEFIS [1]. However, similar to any other algorithm, EFIS too suffers from a few\nlimitations when used in practice. As a major drawback, EFIS depends on\ndetection of the object of interest for feature calculation, a task that is\nhighly application-dependent. In this paper, a new version of EFIS is proposed\nto overcome these limitations. The new EFIS, called self-configuring EFIS\n(SC-EFIS), uses available training data to auto-configure the parameters that\nare fixed in EFIS. As well, the proposed SC-EFIS relies on a feature selection\nprocess that does not require the detection of a region of interest (ROI).\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 18:19:02 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Othman", "A.", ""], ["Tizhoosh", "H. R.", ""], ["Khalvati", "F.", ""]]}, {"id": "1509.04706", "submitter": "Daniil Kazantsev", "authors": "Daniil Kazantsev, Evgueni Ovtchinnikov, William R. B. Lionheart,\n  Philip J. Withers, Peter D. Lee", "title": "Direct high-order edge-preserving regularization for tomographic image\n  reconstruction", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MS cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present a new two-level iterative algorithm for tomographic\nimage reconstruction. The algorithm uses a regularization technique, which we\ncall edge-preserving Laplacian, that preserves sharp edges between objects\nwhile damping spurious oscillations in the areas where the reconstructed image\nis smooth. Our numerical simulations demonstrate that the proposed method\noutperforms total variation (TV) regularization and it is competitive with the\ncombined TV-L2 penalty. Obtained reconstructed images show increased\nsignal-to-noise ratio and visually appealing structural features. Computer\nimplementation and parameter control of the proposed technique is\nstraightforward, which increases the feasibility of it across many tomographic\napplications. In this paper, we applied our method to the under-sampled\ncomputed tomography (CT) projection data and also considered a case of\nreconstruction in emission tomography The MATLAB code is provided to support\nobtained results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 18:23:56 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Kazantsev", "Daniil", ""], ["Ovtchinnikov", "Evgueni", ""], ["Lionheart", "William R. B.", ""], ["Withers", "Philip J.", ""], ["Lee", "Peter D.", ""]]}, {"id": "1509.04751", "submitter": "Cheng-I Wang", "authors": "Tammuz Dubnov and Cheng-i Wang", "title": "Free-body Gesture Tracking and Augmented Reality Improvisation for Floor\n  and Aerial Dance", "comments": "8 pages. Technical paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an updated interactive performance system for floor and\nAerial Dance that controls visual and sonic aspects of the presentation via a\ndepth sensing camera (MS Kinect). In order to detect, measure and track free\nmovement in space, 3 degree of freedom (3-DOF) tracking in space (on the ground\nand in the air) is performed using IR markers with a method for multi target\ntracking capabilities added and described in detail. An improved gesture\ntracking and recognition system, called Action Graph (AG), is described in the\npaper. Action Graph uses an efficient incremental construction from a single\nlong sequence of movement features and automatically captures repeated\nsub-segments in the movement from start to finish with no manual interaction\nneeded with other advanced capabilities discussed as well. By using the new\nmodel for the gesture we can unify an entire choreography piece by dynamically\ntracking and recognizing gestures and sub-portions of the piece. This gives the\nperformer the freedom to improvise based on a set of recorded gestures/portions\nof the choreography and have the system dynamically respond in relation to the\nperformer within a set of related rehearsed actions, an ability that has not\nbeen seen in any other system to date.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 21:54:21 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Dubnov", "Tammuz", ""], ["Wang", "Cheng-i", ""]]}, {"id": "1509.04767", "submitter": "Ziming Zhang", "authors": "Ziming Zhang and Venkatesh Saligrama", "title": "Zero-Shot Learning via Semantic Similarity Embedding", "comments": "accepted for ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a version of the zero-shot learning problem where\nseen class source and target domain data are provided. The goal during\ntest-time is to accurately predict the class label of an unseen target domain\ninstance based on revealed source domain side information (\\eg attributes) for\nunseen classes. Our method is based on viewing each source or target data as a\nmixture of seen class proportions and we postulate that the mixture patterns\nhave to be similar if the two instances belong to the same unseen class. This\nperspective leads us to learning source/target embedding functions that map an\narbitrary source/target domain data into a same semantic space where similarity\ncan be readily measured. We develop a max-margin framework to learn these\nsimilarity functions and jointly optimize parameters by means of cross\nvalidation. Our test results are compelling, leading to significant improvement\nin terms of accuracy on most benchmark datasets for zero-shot recognition.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 23:18:52 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 20:26:08 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Zhang", "Ziming", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1509.04783", "submitter": "Ziming Zhang", "authors": "Ziming Zhang, Yuting Chen, and Venkatesh Saligrama", "title": "Group Membership Prediction", "comments": "accepted for ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The group membership prediction (GMP) problem involves predicting whether or\nnot a collection of instances share a certain semantic property. For instance,\nin kinship verification given a collection of images, the goal is to predict\nwhether or not they share a {\\it familial} relationship. In this context we\npropose a novel probability model and introduce latent {\\em view-specific} and\n{\\em view-shared} random variables to jointly account for the view-specific\nappearance and cross-view similarities among data instances. Our model posits\nthat data from each view is independent conditioned on the shared variables.\nThis postulate leads to a parametric probability model that decomposes group\nmembership likelihood into a tensor product of data-independent parameters and\ndata-dependent factors. We propose learning the data-independent parameters in\na discriminative way with bilinear classifiers, and test our prediction\nalgorithm on challenging visual recognition tasks such as multi-camera person\nre-identification and kinship verification. On most benchmark datasets, our\nmethod can significantly outperform the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 01:22:40 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Zhang", "Ziming", ""], ["Chen", "Yuting", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1509.04853", "submitter": "Anirban Dasgupta", "authors": "Anirban Dasgupta, Shubhobrata Bhattacharya and Aurobinda Routray", "title": "SPECFACE - A Dataset of Human Faces Wearing Spectacles", "comments": "5 pages, 9 figures, 1 Table. arXiv admin note: text overlap with\n  arXiv:1505.04055", "journal-ref": "2016 IEEE Students' Technology Symposium (IEEE TechSym 2016)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a database of human faces for persons wearing spectacles.\nThe database consists of images of faces having significant variations with\nrespect to illumination, head pose, skin color, facial expressions and sizes,\nand nature of spectacles. The database contains data of 60 subjects. This\ndatabase is expected to be a precious resource for the development and\nevaluation of algorithms for face detection, eye detection, head tracking, eye\ngaze tracking, etc., for subjects wearing spectacles. As such, this can be a\nvaluable contribution to the computer vision community.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 08:43:53 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 05:15:48 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Dasgupta", "Anirban", ""], ["Bhattacharya", "Shubhobrata", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1509.04863", "submitter": "Chun-Shien Lu", "authors": "Sung-Hsien Hsieh and Chun-Shien Lu and and Soo-Chang Pei", "title": "Fast Template Matching by Subsampled Circulant Matrix", "comments": "7 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template matching is widely used for many applications in image and signal\nprocessing and usually is time-critical. Traditional methods usually focus on\nhow to reduce the search locations by coarse-to-fine strategy or full search\ncombined with pruning strategy. However, the computation cost of those methods\nis easily dominated by the size of signal N instead of that of template K. This\npaper proposes a probabilistic and fast matching scheme, which computation\ncosts requires O(N) additions and O(K \\log K) multiplications, based on\ncross-correlation. The nuclear idea is to first downsample signal, which size\nbecomes O(K), and then subsequent operations only involves downsampled signals.\nThe probability of successful match depends on cross-correlation between signal\nand the template. We show the sufficient condition for successful match and\nprove that the probability is high for binary signals with K^2/log K >= O(N).\nThe experiments shows this proposed scheme is fast and efficient and supports\nthe theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 09:38:29 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Hsieh", "Sung-Hsien", ""], ["Lu", "Chun-Shien", ""], ["Pei", "and Soo-Chang", ""]]}, {"id": "1509.04874", "submitter": "Lichao Huang", "authors": "Lichao Huang and Yi Yang and Yafeng Deng and Yinan Yu", "title": "DenseBox: Unifying Landmark Localization with End to End Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can a single fully convolutional neural network (FCN) perform on object\ndetection? We introduce DenseBox, a unified end-to-end FCN framework that\ndirectly predicts bounding boxes and object class confidences through all\nlocations and scales of an image. Our contribution is two-fold. First, we show\nthat a single FCN, if designed and optimized carefully, can detect multiple\ndifferent objects extremely accurately and efficiently. Second, we show that\nwhen incorporating with landmark localization during multi-task learning,\nDenseBox further improves object detection accuray. We present experimental\nresults on public benchmark datasets including MALF face detection and KITTI\ncar detection, that indicate our DenseBox is the state-of-the-art system for\ndetecting challenging objects such as faces and cars.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 10:30:37 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2015 00:20:08 GMT"}, {"version": "v3", "created": "Sat, 19 Sep 2015 02:36:04 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Huang", "Lichao", ""], ["Yang", "Yi", ""], ["Deng", "Yafeng", ""], ["Yu", "Yinan", ""]]}, {"id": "1509.04887", "submitter": "Anirban Dasgupta", "authors": "Anirban Dasgupta, Anshit Mandloi, Anjith George and Aurobinda Routray", "title": "An Improved Algorithm for Eye Corner Detection", "comments": null, "journal-ref": "IEEE Conference on Signal Processing and Communications SPCOM,\n  2016", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a modified algorithm for the detection of nasal and temporal\neye corners is presented. The algorithm is a modification of the Santos and\nProenka Method. In the first step, we detect the face and the eyes using\nclassifiers based on Haar-like features. We then segment out the sclera, from\nthe detected eye region. From the segmented sclera, we segment out an\napproximate eyelid contour. Eye corner candidates are obtained using Harris and\nStephens corner detector. We introduce a post-pruning of the Eye corner\ncandidates to locate the eye corners, finally. The algorithm has been tested on\nYale, JAFFE databases as well as our created database.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 11:19:59 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 08:46:38 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Dasgupta", "Anirban", ""], ["Mandloi", "Anshit", ""], ["George", "Anjith", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1509.04916", "submitter": "Mengyang Yu", "authors": "Li Liu, Mengyang Yu, Ling Shao", "title": "Projection Bank: From High-dimensional Data to Medium-length Binary\n  Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, very high-dimensional feature representations, e.g., Fisher Vector,\nhave achieved excellent performance for visual recognition and retrieval.\nHowever, these lengthy representations always cause extremely heavy\ncomputational and storage costs and even become unfeasible in some large-scale\napplications. A few existing techniques can transfer very high-dimensional data\ninto binary codes, but they still require the reduced code length to be\nrelatively long to maintain acceptable accuracies. To target a better balance\nbetween computational efficiency and accuracies, in this paper, we propose a\nnovel embedding method called Binary Projection Bank (BPB), which can\neffectively reduce the very high-dimensional representations to\nmedium-dimensional binary codes without sacrificing accuracies. Instead of\nusing conventional single linear or bilinear projections, the proposed method\nlearns a bank of small projections via the max-margin constraint to optimally\npreserve the intrinsic data similarity. We have systematically evaluated the\nproposed method on three datasets: Flickr 1M, ILSVR2010 and UCF101, showing\ncompetitive retrieval and recognition accuracies compared with state-of-the-art\napproaches, but with a significantly smaller memory footprint and lower coding\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 13:42:42 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Liu", "Li", ""], ["Yu", "Mengyang", ""], ["Shao", "Ling", ""]]}, {"id": "1509.04942", "submitter": "Xu Jia", "authors": "Xu Jia and Efstratios Gavves and Basura Fernando and Tinne Tuytelaars", "title": "Guiding Long-Short Term Memory for Image Caption Generation", "comments": "accepted by ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we focus on the problem of image caption generation. We propose\nan extension of the long short term memory (LSTM) model, which we coin gLSTM\nfor short. In particular, we add semantic information extracted from the image\nas extra input to each unit of the LSTM block, with the aim of guiding the\nmodel towards solutions that are more tightly coupled to the image content.\nAdditionally, we explore different length normalization strategies for beam\nsearch in order to prevent from favoring short sentences. On various benchmark\ndatasets such as Flickr8K, Flickr30K and MS COCO, we obtain results that are on\npar with or even outperform the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 15:02:30 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Jia", "Xu", ""], ["Gavves", "Efstratios", ""], ["Fernando", "Basura", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1509.04954", "submitter": "Heng Yang", "authors": "Heng Yang and Renqiao Zhang and Peter Robinson", "title": "Human and Sheep Facial Landmarks Localisation by Triplet Interpolated\n  Features", "comments": "submitted to WACV2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method for localisation of facial landmarks on\nhuman and sheep. We introduce a new feature extraction scheme called\ntriplet-interpolated feature used at each iteration of the cascaded shape\nregression framework. It is able to extract features from similar semantic\nlocation given an estimated shape, even when head pose variations are large and\nthe facial landmarks are very sparsely distributed. Furthermore, we study the\nimpact of training data imbalance on model performance and propose a training\nsample augmentation scheme that produces more initialisations for training\nsamples from the minority. More specifically, the augmentation number for a\ntraining sample is made to be negatively correlated to the value of the fitted\nprobability density function at the sample's position. We evaluate the proposed\nscheme on both human and sheep facial landmarks localisation. On the benchmark\n300w human face dataset, we demonstrate the benefits of our proposed methods\nand show very competitive performance when comparing to other methods. On a\nnewly created sheep face dataset, we get very good performance despite the fact\nthat we only have a limited number of training samples and a set of sparse\nlandmarks are annotated.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 15:50:01 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Yang", "Heng", ""], ["Zhang", "Renqiao", ""], ["Robinson", "Peter", ""]]}, {"id": "1509.05016", "submitter": "Ashesh Jain", "authors": "Ashesh Jain, Avi Singh, Hema S Koppula, Shane Soh, Ashutosh Saxena", "title": "Recurrent Neural Networks for Driver Activity Anticipation via\n  Sensory-Fusion Architecture", "comments": "Follow-up of ICCV 2015 Brain4Cars http://www.brain4cars.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anticipating the future actions of a human is a widely studied problem in\nrobotics that requires spatio-temporal reasoning. In this work we propose a\ndeep learning approach for anticipation in sensory-rich robotics applications.\nWe introduce a sensory-fusion architecture which jointly learns to anticipate\nand fuse information from multiple sensory streams. Our architecture consists\nof Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM)\nunits to capture long temporal dependencies. We train our architecture in a\nsequence-to-sequence prediction manner, and it explicitly learns to predict the\nfuture given only a partial temporal context. We further introduce a novel loss\nlayer for anticipation which prevents over-fitting and encourages early\nanticipation. We use our architecture to anticipate driving maneuvers several\nseconds before they happen on a natural driving data set of 1180 miles. The\ncontext for maneuver anticipation comes from multiple sensors installed on the\nvehicle. Our approach shows significant improvement over the state-of-the-art\nin maneuver anticipation by increasing the precision from 77.4% to 90.5% and\nrecall from 71.2% to 87.4%.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 19:49:24 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Jain", "Ashesh", ""], ["Singh", "Avi", ""], ["Koppula", "Hema S", ""], ["Soh", "Shane", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1509.05054", "submitter": "Paul Irofti", "authors": "Paul Irofti and Bogdan Dumitrescu", "title": "Overcomplete Dictionary Learning with Jacobi Atom Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning for sparse representations is traditionally approached\nwith sequential atom updates, in which an optimized atom is used immediately\nfor the optimization of the next atoms. We propose instead a Jacobi version, in\nwhich groups of atoms are updated independently, in parallel. Extensive\nnumerical evidence for sparse image representation shows that the parallel\nalgorithms, especially when all atoms are updated simultaneously, give better\ndictionaries than their sequential counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 20:23:06 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Irofti", "Paul", ""], ["Dumitrescu", "Bogdan", ""]]}, {"id": "1509.05064", "submitter": "Vladislav Voroninski", "authors": "Paul Hand, Choongbum Lee, Vladislav Voroninski", "title": "Exact simultaneous recovery of locations and structure from known\n  orientations and corrupted point correspondences", "comments": "arXiv admin note: text overlap with arXiv:1506.01437", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.CO math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $t_1,\\ldots,t_{n_l} \\in \\mathbb{R}^d$ and $p_1,\\ldots,p_{n_s} \\in\n\\mathbb{R}^d$ and consider the bipartite location recovery problem: given a\nsubset of pairwise direction observations $\\{(t_i - p_j) / \\|t_i -\np_j\\|_2\\}_{i,j \\in [n_l] \\times [n_s]}$, where a constant fraction of these\nobservations are arbitrarily corrupted, find $\\{t_i\\}_{i \\in [n_ll]}$ and\n$\\{p_j\\}_{j \\in [n_s]}$ up to a global translation and scale. We study the\nrecently introduced ShapeFit algorithm as a method for solving this bipartite\nlocation recovery problem. In this case, ShapeFit consists of a simple convex\nprogram over $d(n_l + n_s)$ real variables. We prove that this program recovers\na set of $n_l+n_s$ i.i.d. Gaussian locations exactly and with high probability\nif the observations are given by a bipartite Erd\\H{o}s-R\\'{e}nyi graph, $d$ is\nlarge enough, and provided that at most a constant fraction of observations\ninvolving any particular location are adversarially corrupted. This recovery\ntheorem is based on a set of deterministic conditions that we prove are\nsufficient for exact recovery. Finally, we propose a modified pipeline for the\nStructure for Motion problem, based on this bipartite location recovery\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 20:56:53 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Hand", "Paul", ""], ["Lee", "Choongbum", ""], ["Voroninski", "Vladislav", ""]]}, {"id": "1509.05186", "submitter": "Shicong Liu", "authors": "Shicong Liu, Junru Shao, Hongtao Lu", "title": "Accelerated Distance Computation with Encoding Tree for High Dimensional\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel distance to calculate distance between high dimensional\nvector pairs, utilizing vector quantization generated encodings. Vector\nquantization based methods are successful in handling large scale high\ndimensional data. These methods compress vectors into short encodings, and\nallow efficient distance computation between an uncompressed vector and\ncompressed dataset without decompressing explicitly. However for large\ndatasets, these distance computing methods perform excessive computations. We\navoid excessive computations by storing the encodings on an Encoding\nTree(E-Tree), interestingly the memory consumption is also lowered. We also\npropose Encoding Forest(E-Forest) to further lower the computation cost. E-Tree\nand E-Forest is compatible with various existing quantization-based methods. We\nshow by experiments our methods speed-up distance computing for high\ndimensional data drastically, and various existing algorithms can benefit from\nour methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 09:54:33 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 06:40:22 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Liu", "Shicong", ""], ["Shao", "Junru", ""], ["Lu", "Hongtao", ""]]}, {"id": "1509.05194", "submitter": "Shicong Liu", "authors": "Shicong Liu, Junru Shao, Hongtao Lu", "title": "HCLAE: High Capacity Locally Aggregating Encodings for Approximate\n  Nearest Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector quantization-based approaches are successful to solve Approximate\nNearest Neighbor (ANN) problems which are critical to many applications. The\nidea is to generate effective encodings to allow fast distance approximation.\nWe propose quantization-based methods should partition the data space finely\nand exhibit locality of the dataset to allow efficient non-exhaustive search.\nIn this paper, we introduce the concept of High Capacity Locality Aggregating\nEncodings (HCLAE) to this end, and propose Dictionary Annealing (DA) to learn\nHCLAE by a simulated annealing procedure. The quantization error is lower than\nother state-of-the-art. The algorithms of DA can be easily extended to an\nonline learning scheme, allowing effective handle of large scale data. Further,\nwe propose Aggregating-Tree (A-Tree), a non-exhaustive search method using\nHCLAE to perform efficient ANN-Search. A-Tree achieves magnitudes of speed-up\non ANN-Search tasks, compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 10:18:05 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Liu", "Shicong", ""], ["Shao", "Junru", ""], ["Lu", "Hongtao", ""]]}, {"id": "1509.05195", "submitter": "Shicong Liu", "authors": "Shicong Liu, Hongtao Lu, Junru Shao", "title": "Improved Residual Vector Quantization for High-dimensional Approximate\n  Nearest Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantization methods have been introduced to perform large scale approximate\nnearest search tasks. Residual Vector Quantization (RVQ) is one of the\neffective quantization methods. RVQ uses a multi-stage codebook learning scheme\nto lower the quantization error stage by stage. However, there are two major\nlimitations for RVQ when applied to on high-dimensional approximate nearest\nneighbor search: 1. The performance gain diminishes quickly with added stages.\n2. Encoding a vector with RVQ is actually NP-hard. In this paper, we propose an\nimproved residual vector quantization (IRVQ) method, our IRVQ learns codebook\nwith a hybrid method of subspace clustering and warm-started k-means on each\nstage to prevent performance gain from dropping, and uses a multi-path encoding\nscheme to encode a vector with lower distortion. Experimental results on the\nbenchmark datasets show that our method gives substantially improves RVQ and\ndelivers better performance compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 10:19:37 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Liu", "Shicong", ""], ["Lu", "Hongtao", ""], ["Shao", "Junru", ""]]}, {"id": "1509.05251", "submitter": "Mauricio Delbracio", "authors": "Mauricio Delbracio, Guillermo Sapiro", "title": "Hand-held Video Deblurring via Efficient Fourier Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos captured with hand-held cameras often suffer from a significant amount\nof blur, mainly caused by the inevitable natural tremor of the photographer's\nhand. In this work, we present an algorithm that removes blur due to camera\nshake by combining information in the Fourier domain from nearby frames in a\nvideo. The dynamic nature of typical videos with the presence of multiple\nmoving objects and occlusions makes this problem of camera shake removal\nextremely challenging, in particular when low complexity is needed. Given an\ninput video frame, we first create a consistent registered version of\ntemporally adjacent frames. Then, the set of consistently registered frames is\nblock-wise fused in the Fourier domain with weights depending on the Fourier\nspectrum magnitude. The method is motivated from the physiological fact that\ncamera shake blur has a random nature and therefore, nearby video frames are\ngenerally blurred differently. Experiments with numerous videos recorded in the\nwild, along with extensive comparisons, show that the proposed algorithm\nachieves state-of-the-art results while at the same time being much faster than\nits competitors.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 13:37:39 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 18:37:40 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2015 15:22:25 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Delbracio", "Mauricio", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1509.05267", "submitter": "Xavier Gibert", "authors": "Xavier Gibert, Vishal M. Patel, and Rama Chellappa", "title": "Deep Multi-task Learning for Railway Track Inspection", "comments": "Submitted to IEEE Trans. on Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Railroad tracks need to be periodically inspected and monitored to ensure\nsafe transportation. Automated track inspection using computer vision and\npattern recognition methods have recently shown the potential to improve safety\nby allowing for more frequent inspections while reducing human errors.\nAchieving full automation is still very challenging due to the number of\ndifferent possible failure modes as well as the broad range of image variations\nthat can potentially trigger false alarms. Also, the number of defective\ncomponents is very small, so not many training examples are available for the\nmachine to learn a robust anomaly detector. In this paper, we show that\ndetection performance can be improved by combining multiple detectors within a\nmulti-task learning framework. We show that this approach results in better\naccuracy in detecting defects on railway ties and fasteners.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 14:16:46 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Gibert", "Xavier", ""], ["Patel", "Vishal M.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1509.05301", "submitter": "Victor Schetinger", "authors": "Victor Schetinger, Manuel M. Oliveira, Roberto da Silva, Tiago J.\n  Carvalho", "title": "Humans Are Easily Fooled by Digital Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital images are ubiquitous in our modern lives, with uses ranging from\nsocial media to news, and even scientific papers. For this reason, it is\ncrucial evaluate how accurate people are when performing the task of identify\ndoctored images. In this paper, we performed an extensive user study evaluating\nsubjects capacity to detect fake images. After observing an image, users have\nbeen asked if it had been altered or not. If the user answered the image has\nbeen altered, he had to provide evidence in the form of a click on the image.\nWe collected 17,208 individual answers from 383 users, using 177 images\nselected from public forensic databases. Different from other previously\nstudies, our method propose different ways to avoid lucky guess when evaluating\nusers answers. Our results indicate that people show inaccurate skills at\ndifferentiating between altered and non-altered images, with an accuracy of\n58%, and only identifying the modified images 46.5% of the time. We also track\nuser features such as age, answering time, confidence, providing deep analysis\nof how such variables influence on the users' performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 15:47:25 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Schetinger", "Victor", ""], ["Oliveira", "Manuel M.", ""], ["da Silva", "Roberto", ""], ["Carvalho", "Tiago J.", ""]]}, {"id": "1509.05329", "submitter": "S{\\o}ren S{\\o}nderby", "authors": "S{\\o}ren Kaae S{\\o}nderby and Casper Kaae S{\\o}nderby and Lars\n  Maal{\\o}e and Ole Winther", "title": "Recurrent Spatial Transformer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We integrate the recently proposed spatial transformer network (SPN)\n[Jaderberg et. al 2015] into a recurrent neural network (RNN) to form an\nRNN-SPN model. We use the RNN-SPN to classify digits in cluttered MNIST\nsequences. The proposed model achieves a single digit error of 1.5% compared to\n2.9% for a convolutional networks and 2.0% for convolutional networks with SPN\nlayers. The SPN outputs a zoomed, rotated and skewed version of the input\nimage. We investigate different down-sampling factors (ratio of pixel in input\nand output) for the SPN and show that the RNN-SPN model is able to down-sample\nthe input images without deteriorating performance. The down-sampling in\nRNN-SPN can be thought of as adaptive down-sampling that minimizes the\ninformation loss in the regions of interest. We attribute the superior\nperformance of the RNN-SPN to the fact that it can attend to a sequence of\nregions of interest.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 16:50:29 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["S\u00f8nderby", "S\u00f8ren Kaae", ""], ["S\u00f8nderby", "Casper Kaae", ""], ["Maal\u00f8e", "Lars", ""], ["Winther", "Ole", ""]]}, {"id": "1509.05360", "submitter": "Jiaji Huang", "authors": "Jiaji Huang, Qiang Qiu, Robert Calderbank, Guillermo Sapiro", "title": "Geometry-aware Deep Transform", "comments": "to appear in ICCV2015, updated with minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent efforts have been devoted to designing sophisticated deep\nlearning structures, obtaining revolutionary results on benchmark datasets. The\nsuccess of these deep learning methods mostly relies on an enormous volume of\nlabeled training samples to learn a huge number of parameters in a network;\ntherefore, understanding the generalization ability of a learned deep network\ncannot be overlooked, especially when restricted to a small training set, which\nis the case for many applications. In this paper, we propose a novel deep\nlearning objective formulation that unifies both the classification and metric\nlearning criteria. We then introduce a geometry-aware deep transform to enable\na non-linear discriminative and robust feature transform, which shows\ncompetitive performance on small training sets for both synthetic and\nreal-world data. We further support the proposed framework with a formal\n$(K,\\epsilon)$-robustness analysis.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 18:30:10 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2015 19:28:25 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Huang", "Jiaji", ""], ["Qiu", "Qiang", ""], ["Calderbank", "Robert", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1509.05366", "submitter": "Nazli Ikizler-Cinbis", "authors": "Gokhan Tanisik, Cemil Zalluhoglu, Nazli Ikizler-Cinbis", "title": "Facial Descriptors for Human Interaction Recognition In Still Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach in a rarely studied area of computer\nvision: Human interaction recognition in still images. We explore whether the\nfacial regions and their spatial configurations contribute to the recognition\nof interactions. In this respect, our method involves extraction of several\nvisual features from the facial regions, as well as incorporation of scene\ncharacteristics and deep features to the recognition. Extracted multiple\nfeatures are utilized within a discriminative learning framework for\nrecognizing interactions between people. Our designed facial descriptors are\nbased on the observation that relative positions, size and locations of the\nfaces are likely to be important for characterizing human interactions. Since\nthere is no available dataset in this relatively new domain, a comprehensive\nnew dataset which includes several images of human interactions is collected.\nOur experimental results show that faces and scene characteristics contain\nimportant information to recognize interactions between people.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 18:40:15 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Tanisik", "Gokhan", ""], ["Zalluhoglu", "Cemil", ""], ["Ikizler-Cinbis", "Nazli", ""]]}, {"id": "1509.05371", "submitter": "Felix Trier", "authors": "Peter Burkert, Felix Trier, Muhammad Zeshan Afzal, Andreas Dengel,\n  Marcus Liwicki", "title": "DeXpression: Deep Convolutional Neural Network for Expression\n  Recognition", "comments": "Under consideration for publication in Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a convolutional neural network (CNN) architecture for facial\nexpression recognition. The proposed architecture is independent of any\nhand-crafted feature extraction and performs better than the earlier proposed\nconvolutional neural network based approaches. We visualize the automatically\nextracted features which have been learned by the network in order to provide a\nbetter understanding. The standard datasets, i.e. Extended Cohn-Kanade (CKP)\nand MMI Facial Expression Databse are used for the quantitative evaluation. On\nthe CKP set the current state of the art approach, using CNNs, achieves an\naccuracy of 99.2%. For the MMI dataset, currently the best accuracy for emotion\nrecognition is 93.33%. The proposed architecture achieves 99.6% for CKP and\n98.63% for MMI, therefore performing better than the state of the art using\nCNNs. Automatic facial expression recognition has a broad spectrum of\napplications such as human-computer interaction and safety systems. This is due\nto the fact that non-verbal cues are important forms of communication and play\na pivotal role in interpersonal communication. The performance of the proposed\narchitecture endorses the efficacy and reliable usage of the proposed work for\nreal world applications.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 18:49:10 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 19:34:55 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Burkert", "Peter", ""], ["Trier", "Felix", ""], ["Afzal", "Muhammad Zeshan", ""], ["Dengel", "Andreas", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1509.05463", "submitter": "Xi Zhang", "authors": "Xi Zhang and Yanwei Fu and Shanshan Jiang and Leonid Sigal and Gady\n  Agam", "title": "Learning from Synthetic Data Using a Stacked Multichannel Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from synthetic data has many important and practical applications.\nAn example of application is photo-sketch recognition. Using synthetic data is\nchallenging due to the differences in feature distributions between synthetic\nand real data, a phenomenon we term synthetic gap. In this paper, we\ninvestigate and formalize a general framework-Stacked Multichannel Autoencoder\n(SMCAE) that enables bridging the synthetic gap and learning from synthetic\ndata more efficiently. In particular, we show that our SMCAE can not only\ntransform and use synthetic data on the challenging face-sketch recognition\ntask, but that it can also help simulate real images, which can be used for\ntraining classifiers for recognition. Preliminary experiments validate the\neffectiveness of the framework.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 22:26:30 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 19:28:12 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Zhang", "Xi", ""], ["Fu", "Yanwei", ""], ["Jiang", "Shanshan", ""], ["Sigal", "Leonid", ""], ["Agam", "Gady", ""]]}, {"id": "1509.05520", "submitter": "Zhe Chen", "authors": "Zhe Chen, Zhibin Hong and Dacheng Tao", "title": "An Experimental Survey on Correlation Filter-based Tracking", "comments": "13 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over these years, Correlation Filter-based Trackers (CFTs) have aroused\nincreasing interests in the field of visual object tracking, and have achieved\nextremely compelling results in different competitions and benchmarks. In this\npaper, our goal is to review the developments of CFTs with extensive\nexperimental results. 11 trackers are surveyed in our work, based on which a\ngeneral framework is summarized. Furthermore, we investigate different training\nschemes for correlation filters, and also discuss various effective\nimprovements that have been made recently. Comprehensive experiments have been\nconducted to evaluate the effectiveness and efficiency of the surveyed CFTs,\nand comparisons have been made with other competing trackers. The experimental\nresults have shown that state-of-art performance, in terms of robustness, speed\nand accuracy, can be achieved by several recent CFTs, such as MUSTer and SAMF.\nWe find that further improvements for correlation filter-based tracking can be\nmade on estimating scales, applying part-based tracking strategy and\ncooperating with long-term tracking methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 07:07:15 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Chen", "Zhe", ""], ["Hong", "Zhibin", ""], ["Tao", "Dacheng", ""]]}, {"id": "1509.05536", "submitter": "Arnold Wiliem", "authors": "Kun Zhao, Azadeh Alavi, Arnold Wiliem and Brian C. Lovell", "title": "Efficient Clustering on Riemannian Manifolds: A Kernelised Random\n  Projection Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reformulating computer vision problems over Riemannian manifolds has\ndemonstrated superior performance in various computer vision applications. This\nis because visual data often forms a special structure lying on a lower\ndimensional space embedded in a higher dimensional space. However, since these\nmanifolds belong to non-Euclidean topological spaces, exploiting their\nstructures is computationally expensive, especially when one considers the\nclustering analysis of massive amounts of data. To this end, we propose an\nefficient framework to address the clustering problem on Riemannian manifolds.\nThis framework implements random projections for manifold points via kernel\nspace, which can preserve the geometric structure of the original space, but is\ncomputationally efficient. Here, we introduce three methods that follow our\nframework. We then validate our framework on several computer vision\napplications by comparing against popular clustering methods on Riemannian\nmanifolds. Experimental results demonstrate that our framework maintains the\nperformance of the clustering whilst massively reducing computational\ncomplexity by over two orders of magnitude in some cases.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 08:12:17 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Zhao", "Kun", ""], ["Alavi", "Azadeh", ""], ["Wiliem", "Arnold", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1509.05592", "submitter": "Changsoo Je", "authors": "Kwang Hee Lee, Changsoo Je, Sang Wook Lee", "title": "Color-Stripe Structured Light Robust to Surface Color and Discontinuity", "comments": "10 pages, 9 figures, 8th Asian Conference on Computer Vision (ACCV),\n  Tokyo, Japan, November 2007, Proceedings, Part II", "journal-ref": "Computer Vision - ACCV 2007, LNCS 4844, pp. 507-516, Springer\n  Berlin Heidelberg, November 14, 2007", "doi": "10.1007/978-3-540-76390-1_50", "report-no": null, "categories": "cs.CV cs.GR physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple color stripes have been employed for structured light-based rapid\nrange imaging to increase the number of uniquely identifiable stripes. The use\nof multiple color stripes poses two problems: (1) object surface color may\ndisturb the stripe color and (2) the number of adjacent stripes required for\nidentifying a stripe may not be maintained near surface discontinuities such as\noccluding boundaries. In this paper, we present methods to alleviate those\nproblems. Log-gradient filters are employed to reduce the influence of object\ncolors, and color stripes in two and three directions are used to increase the\nchance of identifying correct stripes near surface discontinuities.\nExperimental results demonstrate the effectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 11:26:19 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Lee", "Kwang Hee", ""], ["Je", "Changsoo", ""], ["Lee", "Sang Wook", ""]]}, {"id": "1509.05634", "submitter": "Alona Golts", "authors": "Alona Golts and Michael Elad", "title": "Linearized Kernel Dictionary Learning", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2016.2555241", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new approach of incorporating kernels into\ndictionary learning. The kernel K-SVD algorithm (KKSVD), which has been\nintroduced recently, shows an improvement in classification performance, with\nrelation to its linear counterpart K-SVD. However, this algorithm requires the\nstorage and handling of a very large kernel matrix, which leads to high\ncomputational cost, while also limiting its use to setups with small number of\ntraining examples. We address these problems by combining two ideas: first we\napproximate the kernel matrix using a cleverly sampled subset of its columns\nusing the Nystr\\\"{o}m method; secondly, as we wish to avoid using this matrix\naltogether, we decompose it by SVD to form new \"virtual samples,\" on which any\nlinear dictionary learning can be employed. Our method, termed \"Linearized\nKernel Dictionary Learning\" (LKDL) can be seamlessly applied as a\npre-processing stage on top of any efficient off-the-shelf dictionary learning\nscheme, effectively \"kernelizing\" it. We demonstrate the effectiveness of our\nmethod on several tasks of both supervised and unsupervised classification and\nshow the efficiency of the proposed scheme, its easy integration and\nperformance boosting properties.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 13:52:56 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Golts", "Alona", ""], ["Elad", "Michael", ""]]}, {"id": "1509.05715", "submitter": "Vahan Hovhannisyan", "authors": "Vahan Hovhannisyan, Panos Parpas and Stefanos Zafeiriou", "title": "MAGMA: Multi-level accelerated gradient mirror descent algorithm for\n  large-scale convex composite minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composite convex optimization models arise in several applications, and are\nespecially prevalent in inverse problems with a sparsity inducing norm and in\ngeneral convex optimization with simple constraints. The most widely used\nalgorithms for convex composite models are accelerated first order methods,\nhowever they can take a large number of iterations to compute an acceptable\nsolution for large-scale problems. In this paper we propose to speed up first\norder methods by taking advantage of the structure present in many applications\nand in image processing in particular. Our method is based on multi-level\noptimization methods and exploits the fact that many applications that give\nrise to large scale models can be modelled using varying degrees of fidelity.\nWe use Nesterov's acceleration techniques together with the multi-level\napproach to achieve $\\mathcal{O}(1/\\sqrt{\\epsilon})$ convergence rate, where\n$\\epsilon$ denotes the desired accuracy. The proposed method has a better\nconvergence rate than any other existing multi-level method for convex\nproblems, and in addition has the same rate as accelerated methods, which is\nknown to be optimal for first-order methods. Moreover, as our numerical\nexperiments show, on large-scale face recognition problems our algorithm is\nseveral times faster than the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 17:25:16 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2016 16:46:45 GMT"}, {"version": "v3", "created": "Thu, 14 Jul 2016 13:33:21 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Hovhannisyan", "Vahan", ""], ["Parpas", "Panos", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1509.05844", "submitter": "Zhibo Yang", "authors": "Zhibo Yang, Huanle Xu, Keda Fu and Yong Xia", "title": "Similar Handwritten Chinese Character Discrimination by Weakly\n  Supervised Learning", "comments": "5 figures, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches for handwritten Chinese character recognition suffer\nin classifying similar characters. In this paper, we propose to discriminate\nsimilar handwritten Chinese characters by using weakly supervised learning. Our\napproach learns a discriminative SVM for each similar pair which simultaneously\nlocalizes the discriminative region of similar character and makes the\nclassification. For the first time, similar handwritten Chinese character\nrecognition (SHCCR) is formulated as an optimization problem extended from SVM.\nWe also propose a novel feature descriptor, Gradient Context, and apply\nbag-of-words model to represent regions with different scales. In our method,\nwe do not need to select a sized-fixed sub-window to differentiate similar\ncharacters. The unconstrained property makes our method well adapted to high\nvariance in the size and position of discriminative regions in similar\nhandwritten Chinese characters. We evaluate our proposed approach over the\nCASIA Chinese character data set and the results show that our method\noutperforms the state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2015 03:08:42 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Yang", "Zhibo", ""], ["Xu", "Huanle", ""], ["Fu", "Keda", ""], ["Xia", "Yong", ""]]}, {"id": "1509.05897", "submitter": "Xu Yang", "authors": "Xu Yang", "title": "Face Photo Sketch Synthesis via Larger Patch and Multiresolution Spline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Face photo sketch synthesis has got some researchers' attention in recent\nyears because of its potential applications in digital entertainment and law\nenforcement. Some patches based methods have been proposed to solve this\nproblem. These methods usually focus more on how to get a sketch patch for a\ngiven photo patch than how to blend these generated patches. However, without\nappropriately blending method, some jagged parts and mottled points will appear\nin the entire face sketch. In order to get a smoother sketch, we propose a new\nmethod to reduce such jagged parts and mottled points. In our system, we resort\nto an existed method, which is Markov Random Fields (MRF), to train a crude\nface sketch firstly. Then this crude sketch face sketch will be divided into\nsome larger patches again and retrained by Non-Negative Matrix Factorization\n(NMF). At last, we use Multiresolution Spline and a blend trick named\nfull-coverage trick to blend these retrained patches. The experiment results\nshow that compared with some previous method, we can get a smoother face\nsketch.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2015 14:20:50 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Yang", "Xu", ""]]}, {"id": "1509.05909", "submitter": "Alex Kendall", "authors": "Alex Kendall and Roberto Cipolla", "title": "Modelling Uncertainty in Deep Learning for Camera Relocalization", "comments": "ICRA 2016; Fixed numerical error with rotation results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust and real-time monocular six degree of freedom visual\nrelocalization system. We use a Bayesian convolutional neural network to\nregress the 6-DOF camera pose from a single RGB image. It is trained in an\nend-to-end manner with no need of additional engineering or graph optimisation.\nThe algorithm can operate indoors and outdoors in real time, taking under 6ms\nto compute. It obtains approximately 2m and 6 degrees accuracy for very large\nscale outdoor scenes and 0.5m and 10 degrees accuracy indoors. Using a Bayesian\nconvolutional neural network implementation we obtain an estimate of the\nmodel's relocalization uncertainty and improve state of the art localization\naccuracy on a large scale outdoor dataset. We leverage the uncertainty measure\nto estimate metric relocalization error and to detect the presence or absence\nof the scene in the input image. We show that the model's uncertainty is caused\nby images being dissimilar to the training dataset in either pose or\nappearance.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2015 16:01:05 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 13:30:25 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Kendall", "Alex", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1509.05962", "submitter": "Rakesh Achanta", "authors": "Rakesh Achanta, Trevor Hastie", "title": "Telugu OCR Framework using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the task of Optical Character Recognition(OCR) for\nthe Telugu script. We present an end-to-end framework that segments the text\nimage, classifies the characters and extracts lines using a language model. The\nsegmentation is based on mathematical morphology. The classification module,\nwhich is the most challenging task of the three, is a deep convolutional neural\nnetwork. The language is modelled as a third degree markov chain at the glyph\nlevel. Telugu script is a complex alphasyllabary and the language is\nagglutinative, making the problem hard. In this paper we apply the latest\nadvances in neural networks to achieve state-of-the-art error rates. We also\nreview convolutional neural networks in great detail and expound the\nstatistical justification behind the many tricks needed to make Deep Learning\nwork.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 03:35:05 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 02:29:04 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Achanta", "Rakesh", ""], ["Hastie", "Trevor", ""]]}, {"id": "1509.06003", "submitter": "Fanghui Liu", "authors": "Fanghui Liu, Tao Zhou, Keren Fu, Irene Y.H. Gu, Jie Yang", "title": "Robust Visual Tracking via Inverse Nonnegative Matrix Factorization", "comments": "This paper has been withdrawn by the author due to part-based\n  representation. On one hand, not all data can be successfully identified as\n  'parts' by NMF. On the other hand, inverse sparse representation could not\n  fit this situation. I will give a clearer explanation from clustering instead\n  of part-based representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The establishment of robust target appearance model over time is an\noverriding concern in visual tracking. In this paper, we propose an inverse\nnonnegative matrix factorization (NMF) method for robust appearance modeling.\nRather than using a linear combination of nonnegative basis matrices for each\ntarget image patch in the conventional NMF, the proposed method is a reverse\nthought to conventional NMF tracker. It utilizes both the foreground and\nbackground information, and imposes a local coordinate constraint, where the\nbasis matrix is sparse matrix from the linear combination of candidates with\ncorresponding nonnegative coefficient vectors. Inverse NMF is used as a feature\nencoder, where the resulting coefficient vectors are fed into a SVM classifier\nfor separating the target from the background. The proposed method is tested on\nseveral videos and compared with seven state-of-the-art methods. Our results\nhave provided further support to the effectiveness and robustness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 12:10:37 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 11:18:40 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 07:20:57 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Liu", "Fanghui", ""], ["Zhou", "Tao", ""], ["Fu", "Keren", ""], ["Gu", "Irene Y. H.", ""], ["Yang", "Jie", ""]]}, {"id": "1509.06004", "submitter": "Vlad Olaru", "authors": "Vlad Olaru, Mihai Florea and Cristian Sminchisescu", "title": "A Parallel Framework for Parametric Maximum Flow Problems in Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework that supports the implementation of parallel\nsolutions for the widespread parametric maximum flow computational routines\nused in image segmentation algorithms. The framework is based on supergraphs, a\nspecial construction combining several image graphs into a larger one, and\nworks on various architectures (multi-core or GPU), either locally or remotely\nin a cluster of computing nodes. The framework can also be used for performance\nevaluation of parallel implementations of maximum flow algorithms. We present\nthe case study of a state-of-the-art image segmentation algorithm based on\ngraph cuts, Constrained Parametric Min-Cut (CPMC), that uses the parallel\nframework to solve parametric maximum flow problems, based on a GPU\nimplementation of the well-known push-relabel algorithm. Our results indicate\nthat real-time implementations based on the proposed techniques are possible.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 12:15:08 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 14:08:32 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Olaru", "Vlad", ""], ["Florea", "Mihai", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1509.06016", "submitter": "Lei Deng", "authors": "Lei Deng, Siyuan Huang, Yueqi Duan, Baohua Chen, Jie Zhou", "title": "Image Set Querying Based Localization", "comments": "VCIP2015, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional single image based localization methods usually fail to localize\na querying image when there exist large variations between the querying image\nand the pre-built scene. To address this, we propose an image-set querying\nbased localization approach. When the localization by a single image fails to\nwork, the system will ask the user to capture more auxiliary images. First, a\nlocal 3D model is established for the querying image set. Then, the pose of the\nquerying image set is estimated by solving a nonlinear optimization problem,\nwhich aims to match the local 3D model against the pre-built scene. Experiments\nhave shown the effectiveness and feasibility of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 13:49:30 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Deng", "Lei", ""], ["Huang", "Siyuan", ""], ["Duan", "Yueqi", ""], ["Chen", "Baohua", ""], ["Zhou", "Jie", ""]]}, {"id": "1509.06033", "submitter": "Arsalan Mousavian", "authors": "Arsalan Mousavian, Jana Kosecka", "title": "Deep Convolutional Features for Image Based Retrieval and Scene\n  Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent approaches showed how the representations learned by\nConvolutional Neural Networks can be repurposed for novel tasks. Most commonly\nit has been shown that the activation features of the last fully connected\nlayers (fc7 or fc6) of the network, followed by a linear classifier outperform\nthe state-of-the-art on several recognition challenge datasets. Instead of\nrecognition, this paper focuses on the image retrieval problem and proposes a\nexamines alternative pooling strategies derived for CNN features. The presented\nscheme uses the features maps from an earlier layer 5 of the CNN architecture,\nwhich has been shown to preserve coarse spatial information and is semantically\nmeaningful. We examine several pooling strategies and demonstrate superior\nperformance on the image retrieval task (INRIA Holidays) at the fraction of the\ncomputational cost, while using a relatively small memory requirements. In\naddition to retrieval, we see similar efficiency gains on the SUN397 scene\ncategorization dataset, demonstrating wide applicability of this simple\nstrategy. We also introduce and evaluate a novel GeoPlaces5K dataset from\ndifferent geographical locations in the world for image retrieval that stresses\nmore dramatic changes in appearance and viewpoint.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 17:56:57 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Mousavian", "Arsalan", ""], ["Kosecka", "Jana", ""]]}, {"id": "1509.06035", "submitter": "Karim Afdel", "authors": "H.Ouahi and K.Afdel and M.Machkour", "title": "Image Retrieval Based on LBP Pyramidal Multiresolution using Reversible\n  Watermarking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the medical field, images are increasingly used to facilitate diagnosis of\ndiseases. These images are stored in multimedia databases accompanied by doctor\ns prescriptions and other information related to patients.Search for medical\nimages has become for clinical applications an essential tool to bring\neffective aid in diagnosis. Content Based Image Retrieval (CBIR) is one of the\npossible solutions to effectively manage these databases. Our contribution is\nto define a relevant descriptor to retrieve images based on multiresolution\nanalysis of texture using Local Binary Pattern LBP. This descriptor once\ncalculated and information s relating to the patient; will be placed in the\nimage using the technique of reversible watermarking. Thereby, the image,\ndescriptor of its contents, the BFILE locator and patientrelated information\nbecome a single entity, so even the administrator cannot have access to the\npatient private data.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 18:14:04 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Ouahi", "H.", ""], ["Afdel", "K.", ""], ["Machkour", "M.", ""]]}, {"id": "1509.06041", "submitter": "Quanzeng You", "authors": "Quanzeng You, Jiebo Luo, Hailin Jin, Jianchao Yang", "title": "Robust Image Sentiment Analysis Using Progressively Trained and Domain\n  Transferred Deep Networks", "comments": "9 pages, 5 figures, AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis of online user generated content is important for many\nsocial media analytics tasks. Researchers have largely relied on textual\nsentiment analysis to develop systems to predict political elections, measure\neconomic indicators, and so on. Recently, social media users are increasingly\nusing images and videos to express their opinions and share their experiences.\nSentiment analysis of such large scale visual content can help better extract\nuser sentiments toward events or topics, such as those in image tweets, so that\nprediction of sentiment from visual content is complementary to textual\nsentiment analysis. Motivated by the needs in leveraging large scale yet noisy\ntraining data to solve the extremely challenging problem of image sentiment\nanalysis, we employ Convolutional Neural Networks (CNN). We first design a\nsuitable CNN architecture for image sentiment analysis. We obtain half a\nmillion training samples by using a baseline sentiment algorithm to label\nFlickr images. To make use of such noisy machine labeled data, we employ a\nprogressive strategy to fine-tune the deep network. Furthermore, we improve the\nperformance on Twitter images by inducing domain transfer with a small number\nof manually labeled Twitter images. We have conducted extensive experiments on\nmanually labeled Twitter images. The results show that the proposed CNN can\nachieve better performance in image sentiment analysis than competing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 18:36:01 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["You", "Quanzeng", ""], ["Luo", "Jiebo", ""], ["Jin", "Hailin", ""], ["Yang", "Jianchao", ""]]}, {"id": "1509.06066", "submitter": "Mahyar Najibi", "authors": "Mahyar Najibi, Mohammad Rastegari, Larry S. Davis", "title": "On Large-Scale Retrieval: Binary or n-ary Coding?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing amount of data available in modern-day datasets makes the need to\nefficiently search and retrieve information. To make large-scale search\nfeasible, Distance Estimation and Subset Indexing are the main approaches.\nAlthough binary coding has been popular for implementing both techniques, n-ary\ncoding (known as Product Quantization) is also very effective for Distance\nEstimation. However, their relative performance has not been studied for Subset\nIndexing. We investigate whether binary or n-ary coding works better under\ndifferent retrieval strategies. This leads to the design of a new n-ary coding\nmethod, \"Linear Subspace Quantization (LSQ)\" which, unlike other n-ary\nencoders, can be used as a similarity-preserving embedding. Experiments on\nimage retrieval show that when Distance Estimation is used, n-ary LSQ\noutperforms other methods. However, when Subset Indexing is applied,\ninterestingly, binary codings are more effective and binary LSQ achieves the\nbest accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 22:32:23 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Najibi", "Mahyar", ""], ["Rastegari", "Mohammad", ""], ["Davis", "Larry S.", ""]]}, {"id": "1509.06086", "submitter": "Zuxuan Wu", "authors": "Zuxuan Wu, Yu-Gang Jiang, Xi Wang, Hao Ye, Xiangyang Xue, Jun Wang", "title": "Fusing Multi-Stream Deep Networks for Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies deep network architectures to address the problem of video\nclassification. A multi-stream framework is proposed to fully utilize the rich\nmultimodal information in videos. Specifically, we first train three\nConvolutional Neural Networks to model spatial, short-term motion and audio\nclues respectively. Long Short Term Memory networks are then adopted to explore\nlong-term temporal dynamics. With the outputs of the individual streams, we\npropose a simple and effective fusion method to generate the final predictions,\nwhere the optimal fusion weights are learned adaptively for each class, and the\nlearning process is regularized by automatically estimated class relationships.\nOur contributions are two-fold. First, the proposed multi-stream framework is\nable to exploit multimodal features that are more comprehensive than those\npreviously attempted. Second, we demonstrate that the adaptive fusion method\nusing the class relationship as a regularizer outperforms traditional\nalternatives that estimate the weights in a \"free\" fashion. Our framework\nproduces significantly better results than the state of the arts on two popular\nbenchmarks, 92.2\\% on UCF-101 (without using audio) and 84.9\\% on Columbia\nConsumer Videos.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 00:38:54 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 01:29:44 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Wu", "Zuxuan", ""], ["Jiang", "Yu-Gang", ""], ["Wang", "Xi", ""], ["Ye", "Hao", ""], ["Xue", "Xiangyang", ""], ["Wang", "Jun", ""]]}, {"id": "1509.06113", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine,\n  Pieter Abbeel", "title": "Deep Spatial Autoencoders for Visuomotor Learning", "comments": "Published in the International Conference on Robotics and Automation\n  (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning provides a powerful and flexible framework for\nautomated acquisition of robotic motion skills. However, applying reinforcement\nlearning requires a sufficiently detailed representation of the state,\nincluding the configuration of task-relevant objects. We present an approach\nthat automates state-space construction by learning a state representation\ndirectly from camera images. Our method uses a deep spatial autoencoder to\nacquire a set of feature points that describe the environment for the current\ntask, such as the positions of objects, and then learns a motion skill with\nthese feature points using an efficient reinforcement learning method based on\nlocal linear models. The resulting controller reacts continuously to the\nlearned feature points, allowing the robot to dynamically manipulate objects in\nthe world with closed-loop control. We demonstrate our method with a PR2 robot\non tasks that include pushing a free-standing toy block, picking up a bag of\nrice using a spatula, and hanging a loop of rope on a hook at various\npositions. In each task, our method automatically learns to track task-relevant\nobjects and manipulate their configuration with the robot's arm.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 06:15:12 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 18:03:25 GMT"}, {"version": "v3", "created": "Tue, 1 Mar 2016 17:24:50 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Finn", "Chelsea", ""], ["Tan", "Xin Yu", ""], ["Duan", "Yan", ""], ["Darrell", "Trevor", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1509.06114", "submitter": "Inwook Shim", "authors": "Inwook Shim, Seunghak Shin, Yunsu Bok, Kyungdon Joo, Dong-Geol Choi,\n  Joon-Young Lee, Jaesik Park, Jun-Ho Oh, In So Kweon", "title": "Vision System and Depth Processing for DRC-HUBO+", "comments": "submitted in ICRA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a vision system and a depth processing algorithm for\nDRC-HUBO+, the winner of the DRC finals 2015. Our system is designed to\nreliably capture 3D information of a scene and objects robust to challenging\nenvironment conditions. We also propose a depth-map upsampling method that\nproduces an outliers-free depth map by explicitly handling depth outliers. Our\nsystem is suitable for an interactive robot with real-world that requires\naccurate object detection and pose estimation. We evaluate our depth processing\nalgorithm over state-of-the-art algorithms on several synthetic and real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 06:17:21 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2015 15:05:37 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Shim", "Inwook", ""], ["Shin", "Seunghak", ""], ["Bok", "Yunsu", ""], ["Joo", "Kyungdon", ""], ["Choi", "Dong-Geol", ""], ["Lee", "Joon-Young", ""], ["Park", "Jaesik", ""], ["Oh", "Jun-Ho", ""], ["Kweon", "In So", ""]]}, {"id": "1509.06161", "submitter": "Qijun Zhao", "authors": "Feng Liu, Dan Zeng, Jing Li and Qijun Zhao", "title": "On 3D Face Reconstruction via Cascaded Regression in Shape Space", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascaded regression has been recently applied to reconstructing 3D faces from\nsingle 2D images directly in shape space, and achieved state-of-the-art\nperformance. This paper investigates thoroughly such cascaded regression based\n3D face reconstruction approaches from four perspectives that are not well\nstudied yet: (i) The impact of the number of 2D landmarks; (ii) the impact of\nthe number of 3D vertices; (iii) the way of using standalone automated landmark\ndetection methods; and (iv) the convergence property. To answer these\nquestions, a simplified cascaded regression based 3D face reconstruction method\nis devised, which can be integrated with standalone automated landmark\ndetection methods and reconstruct 3D face shapes that have the same pose and\nexpression as the input face images, rather than normalized pose and\nexpression. Moreover, an effective training method is proposed by disturbing\nthe automatically detected landmarks. Comprehensive evaluation experiments have\nbeen done with comparison to other 3D face reconstruction methods. The results\nnot only deepen the understanding of cascaded regression based 3D face\nreconstruction approaches, but also prove the effectiveness of proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 09:29:38 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 00:54:34 GMT"}, {"version": "v3", "created": "Tue, 18 Apr 2017 02:57:38 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Liu", "Feng", ""], ["Zeng", "Dan", ""], ["Li", "Jing", ""], ["Zhao", "Qijun", ""]]}, {"id": "1509.06243", "submitter": "Albert Gordo", "authors": "Albert Gordo and Jon Almazan and Naila Murray and Florent Perronnin", "title": "LEWIS: Latent Embeddings for Word Images and their Semantics", "comments": "Accepted for publication at the International Conference on Computer\n  Vision (ICCV) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to bring semantics into the tasks of text\nrecognition and retrieval in natural images. Although text recognition and\nretrieval have received a lot of attention in recent years, previous works have\nfocused on recognizing or retrieving exactly the same word used as a query,\nwithout taking the semantics into consideration.\n  In this paper, we ask the following question: \\emph{can we predict semantic\nconcepts directly from a word image, without explicitly trying to transcribe\nthe word image or its characters at any point?} For this goal we propose a\nconvolutional neural network (CNN) with a weighted ranking loss objective that\nensures that the concepts relevant to the query image are ranked ahead of those\nthat are not relevant. This can also be interpreted as learning a Euclidean\nspace where word images and concepts are jointly embedded. This model is\nlearned in an end-to-end manner, from image pixels to semantic concepts, using\na dataset of synthetically generated word images and concepts mined from a\nlexical database (WordNet). Our results show that, despite the complexity of\nthe task, word images and concepts can indeed be associated with a high degree\nof accuracy\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 14:32:43 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Gordo", "Albert", ""], ["Almazan", "Jon", ""], ["Murray", "Naila", ""], ["Perronnin", "Florent", ""]]}, {"id": "1509.06321", "submitter": "Wojciech Samek", "authors": "Wojciech Samek, Alexander Binder, Gr\\'egoire Montavon, Sebastian Bach,\n  Klaus-Robert M\\\"uller", "title": "Evaluating the visualization of what a Deep Neural Network has learned", "comments": "13 pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have demonstrated impressive performance in\ncomplex machine learning tasks such as image classification or speech\nrecognition. However, due to their multi-layer nonlinear structure, they are\nnot transparent, i.e., it is hard to grasp what makes them arrive at a\nparticular classification or recognition decision given a new unseen data\nsample. Recently, several approaches have been proposed enabling one to\nunderstand and interpret the reasoning embodied in a DNN for a single test\nimage. These methods quantify the ''importance'' of individual pixels wrt the\nclassification decision and allow a visualization in terms of a heatmap in\npixel/input space. While the usefulness of heatmaps can be judged subjectively\nby a human, an objective quality measure is missing. In this paper we present a\ngeneral methodology based on region perturbation for evaluating ordered\ncollections of pixels such as heatmaps. We compare heatmaps computed by three\ndifferent methods on the SUN397, ILSVRC2012 and MIT Places data sets. Our main\nresult is that the recently proposed Layer-wise Relevance Propagation (LRP)\nalgorithm qualitatively and quantitatively provides a better explanation of\nwhat made a DNN arrive at a particular classification decision than the\nsensitivity-based approach or the deconvolution method. We provide theoretical\narguments to explain this result and discuss its practical implications.\nFinally, we investigate the use of heatmaps for unsupervised assessment of\nneural network performance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 17:36:22 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Samek", "Wojciech", ""], ["Binder", "Alexander", ""], ["Montavon", "Gr\u00e9goire", ""], ["Bach", "Sebastian", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1509.06451", "submitter": "Shuo Yang", "authors": "Shuo Yang and Ping Luo and Chen Change Loy and Xiaoou Tang", "title": "From Facial Parts Responses to Face Detection: A Deep Learning Approach", "comments": "To appear in ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel deep convolutional network (DCN) that\nachieves outstanding performance on FDDB, PASCAL Face, and AFW. Specifically,\nour method achieves a high recall rate of 90.99% on the challenging FDDB\nbenchmark, outperforming the state-of-the-art method by a large margin of\n2.91%. Importantly, we consider finding faces from a new perspective through\nscoring facial parts responses by their spatial structure and arrangement. The\nscoring mechanism is carefully formulated considering challenging cases where\nfaces are only partially visible. This consideration allows our network to\ndetect faces under severe occlusion and unconstrained pose variation, which are\nthe main difficulty and bottleneck of most existing face detection approaches.\nWe show that despite the use of DCN, our network can achieve practical runtime\nspeed.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 02:59:31 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Yang", "Shuo", ""], ["Luo", "Ping", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1509.06470", "submitter": "Yiyi Liao", "authors": "Yiyi Liao, Sarath Kodagoda, Yue Wang, Lei Shi, Yong Liu", "title": "Understand Scene Categories by Objects: A Semantic Regularized Scene\n  Classifier Using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene classification is a fundamental perception task for environmental\nunderstanding in today's robotics. In this paper, we have attempted to exploit\nthe use of popular machine learning technique of deep learning to enhance scene\nunderstanding, particularly in robotics applications. As scene images have\nlarger diversity than the iconic object images, it is more challenging for deep\nlearning methods to automatically learn features from scene images with less\nsamples. Inspired by human scene understanding based on object knowledge, we\naddress the problem of scene classification by encouraging deep neural networks\nto incorporate object-level information. This is implemented with a\nregularization of semantic segmentation. With only 5 thousand training images,\nas opposed to 2.5 million images, we show the proposed deep architecture\nachieves superior scene classification results to the state-of-the-art on a\npublicly available SUN RGB-D dataset. In addition, performance of semantic\nsegmentation, the regularizer, also reaches a new record with refinement\nderived from predicted scene labels. Finally, we apply our SUN RGB-D dataset\ntrained model to a mobile robot captured images to classify scenes in our\nuniversity demonstrating the generalization ability of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 05:43:27 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Liao", "Yiyi", ""], ["Kodagoda", "Sarath", ""], ["Wang", "Yue", ""], ["Shi", "Lei", ""], ["Liu", "Yong", ""]]}, {"id": "1509.06557", "submitter": "Weilin Huang", "authors": "Yongqiang Gao and Weilin Huang and Yu Qiao", "title": "Local Multi-Grouped Binary Descriptor with Ring-based Pooling\n  Configuration and Optimization", "comments": "To appear in IEEE Trans. on Image Processing, 2015", "journal-ref": null, "doi": "10.1109/TIP.2015.2469093", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local binary descriptors are attracting increasingly attention due to their\ngreat advantages in computational speed, which are able to achieve real-time\nperformance in numerous image/vision applications. Various methods have been\nproposed to learn data-dependent binary descriptors. However, most existing\nbinary descriptors aim overly at computational simplicity at the expense of\nsignificant information loss which causes ambiguity in similarity measure using\nHamming distance. In this paper, by considering multiple features might share\ncomplementary information, we present a novel local binary descriptor, referred\nas Ring-based Multi-Grouped Descriptor (RMGD), to successfully bridge the\nperformance gap between current binary and floated-point descriptors. Our\ncontributions are two-fold. Firstly, we introduce a new pooling configuration\nbased on spatial ring-region sampling, allowing for involving binary tests on\nthe full set of pairwise regions with different shapes, scales and distances.\nThis leads to a more meaningful description than existing methods which\nnormally apply a limited set of pooling configurations. Then, an extended\nAdaboost is proposed for efficient bit selection by emphasizing high variance\nand low correlation, achieving a highly compact representation. Secondly, the\nRMGD is computed from multiple image properties where binary strings are\nextracted. We cast multi-grouped features integration as rankSVM or sparse SVM\nlearning problem, so that different features can compensate strongly for each\nother, which is the key to discriminativeness and robustness. The performance\nof RMGD was evaluated on a number of publicly available benchmarks, where the\nRMGD outperforms the state-of-the-art binary descriptors significantly.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 11:56:21 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Gao", "Yongqiang", ""], ["Huang", "Weilin", ""], ["Qiao", "Yu", ""]]}, {"id": "1509.06576", "submitter": "P. Christopher Staecker", "authors": "Laurence Boxer, P. Christopher Staecker", "title": "Homotopy relations for digital images", "comments": "30 pages, some revisions & corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GN cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce three generalizations of homotopy equivalence in digital images,\nto allow us to express whether a finite and an infinite digital image are\nsimilar with respect to homotopy.\n  We show that these three generalizations are not equivalent to ordinary\nhomotopy equivalence, and give several examples. We show that, like homotopy\nequivalence, our three generalizations imply isomorphism of fundamental groups,\nand are preserved under wedges and Cartesian products.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 12:49:31 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 23:13:11 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Boxer", "Laurence", ""], ["Staecker", "P. Christopher", ""]]}, {"id": "1509.06658", "submitter": "Nikita Prabhu", "authors": "Nikita Prabhu and R. Venkatesh Babu", "title": "Attribute-Graph: A Graph based approach to Image Ranking", "comments": "In IEEE International Conference on Computer Vision (ICCV) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel image representation, termed Attribute-Graph, to rank\nimages by their semantic similarity to a given query image. An Attribute-Graph\nis an undirected fully connected graph, incorporating both local and global\nimage characteristics. The graph nodes characterise objects as well as the\noverall scene context using mid-level semantic attributes, while the edges\ncapture the object topology. We demonstrate the effectiveness of\nAttribute-Graphs by applying them to the problem of image ranking. We benchmark\nthe performance of our algorithm on the 'rPascal' and 'rImageNet' datasets,\nwhich we have created in order to evaluate the ranking performance on complex\nqueries containing multiple objects. Our experimental evaluation shows that\nmodelling images as Attribute-Graphs results in improved ranking performance\nover existing techniques.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 16:01:02 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 04:38:36 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Prabhu", "Nikita", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1509.06690", "submitter": "Irina Kogan A", "authors": "Irina A. Kogan and Peter J. Olver", "title": "Invariants of objects and their images under surjective maps", "comments": "This paper includes corrections and additions to the published\n  version", "journal-ref": "Lobachevskii J. Math. 36 (2015), 260--285", "doi": "10.1134/S1995080215030063", "report-no": null, "categories": "math.DG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the relationships between the differential invariants of objects\nand of their images under a surjective map. We analyze both the case when the\nunderlying transformation group is projectable and hence induces an action on\nthe image, and the case when only a proper subgroup of the entire group acts\nprojectably. In the former case, we establish a constructible isomorphism\nbetween the algebra of differential invariants of the images and the algebra of\nfiber-wise constant (gauge) differential invariants of the objects. In the\nlatter case, we describe residual effects of the full transformation group on\nthe image invariants. Our motivation comes from the problem of reconstruction\nof an object from multiple-view images, with central and parallel projections\nof curves from three-dimensional space to the two-dimensional plane serving as\nour main examples.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 17:19:52 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Kogan", "Irina A.", ""], ["Olver", "Peter J.", ""]]}, {"id": "1509.06720", "submitter": "Umar Iqbal", "authors": "Hashim Yasin, Umar Iqbal, Bj\\\"orn Kr\\\"uger, Andreas Weber, Juergen\n  Gall", "title": "A Dual-Source Approach for 3D Pose Estimation from a Single Image", "comments": "Accepted to CVPR 2016. The source code and models are publicly\n  available. Title changed from the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major challenge for 3D pose estimation from a single RGB image is the\nacquisition of sufficient training data. In particular, collecting large\namounts of training data that contain unconstrained images and are annotated\nwith accurate 3D poses is infeasible. We therefore propose to use two\nindependent training sources. The first source consists of images with\nannotated 2D poses and the second source consists of accurate 3D motion capture\ndata. To integrate both sources, we propose a dual-source approach that\ncombines 2D pose estimation with efficient and robust 3D pose retrieval. In our\nexperiments, we show that our approach achieves state-of-the-art results and is\neven competitive when the skeleton structure of the two sources differ\nsubstantially.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 18:38:16 GMT"}, {"version": "v2", "created": "Sun, 27 Mar 2016 11:43:06 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Yasin", "Hashim", ""], ["Iqbal", "Umar", ""], ["Kr\u00fcger", "Bj\u00f6rn", ""], ["Weber", "Andreas", ""], ["Gall", "Juergen", ""]]}, {"id": "1509.06729", "submitter": "Manolis Tsakiris", "authors": "Manolis C. Tsakiris and Rene Vidal", "title": "Algebraic Clustering of Affine Subspaces", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence (\n  Volume: 40 , Issue: 2 , Feb. 1 2018 )", "doi": "10.1109/TPAMI.2017.2678477", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering is an important problem in machine learning with many\napplications in computer vision and pattern recognition. Prior work has studied\nthis problem using algebraic, iterative, statistical, low-rank and sparse\nrepresentation techniques. While these methods have been applied to both linear\nand affine subspaces, theoretical results have only been established in the\ncase of linear subspaces. For example, algebraic subspace clustering (ASC) is\nguaranteed to provide the correct clustering when the data points are in\ngeneral position and the union of subspaces is transversal. In this paper we\nstudy in a rigorous fashion the properties of ASC in the case of affine\nsubspaces. Using notions from algebraic geometry, we prove that the\nhomogenization trick, which embeds points in a union of affine subspaces into\npoints in a union of linear subspaces, preserves the general position of the\npoints and the transversality of the union of subspaces in the embedded space,\nthus establishing the correctness of ASC for affine subpaces.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 19:04:00 GMT"}, {"version": "v2", "created": "Sun, 24 Apr 2016 01:56:32 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 18:04:32 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Tsakiris", "Manolis C.", ""], ["Vidal", "Rene", ""]]}, {"id": "1509.06825", "submitter": "Lerrel Pinto Mr", "authors": "Lerrel Pinto and Abhinav Gupta", "title": "Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700\n  Robot Hours", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current learning-based robot grasping approaches exploit human-labeled\ndatasets for training the models. However, there are two problems with such a\nmethodology: (a) since each object can be grasped in multiple ways, manually\nlabeling grasp locations is not a trivial task; (b) human labeling is biased by\nsemantics. While there have been attempts to train robots using trial-and-error\nexperiments, the amount of data used in such experiments remains substantially\nlow and hence makes the learner prone to over-fitting. In this paper, we take\nthe leap of increasing the available training data to 40 times more than prior\nwork, leading to a dataset size of 50K data points collected over 700 hours of\nrobot grasping attempts. This allows us to train a Convolutional Neural Network\n(CNN) for the task of predicting grasp locations without severe overfitting. In\nour formulation, we recast the regression problem to an 18-way binary\nclassification over image patches. We also present a multi-stage learning\napproach where a CNN trained in one stage is used to collect hard negatives in\nsubsequent stages. Our experiments clearly show the benefit of using\nlarge-scale datasets (and multi-stage training) for the task of grasping. We\nalso compare to several baselines and show state-of-the-art performance on\ngeneralization to unseen objects for grasping.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 02:08:02 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Pinto", "Lerrel", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1509.06853", "submitter": "Zahid Ansari", "authors": "Abdullah Gubbi, Mohammed Fazle Azeem and Zahid Ansari", "title": "New Fuzzy LBP Features for Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many Local texture features each very in way they implement and\neach of the Algorithm trying improve the performance. An attempt is made in\nthis paper to represent a theoretically very simple and computationally\neffective approach for face recognition. In our implementation the face image\nis divided into 3x3 sub-regions from which the features are extracted using the\nLocal Binary Pattern (LBP) over a window, fuzzy membership function and at the\ncentral pixel. The LBP features possess the texture discriminative property and\ntheir computational cost is very low. By utilising the information from LBP,\nmembership function, and central pixel, the limitations of traditional LBP is\neliminated. The bench mark database like ORL and Sheffield Databases are used\nfor the evaluation of proposed features with SVM classifier. For the proposed\napproach K-fold and ROC curves are obtained and results are compared.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 06:14:26 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Gubbi", "Abdullah", ""], ["Azeem", "Mohammed Fazle", ""], ["Ansari", "Zahid", ""]]}, {"id": "1509.06925", "submitter": "Mengmeng Wang", "authors": "Mengmeng Wang, Yong Liu", "title": "Robust Object Tracking with a Hierarchical Ensemble Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robots enjoy a wide popularity nowadays and have been applied in\nmany applications, such as home security, entertainment, delivery, navigation\nand guidance. It is vital to robots to track objects accurately in these\napplications, so it is necessary to focus on tracking algorithms to improve the\nrobustness and accuracy. In this paper, we propose a robust object tracking\nalgorithm based on a hierarchical ensemble framework which can incorporate\ninformation including individual pixel features, local patches and holistic\ntarget models. The framework combines multiple ensemble models simultaneously\ninstead of using a single ensemble model individually. A discriminative model\nwhich accounts for the matching degree of local patches is adopted via a bottom\nensemble layer, and a generative model which exploits holistic templates is\nused to search for the object through the middle ensemble layer as well as an\nadaptive Kalman filter. We test the proposed tracker on challenging benchmark\nimage sequences. Both qualitative and quantitative evaluations demonstrate that\nthe proposed tracker performs superiorly against several state-of-the-art\nalgorithms, especially when the appearance changes dramatically and the\nocclusions occur.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 11:23:56 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 01:04:34 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Wang", "Mengmeng", ""], ["Liu", "Yong", ""]]}, {"id": "1509.06939", "submitter": "Giulia Pasquale", "authors": "Giulia Pasquale, Tanis Mar, Carlo Ciliberto, Lorenzo Rosasco, Lorenzo\n  Natale", "title": "Enabling Depth-driven Visual Attention on the iCub Humanoid Robot:\n  Instructions for Use and New Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of depth perception in the interactions that humans have\nwithin their nearby space is a well established fact. Consequently, it is also\nwell known that the possibility of exploiting good stereo information would\nease and, in many cases, enable, a large variety of attentional and interactive\nbehaviors on humanoid robotic platforms. However, the difficulty of computing\nreal-time and robust binocular disparity maps from moving stereo cameras often\nprevents from relying on this kind of cue to visually guide robots' attention\nand actions in real-world scenarios. The contribution of this paper is\ntwo-fold: first, we show that the Efficient Large-scale Stereo Matching\nalgorithm (ELAS) by A. Geiger et al. 2010 for computation of the disparity map\nis well suited to be used on a humanoid robotic platform as the iCub robot;\nsecond, we show how, provided with a fast and reliable stereo system,\nimplementing relatively challenging visual behaviors in natural settings can\nrequire much less effort. As a case of study we consider the common situation\nwhere the robot is asked to focus the attention on one object close in the\nscene, showing how a simple but effective disparity-based segmentation solves\nthe problem in this case. Indeed this example paves the way to a variety of\nother similar applications.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 12:18:01 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Pasquale", "Giulia", ""], ["Mar", "Tanis", ""], ["Ciliberto", "Carlo", ""], ["Rosasco", "Lorenzo", ""], ["Natale", "Lorenzo", ""]]}, {"id": "1509.07009", "submitter": "Dengxin Dai", "authors": "Dengxin Dai, Yujian Wang, Yuhua Chen, Luc Van Gool", "title": "Is Image Super-resolution Helpful for Other Vision Tasks?", "comments": "1. Super-Resolution Forest added 2. Scene Recognition task added 3.\n  Title changed 4. More work cited, WACV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great advances made in the field of image super-resolution (ISR)\nduring the last years, the performance has merely been evaluated perceptually.\nThus, it is still unclear whether ISR is helpful for other vision tasks. In\nthis paper, we present the first comprehensive study and analysis of the\nusefulness of ISR for other vision applications. In particular, six ISR methods\nare evaluated on four popular vision tasks, namely edge detection, semantic\nimage segmentation, digit recognition, and scene recognition. We show that\napplying ISR to input images of other vision systems does improve their\nperformance when the input images are of low-resolution. We also study the\ncorrelation between four standard perceptual evaluation criteria (namely PSNR,\nSSIM, IFC, and NQM) and the usefulness of ISR to the vision tasks. Experiments\nshow that they correlate well with each other in general, but perceptual\ncriteria are still not accurate enough to be used as full proxies for the\nusefulness. We hope this work will inspire the community to evaluate ISR\nmethods also in real vision applications, and to adopt ISR as a pre-processing\nstep of other vision tasks if the resolution of their input images is low.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 14:31:02 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 15:09:11 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Dai", "Dengxin", ""], ["Wang", "Yujian", ""], ["Chen", "Yuhua", ""], ["Van Gool", "Luc", ""]]}, {"id": "1509.07075", "submitter": "Siddhant Ahuja", "authors": "Siddhant Ahuja, Peter Iles, Steven L. Waslander", "title": "3D Scan Registration using Curvelet Features in Planetary Environments", "comments": "27 pages in Journal of Field Robotics, 2015", "journal-ref": null, "doi": "10.1002/rob.21616", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topographic mapping in planetary environments relies on accurate 3D scan\nregistration methods. However, most global registration algorithms relying on\nfeatures such as FPFH and Harris-3D show poor alignment accuracy in these\nsettings due to the poor structure of the Mars-like terrain and variable\nresolution, occluded, sparse range data that is hard to register without some\na-priori knowledge of the environment. In this paper, we propose an alternative\napproach to 3D scan registration using the curvelet transform that performs\nmulti-resolution geometric analysis to obtain a set of coefficients indexed by\nscale (coarsest to finest), angle and spatial position. Features are detected\nin the curvelet domain to take advantage of the directional selectivity of the\ntransform. A descriptor is computed for each feature by calculating the 3D\nspatial histogram of the image gradients, and nearest neighbor based matching\nis used to calculate the feature correspondences. Correspondence rejection\nusing Random Sample Consensus identifies inliers, and a locally optimal\nSingular Value Decomposition-based estimation of the rigid-body transformation\naligns the laser scans given the re-projected correspondences in the metric\nspace. Experimental results on a publicly available data-set of planetary\nanalogue indoor facility, as well as simulated and real-world scans from Neptec\nDesign Group's IVIGMS 3D laser rangefinder at the outdoor CSA Mars yard\ndemonstrates improved performance over existing methods in the challenging\nsparse Mars-like terrain.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 17:51:03 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Ahuja", "Siddhant", ""], ["Iles", "Peter", ""], ["Waslander", "Steven L.", ""]]}, {"id": "1509.07225", "submitter": "Chen Sun", "authors": "Chen Sun and Chuang Gan and Ram Nevatia", "title": "Automatic Concept Discovery from Parallel Text and Visual Corpora", "comments": "To appear in ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans connect language and vision to perceive the world. How to build a\nsimilar connection for computers? One possible way is via visual concepts,\nwhich are text terms that relate to visually discriminative entities. We\npropose an automatic visual concept discovery algorithm using parallel text and\nvisual corpora; it filters text terms based on the visual discriminative power\nof the associated images, and groups them into concepts using visual and\nsemantic similarities. We illustrate the applications of the discovered\nconcepts using bidirectional image and sentence retrieval task and image\ntagging task, and show that the discovered concepts not only outperform several\nlarge sets of manually selected concepts significantly, but also achieves the\nstate-of-the-art performance in the retrieval task.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 03:32:51 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Sun", "Chen", ""], ["Gan", "Chuang", ""], ["Nevatia", "Ram", ""]]}, {"id": "1509.07244", "submitter": "Shawn Andrews", "authors": "Shawn Andrews and Ghassan Hamarneh", "title": "Multi-Region Probabilistic Dice Similarity Coefficient using the\n  Aitchison Distance and Bipartite Graph Matching", "comments": "8 pages. 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Validation of image segmentation methods is of critical importance.\nProbabilistic image segmentation is increasingly popular as it captures\nuncertainty in the results. Image segmentation methods that support\nmulti-region (as opposed to binary) delineation are more favourable as they\ncapture interactions between the different objects in the image. The Dice\nsimilarity coefficient (DSC) has been a popular metric for evaluating the\naccuracy of automated or semi-automated segmentation methods by comparing their\nresults to the ground truth. In this work, we develop an extension of the DSC\nto multi-region probabilistic segmentations (with unordered labels). We use\nbipartite graph matching to establish label correspondences and propose two\nfunctions that extend the DSC, one based on absolute probability differences\nand one based on the Aitchison distance. These provide a robust and accurate\nmeasure of multi-region probabilistic segmentation accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 05:56:38 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 06:25:17 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2015 04:11:25 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Andrews", "Shawn", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1509.07473", "submitter": "Andreas Veit", "authors": "Andreas Veit, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala,\n  Serge Belongie", "title": "Learning Visual Clothing Style with Heterogeneous Dyadic Co-occurrences", "comments": "ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid proliferation of smart mobile devices, users now take millions\nof photos every day. These include large numbers of clothing and accessory\nimages. We would like to answer questions like `What outfit goes well with this\npair of shoes?' To answer these types of questions, one has to go beyond\nlearning visual similarity and learn a visual notion of compatibility across\ncategories. In this paper, we propose a novel learning framework to help answer\nthese types of questions. The main idea of this framework is to learn a feature\ntransformation from images of items into a latent space that expresses\ncompatibility. For the feature transformation, we use a Siamese Convolutional\nNeural Network (CNN) architecture, where training examples are pairs of items\nthat are either compatible or incompatible. We model compatibility based on\nco-occurrence in large-scale user behavior data; in particular co-purchase data\nfrom Amazon.com. To learn cross-category fit, we introduce a strategic method\nto sample training data, where pairs of items are heterogeneous dyads, i.e.,\nthe two elements of a pair belong to different high-level categories. While\nthis approach is applicable to a wide variety of settings, we focus on the\nrepresentative problem of learning compatible clothing style. Our results\nindicate that the proposed framework is capable of learning semantic\ninformation about visual style and is able to generate outfits of clothes, with\nitems from different categories, that go well together.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 18:59:01 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Veit", "Andreas", ""], ["Kovacs", "Balazs", ""], ["Bell", "Sean", ""], ["McAuley", "Julian", ""], ["Bala", "Kavita", ""], ["Belongie", "Serge", ""]]}, {"id": "1509.07479", "submitter": "Michael Wilber", "authors": "Michael J. Wilber, Iljung S. Kwak, David Kriegman, Serge Belongie", "title": "Learning Concept Embeddings with Combined Human-Machine Expertise", "comments": "To appear at ICCV 2015. (This version has updated author affiliations\n  and updated footnotes.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our work on \"SNaCK,\" a low-dimensional concept embedding\nalgorithm that combines human expertise with automatic machine similarity\nkernels. Both parts are complimentary: human insight can capture relationships\nthat are not apparent from the object's visual similarity and the machine can\nhelp relieve the human from having to exhaustively specify many constraints. We\nshow that our SNaCK embeddings are useful in several tasks: distinguishing\nprime and nonprime numbers on MNIST, discovering labeling mistakes in the\nCaltech UCSD Birds (CUB) dataset with the help of deep-learned features,\ncreating training datasets for bird classifiers, capturing subjective human\ntaste on a new dataset of 10,000 foods, and qualitatively exploring an\nunstructured set of pictographic characters. Comparisons with the\nstate-of-the-art in these tasks show that SNaCK produces better concept\nembeddings that require less human supervision than the leading methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 19:05:09 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2015 17:19:05 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Wilber", "Michael J.", ""], ["Kwak", "Iljung S.", ""], ["Kriegman", "David", ""], ["Belongie", "Serge", ""]]}, {"id": "1509.07543", "submitter": "Andreas Veit", "authors": "Andreas Veit, Michael Wilber, Rajan Vaish, Serge Belongie, James\n  Davis, Vishal Anand, Anshu Aviral, Prithvijit Chakrabarty, Yash Chandak,\n  Sidharth Chaturvedi, Chinmaya Devaraj, Ankit Dhall, Utkarsh Dwivedi, Sanket\n  Gupte, Sharath N. Sridhar, Karthik Paga, Anuj Pahuja, Aditya Raisinghani,\n  Ayush Sharma, Shweta Sharma, Darpana Sinha, Nisarg Thakkar, K. Bala Vignesh,\n  Utkarsh Verma, Kanniganti Abhishek, Amod Agrawal, Arya Aishwarya, Aurgho\n  Bhattacharjee, Sarveshwaran Dhanasekar, Venkata Karthik Gullapalli, Shuchita\n  Gupta, Chandana G, Kinjal Jain, Simran Kapur, Meghana Kasula, Shashi Kumar,\n  Parth Kundaliya, Utkarsh Mathur, Alankrit Mishra, Aayush Mudgal, Aditya\n  Nadimpalli, Munakala Sree Nihit, Akanksha Periwal, Ayush Sagar, Ayush Shah,\n  Vikas Sharma, Yashovardhan Sharma, Faizal Siddiqui, Virender Singh, Abhinav\n  S., Anurag. D. Yadav", "title": "On Optimizing Human-Machine Task Assignments", "comments": "HCOMP 2015 Work in Progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When crowdsourcing systems are used in combination with machine inference\nsystems in the real world, they benefit the most when the machine system is\ndeeply integrated with the crowd workers. However, if researchers wish to\nintegrate the crowd with \"off-the-shelf\" machine classifiers, this deep\nintegration is not always possible. This work explores two strategies to\nincrease accuracy and decrease cost under this setting. First, we show that\nreordering tasks presented to the human can create a significant accuracy\nimprovement. Further, we show that greedily choosing parameters to maximize\nmachine accuracy is sub-optimal, and joint optimization of the combined system\nimproves performance.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 21:38:07 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Veit", "Andreas", ""], ["Wilber", "Michael", ""], ["Vaish", "Rajan", ""], ["Belongie", "Serge", ""], ["Davis", "James", ""], ["Anand", "Vishal", ""], ["Aviral", "Anshu", ""], ["Chakrabarty", "Prithvijit", ""], ["Chandak", "Yash", ""], ["Chaturvedi", "Sidharth", ""], ["Devaraj", "Chinmaya", ""], ["Dhall", "Ankit", ""], ["Dwivedi", "Utkarsh", ""], ["Gupte", "Sanket", ""], ["Sridhar", "Sharath N.", ""], ["Paga", "Karthik", ""], ["Pahuja", "Anuj", ""], ["Raisinghani", "Aditya", ""], ["Sharma", "Ayush", ""], ["Sharma", "Shweta", ""], ["Sinha", "Darpana", ""], ["Thakkar", "Nisarg", ""], ["Vignesh", "K. Bala", ""], ["Verma", "Utkarsh", ""], ["Abhishek", "Kanniganti", ""], ["Agrawal", "Amod", ""], ["Aishwarya", "Arya", ""], ["Bhattacharjee", "Aurgho", ""], ["Dhanasekar", "Sarveshwaran", ""], ["Gullapalli", "Venkata Karthik", ""], ["Gupta", "Shuchita", ""], ["G", "Chandana", ""], ["Jain", "Kinjal", ""], ["Kapur", "Simran", ""], ["Kasula", "Meghana", ""], ["Kumar", "Shashi", ""], ["Kundaliya", "Parth", ""], ["Mathur", "Utkarsh", ""], ["Mishra", "Alankrit", ""], ["Mudgal", "Aayush", ""], ["Nadimpalli", "Aditya", ""], ["Nihit", "Munakala Sree", ""], ["Periwal", "Akanksha", ""], ["Sagar", "Ayush", ""], ["Shah", "Ayush", ""], ["Sharma", "Vikas", ""], ["Sharma", "Yashovardhan", ""], ["Siddiqui", "Faizal", ""], ["Singh", "Virender", ""], ["S.", "Abhinav", ""], ["Yadav", "Anurag. D.", ""]]}, {"id": "1509.07611", "submitter": "Kanji Tanaka", "authors": "Kanji Tanaka", "title": "Incremental Loop Closure Verification by Guided Sampling", "comments": "Technical report, 7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loop closure detection, the task of identifying locations revisited by a\nrobot in a sequence of odometry and perceptual observations, is typically\nformulated as a combination of two subtasks: (1) bag-of-words image retrieval\nand (2) post-verification using RANSAC geometric verification. The main\ncontribution of this study is the proposal of a novel post-verification\nframework that achieves good precision recall trade-off in loop closure\ndetection. This study is motivated by the fact that not all loop closure\nhypotheses are equally plausible (e.g., owing to mutual consistency between\nloop closure constraints) and that if we have evidence that one hypothesis is\nmore plausible than the others, then it should be verified more frequently. We\ndemonstrate that the problem of loop closure detection can be viewed as an\ninstance of a multi-model hypothesize-and-verify framework and build guided\nsampling strategies on the framework where loop closures proposed using image\nretrieval are verified in a planned order (rather than in a conventional\nuniform order) to operate in a constant time. Experimental results using a\nstereo SLAM system confirm that the proposed strategy, the use of loop closure\nconstraints and robot trajectory hypotheses as a guide, achieves promising\nresults despite the fact that there exists a significant number of false\npositive constraints and hypotheses.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 07:49:50 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Tanaka", "Kanji", ""]]}, {"id": "1509.07615", "submitter": "Kanji Tanaka", "authors": "Enfu Liu, Kanji Tanaka", "title": "Discriminative Map Retrieval Using View-Dependent Map Descriptor", "comments": "Technical Report, 8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Map retrieval, the problem of similarity search over a large collection of 2D\npointset maps previously built by mobile robots, is crucial for autonomous\nnavigation in indoor and outdoor environments. Bag-of-words (BoW) methods\nconstitute a popular approach to map retrieval; however, these methods have\nextremely limited descriptive ability because they ignore the spatial layout\ninformation of the local features. The main contribution of this paper is an\nextension of the bag-of-words map retrieval method to enable the use of spatial\ninformation from local features. Our strategy is to explicitly model a unique\nviewpoint of an input local map; the pose of the local feature is defined with\nrespect to this unique viewpoint, and can be viewed as an additional invariant\nfeature for discriminative map retrieval. Specifically, we wish to determine a\nunique viewpoint that is invariant to moving objects, clutter, occlusions, and\nactual viewpoints. Hence, we perform scene parsing to analyze the scene\nstructure, and consider the \"center\" of the scene structure to be the unique\nviewpoint. Our scene parsing is based on a Manhattan world grammar that imposes\na quasi-Manhattan world constraint to enable the robust detection of a scene\nstructure that is invariant to clutter and moving objects. Experimental results\nusing the publicly available radish dataset validate the efficacy of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 08:02:19 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Liu", "Enfu", ""], ["Tanaka", "Kanji", ""]]}, {"id": "1509.07618", "submitter": "Kanji Tanaka", "authors": "Taisho Tsukamoto, Kanji Tanaka", "title": "Self-localization Using Visual Experience Across Domains", "comments": "Technical Report, 8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we aim to solve the single-view robot self-localization\nproblem by using visual experience across domains. Although the bag-of-words\nmethod constitutes a popular approach to single-view localization, it fails\nbadly when it's visual vocabulary is learned and tested in different domains.\nFurther, we are interested in using a cross-domain setting, in which the visual\nvocabulary is learned in different seasons and routes from the input\nquery/database scenes. Our strategy is to mine a cross-domain visual\nexperience, a library of raw visual images collected in different domains, to\ndiscover the relevant visual patterns that effectively explain the input scene,\nand use them for scene retrieval. In particular, we show that the appearance\nand the pose of the mined visual patterns of a query scene can be efficiently\nand discriminatively matched against those of the database scenes by employing\nimage-to-class distance and spatial pyramid matching. Experimental results\nobtained using a novel cross-domain dataset show that our system achieves\npromising results despite our visual vocabulary being learned and tested in\ndifferent domains.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 08:07:10 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Tsukamoto", "Taisho", ""], ["Tanaka", "Kanji", ""]]}, {"id": "1509.07627", "submitter": "Hirokatsu Kataoka", "authors": "Hirokatsu Kataoka, Kenji Iwata, Yutaka Satoh", "title": "Feature Evaluation of Deep Convolutional Neural Networks for Object\n  Recognition and Detection", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we evaluate convolutional neural network (CNN) features using\nthe AlexNet architecture and very deep convolutional network (VGGNet)\narchitecture. To date, most CNN researchers have employed the last layers\nbefore output, which were extracted from the fully connected feature layers.\nHowever, since it is unlikely that feature representation effectiveness is\ndependent on the problem, this study evaluates additional convolutional layers\nthat are adjacent to fully connected layers, in addition to executing simple\ntuning for feature concatenation (e.g., layer 3 + layer 5 + layer 7) and\ntransformation, using tools such as principal component analysis. In our\nexperiments, we carried out detection and classification tasks using the\nCaltech 101 and Daimler Pedestrian Benchmark Datasets.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 08:26:53 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Kataoka", "Hirokatsu", ""], ["Iwata", "Kenji", ""], ["Satoh", "Yutaka", ""]]}, {"id": "1509.07831", "submitter": "Jaeyong Sung", "authors": "Jaeyong Sung, Ian Lenz, Ashutosh Saxena", "title": "Deep Multimodal Embedding: Manipulating Novel Objects with Point-clouds,\n  Language and Trajectories", "comments": "IEEE International Conference on Robotics and Automation (ICRA), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robot operating in a real-world environment needs to perform reasoning over\na variety of sensor modalities such as vision, language and motion\ntrajectories. However, it is extremely challenging to manually design features\nrelating such disparate modalities. In this work, we introduce an algorithm\nthat learns to embed point-cloud, natural language, and manipulation trajectory\ndata into a shared embedding space with a deep neural network. To learn\nsemantically meaningful spaces throughout our network, we use a loss-based\nmargin to bring embeddings of relevant pairs closer together while driving\nless-relevant cases from different modalities further apart. We use this both\nto pre-train its lower layers and fine-tune our final embedding space, leading\nto a more robust representation. We test our algorithm on the task of\nmanipulating novel objects and appliances based on prior experience with other\nobjects. On a large dataset, we achieve significant improvements in both\naccuracy and inference time over the previous state of the art. We also perform\nend-to-end experiments on a PR2 robot utilizing our learned embedding space.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 18:55:45 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 15:12:33 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Sung", "Jaeyong", ""], ["Lenz", "Ian", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1509.07838", "submitter": "Catalin Ionescu", "authors": "Catalin Ionescu, Orestis Vantzos and Cristian Sminchisescu", "title": "Training Deep Networks with Structured Layers by Matrix Backpropagation", "comments": "This is an extended version of our ICCV 2015 article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network architectures have recently produced excellent results in\na variety of areas in artificial intelligence and visual recognition, well\nsurpassing traditional shallow architectures trained using hand-designed\nfeatures. The power of deep networks stems both from their ability to perform\nlocal computations followed by pointwise non-linearities over increasingly\nlarger receptive fields, and from the simplicity and scalability of the\ngradient-descent training procedure based on backpropagation. An open problem\nis the inclusion of layers that perform global, structured matrix computations\nlike segmentation (e.g. normalized cuts) or higher-order pooling (e.g.\nlog-tangent space metrics defined over the manifold of symmetric positive\ndefinite matrices) while preserving the validity and efficiency of an\nend-to-end deep training framework. In this paper we propose a sound\nmathematical apparatus to formally integrate global structured computation into\ndeep computation architectures. At the heart of our methodology is the\ndevelopment of the theory and practice of backpropagation that generalizes to\nthe calculus of adjoint matrix variations. The proposed matrix backpropagation\nmethodology applies broadly to a variety of problems in machine learning or\ncomputational perception. Here we illustrate it by performing visual\nsegmentation experiments using the BSDS and MSCOCO benchmarks, where we show\nthat deep networks relying on second-order pooling and normalized cuts layers,\ntrained end-to-end using matrix backpropagation, outperform counterparts that\ndo not take advantage of such global layers.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 19:14:27 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2015 21:02:43 GMT"}, {"version": "v3", "created": "Tue, 12 Apr 2016 18:39:01 GMT"}, {"version": "v4", "created": "Thu, 14 Apr 2016 11:30:39 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Ionescu", "Catalin", ""], ["Vantzos", "Orestis", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1509.07845", "submitter": "Xintong Han", "authors": "Bharat Singh, Xintong Han, Zhe Wu, Vlad I. Morariu and Larry S. Davis", "title": "Selecting Relevant Web Trained Concepts for Automated Event Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex event retrieval is a challenging research problem, especially when no\ntraining videos are available. An alternative to collecting training videos is\nto train a large semantic concept bank a priori. Given a text description of an\nevent, event retrieval is performed by selecting concepts linguistically\nrelated to the event description and fusing the concept responses on unseen\nvideos. However, defining an exhaustive concept lexicon and pre-training it\nrequires vast computational resources. Therefore, recent approaches automate\nconcept discovery and training by leveraging large amounts of weakly annotated\nweb data. Compact visually salient concepts are automatically obtained by the\nuse of concept pairs or, more generally, n-grams. However, not all visually\nsalient n-grams are necessarily useful for an event query--some combinations of\nconcepts may be visually compact but irrelevant--and this drastically affects\nperformance. We propose an event retrieval algorithm that constructs pairs of\nautomatically discovered concepts and then prunes those concepts that are\nunlikely to be helpful for retrieval. Pruning depends both on the query and on\nthe specific video instance being evaluated. Our approach also addresses\ncalibration and domain adaptation issues that arise when applying concept\ndetectors to unseen videos. We demonstrate large improvements over other vision\nbased systems on the TRECVID MED 13 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 19:27:54 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Singh", "Bharat", ""], ["Han", "Xintong", ""], ["Wu", "Zhe", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1509.07975", "submitter": "Yogesh Girdhar Yogesh Girdhar", "authors": "Yogesh Girdhar, Gregory Dudek", "title": "Modeling Curiosity in a Mobile Robot for Long-Term Autonomous\n  Exploration and Monitoring", "comments": "20 pages, in-press, Autonomous Robots, 2015. arXiv admin note:\n  substantial text overlap with arXiv:1310.6767", "journal-ref": null, "doi": "10.1007/s10514-015-9500-x", "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to modeling curiosity in a mobile robot,\nwhich is useful for monitoring and adaptive data collection tasks, especially\nin the context of long term autonomous missions where pre-programmed missions\nare likely to have limited utility. We use a realtime topic modeling technique\nto build a semantic perception model of the environment, using which, we plan a\npath through the locations in the world with high semantic information content.\nThe life-long learning behavior of the proposed perception model makes it\nsuitable for long-term exploration missions. We validate the approach using\nsimulated exploration experiments using aerial and underwater data, and\ndemonstrate an implementation on the Aqua underwater robot in a variety of\nscenarios. We find that the proposed exploration paths that are biased towards\nlocations with high topic perplexity, produce better terrain models with high\ndiscriminative power. Moreover, we show that the proposed algorithm implemented\non Aqua robot is able to do tasks such as coral reef inspection, diver\nfollowing, and sea floor exploration, without any prior training or\npreparation.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 13:16:52 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Girdhar", "Yogesh", ""], ["Dudek", "Gregory", ""]]}, {"id": "1509.07979", "submitter": "Yogesh Girdhar", "authors": "Yogesh Girdhar, Walter Cho, Matthew Campbell, Jesus Pineda, Elizabeth\n  Clarke, Hanumant Singh", "title": "Anomaly Detection in Unstructured Environments using Bayesian\n  Nonparametric Scene Modeling", "comments": "6 pages, ICRA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of a Bayesian non-parametric topic modeling\ntechnique for the purpose of anomaly detection in video data. We present\nresults from two experiments. The first experiment shows that the proposed\ntechnique is automatically able characterize the underlying terrain, and detect\nanomalous flora in image data collected by an underwater robot. The second\nexperiment shows that the same technique can be used on images from a static\ncamera in a dynamic unstructured environment. In the second dataset, consisting\nof video data from a static seafloor camera capturing images of a busy coral\nreef, the proposed technique was able to detect all three instances of an\nunderwater vehicle passing in front of the camera, amongst many other\nobservations of fishes, debris, lighting changes due to surface waves, and\nbenthic flora.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 13:51:39 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 02:45:52 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Girdhar", "Yogesh", ""], ["Cho", "Walter", ""], ["Campbell", "Matthew", ""], ["Pineda", "Jesus", ""], ["Clarke", "Elizabeth", ""], ["Singh", "Hanumant", ""]]}, {"id": "1509.08038", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Jun Miao, Laiyun Qing, Xilin Chen", "title": "Deep Trans-layer Unsupervised Networks for Representation Learning", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning features from massive unlabelled data is a vast prevalent topic for\nhigh-level tasks in many machine learning applications. The recent great\nimprovements on benchmark data sets achieved by increasingly complex\nunsupervised learning methods and deep learning models with lots of parameters\nusually requires many tedious tricks and much expertise to tune. However,\nfilters learned by these complex architectures are quite similar to standard\nhand-crafted features visually. In this paper, unsupervised learning methods,\nsuch as PCA or auto-encoder, are employed as the building block to learn filter\nbanks at each layer. The lower layer responses are transferred to the last\nlayer (trans-layer) to form a more complete representation retaining more\ninformation. In addition, some beneficial methods such as local contrast\nnormalization and whitening are added to the proposed deep trans-layer networks\nto further boost performance. The trans-layer representations are followed by\nblock histograms with binary encoder schema to learn translation and rotation\ninvariant representations, which are utilized to do high-level tasks such as\nrecognition and classification. Compared to traditional deep learning methods,\nthe implemented feature learning method has much less parameters and is\nvalidated in several typical experiments, such as digit recognition on MNIST\nand MNIST variations, object recognition on Caltech 101 dataset and face\nverification on LFW dataset. The deep trans-layer unsupervised learning\nachieves 99.45% accuracy on MNIST dataset, 67.11% accuracy on 15 samples per\nclass and 75.98% accuracy on 30 samples per class on Caltech 101 dataset,\n87.10% on LFW dataset.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 00:46:08 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Zhu", "Wentao", ""], ["Miao", "Jun", ""], ["Qing", "Laiyun", ""], ["Chen", "Xilin", ""]]}, {"id": "1509.08067", "submitter": "Tianfu Wu", "authors": "Tianfu Wu and Yang Lu and Song-Chun Zhu", "title": "Online Object Tracking, Learning and Parsing with And-Or Graphs", "comments": "17 pages, Reproducibility: The source code is released with this\n  paper for reproducing all results, which is available at\n  https://github.com/tfwu/RGM-AOGTracker", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method, called AOGTracker, for simultaneously tracking,\nlearning and parsing (TLP) of unknown objects in video sequences with a\nhierarchical and compositional And-Or graph (AOG) representation. %The AOG\ncaptures both structural and appearance variations of a target object in a\nprincipled way. The TLP method is formulated in the Bayesian framework with a\nspatial and a temporal dynamic programming (DP) algorithms inferring object\nbounding boxes on-the-fly. During online learning, the AOG is discriminatively\nlearned using latent SVM to account for appearance (e.g., lighting and partial\nocclusion) and structural (e.g., different poses and viewpoints) variations of\na tracked object, as well as distractors (e.g., similar objects) in background.\nThree key issues in online inference and learning are addressed: (i)\nmaintaining purity of positive and negative examples collected online, (ii)\ncontroling model complexity in latent structure learning, and (iii) identifying\ncritical moments to re-learn the structure of AOG based on its intrackability.\nThe intrackability measures uncertainty of an AOG based on its score maps in a\nframe. In experiments, our AOGTracker is tested on two popular tracking\nbenchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks,\nand the VOT benchmarks --- VOT 2013, 2014, 2015 and TIR2015 (thermal imagery\ntracking). In the former, our AOGTracker outperforms state-of-the-art tracking\nalgorithms including two trackers based on deep convolutional network. In the\nlatter, our AOGTracker outperforms all other trackers in VOT2013 and is\ncomparable to the state-of-the-art methods in VOT2014, 2015 and TIR2015.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 08:14:57 GMT"}, {"version": "v2", "created": "Fri, 6 May 2016 01:10:42 GMT"}, {"version": "v3", "created": "Wed, 11 May 2016 19:35:37 GMT"}, {"version": "v4", "created": "Sun, 15 May 2016 06:02:48 GMT"}, {"version": "v5", "created": "Wed, 18 May 2016 05:44:50 GMT"}, {"version": "v6", "created": "Sat, 3 Sep 2016 00:26:58 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Wu", "Tianfu", ""], ["Lu", "Yang", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1509.08075", "submitter": "Hamid Izadinia", "authors": "Hamid Izadinia, Fereshteh Sadeghi, Santosh Kumar Divvala, Yejin Choi,\n  Ali Farhadi", "title": "Segment-Phrase Table for Semantic Segmentation, Visual Entailment and\n  Paraphrasing", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Segment-Phrase Table (SPT), a large collection of bijective\nassociations between textual phrases and their corresponding segmentations.\nLeveraging recent progress in object recognition and natural language\nsemantics, we show how we can successfully build a high-quality segment-phrase\ntable using minimal human supervision. More importantly, we demonstrate the\nunique value unleashed by this rich bimodal resource, for both vision as well\nas natural language understanding. First, we show that fine-grained textual\nlabels facilitate contextual reasoning that helps in satisfying semantic\nconstraints across image segments. This feature enables us to achieve\nstate-of-the-art segmentation results on benchmark datasets. Next, we show that\nthe association of high-quality segmentations to textual phrases aids in richer\nsemantic understanding and reasoning of these textual phrases. Leveraging this\nfeature, we motivate the problem of visual entailment and visual paraphrasing,\nand demonstrate its utility on a large dataset.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 10:01:42 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Izadinia", "Hamid", ""], ["Sadeghi", "Fereshteh", ""], ["Divvala", "Santosh Kumar", ""], ["Choi", "Yejin", ""], ["Farhadi", "Ali", ""]]}, {"id": "1509.08082", "submitter": "Martin Welk", "authors": "Martin Welk", "title": "Multivariate Median Filters and Partial Differential Equations", "comments": "v2: Minor revision; a few equations, some text, and one reference\n  added; typos corrected", "journal-ref": "Journal of Mathematical Imaging and Vision, 56 (2016) 320-351", "doi": "10.1007/s10851-016-0645-9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate median filters have been proposed as generalisations of the\nwell-established median filter for grey-value images to multi-channel images.\nAs multivariate median, most of the recent approaches use the $L^1$ median,\ni.e.\\ the minimiser of an objective function that is the sum of distances to\nall input points. Many properties of univariate median filters generalise to\nsuch a filter. However, the famous result by Guichard and Morel about\napproximation of the mean curvature motion PDE by median filtering does not\nhave a comparably simple counterpart for $L^1$ multivariate median filtering.\nWe discuss the affine equivariant Oja median and the affine equivariant\ntransformation--retransformation $L^1$ median as alternatives to $L^1$ median\nfiltering. We analyse multivariate median filters in a space-continuous\nsetting, including the formulation of a space-continuous version of the\ntransformation--retransformation $L^1$ median, and derive PDEs approximated by\nthese filters in the cases of bivariate planar images, three-channel volume\nimages and three-channel planar images. The PDEs for the affine equivariant\nfilters can be interpreted geometrically as combinations of a diffusion and a\nprincipal-component-wise curvature motion contribution with a cross-effect term\nbased on torsions of principal components. Numerical experiments are presented\nthat demonstrate the validity of the approximation results.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 11:02:31 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 15:11:37 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Welk", "Martin", ""]]}, {"id": "1509.08147", "submitter": "Abhishek Kar", "authors": "Abhishek Kar, Shubham Tulsiani, Jo\\~ao Carreira, Jitendra Malik", "title": "Amodal Completion and Size Constancy in Natural Scenes", "comments": "Accepted to ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of enriching current object detection systems with\nveridical object sizes and relative depth estimates from a single image. There\nare several technical challenges to this, such as occlusions, lack of\ncalibration data and the scale ambiguity between object size and distance.\nThese have not been addressed in full generality in previous work. Here we\npropose to tackle these issues by building upon advances in object recognition\nand using recently created large-scale datasets. We first introduce the task of\namodal bounding box completion, which aims to infer the the full extent of the\nobject instances in the image. We then propose a probabilistic framework for\nlearning category-specific object size distributions from available annotations\nand leverage these in conjunction with amodal completion to infer veridical\nsizes in novel images. Finally, we introduce a focal length prediction approach\nthat exploits scene recognition to overcome inherent scaling ambiguities and we\ndemonstrate qualitative results on challenging real-world scenes.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 21:39:42 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 04:34:06 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Kar", "Abhishek", ""], ["Tulsiani", "Shubham", ""], ["Carreira", "Jo\u00e3o", ""], ["Malik", "Jitendra", ""]]}, {"id": "1509.08182", "submitter": "Bin Liu", "authors": "Yi Dai, Bin Liu", "title": "Robust video object tracking using particle filter with likelihood based\n  feature fusion and adaptive template updating", "comments": "5 pages, 5 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust algorithm solution is proposed for tracking an object in complex\nvideo scenes. In this solution, the bootstrap particle filter (PF) is\ninitialized by an object detector, which models the time-evolving background of\nthe video signal by an adaptive Gaussian mixture. The motion of the object is\nexpressed by a Markov model, which defines the state transition prior. The\ncolor and texture features are used to represent the object, and a marginal\nlikelihood based feature fusion approach is proposed. A corresponding object\ntemplate model updating procedure is developed to account for possible scale\nchanges of the object in the tracking process. Experimental results show that\nour algorithm beats several existing alternatives in tackling challenging\nscenarios in video tracking tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 03:21:58 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Dai", "Yi", ""], ["Liu", "Bin", ""]]}, {"id": "1509.08197", "submitter": "Xuan Luo", "authors": "Xuan Luo, Xuejiao Bai, Shuo Li, Hongtao Lu, Sei-ichiro Kamata", "title": "Fast Non-local Stereo Matching based on Hierarchical Disparity\n  Prediction", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo matching is the key step in estimating depth from two or more images.\nRecently, some tree-based non-local stereo matching methods have been proposed,\nwhich achieved state-of-the-art performance. The algorithms employed some tree\nstructures to aggregate cost and thus improved the performance and reduced the\ncoputation load of the stereo matching. However, the computational complexity\nof these tree-based algorithms is still high because they search over the\nentire disparity range. In addition, the extreme greediness of the minimum\nspanning tree (MST) causes the poor performance in large areas with similar\ncolors but varying disparities. In this paper, we propose an efficient stereo\nmatching method using a hierarchical disparity prediction (HDP) framework to\ndramatically reduce the disparity search range so as to speed up the tree-based\nnon-local stereo methods. Our disparity prediction scheme works on a graph\npyramid derived from an image whose disparity to be estimated. We utilize the\ndisparity of a upper graph to predict a small disparity range for the lower\ngraph. Some independent disparity trees (DT) are generated to form a disparity\nprediction forest (HDPF) over which the cost aggregation is made. When combined\nwith the state-of-the-art tree-based methods, our scheme not only dramatically\nspeeds up the original methods but also improves their performance by\nalleviating the second drawback of the tree-based methods. This is partially\nbecause our DTs overcome the extreme greediness of the MST. Extensive\nexperimental results on some benchmark datasets demonstrate the effectiveness\nand efficiency of our framework. For example, the segment-tree based stereo\nmatching becomes about 25.57 times faster and 2.2% more accurate over the\nMiddlebury 2006 full-size dataset.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 05:00:01 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Luo", "Xuan", ""], ["Bai", "Xuejiao", ""], ["Li", "Shuo", ""], ["Lu", "Hongtao", ""], ["Kamata", "Sei-ichiro", ""]]}, {"id": "1509.08329", "submitter": "Alberto N. Escalante-B.", "authors": "Alberto N. Escalante-B., Laurenz Wiskott", "title": "Theoretical Analysis of the Optimal Free Responses of Graph-Based SFA\n  for the Design of Training Graphs", "comments": "29 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow feature analysis (SFA) is an unsupervised learning algorithm that\nextracts slowly varying features from a time series. Graph-based SFA (GSFA) is\na supervised extension that can solve regression problems if followed by a\npost-processing regression algorithm. A training graph specifies arbitrary\nconnections between the training samples. The connections in current graphs,\nhowever, only depend on the rank of the involved labels. Exploiting the exact\nlabel values makes further improvements in estimation accuracy possible.\n  In this article, we propose the exact label learning (ELL) method to create a\ngraph that codes the desired label explicitly, so that GSFA is able to extract\na normalized version of it directly. The ELL method is used for three tasks:\n(1) We estimate gender from artificial images of human faces (regression) and\nshow the advantage of coding additional labels, particularly skin color. (2) We\nanalyze two existing graphs for regression. (3) We extract compact\ndiscriminative features to classify traffic sign images. When the number of\noutput features is limited, a higher classification rate is obtained compared\nto a graph equivalent to nonlinear Fisher discriminant analysis. The method is\nversatile, directly supports multiple labels, and provides higher accuracy\ncompared to current graphs for the problems considered.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 14:19:59 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Escalante-B.", "Alberto N.", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "1509.08379", "submitter": "Yang Lu", "authors": "Yang Lu, Song-Chun Zhu, Ying Nian Wu", "title": "Learning FRAME Models Using CNN Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional neural network (ConvNet or CNN) has proven to be very\nsuccessful in many tasks such as those in computer vision. In this conceptual\npaper, we study the generative perspective of the discriminative CNN. In\nparticular, we propose to learn the generative FRAME (Filters, Random field,\nAnd Maximum Entropy) model using the highly expressive filters pre-learned by\nthe CNN at the convolutional layers. We show that the learning algorithm can\ngenerate realistic and rich object and texture patterns in natural scenes. We\nexplain that each learned model corresponds to a new CNN unit at a layer above\nthe layer of filters employed by the model. We further show that it is possible\nto learn a new layer of CNN units using a generative CNN model, which is a\nproduct of experts model, and the learning algorithm admits an EM\ninterpretation with binary latent variables.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 16:17:09 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 03:50:54 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2015 21:40:57 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Lu", "Yang", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1509.08383", "submitter": "Ang Li", "authors": "Ang Li, Feng Tang, Yanwen Guo and Hai Tao", "title": "Efficient Discriminative Nonorthogonal Binary Subspace with its\n  Application to Visual Tracking", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the crucial problems in visual tracking is how the object is\nrepresented. Conventional appearance-based trackers are using increasingly more\ncomplex features in order to be robust. However, complex representations\ntypically not only require more computation for feature extraction, but also\nmake the state inference complicated. We show that with a careful feature\nselection scheme, extremely simple yet discriminative features can be used for\nrobust object tracking. The central component of the proposed method is a\nsuccinct and discriminative representation of the object using discriminative\nnon-orthogonal binary subspace (DNBS) which is spanned by Haar-like features.\nThe DNBS representation inherits the merits of the original NBS in that it\nefficiently describes the object. It also incorporates the discriminative\ninformation to distinguish foreground from background. However, the problem of\nfinding the DNBS bases from an over-complete dictionary is NP-hard. We propose\na greedy algorithm called discriminative optimized orthogonal matching pursuit\n(D-OOMP) to solve this problem. An iterative formulation named iterative D-OOMP\nis further developed to drastically reduce the redundant computation between\niterations and a hierarchical selection strategy is integrated for reducing the\nsearch space of features. The proposed DNBS representation is applied to object\ntracking through SSD-based template matching. We validate the effectiveness of\nour method through extensive experiments on challenging videos with comparisons\nagainst several state-of-the-art trackers and demonstrate its capability to\ntrack objects in clutter and moving background.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 16:27:26 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Li", "Ang", ""], ["Tang", "Feng", ""], ["Guo", "Yanwen", ""], ["Tao", "Hai", ""]]}, {"id": "1509.08439", "submitter": "Sanath Narayan", "authors": "Sanath Narayan, Kalpathi R. Ramakrishnan", "title": "Hyper-Fisher Vectors for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel encoding scheme combining Fisher vector and\nbag-of-words encodings has been proposed for recognizing action in videos. The\nproposed Hyper-Fisher vector encoding is sum of local Fisher vectors which are\ncomputed based on the traditional Bag-of-Words (BoW) encoding. Thus, the\nproposed encoding is simple and yet an effective representation over the\ntraditional Fisher Vector encoding. By extensive evaluation on challenging\naction recognition datasets, viz., Youtube, Olympic Sports, UCF50 and HMDB51,\nwe show that the proposed Hyper-Fisher Vector encoding improves the recognition\nperformance by around 2-3% compared to the improved Fisher Vector encoding. We\nalso perform experiments to show that the performance of the Hyper-Fisher\nVector is robust to the dictionary size of the BoW encoding.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 19:25:34 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Narayan", "Sanath", ""], ["Ramakrishnan", "Kalpathi R.", ""]]}, {"id": "1509.08647", "submitter": "Eduardo M. Pereira", "authors": "Eduardo M. Pereira, Jaime S. Cardoso, Ricardo Morla", "title": "Long-Range Trajectories from Global and Local Motion Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion is a fundamental cue for scene analysis and human activity understan-\nding in videos. It can be encoded in trajectories for tracking objects and for\naction recognition, or in form of flow to address behaviour analysis in crowded\nscenes. Each approach can only be applied on limited scenarios. We propose a\nmotion-based system that represents the spatial and temporal features of the\nflow in terms of long-range trajectories. The novelty resides on the system\nformulation, its generic approach to handle scene variability and motion\nvariations, motion integration from local and global representations, and the\nresulting long-range trajectories that overcome trajectory-based approach\nproblems. We report the results and conclusions that state its pertinence on\ndifferent scenarios, comparing and correlating the extracted trajectories of\nindividual pedestrians, manually annotated. We also propose an evaluation\nframework and stress the diverse system characteristics that can be used for\nhuman activity tasks, namely on motion segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 09:02:57 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Pereira", "Eduardo M.", ""], ["Cardoso", "Jaime S.", ""], ["Morla", "Ricardo", ""]]}, {"id": "1509.08715", "submitter": "Amelia Carolina Sparavigna", "authors": "Roberto Marazzato, Amelia Carolina Sparavigna", "title": "Retinex filtering of foggy images: generation of a bulk set with\n  selection and ranking", "comments": "Keywords: GIMP Retinex, GIMP, Image processing, Bulk generation of\n  images, Bulk manipulation of images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are proposing the use of GIMP Retinex, a filter of the GNU\nImage Manipulation Program, for enhancing foggy images. This filter involves\nadjusting four different parameters to find the output image which has to be\npreferred according to some specific purposes. Aiming to obtain a processing,\nwhich is able of choosing automatically the best image from a given set, we are\nproposing a method for the generation a bulk set of GIMP Retinex filtered\nimages and a preliminary approach for selecting and ranking them.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 12:26:08 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Marazzato", "Roberto", ""], ["Sparavigna", "Amelia Carolina", ""]]}, {"id": "1509.08745", "submitter": "Vincent Gripon", "authors": "Guillaume Souli\\'e, Vincent Gripon, Ma\\\"elys Robert", "title": "Compression of Deep Neural Networks on the Fly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to their state-of-the-art performance, deep neural networks are\nincreasingly used for object recognition. To achieve these results, they use\nmillions of parameters to be trained. However, when targeting embedded\napplications the size of these models becomes problematic. As a consequence,\ntheir usage on smartphones or other resource limited devices is prohibited. In\nthis paper we introduce a novel compression method for deep neural networks\nthat is performed during the learning phase. It consists in adding an extra\nregularization term to the cost function of fully-connected layers. We combine\nthis method with Product Quantization (PQ) of the trained weights for higher\nsavings in storage consumption. We evaluate our method on two data sets (MNIST\nand CIFAR10), on which we achieve significantly larger compression rates than\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 13:32:30 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2015 10:22:13 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2015 13:08:50 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2016 12:58:32 GMT"}, {"version": "v5", "created": "Fri, 18 Mar 2016 09:33:01 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Souli\u00e9", "Guillaume", ""], ["Gripon", "Vincent", ""], ["Robert", "Ma\u00eblys", ""]]}, {"id": "1509.08902", "submitter": "Gaurav Sharma", "authors": "Gaurav Sharma and Bernt Schiele", "title": "Scalable Nonlinear Embeddings for Semantic Category-based Image\n  Retrieval", "comments": "ICCV 2015 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for the task of supervised discriminative\ndistance learning by nonlinearly embedding vectors into a low dimensional\nEuclidean space. We work in the challenging setting where supervision is with\nconstraints on similar and dissimilar pairs while training. The proposed method\nis derived by an approximate kernelization of a linear Mahalanobis-like\ndistance metric learning algorithm and can also be seen as a kernel neural\nnetwork. The number of model parameters and test time evaluation complexity of\nthe proposed method are O(dD) where D is the dimensionality of the input\nfeatures and d is the dimension of the projection space - this is in contrast\nto the usual kernelization methods as, unlike them, the complexity does not\nscale linearly with the number of training examples. We propose a stochastic\ngradient based learning algorithm which makes the method scalable (w.r.t. the\nnumber of training examples), while being nonlinear. We train the method with\nup to half a million training pairs of 4096 dimensional CNN features. We give\nempirical comparisons with relevant baselines on seven challenging datasets for\nthe task of low dimensional semantic category based image retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 19:41:33 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Sharma", "Gaurav", ""], ["Schiele", "Bernt", ""]]}, {"id": "1509.08969", "submitter": "Suren Vagharshakyan", "authors": "Suren Vagharshakyan, Robert Bregovic and Atanas Gotchev", "title": "Light Field Reconstruction Using Shearlet Transform", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we develop an image based rendering technique based on light\nfield reconstruction from a limited set of perspective views acquired by\ncameras. Our approach utilizes sparse representation of epipolar-plane images\nin a directionally sensitive transform domain, obtained by an adapted discrete\nshearlet transform. The used iterative thresholding algorithm provides\nhigh-quality reconstruction results for relatively big disparities between\nneighboring views. The generated densely sampled light field of a given 3D\nscene is thus suitable for all applications which requires light field\nreconstruction. The proposed algorithm is compared favorably against state of\nthe art depth image based rendering techniques.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 22:37:21 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Vagharshakyan", "Suren", ""], ["Bregovic", "Robert", ""], ["Gotchev", "Atanas", ""]]}, {"id": "1509.08970", "submitter": "Priyadarshini Panda", "authors": "Priyadarshini Panda, Swagath Venkataramani, Abhronil Sengupta, Anand\n  Raghunathan and Kaushik Roy", "title": "Energy-Efficient Object Detection using Semantic Decomposition", "comments": "10 pages, 13 figures, 3 algorithms, Submitted to IEEE TVLSI(Under\n  Review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-learning algorithms offer immense possibilities in the development of\nseveral cognitive applications. In fact, large scale machine-learning\nclassifiers now represent the state-of-the-art in a wide range of object\ndetection/classification problems. However, the network complexities of\nlarge-scale classifiers present them as one of the most challenging and energy\nintensive workloads across the computing spectrum. In this paper, we present a\nnew approach to optimize energy efficiency of object detection tasks using\nsemantic decomposition to build a hierarchical classification framework. We\nobserve that certain semantic information like color/texture are common across\nvarious images in real-world datasets for object detection applications. We\nexploit these common semantic features to distinguish the objects of interest\nfrom the remaining inputs (non-objects of interest) in a dataset at a lower\ncomputational effort. We propose a 2-stage hierarchical classification\nframework, with increasing levels of complexity, wherein the first stage is\ntrained to recognize the broad representative semantic features relevant to the\nobject of interest. The first stage rejects the input instances that do not\nhave the representative features and passes only the relevant instances to the\nsecond stage. Our methodology thus allows us to reject certain information at\nlower complexity and utilize the full computational effort of a network only on\na smaller fraction of inputs to perform detection. We use color and texture as\ndistinctive traits to carry out several experiments for object detection. Our\nexperiments on the Caltech101/CIFAR10 dataset show that the proposed method\nyields 1.93x/1.46x improvement in average energy, respectively, over the\ntraditional single classifier model.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 22:56:33 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 23:21:51 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 14:38:32 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Panda", "Priyadarshini", ""], ["Venkataramani", "Swagath", ""], ["Sengupta", "Abhronil", ""], ["Raghunathan", "Anand", ""], ["Roy", "Kaushik", ""]]}, {"id": "1509.08971", "submitter": "Priyadarshini Panda", "authors": "Priyadarshini Panda, Abhronil Sengupta and Kaushik Roy", "title": "Conditional Deep Learning for Energy-Efficient and Enhanced Pattern\n  Recognition", "comments": "6 pages, 10 figures, 2 algorithms < Accepted for Design and\n  Automation Test in Europe (DATE) conference, 2016>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning neural networks have emerged as one of the most powerful\nclassification tools for vision related applications. However, the\ncomputational and energy requirements associated with such deep nets can be\nquite high, and hence their energy-efficient implementation is of great\ninterest. Although traditionally the entire network is utilized for the\nrecognition of all inputs, we observe that the classification difficulty varies\nwidely across inputs in real-world datasets; only a small fraction of inputs\nrequire the full computational effort of a network, while a large majority can\nbe classified correctly with very low effort. In this paper, we propose\nConditional Deep Learning (CDL) where the convolutional layer features are used\nto identify the variability in the difficulty of input instances and\nconditionally activate the deeper layers of the network. We achieve this by\ncascading a linear network of output neurons for each convolutional layer and\nmonitoring the output of the linear network to decide whether classification\ncan be terminated at the current stage or not. The proposed methodology thus\nenables the network to dynamically adjust the computational effort depending\nupon the difficulty of the input data while maintaining competitive\nclassification accuracy. We evaluate our approach on the MNIST dataset. Our\nexperiments demonstrate that our proposed CDL yields 1.91x reduction in average\nnumber of operations per input, which translates to 1.84x improvement in\nenergy. In addition, our results show an improvement in classification accuracy\nfrom 97.5% to 98.9% as compared to the original network.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 23:08:09 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 13:56:35 GMT"}, {"version": "v3", "created": "Sat, 3 Oct 2015 12:23:00 GMT"}, {"version": "v4", "created": "Tue, 6 Oct 2015 01:45:50 GMT"}, {"version": "v5", "created": "Tue, 24 Nov 2015 17:04:59 GMT"}, {"version": "v6", "created": "Thu, 28 Jan 2016 18:34:42 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Panda", "Priyadarshini", ""], ["Sengupta", "Abhronil", ""], ["Roy", "Kaushik", ""]]}, {"id": "1509.08973", "submitter": "Tadahiro Taniguchi", "authors": "Tadahiro Taniguchi, Takayuki Nagai, Tomoaki Nakamura, Naoto Iwahashi,\n  Tetsuya Ogata, and Hideki Asoh", "title": "Symbol Emergence in Robotics: A Survey", "comments": "submitted to Advanced Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can learn the use of language through physical interaction with their\nenvironment and semiotic communication with other people. It is very important\nto obtain a computational understanding of how humans can form a symbol system\nand obtain semiotic skills through their autonomous mental development.\nRecently, many studies have been conducted on the construction of robotic\nsystems and machine-learning methods that can learn the use of language through\nembodied multimodal interaction with their environment and other systems.\nUnderstanding human social interactions and developing a robot that can\nsmoothly communicate with human users in the long term, requires an\nunderstanding of the dynamics of symbol systems and is crucially important. The\nembodied cognition and social interaction of participants gradually change a\nsymbol system in a constructive manner. In this paper, we introduce a field of\nresearch called symbol emergence in robotics (SER). SER is a constructive\napproach towards an emergent symbol system. The emergent symbol system is\nsocially self-organized through both semiotic communications and physical\ninteractions with autonomous cognitive developmental agents, i.e., humans and\ndevelopmental robots. Specifically, we describe some state-of-art research\ntopics concerning SER, e.g., multimodal categorization, word discovery, and a\ndouble articulation analysis, that enable a robot to obtain words and their\nembodied meanings from raw sensory--motor information, including visual\ninformation, haptic information, auditory information, and acoustic speech\nsignals, in a totally unsupervised manner. Finally, we suggest future\ndirections of research in SER.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 23:16:48 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Taniguchi", "Tadahiro", ""], ["Nagai", "Takayuki", ""], ["Nakamura", "Tomoaki", ""], ["Iwahashi", "Naoto", ""], ["Ogata", "Tetsuya", ""], ["Asoh", "Hideki", ""]]}, {"id": "1509.09014", "submitter": "Rofael Emil", "authors": "Rofael Emil Fayez Behnam", "title": "Stats-Calculus Pose Descriptor Feeding A Discrete HMM Low-latency\n  Detection and Recognition System For 3D Skeletal Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of human actions, under low observational latency, is a growing\ninterest topic, nowadays. Many approaches have been represented based on a\nprovided set of 3D Cartesian coordinates system originated at a certain\nspecific point located on a root joint. In this paper, We will present a\nstatistical detection and recognition system using Hidden Markov Model using 7\ntypes of pose descriptors. * Cartesian Calculus Pose descriptor. * Angular\nCalculus Pose descriptor. * Mixed-mode Stats-Calculus Pose descriptor. *\nCentro-Stats-Calculus Pose descriptor. * Rela-Centro-Stats-Calculus Pose\ndescriptor. * Rela-Centro-Stats-Calculus DCT Pose descriptor. *\nRela-Centro-Stats-Calculus DCT-AMDF Pose descriptor. Stats-Calculus is a\nfeature extracting technique, that is developed on Moving Pose descriptor , but\nusing a combination of Statistics measures and Calculus measures.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 05:02:52 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 04:01:50 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2015 17:27:58 GMT"}, {"version": "v4", "created": "Fri, 16 Oct 2015 19:53:00 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Behnam", "Rofael Emil Fayez", ""]]}, {"id": "1509.09089", "submitter": "Yanwei Pang", "authors": "Yanwei Pang, Li Ye, Xuelong Li, and Jing Pan", "title": "Moving Object Detection in Video Using Saliency Map and Subspace\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving object detection is a key to intelligent video analysis. On the one\nhand, what moves is not only interesting objects but also noise and cluttered\nbackground. On the other hand, moving objects without rich texture are prone\nnot to be detected. So there are undesirable false alarms and missed alarms in\nmany algorithms of moving object detection. To reduce the false alarms and\nmissed alarms, in this paper, we propose to incorporate a saliency map into an\nincremental subspace analysis framework where the saliency map makes estimated\nbackground has less chance than foreground (i.e., moving objects) to contain\nsalient objects. The proposed objective function systematically takes account\ninto the properties of sparsity, low-rank, connectivity, and saliency. An\nalternative minimization algorithm is proposed to seek the optimal solutions.\nExperimental results on the Perception Test Images Sequences demonstrate that\nthe proposed method is effective in reducing false alarms and missed alarms.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 09:13:20 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Pang", "Yanwei", ""], ["Ye", "Li", ""], ["Li", "Xuelong", ""], ["Pan", "Jing", ""]]}, {"id": "1509.09114", "submitter": "Karteek Alahari", "authors": "Yang Hua, Karteek Alahari, Cordelia Schmid", "title": "Online Object Tracking with Proposal Selection", "comments": "ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking-by-detection approaches are some of the most successful object\ntrackers in recent years. Their success is largely determined by the detector\nmodel they learn initially and then update over time. However, under\nchallenging conditions where an object can undergo transformations, e.g.,\nsevere rotation, these methods are found to be lacking. In this paper, we\naddress this problem by formulating it as a proposal selection task and making\ntwo contributions. The first one is introducing novel proposals estimated from\nthe geometric transformations undergone by the object, and building a rich\ncandidate set for predicting the object location. The second one is devising a\nnovel selection strategy using multiple cues, i.e., detection score and\nedgeness score computed from state-of-the-art object edges and motion\nboundaries. We extensively evaluate our approach on the visual object tracking\n2014 challenge and online tracking benchmark datasets, and show the best\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 10:38:27 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Hua", "Yang", ""], ["Alahari", "Karteek", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1509.09243", "submitter": "Anand Rangarajan", "authors": "Yuan Zhou, Anand Rangarajan and Paul Gader", "title": "A spatial compositional model (SCM) for linear unmixing and endmember\n  uncertainty estimation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2618002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The normal compositional model (NCM) has been extensively used in\nhyperspectral unmixing. However, most of the previous research has focused on\nestimation of endmembers and/or their variability. Also, little work has\nemployed spatial information in NCM. In this paper, we show that NCM can be\nused for calculating the uncertainty of the estimated endmembers with spatial\npriors incorporated for better unmixing. This results in a spatial\ncompositional model (SCM) which features (i) spatial priors that force\nneighboring abundances to be similar based on their pixel similarity and (ii) a\nposterior that is obtained from a likelihood model which does not assume pixel\nindependence. The resulting algorithm turns out to be easy to implement and\nefficient to run. We compared SCM with current state-of-the-art algorithms on\nsynthetic and real images. The results show that SCM can in the main provide\nmore accurate endmembers and abundances. Moreover, the estimated uncertainty\ncan serve as a prediction of endmember error under certain conditions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 16:24:45 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Zhou", "Yuan", ""], ["Rangarajan", "Anand", ""], ["Gader", "Paul", ""]]}, {"id": "1509.09294", "submitter": "Armin Mustafa", "authors": "Armin Mustafa and Hansung Kim and Jean-Yves Guillemaut and Adrian\n  Hilton", "title": "General Dynamic Scene Reconstruction from Multiple View Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a general approach to dynamic scene reconstruction from\nmultiple moving cameras without prior knowledge or limiting constraints on the\nscene structure, appearance, or illumination. Existing techniques for dynamic\nscene reconstruction from multiple wide-baseline camera views primarily focus\non accurate reconstruction in controlled environments, where the cameras are\nfixed and calibrated and background is known. These approaches are not robust\nfor general dynamic scenes captured with sparse moving cameras. Previous\napproaches for outdoor dynamic scene reconstruction assume prior knowledge of\nthe static background appearance and structure. The primary contributions of\nthis paper are twofold: an automatic method for initial coarse dynamic scene\nsegmentation and reconstruction without prior knowledge of background\nappearance or structure; and a general robust approach for joint segmentation\nrefinement and dense reconstruction of dynamic scenes from multiple\nwide-baseline static or moving cameras. Evaluation is performed on a variety of\nindoor and outdoor scenes with cluttered backgrounds and multiple dynamic\nnon-rigid objects such as people. Comparison with state-of-the-art approaches\ndemonstrates improved accuracy in both multiple view segmentation and dense\nreconstruction. The proposed approach also eliminates the requirement for prior\nknowledge of scene structure and appearance.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 18:37:24 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Mustafa", "Armin", ""], ["Kim", "Hansung", ""], ["Guillemaut", "Jean-Yves", ""], ["Hilton", "Adrian", ""]]}]