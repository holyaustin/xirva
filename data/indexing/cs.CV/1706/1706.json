[{"id": "1706.00051", "submitter": "Morteza Mardani", "authors": "Morteza Mardani, Enhao Gong, Joseph Y. Cheng, Shreyas Vasanawala, Greg\n  Zaharchuk, Marcus Alley, Neil Thakur, Song Han, William Dally, John M. Pauly,\n  and Lei Xing", "title": "Deep Generative Adversarial Networks for Compressed Sensing Automates\n  MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance image (MRI) reconstruction is a severely ill-posed linear\ninverse task demanding time and resource intensive computations that can\nsubstantially trade off {\\it accuracy} for {\\it speed} in real-time imaging. In\naddition, state-of-the-art compressed sensing (CS) analytics are not cognizant\nof the image {\\it diagnostic quality}. To cope with these challenges we put\nforth a novel CS framework that permeates benefits from generative adversarial\nnetworks (GAN) to train a (low-dimensional) manifold of diagnostic-quality MR\nimages from historical patients. Leveraging a mixture of least-squares (LS)\nGANs and pixel-wise $\\ell_1$ cost, a deep residual network with skip\nconnections is trained as the generator that learns to remove the {\\it\naliasing} artifacts by projecting onto the manifold. LSGAN learns the texture\ndetails, while $\\ell_1$ controls the high-frequency noise. A multilayer\nconvolutional neural network is then jointly trained based on diagnostic\nquality images to discriminate the projection quality. The test phase performs\nfeed-forward propagation over the generator network that demands a very low\ncomputational overhead. Extensive evaluations are performed on a large\ncontrast-enhanced MR dataset of pediatric patients. In particular, images rated\nbased on expert radiologists corroborate that GANCS retrieves high contrast\nimages with detailed texture relative to conventional CS, and pixel-wise\nschemes. In addition, it offers reconstruction under a few milliseconds, two\norders of magnitude faster than state-of-the-art CS-MRI schemes.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 19:12:14 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Mardani", "Morteza", ""], ["Gong", "Enhao", ""], ["Cheng", "Joseph Y.", ""], ["Vasanawala", "Shreyas", ""], ["Zaharchuk", "Greg", ""], ["Alley", "Marcus", ""], ["Thakur", "Neil", ""], ["Han", "Song", ""], ["Dally", "William", ""], ["Pauly", "John M.", ""], ["Xing", "Lei", ""]]}, {"id": "1706.00079", "submitter": "Sourish Chaudhuri", "authors": "Ken Hoover, Sourish Chaudhuri, Caroline Pantofaru, Malcolm Slaney, Ian\n  Sturdy", "title": "Putting a Face to the Voice: Fusing Audio and Visual Signals Across a\n  Video to Determine Speakers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a system that associates faces with voices in a\nvideo by fusing information from the audio and visual signals. The thesis\nunderlying our work is that an extremely simple approach to generating (weak)\nspeech clusters can be combined with visual signals to effectively associate\nfaces and voices by aggregating statistics across a video. This approach does\nnot need any training data specific to this task and leverages the natural\ncoherence of information in the audio and visual streams. It is particularly\napplicable to tracking speakers in videos on the web where a priori information\nabout the environment (e.g., number of speakers, spatial signals for\nbeamforming) is not available. We performed experiments on a real-world dataset\nusing this analysis framework to determine the speaker in a video. Given a\nground truth labeling determined by human rater consensus, our approach had\n~71% accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 20:35:26 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Hoover", "Ken", ""], ["Chaudhuri", "Sourish", ""], ["Pantofaru", "Caroline", ""], ["Slaney", "Malcolm", ""], ["Sturdy", "Ian", ""]]}, {"id": "1706.00082", "submitter": "Marco Marchesi", "authors": "Marco Marchesi", "title": "Megapixel Size Image Creation using Generative Adversarial Networks", "comments": "3 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its appearance, Generative Adversarial Networks (GANs) have received a\nlot of interest in the AI community. In image generation several projects\nshowed how GANs are able to generate photorealistic images but the results so\nfar did not look adequate for the quality standard of visual media production\nindustry. We present an optimized image generation process based on a Deep\nConvolutional Generative Adversarial Networks (DCGANs), in order to create\nphotorealistic high-resolution images (up to 1024x1024 pixels). Furthermore,\nthe system was fed with a limited dataset of images, less than two thousand\nimages. All these results give more clue about future exploitation of GANs in\nComputer Graphics and Visual Effects.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 20:43:19 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Marchesi", "Marco", ""]]}, {"id": "1706.00083", "submitter": "Fariborz Taherkhani", "authors": "Fariborz Taherkhani", "title": "Blood capillaries and vessels segmentation in optical coherence\n  tomography angiogram using fuzzy C-means and Curvelet transform", "comments": "arXiv admin note: This paper has been removed from arXiv as the\n  submitter did not have ownership of the data presented in this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper has been removed from arXiv as the submitter did not have\nownership of the data presented in this work.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 20:44:55 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Taherkhani", "Fariborz", ""]]}, {"id": "1706.00120", "submitter": "Kisuk Lee", "authors": "Kisuk Lee, Jonathan Zung, Peter Li, Viren Jain, H. Sebastian Seung", "title": "Superhuman Accuracy on the SNEMI3D Connectomics Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the past decade, convolutional networks have been used for 3D\nreconstruction of neurons from electron microscopic (EM) brain images. Recent\nyears have seen great improvements in accuracy, as evidenced by submissions to\nthe SNEMI3D benchmark challenge. Here we report the first submission to surpass\nthe estimate of human accuracy provided by the SNEMI3D leaderboard. A variant\nof 3D U-Net is trained on a primary task of predicting affinities between\nnearest neighbor voxels, and an auxiliary task of predicting long-range\naffinities. The training data is augmented by simulated image defects. The\nnearest neighbor affinities are used to create an oversegmentation, and then\nsupervoxels are greedily agglomerated based on mean affinity. The resulting\nSNEMI3D score exceeds the estimate of human accuracy by a large margin. While\none should be cautious about extrapolating from the SNEMI3D benchmark to\nreal-world accuracy of large-scale neural circuit reconstruction, our result\ninspires optimism that the goal of full automation may be realizable in the\nfuture.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 23:19:37 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Lee", "Kisuk", ""], ["Zung", "Jonathan", ""], ["Li", "Peter", ""], ["Jain", "Viren", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1706.00130", "submitter": "Huan Ling", "authors": "Huan Ling, Sanja Fidler", "title": "Teaching Machines to Describe Images via Natural Language Feedback", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots will eventually be part of every household. It is thus critical to\nenable algorithms to learn from and be guided by non-expert users. In this\npaper, we bring a human in the loop, and enable a human teacher to give\nfeedback to a learning agent in the form of natural language. We argue that a\ndescriptive sentence can provide a much stronger learning signal than a numeric\nreward in that it can easily point to where the mistakes are and how to correct\nthem. We focus on the problem of image captioning in which the quality of the\noutput can easily be judged by non-experts. We propose a hierarchical\nphrase-based captioning model trained with policy gradients, and design a\nfeedback network that provides reward to the learner by conditioning on the\nhuman-provided feedback. We show that by exploiting descriptive feedback our\nmodel learns to perform better than when given independently written human\ncaptions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 00:24:55 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 16:47:40 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Ling", "Huan", ""], ["Fidler", "Sanja", ""]]}, {"id": "1706.00140", "submitter": "Xiao-Xiang Hu", "authors": "Xiaoxiang Hu, Yujiu Yang", "title": "Faster Spatially Regularized Correlation Filters for Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminatively learned correlation filters (DCF) have been widely used in\nonline visual tracking filed due to its simplicity and efficiency. These\nmethods utilize a periodic assumption of the training samples to construct a\ncirculant data matrix, which implicitly increases the training samples and\nreduces both storage and computational complexity.The periodic assumption also\nintroduces unwanted boundary effects. Recently, Spatially Regularized\nCorrelation Filters (SRDCF) solved this issue by introducing penalization on\ncorrelation filter coefficients depending on their spatial location. However,\nSRDCF's efficiency dramatically decreased due to the breaking of circulant\nstructure.\n  We propose Faster Spatially Regularized Discriminative Correlation Filters\n(FSRDCF) for tracking. The FSRDCF is constructed from Ridge Regression, the\ncirculant structure of training samples in the spatial domain is fully used,\nmore importantly, we further exploit the circulant structure of regularization\nfunction in the Fourier domain, which allows our problem to be solved more\ndirectly and efficiently. Experiments are conducted on three benchmark\ndatasets: OTB-2013, OTB-2015 and VOT2016. Our approach achieves equivalent\nperformance to the baseline tracker SRDCF on all three datasets. On OTB-2013\nand OTB-2015 datasets, our approach obtains a more than twice faster running\nspeed and a more than third times shorter start-up time than the SRDCF. For\nstate-of-the-art comparison, our approach demonstrates superior performance\ncompared to other non-spatial-regularization trackers.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 01:11:57 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Hu", "Xiaoxiang", ""], ["Yang", "Yujiu", ""]]}, {"id": "1706.00150", "submitter": "Ellen Gasparovic", "authors": "James Damon and Ellen Gasparovic", "title": "Shape and Positional Geometry of Multi-Object Configurations", "comments": "This paper presents material relevant for two and three dimensional\n  images that builds on and makes many references to a previous paper by the\n  authors, arXiv:1402.5517", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work, we introduced a method for modeling a configuration of\nobjects in 2D and 3D images using a mathematical \"medial/skeletal linking\nstructure.\" In this paper, we show how these structures allow us to capture\npositional properties of a multi-object configuration in addition to the shape\nproperties of the individual objects. In particular, we introduce numerical\ninvariants for positional properties which measure the closeness of neighboring\nobjects, including identifying the parts of the objects which are close, and\nthe \"relative significance\" of objects compared with the other objects in the\nconfiguration. Using these numerical measures, we introduce a hierarchical\nordering and relations between the individual objects, and quantitative\ncriteria for identifying subconfigurations. In addition, the invariants provide\na \"proximity matrix\" which yields a unique set of weightings measuring overall\nproximity of objects in the configuration. Furthermore, we show that these\ninvariants, which are volumetrically defined and involve external regions, may\nbe computed via integral formulas in terms of \"skeletal linking integrals\"\ndefined on the internal skeletal structures of the objects.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 02:12:06 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Damon", "James", ""], ["Gasparovic", "Ellen", ""]]}, {"id": "1706.00153", "submitter": "Yuxin Peng", "authors": "Xin Huang, Yuxin Peng, and Mingkuan Yuan", "title": "Cross-modal Common Representation Learning by Hybrid Transfer Network", "comments": "To appear in the proceedings of 26th International Joint Conference\n  on Artificial Intelligence (IJCAI), Melbourne, Australia, Aug. 19-25, 2017. 8\n  pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNN-based cross-modal retrieval is a research hotspot to retrieve across\ndifferent modalities as image and text, but existing methods often face the\nchallenge of insufficient cross-modal training data. In single-modal scenario,\nsimilar problem is usually relieved by transferring knowledge from large-scale\nauxiliary datasets (as ImageNet). Knowledge from such single-modal datasets is\nalso very useful for cross-modal retrieval, which can provide rich general\nsemantic information that can be shared across different modalities. However,\nit is challenging to transfer useful knowledge from single-modal (as image)\nsource domain to cross-modal (as image/text) target domain. Knowledge in source\ndomain cannot be directly transferred to both two different modalities in\ntarget domain, and the inherent cross-modal correlation contained in target\ndomain provides key hints for cross-modal retrieval which should be preserved\nduring transfer process. This paper proposes Cross-modal Hybrid Transfer\nNetwork (CHTN) with two subnetworks: Modal-sharing transfer subnetwork utilizes\nthe modality in both source and target domains as a bridge, for transferring\nknowledge to both two modalities simultaneously; Layer-sharing correlation\nsubnetwork preserves the inherent cross-modal semantic correlation to further\nadapt to cross-modal retrieval task. Cross-modal data can be converted to\ncommon representation by CHTN for retrieval, and comprehensive experiment on 3\ndatasets shows its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 02:53:57 GMT"}, {"version": "v2", "created": "Sat, 24 Jun 2017 14:08:19 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Huang", "Xin", ""], ["Peng", "Yuxin", ""], ["Yuan", "Mingkuan", ""]]}, {"id": "1706.00212", "submitter": "Wendong Zhang", "authors": "Wendong Zhang, Bingbing Ni, Yichao Yan, Jingwei Xu, Xiaokang Yang", "title": "Depth Structure Preserving Scene Image Generation", "comments": "There is an error in the first paragraph in Section 4.4. Actually, we\n  train and test another new CGAN model with the input in our model to evaluate\n  the improvements. This error can lead readers misunderstand the improvements\n  of our model and make the comparison unfair. Therefore, we request to\n  withdraw the current submission and will submit a final version later", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key to automatically generate natural scene images is to properly arrange\namong various spatial elements, especially in the depth direction. To this end,\nwe introduce a novel depth structure preserving scene image generation network\n(DSP-GAN), which favors a hierarchical and heterogeneous architecture, for the\npurpose of depth structure preserving scene generation. The main trunk of the\nproposed infrastructure is built on a Hawkes point process that models the\nspatial dependency between different depth layers. Within each layer generative\nadversarial sub-networks are trained collaboratively to generate realistic\nscene components, conditioned on the layer information produced by the point\nprocess. We experiment our model on a sub-set of SUNdataset with annotated\nscene images and demonstrate that our models are capable of generating\ndepth-realistic natural scene image.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 09:03:08 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 03:56:53 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Zhang", "Wendong", ""], ["Ni", "Bingbing", ""], ["Yan", "Yichao", ""], ["Xu", "Jingwei", ""], ["Yang", "Xiaokang", ""]]}, {"id": "1706.00227", "submitter": "Congcong Jin", "authors": "Congcong Jin, Jihua Zhu, Yaochen Li, Shaoyi Du, Zhongyu Li, Huimin Lu", "title": "An Effective Approach for Point Clouds Registration Based on the Hard\n  and Soft Assignments", "comments": "23 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For the registration of partially overlapping point clouds, this paper\nproposes an effective approach based on both the hard and soft assignments.\nGiven two initially posed clouds, it firstly establishes the forward\ncorrespondence for each point in the data shape and calculates the value of\nbinary variable, which can indicate whether this point correspondence is\nlocated in the overlapping areas or not. Then, it establishes the bilateral\ncorrespondence and computes bidirectional distances for each point in the\noverlapping areas. Based on the ratio of bidirectional distances, the\nexponential function is selected and utilized to calculate the probability\nvalue, which can indicate the reliability of the point correspondence.\nSubsequently, both the values of hard and soft assignments are embedded into\nthe proposed objective function for registration of partially overlapping point\nclouds and a novel variant of ICP algorithm is proposed to obtain the optimal\nrigid transformation. The proposed approach can achieve good registration of\npoint clouds, even when their overlap percentage is low. Experimental results\ntested on public data sets illustrate its superiority over previous approaches\non accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 09:31:04 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Jin", "Congcong", ""], ["Zhu", "Jihua", ""], ["Li", "Yaochen", ""], ["Du", "Shaoyi", ""], ["Li", "Zhongyu", ""], ["Lu", "Huimin", ""]]}, {"id": "1706.00322", "submitter": "Stefano Alletto", "authors": "Stefano Alletto, Davide Abati, Simone Calderara, Rita Cucchiara, Luca\n  Rigazio", "title": "TransFlow: Unsupervised Motion Flow by Joint Geometric and Pixel-level\n  Estimation", "comments": "We have found a bug in the flow evaluation code compromising the\n  experimental evaluation and the results provided in the paper are no longer\n  correct. We are currently working on a new experimental campaign but we\n  estimate that results will be available in a few weeks and will drastically\n  change the paper, hence the withdraw request", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address unsupervised optical flow estimation for ego-centric motion. We\nargue that optical flow can be cast as a geometrical warping between two\nsuccessive video frames and devise a deep architecture to estimate such\ntransformation in two stages. First, a dense pixel-level flow is computed with\na geometric prior imposing strong spatial constraints. Such prior is typical of\ndriving scenes, where the point of view is coherent with the vehicle motion. We\nshow how such global transformation can be approximated with an homography and\nhow spatial transformer layers can be employed to compute the flow field\nimplied by such transformation. The second stage then refines the prediction\nfeeding a second deeper network. A final reconstruction loss compares the\nwarping of frame X(t) with the subsequent frame X(t+1) and guides both\nestimates. The model, which we named TransFlow, performs favorably compared to\nother unsupervised algorithms, and shows better generalization compared to\nsupervised methods with a 3x reduction in error on unseen data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 14:33:42 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 14:55:01 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 09:34:47 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Alletto", "Stefano", ""], ["Abati", "Davide", ""], ["Calderara", "Simone", ""], ["Cucchiara", "Rita", ""], ["Rigazio", "Luca", ""]]}, {"id": "1706.00384", "submitter": "Ying Zhang", "authors": "Ying Zhang and Tao Xiang and Timothy M. Hospedales and Huchuan Lu", "title": "Deep Mutual Learning", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model distillation is an effective and widely used technique to transfer\nknowledge from a teacher to a student network. The typical application is to\ntransfer from a powerful large network or ensemble to a small network, that is\nbetter suited to low-memory or fast execution requirements. In this paper, we\npresent a deep mutual learning (DML) strategy where, rather than one way\ntransfer between a static pre-defined teacher and a student, an ensemble of\nstudents learn collaboratively and teach each other throughout the training\nprocess. Our experiments show that a variety of network architectures benefit\nfrom mutual learning and achieve compelling results on CIFAR-100 recognition\nand Market-1501 person re-identification benchmarks. Surprisingly, it is\nrevealed that no prior powerful teacher network is necessary -- mutual learning\nof a collection of simple student networks works, and moreover outperforms\ndistillation from a more powerful yet static teacher.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 16:57:15 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Zhang", "Ying", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy M.", ""], ["Lu", "Huchuan", ""]]}, {"id": "1706.00388", "submitter": "Sergey Zagoruyko", "authors": "Sergey Zagoruyko and Nikos Komodakis", "title": "DiracNets: Training Very Deep Neural Networks Without Skip-Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks with skip-connections, such as ResNet, show excellent\nperformance in various image classification benchmarks. It is though observed\nthat the initial motivation behind them - training deeper networks - does not\nactually hold true, and the benefits come from increased capacity, rather than\nfrom depth. Motivated by this, and inspired from ResNet, we propose a simple\nDirac weight parameterization, which allows us to train very deep plain\nnetworks without explicit skip-connections, and achieve nearly the same\nperformance. This parameterization has a minor computational cost at training\ntime and no cost at all at inference, as both Dirac parameterization and batch\nnormalization can be folded into convolutional filters, so that network becomes\na simple chain of convolution-ReLU pairs. We are able to match ResNet-1001\naccuracy on CIFAR-10 with 28-layer wider plain DiracNet, and closely match\nResNets on ImageNet. Our parameterization also mostly eliminates the need of\ncareful initialization in residual and non-residual networks. The code and\nmodels for our experiments are available at\nhttps://github.com/szagoruyko/diracnets\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 17:02:11 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 14:08:57 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Zagoruyko", "Sergey", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1706.00396", "submitter": "Jun Qin", "authors": "Ali Mahdi and Jun Qin", "title": "Line Profile Based Segmentation Algorithm for Touching Corn Kernels", "comments": "We found some results in this paper may not be correct. Therefore, we\n  require to withdraw this paper. Thanks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation of touching objects plays a key role in providing accurate\nclassification for computer vision technologies. A new line profile based\nimaging segmentation algorithm has been developed to provide a robust and\naccurate segmentation of a group of touching corns. The performance of the line\nprofile based algorithm has been compared to a watershed based imaging\nsegmentation algorithm. Both algorithms are tested on three different patterns\nof images, which are isolated corns, single-lines, and random distributed\nformations. The experimental results show that the algorithm can segment a\nlarge number of touching corn kernels efficiently and accurately.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 17:15:02 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 04:14:35 GMT"}, {"version": "v3", "created": "Fri, 4 Aug 2017 00:27:43 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Mahdi", "Ali", ""], ["Qin", "Jun", ""]]}, {"id": "1706.00409", "submitter": "Guillaume Lample", "authors": "Guillaume Lample, Neil Zeghidour, Nicolas Usunier, Antoine Bordes,\n  Ludovic Denoyer, Marc'Aurelio Ranzato", "title": "Fader Networks: Manipulating Images by Sliding Attributes", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new encoder-decoder architecture that is trained to\nreconstruct images by disentangling the salient information of the image and\nthe values of attributes directly in the latent space. As a result, after\ntraining, our model can generate different realistic versions of an input image\nby varying the attribute values. By using continuous attribute values, we can\nchoose how much a specific attribute is perceivable in the generated image.\nThis property could allow for applications where users can modify an image\nusing sliding knobs, like faders on a mixing console, to change the facial\nexpression of a portrait, or to update the color of some objects. Compared to\nthe state-of-the-art which mostly relies on training adversarial networks in\npixel space by altering attribute values at train time, our approach results in\nmuch simpler training schemes and nicely scales to multiple attributes. We\npresent evidence that our model can significantly change the perceived value of\nthe attributes while preserving the naturalness of images.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 17:48:24 GMT"}, {"version": "v2", "created": "Sun, 28 Jan 2018 16:12:14 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Lample", "Guillaume", ""], ["Zeghidour", "Neil", ""], ["Usunier", "Nicolas", ""], ["Bordes", "Antoine", ""], ["Denoyer", "Ludovic", ""], ["Ranzato", "Marc'Aurelio", ""]]}, {"id": "1706.00447", "submitter": "Allan Pinto", "authors": "Allan Pinto, Daniel Moreira, Aparna Bharati, Joel Brogan, Kevin\n  Bowyer, Patrick Flynn, Walter Scheirer and Anderson Rocha", "title": "Provenance Filtering for Multimedia Phylogeny", "comments": "5 pages, Accepted in IEEE International Conference on Image\n  Processing (ICIP), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Departing from traditional digital forensics modeling, which seeks to analyze\nsingle objects in isolation, multimedia phylogeny analyzes the evolutionary\nprocesses that influence digital objects and collections over time. One of its\nintegral pieces is provenance filtering, which consists of searching a\npotentially large pool of objects for the most related ones with respect to a\ngiven query, in terms of possible ancestors (donors or contributors) and\ndescendants. In this paper, we propose a two-tiered provenance filtering\napproach to find all the potential images that might have contributed to the\ncreation process of a given query $q$. In our solution, the first (coarse) tier\naims to find the most likely \"host\" images --- the major donor or background\n--- contributing to a composite/doctored image. The search is then refined in\nthe second tier, in which we search for more specific (potentially small) parts\nof the query that might have been extracted from other images and spliced into\nthe query image. Experimental results with a dataset containing more than a\nmillion images show that the two-tiered solution underpinned by the context of\nthe query is highly useful for solving this difficult task.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 18:12:57 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Pinto", "Allan", ""], ["Moreira", "Daniel", ""], ["Bharati", "Aparna", ""], ["Brogan", "Joel", ""], ["Bowyer", "Kevin", ""], ["Flynn", "Patrick", ""], ["Scheirer", "Walter", ""], ["Rocha", "Anderson", ""]]}, {"id": "1706.00493", "submitter": "Ling Zhang", "authors": "Ling Zhang, Le Lu, Ronald M. Summers, Electron Kebebew, Jianhua Yao", "title": "Personalized Pancreatic Tumor Growth Prediction via Group Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor growth prediction, a highly challenging task, has long been viewed as a\nmathematical modeling problem, where the tumor growth pattern is personalized\nbased on imaging and clinical data of a target patient. Though mathematical\nmodels yield promising results, their prediction accuracy may be limited by the\nabsence of population trend data and personalized clinical characteristics. In\nthis paper, we propose a statistical group learning approach to predict the\ntumor growth pattern that incorporates both the population trend and\npersonalized data, in order to discover high-level features from multimodal\nimaging data. A deep convolutional neural network approach is developed to\nmodel the voxel-wise spatio-temporal tumor progression. The deep features are\ncombined with the time intervals and the clinical factors to feed a process of\nfeature selection. Our predictive model is pretrained on a group data set and\npersonalized on the target patient data to estimate the future spatio-temporal\nprogression of the patient's tumor. Multimodal imaging data at multiple time\npoints are used in the learning, personalization and inference stages. Our\nmethod achieves a Dice coefficient of 86.8% +- 3.6% and RVD of 7.9% +- 5.4% on\na pancreatic tumor data set, outperforming the DSC of 84.4% +- 4.0% and RVD\n13.9% +- 9.8% obtained by a previous state-of-the-art model-based method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 20:57:53 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Zhang", "Ling", ""], ["Lu", "Le", ""], ["Summers", "Ronald M.", ""], ["Kebebew", "Electron", ""], ["Yao", "Jianhua", ""]]}, {"id": "1706.00510", "submitter": "Mahmoud Yassien Shams El Den", "authors": "M. Y. Shams, A. S. Tolba, S.H. Sarhan", "title": "A Vision System for Multi-View Face Recognition", "comments": "7 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal biometric identification has been grown a great attention in the\nmost interests in the security fields. In the real world there exist modern\nsystem devices that are able to detect, recognize, and classify the human\nidentities with reliable and fast recognition rates. Unfortunately most of\nthese systems rely on one modality, and the reliability for two or more\nmodalities are further decreased. The variations of face images with respect to\ndifferent poses are considered as one of the important challenges in face\nrecognition systems. In this paper, we propose a multimodal biometric system\nthat able to detect the human face images that are not only one view face\nimage, but also multi-view face images. Each subject entered to the system\nadjusted their face at front of the three cameras, and then the features of the\nface images are extracted based on Speeded Up Robust Features (SURF) algorithm.\nWe utilize Multi-Layer Perceptron (MLP) and combined classifiers based on both\nLearning Vector Quantization (LVQ), and Radial Basis Function (RBF) for\nclassification purposes. The proposed system has been tested using SDUMLA-HMT,\nand CASIA datasets. Furthermore, we collected a database of multi-view face\nimages by which we take the additive white Gaussian noise into considerations.\nThe results indicated the reliability, robustness of the proposed system with\ndifferent poses and variations including noise images.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 22:10:31 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Shams", "M. Y.", ""], ["Tolba", "A. S.", ""], ["Sarhan", "S. H.", ""]]}, {"id": "1706.00527", "submitter": "Terry Taewoong Um", "authors": "Terry Taewoong Um, Franz Michael Josef Pfister, Daniel Pichler,\n  Satoshi Endo, Muriel Lang, Sandra Hirche, Urban Fietzek, Dana Kuli\\'c", "title": "Data Augmentation of Wearable Sensor Data for Parkinson's Disease\n  Monitoring using Convolutional Neural Networks", "comments": "ICMI2017 (oral session)", "journal-ref": null, "doi": "10.1145/3136755.3136817", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While convolutional neural networks (CNNs) have been successfully applied to\nmany challenging classification applications, they typically require large\ndatasets for training. When the availability of labeled data is limited, data\naugmentation is a critical preprocessing step for CNNs. However, data\naugmentation for wearable sensor data has not been deeply investigated yet.\n  In this paper, various data augmentation methods for wearable sensor data are\nproposed. The proposed methods and CNNs are applied to the classification of\nthe motor state of Parkinson's Disease patients, which is challenging due to\nsmall dataset size, noisy labels, and large intra-class variability.\nAppropriate augmentation improves the classification performance from 77.54\\%\nto 86.88\\%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 00:43:11 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 20:01:02 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Um", "Terry Taewoong", ""], ["Pfister", "Franz Michael Josef", ""], ["Pichler", "Daniel", ""], ["Endo", "Satoshi", ""], ["Lang", "Muriel", ""], ["Hirche", "Sandra", ""], ["Fietzek", "Urban", ""], ["Kuli\u0107", "Dana", ""]]}, {"id": "1706.00530", "submitter": "Yuchao Dai Dr.", "authors": "Jing Zhang, Bo Li, Yuchao Dai, Fatih Porikli and Mingyi He", "title": "Integrated Deep and Shallow Networks for Salient Object Detection", "comments": "Accepted by IEEE International Conference on Image Processing (ICIP)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural network (CNN) based salient object detection\nmethods have achieved state-of-the-art performance and outperform those\nunsupervised methods with a wide margin. In this paper, we propose to integrate\ndeep and unsupervised saliency for salient object detection under a unified\nframework. Specifically, our method takes results of unsupervised saliency\n(Robust Background Detection, RBD) and normalized color images as inputs, and\ndirectly learns an end-to-end mapping between inputs and the corresponding\nsaliency maps. The color images are fed into a Fully Convolutional Neural\nNetworks (FCNN) adapted from semantic segmentation to exploit high-level\nsemantic cues for salient object detection. Then the results from deep FCNN and\nRBD are concatenated to feed into a shallow network to map the concatenated\nfeature maps to saliency maps. Finally, to obtain a spatially consistent\nsaliency map with sharp object boundaries, we fuse superpixel level saliency\nmap at multi-scale. Extensive experimental results on 8 benchmark datasets\ndemonstrate that the proposed method outperforms the state-of-the-art\napproaches with a margin.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 00:52:55 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Zhang", "Jing", ""], ["Li", "Bo", ""], ["Dai", "Yuchao", ""], ["Porikli", "Fatih", ""], ["He", "Mingyi", ""]]}, {"id": "1706.00552", "submitter": "Puyang Wang", "authors": "Puyang Wang, He Zhang and Vishal M. Patel", "title": "SAR Image Despeckling Using a Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2017.2758203", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic Aperture Radar (SAR) images are often contaminated by a\nmultiplicative noise known as speckle. Speckle makes the processing and\ninterpretation of SAR images difficult. We propose a deep learning-based\napproach called, Image Despeckling Convolutional Neural Network (ID-CNN), for\nautomatically removing speckle from the input noisy images. In particular,\nID-CNN uses a set of convolutional layers along with batch normalization and\nrectified linear unit (ReLU) activation function and a component-wise division\nresidual layer to estimate speckle and it is trained in an end-to-end fashion\nusing a combination of Euclidean loss and Total Variation (TV) loss. Extensive\nexperiments on synthetic and real SAR images show that the proposed method\nachieves significant improvements over the state-of-the-art speckle reduction\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 04:31:43 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 02:45:39 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Wang", "Puyang", ""], ["Zhang", "He", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1706.00553", "submitter": "Srikrishna Karanam", "authors": "Srikrishna Karanam and Eric Lam and Richard J. Radke", "title": "Rank Persistence: Assessing the Temporal Performance of Real-World\n  Person Re-Identification", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing useful person re-identification systems for real-world applications\nrequires attention to operational aspects not typically considered in academic\nresearch. Here, we focus on the temporal aspect of re-identification; that is,\ninstead of finding a match to a probe person of interest in a fixed candidate\ngallery, we consider the more realistic scenario in which the gallery is\ncontinuously populated by new candidates over a long time period. A key\nquestion of interest for an operator of such a system is: how long is a correct\nmatch to a probe likely to remain in a rank-k shortlist of possible candidates?\nWe propose to distill this information into a Rank Persistence Curve (RPC),\nwhich allows different algorithms' temporal performance characteristics to be\ndirectly compared. We present examples to illustrate the RPC using a new\nlong-term dataset with multiple candidate reappearances, and discuss\nconsiderations for future re-identification research that explicitly involves\ntemporal aspects.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 04:34:08 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 02:00:52 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Karanam", "Srikrishna", ""], ["Lam", "Eric", ""], ["Radke", "Richard J.", ""]]}, {"id": "1706.00556", "submitter": "Yang Song", "authors": "Yang Song, Zhifei Zhang, Hairong Qi", "title": "r-BTN: Cross-domain Face Composite and Synthesis from Limited Facial\n  Patches", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We start by asking an interesting yet challenging question, \"If an eyewitness\ncan only recall the eye features of the suspect, such that the forensic artist\ncan only produce a sketch of the eyes (e.g., the top-left sketch shown in Fig.\n1), can advanced computer vision techniques help generate the whole face\nimage?\" A more generalized question is that if a large proportion (e.g., more\nthan 50%) of the face/sketch is missing, can a realistic whole face\nsketch/image still be estimated. Existing face completion and generation\nmethods either do not conduct domain transfer learning or can not handle large\nmissing area. For example, the inpainting approach tends to blur the generated\nregion when the missing area is large (i.e., more than 50%). In this paper, we\nexploit the potential of deep learning networks in filling large missing region\n(e.g., as high as 95% missing) and generating realistic faces with\nhigh-fidelity in cross domains. We propose the recursive generation by\nbidirectional transformation networks (r-BTN) that recursively generates a\nwhole face/sketch from a small sketch/face patch. The large missing area and\nthe cross domain challenge make it difficult to generate satisfactory results\nusing a unidirectional cross-domain learning structure. On the other hand, a\nforward and backward bidirectional learning between the face and sketch domains\nwould enable recursive estimation of the missing region in an incremental\nmanner (Fig. 1) and yield appealing results. r-BTN also adopts an adversarial\nconstraint to encourage the generation of realistic faces/sketches. Extensive\nexperiments have been conducted to demonstrate the superior performance from\nr-BTN as compared to existing potential solutions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 05:07:37 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 20:08:27 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Song", "Yang", ""], ["Zhang", "Zhifei", ""], ["Qi", "Hairong", ""]]}, {"id": "1706.00597", "submitter": "Ying Fu", "authors": "Guangtao Nie, Ying Fu, Yinqiang Zheng, Hua Huang", "title": "Image Restoration from Patch-based Compressed Sensing Measurement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A series of methods have been proposed to reconstruct an image from\ncompressively sensed random measurement, but most of them have high time\ncomplexity and are inappropriate for patch-based compressed sensing capture,\nbecause of their serious blocky artifacts in the restoration results. In this\npaper, we present a non-iterative image reconstruction method from patch-based\ncompressively sensed random measurement. Our method features two cascaded\nnetworks based on residual convolution neural network to learn the end-to-end\nfull image restoration, which is capable of reconstructing image patches and\nremoving the blocky effect with low time cost. Experimental results on\nsynthetic and real data show that our method outperforms state-of-the-art\ncompressive sensing (CS) reconstruction methods with patch-based CS\nmeasurement. To demonstrate the effectiveness of our method in more general\nsetting, we apply the de-block process in our method to JPEG compression\nartifacts removal and achieve outstanding performance as well.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 09:08:06 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Nie", "Guangtao", ""], ["Fu", "Ying", ""], ["Zheng", "Yinqiang", ""], ["Huang", "Hua", ""]]}, {"id": "1706.00598", "submitter": "J\\\"orn-Henrik Jacobsen", "authors": "J\\\"orn-Henrik Jacobsen, Bert de Brabandere, Arnold W.M. Smeulders", "title": "Dynamic Steerable Blocks in Deep Residual Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filters in convolutional networks are typically parameterized in a pixel\nbasis, that does not take prior knowledge about the visual world into account.\nWe investigate the generalized notion of frames designed with image properties\nin mind, as alternatives to this parametrization. We show that frame-based\nResNets and Densenets can improve performance on Cifar-10+ consistently, while\nhaving additional pleasant properties like steerability. By exploiting these\ntransformation properties explicitly, we arrive at dynamic steerable blocks.\nThey are an extension of residual blocks, that are able to seamlessly transform\nfilters under pre-defined transformations, conditioned on the input at training\nand inference time. Dynamic steerable blocks learn the degree of invariance\nfrom data and locally adapt filters, allowing them to apply a different\ngeometrical variant of the same filter to each location of the feature map.\nWhen evaluated on the Berkeley Segmentation contour detection dataset, our\napproach outperforms all competing approaches that do not utilize pre-training.\nOur results highlight the benefits of image-based regularization to deep\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 09:08:09 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 15:12:52 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Jacobsen", "J\u00f6rn-Henrik", ""], ["de Brabandere", "Bert", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1706.00613", "submitter": "Valentin Tschannen", "authors": "Valentin Tschannen, Matthias Delescluse, Mathieu Rodriguez and Janis\n  Keuper", "title": "Facies classification from well logs using an inception convolutional\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea to use automated algorithms to determine geological facies from well\nlogs is not new (see e.g Busch et al. (1987); Rabaute (1998)) but the recent\nand dramatic increase in research in the field of machine learning makes it a\ngood time to revisit the topic. Following an exercise proposed by Dubois et al.\n(2007) and Hall (2016) we employ a modern type of deep convolutional network,\ncalled \\textit{inception network} (Szegedy et al., 2015), to tackle the\nsupervised classification task and we discuss the methodological limits of such\nproblem as well as further research opportunities.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 10:13:28 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Tschannen", "Valentin", ""], ["Delescluse", "Matthias", ""], ["Rodriguez", "Mathieu", ""], ["Keuper", "Janis", ""]]}, {"id": "1706.00631", "submitter": "Bingzhang Hu", "authors": "BingZhang Hu and Feng Zheng and Ling Shao", "title": "Dual-reference Face Retrieval", "comments": "Accepted at AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face retrieval has received much attention over the past few decades, and\nmany efforts have been made in retrieving face images against pose,\nillumination, and expression variations. However, the conventional works fail\nto meet the requirements of a potential and novel task --- retrieving a\nperson's face image at a specific age, especially when the specific 'age' is\nnot given as a numeral, i.e. 'retrieving someone's image at the similar age\nperiod shown by another person's image'. To tackle this problem, we propose a\ndual reference face retrieval framework in this paper, where the system takes\ntwo inputs: an identity reference image which indicates the target identity and\nan age reference image which reflects the target age. In our framework, the raw\nimages are first projected on a joint manifold, which preserves both the age\nand identity locality. Then two similarity metrics of age and identity are\nexploited and optimized by utilizing our proposed quartet-based model. The\nexperiments show promising results, outperforming hierarchical methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 11:14:50 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 11:15:16 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Hu", "BingZhang", ""], ["Zheng", "Feng", ""], ["Shao", "Ling", ""]]}, {"id": "1706.00672", "submitter": "Nathanael Lemessa Baisa", "authors": "Nathanael L. Baisa and Andrew Wallace", "title": "Development of a N-type GM-PHD Filter for Multiple Target, Multiple Type\n  Visual Tracking", "comments": "arXiv admin note: text overlap with arXiv:1705.04757", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework that extends the standard Probability Hypothesis\nDensity (PHD) filter for multiple targets having $N\\geq2$ different types based\non Random Finite Set theory, taking into account not only background clutter,\nbut also confusions among detections of different target types, which are in\ngeneral different in character from background clutter. Under Gaussianity and\nlinearity assumptions, our framework extends the existing Gaussian mixture (GM)\nimplementation of the standard PHD filter to create a N-type GM-PHD filter. The\nmethodology is applied to real video sequences by integrating object detectors'\ninformation into this filter for two scenarios. For both cases, Munkres's\nvariant of the Hungarian assignment algorithm is used to associate tracked\ntarget identities between frames. This approach is evaluated and compared to\nboth raw detection and independent GM-PHD filters using the Optimal Sub-pattern\nAssignment metric and discrimination rate. This shows the improved performance\nof our strategy on real video sequences.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 18:03:33 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 15:08:43 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2018 08:47:01 GMT"}, {"version": "v4", "created": "Thu, 6 Sep 2018 11:36:51 GMT"}, {"version": "v5", "created": "Sun, 3 Feb 2019 22:44:54 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Baisa", "Nathanael L.", ""], ["Wallace", "Andrew", ""]]}, {"id": "1706.00699", "submitter": "Alexander Richard", "authors": "Alexander Richard, Hilde Kuehne, Juergen Gall", "title": "Action Sets: Weakly Supervised Action Segmentation without Ordering\n  Constraints", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action detection and temporal segmentation of actions in videos are topics of\nincreasing interest. While fully supervised systems have gained much attention\nlately, full annotation of each action within the video is costly and\nimpractical for large amounts of video data. Thus, weakly supervised action\ndetection and temporal segmentation methods are of great importance. While most\nworks in this area assume an ordered sequence of occurring actions to be given,\nour approach only uses a set of actions. Such action sets provide much less\nsupervision since neither action ordering nor the number of action occurrences\nare known. In exchange, they can be easily obtained, for instance, from\nmeta-tags, while ordered sequences still require human annotation. We introduce\na system that automatically learns to temporally segment and label actions in a\nvideo, where the only supervision that is used are action sets. An evaluation\non three datasets shows that our method still achieves good results although\nthe amount of supervision is significantly smaller than for other related\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 14:34:21 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 17:30:28 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Richard", "Alexander", ""], ["Kuehne", "Hilde", ""], ["Gall", "Juergen", ""]]}, {"id": "1706.00712", "submitter": "Jianming Liang PhD", "authors": "Nima Tajbakhsh, Jae Y. Shin, Suryakanth R. Gurudu, R. Todd Hurst,\n  Christopher B. Kendall, Michael B. Gotway, and Jianming Liang", "title": "Convolutional Neural Networks for Medical Image Analysis: Full Training\n  or Fine Tuning?", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging. 35(5):1299-1312 (2016)", "doi": "10.1109/TMI.2016.2535302", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a deep convolutional neural network (CNN) from scratch is difficult\nbecause it requires a large amount of labeled training data and a great deal of\nexpertise to ensure proper convergence. A promising alternative is to fine-tune\na CNN that has been pre-trained using, for instance, a large set of labeled\nnatural images. However, the substantial differences between natural and\nmedical images may advise against such knowledge transfer. In this paper, we\nseek to answer the following central question in the context of medical image\nanalysis: \\emph{Can the use of pre-trained deep CNNs with sufficient\nfine-tuning eliminate the need for training a deep CNN from scratch?} To\naddress this question, we considered 4 distinct medical imaging applications in\n3 specialties (radiology, cardiology, and gastroenterology) involving\nclassification, detection, and segmentation from 3 different imaging\nmodalities, and investigated how the performance of deep CNNs trained from\nscratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner.\nOur experiments consistently demonstrated that (1) the use of a pre-trained CNN\nwith adequate fine-tuning outperformed or, in the worst case, performed as well\nas a CNN trained from scratch; (2) fine-tuned CNNs were more robust to the size\nof training sets than CNNs trained from scratch; (3) neither shallow tuning nor\ndeep tuning was the optimal choice for a particular application; and (4) our\nlayer-wise fine-tuning scheme could offer a practical way to reach the best\nperformance for the application at hand based on the amount of available data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 15:04:43 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Tajbakhsh", "Nima", ""], ["Shin", "Jae Y.", ""], ["Gurudu", "Suryakanth R.", ""], ["Hurst", "R. Todd", ""], ["Kendall", "Christopher B.", ""], ["Gotway", "Michael B.", ""], ["Liang", "Jianming", ""]]}, {"id": "1706.00719", "submitter": "Jianming Liang PhD", "authors": "Jae Y. Shin, Nima Tajbakhsh, R. Todd Hurst, Christopher B. Kendall,\n  and Jianming Liang", "title": "Automating Carotid Intima-Media Thickness Video Interpretation with\n  Convolutional Neural Networks", "comments": "J. Y. Shin, N. Tajbakhsh, R. T. Hurst, C. B. Kendall, and J. Liang.\n  Automating carotid intima-media thickness video interpretation with\n  convolutional neural networks. CVPR 2016, pp 2526-2535; N. Tajbakhsh, J. Y.\n  Shin, R. T. Hurst, C. B. Kendall, and J. Liang. Automatic interpretation of\n  CIMT videos using convolutional neural networks. Deep Learning for Medical\n  Image Analysis, Academic Press, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular disease (CVD) is the leading cause of mortality yet largely\npreventable, but the key to prevention is to identify at-risk individuals\nbefore adverse events. For predicting individual CVD risk, carotid intima-media\nthickness (CIMT), a noninvasive ultrasound method, has proven to be valuable,\noffering several advantages over CT coronary artery calcium score. However,\neach CIMT examination includes several ultrasound videos, and interpreting each\nof these CIMT videos involves three operations: (1) select three end-diastolic\nultrasound frames (EUF) in the video, (2) localize a region of interest (ROI)\nin each selected frame, and (3) trace the lumen-intima interface and the\nmedia-adventitia interface in each ROI to measure CIMT. These operations are\ntedious, laborious, and time consuming, a serious limitation that hinders the\nwidespread utilization of CIMT in clinical practice. To overcome this\nlimitation, this paper presents a new system to automate CIMT video\ninterpretation. Our extensive experiments demonstrate that the suggested system\nsignificantly outperforms the state-of-the-art methods. The superior\nperformance is attributable to our unified framework based on convolutional\nneural networks (CNNs) coupled with our informative image representation and\neffective post-processing of the CNN outputs, which are uniquely designed for\neach of the above three operations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 15:21:09 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Shin", "Jae Y.", ""], ["Tajbakhsh", "Nima", ""], ["Hurst", "R. Todd", ""], ["Kendall", "Christopher B.", ""], ["Liang", "Jianming", ""]]}, {"id": "1706.00815", "submitter": "Lena Bartell", "authors": "Lena R. Bartell, Lawrence J. Bonassar, Itai Cohen", "title": "A watershed-based algorithm to segment and classify cells in\n  fluorescence microscopy images", "comments": "8 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging assays of cellular function, especially those using fluorescent\nstains, are ubiquitous in the biological and medical sciences. Despite advances\nin computer vision, such images are often analyzed using only manual or\nrudimentary automated processes. Watershed-based segmentation is an effective\ntechnique for identifying objects in images; it outperforms commonly used image\nanalysis methods, but requires familiarity with computer-vision techniques to\nbe applied successfully. In this report, we present and implement a\nwatershed-based image analysis and classification algorithm in a GUI, enabling\na broad set of users to easily understand the algorithm and adjust the\nparameters to their specific needs. As an example, we implement this algorithm\nto find and classify cells in a complex imaging assay for mitochondrial\nfunction. In a second example, we demonstrate a workflow using manual\ncomparisons and receiver operator characteristics to optimize the algorithm\nparameters for finding live and dead cells in a standard viability assay.\nOverall, this watershed-based algorithm is more advanced than traditional\nthresholding and can produce optimized, automated results. By incorporating\nassociated pre-processing steps in the GUI, the algorithm is also easily\nadjusted, rendering it user-friendly.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 18:38:10 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Bartell", "Lena R.", ""], ["Bonassar", "Lawrence J.", ""], ["Cohen", "Itai", ""]]}, {"id": "1706.00826", "submitter": "Sagie Benaim", "authors": "Sagie Benaim and Lior Wolf", "title": "One-Sided Unsupervised Domain Mapping", "comments": "to be published in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unsupervised domain mapping, the learner is given two unmatched datasets\n$A$ and $B$. The goal is to learn a mapping $G_{AB}$ that translates a sample\nin $A$ to the analog sample in $B$. Recent approaches have shown that when\nlearning simultaneously both $G_{AB}$ and the inverse mapping $G_{BA}$,\nconvincing mappings are obtained. In this work, we present a method of learning\n$G_{AB}$ without learning $G_{BA}$. This is done by learning a mapping that\nmaintains the distance between a pair of samples. Moreover, good mappings are\nobtained, even by maintaining the distance between different parts of the same\nsample before and after mapping. We present experimental results that the new\nmethod not only allows for one sided mapping learning, but also leads to\npreferable numerical results over the existing circularity-based constraint.\nOur entire code is made publicly available at\nhttps://github.com/sagiebenaim/DistanceGAN .\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 19:31:30 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 17:15:56 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Benaim", "Sagie", ""], ["Wolf", "Lior", ""]]}, {"id": "1706.00827", "submitter": "Daniel Barath", "authors": "Daniel Barath, Jiri Matas", "title": "Multi-Class Model Fitting by Energy Minimization and Mode-Seeking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general formulation, called Multi-X, for multi-class\nmulti-instance model fitting - the problem of interpreting the input data as a\nmixture of noisy observations originating from multiple instances of multiple\nclasses. We extend the commonly used alpha-expansion-based technique with a new\nmove in the label space. The move replaces a set of labels with the\ncorresponding density mode in the model parameter domain, thus achieving fast\nand robust optimization. Key optimization parameters like the bandwidth of the\nmode seeking are set automatically within the algorithm. Considering that a\ngroup of outliers may form spatially coherent structures in the data, we\npropose a cross-validation-based technique removing statistically insignificant\ninstances. Multi-X outperforms significantly the state-of-the-art on publicly\navailable datasets for diverse problems: multiple plane and rigid motion\ndetection; motion segmentation; simultaneous plane and cylinder fitting; circle\nand line fitting.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 19:32:07 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 08:22:07 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Barath", "Daniel", ""], ["Matas", "Jiri", ""]]}, {"id": "1706.00842", "submitter": "Grzegorz Chlebus", "authors": "Grzegorz Chlebus and Hans Meine and Jan Hendrik Moltz and Andrea\n  Schenk", "title": "Neural Network-Based Automatic Liver Tumor Segmentation With Random\n  Forest-Based Candidate Filtering", "comments": "Submitted to the ISBI 2017 LiTS Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully automatic method employing convolutional neural networks\nbased on the 2D U-net architecture and random forest classifier to solve the\nautomatic liver lesion segmentation problem of the ISBI 2017 Liver Tumor\nSegmentation Challenge (LiTS). In order to constrain the ROI in which the\ntumors could be located, a liver segmentation is performed first. For the organ\nsegmentation, an ensemble of convolutional networks is trained to segment a\nliver using a set of 179 liver CT datasets from liver surgery planning. Inside\nof the liver ROI a neural network, trained using 127 challenge training\ndatasets, identifies tumor candidates, which are subsequently filtered with a\nrandom forest classifier yielding the final tumor segmentation. The evaluation\non the 70 challenge test cases resulted in a mean Dice coefficient of 0.65,\nranking our method in the second place.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 20:33:22 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 07:24:40 GMT"}, {"version": "v3", "created": "Tue, 27 Jun 2017 07:07:20 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Chlebus", "Grzegorz", ""], ["Meine", "Hans", ""], ["Moltz", "Jan Hendrik", ""], ["Schenk", "Andrea", ""]]}, {"id": "1706.00885", "submitter": "Xin Wang", "authors": "Xin Wang, Yujia Luo, Daniel Crankshaw, Alexey Tumanov, Fisher Yu,\n  Joseph E. Gonzalez", "title": "IDK Cascades: Fast Deep Learning by Learning not to Overthink", "comments": "UAI 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep learning have led to substantial increases in prediction\naccuracy but have been accompanied by increases in the cost of rendering\npredictions. We conjecture that fora majority of real-world inputs, the recent\nadvances in deep learning have created models that effectively \"overthink\" on\nsimple inputs. In this paper, we revisit the classic question of building model\ncascades that primarily leverage class asymmetry to reduce cost. We introduce\nthe \"I Don't Know\"(IDK) prediction cascades framework, a general framework to\nsystematically compose a set of pre-trained models to accelerate inference\nwithout a loss in prediction accuracy. We propose two search based methods for\nconstructing cascades as well as a new cost-aware objective within this\nframework. The proposed IDK cascade framework can be easily adopted in the\nexisting model serving systems without additional model re-training. We\nevaluate the proposed techniques on a range of benchmarks to demonstrate the\neffectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 02:29:12 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 21:03:43 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 17:19:04 GMT"}, {"version": "v4", "created": "Wed, 27 Jun 2018 07:11:26 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Wang", "Xin", ""], ["Luo", "Yujia", ""], ["Crankshaw", "Daniel", ""], ["Tumanov", "Alexey", ""], ["Yu", "Fisher", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1706.00893", "submitter": "Nazanin Mehrasa", "authors": "Nazanin Mehrasa, Yatao Zhong, Frederick Tung, Luke Bornn, and Greg\n  Mori", "title": "Learning Person Trajectory Representations for Team Activity Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity analysis in which multiple people interact across a large space is\nchallenging due to the interplay of individual actions and collective group\ndynamics. We propose an end-to-end approach for learning person trajectory\nrepresentations for group activity analysis. The learned representations encode\nrich spatio-temporal dependencies and capture useful motion patterns for\nrecognizing individual events, as well as characteristic group dynamics that\ncan be used to identify groups from their trajectories alone. We develop our\ndeep learning approach in the context of team sports, which provide\nwell-defined sets of events (e.g. pass, shot) and groups of people (teams).\nAnalysis of events and team formations using NHL hockey and NBA basketball\ndatasets demonstrate the generality of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 03:44:42 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Mehrasa", "Nazanin", ""], ["Zhong", "Yatao", ""], ["Tung", "Frederick", ""], ["Bornn", "Luke", ""], ["Mori", "Greg", ""]]}, {"id": "1706.00906", "submitter": "Hu Han", "authors": "Hu Han and Anil K. Jain and Fang Wang and Shiguang Shan and Xilin Chen", "title": "Heterogeneous Face Attribute Estimation: A Deep Multi-Task Learning\n  Approach", "comments": "To appear in the IEEE Trans. Pattern Analysis and Machine\n  Intelligence (final)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face attribute estimation has many potential applications in video\nsurveillance, face retrieval, and social media. While a number of methods have\nbeen proposed for face attribute estimation, most of them did not explicitly\nconsider the attribute correlation and heterogeneity (e.g., ordinal vs. nominal\nand holistic vs. local) during feature representation learning. In this paper,\nwe present a Deep Multi-Task Learning (DMTL) approach to jointly estimate\nmultiple heterogeneous attributes from a single face image. In DMTL, we tackle\nattribute correlation and heterogeneity with convolutional neural networks\n(CNNs) consisting of shared feature learning for all the attributes, and\ncategory-specific feature learning for heterogeneous attributes. We also\nintroduce an unconstrained face database (LFW+), an extension of public-domain\nLFW, with heterogeneous demographic attributes (age, gender, and race) obtained\nvia crowdsourcing. Experimental results on benchmarks with multiple face\nattributes (MORPH II, LFW+, CelebA, LFWA, and FotW) show that the proposed\napproach has superior performance compared to state of the art. Finally,\nevaluations on a public-domain face database (LAP) with a single attribute show\nthat the proposed approach has excellent generalization ability.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 07:37:59 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 08:38:51 GMT"}, {"version": "v3", "created": "Thu, 28 Sep 2017 11:11:57 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Han", "Hu", ""], ["Jain", "Anil K.", ""], ["Wang", "Fang", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1706.00909", "submitter": "Philip H\\\"ausser", "authors": "Philip H\\\"ausser and Alexander Mordvintsev and Daniel Cremers", "title": "Learning by Association - A versatile semi-supervised training method\n  for neural networks", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world scenarios, labeled data for a specific machine learning\ntask is costly to obtain. Semi-supervised training methods make use of\nabundantly available unlabeled data and a smaller number of labeled examples.\nWe propose a new framework for semi-supervised training of deep neural networks\ninspired by learning in humans. \"Associations\" are made from embeddings of\nlabeled samples to those of unlabeled ones and back. The optimization schedule\nencourages correct association cycles that end up at the same class from which\nthe association was started and penalizes wrong associations ending at a\ndifferent class. The implementation is easy to use and can be added to any\nexisting end-to-end training setup. We demonstrate the capabilities of learning\nby association on several data sets and show that it can improve performance on\nclassification tasks tremendously by making use of additionally available\nunlabeled data. In particular, for cases with few labeled data, our training\nscheme outperforms the current state of the art on SVHN.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 08:08:56 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["H\u00e4usser", "Philip", ""], ["Mordvintsev", "Alexander", ""], ["Cremers", "Daniel", ""]]}, {"id": "1706.00917", "submitter": "Siham Tabik", "authors": "Emilio Guirado, Siham Tabik, Domingo Alcaraz-Segura, Javier Cabello,\n  Francisco Herrera", "title": "Deep-Learning Convolutional Neural Networks for scattered shrub\n  detection with Google Earth Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing demand for accurate high-resolution land cover maps in\nmany fields, e.g., in land-use planning and biodiversity conservation.\nDeveloping such maps has been performed using Object-Based Image Analysis\n(OBIA) methods, which usually reach good accuracies, but require a high human\nsupervision and the best configuration for one image can hardly be extrapolated\nto a different image. Recently, the deep learning Convolutional Neural Networks\n(CNNs) have shown outstanding results in object recognition in the field of\ncomputer vision. However, they have not been fully explored yet in land cover\nmapping for detecting species of high biodiversity conservation interest. This\npaper analyzes the potential of CNNs-based methods for plant species detection\nusing free high-resolution Google Earth T M images and provides an objective\ncomparison with the state-of-the-art OBIA-methods. We consider as case study\nthe detection of Ziziphus lotus shrubs, which are protected as a priority\nhabitat under the European Union Habitats Directive. According to our results,\ncompared to OBIA-based methods, the proposed CNN-based detection model, in\ncombination with data-augmentation, transfer learning and pre-processing,\nachieves higher performance with less human intervention and the knowledge it\nacquires in the first image can be transferred to other images, which makes the\ndetection process very fast. The provided methodology can be systematically\nreproduced for other species detection.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 09:13:22 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Guirado", "Emilio", ""], ["Tabik", "Siham", ""], ["Alcaraz-Segura", "Domingo", ""], ["Cabello", "Javier", ""], ["Herrera", "Francisco", ""]]}, {"id": "1706.00931", "submitter": "Xiangbo Shu", "authors": "Xiangbo Shu, Jinhui Tang, Guo-Jun Qi, Yan Song, Zechao Li, and Liyan\n  Zhang", "title": "Concurrence-Aware Long Short-Term Sub-Memories for Person-Person Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Long Short-Term Memory (LSTM) has become a popular choice to model\nindividual dynamics for single-person action recognition due to its ability of\nmodeling the temporal information in various ranges of dynamic contexts.\nHowever, existing RNN models only focus on capturing the temporal dynamics of\nthe person-person interactions by naively combining the activity dynamics of\nindividuals or modeling them as a whole. This neglects the inter-related\ndynamics of how person-person interactions change over time. To this end, we\npropose a novel Concurrence-Aware Long Short-Term Sub-Memories (Co-LSTSM) to\nmodel the long-term inter-related dynamics between two interacting people on\nthe bounding boxes covering people. Specifically, for each frame, two\nsub-memory units store individual motion information, while a concurrent LSTM\nunit selectively integrates and stores inter-related motion information between\ninteracting people from these two sub-memory units via a new co-memory cell.\nExperimental results on the BIT and UT datasets show the superiority of\nCo-LSTSM compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 11:07:23 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Shu", "Xiangbo", ""], ["Tang", "Jinhui", ""], ["Qi", "Guo-Jun", ""], ["Song", "Yan", ""], ["Li", "Zechao", ""], ["Zhang", "Liyan", ""]]}, {"id": "1706.00932", "submitter": "Yusuf Aytar", "authors": "Yusuf Aytar, Carl Vondrick, Antonio Torralba", "title": "See, Hear, and Read: Deep Aligned Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We capitalize on large amounts of readily-available, synchronous data to\nlearn a deep discriminative representations shared across three major natural\nmodalities: vision, sound and language. By leveraging over a year of sound from\nvideo and millions of sentences paired with images, we jointly train a deep\nconvolutional network for aligned representation learning. Our experiments\nsuggest that this representation is useful for several tasks, such as\ncross-modal retrieval or transferring classifiers between modalities. Moreover,\nalthough our network is only trained with image+text and image+sound pairs, it\ncan transfer between text and sound as well, a transfer the network never\nobserved during training. Visualizations of our representation reveal many\nhidden units which automatically emerge to detect concepts, independent of the\nmodality.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 11:11:13 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Aytar", "Yusuf", ""], ["Vondrick", "Carl", ""], ["Torralba", "Antonio", ""]]}, {"id": "1706.00984", "submitter": "Daniel Barath", "authors": "Daniel Barath, Jiri Matas", "title": "Graph-Cut RANSAC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel method for robust estimation, called Graph-Cut RANSAC, GC-RANSAC in\nshort, is introduced. To separate inliers and outliers, it runs the graph-cut\nalgorithm in the local optimization (LO) step which is applied when a\nso-far-the-best model is found. The proposed LO step is conceptually simple,\neasy to implement, globally optimal and efficient. GC-RANSAC is shown\nexperimentally, both on synthesized tests and real image pairs, to be more\ngeometrically accurate than state-of-the-art methods on a range of problems,\ne.g. line fitting, homography, affine transformation, fundamental and essential\nmatrix estimation. It runs in real-time for many problems at a speed\napproximately equal to that of the less accurate alternatives (in milliseconds\non standard CPU).\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 17:52:53 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 08:29:03 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Barath", "Daniel", ""], ["Matas", "Jiri", ""]]}, {"id": "1706.00999", "submitter": "Rodrigo Barros", "authors": "J\\^onatas Wehrmann, Anderson Mattjie, Rodrigo C. Barros", "title": "Order embeddings and character-level convolutions for multimodal\n  alignment", "comments": "7 pages, 5 figures, submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the novel and fast advances in the area of deep neural networks, several\nchallenging image-based tasks have been recently approached by researchers in\npattern recognition and computer vision. In this paper, we address one of these\ntasks, which is to match image content with natural language descriptions,\nsometimes referred as multimodal content retrieval. Such a task is particularly\nchallenging considering that we must find a semantic correspondence between\ncaptions and the respective image, a challenge for both computer vision and\nnatural language processing areas. For such, we propose a novel multimodal\napproach based solely on convolutional neural networks for aligning images with\ntheir captions by directly convolving raw characters. Our proposed\ncharacter-based textual embeddings allow the replacement of both\nword-embeddings and recurrent neural networks for text understanding, saving\nprocessing time and requiring fewer learnable parameters. Our method is based\non the idea of projecting both visual and textual information into a common\nembedding space. For training such embeddings we optimize a contrastive loss\nfunction that is computed to minimize order-violations between images and their\nrespective descriptions. We achieve state-of-the-art performance in the largest\nand most well-known image-text alignment dataset, namely Microsoft COCO, with a\nmethod that is conceptually much simpler and that possesses considerably fewer\nparameters than current approaches.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 20:24:32 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Wehrmann", "J\u00f4natas", ""], ["Mattjie", "Anderson", ""], ["Barros", "Rodrigo C.", ""]]}, {"id": "1706.01000", "submitter": "Xin Yuan", "authors": "Xin Yuan, Raziel Haimi-Cohen", "title": "Image Compression Based on Compressive Sensing: End-to-End Comparison\n  with JPEG", "comments": "17 pages, 13 figures", "journal-ref": "IEEE Transactions on Multimedia 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end image compression system based on compressive\nsensing. The presented system integrates the conventional scheme of compressive\nsampling and reconstruction with quantization and entropy coding. The\ncompression performance, in terms of decoded image quality versus data rate, is\nshown to be comparable with JPEG and significantly better at the low rate\nrange. We study the parameters that influence the system performance, including\n(i) the choice of sensing matrix, (ii) the trade-off between quantization and\ncompression ratio, and (iii) the reconstruction algorithms. We propose an\neffective method to jointly control the quantization step and compression ratio\nin order to achieve near optimal quality at any given bit rate. Furthermore,\nour proposed image compression system can be directly used in the compressive\nsensing camera, e.g. the single pixel camera, to construct a hardware\ncompressive sampling system.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 20:35:30 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 19:34:53 GMT"}, {"version": "v3", "created": "Sun, 19 Jan 2020 03:26:54 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Yuan", "Xin", ""], ["Haimi-Cohen", "Raziel", ""]]}, {"id": "1706.01021", "submitter": "Fuwen Tan", "authors": "Fuwen Tan, Crispin Bernier, Benjamin Cohen, Vicente Ordonez, Connelly\n  Barnes", "title": "Where and Who? Automatic Semantic-Aware Person Composition", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image compositing is a method used to generate realistic yet fake imagery by\ninserting contents from one image to another. Previous work in compositing has\nfocused on improving appearance compatibility of a user selected foreground\nsegment and a background image (i.e. color and illumination consistency). In\nthis work, we instead develop a fully automated compositing model that\nadditionally learns to select and transform compatible foreground segments from\na large collection given only an input image background. To simplify the task,\nwe restrict our problem by focusing on human instance composition, because\nhuman segments exhibit strong correlations with their background and because of\nthe availability of large annotated data. We develop a novel branching\nConvolutional Neural Network (CNN) that jointly predicts candidate person\nlocations given a background image. We then use pre-trained deep feature\nrepresentations to retrieve person instances from a large segment database.\nExperimental results show that our model can generate composite images that\nlook visually convincing. We also develop a user interface to demonstrate the\npotential application of our method.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 03:28:48 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 05:10:30 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Tan", "Fuwen", ""], ["Bernier", "Crispin", ""], ["Cohen", "Benjamin", ""], ["Ordonez", "Vicente", ""], ["Barnes", "Connelly", ""]]}, {"id": "1706.01039", "submitter": "Xiangbo Shu", "authors": "Xiangbo Shu, Jinhui Tang, Zechao Li, Hanjiang Lai, Liyan Zhang, and\n  Shuicheng Yan", "title": "Personalized Age Progression with Bi-level Aging Dictionary Learning", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2705122", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age progression is defined as aesthetically re-rendering the aging face at\nany future age for an individual face. In this work, we aim to automatically\nrender aging faces in a personalized way. Basically, for each age group, we\nlearn an aging dictionary to reveal its aging characteristics (e.g., wrinkles),\nwhere the dictionary bases corresponding to the same index yet from two\nneighboring aging dictionaries form a particular aging pattern cross these two\nage groups, and a linear combination of all these patterns expresses a\nparticular personalized aging process. Moreover, two factors are taken into\nconsideration in the dictionary learning process. First, beyond the aging\ndictionaries, each person may have extra personalized facial characteristics,\ne.g. mole, which are invariant in the aging process. Second, it is challenging\nor even impossible to collect faces of all age groups for a particular person,\nyet much easier and more practical to get face pairs from neighboring age\ngroups. To this end, we propose a novel Bi-level Dictionary Learning based\nPersonalized Age Progression (BDL-PAP) method. Here, bi-level dictionary\nlearning is formulated to learn the aging dictionaries based on face pairs from\nneighboring age groups. Extensive experiments well demonstrate the advantages\nof the proposed BDL-PAP over other state-of-the-arts in term of personalized\nage progression, as well as the performance gain for cross-age face\nverification by synthesizing aging faces.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 07:08:16 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Shu", "Xiangbo", ""], ["Tang", "Jinhui", ""], ["Li", "Zechao", ""], ["Lai", "Hanjiang", ""], ["Zhang", "Liyan", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1706.01040", "submitter": "Huimin Lu", "authors": "Huimin Lu, Yujie Li, Min Chen, Hyoungseop Kim, Seiichi Serikawa", "title": "Brain Intelligence: Go Beyond Artificial Intelligence", "comments": "15 pages, Mobile Networks and Applications, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial intelligence (AI) is an important technology that supports daily\nsocial life and economic activities. It contributes greatly to the sustainable\ngrowth of Japan's economy and solves various social problems. In recent years,\nAI has attracted attention as a key for growth in developed countries such as\nEurope and the United States and developing countries such as China and India.\nThe attention has been focused mainly on developing new artificial intelligence\ninformation communication technology (ICT) and robot technology (RT). Although\nrecently developed AI technology certainly excels in extracting certain\npatterns, there are many limitations. Most ICT models are overly dependent on\nbig data, lack a self-idea function, and are complicated. In this paper, rather\nthan merely developing next-generation artificial intelligence technology, we\naim to develop a new concept of general-purpose intelligence cognition\ntechnology called Beyond AI. Specifically, we plan to develop an intelligent\nlearning model called Brain Intelligence (BI) that generates new ideas about\nevents without having experienced them by using artificial life with an imagine\nfunction. We will also conduct demonstrations of the developed BI intelligence\nlearning model on automatic driving, precision medical care, and industrial\nrobots.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 08:16:03 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Lu", "Huimin", ""], ["Li", "Yujie", ""], ["Chen", "Min", ""], ["Kim", "Hyoungseop", ""], ["Serikawa", "Seiichi", ""]]}, {"id": "1706.01061", "submitter": "Zhifeng Li", "authors": "Hao Wang, Zhifeng Li, Xing Ji, and Yitong Wang", "title": "Face R-CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faster R-CNN is one of the most representative and successful methods for\nobject detection, and has been becoming increasingly popular in various\nobjection detection applications. In this report, we propose a robust deep face\ndetection approach based on Faster R-CNN. In our approach, we exploit several\nnew techniques including new multi-task loss function design, online hard\nexample mining, and multi-scale training strategy to improve Faster R-CNN in\nmultiple aspects. The proposed approach is well suited for face detection, so\nwe call it Face R-CNN. Extensive experiments are conducted on two most popular\nand challenging face detection benchmarks, FDDB and WIDER FACE, to demonstrate\nthe superiority of the proposed approach over state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 11:53:57 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Wang", "Hao", ""], ["Li", "Zhifeng", ""], ["Ji", "Xing", ""], ["Wang", "Yitong", ""]]}, {"id": "1706.01115", "submitter": "Yong Khoo", "authors": "Yong Khoo, Seo-hyeon Keun", "title": "A Random-Fern based Feature Approach for Image Matching", "comments": "Computer Imaging, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image or object recognition is an important task in computer vision. With the\nhight-speed processing power on modern platforms and the availability of mobile\nphones everywhere, millions of photos are uploaded to the internet per minute,\nit is critical to establish a generic framework for fast and accurate image\nprocessing for automatic recognition and information retrieval. In this paper,\nwe proposed an efficient image recognition and matching method that is\noriginally derived from Naive Bayesian classification method to construct a\nprobabilistic model. Our method support real-time performance and have very\nhigh ability to distinguish similar images with high details. Experiments are\nconducted together with intensive comparison with state-of-the-arts on image\nmatching, such as Ferns recognition and SIFT recognition. The results\ndemonstrate satisfactory performance.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 17:41:57 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Khoo", "Yong", ""], ["Keun", "Seo-hyeon", ""]]}, {"id": "1706.01148", "submitter": "Gerda Bortsova", "authors": "Gerda Bortsova, Gijs van Tulder, Florian Dubost, Tingying Peng, Nassir\n  Navab, Aad van der Lugt, Daniel Bos, Marleen de Bruijne", "title": "Segmentation of Intracranial Arterial Calcification with Deeply\n  Supervised Residual Dropout Networks", "comments": "Accepted for MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intracranial carotid artery calcification (ICAC) is a major risk factor for\nstroke, and might contribute to dementia and cognitive decline. Reliance on\ntime-consuming manual annotation of ICAC hampers much demanded further research\ninto the relationship between ICAC and neurological diseases. Automation of\nICAC segmentation is therefore highly desirable, but difficult due to the\nproximity of the lesions to bony structures with a similar attenuation\ncoefficient. In this paper, we propose a method for automatic segmentation of\nICAC; the first to our knowledge. Our method is based on a 3D fully\nconvolutional neural network that we extend with two regularization techniques.\nFirstly, we use deep supervision (hidden layers supervision) to encourage\ndiscriminative features in the hidden layers. Secondly, we augment the network\nwith skip connections, as in the recently developed ResNet, and dropout layers,\ninserted in a way that skip connections circumvent them. We investigate the\neffect of skip connections and dropout. In addition, we propose a simple\nproblem-specific modification of the network objective function that restricts\nthe focus to the most important image regions and simplifies the optimization.\nWe train and validate our model using 882 CT scans and test on 1,000. Our\nregularization techniques and objective improve the average Dice score by 7.1%,\nyielding an average Dice of 76.2% and 97.7% correlation between predicted ICAC\nvolumes and manual annotations.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 21:13:12 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Bortsova", "Gerda", ""], ["van Tulder", "Gijs", ""], ["Dubost", "Florian", ""], ["Peng", "Tingying", ""], ["Navab", "Nassir", ""], ["van der Lugt", "Aad", ""], ["Bos", "Daniel", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1706.01159", "submitter": "Vladislav Samsonov", "authors": "Vladislav Samsonov", "title": "Deep Frame Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This work presents a supervised learning based approach to the computer\nvision problem of frame interpolation. The presented technique could also be\nused in the cartoon animations since drawing each individual frame consumes a\nnoticeable amount of time. The most existing solutions to this problem use\nunsupervised methods and focus only on real life videos with already high frame\nrate. However, the experiments show that such methods do not work as well when\nthe frame rate becomes low and object displacements between frames becomes\nlarge. This is due to the fact that interpolation of the large displacement\nmotion requires knowledge of the motion structure thus the simple techniques\nsuch as frame averaging start to fail. In this work the deep convolutional\nneural network is used to solve the frame interpolation problem. In addition,\nit is shown that incorporating the prior information such as optical flow\nimproves the interpolation quality significantly.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 23:22:30 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 09:08:58 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Samsonov", "Vladislav", ""]]}, {"id": "1706.01171", "submitter": "Rao Muhammad Anwer", "authors": "Rao Muhammad Anwer, Fahad Shahbaz Khan, Joost van de Weijer, Matthieu\n  Molinier, Jorma Laaksonen", "title": "Binary Patterns Encoded Convolutional Neural Networks for Texture\n  Recognition and Remote Sensing Scene Classification", "comments": "To appear in ISPRS Journal of Photogrammetry and Remote Sensing", "journal-ref": null, "doi": "10.1016/j.isprsjprs.2018.01.023", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing discriminative powerful texture features robust to realistic\nimaging conditions is a challenging computer vision problem with many\napplications, including material recognition and analysis of satellite or\naerial imagery. In the past, most texture description approaches were based on\ndense orderless statistical distribution of local features. However, most\nrecent approaches to texture recognition and remote sensing scene\nclassification are based on Convolutional Neural Networks (CNNs). The d facto\npractice when learning these CNN models is to use RGB patches as input with\ntraining performed on large amounts of labeled data (ImageNet). In this paper,\nwe show that Binary Patterns encoded CNN models, codenamed TEX-Nets, trained\nusing mapped coded images with explicit texture information provide\ncomplementary information to the standard RGB deep models. Additionally, two\ndeep architectures, namely early and late fusion, are investigated to combine\nthe texture and color information. To the best of our knowledge, we are the\nfirst to investigate Binary Patterns encoded CNNs and different deep network\nfusion architectures for texture recognition and remote sensing scene\nclassification. We perform comprehensive experiments on four texture\nrecognition datasets and four remote sensing scene classification benchmarks:\nUC-Merced with 21 scene categories, WHU-RS19 with 19 scene classes, RSSCN7 with\n7 categories and the recently introduced large scale aerial image dataset (AID)\nwith 30 aerial scene types. We demonstrate that TEX-Nets provide complementary\ninformation to standard RGB deep model of the same network architecture. Our\nlate fusion TEX-Net architecture always improves the overall performance\ncompared to the standard RGB network on both recognition problems. Our final\ncombination outperforms the state-of-the-art without employing fine-tuning or\nensemble of RGB network architectures.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 00:53:06 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 10:27:27 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Anwer", "Rao Muhammad", ""], ["Khan", "Fahad Shahbaz", ""], ["van de Weijer", "Joost", ""], ["Molinier", "Matthieu", ""], ["Laaksonen", "Jorma", ""]]}, {"id": "1706.01209", "submitter": "You Hao", "authors": "Hanlin Mo and You Hao and Shirui Li and Hua Li", "title": "A Kind of Affine Weighted Moment Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new kind of geometric invariants is proposed in this paper, which is called\naffine weighted moment invariant (AWMI). By combination of local affine\ndifferential invariants and a framework of global integral, they can more\neffectively extract features of images and help to increase the number of\nlow-order invariants and to decrease the calculating cost. The experimental\nresults show that AWMIs have good stability and distinguishability and achieve\nbetter results in image retrieval than traditional moment invariants. An\nextension to 3D is straightforward.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 06:37:07 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 00:43:58 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Mo", "Hanlin", ""], ["Hao", "You", ""], ["Li", "Shirui", ""], ["Li", "Hua", ""]]}, {"id": "1706.01231", "submitter": "Jingkuan Song Dr.", "authors": "Jingkuan Song, Zhao Guo, Lianli Gao, Wu Liu, Dongxiang Zhang, Heng Tao\n  Shen", "title": "Hierarchical LSTM with Adjusted Temporal Attention for Video Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress has been made in using attention based encoder-decoder\nframework for video captioning. However, most existing decoders apply the\nattention mechanism to every generated word including both visual words (e.g.,\n\"gun\" and \"shooting\") and non-visual words (e.g. \"the\", \"a\"). However, these\nnon-visual words can be easily predicted using natural language model without\nconsidering visual signals or attention. Imposing attention mechanism on\nnon-visual words could mislead and decrease the overall performance of video\ncaptioning. To address this issue, we propose a hierarchical LSTM with adjusted\ntemporal attention (hLSTMat) approach for video captioning. Specifically, the\nproposed framework utilizes the temporal attention for selecting specific\nframes to predict the related words, while the adjusted temporal attention is\nfor deciding whether to depend on the visual information or the language\ncontext information. Also, a hierarchical LSTMs is designed to simultaneously\nconsider both low-level visual information and high-level language context\ninformation to support the video caption generation. To demonstrate the\neffectiveness of our proposed framework, we test our method on two prevalent\ndatasets: MSVD and MSR-VTT, and experimental results show that our approach\noutperforms the state-of-the-art methods on both two datasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 08:09:20 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Song", "Jingkuan", ""], ["Guo", "Zhao", ""], ["Gao", "Lianli", ""], ["Liu", "Wu", ""], ["Zhang", "Dongxiang", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1706.01237", "submitter": "Dong Li", "authors": "Dong Li, Hsin-Ying Lee, Jia-Bin Huang, Shengjin Wang, and Ming-Hsuan\n  Yang", "title": "Learning Structured Semantic Embeddings for Visual Recognition", "comments": "9 pages, 6 figures, 5 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous embedding models have been recently explored to incorporate semantic\nknowledge into visual recognition. Existing methods typically focus on\nminimizing the distance between the corresponding images and texts in the\nembedding space but do not explicitly optimize the underlying structure. Our\nkey observation is that modeling the pairwise image-image relationship improves\nthe discrimination ability of the embedding model. In this paper, we propose\nthe structured discriminative and difference constraints to learn\nvisual-semantic embeddings. First, we exploit the discriminative constraints to\ncapture the intra- and inter-class relationships of image embeddings. The\ndiscriminative constraints encourage separability for image instances of\ndifferent classes. Second, we align the difference vector between a pair of\nimage embeddings with that of the corresponding word embeddings. The difference\nconstraints help regularize image embeddings to preserve the semantic\nrelationships among word embeddings. Extensive evaluations demonstrate the\neffectiveness of the proposed structured embeddings for single-label\nclassification, multi-label classification, and zero-shot recognition.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 08:43:23 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Li", "Dong", ""], ["Lee", "Hsin-Ying", ""], ["Huang", "Jia-Bin", ""], ["Wang", "Shengjin", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1706.01307", "submitter": "Benjamin Graham", "authors": "Benjamin Graham, Laurens van der Maaten", "title": "Submanifold Sparse Convolutional Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional network are the de-facto standard for analysing spatio-temporal\ndata such as images, videos, 3D shapes, etc. Whilst some of this data is\nnaturally dense (for instance, photos), many other data sources are inherently\nsparse. Examples include pen-strokes forming on a piece of paper, or (colored)\n3D point clouds that were obtained using a LiDAR scanner or RGB-D camera.\nStandard \"dense\" implementations of convolutional networks are very inefficient\nwhen applied on such sparse data. We introduce a sparse convolutional operation\ntailored to processing sparse data that differs from prior work on sparse\nconvolutional networks in that it operates strictly on submanifolds, rather\nthan \"dilating\" the observation with every layer in the network. Our empirical\nanalysis of the resulting submanifold sparse convolutional networks shows that\nthey perform on par with state-of-the-art methods whilst requiring\nsubstantially less computation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 13:25:24 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Graham", "Benjamin", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1706.01322", "submitter": "Alexander Kuhnle", "authors": "Alexander Kuhnle and Ann Copestake", "title": "Deep learning evaluation using deep linguistic processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss problems with the standard approaches to evaluation for tasks like\nvisual question answering, and argue that artificial data can be used to\naddress these as a complement to current practice. We demonstrate that with the\nhelp of existing 'deep' linguistic processing technology we are able to create\nchallenging abstract datasets, which enable us to investigate the language\nunderstanding abilities of multimodal deep learning models in detail, as\ncompared to a single performance value on a static and monolithic dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 13:53:56 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 10:37:02 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Kuhnle", "Alexander", ""], ["Copestake", "Ann", ""]]}, {"id": "1706.01340", "submitter": "Robin Ruede", "authors": "Robin Ruede, Markus M\\\"uller, Sebastian St\\\"uker, Alex Waibel", "title": "Yeah, Right, Uh-Huh: A Deep Learning Backchannel Predictor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.HC cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using supporting backchannel (BC) cues can make human-computer interaction\nmore social. BCs provide a feedback from the listener to the speaker indicating\nto the speaker that he is still listened to. BCs can be expressed in different\nways, depending on the modality of the interaction, for example as gestures or\nacoustic cues. In this work, we only considered acoustic cues. We are proposing\nan approach towards detecting BC opportunities based on acoustic input features\nlike power and pitch. While other works in the field rely on the use of a\nhand-written rule set or specialized features, we made use of artificial neural\nnetworks. They are capable of deriving higher order features from input\nfeatures themselves. In our setup, we first used a fully connected feed-forward\nnetwork to establish an updated baseline in comparison to our previously\nproposed setup. We also extended this setup by the use of Long Short-Term\nMemory (LSTM) networks which have shown to outperform feed-forward based setups\non various tasks. Our best system achieved an F1-Score of 0.37 using power and\npitch features. Adding linguistic information using word2vec, the score\nincreased to 0.39.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 17:05:26 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Ruede", "Robin", ""], ["M\u00fcller", "Markus", ""], ["St\u00fcker", "Sebastian", ""], ["Waibel", "Alex", ""]]}, {"id": "1706.01380", "submitter": "Harish RaviPrakash", "authors": "Harish RaviPrakash, Milena Korostenskaja, Eduardo Castillo, Ki Lee,\n  James Baumgartner, Ulas Bagci", "title": "Automatic Response Assessment in Regions of Language Cortex in Epilepsy\n  Patients Using ECoG-based Functional Mapping and Machine Learning", "comments": "This paper will appear in the Proceedings of IEEE International\n  Conference on Systems, Man and Cybernetics (SMC) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate localization of brain regions responsible for language and cognitive\nfunctions in Epilepsy patients should be carefully determined prior to surgery.\nElectrocorticography (ECoG)-based Real Time Functional Mapping (RTFM) has been\nshown to be a safer alternative to the electrical cortical stimulation mapping\n(ESM), which is currently the clinical/gold standard. Conventional methods for\nanalyzing RTFM signals are based on statistical comparison of signal power at\ncertain frequency bands. Compared to gold standard (ESM), they have limited\naccuracies when assessing channel responses.\n  In this study, we address the accuracy limitation of the current RTFM signal\nestimation methods by analyzing the full frequency spectrum of the signal and\nreplacing signal power estimation methods with machine learning algorithms,\nspecifically random forest (RF), as a proof of concept. We train RF with power\nspectral density of the time-series RTFM signal in supervised learning\nframework where ground truth labels are obtained from the ESM. Results obtained\nfrom RTFM of six adult patients in a strictly controlled experimental setup\nreveal the state of the art detection accuracy of $\\approx 78\\%$ for the\nlanguage comprehension task, an improvement of $23\\%$ over the conventional\nRTFM estimation method. To the best of our knowledge, this is the first study\nexploring the use of machine learning approaches for determining RTFM signal\ncharacteristics, and using the whole-frequency band for better region\nlocalization. Our results demonstrate the feasibility of machine learning based\nRTFM signal analysis method over the full spectrum to be a clinical routine in\nthe near future.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 16:50:04 GMT"}, {"version": "v2", "created": "Sun, 6 Aug 2017 21:05:14 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["RaviPrakash", "Harish", ""], ["Korostenskaja", "Milena", ""], ["Castillo", "Eduardo", ""], ["Lee", "Ki", ""], ["Baumgartner", "James", ""], ["Bagci", "Ulas", ""]]}, {"id": "1706.01396", "submitter": "Jinsung Yoon", "authors": "Jinsung Yoon, William R. Zame, Mihaela van der Schaar", "title": "ToPs: Ensemble Learning with Trees of Predictors", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": "10.1109/TSP.2018.2807402", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to ensemble learning. Our approach constructs a\ntree of subsets of the feature space and associates a predictor (predictive\nmodel) - determined by training one of a given family of base learners on an\nendogenously determined training set - to each node of the tree; we call the\nresulting object a tree of predictors. The (locally) optimal tree of predictors\nis derived recursively; each step involves jointly optimizing the split of the\nterminal nodes of the previous tree and the choice of learner and training set\n(hence predictor) for each set in the split. The feature vector of a new\ninstance determines a unique path through the optimal tree of predictors; the\nfinal prediction aggregates the predictions of the predictors along this path.\nWe derive loss bounds for the final predictor in terms of the Rademacher\ncomplexity of the base learners. We report the results of a number of\nexperiments on a variety of datasets, showing that our approach provides\nstatistically significant improvements over state-of-the-art machine learning\nalgorithms, including various ensemble learning methods. Our approach works\nbecause it allows us to endogenously create more complex learners - when needed\n- and endogenously match both the learner and the training set to the\ncharacteristics of the dataset while still avoiding over-fitting.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 16:08:11 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 14:50:36 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Yoon", "Jinsung", ""], ["Zame", "William R.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1706.01406", "submitter": "Alessandro Aimar", "authors": "Alessandro Aimar, Hesham Mostafa, Enrico Calabrese, Antonio\n  Rios-Navarro, Ricardo Tapiador-Morales, Iulia-Alexandra Lungu, Moritz B.\n  Milde, Federico Corradi, Alejandro Linares-Barranco, Shih-Chii Liu, Tobi\n  Delbruck", "title": "NullHop: A Flexible Convolutional Neural Network Accelerator Based on\n  Sparse Representations of Feature Maps", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2018.2852335", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have become the dominant neural network\narchitecture for solving many state-of-the-art (SOA) visual processing tasks.\nEven though Graphical Processing Units (GPUs) are most often used in training\nand deploying CNNs, their power efficiency is less than 10 GOp/s/W for\nsingle-frame runtime inference. We propose a flexible and efficient CNN\naccelerator architecture called NullHop that implements SOA CNNs useful for\nlow-power and low-latency application scenarios. NullHop exploits the sparsity\nof neuron activations in CNNs to accelerate the computation and reduce memory\nrequirements. The flexible architecture allows high utilization of available\ncomputing resources across kernel sizes ranging from 1x1 to 7x7. NullHop can\nprocess up to 128 input and 128 output feature maps per layer in a single pass.\nWe implemented the proposed architecture on a Xilinx Zynq FPGA platform and\npresent results showing how our implementation reduces external memory\ntransfers and compute time in five different CNNs ranging from small ones up to\nthe widely known large VGG16 and VGG19 CNNs. Post-synthesis simulations using\nMentor Modelsim in a 28nm process with a clock frequency of 500 MHz show that\nthe VGG19 network achieves over 450 GOp/s. By exploiting sparsity, NullHop\nachieves an efficiency of 368%, maintains over 98% utilization of the MAC\nunits, and achieves a power efficiency of over 3TOp/s/W in a core area of\n6.3mm$^2$. As further proof of NullHop's usability, we interfaced its FPGA\nimplementation with a neuromorphic event camera for real time interactive\ndemonstrations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 16:20:24 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 10:05:33 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Aimar", "Alessandro", ""], ["Mostafa", "Hesham", ""], ["Calabrese", "Enrico", ""], ["Rios-Navarro", "Antonio", ""], ["Tapiador-Morales", "Ricardo", ""], ["Lungu", "Iulia-Alexandra", ""], ["Milde", "Moritz B.", ""], ["Corradi", "Federico", ""], ["Linares-Barranco", "Alejandro", ""], ["Liu", "Shih-Chii", ""], ["Delbruck", "Tobi", ""]]}, {"id": "1706.01433", "submitter": "Nicholas Watters", "authors": "Nicholas Watters, Andrea Tacchetti, Theophane Weber, Razvan Pascanu,\n  Peter Battaglia, Daniel Zoran", "title": "Visual Interaction Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From just a glance, humans can make rich predictions about the future state\nof a wide range of physical systems. On the other hand, modern approaches from\nengineering, robotics, and graphics are often restricted to narrow domains and\nrequire direct measurements of the underlying states. We introduce the Visual\nInteraction Network, a general-purpose model for learning the dynamics of a\nphysical system from raw visual observations. Our model consists of a\nperceptual front-end based on convolutional neural networks and a dynamics\npredictor based on interaction networks. Through joint training, the perceptual\nfront-end learns to parse a dynamic visual scene into a set of factored latent\nobject representations. The dynamics predictor learns to roll these states\nforward in time by computing their interactions and dynamics, producing a\npredicted physical trajectory of arbitrary length. We found that from just six\ninput video frames the Visual Interaction Network can generate accurate future\ntrajectories of hundreds of time steps on a wide range of physical systems. Our\nmodel can also be applied to scenes with invisible objects, inferring their\nfuture states from their effects on the visible objects, and can implicitly\ninfer the unknown mass of objects. Our results demonstrate that the perceptual\nmodule and the object-based dynamics predictor module can induce factored\nlatent representations that support accurate dynamical predictions. This work\nopens new opportunities for model-based decision-making and planning from raw\nsensory observations in complex physical environments.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 17:38:52 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Watters", "Nicholas", ""], ["Tacchetti", "Andrea", ""], ["Weber", "Theophane", ""], ["Pascanu", "Razvan", ""], ["Battaglia", "Peter", ""], ["Zoran", "Daniel", ""]]}, {"id": "1706.01487", "submitter": "Suman Ghosh", "authors": "Suman K.Ghosh, Ernest Valveny, Andrew D. Bagdanov", "title": "Visual attention models for scene text recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an approach to lexicon-free recognition of text in\nscene images. Our approach relies on a LSTM-based soft visual attention model\nlearned from convolutional features. A set of feature vectors are derived from\nan intermediate convolutional layer corresponding to different areas of the\nimage. This permits encoding of spatial information into the image\nrepresentation. In this way, the framework is able to learn how to selectively\nfocus on different parts of the image. At every time step the recognizer emits\none character using a weighted combination of the convolutional feature vectors\naccording to the learned attention model. Training can be done end-to-end using\nonly word level annotations. In addition, we show that modifying the beam\nsearch algorithm by integrating an explicit language model leads to\nsignificantly better recognition results. We validate the performance of our\napproach on standard SVT and ICDAR'03 scene text datasets, showing\nstate-of-the-art performance in unconstrained text recognition.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 18:34:37 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Ghosh", "Suman K.", ""], ["Valveny", "Ernest", ""], ["Bagdanov", "Andrew D.", ""]]}, {"id": "1706.01509", "submitter": "Prudhvi Raj Dachapally", "authors": "Prudhvi Raj Dachapally", "title": "Facial Emotion Detection Using Convolutional Neural Networks and\n  Representational Autoencoder Units", "comments": "6 pages, 8 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion being a subjective thing, leveraging knowledge and science behind\nlabeled data and extracting the components that constitute it, has been a\nchallenging problem in the industry for many years. With the evolution of deep\nlearning in computer vision, emotion recognition has become a widely-tackled\nresearch problem. In this work, we propose two independent methods for this\nvery task. The first method uses autoencoders to construct a unique\nrepresentation of each emotion, while the second method is an 8-layer\nconvolutional neural network (CNN). These methods were trained on the\nposed-emotion dataset (JAFFE), and to test their robustness, both the models\nwere also tested on 100 random images from the Labeled Faces in the Wild (LFW)\ndataset, which consists of images that are candid than posed. The results show\nthat with more fine-tuning and depth, our CNN model can outperform the\nstate-of-the-art methods for emotion recognition. We also propose some exciting\nideas for expanding the concept of representational autoencoders to improve\ntheir performance.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 19:25:34 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Dachapally", "Prudhvi Raj", ""]]}, {"id": "1706.01531", "submitter": "Roghaiyeh Soleimani", "authors": "Roghayeh Soleymani, Eric Granger, Giorgio Fumera", "title": "Progressive Boosting for Class Imbalance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition applications often suffer from skewed data distributions\nbetween classes, which may vary during operations w.r.t. the design data.\nTwo-class classification systems designed using skewed data tend to recognize\nthe majority class better than the minority class of interest. Several\ndata-level techniques have been proposed to alleviate this issue by up-sampling\nminority samples or under-sampling majority samples. However, some informative\nsamples may be neglected by random under-sampling and adding synthetic positive\nsamples through up-sampling adds to training complexity. In this paper, a new\nensemble learning algorithm called Progressive Boosting (PBoost) is proposed\nthat progressively inserts uncorrelated groups of samples into a Boosting\nprocedure to avoid loss of information while generating a diverse pool of\nclassifiers. Base classifiers in this ensemble are generated from one iteration\nto the next, using subsets from a validation set that grows gradually in size\nand imbalance. Consequently, PBoost is more robust to unknown and variable\nlevels of skew in operational data, and has lower computation complexity than\nBoosting ensembles in literature. In PBoost, a new loss factor is proposed to\navoid bias of performance towards the negative class. Using this loss factor,\nthe weight update of samples and classifier contribution in final predictions\nare set based on the ability to recognize both classes. Using the proposed loss\nfactor instead of standard accuracy can avoid biasing performance in any\nBoosting ensemble. The proposed approach was validated and compared using\nsynthetic data, videos from the FIA dataset that emulates face\nre-identification applications, and KEEL collection of datasets. Results show\nthat PBoost can outperform state of the art techniques in terms of both\naccuracy and complexity over different levels of imbalance and overlap between\nclasses.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 20:32:55 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Soleymani", "Roghayeh", ""], ["Granger", "Eric", ""], ["Fumera", "Giorgio", ""]]}, {"id": "1706.01553", "submitter": "Paul Amayo Mr", "authors": "Paul Amayo, Pedro Pinies, Lina M. Paz and Paul Newman", "title": "Geometric Multi-Model Fitting with a Convex Relaxation Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to fit and segment multi-structural data via convex\nrelaxation. Unlike greedy methods --which maximise the number of inliers-- this\napproach efficiently searches for a soft assignment of points to models by\nminimising the energy of the overall classification. Our approach is similar to\nstate-of-the-art energy minimisation techniques which use a global energy.\nHowever, we deal with the scaling factor (as the number of models increases) of\nthe original combinatorial problem by relaxing the solution. This relaxation\nbrings two advantages: first, by operating in the continuous domain we can\nparallelize the calculations. Second, it allows for the use of different\nmetrics which results in a more general formulation.\n  We demonstrate the versatility of our technique on two different problems of\nestimating structure from images: plane extraction from RGB-D data and\nhomography estimation from pairs of images. In both cases, we report accurate\nresults on publicly available datasets, in most of the cases outperforming the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 22:49:59 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Amayo", "Paul", ""], ["Pinies", "Pedro", ""], ["Paz", "Lina M.", ""], ["Newman", "Paul", ""]]}, {"id": "1706.01554", "submitter": "Jiasen Lu", "authors": "Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, Dhruv Batra", "title": "Best of Both Worlds: Transferring Knowledge from Discriminative Learning\n  to a Generative Visual Dialog Model", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel training framework for neural sequence models,\nparticularly for grounded dialog generation. The standard training paradigm for\nthese models is maximum likelihood estimation (MLE), or minimizing the\ncross-entropy of the human responses. Across a variety of domains, a recurring\nproblem with MLE trained generative neural dialog models (G) is that they tend\nto produce 'safe' and generic responses (\"I don't know\", \"I can't tell\"). In\ncontrast, discriminative dialog models (D) that are trained to rank a list of\ncandidate human responses outperform their generative counterparts; in terms of\nautomatic metrics, diversity, and informativeness of the responses. However, D\nis not useful in practice since it cannot be deployed to have real\nconversations with users.\n  Our work aims to achieve the best of both worlds -- the practical usefulness\nof G and the strong performance of D -- via knowledge transfer from D to G. Our\nprimary contribution is an end-to-end trainable generative visual dialog model,\nwhere G receives gradients from D as a perceptual (not adversarial) loss of the\nsequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS)\napproximation to the discrete distribution -- specifically, an RNN augmented\nwith a sequence of GS samplers, coupled with the straight-through gradient\nestimator to enable end-to-end differentiability. We also introduce a stronger\nencoder for visual dialog, and employ a self-attention mechanism for answer\nencoding along with a metric learning loss to aid D in better capturing\nsemantic similarities in answer responses. Overall, our proposed model\noutperforms state-of-the-art on the VisDial dataset by a significant margin\n(2.67% on recall@10). The source code can be downloaded from\nhttps://github.com/jiasenlu/visDial.pytorch.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 22:50:37 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 20:27:07 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Lu", "Jiasen", ""], ["Kannan", "Anitha", ""], ["Yang", "Jianwei", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1706.01580", "submitter": "Hasnain Vohra", "authors": "Hasnain Vohra, Maxim Bazik, Matthew Antone, Joseph Mundy and William\n  Stephenson", "title": "Global-Local Airborne Mapping (GLAM): Reconstructing a City from Aerial\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular visual SLAM has become an attractive practical approach for robot\nlocalization and 3D environment mapping, since cameras are small, lightweight,\ninexpensive, and produce high-rate, high-resolution data streams. Although\nnumerous robust tools have been developed, most existing systems are designed\nto operate in terrestrial environments and at relatively small scale (a few\nthousand frames) due to constraints on computation and storage.\n  In this paper, we present a feature-based visual SLAM system for aerial video\nwhose simple design permits near real-time operation, and whose scalability\npermits large-area mapping using tens of thousands of frames, all on a single\nconventional computer. Our approach consists of two parallel threads: the first\nincrementally creates small locally consistent submaps and estimates camera\nposes at video rate; the second aligns these submaps with one another to\nproduce a single globally consistent map via factor graph optimization over\nboth poses and landmarks. Scale drift is minimized through the use of\n7-degree-of-freedom similarity transformations during submap alignment.\n  We quantify our system's performance on both simulated and real data sets,\nand demonstrate city-scale map reconstruction accurate to within 2 meters using\nnearly 90,000 aerial video frames - to our knowledge, the largest and fastest\nsuch reconstruction to date.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 01:54:27 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 05:39:15 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Vohra", "Hasnain", ""], ["Bazik", "Maxim", ""], ["Antone", "Matthew", ""], ["Mundy", "Joseph", ""], ["Stephenson", "William", ""]]}, {"id": "1706.01604", "submitter": "Manolis Tsakiris", "authors": "Manolis C. Tsakiris and Rene Vidal", "title": "Hyperplane Clustering Via Dual Principal Component Pursuit", "comments": null, "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, PMLR 70:3472-3481, 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the theoretical analysis of a recently proposed single subspace\nlearning algorithm, called Dual Principal Component Pursuit (DPCP), to the case\nwhere the data are drawn from of a union of hyperplanes. To gain insight into\nthe properties of the $\\ell_1$ non-convex problem associated with DPCP, we\ndevelop a geometric analysis of a closely related continuous optimization\nproblem. Then transferring this analysis to the discrete problem, our results\nstate that as long as the hyperplanes are sufficiently separated, the dominant\nhyperplane is sufficiently dominant and the points are uniformly distributed\ninside the associated hyperplanes, then the non-convex DPCP problem has a\nunique global solution, equal to the normal vector of the dominant hyperplane.\nThis suggests the correctness of a sequential hyperplane learning algorithm\nbased on DPCP. A thorough experimental evaluation reveals that hyperplane\nlearning schemes based on DPCP dramatically improve over the state-of-the-art\nmethods for the case of synthetic data, while are competitive to the\nstate-of-the-art in the case of 3D plane clustering for Kinect data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 04:27:24 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 19:20:23 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Tsakiris", "Manolis C.", ""], ["Vidal", "Rene", ""]]}, {"id": "1706.01644", "submitter": "Shuo Li", "authors": "Liansheng Wang, Shusheng Li, Shuo Li", "title": "Volume Calculation of CT lung Lesions based on Halton Low-discrepancy\n  Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volume calculation from the Computed Tomography (CT) lung lesions data is a\nsignificant parameter for clinical diagnosis. The volume is widely used to\nassess the severity of the lung nodules and track its progression, however, the\naccuracy and efficiency of previous studies are not well achieved for clinical\nuses. It remains to be a challenging task due to its tight attachment to the\nlung wall, inhomogeneous background noises and large variations in sizes and\nshape. In this paper, we employ Halton low-discrepancy sequences to calculate\nthe volume of the lung lesions. The proposed method directly compute the volume\nwithout the procedure of three-dimension (3D) model reconstruction and surface\ntriangulation, which significantly improves the efficiency and reduces the\ncomplexity. The main steps of the proposed method are: (1) generate a certain\nnumber of random points in each slice using Halton low-discrepancy sequences\nand calculate the lesion area of each slice through the proportion; (2) obtain\nthe volume by integrating the areas in the sagittal direction. In order to\nevaluate our proposed method, the experiments were conducted on the sufficient\ndata sets with different size of lung lesions. With the uniform distribution of\nrandom points, our proposed method achieves more accurate results compared with\nother methods, which demonstrates the robustness and accuracy for the volume\ncalculation of CT lung lesions. In addition, our proposed method is easy to\nfollow and can be extensively applied to other applications, e.g., volume\ncalculation of liver tumor, atrial wall aneurysm, etc.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 07:52:04 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Wang", "Liansheng", ""], ["Li", "Shusheng", ""], ["Li", "Shuo", ""]]}, {"id": "1706.01649", "submitter": "Daniel Barath", "authors": "Daniel Barath, Tekla Toth, Levente Hajder", "title": "A Minimal Solution for Two-view Focal-length Estimation using Two Affine\n  Correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A minimal solution using two affine correspondences is presented to estimate\nthe common focal length and the fundamental matrix between two semi-calibrated\ncameras - known intrinsic parameters except a common focal length. To the best\nof our knowledge, this problem is unsolved. The proposed approach extends point\ncorrespondence-based techniques with linear constraints derived from local\naffine transformations. The obtained multivariate polynomial system is\nefficiently solved by the hidden-variable technique. Observing the geometry of\nlocal affinities, we introduce novel conditions eliminating invalid roots. To\nselect the best one out of the remaining candidates, a root selection technique\nis proposed outperforming the recent ones especially in case of high-level\nnoise. The proposed 2-point algorithm is validated on both synthetic data and\n104 publicly available real image pairs. A Matlab implementation of the\nproposed solution is included in the paper.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 08:19:18 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Barath", "Daniel", ""], ["Toth", "Tekla", ""], ["Hajder", "Levente", ""]]}, {"id": "1706.01671", "submitter": "Amir Bar", "authors": "Amir Bar, Lior Wolf, Orna Bergman Amitai, Eyal Toledano and Eldad\n  Elnekave", "title": "Compression Fractures Detection on CT", "comments": null, "journal-ref": "Proc. SPIE 10134, Medical Imaging 2017: Computer-Aided Diagnosis,\n  1013440 (March 3, 2017)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of a vertebral compression fracture is highly indicative of\nosteoporosis and represents the single most robust predictor for development of\na second osteoporotic fracture in the spine or elsewhere. Less than one third\nof vertebral compression fractures are diagnosed clinically. We present an\nautomated method for detecting spine compression fractures in Computed\nTomography (CT) scans. The algorithm is composed of three processes. First, the\nspinal column is segmented and sagittal patches are extracted. The patches are\nthen binary classified using a Convolutional Neural Network (CNN). Finally a\nRecurrent Neural Network (RNN) is utilized to predict whether a vertebral\nfracture is present in the series of patches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 09:29:10 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Bar", "Amir", ""], ["Wolf", "Lior", ""], ["Amitai", "Orna Bergman", ""], ["Toledano", "Eyal", ""], ["Elnekave", "Eldad", ""]]}, {"id": "1706.01789", "submitter": "Marek Kowalski", "authors": "Marek Kowalski, Jacek Naruniec, Tomasz Trzcinski", "title": "Deep Alignment Network: A convolutional neural network for robust face\n  alignment", "comments": "IEEE Conference on Computer Vision and Pattern Recognition Workshop\n  (CVPRW) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Deep Alignment Network (DAN), a robust face\nalignment method based on a deep neural network architecture. DAN consists of\nmultiple stages, where each stage improves the locations of the facial\nlandmarks estimated by the previous stage. Our method uses entire face images\nat all stages, contrary to the recently proposed face alignment methods that\nrely on local patches. This is possible thanks to the use of landmark heatmaps\nwhich provide visual information about landmark locations estimated at the\nprevious stages of the algorithm. The use of entire face images rather than\npatches allows DAN to handle face images with large variation in head pose and\ndifficult initializations. An extensive evaluation on two publicly available\ndatasets shows that DAN reduces the state-of-the-art failure rate by up to 70%.\nOur method has also been submitted for evaluation as part of the Menpo\nchallenge.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 14:36:52 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 06:27:00 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Kowalski", "Marek", ""], ["Naruniec", "Jacek", ""], ["Trzcinski", "Tomasz", ""]]}, {"id": "1706.01797", "submitter": "Li Si-Yao", "authors": "Li Si-Yao, Dongwei Ren, Qian Yin", "title": "Understanding Kernel Size in Blind Deconvolution", "comments": "Accepted by WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most blind deconvolution methods usually pre-define a large kernel size to\nguarantee the support domain. Blur kernel estimation error is likely to be\nintroduced, yielding severe artifacts in deblurring results. In this paper, we\nfirst theoretically and experimentally analyze the mechanism to estimation\nerror in oversized kernel, and show that it holds even on blurry images without\nnoises. Then to suppress this adverse effect, we propose a low rank-based\nregularization on blur kernel to exploit the structural information in degraded\nkernels, by which larger-kernel effect can be effectively suppressed. And we\npropose an efficient optimization algorithm to solve it. Experimental results\non benchmark datasets show that the proposed method is comparable with the\nstate-of-the-arts by accordingly setting proper kernel size, and performs much\nbetter in handling larger-size kernels quantitatively and qualitatively. The\ndeblurring results on real-world blurry images further validate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 14:46:45 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 13:39:20 GMT"}, {"version": "v3", "created": "Sun, 19 Nov 2017 16:10:03 GMT"}, {"version": "v4", "created": "Thu, 5 Jul 2018 13:29:32 GMT"}, {"version": "v5", "created": "Fri, 22 Feb 2019 11:02:51 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Si-Yao", "Li", ""], ["Ren", "Dongwei", ""], ["Yin", "Qian", ""]]}, {"id": "1706.01805", "submitter": "Yuan Xue", "authors": "Yuan Xue, Tao Xu, Han Zhang, Rodney Long and Xiaolei Huang", "title": "SegAN: Adversarial Network with Multi-scale $L_1$ Loss for Medical Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": "10.1007/s12021-018-9377-x", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by classic generative adversarial networks (GAN), we propose a novel\nend-to-end adversarial neural network, called SegAN, for the task of medical\nimage segmentation. Since image segmentation requires dense, pixel-level\nlabeling, the single scalar real/fake output of a classic GAN's discriminator\nmay be ineffective in producing stable and sufficient gradient feedback to the\nnetworks. Instead, we use a fully convolutional neural network as the segmentor\nto generate segmentation label maps, and propose a novel adversarial critic\nnetwork with a multi-scale $L_1$ loss function to force the critic and\nsegmentor to learn both global and local features that capture long- and\nshort-range spatial relationships between pixels. In our SegAN framework, the\nsegmentor and critic networks are trained in an alternating fashion in a\nmin-max game: The critic takes as input a pair of images, (original_image $*$\npredicted_label_map, original_image $*$ ground_truth_label_map), and then is\ntrained by maximizing a multi-scale loss function; The segmentor is trained\nwith only gradients passed along by the critic, with the aim to minimize the\nmulti-scale loss function. We show that such a SegAN framework is more\neffective and stable for the segmentation task, and it leads to better\nperformance than the state-of-the-art U-net segmentation method. We tested our\nSegAN method using datasets from the MICCAI BRATS brain tumor segmentation\nchallenge. Extensive experimental results demonstrate the effectiveness of the\nproposed SegAN with multi-scale loss: on BRATS 2013 SegAN gives performance\ncomparable to the state-of-the-art for whole tumor and tumor core segmentation\nwhile achieves better precision and sensitivity for Gd-enhance tumor core\nsegmentation; on BRATS 2015 SegAN achieves better performance than the\nstate-of-the-art in both dice score and precision.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 15:01:57 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 01:41:03 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Xue", "Yuan", ""], ["Xu", "Tao", ""], ["Zhang", "Han", ""], ["Long", "Rodney", ""], ["Huang", "Xiaolei", ""]]}, {"id": "1706.01820", "submitter": "Marek Kowalski", "authors": "Marek Kowalski, Jacek Naruniec", "title": "Face Alignment Using K-Cluster Regression Forests With Weighted\n  Splitting", "comments": "Postprint of an article published in IEEE Signal Processing Letters\n  in 2016. A video explaining the method:\n  https://www.youtube.com/watch?v=F4tgihZLrYw", "journal-ref": "IEEE Signal Processing Letters, vol. 23, no. 11, pp. 1567-1571\n  (Nov. 2016)", "doi": "10.1109/LSP.2016.2608139", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a face alignment pipeline based on two novel methods:\nweighted splitting for K-cluster Regression Forests and 3D Affine Pose\nRegression for face shape initialization. Our face alignment method is based on\nthe Local Binary Feature framework, where instead of standard regression\nforests and pixel difference features used in the original method, we use our\nK-cluster Regression Forests with Weighted Splitting (KRFWS) and Pyramid HOG\nfeatures. We also use KRFWS to perform Affine Pose Regression (APR) and\n3D-Affine Pose Regression (3D-APR), which intend to improve the face shape\ninitialization. APR applies a rigid 2D transform to the initial face shape that\ncompensates for inaccuracy in the initial face location, size and in-plane\nrotation. 3D-APR estimates the parameters of a 3D transform that additionally\ncompensates for out-of-plane rotation. The resulting pipeline, consisting of\nAPR and 3D-APR followed by face alignment, shows an improvement of 20% over\nstandard LBF on the challenging IBUG dataset, and state-of-theart accuracy on\nthe entire 300-W dataset.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 15:42:12 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Kowalski", "Marek", ""], ["Naruniec", "Jacek", ""]]}, {"id": "1706.01855", "submitter": "Micha{\\l} Byra", "authors": "Micha{\\l} Byra, Katarzyna Dobruch-Sobczak, Hanna\n  Piotrzkowska-Wr\\'oblewska and Andrzej Nowicki", "title": "Added value of morphological features to breast lesion diagnosis in\n  ultrasound", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound imaging plays an important role in breast lesion differentiation.\nHowever, diagnostic accuracy depends on ultrasonographer experience. Various\ncomputer aided diagnosis systems has been developed to improve breast cancer\ndetection and reduce the number of unnecessary biopsies. In this study, our aim\nwas to improve breast lesion classification based on the BI-RADS (Breast\nImaging - Reporting and Data System). This was accomplished by combining the\nBI-RADS with morphological features which assess lesion boundary. A dataset of\n214 lesion images was used for analysis. 30 morphological features were\nextracted and feature selection scheme was applied to find features which\nimprove the BI-RADS classification performance. Additionally, the best\nperforming morphological feature subset was indicated. We obtained a better\nclassification by combining the BI-RADS with six morphological features. These\nfeatures were the extent, overlap ratio, NRL entropy, circularity,\nelliptic-normalized circumference and the normalized residual value. The area\nunder the receiver operating curve calculated with the use of the combined\nclassifier was 0.986. The best performing morphological feature subset\ncontained six features: the DWR, NRL entropy, normalized residual value,\noverlap ratio, extent and the morphological closing ratio. For this set, the\narea under the curve was 0.901. The combination of the radiologist's experience\nrelated to the BI-RADS and the morphological features leads to a more effective\nbreast lesion classification.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 17:08:50 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Byra", "Micha\u0142", ""], ["Dobruch-Sobczak", "Katarzyna", ""], ["Piotrzkowska-Wr\u00f3blewska", "Hanna", ""], ["Nowicki", "Andrzej", ""]]}, {"id": "1706.01862", "submitter": "Jian Cheng", "authors": "Jian Cheng, Peter J. Basser", "title": "Director Field Analysis (DFA): Exploring Local White Matter Geometric\n  Structure in diffusion MRI", "comments": "Accepted by Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2017.10.003", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Diffusion Tensor Imaging (DTI) or High Angular Resolution Diffusion\nImaging (HARDI), a tensor field or a spherical function field (e.g., an\norientation distribution function field), can be estimated from measured\ndiffusion weighted images. In this paper, inspired by the microscopic\ntheoretical treatment of phases in liquid crystals, we introduce a novel\nmathematical framework, called Director Field Analysis (DFA), to study local\ngeometric structural information of white matter based on the reconstructed\ntensor field or spherical function field: 1) We propose a set of mathematical\ntools to process general director data, which consists of dyadic tensors that\nhave orientations but no direction. 2) We propose Orientational Order (OO) and\nOrientational Dispersion (OD) indices to describe the degree of alignment and\ndispersion of a spherical function in a single voxel or in a region,\nrespectively; 3) We also show how to construct a local orthogonal coordinate\nframe in each voxel exhibiting anisotropic diffusion; 4) Finally, we define\nthree indices to describe three types of orientational distortion (splay, bend,\nand twist) in a local spatial neighborhood, and a total distortion index to\ndescribe distortions of all three types. To our knowledge, this is the first\nwork to quantitatively describe orientational distortion (splay, bend, and\ntwist) in general spherical function fields from DTI or HARDI data. The\nproposed DFA and its related mathematical tools can be used to process not only\ndiffusion MRI data but also general director field data, and the proposed\nscalar indices are useful for detecting local geometric changes of white matter\nfor voxel-based or tract-based analysis in both DTI and HARDI acquisitions. The\nrelated codes and a tutorial for DFA will be released in DMRITool.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 17:23:22 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 13:49:56 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Cheng", "Jian", ""], ["Basser", "Peter J.", ""]]}, {"id": "1706.01869", "submitter": "Kevin Matzen", "authors": "Kevin Matzen, Kavita Bala, Noah Snavely", "title": "StreetStyle: Exploring world-wide clothing styles from millions of\n  photos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each day billions of photographs are uploaded to photo-sharing services and\nsocial media platforms. These images are packed with information about how\npeople live around the world. In this paper we exploit this rich trove of data\nto understand fashion and style trends worldwide. We present a framework for\nvisual discovery at scale, analyzing clothing and fashion across millions of\nimages of people around the world and spanning several years. We introduce a\nlarge-scale dataset of photos of people annotated with clothing attributes, and\nuse this dataset to train attribute classifiers via deep learning. We also\npresent a method for discovering visually consistent style clusters that\ncapture useful visual correlations in this massive dataset. Using these tools,\nwe analyze millions of photos to derive visual insight, producing a\nfirst-of-its-kind analysis of global and per-city fashion choices and\nspatio-temporal trends.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 17:44:43 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Matzen", "Kevin", ""], ["Bala", "Kavita", ""], ["Snavely", "Noah", ""]]}, {"id": "1706.01912", "submitter": "Wufeng Xue", "authors": "Wufeng Xue, Andrea Lum, Ashley Mercado, Mark Landis, James Warringto\n  and Shuo Li", "title": "Full Quantification of Left Ventricle via Deep Multitask Learning\n  Network Respecting Intra- and Inter-Task Relatedness", "comments": "Accepted at MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac left ventricle (LV) quantification is among the most clinically\nimportant tasks for identification and diagnosis of cardiac diseases, yet still\na challenge due to the high variability of cardiac structure and the complexity\nof temporal dynamics. Full quantification, i.e., to simultaneously quantify all\nLV indices including two areas (cavity and myocardium), six regional wall\nthicknesses (RWT), three LV dimensions, and one cardiac phase, is even more\nchallenging since the uncertain relatedness intra and inter each type of\nindices may hinder the learning procedure from better convergence and\ngeneralization. In this paper, we propose a newly-designed multitask learning\nnetwork (FullLVNet), which is constituted by a deep convolution neural network\n(CNN) for expressive feature embedding of cardiac structure; two followed\nparallel recurrent neural network (RNN) modules for temporal dynamic modeling;\nand four linear models for the final estimation. During the final estimation,\nboth intra- and inter-task relatedness are modeled to enforce improvement of\ngeneralization: 1) respecting intra-task relatedness, group lasso is applied to\neach of the regression tasks for sparse and common feature selection and\nconsistent prediction; 2) respecting inter-task relatedness, three phase-guided\nconstraints are proposed to penalize violation of the temporal behavior of the\nobtained LV indices. Experiments on MR sequences of 145 subjects show that\nFullLVNet achieves high accurate prediction with our intra- and inter-task\nrelatedness, leading to MAE of 190mm$^2$, 1.41mm, 2.68mm for average areas,\nRWT, dimensions and error rate of 10.4\\% for the phase classification. This\nendows our method a great potential in comprehensive clinical assessment of\nglobal, regional and dynamic cardiac function.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 18:22:44 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 16:12:49 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Xue", "Wufeng", ""], ["Lum", "Andrea", ""], ["Mercado", "Ashley", ""], ["Landis", "Mark", ""], ["Warringto", "James", ""], ["Li", "Shuo", ""]]}, {"id": "1706.02003", "submitter": "Seungryul Baek", "authors": "Seungryul Baek, Kwang In Kim, Tae-Kyun Kim", "title": "Deep Convolutional Decision Jungle for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method called deep convolutional decision jungle (CDJ) and\nits learning algorithm for image classification. The CDJ maintains the\nstructure of standard convolutional neural networks (CNNs), i.e. multiple\nlayers of multiple response maps fully connected. Each response map-or node-in\nboth the convolutional and fully-connected layers selectively respond to class\nlabels s.t. each data sample travels via a specific soft route of those\nactivated nodes. The proposed method CDJ automatically learns features, whereas\ndecision forests and jungles require pre-defined feature sets. Compared to\nCNNs, the method embeds the benefits of using data-dependent discriminative\nfunctions, which better handles multi-modal/heterogeneous data; further,the\nmethod offers more diverse sparse network responses, which in turn can be used\nfor cost-effective learning/classification. The network is learnt by combining\nconventional softmax and proposed entropy losses in each layer. The entropy\nloss,as used in decision tree growing, measures the purity of data activation\naccording to the class label distribution. The back-propagation rule for the\nproposed loss function is derived from stochastic gradient descent (SGD)\noptimization of CNNs. We show that our proposed method outperforms\nstate-of-the-art methods on three public image classification benchmarks and\none face verification dataset. We also demonstrate the use of auxiliary data\nlabels, when available, which helps our method to learn more discriminative\nrouting and representations and leads to improved classification.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 23:03:44 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 12:53:33 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Baek", "Seungryul", ""], ["Kim", "Kwang In", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1706.02021", "submitter": "Anbang Yao", "authors": "Yiwen Guo, Anbang Yao, Hao Zhao, Yurong Chen", "title": "Network Sketching: Exploiting Binary Structure in Deep CNNs", "comments": "To appear in CVPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) with deep architectures have\nsubstantially advanced the state-of-the-art in computer vision tasks. However,\ndeep networks are typically resource-intensive and thus difficult to be\ndeployed on mobile devices. Recently, CNNs with binary weights have shown\ncompelling efficiency to the community, whereas the accuracy of such models is\nusually unsatisfactory in practice. In this paper, we introduce network\nsketching as a novel technique of pursuing binary-weight CNNs, targeting at\nmore faithful inference and better trade-off for practical applications. Our\nbasic idea is to exploit binary structure directly in pre-trained filter banks\nand produce binary-weight models via tensor expansion. The whole process can be\ntreated as a coarse-to-fine model approximation, akin to the pencil drawing\nsteps of outlining and shading. To further speedup the generated models, namely\nthe sketches, we also propose an associative implementation of binary tensor\nconvolutions. Experimental results demonstrate that a proper sketch of AlexNet\n(or ResNet) outperforms the existing binary-weight models by large margins on\nthe ImageNet large scale classification task, while the committed memory for\nnetwork parameters only exceeds a little.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 01:53:44 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Guo", "Yiwen", ""], ["Yao", "Anbang", ""], ["Zhao", "Hao", ""], ["Chen", "Yurong", ""]]}, {"id": "1706.02025", "submitter": "Pablo M\\'arquez-Neila", "authors": "Pablo M\\'arquez-Neila, Mathieu Salzmann, and Pascal Fua", "title": "Imposing Hard Constraints on Deep Networks: Promises and Limitations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imposing constraints on the output of a Deep Neural Net is one way to improve\nthe quality of its predictions while loosening the requirements for labeled\ntraining data. Such constraints are usually imposed as soft constraints by\nadding new terms to the loss function that is minimized during training. An\nalternative is to impose them as hard constraints, which has a number of\ntheoretical benefits but has not been explored so far due to the perceived\nintractability of the problem.\n  In this paper, we show that imposing hard constraints can in fact be done in\na computationally feasible way and delivers reasonable results. However, the\ntheoretical benefits do not materialize and the resulting technique is no\nbetter than existing ones relying on soft constraints. We analyze the reasons\nfor this and hope to spur other researchers into proposing better solutions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 02:03:00 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["M\u00e1rquez-Neila", "Pablo", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1706.02042", "submitter": "Xiaoguang Han", "authors": "Xiaoguang Han and Chang Gao and Yizhou Yu", "title": "DeepSketch2Face: A Deep Learning Based Sketching System for 3D Face and\n  Caricature Modeling", "comments": "12 pages, 16 figures, to appear in SIGGRAPH 2017", "journal-ref": null, "doi": "10.1145/3072959.3073629", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face modeling has been paid much attention in the field of visual computing.\nThere exist many scenarios, including cartoon characters, avatars for social\nmedia, 3D face caricatures as well as face-related art and design, where\nlow-cost interactive face modeling is a popular approach especially among\namateur users. In this paper, we propose a deep learning based sketching system\nfor 3D face and caricature modeling. This system has a labor-efficient\nsketching interface, that allows the user to draw freehand imprecise yet\nexpressive 2D lines representing the contours of facial features. A novel CNN\nbased deep regression network is designed for inferring 3D face models from 2D\nsketches. Our network fuses both CNN and shape based features of the input\nsketch, and has two independent branches of fully connected layers generating\nindependent subsets of coefficients for a bilinear face representation. Our\nsystem also supports gesture based interactions for users to further manipulate\ninitial face models. Both user studies and numerical results indicate that our\nsketching system can help users create face models quickly and effectively. A\nsignificantly expanded face database with diverse identities, expressions and\nlevels of exaggeration is constructed to promote further research and\nevaluation of face modeling techniques.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 04:02:27 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Han", "Xiaoguang", ""], ["Gao", "Chang", ""], ["Yu", "Yizhou", ""]]}, {"id": "1706.02051", "submitter": "Veronika Cheplygina", "authors": "Isabel Pino Pe\\~na, Veronika Cheplygina, Sofia Paschaloudi, Morten\n  Vuust, Jesper Carl, Ulla M{\\o}ller Weinreich, Lasse Riis {\\O}stergaard and\n  Marleen de Bruijne", "title": "Automatic Emphysema Detection using Weakly Labeled HRCT Lung Images", "comments": "Accepted at PLoS ONE", "journal-ref": null, "doi": "10.1371/journal.pone.0205397", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for automatically quantifying emphysema regions using\nHigh-Resolution Computed Tomography (HRCT) scans of patients with chronic\nobstructive pulmonary disease (COPD) that does not require manually annotated\nscans for training is presented. HRCT scans of controls and of COPD patients\nwith diverse disease severity are acquired at two different centers. Textural\nfeatures from co-occurrence matrices and Gaussian filter banks are used to\ncharacterize the lung parenchyma in the scans. Two robust versions of multiple\ninstance learning (MIL) classifiers, miSVM and MILES, are investigated. The\nclassifiers are trained with the weak labels extracted from the forced\nexpiratory volume in one minute (FEV$_1$) and diffusing capacity of the lungs\nfor carbon monoxide (DLCO). At test time, the classifiers output a patient\nlabel indicating overall COPD diagnosis and local labels indicating the\npresence of emphysema. The classifier performance is compared with manual\nannotations by two radiologists, a classical density based method, and\npulmonary function tests (PFTs). The miSVM classifier performed better than\nMILES on both patient and emphysema classification. The classifier has a\nstronger correlation with PFT than the density based method, the percentage of\nemphysema in the intersection of annotations from both radiologists, and the\npercentage of emphysema annotated by one of the radiologists. The correlation\nbetween the classifier and the PFT is only outperformed by the second\nradiologist. The method is therefore promising for facilitating assessment of\nemphysema and reducing inter-observer variability.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 05:38:49 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 12:48:31 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Pe\u00f1a", "Isabel Pino", ""], ["Cheplygina", "Veronika", ""], ["Paschaloudi", "Sofia", ""], ["Vuust", "Morten", ""], ["Carl", "Jesper", ""], ["Weinreich", "Ulla M\u00f8ller", ""], ["\u00d8stergaard", "Lasse Riis", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1706.02054", "submitter": "Kanji Tanaka", "authors": "Fei Xiaoxiao, Tanaka Kanji", "title": "Unsupervised Place Discovery for Place-Specific Change Classifier", "comments": "6 pages, 6 figures, 2 tables, Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we address the problem of supervised change detection for\nrobotic map learning applications, in which the aim is to train a\nplace-specific change classifier (e.g., support vector machine (SVM)) to\npredict changes from a robot's view image. An open question is the manner in\nwhich to partition a robot's workspace into places (e.g., SVMs) to maximize the\noverall performance of change classifiers. This is a chicken-or-egg problem: if\nwe have a well-trained change classifier, partitioning the robot's workspace\ninto places is rather easy. However, training a change classifier requires a\nset of place-specific training data. In this study, we address this novel\nproblem, which we term unsupervised place discovery. In addition, we present a\nsolution powered by convolutional-feature-based visual place recognition, and\nvalidate our approach by applying it to two place-specific change classifiers,\nnamely, nuisance and anomaly predictors.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 06:00:49 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Xiaoxiao", "Fei", ""], ["Kanji", "Tanaka", ""]]}, {"id": "1706.02055", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina and Adria Perez-Rovira and Wieying Kuo and Harm A.\n  W. M. Tiddens and Marleen de Bruijne", "title": "Early Experiences with Crowdsourcing Airway Annotations in Chest CT", "comments": null, "journal-ref": "LABELS 2016, DLMIA 2016: Deep Learning and Data Labeling for\n  Medical Applications pp 209-218", "doi": "10.1007/978-3-319-46976-8_22", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring airways in chest computed tomography (CT) images is important for\ncharacterizing diseases such as cystic fibrosis, yet very time-consuming to\nperform manually. Machine learning algorithms offer an alternative, but need\nlarge sets of annotated data to perform well. We investigate whether\ncrowdsourcing can be used to gather airway annotations which can serve directly\nfor measuring the airways, or as training data for the algorithms. We generate\nimage slices at known locations of airways and request untrained crowd workers\nto outline the airway lumen and airway wall. Our results show that the workers\nare able to interpret the images, but that the instructions are too complex,\nleading to many unusable annotations. After excluding unusable annotations,\nquantitative results show medium to high correlations with expert measurements\nof the airways. Based on this positive experience, we describe a number of\nfurther research directions and provide insight into the challenges of\ncrowdsourcing in medical images from the perspective of first-time users.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 06:09:04 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Cheplygina", "Veronika", ""], ["Perez-Rovira", "Adria", ""], ["Kuo", "Wieying", ""], ["Tiddens", "Harm A. W. M.", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1706.02071", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Swaminathan Gurumurthy and Ravi Kiran Sarvadevabhatla and Venkatesh\n  Babu Radhakrishnan", "title": "DeLiGAN : Generative Adversarial Networks for Diverse and Limited Data", "comments": "Accepted at CVPR-2017. Code for training the GAN models and computing\n  modified inception-scores can be found at https://github.com/val-iisc/deligan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of recent approaches for generating images, called Generative\nAdversarial Networks (GAN), have been used to generate impressively realistic\nimages of objects, bedrooms, handwritten digits and a variety of other image\nmodalities. However, typical GAN-based approaches require large amounts of\ntraining data to capture the diversity across the image modality. In this\npaper, we propose DeLiGAN -- a novel GAN-based architecture for diverse and\nlimited training data scenarios. In our approach, we reparameterize the latent\ngenerative space as a mixture model and learn the mixture model's parameters\nalong with those of GAN. This seemingly simple modification to the GAN\nframework is surprisingly effective and results in models which enable\ndiversity in generated samples although trained with limited data. In our work,\nwe show that DeLiGAN can generate images of handwritten digits, objects and\nhand-drawn sketches, all using limited amounts of data. To quantitatively\ncharacterize intra-class diversity of generated samples, we also introduce a\nmodified version of \"inception-score\", a measure which has been found to\ncorrelate well with human assessment of generated samples.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 07:27:20 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Gurumurthy", "Swaminathan", ""], ["Sarvadevabhatla", "Ravi Kiran", ""], ["Radhakrishnan", "Venkatesh Babu", ""]]}, {"id": "1706.02135", "submitter": "Quoc Viet Pham", "authors": "Viet-Quoc Pham, Satoshi Ito and Tatsuo Kozakaya", "title": "BiSeg: Simultaneous Instance Segmentation and Semantic Segmentation with\n  Fully Convolutional Networks", "comments": "BMVC2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and effective framework for simultaneous semantic\nsegmentation and instance segmentation with Fully Convolutional Networks\n(FCNs). The method, called BiSeg, predicts instance segmentation as a posterior\nin Bayesian inference, where semantic segmentation is used as a prior. We\nextend the idea of position-sensitive score maps used in recent methods to a\nfusion of multiple score maps at different scales and partition modes, and\nadopt it as a robust likelihood for instance segmentation inference. As both\nBayesian inference and map fusion are performed per pixel, BiSeg is a fully\nconvolutional end-to-end solution that inherits all the advantages of FCNs. We\ndemonstrate state-of-the-art instance segmentation accuracy on PASCAL VOC.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 11:23:44 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 02:28:21 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Pham", "Viet-Quoc", ""], ["Ito", "Satoshi", ""], ["Kozakaya", "Tatsuo", ""]]}, {"id": "1706.02179", "submitter": "S\\'ebastien Ehrhardt", "authors": "S\\'ebastien Ehrhardt, Aron Monszpart, Andrea Vedaldi, Niloy Mitra", "title": "Learning to Represent Mechanics via Long-term Extrapolation and\n  Interpolation", "comments": "arXiv admin note: text overlap with arXiv:1703.00247", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the basic laws of Newtonian mechanics are well understood, explaining a\nphysical scenario still requires manually modeling the problem with suitable\nequations and associated parameters. In order to adopt such models for\nartificial intelligence, researchers have handcrafted the relevant states, and\nthen used neural networks to learn the state transitions using simulation runs\nas training data. Unfortunately, such approaches can be unsuitable for modeling\ncomplex real-world scenarios, where manually authoring relevant state spaces\ntend to be challenging. In this work, we investigate if neural networks can\nimplicitly learn physical states of real-world mechanical processes only based\non visual data, and thus enable long-term physical extrapolation. We develop a\nrecurrent neural network architecture for this task and also characterize\nresultant uncertainties in the form of evolving variance estimates. We evaluate\nour setup to extrapolate motion of a rolling ball on bowl of varying shape and\norientation using only images as input, and report competitive results with\napproaches that assume access to internal physics models and parameters.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 15:45:48 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 09:31:22 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Ehrhardt", "S\u00e9bastien", ""], ["Monszpart", "Aron", ""], ["Vedaldi", "Andrea", ""], ["Mitra", "Niloy", ""]]}, {"id": "1706.02185", "submitter": "Li Cheng", "authors": "He Zhao, Huiqi Li, Li Cheng", "title": "Synthesizing Filamentary Structured Images with GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at synthesizing filamentary structured images such as retinal\nfundus images and neuronal images, as follows: Given a ground-truth, to\ngenerate multiple realistic looking phantoms. A ground-truth could be a binary\nsegmentation map containing the filamentary structured morphology, while the\nsynthesized output image is of the same size as the ground-truth and has\nsimilar visual appearance to what have been presented in the training set. Our\napproach is inspired by the recent progresses in generative adversarial nets\n(GANs) as well as image style transfer. In particular, it is dedicated to our\nproblem context with the following properties: Rather than large-scale dataset,\nit works well in the presence of as few as 10 training examples, which is\ncommon in medical image analysis; It is capable of synthesizing diverse images\nfrom the same ground-truth; Last and importantly, the synthetic images produced\nby our approach are demonstrated to be useful in boosting image analysis\nperformance. Empirical examination over various benchmarks of fundus and\nneuronal images demonstrate the advantages of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 13:37:09 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Zhao", "He", ""], ["Li", "Huiqi", ""], ["Cheng", "Li", ""]]}, {"id": "1706.02189", "submitter": "Fatemehsadat Saleh", "authors": "Fatemeh Sadat Saleh, Mohammad Sadegh Aliakbarian, Mathieu Salzmann,\n  Lars Petersson, Jose M. Alvarez and Stephen Gould", "title": "Incorporating Network Built-in Priors in Weakly-supervised Semantic\n  Segmentation", "comments": "14 pages, 11 figures, 8 tables, Accepted in IEEE Transaction on\n  Pattern Analysis and Machine Intelligence (IEEE TPAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2713785", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel-level annotations are expensive and time consuming to obtain. Hence,\nweak supervision using only image tags could have a significant impact in\nsemantic segmentation. Recently, CNN-based methods have proposed to fine-tune\npre-trained networks using image tags. Without additional information, this\nleads to poor localization accuracy. This problem, however, was alleviated by\nmaking use of objectness priors to generate foreground/background masks.\nUnfortunately these priors either require pixel-level annotations/bounding\nboxes, or still yield inaccurate object boundaries. Here, we propose a novel\nmethod to extract accurate masks from networks pre-trained for the task of\nobject recognition, thus forgoing external objectness modules. We first show\nhow foreground/background masks can be obtained from the activations of\nhigher-level convolutional layers of a network. We then show how to obtain\nmulti-class masks by the fusion of foreground/background ones with information\nextracted from a weakly-supervised localization network. Our experiments\nevidence that exploiting these masks in conjunction with a weakly-supervised\ntraining loss yields state-of-the-art tag-based weakly-supervised semantic\nsegmentation results.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 03:23:17 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Saleh", "Fatemeh Sadat", ""], ["Aliakbarian", "Mohammad Sadegh", ""], ["Salzmann", "Mathieu", ""], ["Petersson", "Lars", ""], ["Alvarez", "Jose M.", ""], ["Gould", "Stephen", ""]]}, {"id": "1706.02240", "submitter": "Martin Schrimpf", "authors": "Hanlin Tang, Martin Schrimpf, Bill Lotter, Charlotte Moerman, Ana\n  Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, Gabriel Kreiman", "title": "Recurrent computations for visual pattern completion", "comments": null, "journal-ref": null, "doi": "10.1073/pnas.1719397115", "report-no": null, "categories": "q-bio.NC cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making inferences from partial information constitutes a critical aspect of\ncognition. During visual perception, pattern completion enables recognition of\npoorly visible or occluded objects. We combined psychophysics, physiology and\ncomputational models to test the hypothesis that pattern completion is\nimplemented by recurrent computations and present three pieces of evidence that\nare consistent with this hypothesis. First, subjects robustly recognized\nobjects even when rendered <15% visible, but recognition was largely impaired\nwhen processing was interrupted by backward masking. Second, invasive\nphysiological responses along the human ventral cortex exhibited visually\nselective responses to partially visible objects that were delayed compared to\nwhole objects, suggesting the need for additional computations. These\nphysiological delays were correlated with the effects of backward masking.\nThird, state-of-the-art feed-forward computational architectures were not\nrobust to partial visibility. However, recognition performance was recovered\nwhen the model was augmented with attractor-based recurrent connectivity. These\nresults provide a strong argument of plausibility for the role of recurrent\ncomputations in making visual inferences from partial information.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 16:23:28 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 12:29:22 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Tang", "Hanlin", ""], ["Schrimpf", "Martin", ""], ["Lotter", "Bill", ""], ["Moerman", "Charlotte", ""], ["Paredes", "Ana", ""], ["Caro", "Josue Ortega", ""], ["Hardesty", "Walter", ""], ["Cox", "David", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "1706.02248", "submitter": "Yuri G. Gordienko", "authors": "Yuriy Kochura, Sergii Stirenko, Anis Rojbi, Oleg Alienin, Michail\n  Novotarskiy, and Yuri Gordienko", "title": "Comparative Analysis of Open Source Frameworks for Machine Learning with\n  Use Case in Single-Threaded and Multi-Threaded Modes", "comments": "4 pages, 6 figures, 4 tables; XIIth International Scientific and\n  Technical Conference on Computer Sciences and Information Technologies (CSIT\n  2017), Lviv, Ukraine", "journal-ref": "Proceedings of 12th International Scientific and Technical\n  Conference on Computer Sciences and Information Technologies (CSIT), 5-8\n  Sept. 2017, (Lviv, Ukraine), vol.1, pp. 373-376, IEEE", "doi": "10.1109/STC-CSIT.2017.8098808", "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic features of some of the most versatile and popular open source\nframeworks for machine learning (TensorFlow, Deep Learning4j, and H2O) are\nconsidered and compared. Their comparative analysis was performed and\nconclusions were made as to the advantages and disadvantages of these\nplatforms. The performance tests for the de facto standard MNIST data set were\ncarried out on H2O framework for deep learning algorithms designed for CPU and\nGPU platforms for single-threaded and multithreaded modes of operation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 16:41:21 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Kochura", "Yuriy", ""], ["Stirenko", "Sergii", ""], ["Rojbi", "Anis", ""], ["Alienin", "Oleg", ""], ["Novotarskiy", "Michail", ""], ["Gordienko", "Yuri", ""]]}, {"id": "1706.02257", "submitter": "Oluwatobi Olabiyi", "authors": "Oluwatobi Olabiyi, Eric Martinson, Vijay Chintalapudi, Rui Guo", "title": "Driver Action Prediction Using Deep (Bidirectional) Recurrent Neural\n  Network", "comments": "ITSC'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced driver assistance systems (ADAS) can be significantly improved with\neffective driver action prediction (DAP). Predicting driver actions early and\naccurately can help mitigate the effects of potentially unsafe driving\nbehaviors and avoid possible accidents. In this paper, we formulate driver\naction prediction as a timeseries anomaly prediction problem. While the anomaly\n(driver actions of interest) detection might be trivial in this context,\nfinding patterns that consistently precede an anomaly requires searching for or\nextracting features across multi-modal sensory inputs. We present such a driver\naction prediction system, including a real-time data acquisition, processing\nand learning framework for predicting future or impending driver action. The\nproposed system incorporates camera-based knowledge of the driving environment\nand the driver themselves, in addition to traditional vehicle dynamics. It then\nuses a deep bidirectional recurrent neural network (DBRNN) to learn the\ncorrelation between sensory inputs and impending driver behavior achieving\naccurate and high horizon action prediction. The proposed system performs\nbetter than other existing systems on driver action prediction tasks and can\naccurately predict key driver actions including acceleration, braking, lane\nchange and turning at durations of 5sec before the action is executed by the\ndriver.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 17:00:08 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Olabiyi", "Oluwatobi", ""], ["Martinson", "Eric", ""], ["Chintalapudi", "Vijay", ""], ["Guo", "Rui", ""]]}, {"id": "1706.02331", "submitter": "Santhosh Kumar Ramakrishnan", "authors": "Santhosh K. Ramakrishnan, Swarna Kamlam Ravindran, Anurag Mittal", "title": "CoMaL Tracking: Tracking Points at the Object Boundaries", "comments": "10 pages, 10 figures, to appear in 1st Joint BMTT-PETS Workshop on\n  Tracking and Surveillance, CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional point tracking algorithms such as the KLT use local 2D\ninformation aggregation for feature detection and tracking, due to which their\nperformance degrades at the object boundaries that separate multiple objects.\nRecently, CoMaL Features have been proposed that handle such a case. However,\nthey proposed a simple tracking framework where the points are re-detected in\neach frame and matched. This is inefficient and may also lose many points that\nare not re-detected in the next frame. We propose a novel tracking algorithm to\naccurately and efficiently track CoMaL points. For this, the level line segment\nassociated with the CoMaL points is matched to MSER segments in the next frame\nusing shape-based matching and the matches are further filtered using\ntexture-based matching. Experiments show improvements over a simple\nre-detect-and-match framework as well as KLT in terms of speed/accuracy on\ndifferent real-world applications, especially at the object boundaries.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 18:38:05 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Ramakrishnan", "Santhosh K.", ""], ["Ravindran", "Swarna Kamlam", ""], ["Mittal", "Anurag", ""]]}, {"id": "1706.02332", "submitter": "Matthijs Douze", "authors": "Matthijs Douze and Arthur Szlam and Bharath Hariharan and Herv\\'e\n  J\\'egou", "title": "Low-shot learning with large-scale diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of inferring image labels from images when\nonly a few annotated examples are available at training time. This setup is\noften referred to as low-shot learning, where a standard approach is to\nre-train the last few layers of a convolutional neural network learned on\nseparate classes for which training examples are abundant. We consider a\nsemi-supervised setting based on a large collection of images to support label\npropagation. This is possible by leveraging the recent advances on large-scale\nsimilarity graph construction.\n  We show that despite its conceptual simplicity, scaling label propagation up\nto hundred millions of images leads to state of the art accuracy in the\nlow-shot learning regime.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 18:40:26 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 13:59:23 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 15:31:37 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Douze", "Matthijs", ""], ["Szlam", "Arthur", ""], ["Hariharan", "Bharath", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1706.02337", "submitter": "Xiao Yang", "authors": "Xiao Yang, Ersin Yumer, Paul Asente, Mike Kraley, Daniel Kifer, C. Lee\n  Giles", "title": "Learning to Extract Semantic Structure from Documents Using Multimodal\n  Fully Convolutional Neural Network", "comments": "CVPR 2017 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end, multimodal, fully convolutional network for\nextracting semantic structures from document images. We consider document\nsemantic structure extraction as a pixel-wise segmentation task, and propose a\nunified model that classifies pixels based not only on their visual appearance,\nas in the traditional page segmentation task, but also on the content of\nunderlying text. Moreover, we propose an efficient synthetic document\ngeneration process that we use to generate pretraining data for our network.\nOnce the network is trained on a large set of synthetic documents, we fine-tune\nthe network on unlabeled real documents using a semi-supervised approach. We\nsystematically study the optimum network architecture and show that both our\nmultimodal approach and the synthetic data pretraining significantly boost the\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 18:51:31 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Yang", "Xiao", ""], ["Yumer", "Ersin", ""], ["Asente", "Paul", ""], ["Kraley", "Mike", ""], ["Kifer", "Daniel", ""], ["Giles", "C. Lee", ""]]}, {"id": "1706.02342", "submitter": "Mehran Khodabandeh", "authors": "Mehran Khodabandeh, Zhiwei Deng, Mostafa S. Ibrahim, Shinichi Satoh,\n  Greg Mori", "title": "Active Learning for Structured Prediction from Partially Labeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general purpose active learning algorithm for structured\nprediction, gathering labeled data for training a model that outputs a set of\nrelated labels for an image or video. Active learning starts with a limited\ninitial training set, then iterates querying a user for labels on unlabeled\ndata and retraining the model. We propose a novel algorithm for selecting data\nfor labeling, choosing examples to maximize expected information gain based on\nbelief propagation inference. This is a general purpose method and can be\napplied to a variety of tasks or models. As a specific example we demonstrate\nthis framework for learning to recognize human actions and group activities in\nvideo sequences. Experiments show that our proposed algorithm outperforms\nprevious active learning methods and can achieve accuracy comparable to fully\nsupervised methods while utilizing significantly less labeled data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 19:01:25 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 06:23:32 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Khodabandeh", "Mehran", ""], ["Deng", "Zhiwei", ""], ["Ibrahim", "Mostafa S.", ""], ["Satoh", "Shinichi", ""], ["Mori", "Greg", ""]]}, {"id": "1706.02379", "submitter": "Hao Li", "authors": "Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, Tom\n  Goldstein", "title": "Training Quantized Nets: A Deeper Understanding", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, deep neural networks are deployed on low-power portable devices by\nfirst training a full-precision model using powerful hardware, and then\nderiving a corresponding low-precision model for efficient inference on such\nsystems. However, training models directly with coarsely quantized weights is a\nkey step towards learning on embedded platforms that have limited computing\nresources, memory capacity, and power consumption. Numerous recent publications\nhave studied methods for training quantized networks, but these studies have\nmostly been empirical. In this work, we investigate training methods for\nquantized neural networks from a theoretical viewpoint. We first explore\naccuracy guarantees for training methods under convexity assumptions. We then\nlook at the behavior of these algorithms for non-convex problems, and show that\ntraining algorithms that exploit high-precision representations have an\nimportant greedy search phase that purely quantized training methods lack,\nwhich explains the difficulty of training using low-precision arithmetic.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 21:01:15 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 10:28:36 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 16:32:39 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Li", "Hao", ""], ["De", "Soham", ""], ["Xu", "Zheng", ""], ["Studer", "Christoph", ""], ["Samet", "Hanan", ""], ["Goldstein", "Tom", ""]]}, {"id": "1706.02393", "submitter": "Denis Gudovskiy", "authors": "Denis A. Gudovskiy, Luca Rigazio", "title": "ShiftCNN: Generalized Low-Precision Architecture for Inference of\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce ShiftCNN, a generalized low-precision architecture\nfor inference of multiplierless convolutional neural networks (CNNs). ShiftCNN\nis based on a power-of-two weight representation and, as a result, performs\nonly shift and addition operations. Furthermore, ShiftCNN substantially reduces\ncomputational cost of convolutional layers by precomputing convolution terms.\nSuch an optimization can be applied to any CNN architecture with a relatively\nsmall codebook of weights and allows to decrease the number of product\noperations by at least two orders of magnitude. The proposed architecture\ntargets custom inference accelerators and can be realized on FPGAs or ASICs.\nExtensive evaluation on ImageNet shows that the state-of-the-art CNNs can be\nconverted without retraining into ShiftCNN with less than 1% drop in accuracy\nwhen the proposed quantization algorithm is employed. RTL simulations,\ntargeting modern FPGAs, show that power consumption of convolutional layers is\nreduced by a factor of 4 compared to conventional 8-bit fixed-point\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 22:00:25 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Gudovskiy", "Denis A.", ""], ["Rigazio", "Luca", ""]]}, {"id": "1706.02413", "submitter": "Charles Ruizhongtai Qi", "authors": "Charles R. Qi, Li Yi, Hao Su, Leonidas J. Guibas", "title": "PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric\n  Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few prior works study deep learning on point sets. PointNet by Qi et al. is a\npioneer in this direction. However, by design PointNet does not capture local\nstructures induced by the metric space points live in, limiting its ability to\nrecognize fine-grained patterns and generalizability to complex scenes. In this\nwork, we introduce a hierarchical neural network that applies PointNet\nrecursively on a nested partitioning of the input point set. By exploiting\nmetric space distances, our network is able to learn local features with\nincreasing contextual scales. With further observation that point sets are\nusually sampled with varying densities, which results in greatly decreased\nperformance for networks trained on uniform densities, we propose novel set\nlearning layers to adaptively combine features from multiple scales.\nExperiments show that our network called PointNet++ is able to learn deep point\nset features efficiently and robustly. In particular, results significantly\nbetter than state-of-the-art have been obtained on challenging benchmarks of 3D\npoint clouds.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 23:37:44 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Qi", "Charles R.", ""], ["Yi", "Li", ""], ["Su", "Hao", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1706.02417", "submitter": "Joshua Peterson", "authors": "Joshua C. Peterson, Joshua T. Abbott, Thomas L. Griffiths", "title": "Evaluating (and improving) the correspondence between deep neural\n  networks and human representations", "comments": "35 pages, 8 figures, accepted for publication in Cognitive Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decades of psychological research have been aimed at modeling how people\nlearn features and categories. The empirical validation of these theories is\noften based on artificial stimuli with simple representations. Recently, deep\nneural networks have reached or surpassed human accuracy on tasks such as\nidentifying objects in natural images. These networks learn representations of\nreal-world stimuli that can potentially be leveraged to capture psychological\nrepresentations. We find that state-of-the-art object classification networks\nprovide surprisingly accurate predictions of human similarity judgments for\nnatural images, but fail to capture some of the structure represented by\npeople. We show that a simple transformation that corrects these discrepancies\ncan be obtained through convex optimization. We use the resulting\nrepresentations to predict the difficulty of learning novel categories of\nnatural images. Our results extend the scope of psychological experiments and\ncomputational modeling by enabling tractable use of large natural stimulus\nsets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 00:05:13 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 02:36:18 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 00:57:30 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Peterson", "Joshua C.", ""], ["Abbott", "Joshua T.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1706.02425", "submitter": "Ying Chen", "authors": "Nuhad A. Malalla, Ying Chen", "title": "C-arm Tomographic Imaging Technique for Nephrolithiasis and Detection of\n  Kidney Stones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigated a C-arm tomographic technique as a new three\ndimensional (3D) kidney imaging method for nephrolithiasis and kidney stone\ndetection over view angle less than 180o. Our C-arm tomographic technique\nprovides a series of two dimensional (2D) images with a single scan over 40o\nview angle. Experimental studies were performed with a kidney phantom that was\nformed from a pig kidney with two embedded kidney stones. Different\nreconstruction methods were developed for C-arm tomographic technique to\ngenerate 3D kidney information including: point by point back projection (BP),\nfiltered back projection (FBP), simultaneous algebraic reconstruction technique\n(SART) and maximum likelihood expectation maximization (MLEM). Computer\nsimulation study was also done with simulated 3D spherical object to evaluate\nthe reconstruction results. Preliminary results demonstrated the capability of\nour C-arm tomographic technique to generate 3D kidney information for kidney\nstone detection with low exposure of radiation. The kidney stones are visible\non reconstructed planes with identifiable shapes and sizes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 01:43:22 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Malalla", "Nuhad A.", ""], ["Chen", "Ying", ""]]}, {"id": "1706.02430", "submitter": "Zhongliang Yang", "authors": "Zhongliang Yang, Yu-Jin Zhang, Sadaqat ur Rehman, Yongfeng Huang", "title": "Image Captioning with Object Detection and Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generating a natural language description of an image is a task\nclose to the heart of image understanding. In this paper, we present a\nmulti-model neural network method closely related to the human visual system\nthat automatically learns to describe the content of images. Our model consists\nof two sub-models: an object detection and localization model, which extract\nthe information of objects and their spatial relationship in images\nrespectively; Besides, a deep recurrent neural network (RNN) based on long\nshort-term memory (LSTM) units with attention mechanism for sentences\ngeneration. Each word of the description will be automatically aligned to\ndifferent objects of the input image when it is generated. This is similar to\nthe attention mechanism of the human visual system. Experimental results on the\nCOCO dataset showcase the merit of the proposed method, which outperforms\nprevious benchmark models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 02:23:33 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Yang", "Zhongliang", ""], ["Zhang", "Yu-Jin", ""], ["Rehman", "Sadaqat ur", ""], ["Huang", "Yongfeng", ""]]}, {"id": "1706.02434", "submitter": "D\\'ario Oliveira", "authors": "Dario Augusto Borges Oliveira, Laura Leal-Taixe, Raul Queiroz Feitosa,\n  Bodo Rosenhahn", "title": "Automatic tracking of vessel-like structures from a single starting\n  point", "comments": null, "journal-ref": null, "doi": "10.1016/j.compmedimag.2015.11.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of vascular networks is an important topic in the medical\nimage analysis community. While most methods focus on single vessel tracking,\nthe few solutions that exist for tracking complete vascular networks are\nusually computationally intensive and require a lot of user interaction. In\nthis paper we present a method to track full vascular networks iteratively\nusing a single starting point. Our approach is based on a cloud of sampling\npoints distributed over concentric spherical layers. We also proposed a vessel\nmodel and a metric of how well a sample point fits this model. Then, we\nimplement the network tracking as a min-cost flow problem, and propose a novel\noptimization scheme to iteratively track the vessel structure by inherently\nhandling bifurcations and paths. The method was tested using both synthetic and\nreal images. On the 9 different data-sets of synthetic blood vessels, we\nachieved maximum accuracies of more than 98\\%. We further use the synthetic\ndata-set to analyse the sensibility of our method to parameter setting, showing\nthe robustness of the proposed algorithm. For real images, we used coronary,\ncarotid and pulmonary data to segment vascular structures and present the\nvisual results. Still for real images, we present numerical and visual results\nfor networks of nerve fibers in the olfactory system. Further visual results\nalso show the potential of our approach for identifying vascular networks\ntopologies. The presented method delivers good results for the several\ndifferent datasets tested and have potential for segmenting vessel-like\nstructures. Also, the topology information, inherently extracted, can be used\nfor further analysis to computed aided diagnosis and surgical planning.\nFinally, the method's modular aspect holds potential for problem-oriented\nadjustments and improvements.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 02:45:27 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Oliveira", "Dario Augusto Borges", ""], ["Leal-Taixe", "Laura", ""], ["Feitosa", "Raul Queiroz", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1706.02493", "submitter": "Zhe Wang", "authors": "Zhe Wang, Hongsheng Li, Wanli Ouyang, and Xiaogang Wang", "title": "Learning Deep Representations for Scene Labeling with Semantic Context\n  Guided Supervision", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene labeling is a challenging classification problem where each input image\nrequires a pixel-level prediction map. Recently, deep-learning-based methods\nhave shown their effectiveness on solving this problem. However, we argue that\nthe large intra-class variation provides ambiguous training information and\nhinders the deep models' ability to learn more discriminative deep feature\nrepresentations. Unlike existing methods that mainly utilize semantic context\nfor regularizing or smoothing the prediction map, we design novel supervisions\nfrom semantic context for learning better deep feature representations. Two\ntypes of semantic context, scene names of images and label map statistics of\nimage patches, are exploited to create label hierarchies between the original\nclasses and newly created subclasses as the learning supervisions. Such\nsubclasses show lower intra-class variation, and help CNN detect more\nmeaningful visual patterns and learn more effective deep features. Novel\ntraining strategies and network structure that take advantages of such label\nhierarchies are introduced. Our proposed method is evaluated extensively on\nfour popular datasets, Stanford Background (8 classes), SIFTFlow (33 classes),\nBarcelona (170 classes) and LM+Sun datasets (232 classes) with 3 different\nnetworks structures, and show state-of-the-art performance. The experiments\nshow that our proposed method makes deep models learn more discriminative\nfeature representations without increasing model size or complexity.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 09:44:00 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 04:15:55 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Wang", "Zhe", ""], ["Li", "Hongsheng", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1706.02577", "submitter": "Magnus Andersson", "authors": "Alvaro Rodriquez, Hanqing Zhang, Jonatan Klaminder, Tomas Brodin,\n  Patrik L. Andersson, Magnus Andersson", "title": "ToxTrac: a fast and robust software for tracking organisms", "comments": "File contains supplementary materials (user guide)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  1. Behavioral analysis based on video recording is becoming increasingly\npopular within research fields such as; ecology, medicine, ecotoxicology, and\ntoxicology. However, the programs available to analyze the data, which are;\nfree of cost, user-friendly, versatile, robust, fast and provide reliable\nstatistics for different organisms (invertebrates, vertebrates and mammals) are\nsignificantly limited.\n  2. We present an automated open-source executable software (ToxTrac) for\nimage-based tracking that can simultaneously handle several organisms monitored\nin a laboratory environment. We compare the performance of ToxTrac with current\naccessible programs on the web.\n  3. The main advantages of ToxTrac are: i) no specific knowledge of the\ngeometry of the tracked bodies is needed; ii) processing speed, ToxTrac can\noperate at a rate >25 frames per second in HD videos using modern desktop\ncomputers; iii) simultaneous tracking of multiple organisms in multiple arenas;\niv) integrated distortion correction and camera calibration; v) robust against\nfalse positives; vi) preservation of individual identification if crossing\noccurs; vii) useful statistics and heat maps in real scale are exported in:\nimage, text and excel formats.\n  4. ToxTrac can be used for high speed tracking of insects, fish, rodents or\nother species, and provides useful locomotor information. We suggest using\nToxTrac for future studies of animal behavior independent of research area.\nDownload ToxTrac here: https://toxtrac.sourceforge.io\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 13:37:38 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Rodriquez", "Alvaro", ""], ["Zhang", "Hanqing", ""], ["Klaminder", "Jonatan", ""], ["Brodin", "Tomas", ""], ["Andersson", "Patrik L.", ""], ["Andersson", "Magnus", ""]]}, {"id": "1706.02631", "submitter": "Zhiwu Huang", "authors": "Jiqing Wu, Zhiwu Huang, Dinesh Acharya, Wen Li, Janine Thoma, Danda\n  Pani Paudel, Luc Van Gool", "title": "Sliced Wasserstein Generative Models", "comments": "This paper is accepted by CVPR 2019, accidentally uploaded as a new\n  submission (arXiv:1904.05408, which has been withdrawn). The code is\n  available at this https URL https:// github.com/musikisomorphie/swd.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In generative modeling, the Wasserstein distance (WD) has emerged as a useful\nmetric to measure the discrepancy between generated and real data\ndistributions. Unfortunately, it is challenging to approximate the WD of\nhigh-dimensional distributions. In contrast, the sliced Wasserstein distance\n(SWD) factorizes high-dimensional distributions into their multiple\none-dimensional marginal distributions and is thus easier to approximate. In\nthis paper, we introduce novel approximations of the primal and dual SWD.\nInstead of using a large number of random projections, as it is done by\nconventional SWD approximation methods, we propose to approximate SWDs with a\nsmall number of parameterized orthogonal projections in an end-to-end deep\nlearning fashion. As concrete applications of our SWD approximations, we design\ntwo types of differentiable SWD blocks to equip modern generative\nframeworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In\nthe experiments, we not only show the superiority of the proposed generative\nmodels on standard image synthesis benchmarks, but also demonstrate the\nstate-of-the-art performance on challenging high resolution image and video\ngeneration in an unsupervised manner.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 15:16:36 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 16:58:22 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 09:01:58 GMT"}, {"version": "v4", "created": "Mon, 15 Apr 2019 20:56:19 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Wu", "Jiqing", ""], ["Huang", "Zhiwu", ""], ["Acharya", "Dinesh", ""], ["Li", "Wen", ""], ["Thoma", "Janine", ""], ["Paudel", "Danda Pani", ""], ["Van Gool", "Luc", ""]]}, {"id": "1706.02672", "submitter": "Kumar Sankar Ray", "authors": "Kumar S. Ray, Soma Chakraborty", "title": "An Efficient Approach for Object Detection and Tracking of Objects in a\n  Video with Variable Background", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach to create an automated visual\nsurveillance system which is very efficient in detecting and tracking moving\nobjects in a video captured by moving camera without any apriori information\nabout the captured scene. Separating foreground from the background is\nchallenging job in videos captured by moving camera as both foreground and\nbackground information change in every consecutive frames of the image\nsequence; thus a pseudo-motion is perceptive in background. In the proposed\nalgorithm, the pseudo-motion in background is estimated and compensated using\nphase correlation of consecutive frames based on the principle of Fourier shift\ntheorem. Then a method is proposed to model an acting background from recent\nhistory of commonality of the current frame and the foreground is detected by\nthe differences between the background model and the current frame. Further\nexploiting the recent history of dissimilarities of the current frame, actual\nmoving objects are detected in the foreground. Next, a two-stepped\nmorphological operation is proposed to refine the object region for an optimum\nobject size. Each object is attributed by its centroid, dimension and three\nhighest peaks of its gray value histogram. Finally, each object is tracked\nusing Kalman filter based on its attributes. The major advantage of this\nalgorithm over most of the existing object detection and tracking algorithms is\nthat, it does not require initialization of object position in the first frame\nor training on sample data to perform. Performance of the algorithm is tested\non benchmark videos containing variable background and very satisfiable results\nis achieved. The performance of the algorithm is also comparable with some of\nthe state-of-the-art algorithms for object detection and tracking.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 08:23:35 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Ray", "Kumar S.", ""], ["Chakraborty", "Soma", ""]]}, {"id": "1706.02677", "submitter": "Ross Girshick", "authors": "Priya Goyal, Piotr Doll\\'ar, Ross Girshick, Pieter Noordhuis, Lukasz\n  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, Kaiming He", "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour", "comments": "Tech report (v2: correct typos)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning thrives with large neural networks and large datasets. However,\nlarger networks and larger datasets result in longer training times that impede\nresearch and development progress. Distributed synchronous SGD offers a\npotential solution to this problem by dividing SGD minibatches over a pool of\nparallel workers. Yet to make this scheme efficient, the per-worker workload\nmust be large, which implies nontrivial growth in the SGD minibatch size. In\nthis paper, we empirically show that on the ImageNet dataset large minibatches\ncause optimization difficulties, but when these are addressed the trained\nnetworks exhibit good generalization. Specifically, we show no loss of accuracy\nwhen training with large minibatch sizes up to 8192 images. To achieve this\nresult, we adopt a hyper-parameter-free linear scaling rule for adjusting\nlearning rates as a function of minibatch size and develop a new warmup scheme\nthat overcomes optimization challenges early in training. With these simple\ntechniques, our Caffe2-based system trains ResNet-50 with a minibatch size of\n8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using\ncommodity hardware, our implementation achieves ~90% scaling efficiency when\nmoving from 8 to 256 GPUs. Our findings enable training visual recognition\nmodels on internet-scale data with high efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 16:51:53 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 21:53:41 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Goyal", "Priya", ""], ["Doll\u00e1r", "Piotr", ""], ["Girshick", "Ross", ""], ["Noordhuis", "Pieter", ""], ["Wesolowski", "Lukasz", ""], ["Kyrola", "Aapo", ""], ["Tulloch", "Andrew", ""], ["Jia", "Yangqing", ""], ["He", "Kaiming", ""]]}, {"id": "1706.02684", "submitter": "Jean-Charles Vialatte", "authors": "Jean-Charles Vialatte, Vincent Gripon, Gilles Coppin", "title": "Learning Local Receptive Fields and their Weight Sharing Scheme on\n  Graphs", "comments": "To appear in 2017, 5th IEEE Global Conference on Signal and\n  Information Processing, 5 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple and generic layer formulation that extends the properties\nof convolutional layers to any domain that can be described by a graph. Namely,\nwe use the support of its adjacency matrix to design learnable weight sharing\nfilters able to exploit the underlying structure of signals in the same fashion\nas for images. The proposed formulation makes it possible to learn the weights\nof the filter as well as a scheme that controls how they are shared across the\ngraph. We perform validation experiments with image datasets and show that\nthese filters offer performances comparable with convolutional ones.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 17:03:34 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 14:56:59 GMT"}, {"version": "v3", "created": "Thu, 5 Oct 2017 16:32:20 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Vialatte", "Jean-Charles", ""], ["Gripon", "Vincent", ""], ["Coppin", "Gilles", ""]]}, {"id": "1706.02698", "submitter": "Yu Zhang", "authors": "Daniel L. Lau, Yu Zhang, Kai Liu", "title": "Structured Light Phase Measuring Profilometry Pattern Design for Binary\n  Spatial Light Modulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Structured light illumination is an active 3-D scanning technique based on\nprojecting/capturing a set of striped patterns and measuring the warping of the\npatterns as they reflect off a target object's surface. In the case of phase\nmeasuring profilometry (PMP), the projected patterns are composed of a rolling\nsinusoidal wave, but as a set of time-multiplexed patterns, PMP requires the\ntarget surface to remain motionless or for scanning to be performed at such\nhigh rates that any movement is small. But high speed scanning places a\nsignificant burden on the projector electronics to produce contone patterns\ninside of short exposure intervals. Binary patterns are, therefore, of great\nvalue, but converting contone patterns into binary comes with significant risk.\nAs such, this paper introduces a contone-to-binary conversion algorithm for\nderiving binary patterns that best mimic their contone counterparts.\nExperimental results will show a greater than 3 times reduction in pattern\nnoise over traditional halftoning procedures.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 17:58:10 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Lau", "Daniel L.", ""], ["Zhang", "Yu", ""], ["Liu", "Kai", ""]]}, {"id": "1706.02715", "submitter": "Yu Zhang", "authors": "Yu Zhang, Daniel L. Lau, and Ying Yu", "title": "Causes and Corrections for Bimodal Multipath Scanning with Structured\n  Light", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Structured light illumination is an active 3-D scanning technique based on\nprojecting/capturing a set of striped patterns and measuring the warping of the\npatterns as they reflect off a target object's surface. As designed, each pixel\nin the camera sees exactly one pixel from the projector; however, there are\nexceptions to this when the scanned surface has a complicated geometry with\nstep edges and other discontinuities in depth or where the target surface has\nspecularities that reflect light away from the camera. These situations are\ngenerally referred to multipath where a given camera pixel receives light from\nmultiple positions from the projector. In the case of bimodal multipath, the\ncamera pixel receives light from exactly two positions from the projector which\noccurs when light bounce back from a reflective surface or along a step edge\nwhere the edge slices through a pixel so that the pixel sees both a foreground\nand background surface. In this paper, we present a general mathematical model\nand address the bimodal multipath issue in a phase measuring profilometry\nscanner to measure the constructive and destructive interference between the\ntwo light paths, and by taking advantage of this interesting cue, separate the\npaths and make two separated depth measurements. We also validate our algorithm\nwith both simulation and a number of challenging real cases.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 18:01:04 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Zhang", "Yu", ""], ["Lau", "Daniel L.", ""], ["Yu", "Ying", ""]]}, {"id": "1706.02735", "submitter": "Alfredo Canziani", "authors": "Alfredo Canziani, Eugenio Culurciello", "title": "CortexNet: a Generic Network Family for Robust Visual Temporal\n  Representations", "comments": "8 pages, 4 figures. Edit: 4.2 - define n = t - 1; fix grammar/meaning\n  in last sentence. 5.2 - add Open Images data set ref", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past five years we have observed the rise of incredibly well\nperforming feed-forward neural networks trained supervisedly for vision related\ntasks. These models have achieved super-human performance on object\nrecognition, localisation, and detection in still images. However, there is a\nneed to identify the best strategy to employ these networks with temporal\nvisual inputs and obtain a robust and stable representation of video data.\nInspired by the human visual system, we propose a deep neural network family,\nCortexNet, which features not only bottom-up feed-forward connections, but also\nit models the abundant top-down feedback and lateral connections, which are\npresent in our visual cortex. We introduce two training schemes - the\nunsupervised MatchNet and weakly supervised TempoNet modes - where a network\nlearns how to correctly anticipate a subsequent frame in a video clip or the\nidentity of its predominant subject, by learning egomotion clues and how to\nautomatically track several objects in the current scene. Find the project\nwebsite at https://engineering.purdue.edu/elab/CortexNet/.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 19:17:52 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 17:53:32 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Canziani", "Alfredo", ""], ["Culurciello", "Eugenio", ""]]}, {"id": "1706.02823", "submitter": "Wenqi Xian", "authors": "Wenqi Xian, Patsorn Sangkloy, Varun Agrawal, Amit Raj, Jingwan Lu,\n  Chen Fang, Fisher Yu, James Hays", "title": "TextureGAN: Controlling Deep Image Synthesis with Texture Patches", "comments": "CVPR 2018 spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate deep image synthesis guided by sketch, color,\nand texture. Previous image synthesis methods can be controlled by sketch and\ncolor strokes but we are the first to examine texture control. We allow a user\nto place a texture patch on a sketch at arbitrary locations and scales to\ncontrol the desired output texture. Our generative network learns to synthesize\nobjects consistent with these texture suggestions. To achieve this, we develop\na local texture loss in addition to adversarial and content loss to train the\ngenerative network. We conduct experiments using sketches generated from real\nimages and textures sampled from a separate texture database and results show\nthat our proposed algorithm is able to generate plausible images that are\nfaithful to user controls. Ablation studies show that our proposed pipeline can\ngenerate more realistic images than adapting existing methods directly.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 03:35:08 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 08:19:15 GMT"}, {"version": "v3", "created": "Sat, 14 Apr 2018 20:11:56 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Xian", "Wenqi", ""], ["Sangkloy", "Patsorn", ""], ["Agrawal", "Varun", ""], ["Raj", "Amit", ""], ["Lu", "Jingwan", ""], ["Fang", "Chen", ""], ["Yu", "Fisher", ""], ["Hays", "James", ""]]}, {"id": "1706.02850", "submitter": "Alessandro Corbetta", "authors": "Alessandro Corbetta, Vlado Menkovski, Federico Toschi", "title": "Weakly supervised training of deep convolutional neural networks for\n  overhead pedestrian localization in depth fields", "comments": null, "journal-ref": "Advanced Video and Signal Based Surveillance (AVSS), 2017 14th\n  IEEE International Conference on", "doi": "10.1109/AVSS.2017.8078490", "report-no": null, "categories": "cs.CV cs.NE physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overhead depth map measurements capture sufficient amount of information to\nenable human experts to track pedestrians accurately. However, fully automating\nthis process using image analysis algorithms can be challenging. Even though\nhand-crafted image analysis algorithms are successful in many common cases,\nthey fail frequently when there are complex interactions of multiple objects in\nthe image. Many of the assumptions underpinning the hand-crafted solutions do\nnot hold in these cases and the multitude of exceptions are hard to model\nprecisely. Deep Learning (DL) algorithms, on the other hand, do not require\nhand crafted solutions and are the current state-of-the-art in object\nlocalization in images. However, they require exceeding amount of annotations\nto produce successful models. In the case of object localization these\nannotations are difficult and time consuming to produce. In this work we\npresent an approach for developing pedestrian localization models using DL\nalgorithms with efficient weak supervision from an expert. We circumvent the\nneed for annotation of large corpus of data by annotating only small amount of\npatches and relying on synthetic data augmentation as a vehicle for injecting\nexpert knowledge in the model training. This approach of weak supervision\nthrough expert selection of representative patches, suitable transformations\nand synthetic data augmentations enables us to successfully develop DL models\nfor pedestrian localization efficiently.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 07:14:08 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Corbetta", "Alessandro", ""], ["Menkovski", "Vlado", ""], ["Toschi", "Federico", ""]]}, {"id": "1706.02863", "submitter": "Shuo Yang", "authors": "Shuo Yang, Yuanjun Xiong, Chen Change Loy, Xiaoou Tang", "title": "Face Detection through Scale-Friendly Deep Convolutional Networks", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we share our experience in designing a convolutional\nnetwork-based face detector that could handle faces of an extremely wide range\nof scales. We show that faces with different scales can be modeled through a\nspecialized set of deep convolutional networks with different structures. These\ndetectors can be seamlessly integrated into a single unified network that can\nbe trained end-to-end. In contrast to existing deep models that are designed\nfor wide scale range, our network does not require an image pyramid input and\nthe model is of modest complexity. Our network, dubbed ScaleFace, achieves\npromising performance on WIDER FACE and FDDB datasets with practical runtime\nspeed. Specifically, our method achieves 76.4 average precision on the\nchallenging WIDER FACE dataset and 96% recall rate on the FDDB dataset with 7\nframes per second (fps) for 900 * 1300 input image.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 08:20:56 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Yang", "Shuo", ""], ["Xiong", "Yuanjun", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1706.02867", "submitter": "Milad Niknejad", "authors": "Milad Niknejad, Jose M. Bioucas-Dias, Mario A. T. Figueiredo", "title": "Class-specific Poisson denoising by patch-based importance sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of recovering images degraded by\nPoisson noise, where the image is known to belong to a specific class. In the\nproposed method, a dataset of clean patches from images of the class of\ninterest is clustered using multivariate Gaussian distributions. In order to\nrecover the noisy image, each noisy patch is assigned to one of these\ndistributions, and the corresponding minimum mean squared error (MMSE) estimate\nis obtained. We propose to use a self-normalized importance sampling approach,\nwhich is a method of the Monte-Carlo family, for the both determining the most\nlikely distribution and approximating the MMSE estimate of the clean patch.\nExperimental results shows that our proposed method outperforms other methods\nfor Poisson denoising at a low SNR regime.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 08:47:26 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Niknejad", "Milad", ""], ["Bioucas-Dias", "Jose M.", ""], ["Figueiredo", "Mario A. T.", ""]]}, {"id": "1706.02884", "submitter": "Serena Yeung", "authors": "Serena Yeung, Vignesh Ramanathan, Olga Russakovsky, Liyue Shen, Greg\n  Mori, Li Fei-Fei", "title": "Learning to Learn from Noisy Web Videos", "comments": "To appear in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the simultaneously very diverse and intricately fine-grained\nset of possible human actions is a critical open problem in computer vision.\nManually labeling training videos is feasible for some action classes but\ndoesn't scale to the full long-tailed distribution of actions. A promising way\nto address this is to leverage noisy data from web queries to learn new\nactions, using semi-supervised or \"webly-supervised\" approaches. However, these\nmethods typically do not learn domain-specific knowledge, or rely on iterative\nhand-tuned data labeling policies. In this work, we instead propose a\nreinforcement learning-based formulation for selecting the right examples for\ntraining a classifier from noisy web search results. Our method uses Q-learning\nto learn a data labeling policy on a small labeled training dataset, and then\nuses this to automatically label noisy web data for new visual concepts.\nExperiments on the challenging Sports-1M action recognition benchmark as well\nas on additional fine-grained and newly emerging action classes demonstrate\nthat our method is able to learn good labeling policies for noisy data and use\nthis to learn accurate visual concept classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 10:25:05 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Yeung", "Serena", ""], ["Ramanathan", "Vignesh", ""], ["Russakovsky", "Olga", ""], ["Shen", "Liyue", ""], ["Mori", "Greg", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1706.02888", "submitter": "Joakim Johnander", "authors": "Joakim Johnander, Martin Danelljan, Fahad Shahbaz Khan, Michael\n  Felsberg", "title": "DCCO: Towards Deformable Continuous Convolution Operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative Correlation Filter (DCF) based methods have shown competitive\nperformance on tracking benchmarks in recent years. Generally, DCF based\ntrackers learn a rigid appearance model of the target. However, this reliance\non a single rigid appearance model is insufficient in situations where the\ntarget undergoes non-rigid transformations. In this paper, we propose a unified\nformulation for learning a deformable convolution filter. In our framework, the\ndeformable filter is represented as a linear combination of sub-filters. Both\nthe sub-filter coefficients and their relative locations are inferred jointly\nin our formulation. Experiments are performed on three challenging tracking\nbenchmarks: OTB- 2015, TempleColor and VOT2016. Our approach improves the\nbaseline method, leading to performance comparable to state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 10:39:32 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Johnander", "Joakim", ""], ["Danelljan", "Martin", ""], ["Khan", "Fahad Shahbaz", ""], ["Felsberg", "Michael", ""]]}, {"id": "1706.02889", "submitter": "Antonio Pertusa", "authors": "Antonio Pertusa, Antonio-Javier Gallego, Marisa Bernabeu", "title": "MirBot: A collaborative object recognition system for smartphones using\n  convolutional neural networks", "comments": "Accepted in Neurocomputing, 2018", "journal-ref": "Neurocomputing, vol 293, 2018, Pages 87-99", "doi": "10.1016/j.neucom.2018.03.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MirBot is a collaborative application for smartphones that allows users to\nperform object recognition. This app can be used to take a photograph of an\nobject, select the region of interest and obtain the most likely class (dog,\nchair, etc.) by means of similarity search using features extracted from a\nconvolutional neural network (CNN). The answers provided by the system can be\nvalidated by the user so as to improve the results for future queries. All the\nimages are stored together with a series of metadata, thus enabling a\nmultimodal incremental dataset labeled with synset identifiers from the WordNet\nontology. This dataset grows continuously thanks to the users' feedback, and is\npublicly available for research. This work details the MirBot object\nrecognition system, analyzes the statistics gathered after more than four years\nof usage, describes the image classification methodology, and performs an\nexhaustive evaluation using handcrafted features, convolutional neural codes\nand different transfer learning techniques. After comparing various models and\ntransformation methods, the results show that the CNN features maintain the\naccuracy of MirBot constant over time, despite the increasing number of new\nclasses. The app is freely available at the Apple and Google Play stores.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 10:50:43 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 08:34:12 GMT"}, {"version": "v3", "created": "Sat, 24 Mar 2018 08:30:28 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Pertusa", "Antonio", ""], ["Gallego", "Antonio-Javier", ""], ["Bernabeu", "Marisa", ""]]}, {"id": "1706.02908", "submitter": "Mikkel Fly Kragh", "authors": "Mikkel Kragh and James Underwood", "title": "Multi-Modal Obstacle Detection in Unstructured Environments with\n  Conditional Random Fields", "comments": "This is the accepted version of the following article: Kragh M,\n  Underwood J. Multimodal obstacle detection in unstructured environments with\n  conditional random fields. J Field Robotics. 2019, 1-20., which has been\n  published in final form at https://doi.org/10.1002/rob.21866", "journal-ref": null, "doi": "10.1002/rob.21866", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable obstacle detection and classification in rough and unstructured\nterrain such as agricultural fields or orchards remains a challenging problem.\nThese environments involve large variations in both geometry and appearance,\nchallenging perception systems that rely on only a single sensor modality.\nGeometrically, tall grass, fallen leaves, or terrain roughness can mistakenly\nbe perceived as nontraversable or might even obscure actual obstacles.\nLikewise, traversable grass or dirt roads and obstacles such as trees and\nbushes might be visually ambiguous. In this paper, we combine appearance- and\ngeometry-based detection methods by probabilistically fusing lidar and camera\nsensing with semantic segmentation using a conditional random field. We apply a\nstate-of-the-art multimodal fusion algorithm from the scene analysis domain and\nadjust it for obstacle detection in agriculture with moving ground vehicles.\nThis involves explicitly handling sparse point cloud data and exploiting both\nspatial, temporal, and multimodal links between corresponding 2D and 3D\nregions. The proposed method was evaluated on a diverse data set, comprising a\ndairy paddock and different orchards gathered with a perception research robot\nin Australia. Results showed that for a two-class classification problem\n(ground and nonground), only the camera leveraged from information provided by\nthe other modality with an increase in the mean classification score of 0.5%.\nHowever, as more classes were introduced (ground, sky, vegetation, and object),\nboth modalities complemented each other with improvements of 1.4% in 2D and\n7.9% in 3D. Finally, introducing temporal links between successive frames\nresulted in improvements of 0.2% in 2D and 1.5% in 3D.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 11:48:25 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 14:04:28 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Kragh", "Mikkel", ""], ["Underwood", "James", ""]]}, {"id": "1706.02932", "submitter": "James Thewlis", "authors": "James Thewlis and Hakan Bilen and Andrea Vedaldi", "title": "Unsupervised learning of object frames by dense equivariant image\n  labelling", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges of visual perception is to extract abstract models\nof 3D objects and object categories from visual measurements, which are\naffected by complex nuisance factors such as viewpoint, occlusion, motion, and\ndeformations. Starting from the recent idea of viewpoint factorization, we\npropose a new approach that, given a large number of images of an object and no\nother supervision, can extract a dense object-centric coordinate frame. This\ncoordinate frame is invariant to deformations of the images and comes with a\ndense equivariant labelling neural network that can map image pixels to their\ncorresponding object coordinates. We demonstrate the applicability of this\nmethod to simple articulated objects and deformable objects such as human\nfaces, learning embeddings from random synthetic transformations or optical\nflow correspondences, all without any manual supervision.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 12:49:36 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 02:36:48 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Thewlis", "James", ""], ["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1706.02933", "submitter": "Carlos Jos\\'e D\\'iaz Baso", "authors": "C.J. Diaz Baso and A. Asensio Ramos", "title": "Enhancing SDO/HMI images using deep learning", "comments": "13 pages, 10 figures. Accepted for publication in Astronomy &\n  Astrophysics", "journal-ref": "A&A 614, A5 (2018)", "doi": "10.1051/0004-6361/201731344", "report-no": null, "categories": "astro-ph.SR astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Helioseismic and Magnetic Imager (HMI) provides continuum images and\nmagnetograms with a cadence better than one per minute. It has been\ncontinuously observing the Sun 24 hours a day for the past 7 years. The obvious\ntrade-off between full disk observations and spatial resolution makes HMI not\nenough to analyze the smallest-scale events in the solar atmosphere. Our aim is\nto develop a new method to enhance HMI data, simultaneously deconvolving and\nsuper-resolving images and magnetograms. The resulting images will mimic\nobservations with a diffraction-limited telescope twice the diameter of HMI.\nOur method, which we call Enhance, is based on two deep fully convolutional\nneural networks that input patches of HMI observations and output deconvolved\nand super-resolved data. The neural networks are trained on synthetic data\nobtained from simulations of the emergence of solar active regions. We have\nobtained deconvolved and supper-resolved HMI images. To solve this ill-defined\nproblem with infinite solutions we have used a neural network approach to add\nprior information from the simulations. We test Enhance against Hinode data\nthat has been degraded to a 28 cm diameter telescope showing very good\nconsistency. The code is open source.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 12:54:03 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 19:45:05 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Baso", "C. J. Diaz", ""], ["Ramos", "A. Asensio", ""]]}, {"id": "1706.03008", "submitter": "Jos\\'e Ignacio Orlando Eng", "authors": "Jos\\'e Ignacio Orlando, Elena Prokofyeva, Mariana del Fresno and\n  Matthew B. Blaschko", "title": "An Ensemble Deep Learning Based Approach for Red Lesion Detection in\n  Fundus Images", "comments": "Accepted for publication in Computer Methods and Programs in\n  Biomedicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy is one of the leading causes of preventable blindness in\nthe world. Its earliest sign are red lesions, a general term that groups both\nmicroaneurysms and hemorrhages. In daily clinical practice, these lesions are\nmanually detected by physicians using fundus photographs. However, this task is\ntedious and time consuming, and requires an intensive effort due to the small\nsize of the lesions and their lack of contrast. Computer-assisted diagnosis of\nDR based on red lesion detection is being actively explored due to its\nimprovement effects both in clinicians consistency and accuracy. Several\nmethods for detecting red lesions have been proposed in the literature, most of\nthem based on characterizing lesion candidates using hand crafted features, and\nclassifying them into true or false positive detections. Deep learning based\napproaches, by contrast, are scarce in this domain due to the high expense of\nannotating the lesions manually. In this paper we propose a novel method for\nred lesion detection based on combining both deep learned and domain knowledge.\nFeatures learned by a CNN are augmented by incorporating hand crafted features.\nSuch ensemble vector of descriptors is used afterwards to identify true lesion\ncandidates using a Random Forest classifier. We empirically observed that\ncombining both sources of information significantly improve results with\nrespect to using each approach separately. Furthermore, our method reported the\nhighest performance on a per-lesion basis on DIARETDB1 and e-ophtha, and for\nscreening and need for referral on MESSIDOR compared to a second human expert.\nResults highlight the fact that integrating manually engineered approaches with\ndeep learned features is relevant to improve results when the networks are\ntrained from lesion-level annotated data. An open source implementation of our\nsystem is publicly available online.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 15:47:11 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 19:44:36 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Orlando", "Jos\u00e9 Ignacio", ""], ["Prokofyeva", "Elena", ""], ["del Fresno", "Mariana", ""], ["Blaschko", "Matthew B.", ""]]}, {"id": "1706.03015", "submitter": "Jie Miao", "authors": "Jie Miao, Xiangmin Xu, Xiaofen Xing, Dacheng Tao", "title": "Manifold Regularized Slow Feature Analysis for Dynamic Texture\n  Recognition", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic textures exist in various forms, e.g., fire, smoke, and traffic jams,\nbut recognizing dynamic texture is challenging due to the complex temporal\nvariations. In this paper, we present a novel approach stemmed from slow\nfeature analysis (SFA) for dynamic texture recognition. SFA extracts slowly\nvarying features from fast varying signals. Fortunately, SFA is capable to\nleach invariant representations from dynamic textures. However, complex\ntemporal variations require high-level semantic representations to fully\nachieve temporal slowness, and thus it is impractical to learn a high-level\nrepresentation from dynamic textures directly by SFA. In order to learn a\nrobust low-level feature to resolve the complexity of dynamic textures, we\npropose manifold regularized SFA (MR-SFA) by exploring the neighbor\nrelationship of the initial state of each temporal transition and retaining the\nlocality of their variations. Therefore, the learned features are not only\nslowly varying, but also partly predictable. MR-SFA for dynamic texture\nrecognition is proposed in the following steps: 1) learning feature extraction\nfunctions as convolution filters by MR-SFA, 2) extracting local features by\nconvolution and pooling, and 3) employing Fisher vectors to form a video-level\nrepresentation for classification. Experimental results on dynamic texture and\ndynamic scene recognition datasets validate the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 16:06:25 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Miao", "Jie", ""], ["Xu", "Xiangmin", ""], ["Xing", "Xiaofen", ""], ["Tao", "Dacheng", ""]]}, {"id": "1706.03038", "submitter": "Mohammadamin Barekatain", "authors": "Mohammadamin Barekatain, Miquel Mart\\'i, Hsueh-Fu Shih, Samuel Murray,\n  Kotaro Nakayama, Yutaka Matsuo and Helmut Prendinger", "title": "Okutama-Action: An Aerial View Video Dataset for Concurrent Human Action\n  Detection", "comments": "Computer Vision and Pattern Recognition Workshops (CVPRW), Hawaii,\n  USA, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress in the development of human action detection\ndatasets and algorithms, no current dataset is representative of real-world\naerial view scenarios. We present Okutama-Action, a new video dataset for\naerial view concurrent human action detection. It consists of 43 minute-long\nfully-annotated sequences with 12 action classes. Okutama-Action features many\nchallenges missing in current datasets, including dynamic transition of\nactions, significant changes in scale and aspect ratio, abrupt camera movement,\nas well as multi-labeled actors. As a result, our dataset is more challenging\nthan existing ones, and will help push the field forward to enable real-world\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 16:54:51 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 16:04:01 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Barekatain", "Mohammadamin", ""], ["Mart\u00ed", "Miquel", ""], ["Shih", "Hsueh-Fu", ""], ["Murray", "Samuel", ""], ["Nakayama", "Kotaro", ""], ["Matsuo", "Yutaka", ""], ["Prendinger", "Helmut", ""]]}, {"id": "1706.03112", "submitter": "Rameswar Panda", "authors": "Rameswar Panda, Amran Bhuiyan, Vittorio Murino, Amit K. Roy-Chowdhury", "title": "Unsupervised Adaptive Re-identification in Open World Dynamic Camera\n  Networks", "comments": "CVPR 2017 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is an open and challenging problem in computer\nvision. Existing approaches have concentrated on either designing the best\nfeature representation or learning optimal matching metrics in a static setting\nwhere the number of cameras are fixed in a network. Most approaches have\nneglected the dynamic and open world nature of the re-identification problem,\nwhere a new camera may be temporarily inserted into an existing system to get\nadditional information. To address such a novel and very practical problem, we\npropose an unsupervised adaptation scheme for re-identification models in a\ndynamic camera network. First, we formulate a domain perceptive\nre-identification method based on geodesic flow kernel that can effectively\nfind the best source camera (already installed) to adapt with a newly\nintroduced target camera, without requiring a very expensive training phase.\nSecond, we introduce a transitive inference algorithm for re-identification\nthat can exploit the information from best source camera to improve the\naccuracy across other camera pairs in a network of multiple cameras. Extensive\nexperiments on four benchmark datasets demonstrate that the proposed approach\nsignificantly outperforms the state-of-the-art unsupervised learning based\nalternatives whilst being extremely efficient to compute.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 20:17:55 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Panda", "Rameswar", ""], ["Bhuiyan", "Amran", ""], ["Murino", "Vittorio", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1706.03114", "submitter": "Rameswar Panda", "authors": "Rameswar Panda, Amit K. Roy-Chowdhury", "title": "Collaborative Summarization of Topic-Related Videos", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large collections of videos are grouped into clusters by a topic keyword,\nsuch as Eiffel Tower or Surfing, with many important visual concepts repeating\nacross them. Such a topically close set of videos have mutual influence on each\nother, which could be used to summarize one of them by exploiting information\nfrom others in the set. We build on this intuition to develop a novel approach\nto extract a summary that simultaneously captures both important\nparticularities arising in the given video, as well as, generalities identified\nfrom the set of videos. The topic-related videos provide visual context to\nidentify the important parts of the video being summarized. We achieve this by\ndeveloping a collaborative sparse optimization method which can be efficiently\nsolved by a half-quadratic minimization algorithm. Our work builds upon the\nidea of collaborative techniques from information retrieval and natural\nlanguage processing, which typically use the attributes of other similar\nobjects to predict the attribute of a given object. Experiments on two\nchallenging and diverse datasets well demonstrate the efficacy of our approach\nover state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 20:23:43 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Panda", "Rameswar", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1706.03121", "submitter": "Rameswar Panda", "authors": "Rameswar Panda, Amit K. Roy-Chowdhury", "title": "Multi-View Surveillance Video Summarization via Joint Embedding and\n  Sparse Optimization", "comments": "IEEE Trans. on Multimedia, 2017 (In Press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most traditional video summarization methods are designed to generate\neffective summaries for single-view videos, and thus they cannot fully exploit\nthe complicated intra and inter-view correlations in summarizing multi-view\nvideos in a camera network. In this paper, with the aim of summarizing\nmulti-view videos, we introduce a novel unsupervised framework via joint\nembedding and sparse representative selection. The objective function is\ntwo-fold. The first is to capture the multi-view correlations via an embedding,\nwhich helps in extracting a diverse set of representatives. The second is to\nuse a `2;1- norm to model the sparsity while selecting representative shots for\nthe summary. We propose to jointly optimize both of the objectives, such that\nembedding can not only characterize the correlations, but also indicate the\nrequirements of sparse representative selection. We present an efficient\nalternating algorithm based on half-quadratic minimization to solve the\nproposed non-smooth and non-convex objective with convergence analysis. A key\nadvantage of the proposed approach with respect to the state-of-the-art is that\nit can summarize multi-view videos without assuming any prior\ncorrespondences/alignment between them, e.g., uncalibrated camera networks.\nRigorous experiments on several multi-view datasets demonstrate that our\napproach clearly outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 20:56:20 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Panda", "Rameswar", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1706.03123", "submitter": "Rameswar Panda", "authors": "Rameswar Panda, Niluthpol Chowdhury Mithun, Amit K. Roy-Chowdhury", "title": "Diversity-aware Multi-Video Summarization", "comments": "IEEE Trans. on Image Processing, 2017 (In Press)", "journal-ref": null, "doi": "10.1109/TIP.2017.2708902", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most video summarization approaches have focused on extracting a summary from\na single video; we propose an unsupervised framework for summarizing a\ncollection of videos. We observe that each video in the collection may contain\nsome information that other videos do not have, and thus exploring the\nunderlying complementarity could be beneficial in creating a diverse\ninformative summary. We develop a novel diversity-aware sparse optimization\nmethod for multi-video summarization by exploring the complementarity within\nthe videos. Our approach extracts a multi-video summary which is both\ninteresting and representative in describing the whole video collection. To\nefficiently solve our optimization problem, we develop an alternating\nminimization algorithm that minimizes the overall objective function with\nrespect to one video at a time while fixing the other videos. Moreover, we\nintroduce a new benchmark dataset, Tour20, that contains 140 videos with\nmultiple human created summaries, which were acquired in a controlled\nexperiment. Finally, by extensive experiments on the new Tour20 dataset and\nseveral other multi-view datasets, we show that the proposed approach clearly\noutperforms the state-of-the-art methods on the two problems-topic-oriented\nvideo summarization and multi-view video summarization in a camera network.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 20:58:07 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Panda", "Rameswar", ""], ["Mithun", "Niluthpol Chowdhury", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "1706.03129", "submitter": "Ali Taimori", "authors": "Ali Taimori, Farokh Marvasti", "title": "Measurement-Adaptive Sparse Image Sampling and Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an adaptive and intelligent sparse model for digital\nimage sampling and recovery. In the proposed sampler, we adaptively determine\nthe number of required samples for retrieving image based on\nspace-frequency-gradient information content of image patches. By leveraging\ntexture in space, sparsity locations in DCT domain, and directional\ndecomposition of gradients, the sampler structure consists of a combination of\nuniform, random, and nonuniform sampling strategies. For reconstruction, we\nmodel the recovery problem as a two-state cellular automaton to iteratively\nrestore image with scalable windows from generation to generation. We\ndemonstrate the recovery algorithm quickly converges after a few generations\nfor an image with arbitrary degree of texture. For a given number of\nmeasurements, extensive experiments on standard image-sets, infra-red, and\nmega-pixel range imaging devices show that the proposed measurement matrix\nconsiderably increases the overall recovery performance, or equivalently\ndecreases the number of sampled pixels for a specific recovery quality compared\nto random sampling matrix and Gaussian linear combinations employed by the\nstate-of-the-art compressive sensing methods. In practice, the proposed\nmeasurement-adaptive sampling/recovery framework includes various applications\nfrom intelligent compressive imaging-based acquisition devices to computer\nvision and graphics, and image processing technology. Simulation codes are\navailable online for reproduction purposes.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 21:05:37 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 19:51:17 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Taimori", "Ali", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1706.03142", "submitter": "Stephan Saalfeld", "authors": "Larissa Heinrich, John A. Bogovic, Stephan Saalfeld", "title": "Deep Learning for Isotropic Super-Resolution from Non-Isotropic 3D\n  Electron Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most sophisticated existing methods to generate 3D isotropic\nsuper-resolution (SR) from non-isotropic electron microscopy (EM) are based on\nlearned dictionaries. Unfortunately, none of the existing methods generate\npractically satisfying results. For 2D natural images, recently developed\nsuper-resolution methods that use deep learning have been shown to\nsignificantly outperform the previous state of the art.\n  We have adapted one of the most successful architectures (FSRCNN) for 3D\nsuper-resolution, and compared its performance to a 3D U-Net architecture that\nhas not been used previously to generate super-resolution.\n  We trained both architectures on artificially downscaled isotropic ground\ntruth from focused ion beam milling scanning EM (FIB-SEM) and tested the\nperformance for various hyperparameter settings.\n  Our results indicate that both architectures can successfully generate 3D\nisotropic super-resolution from non-isotropic EM, with the U-Net performing\nconsistently better. We propose several promising directions for practical\napplication.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 22:17:16 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Heinrich", "Larissa", ""], ["Bogovic", "John A.", ""], ["Saalfeld", "Stephan", ""]]}, {"id": "1706.03154", "submitter": "Fan Yang", "authors": "Fan Yang, Ajinkya Kale, Yury Bubnov, Leon Stein, Qiaosong Wang, Hadi\n  Kiapour, Robinson Piramuthu", "title": "Visual Search at eBay", "comments": "To appear in 23rd SIGKDD Conference on Knowledge Discovery and Data\n  Mining (KDD), 2017. A demonstration video can be found at\n  https://youtu.be/iYtjs32vh4g", "journal-ref": null, "doi": "10.1145/3097983.3098162", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel end-to-end approach for scalable visual\nsearch infrastructure. We discuss the challenges we faced for a massive\nvolatile inventory like at eBay and present our solution to overcome those. We\nharness the availability of large image collection of eBay listings and\nstate-of-the-art deep learning techniques to perform visual search at scale.\nSupervised approach for optimized search limited to top predicted categories\nand also for compact binary signature are key to scale up without compromising\naccuracy and precision. Both use a common deep neural network requiring only a\nsingle forward inference. The system architecture is presented with in-depth\ndiscussions of its basic components and optimizations for a trade-off between\nsearch relevance and latency. This solution is currently deployed in a\ndistributed cloud infrastructure and fuels visual search in eBay ShopBot and\nClose5. We show benchmark on ImageNet dataset on which our approach is faster\nand more accurate than several unsupervised baselines. We share our learnings\nwith the hope that visual search becomes a first class citizen for all large\nscale search engines rather than an afterthought.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 00:02:34 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 17:21:23 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Yang", "Fan", ""], ["Kale", "Ajinkya", ""], ["Bubnov", "Yury", ""], ["Stein", "Leon", ""], ["Wang", "Qiaosong", ""], ["Kiapour", "Hadi", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1706.03160", "submitter": "Lin Wu", "authors": "Lin Wu, Yang Wang, Junbin Gao, Xue Li", "title": "Deep Adaptive Feature Embedding with Local Sample Distributions for\n  Person Re-identification", "comments": "Published on Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2017.08.029", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id) aims to match pedestrians observed by\ndisjoint camera views. It attracts increasing attention in computer vision due\nto its importance to surveillance system. To combat the major challenge of\ncross-view visual variations, deep embedding approaches are proposed by\nlearning a compact feature space from images such that the Euclidean distances\ncorrespond to their cross-view similarity metric. However, the global Euclidean\ndistance cannot faithfully characterize the ideal similarity in a complex\nvisual feature space because features of pedestrian images exhibit unknown\ndistributions due to large variations in poses, illumination and occlusion.\nMoreover, intra-personal training samples within a local range are robust to\nguide deep embedding against uncontrolled variations, which however, cannot be\ncaptured by a global Euclidean distance. In this paper, we study the problem of\nperson re-id by proposing a novel sampling to mine suitable \\textit{positives}\n(i.e. intra-class) within a local range to improve the deep embedding in the\ncontext of large intra-class variations. Our method is capable of learning a\ndeep similarity metric adaptive to local sample structure by minimizing each\nsample's local distances while propagating through the relationship between\nsamples to attain the whole intra-class minimization. To this end, a novel\nobjective function is proposed to jointly optimize similarity metric learning,\nlocal positive mining and robust deep embedding. This yields local\ndiscriminations by selecting local-ranged positive samples, and the learned\nfeatures are robust to dramatic intra-class variations. Experiments on\nbenchmarks show state-of-the-art results achieved by our method.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 00:49:43 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 02:37:44 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Wu", "Lin", ""], ["Wang", "Yang", ""], ["Gao", "Junbin", ""], ["Li", "Xue", ""]]}, {"id": "1706.03182", "submitter": "Chenchu Xu", "authors": "Chenchu Xu, Lei Xu, Zhifan Gao, Shen zhao, Heye Zhang, Yanping Zhang,\n  Xiuquan Du, Shu Zhao, Dhanjoo Ghista, Shuo Li", "title": "Direct detection of pixel-level myocardial infarction areas via a\n  deep-learning algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection of the myocardial infarction (MI) area is crucial for\nearly diagnosis planning and follow-up management. In this study, we propose an\nend-to-end deep-learning algorithm framework (OF-RNN ) to accurately detect the\nMI area at the pixel level. Our OF-RNN consists of three different function\nlayers: the heart localization layers, which can accurately and automatically\ncrop the region-of-interest (ROI) sequences, including the left ventricle,\nusing the whole cardiac magnetic resonance image sequences; the motion\nstatistical layers, which are used to build a time-series architecture to\ncapture two types of motion features (at the pixel-level) by integrating the\nlocal motion features generated by long short-term memory-recurrent neural\nnetworks and the global motion features generated by deep optical flows from\nthe whole ROI sequence, which can effectively characterize myocardial\nphysiologic function; and the fully connected discriminate layers, which use\nstacked auto-encoders to further learn these features, and they use a softmax\nclassifier to build the correspondences from the motion features to the tissue\nidentities (infarction or not) for each pixel. Through the seamless connection\nof each layer, our OF-RNN can obtain the area, position, and shape of the MI\nfor each patient. Our proposed framework yielded an overall classification\naccuracy of 94.35% at the pixel level, from 114 clinical subjects. These\nresults indicate the potential of our proposed method in aiding standardized MI\nassessments.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 05:03:38 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Xu", "Chenchu", ""], ["Xu", "Lei", ""], ["Gao", "Zhifan", ""], ["zhao", "Shen", ""], ["Zhang", "Heye", ""], ["Zhang", "Yanping", ""], ["Du", "Xiuquan", ""], ["Zhao", "Shu", ""], ["Ghista", "Dhanjoo", ""], ["Li", "Shuo", ""]]}, {"id": "1706.03190", "submitter": "Donghao Luo", "authors": "Donghao Luo, Bingbing Ni, Yichao Yan, Xiaokang Yang", "title": "Image Matching via Loopy RNN", "comments": "6 pages, 5 figures, International Joint Conference on Artificial\n  Intelligence(IJCAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing matching algorithms are one-off algorithms, i.e., they usually\nmeasure the distance between the two image feature representation vectors for\nonly one time. In contrast, human's vision system achieves this task, i.e.,\nimage matching, by recursively looking at specific/related parts of both images\nand then making the final judgement. Towards this end, we propose a novel loopy\nrecurrent neural network (Loopy RNN), which is capable of aggregating\nrelationship information of two input images in a progressive/iterative manner\nand outputting the consolidated matching score in the final iteration. A Loopy\nRNN features two uniqueness. First, built on conventional long short-term\nmemory (LSTM) nodes, it links the output gate of the tail node to the input\ngate of the head node, thus it brings up symmetry property required for\nmatching. Second, a monotonous loss designed for the proposed network\nguarantees increasing confidence during the recursive matching process.\nExtensive experiments on several image matching benchmarks demonstrate the\ngreat potential of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 06:48:16 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 12:43:12 GMT"}, {"version": "v3", "created": "Sun, 18 Jun 2017 15:58:23 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Luo", "Donghao", ""], ["Ni", "Bingbing", ""], ["Yan", "Yichao", ""], ["Yang", "Xiaokang", ""]]}, {"id": "1706.03220", "submitter": "Harit Pandya", "authors": "Aseem Saxena, Harit Pandya, Gourav Kumar, Ayush Gaud, K. Madhava\n  Krishna", "title": "Exploring Convolutional Networks for End-to-End Visual Servoing", "comments": "IEEE ICRA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Present image based visual servoing approaches rely on extracting hand\ncrafted visual features from an image. Choosing the right set of features is\nimportant as it directly affects the performance of any approach. Motivated by\nrecent breakthroughs in performance of data driven methods on recognition and\nlocalization tasks, we aim to learn visual feature representations suitable for\nservoing tasks in unstructured and unknown environments. In this paper, we\npresent an end-to-end learning based approach for visual servoing in diverse\nscenes where the knowledge of camera parameters and scene geometry is not\navailable a priori. This is achieved by training a convolutional neural network\nover color images with synchronised camera poses. Through experiments performed\nin simulation and on a quadrotor, we demonstrate the efficacy and robustness of\nour approach for a wide range of camera poses in both indoor as well as outdoor\nenvironments.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 10:57:44 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Saxena", "Aseem", ""], ["Pandya", "Harit", ""], ["Kumar", "Gourav", ""], ["Gaud", "Ayush", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "1706.03227", "submitter": "Zhigang Li", "authors": "Zhigang Li, Yupin Luo", "title": "Generate Identity-Preserving Faces by Generative Adversarial Networks", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating identity-preserving faces aims to generate various face images\nkeeping the same identity given a target face image. Although considerable\ngenerative models have been developed in recent years, it is still challenging\nto simultaneously acquire high quality of facial images and preserve the\nidentity. Here we propose a compelling method using generative adversarial\nnetworks (GAN). Concretely, we leverage the generator of trained GAN to\ngenerate plausible faces and FaceNet as an identity-similarity discriminator to\nensure the identity. Experimental results show that our method is qualified to\ngenerate both plausible and identity-preserving faces with high quality. In\naddition, our method provides a universal framework which can be realized in\nvarious ways by combining different face generators and identity-similarity\ndiscriminator.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 12:37:54 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 14:58:05 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Li", "Zhigang", ""], ["Luo", "Yupin", ""]]}, {"id": "1706.03261", "submitter": "Andr\\'es Almansa", "authors": "Cecilia Aguerrebere, Andr\\'es Almansa, Julie Delon, Yann Gousseau,\n  Pablo Mus\\'e", "title": "A Bayesian Hyperprior Approach for Joint Image Denoising and\n  Interpolation, with an Application to HDR Imaging", "comments": "Some figures are reduced to comply with arxiv's size constraints.\n  Full size images are available as HAL technical report hal-01107519v5, IEEE\n  Transactions on Computational Imaging, 2017", "journal-ref": null, "doi": "10.1109/TCI.2017.2704439", "report-no": null, "categories": "cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, impressive denoising results have been achieved by Bayesian\napproaches which assume Gaussian models for the image patches. This improvement\nin performance can be attributed to the use of per-patch models. Unfortunately\nsuch an approach is particularly unstable for most inverse problems beyond\ndenoising. In this work, we propose the use of a hyperprior to model image\npatches, in order to stabilize the estimation procedure. There are two main\nadvantages to the proposed restoration scheme: Firstly it is adapted to\ndiagonal degradation matrices, and in particular to missing data problems (e.g.\ninpainting of missing pixels or zooming). Secondly it can deal with signal\ndependent noise models, particularly suited to digital cameras. As such, the\nscheme is especially adapted to computational photography. In order to\nillustrate this point, we provide an application to high dynamic range imaging\nfrom a single image taken with a modified sensor, which shows the effectiveness\nof the proposed scheme.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 17:37:01 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Aguerrebere", "Cecilia", ""], ["Almansa", "Andr\u00e9s", ""], ["Delon", "Julie", ""], ["Gousseau", "Yann", ""], ["Mus\u00e9", "Pablo", ""]]}, {"id": "1706.03282", "submitter": "Alexandre de Siqueira", "authors": "Alexandre Fioravante de Siqueira and Wagner Massayuki Nakasuga and\n  Sandro Guedes and Lothar Ratschbacher", "title": "Segmentation of nearly isotropic overlapped tracks in photomicrographs\n  using successive erosions as watershed markers", "comments": "31 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The major challenges of automatic track counting are distinguishing tracks\nand material defects, identifying small tracks and defects of similar size, and\ndetecting overlapping tracks. Here we address the latter issue using WUSEM, an\nalgorithm which combines the watershed transform, morphological erosions and\nlabeling to separate regions in photomicrographs. WUSEM shows reliable results\nwhen used in photomicrographs presenting almost isotropic objects. We tested\nthis method in two datasets of diallyl phthalate (DAP) photomicrographs and\ncompared the results when counting manually and using the classic watershed.\nThe mean automatic/manual efficiency ratio when using WUSEM in the test\ndatasets is 0.97 +/- 0.11.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 21:18:25 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 16:04:00 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["de Siqueira", "Alexandre Fioravante", ""], ["Nakasuga", "Wagner Massayuki", ""], ["Guedes", "Sandro", ""], ["Ratschbacher", "Lothar", ""]]}, {"id": "1706.03285", "submitter": "Caner Sahin", "authors": "Caner Sahin and Tae-Kyun Kim", "title": "Recovering 6D Object Pose: A Review and Multi-modal Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of studies analyse object detection and pose estimation at\nvisual level in 2D, discussing the effects of challenges such as occlusion,\nclutter, texture, etc., on the performances of the methods, which work in the\ncontext of RGB modality. Interpreting the depth data, the study in this paper\npresents thorough multi-modal analyses. It discusses the above-mentioned\nchallenges for full 6D object pose estimation in RGB-D images comparing the\nperformances of several 6D detectors in order to answer the following\nquestions: What is the current position of the computer vision community for\nmaintaining \"automation\" in robotic manipulation? What next steps should the\ncommunity take for improving \"autonomy\" in robotics while handling objects? Our\nfindings include: (i) reasonably accurate results are obtained on\ntextured-objects at varying viewpoints with cluttered backgrounds. (ii) Heavy\nexistence of occlusion and clutter severely affects the detectors, and\nsimilar-looking distractors is the biggest challenge in recovering instances'\n6D. (iii) Template-based methods and random forest-based learning algorithms\nunderlie object detection and 6D pose estimation. Recent paradigm is to learn\ndeep discriminative feature representations and to adopt CNNs taking RGB images\nas input. (iv) Depending on the availability of large-scale 6D annotated depth\ndatasets, feature representations can be learnt on these datasets, and then the\nlearnt representations can be customized for the 6D problem.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 21:37:00 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 19:23:54 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Sahin", "Caner", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1706.03292", "submitter": "Hao Zhang", "authors": "Hao Zhang, Zeyu Zheng, Shizhen Xu, Wei Dai, Qirong Ho, Xiaodan Liang,\n  Zhiting Hu, Jinliang Wei, Pengtao Xie, Eric P. Xing", "title": "Poseidon: An Efficient Communication Architecture for Distributed Deep\n  Learning on GPU Clusters", "comments": "To appear in 2017 USENIX Annual Technical Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models can take weeks to train on a single GPU-equipped\nmachine, necessitating scaling out DL training to a GPU-cluster. However,\ncurrent distributed DL implementations can scale poorly due to substantial\nparameter synchronization over the network, because the high throughput of GPUs\nallows more data batches to be processed per unit time than CPUs, leading to\nmore frequent network synchronization. We present Poseidon, an efficient\ncommunication architecture for distributed DL on GPUs. Poseidon exploits the\nlayered model structures in DL programs to overlap communication and\ncomputation, reducing bursty network communication. Moreover, Poseidon uses a\nhybrid communication scheme that optimizes the number of bytes required to\nsynchronize each layer, according to layer properties and the number of\nmachines. We show that Poseidon is applicable to different DL frameworks by\nplugging Poseidon into Caffe and TensorFlow. We show that Poseidon enables\nCaffe and TensorFlow to achieve 15.5x speed-up on 16 single-GPU machines, even\nwith limited bandwidth (10GbE) and the challenging VGG19-22K network for image\nclassification. Moreover, Poseidon-enabled TensorFlow achieves 31.5x speed-up\nwith 32 single-GPU machines on Inception-V3, a 50% improvement over the\nopen-source TensorFlow (20x speed-up).\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 01:11:06 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Zhang", "Hao", ""], ["Zheng", "Zeyu", ""], ["Xu", "Shizhen", ""], ["Dai", "Wei", ""], ["Ho", "Qirong", ""], ["Liang", "Xiaodan", ""], ["Hu", "Zhiting", ""], ["Wei", "Jinliang", ""], ["Xie", "Pengtao", ""], ["Xing", "Eric P.", ""]]}, {"id": "1706.03309", "submitter": "Yicheng Zhang", "authors": "Yicheng Zhang, Qiang Ling", "title": "Bicycle Detection Based On Multi-feature and Multi-frame Fusion in\n  low-resolution traffic videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a major type of transportation equipments, bicycles, including electrical\nbicycles, are distributed almost everywhere in China. The accidents caused by\nbicycles have become a serious threat to the public safety. So bicycle\ndetection is one major task of traffic video surveillance systems in China. In\nthis paper, a method based on multi-feature and multi-frame fusion is presented\nfor bicycle detection in low-resolution traffic videos. It first extracts some\ngeometric features of objects from each frame image, then concatenate multiple\nfeatures into a feature vector and use linear support vector machine (SVM) to\nlearn a classifier, or put these features into a cascade classifier, to yield a\npreliminary detection result regarding whether an object is a bicycle. It\nfurther fuses these preliminary detection results from multiple frames to\nprovide a more reliable detection decision, together with a confidence level of\nthat decision. Experimental results show that this method based on\nmulti-feature and multi-frame fusion can identify bicycles with high accuracy\nand low computational complexity. It is, therefore, applicable for real-time\ntraffic video surveillance systems.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 04:51:15 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Zhang", "Yicheng", ""], ["Ling", "Qiang", ""]]}, {"id": "1706.03319", "submitter": "LvMin Zhang", "authors": "Lvmin Zhang and Yi Ji and Xin Lin", "title": "Style Transfer for Anime Sketches with Enhanced Residual U-net and\n  Auxiliary Classifier GAN", "comments": "Submitted to ACPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, with the revolutionary neural style transferring methods,\ncreditable paintings can be synthesized automatically from content images and\nstyle images. However, when it comes to the task of applying a painting's style\nto an anime sketch, these methods will just randomly colorize sketch lines as\noutputs and fail in the main task: specific style tranfer. In this paper, we\nintegrated residual U-net to apply the style to the gray-scale sketch with\nauxiliary classifier generative adversarial network (AC-GAN). The whole process\nis automatic and fast, and the results are creditable in the quality of art\nstyle as well as colorization.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 07:56:32 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 02:32:19 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Zhang", "Lvmin", ""], ["Ji", "Yi", ""], ["Lin", "Xin", ""]]}, {"id": "1706.03372", "submitter": "Qiang Zheng", "authors": "Qiang Zheng, Steven Warner, Gregory Tasian, and Yong Fan", "title": "A dynamic graph-cuts method with integrated multiple feature maps for\n  segmenting kidneys in ultrasound images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To improve kidney segmentation in clinical ultrasound (US) images,\nwe develop a new graph cuts based method to segment kidney US images by\nintegrating original image intensity information and texture feature maps\nextracted using Gabor filters. Methods: To handle large appearance variation\nwithin kidney images and improve computational efficiency, we build a graph of\nimage pixels close to kidney boundary instead of building a graph of the whole\nimage. To make the kidney segmentation robust to weak boundaries, we adopt\nlocalized regional information to measure similarity between image pixels for\ncomputing edge weights to build the graph of image pixels. The localized graph\nis dynamically updated and the GC based segmentation iteratively progresses\nuntil convergence. The proposed method has been evaluated and compared with\nstate of the art image segmentation methods based on clinical kidney US images\nof 85 subjects. We randomly selected US images of 20 subjects as training data\nfor tuning the parameters, and validated the methods based on US images of the\nremaining 65 subjects. The segmentation results have been quantitatively\nanalyzed using 3 metrics, including Dice Index, Jaccard Index, and Mean\nDistance. Results: Experiment results demonstrated that the proposed method\nobtained segmentation results for bilateral kidneys of 65 subjects with average\nDice index of 0.9581, Jaccard index of 0.9204, and Mean Distance of 1.7166,\nbetter than other methods under comparison (p<10-19, paired Wilcoxon rank sum\ntests). Conclusions: The proposed method achieved promising performance for\nsegmenting kidneys in US images, better than segmentation methods that built on\nany single channel of image information. This method will facilitate extraction\nof kidney characteristics that may predict important clinical outcomes such\nprogression chronic kidney disease.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 16:17:51 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Zheng", "Qiang", ""], ["Warner", "Steven", ""], ["Tasian", "Gregory", ""], ["Fan", "Yong", ""]]}, {"id": "1706.03424", "submitter": "Weixun Zhou", "authors": "Weixun Zhou, Shawn Newsam, Congmin Li, Zhenfeng Shao", "title": "PatternNet: A Benchmark Dataset for Performance Evaluation of Remote\n  Sensing Image Retrieval", "comments": "49 pages", "journal-ref": null, "doi": "10.1016/j.isprsjprs.2018.01.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing image retrieval(RSIR), which aims to efficiently retrieve data\nof interest from large collections of remote sensing data, is a fundamental\ntask in remote sensing. Over the past several decades, there has been\nsignificant effort to extract powerful feature representations for this task\nsince the retrieval performance depends on the representative strength of the\nfeatures. Benchmark datasets are also critical for developing, evaluating, and\ncomparing RSIR approaches. Current benchmark datasets are deficient in that 1)\nthey were originally collected for land use/land cover classification and not\nimage retrieval, 2) they are relatively small in terms of the number of classes\nas well the number of sample images per class, and 3) the retrieval performance\nhas saturated. These limitations have severely restricted the development of\nnovel feature representations for RSIR, particularly the recent deep-learning\nbased features which require large amounts of training data. We therefore\npresent in this paper, a new large-scale remote sensing dataset termed\n\"PatternNet\" that was collected specifically for RSIR. PatternNet was collected\nfrom high-resolution imagery and contains 38 classes with 800 images per class.\nWe also provide a thorough review of RSIR approaches ranging from traditional\nhandcrafted feature based methods to recent deep learning based ones. We\nevaluate over 35 methods to establish extensive baseline results for future\nRSIR research using the PatternNet benchmark.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 23:45:07 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 04:37:30 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Zhou", "Weixun", ""], ["Newsam", "Shawn", ""], ["Li", "Congmin", ""], ["Shao", "Zhenfeng", ""]]}, {"id": "1706.03431", "submitter": "Ellen Gasparovic", "authors": "James Damon and Ellen Gasparovic", "title": "Modeling Multi-Object Configurations via Medial/Skeletal Linking\n  Structures", "comments": "This paper presents material relevant for two and three dimensional\n  images that builds on and references a previous paper by the authors,\n  arXiv:1402.5517", "journal-ref": "International Journal of Computer Vision, 2017", "doi": "10.1007/s11263-017-1019-5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for modeling a configuration of objects in 2D or 3D\nimages using a mathematical \"skeletal linking structure\" which will\nsimultaneously capture the individual shape features of the objects and their\npositional information relative to one another. The objects may either have\nsmooth boundaries and be disjoint from the others or share common portions of\ntheir boundaries with other objects in a piecewise smooth manner. These\nstructures include a special class of \"Blum medial linking structures,\" which\nare intrinsically associated to the configuration and build upon the Blum\nmedial axes of the individual objects. We give a classification of the\nproperties of Blum linking structures for generic configurations. The skeletal\nlinking structures add increased flexibility for modeling configurations of\nobjects by relaxing the Blum conditions and they extend in a minimal way the\nindividual \"skeletal structures\" which have been previously used for modeling\nindividual objects and capturing their geometric properties. This allows for\nthe mathematical methods introduced for single objects to be significantly\nextended to the entire configuration of objects. These methods not only capture\nthe internal shape structures of the individual objects but also the external\nstructure of the neighboring regions of the objects.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 01:16:36 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Damon", "James", ""], ["Gasparovic", "Ellen", ""]]}, {"id": "1706.03458", "submitter": "Xingjian Shi", "authors": "Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung,\n  Wai-kin Wong, Wang-chun Woo", "title": "Deep Learning for Precipitation Nowcasting: A Benchmark and A New Model", "comments": "NIPS 2017 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the goal of making high-resolution forecasts of regional rainfall,\nprecipitation nowcasting has become an important and fundamental technology\nunderlying various public services ranging from rainstorm warnings to flight\nsafety. Recently, the Convolutional LSTM (ConvLSTM) model has been shown to\noutperform traditional optical flow based methods for precipitation nowcasting,\nsuggesting that deep learning models have a huge potential for solving the\nproblem. However, the convolutional recurrence structure in ConvLSTM-based\nmodels is location-invariant while natural motion and transformation (e.g.,\nrotation) are location-variant in general. Furthermore, since\ndeep-learning-based precipitation nowcasting is a newly emerging area, clear\nevaluation protocols have not yet been established. To address these problems,\nwe propose both a new model and a benchmark for precipitation nowcasting.\nSpecifically, we go beyond ConvLSTM and propose the Trajectory GRU (TrajGRU)\nmodel that can actively learn the location-variant structure for recurrent\nconnections. Besides, we provide a benchmark that includes a real-world\nlarge-scale dataset from the Hong Kong Observatory, a new training loss, and a\ncomprehensive evaluation protocol to facilitate future research and gauge the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 04:02:03 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 06:31:47 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Shi", "Xingjian", ""], ["Gao", "Zhihan", ""], ["Lausen", "Leonard", ""], ["Wang", "Hao", ""], ["Yeung", "Dit-Yan", ""], ["Wong", "Wai-kin", ""], ["Woo", "Wang-chun", ""]]}, {"id": "1706.03466", "submitter": "Siyuan Qiao", "authors": "Siyuan Qiao, Chenxi Liu, Wei Shen and Alan Yuille", "title": "Few-Shot Image Recognition by Predicting Parameters from Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in the few-shot learning problem. In\nparticular, we focus on a challenging scenario where the number of categories\nis large and the number of examples per novel category is very limited, e.g. 1,\n2, or 3. Motivated by the close relationship between the parameters and the\nactivations in a neural network associated with the same category, we propose a\nnovel method that can adapt a pre-trained neural network to novel categories by\ndirectly predicting the parameters from the activations. Zero training is\nrequired in adaptation to novel categories, and fast inference is realized by a\nsingle forward pass. We evaluate our method by doing few-shot image recognition\non the ImageNet dataset, which achieves the state-of-the-art classification\naccuracy on novel categories by a significant margin while keeping comparable\nperformance on the large-scale categories. We also test our method on the\nMiniImageNet dataset and it strongly outperforms the previous state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 04:57:29 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 03:14:05 GMT"}, {"version": "v3", "created": "Sat, 25 Nov 2017 18:50:39 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Qiao", "Siyuan", ""], ["Liu", "Chenxi", ""], ["Shen", "Wei", ""], ["Yuille", "Alan", ""]]}, {"id": "1706.03497", "submitter": "Yuichi Yagi", "authors": "Yuichi Yagi", "title": "A filter based approach for inbetweening", "comments": "10 pages, in Japanese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a filter based approach for inbetweening. We train a convolutional\nneural network to generate intermediate frames. This network aim to generate\nsmooth animation of line drawings. Our method can process scanned images\ndirectly. Our method does not need to compute correspondence of lines and\ntopological changes explicitly. We experiment our method with real animation\nproduction data. The results show that our method can generate intermediate\nframes partially.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 08:04:42 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Yagi", "Yuichi", ""]]}, {"id": "1706.03509", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina and Pim Moeskops and Mitko Veta and Behdad Dasht\n  Bozorg and Josien Pluim", "title": "Exploring the similarity of medical imaging classification problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning is ubiquitous in medical image analysis. In this paper we\nconsider the problem of meta-learning -- predicting which methods will perform\nwell in an unseen classification problem, given previous experience with other\nclassification problems. We investigate the first step of such an approach: how\nto quantify the similarity of different classification problems. We\ncharacterize datasets sampled from six classification problems by performance\nranks of simple classifiers, and define the similarity by the inverse of\nEuclidean distance in this meta-feature space. We visualize the similarities in\na 2D space, where meaningful clusters start to emerge, and show that the\nproposed representation can be used to classify datasets according to their\norigin with 89.3\\% accuracy. These findings, together with the observations of\nrecent trends in machine learning, suggest that meta-learning could be a\nvaluable tool for the medical imaging community.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 08:28:17 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Cheplygina", "Veronika", ""], ["Moeskops", "Pim", ""], ["Veta", "Mitko", ""], ["Bozorg", "Behdad Dasht", ""], ["Pluim", "Josien", ""]]}, {"id": "1706.03581", "submitter": "Artsiom Ablavatski", "authors": "Artsiom Ablavatski, Shijian Lu and Jianfei Cai", "title": "Enriched Deep Recurrent Visual Attention Model for Multiple Object\n  Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/WACV.2017.113", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design an Enriched Deep Recurrent Visual Attention Model (EDRAM) - an\nimproved attention-based architecture for multiple object recognition. The\nproposed model is a fully differentiable unit that can be optimized end-to-end\nby using Stochastic Gradient Descent (SGD). The Spatial Transformer (ST) was\nemployed as visual attention mechanism which allows to learn the geometric\ntransformation of objects within images. With the combination of the Spatial\nTransformer and the powerful recurrent architecture, the proposed EDRAM can\nlocalize and recognize objects simultaneously. EDRAM has been evaluated on two\npublicly available datasets including MNIST Cluttered (with 70K cluttered\ndigits) and SVHN (with up to 250k real world images of house numbers).\nExperiments show that it obtains superior performance as compared with the\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 11:55:35 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Ablavatski", "Artsiom", ""], ["Lu", "Shijian", ""], ["Cai", "Jianfei", ""]]}, {"id": "1706.03646", "submitter": "Xinggang Wang", "authors": "Xinggang Wang, Kaibing Chen, Zilong Huang, Cong Yao and Wenyu Liu", "title": "Point Linking Network for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a core problem in computer vision. With the development\nof deep ConvNets, the performance of object detectors has been dramatically\nimproved. The deep ConvNets based object detectors mainly focus on regressing\nthe coordinates of bounding box, e.g., Faster-R-CNN, YOLO and SSD. Different\nfrom these methods that considering bounding box as a whole, we propose a novel\nobject bounding box representation using points and links and implemented using\ndeep ConvNets, termed as Point Linking Network (PLN). Specifically, we regress\nthe corner/center points of bounding-box and their links using a fully\nconvolutional network; then we map the corner points and their links back to\nmultiple bounding boxes; finally an object detection result is obtained by\nfusing the multiple bounding boxes. PLN is naturally robust to object occlusion\nand flexible to object scale variation and aspect ratio variation. In the\nexperiments, PLN with the Inception-v2 model achieves state-of-the-art\nsingle-model and single-scale results on the PASCAL VOC 2007, the PASCAL VOC\n2012 and the COCO detection benchmarks without bells and whistles. The source\ncode will be released.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 14:02:01 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 05:04:37 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Wang", "Xinggang", ""], ["Chen", "Kaibing", ""], ["Huang", "Zilong", ""], ["Yao", "Cong", ""], ["Liu", "Wenyu", ""]]}, {"id": "1706.03686", "submitter": "Kang Han", "authors": "Kang Han, Wanggen Wan, Haiyan Yao, and Li Hou", "title": "Image Crowd Counting Using Convolutional Neural Network and Markov\n  Random Field", "comments": "6 pages, 6 figures, JACIII Vol.21 No.4", "journal-ref": "JACIII Vol.21 No.4 2017 pp. 632-638", "doi": "10.20965/jaciii.2017.p0632(2017)", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a method called Convolutional Neural Network-Markov\nRandom Field (CNN-MRF) to estimate the crowd count in a still image. We first\ndivide the dense crowd visible image into overlapping patches and then use a\ndeep convolutional neural network to extract features from each patch image,\nfollowed by a fully connected neural network to regress the local patch crowd\ncount. Since the local patches have overlapping portions, the crowd count of\nthe adjacent patches has a high correlation. We use this correlation and the\nMarkov random field to smooth the counting results of the local patches.\nExperiments show that our approach significantly outperforms the\nstate-of-the-art methods on UCF and Shanghaitech crowd counting datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 15:29:48 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 11:47:45 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 02:24:34 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Han", "Kang", ""], ["Wan", "Wanggen", ""], ["Yao", "Haiyan", ""], ["Hou", "Li", ""]]}, {"id": "1706.03699", "submitter": "Bartlomiej Placzek", "authors": "Bartlomiej Placzek, Jolnta Golosz", "title": "The in-town monitoring system for ambulance dispatch centre", "comments": "6 pages, 6 figures", "journal-ref": "Journal of Medical Informatics & Technologies, vol. 5, MI101--106,\n  2003", "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents the vehicles integrated monitoring system giving\npriorities for emergency vehicles. The described system exploits the data\ngathered by: geographical positioning systems and geographical information\nsystems. The digital maps and roadside cameras provide the dispatchers with\naims for in town ambulances traffic management. The method of vehicles\npositioning in the city network and algorithms for ambulances recognition by\nimage processing techniques have been discussed in the paper. These priorities\nare needed for an efficient life-saving actions that require the real-time\ncontrolling strategies.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 22:21:06 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Placzek", "Bartlomiej", ""], ["Golosz", "Jolnta", ""]]}, {"id": "1706.03702", "submitter": "Adam Harrison", "authors": "Adam P. Harrison and Ziyue Xu and Kevin George and Le Lu and Ronald M.\n  Summers and Daniel J. Mollura", "title": "Progressive and Multi-Path Holistically Nested Neural Networks for\n  Pathological Lung Segmentation from CT Images", "comments": "8 Pages, 4 figures, MICCAI 2007", "journal-ref": "Proc. MICCAI (2017); pp 621-629", "doi": "10.1007/978-3-319-66179-7_71", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pathological lung segmentation (PLS) is an important, yet challenging,\nmedical image application due to the wide variability of pathological lung\nappearance and shape. Because PLS is often a pre-requisite for other imaging\nanalytics, methodological simplicity and generality are key factors in\nusability. Along those lines, we present a bottom-up deep-learning based\napproach that is expressive enough to handle variations in appearance, while\nremaining unaffected by any variations in shape. We incorporate the deeply\nsupervised learning framework, but enhance it with a simple, yet effective,\nprogressive multi-path scheme, which more reliably merges outputs from\ndifferent network stages. The result is a deep model able to produce finer\ndetailed masks, which we call progressive holistically-nested networks\n(P-HNNs). Using extensive cross-validation, our method is tested on\nmulti-institutional datasets comprising 929 CT scans (848 publicly available),\nof pathological lungs, reporting mean dice scores of 0.985 and demonstrating\nsignificant qualitative and quantitative improvements over state-of-the art\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 15:54:35 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Harrison", "Adam P.", ""], ["Xu", "Ziyue", ""], ["George", "Kevin", ""], ["Lu", "Le", ""], ["Summers", "Ronald M.", ""], ["Mollura", "Daniel J.", ""]]}, {"id": "1706.03725", "submitter": "Zhiyuan Shi", "authors": "Zhiyuan Shi, Timothy M. Hospedales, Tao Xiang", "title": "Transferring a Semantic Representation for Person Re-Identification and\n  Search", "comments": "cvpr 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning semantic attributes for person re-identification and\ndescription-based person search has gained increasing interest due to\nattributes' great potential as a pose and view-invariant representation.\nHowever, existing attribute-centric approaches have thus far underperformed\nstate-of-the-art conventional approaches. This is due to their non-scalable\nneed for extensive domain (camera) specific annotation. In this paper we\npresent a new semantic attribute learning approach for person re-identification\nand search. Our model is trained on existing fashion photography datasets --\neither weakly or strongly labelled. It can then be transferred and adapted to\nprovide a powerful semantic description of surveillance person detections,\nwithout requiring any surveillance domain supervision. The resulting\nrepresentation is useful for both unsupervised and supervised person\nre-identification, achieving state-of-the-art and near state-of-the-art\nperformance respectively. Furthermore, as a semantic representation it allows\ndescription-based person search to be integrated within the same framework.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 16:52:57 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Shi", "Zhiyuan", ""], ["Hospedales", "Timothy M.", ""], ["Xiang", "Tao", ""]]}, {"id": "1706.03729", "submitter": "Wenling Shang", "authors": "Wenling Shang and Kihyuk Sohn and Yuandong Tian", "title": "Channel-Recurrent Autoencoding for Image Modeling", "comments": "Code: https://github.com/WendyShang/crVAE. Supplementary Materials:\n  http://www-personal.umich.edu/~shangw/wacv18_supplementary_material.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent successes in synthesizing faces and bedrooms, existing\ngenerative models struggle to capture more complex image types, potentially due\nto the oversimplification of their latent space constructions. To tackle this\nissue, building on Variational Autoencoders (VAEs), we integrate recurrent\nconnections across channels to both inference and generation steps, allowing\nthe high-level features to be captured in global-to-local, coarse-to-fine\nmanners. Combined with adversarial loss, our channel-recurrent VAE-GAN\n(crVAE-GAN) outperforms VAE-GAN in generating a diverse spectrum of high\nresolution images while maintaining the same level of computational efficacy.\nOur model produces interpretable and expressive latent representations to\nbenefit downstream tasks such as image completion. Moreover, we propose two\nnovel regularizations, namely the KL objective weighting scheme over time steps\nand mutual information maximization between transformed latent variables and\nthe outputs, to enhance the training.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 17:01:34 GMT"}, {"version": "v2", "created": "Sun, 11 Mar 2018 09:00:51 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Shang", "Wenling", ""], ["Sohn", "Kihyuk", ""], ["Tian", "Yuandong", ""]]}, {"id": "1706.03736", "submitter": "Ignacio Heredia", "authors": "Ignacio Heredia", "title": "Large-Scale Plant Classification with Deep Neural Networks", "comments": "5 pages, 3 figures, 1 table. Published at Proocedings of ACM\n  Computing Frontiers Conference 2017", "journal-ref": "ACM CF'17 Proceedings of the Computing Frontiers Conference\n  (2017), 259-262", "doi": "10.1145/3075564.3075590", "report-no": null, "categories": "cs.LG cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the potential of applying deep learning techniques for\nplant classification and its usage for citizen science in large-scale\nbiodiversity monitoring. We show that plant classification using near\nstate-of-the-art convolutional network architectures like ResNet50 achieves\nsignificant improvements in accuracy compared to the most widespread plant\nclassification application in test sets composed of thousands of different\nspecies labels. We find that the predictions can be confidently used as a\nbaseline classification in citizen science communities like iNaturalist (or its\nSpanish fork, Natusfera) which in turn can share their data with biodiversity\nportals like GBIF.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 17:16:20 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Heredia", "Ignacio", ""]]}, {"id": "1706.03744", "submitter": "Julian Faber", "authors": "J.S. Hammudoglu, J. Sparreboom, J.I. Rauhamaa, J.K. Faber, L.C.\n  Guerchi, I.P. Samiotis, S.P. Rao and J.A. Pouwelse", "title": "Portable Trust: biometric-based authentication and blockchain storage\n  for self-sovereign identity systems", "comments": "Delft University of Technology student project report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devised a mobile biometric-based authentication system only relying on\nlocal processing. Our Android open source solution explores the capability of\ncurrent smartphones to acquire, process and match fingerprints using only its\nbuilt-in hardware. Our architecture is specifically designed to run completely\nlocally and autonomously, not requiring any cloud service, server, or\npermissioned access to fingerprint reader hardware. It involves three main\nstages, starting with the fingerprint acquisition using the smartphone camera,\nfollowed by a processing pipeline to obtain minutiae features and a final step\nfor matching against other locally stored fingerprints, based on Oriented FAST\nand Rotated BRIEF (ORB) descriptors. We obtained a mean matching accuracy of\n55%, with the highest value of 67% for thumb fingers. Our ability to capture\nand process a finger fingerprint in mere seconds using a smartphone makes this\nwork usable in a wide range of scenarios, for instance, offline remote regions.\nThis work is specifically designed to be a key building block for a\nself-sovereign identity solution and integrate with our permissionless\nblockchain for identity and key attestation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 17:33:12 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Hammudoglu", "J. S.", ""], ["Sparreboom", "J.", ""], ["Rauhamaa", "J. I.", ""], ["Faber", "J. K.", ""], ["Guerchi", "L. C.", ""], ["Samiotis", "I. P.", ""], ["Rao", "S. P.", ""], ["Pouwelse", "J. A.", ""]]}, {"id": "1706.03825", "submitter": "Daniel Smilkov", "authors": "Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi\\'egas, Martin\n  Wattenberg", "title": "SmoothGrad: removing noise by adding noise", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining the output of a deep network remains a challenge. In the case of\nan image classifier, one type of explanation is to identify pixels that\nstrongly influence the final decision. A starting point for this strategy is\nthe gradient of the class score function with respect to the input image. This\ngradient can be interpreted as a sensitivity map, and there are several\ntechniques that elaborate on this basic idea. This paper makes two\ncontributions: it introduces SmoothGrad, a simple method that can help visually\nsharpen gradient-based sensitivity maps, and it discusses lessons in the\nvisualization of these maps. We publish the code for our experiments and a\nwebsite with our results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 19:53:30 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Smilkov", "Daniel", ""], ["Thorat", "Nikhil", ""], ["Kim", "Been", ""], ["Vi\u00e9gas", "Fernanda", ""], ["Wattenberg", "Martin", ""]]}, {"id": "1706.03860", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani and George Atia", "title": "Subspace Clustering via Optimal Direction Search", "comments": null, "journal-ref": "IEEE Signal Processing Letters ( Volume: 24, Issue: 12, Dec. 2017\n  )", "doi": "10.1109/LSP.2017.2757901", "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter presents a new spectral-clustering-based approach to the subspace\nclustering problem. Underpinning the proposed method is a convex program for\noptimal direction search, which for each data point d finds an optimal\ndirection in the span of the data that has minimum projection on the other data\npoints and non-vanishing projection on d. The obtained directions are\nsubsequently leveraged to identify a neighborhood set for each data point. An\nalternating direction method of multipliers framework is provided to\nefficiently solve for the optimal directions. The proposed method is shown to\nnotably outperform the existing subspace clustering methods, particularly for\nunwieldy scenarios involving high levels of noise and close subspaces, and\nyields the state-of-the-art results for the problem of face clustering using\nsubspace segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 21:52:57 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 22:56:21 GMT"}, {"version": "v3", "created": "Sun, 23 Jul 2017 20:36:57 GMT"}, {"version": "v4", "created": "Sun, 26 Nov 2017 15:43:15 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1706.03863", "submitter": "Kwang In Kim", "authors": "James Tompkin, Kwang In Kim, Hanspeter Pfister and Christian Theobalt", "title": "Criteria Sliders: Learning Continuous Database Criteria via Interactive\n  Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large databases are often organized by hand-labeled metadata, or criteria,\nwhich are expensive to collect. We can use unsupervised learning to model\ndatabase variation, but these models are often high dimensional, complex to\nparameterize, or require expert knowledge. We learn low-dimensional continuous\ncriteria via interactive ranking, so that the novice user need only describe\nthe relative ordering of examples. This is formed as semi-supervised label\npropagation in which we maximize the information gained from a limited number\nof examples. Further, we actively suggest data points to the user to rank in a\nmore informative way than existing work. Our efficient approach allows users to\ninteractively organize thousands of data points along 1D and 2D continuous\nsliders. We experiment with datasets of imagery and geometry to demonstrate\nthat our tool is useful for quickly assessing and organizing the content of\nlarge databases.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 21:59:26 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Tompkin", "James", ""], ["Kim", "Kwang In", ""], ["Pfister", "Hanspeter", ""], ["Theobalt", "Christian", ""]]}, {"id": "1706.03867", "submitter": "Mahmoud Afifi", "authors": "Islam A.T.F. Taj-Eddin, Mahmoud Afifi, Mostafa Korashy, Ali H. Ahmed,\n  Ng Yoke Cheng, Evelyng Hernandez and Salma M. Abdel-latif", "title": "Can We See Photosynthesis? Magnifying the Tiny Color Changes of Plant\n  Green Leaves Using Eulerian Video Magnification", "comments": "7 pages, 3 figures", "journal-ref": "J. Electron. Imaging, 2017", "doi": "10.1117/1.JEI.26.6.060501", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant aliveness is proven through laboratory experiments and special\nscientific instruments. In this paper, we aim to detect the degree of animation\nof plants based on the magnification of the small color changes in the plant's\ngreen leaves using the Eulerian video magnification. Capturing the video under\na controlled environment, e.g., using a tripod and direct current (DC) light\nsources, reduces camera movements and minimizes light fluctuations; we aim to\nreduce the external factors as much as possible. The acquired video is then\nstabilized and a proposed algorithm used to reduce the illumination variations.\nLastly, the Euler magnification is utilized to magnify the color changes on the\nlight invariant video. The proposed system does not require any special purpose\ninstruments as it uses a digital camera with a regular frame rate. The results\nof magnified color changes on both natural and plastic leaves show that the\nlive green leaves have color changes in contrast to the plastic leaves. Hence,\nwe can argue that the color changes of the leaves are due to biological\noperations, such as photosynthesis. To date, this is possibly the first work\nthat focuses on interpreting visually, some biological operations of plants\nwithout any special purpose instruments.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 23:04:33 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 17:21:55 GMT"}, {"version": "v3", "created": "Tue, 29 Aug 2017 16:50:37 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Taj-Eddin", "Islam A. T. F.", ""], ["Afifi", "Mahmoud", ""], ["Korashy", "Mostafa", ""], ["Ahmed", "Ali H.", ""], ["Cheng", "Ng Yoke", ""], ["Hernandez", "Evelyng", ""], ["Abdel-latif", "Salma M.", ""]]}, {"id": "1706.03875", "submitter": "Siwei Lyu", "authors": "Longyin Wen, Honggang Qi, Siwei Lyu", "title": "Contrast Enhancement Estimation for Digital Image Forensics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inconsistency in contrast enhancement can be used to expose image forgeries.\nIn this work, we describe a new method to estimate contrast enhancement from a\nsingle image. Our method takes advantage of the nature of contrast enhancement\nas a mapping between pixel values, and the distinct characteristics it\nintroduces to the image pixel histogram. Our method recovers the original pixel\nhistogram and the contrast enhancement simultaneously from a single image with\nan iterative algorithm. Unlike previous methods, our method is robust in the\npresence of additive noise perturbations that are used to hide the traces of\ncontrast enhancement. Furthermore, we also develop an e effective method to to\ndetect image regions undergone contrast enhancement transformations that are\ndifferent from the rest of the image, and use this method to detect composite\nimages. We perform extensive experimental evaluations to demonstrate the\nefficacy and efficiency of our method method.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 00:13:53 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Wen", "Longyin", ""], ["Qi", "Honggang", ""], ["Lyu", "Siwei", ""]]}, {"id": "1706.03907", "submitter": "Brendan Ruff", "authors": "Brendan Ruff", "title": "Deep Control - a simple automatic gain control for memory efficient and\n  high performance training of deep convolutional neural networks", "comments": "Submitted to BMVC 2017 on 2nd May 2017", "journal-ref": null, "doi": null, "report-no": "Subject of patent application GB1619779.0 23-Nov-2016", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a deep convolutional neural net typically starts with a random\ninitialisation of all filters in all layers which severely reduces the forward\nsignal and back-propagated error and leads to slow and sub-optimal training.\nTechniques that counter that focus on either increasing the signal or\nincreasing the gradients adaptively but the model behaves very differently at\nthe beginning of training compared to later when stable pathways through the\nnet have been established. To compound this problem the effective minibatch\nsize varies greatly between layers at different depths and between individual\nfilters as activation sparsity typically increases with depth leading to a\nreduction in effective learning rate since gradients may superpose rather than\nadd and this further compounds the covariate shift problem as deeper neurons\nare less able to adapt to upstream shift.\n  Proposed here is a method of automatic gain control of the signal built into\neach convolutional neuron that achieves equivalent or superior performance than\nbatch normalisation and is compatible with single sample or minibatch gradient\ndescent. The same model is used both for training and inference.\n  The technique comprises a scaled per sample map mean subtraction from the raw\nconvolutional filter output followed by scaling of the difference.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 05:29:05 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Ruff", "Brendan", ""]]}, {"id": "1706.03912", "submitter": "Zhe Li", "authors": "Zhe Li, Xiaoyu Wang, Xutao Lv, Tianbao Yang", "title": "SEP-Nets: Small and Effective Pattern Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While going deeper has been witnessed to improve the performance of\nconvolutional neural networks (CNN), going smaller for CNN has received\nincreasing attention recently due to its attractiveness for mobile/embedded\napplications. It remains an active and important topic how to design a small\nnetwork while retaining the performance of large and deep CNNs (e.g., Inception\nNets, ResNets). Albeit there are already intensive studies on compressing the\nsize of CNNs, the considerable drop of performance is still a key concern in\nmany designs. This paper addresses this concern with several new contributions.\nFirst, we propose a simple yet powerful method for compressing the size of deep\nCNNs based on parameter binarization. The striking difference from most\nprevious work on parameter binarization/quantization lies at different\ntreatments of $1\\times 1$ convolutions and $k\\times k$ convolutions ($k>1$),\nwhere we only binarize $k\\times k$ convolutions into binary patterns. The\nresulting networks are referred to as pattern networks. By doing this, we show\nthat previous deep CNNs such as GoogLeNet and Inception-type Nets can be\ncompressed dramatically with marginal drop in performance. Second, in light of\nthe different functionalities of $1\\times 1$ (data projection/transformation)\nand $k\\times k$ convolutions (pattern extraction), we propose a new block\nstructure codenamed the pattern residual block that adds transformed feature\nmaps generated by $1\\times 1$ convolutions to the pattern feature maps\ngenerated by $k\\times k$ convolutions, based on which we design a small network\nwith $\\sim 1$ million parameters. Combining with our parameter binarization, we\nachieve better performance on ImageNet than using similar sized networks\nincluding recently released Google MobileNets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 06:07:26 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Li", "Zhe", ""], ["Wang", "Xiaoyu", ""], ["Lv", "Xutao", ""], ["Yang", "Tianbao", ""]]}, {"id": "1706.03947", "submitter": "Jinzhuo Wang", "authors": "Xiongtao Chen, Wenmin Wang, Jinzhuo Wang, Weimian Li, Baoyang Chen", "title": "Long-Term Video Interpolation with Bidirectional Predictive Network", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the challenging task of long-term video interpolation.\nUnlike most existing methods that only generate few intermediate frames between\nexisting adjacent ones, we attempt to speculate or imagine the procedure of an\nepisode and further generate multiple frames between two non-consecutive frames\nin videos. In this paper, we present a novel deep architecture called\nbidirectional predictive network (BiPN) that predicts intermediate frames from\ntwo opposite directions. The bidirectional architecture allows the model to\nlearn scene transformation with time as well as generate longer video\nsequences. Besides, our model can be extended to predict multiple possible\nprocedures by sampling different noise vectors. A joint loss composed of clues\nin image and feature spaces and adversarial loss is designed to train our\nmodel. We demonstrate the advantages of BiPN on two benchmarks Moving 2D Shapes\nand UCF101 and report competitive results to recent approaches.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 08:15:32 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Chen", "Xiongtao", ""], ["Wang", "Wenmin", ""], ["Wang", "Jinzhuo", ""], ["Li", "Weimian", ""], ["Chen", "Baoyang", ""]]}, {"id": "1706.04008", "submitter": "Patrick Putzky", "authors": "Patrick Putzky, Max Welling", "title": "Recurrent Inference Machines for Solving Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the recent research on solving iterative inference problems focuses\non moving away from hand-chosen inference algorithms and towards learned\ninference. In the latter, the inference process is unrolled in time and\ninterpreted as a recurrent neural network (RNN) which allows for joint learning\nof model and inference parameters with back-propagation through time. In this\nframework, the RNN architecture is directly derived from a hand-chosen\ninference algorithm, effectively limiting its capabilities. We propose a\nlearning framework, called Recurrent Inference Machines (RIM), in which we turn\nalgorithm construction the other way round: Given data and a task, train an RNN\nto learn an inference algorithm. Because RNNs are Turing complete [1, 2] they\nare capable to implement any inference algorithm. The framework allows for an\nabstraction which removes the need for domain knowledge. We demonstrate in\nseveral image restoration experiments that this abstraction is effective,\nallowing us to achieve state-of-the-art performance on image denoising and\nsuper-resolution tasks and superior across-task generalization.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 11:24:41 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Putzky", "Patrick", ""], ["Welling", "Max", ""]]}, {"id": "1706.04034", "submitter": "Pedro F. Proen\\c{c}a", "authors": "Pedro F. Proenca and Yang Gao", "title": "Probabilistic RGB-D Odometry based on Points, Lines and Planes Under\n  Depth Uncertainty", "comments": "Major update: more results, depth filter released as opensource, 34\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a robust visual odometry method for structured\nenvironments that combines point features with line and plane segments,\nextracted through an RGB-D camera. Noisy depth maps are processed by a\nprobabilistic depth fusion framework based on Mixtures of Gaussians to denoise\nand derive the depth uncertainty, which is then propagated throughout the\nvisual odometry pipeline. Probabilistic 3D plane and line fitting solutions are\nused to model the uncertainties of the feature parameters and pose is estimated\nby combining the three types of primitives based on their uncertainties.\nPerformance evaluation on RGB-D sequences collected in this work and two public\nRGB-D datasets: TUM and ICL-NUIM show the benefit of using the proposed depth\nfusion framework and combining the three feature-types, particularly in scenes\nwith low-textured surfaces, dynamic objects and missing depth measurements.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 13:03:05 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 11:23:03 GMT"}, {"version": "v3", "created": "Thu, 18 Jan 2018 17:07:59 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Proenca", "Pedro F.", ""], ["Gao", "Yang", ""]]}, {"id": "1706.04041", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Yao Wang", "title": "Text Extraction From Texture Images Using Masked Signal Decomposition", "comments": "arXiv admin note: text overlap with arXiv:1704.07711", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text extraction is an important problem in image processing with applications\nfrom optical character recognition to autonomous driving. Most of the\ntraditional text segmentation algorithms consider separating text from a simple\nbackground (which usually has a different color from texts). In this work we\nconsider separating texts from a textured background, that has similar color to\ntexts. We look at this problem from a signal decomposition perspective, and\nconsider a more realistic scenario where signal components are overlaid on top\nof each other (instead of adding together). When the signals are overlaid, to\nseparate signal components, we need to find a binary mask which shows the\nsupport of each component. Because directly solving the binary mask is\nintractable, we relax this problem to the approximated continuous problem, and\nsolve it by alternating optimization method. We show that the proposed\nalgorithm achieves significantly better results than other recent works on\nseveral challenging images.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 20:52:00 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 12:27:37 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 02:07:35 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Minaee", "Shervin", ""], ["Wang", "Yao", ""]]}, {"id": "1706.04048", "submitter": "Chong Chen", "authors": "Chong Chen and Ozan \\\"Oktem", "title": "Indirect Image Registration with Large Diffeomorphic Deformations", "comments": "43 pages, 4 figures, 1 table; revised", "journal-ref": "SIAM Journal on Imaging Sciences 2018", "doi": "10.1137/17M1134627", "report-no": null, "categories": "math.NA cs.CV math.DS math.FA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper adapts the large deformation diffeomorphic metric mapping framework\nfor image registration to the indirect setting where a template is registered\nagainst a target that is given through indirect noisy observations. The\nregistration uses diffeomorphisms that transform the template through a (group)\naction. These diffeomorphisms are generated by solving a flow equation that is\ndefined by a velocity field with certain regularity. The theoretical analysis\nincludes a proof that indirect image registration has solutions (existence)\nthat are stable and that converge as the data error tends so zero, so it\nbecomes a well-defined regularization method. The paper concludes with examples\nof indirect image registration in 2D tomography with very sparse and/or highly\nnoisy data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 13:24:13 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 08:35:27 GMT"}, {"version": "v3", "created": "Wed, 11 Oct 2017 07:48:53 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Chen", "Chong", ""], ["\u00d6ktem", "Ozan", ""]]}, {"id": "1706.04062", "submitter": "Liang Yanchao", "authors": "Yanchao Liang, Jianhua Li", "title": "Deep Learning-Based Food Calorie Estimation Method in Dietary Assessment", "comments": "12 pages. arXiv admin note: substantial text overlap with\n  arXiv:1705.07632", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obesity treatment requires obese patients to record all food intakes per day.\nComputer vision has been introduced to estimate calories from food images. In\norder to increase accuracy of detection and reduce the error of volume\nestimation in food calorie estimation, we present our calorie estimation method\nin this paper. To estimate calorie of food, a top view and side view is needed.\nFaster R-CNN is used to detect the food and calibration object. GrabCut\nalgorithm is used to get each food's contour. Then the volume is estimated with\nthe food and corresponding object. Finally we estimate each food's calorie. And\nthe experiment results show our estimation method is effective.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 09:42:28 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 01:28:15 GMT"}, {"version": "v3", "created": "Tue, 2 Jan 2018 10:47:35 GMT"}, {"version": "v4", "created": "Sun, 18 Feb 2018 08:13:04 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Liang", "Yanchao", ""], ["Li", "Jianhua", ""]]}, {"id": "1706.04122", "submitter": "Iman Abbasnejad", "authors": "Iman Abbasnejad, Sridha Sridharan, Simon Denman, Clinton Fookes, Simon\n  Lucey", "title": "Joint Max Margin and Semantic Features for Continuous Event Detection in\n  Complex Scenes", "comments": "submit to journal of Computer Vision and Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the problem of complex event detection in the continuous domain\n(i.e. events with unknown starting and ending locations) is addressed. Existing\nevent detection methods are limited to features that are extracted from the\nlocal spatial or spatio-temporal patches from the videos. However, this makes\nthe model vulnerable to the events with similar concepts e.g. \"Open drawer\" and\n\"Open cupboard\". In this work, in order to address the aforementioned\nlimitations we present a novel model based on the combination of semantic and\ntemporal features extracted from video frames. We train a max-margin classifier\non top of the extracted features in an adaptive framework that is able to\ndetect the events with unknown starting and ending locations. Our model is\nbased on the Bidirectional Region Neural Network and large margin Structural\nOutput SVM. The generality of our model allows it to be simply applied to\ndifferent labeled and unlabeled datasets. We finally test our algorithm on\nthree challenging datasets, \"UCF 101-Action Recognition\", \"MPII Cooking\nActivities\" and \"Hollywood\", and we report state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 15:30:16 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Abbasnejad", "Iman", ""], ["Sridharan", "Sridha", ""], ["Denman", "Simon", ""], ["Fookes", "Clinton", ""], ["Lucey", "Simon", ""]]}, {"id": "1706.04124", "submitter": "Jinzhuo Wang", "authors": "Baoyang Chen, Wenmin Wang, Jinzhuo Wang, Xiongtao Chen", "title": "Video Imagination from a Single Image with Transformation Generation", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on a challenging task: synthesizing multiple imaginary\nvideos given a single image. Major problems come from high dimensionality of\npixel space and the ambiguity of potential motions. To overcome those problems,\nwe propose a new framework that produce imaginary videos by transformation\ngeneration. The generated transformations are applied to the original image in\na novel volumetric merge network to reconstruct frames in imaginary video.\nThrough sampling different latent variables, our method can output different\nimaginary video samples. The framework is trained in an adversarial way with\nunsupervised learning. For evaluation, we propose a new assessment metric\n$RIQA$. In experiments, we test on 3 datasets varying from synthetic data to\nnatural scene. Our framework achieves promising performance in image quality\nassessment. The visual inspection indicates that it can successfully generate\ndiverse five-frame videos in acceptable perceptual quality.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 15:31:10 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 07:51:22 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Chen", "Baoyang", ""], ["Wang", "Wenmin", ""], ["Wang", "Jinzhuo", ""], ["Chen", "Xiongtao", ""]]}, {"id": "1706.04215", "submitter": "Ashwinkumar Ganesan", "authors": "Mandar Haldekar, Ashwinkumar Ganesan, Tim Oates", "title": "Identifying Spatial Relations in Images using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches to building a large scale knowledge graph have usually\nrelied on extracting information (entities, their properties, and relations\nbetween them) from unstructured text (e.g. Dbpedia). Recent advances in\nConvolutional Neural Networks (CNN) allow us to shift our focus to learning\nentities and relations from images, as they build robust models that require\nlittle or no pre-processing of the images. In this paper, we present an\napproach to identify and extract spatial relations (e.g., The girl is standing\nbehind the table) from images using CNNs. Our research addresses two specific\nchallenges: providing insight into how spatial relations are learned by the\nnetwork and which parts of the image are used to predict these relations. We\nuse the pre-trained network VGGNet to extract features from an image and train\na Multi-layer Perceptron (MLP) on a set of synthetic images and the sun09\ndataset to extract spatial relations. The MLP predicts spatial relations\nwithout a bounding box around the objects or the space in the image depicting\nthe relation. To understand how the spatial relations are represented in the\nnetwork, a heatmap is overlayed on the image to show the regions that are\ndeemed important by the network. Also, we analyze the MLP to show the\nrelationship between the activation of consistent groups of nodes and the\nprediction of a spatial relation. We show how the loss of these groups affects\nthe networks ability to identify relations.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 18:24:11 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Haldekar", "Mandar", ""], ["Ganesan", "Ashwinkumar", ""], ["Oates", "Tim", ""]]}, {"id": "1706.04254", "submitter": "Roger Gomez Nieto", "authors": "Roger Gomez Nieto, Andres Marino Alvarez Meza, Julian David Echeverry\n  Correa, Alvaro Angel Orozco Gutierrez", "title": "Automatic Localization of Deep Stimulation Electrodes Using\n  Trajectory-based Segmentation Approach", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parkinson's disease (PD) is a degenerative condition of the nervous system,\nwhich manifests itself primarily as muscle stiffness, hypokinesia,\nbradykinesia, and tremor. In patients suffering from advanced stages of PD,\nDeep Brain Stimulation neurosurgery (DBS) is the best alternative to medical\ntreatment, especially when they become tolerant to the drugs. This surgery\nproduces a neuronal activity, a result from electrical stimulation, whose\nquantification is known as Volume of Tissue Activated (VTA). To locate\ncorrectly the VTA in the cerebral volume space, one should be aware exactly the\nlocation of the tip of the DBS electrodes, as well as their spatial projection.\n  In this paper, we automatically locate DBS electrodes using a threshold-based\nmedical imaging segmentation methodology, determining the optimal value of this\nthreshold adaptively. The proposed methodology allows the localization of DBS\nelectrodes in Computed Tomography (CT) images, with high noise tolerance, using\nautomatic threshold detection methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 21:06:35 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Nieto", "Roger Gomez", ""], ["Meza", "Andres Marino Alvarez", ""], ["Correa", "Julian David Echeverry", ""], ["Gutierrez", "Alvaro Angel Orozco", ""]]}, {"id": "1706.04256", "submitter": "Ulugbek Kamilov", "authors": "Kevin Degraux, Ulugbek S. Kamilov, Petros T. Boufounos, Dehong Liu", "title": "Online Convolutional Dictionary Learning for Multimodal Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational imaging methods that can exploit multiple modalities have the\npotential to enhance the capabilities of traditional sensing systems. In this\npaper, we propose a new method that reconstructs multimodal images from their\nlinear measurements by exploiting redundancies across different modalities. Our\nmethod combines a convolutional group-sparse representation of images with\ntotal variation (TV) regularization for high-quality multimodal imaging. We\ndevelop an online algorithm that enables the unsupervised learning of\nconvolutional dictionaries on large-scale datasets that are typical in such\napplications. We illustrate the benefit of our approach in the context of joint\nintensity-depth imaging.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 21:08:33 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Degraux", "Kevin", ""], ["Kamilov", "Ulugbek S.", ""], ["Boufounos", "Petros T.", ""], ["Liu", "Dehong", ""]]}, {"id": "1706.04261", "submitter": "Raghav Goyal", "authors": "Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna\n  Materzy\\'nska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend,\n  Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo\n  Bax, Roland Memisevic", "title": "The \"something something\" video database for learning and evaluating\n  visual common sense", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks trained on datasets such as ImageNet have led to major\nadvances in visual object classification. One obstacle that prevents networks\nfrom reasoning more deeply about complex scenes and situations, and from\nintegrating visual knowledge with natural language, like humans do, is their\nlack of common sense knowledge about the physical world. Videos, unlike still\nimages, contain a wealth of detailed information about the physical world.\nHowever, most labelled video datasets represent high-level concepts rather than\ndetailed physical aspects about actions and scenes. In this work, we describe\nour ongoing collection of the \"something-something\" database of video\nprediction tasks whose solutions require a common sense understanding of the\ndepicted situation. The database currently contains more than 100,000 videos\nacross 174 classes, which are defined as caption-templates. We also describe\nthe challenges in crowd-sourcing this data at scale.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 21:26:19 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 21:15:13 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Goyal", "Raghav", ""], ["Kahou", "Samira Ebrahimi", ""], ["Michalski", "Vincent", ""], ["Materzy\u0144ska", "Joanna", ""], ["Westphal", "Susanne", ""], ["Kim", "Heuna", ""], ["Haenel", "Valentin", ""], ["Fruend", "Ingo", ""], ["Yianilos", "Peter", ""], ["Mueller-Freitag", "Moritz", ""], ["Hoppe", "Florian", ""], ["Thurau", "Christian", ""], ["Bax", "Ingo", ""], ["Memisevic", "Roland", ""]]}, {"id": "1706.04264", "submitter": "Md Abul Hasnat", "authors": "Md. Abul Hasnat and Julien Bohn\\'e and Jonathan Milgram and St\\'ephane\n  Gentric and Liming Chen", "title": "von Mises-Fisher Mixture Model-based Deep learning: Application to Face\n  Verification", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of pattern recognition tasks, \\textit{e.g.}, face verification, can\nbe boiled down to classification or clustering of unit length directional\nfeature vectors whose distance can be simply computed by their angle. In this\npaper, we propose the von Mises-Fisher (vMF) mixture model as the theoretical\nfoundation for an effective deep-learning of such directional features and\nderive a novel vMF Mixture Loss and its corresponding vMF deep features. The\nproposed vMF feature learning achieves the characteristics of discriminative\nlearning, \\textit{i.e.}, compacting the instances of the same class while\nincreasing the distance of instances from different classes. Moreover, it\nsubsumes a number of popular loss functions as well as an effective method in\ndeep learning, namely normalization. We conduct extensive experiments on face\nverification using 4 different challenging face datasets, \\textit{i.e.}, LFW,\nYouTube faces, CACD and IJB-A. Results show the effectiveness and excellent\ngeneralization ability of the proposed approach as it achieves state-of-the-art\nresults on the LFW, YouTube faces and CACD datasets and competitive results on\nthe IJB-A dataset.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 21:43:05 GMT"}, {"version": "v2", "created": "Sun, 31 Dec 2017 22:24:58 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Hasnat", "Md. Abul", ""], ["Bohn\u00e9", "Julien", ""], ["Milgram", "Jonathan", ""], ["Gentric", "St\u00e9phane", ""], ["Chen", "Liming", ""]]}, {"id": "1706.04269", "submitter": "Humam Alwassel", "authors": "Humam Alwassel, Fabian Caba Heilbron, Bernard Ghanem", "title": "Action Search: Spotting Actions in Videos and Its Application to\n  Temporal Action Localization", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art temporal action detectors inefficiently search the entire\nvideo for specific actions. Despite the encouraging progress these methods\nachieve, it is crucial to design automated approaches that only explore parts\nof the video which are the most relevant to the actions being searched for. To\naddress this need, we propose the new problem of action spotting in video,\nwhich we define as finding a specific action in a video while observing a small\nportion of that video. Inspired by the observation that humans are extremely\nefficient and accurate in spotting and finding action instances in video, we\npropose Action Search, a novel Recurrent Neural Network approach that mimics\nthe way humans spot actions. Moreover, to address the absence of data recording\nthe behavior of human annotators, we put forward the Human Searches dataset,\nwhich compiles the search sequences employed by human annotators spotting\nactions in the AVA and THUMOS14 datasets. We consider temporal action\nlocalization as an application of the action spotting problem. Experiments on\nthe THUMOS14 dataset reveal that our model is not only able to explore the\nvideo efficiently (observing on average 17.3% of the video) but it also\naccurately finds human activities with 30.8% mAP.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 22:15:09 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 16:27:25 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Alwassel", "Humam", ""], ["Heilbron", "Fabian Caba", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1706.04277", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi, Abdelrahman Abdelhamed", "title": "AFIF4: Deep Gender Classification based on AdaBoost-based Fusion of\n  Isolated Facial Features and Foggy Faces", "comments": "26 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gender classification aims at recognizing a person's gender. Despite the high\naccuracy achieved by state-of-the-art methods for this task, there is still\nroom for improvement in generalized and unrestricted datasets. In this paper,\nwe advocate a new strategy inspired by the behavior of humans in gender\nrecognition. Instead of dealing with the face image as a sole feature, we rely\non the combination of isolated facial features and a holistic feature which we\ncall the foggy face. Then, we use these features to train deep convolutional\nneural networks followed by an AdaBoost-based score fusion to infer the final\ngender class. We evaluate our method on four challenging datasets to\ndemonstrate its efficacy in achieving better or on-par accuracy with\nstate-of-the-art methods. In addition, we present a new face dataset that\nintensifies the challenges of occluded faces and illumination changes, which we\nbelieve to be a much-needed resource for gender classification research.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 23:15:14 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 00:48:27 GMT"}, {"version": "v3", "created": "Sun, 10 Sep 2017 02:54:38 GMT"}, {"version": "v4", "created": "Sat, 30 Sep 2017 01:00:35 GMT"}, {"version": "v5", "created": "Sat, 18 Nov 2017 02:26:50 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Afifi", "Mahmoud", ""], ["Abdelhamed", "Abdelrahman", ""]]}, {"id": "1706.04284", "submitter": "Ding Liu", "authors": "Ding Liu, Bihan Wen, Xianming Liu, Zhangyang Wang, Thomas S. Huang", "title": "When Image Denoising Meets High-Level Vision Tasks: A Deep Learning\n  Approach", "comments": "the 27th International Joint Conference on Artificial Intelligence\n  (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventionally, image denoising and high-level vision tasks are handled\nseparately in computer vision. In this paper, we cope with the two jointly and\nexplore the mutual influence between them. First we propose a convolutional\nneural network for image denoising which achieves the state-of-the-art\nperformance. Second we propose a deep neural network solution that cascades two\nmodules for image denoising and various high-level tasks, respectively, and use\nthe joint loss for updating only the denoising network via back-propagation. We\ndemonstrate that on one hand, the proposed denoiser has the generality to\novercome the performance degradation of different high-level vision tasks. On\nthe other hand, with the guidance of high-level vision information, the\ndenoising network can generate more visually appealing results. To the best of\nour knowledge, this is the first work investigating the benefit of exploiting\nimage semantics simultaneously for image denoising and high-level vision tasks\nvia deep learning. The code is available online\nhttps://github.com/Ding-Liu/DeepDenoising.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 00:04:56 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 03:30:51 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 19:33:33 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Liu", "Ding", ""], ["Wen", "Bihan", ""], ["Liu", "Xianming", ""], ["Wang", "Zhangyang", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1706.04285", "submitter": "Chenxing Xia", "authors": "Chenxing Xia and Hanling Zhang and Xiuju Gao", "title": "Saliency detection by aggregating complementary background template with\n  optimization framework", "comments": "28 pages,10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an unsupervised bottom-up saliency detection approach by\naggregating complementary background template with refinement. Feature vectors\nare extracted from each superpixel to cover regional color, contrast and\ntexture information. By using these features, a coarse detection for salient\nregion is realized based on background template achieved by different\ncombinations of boundary regions instead of only treating four boundaries as\nbackground. Then, by ranking the relevance of the image nodes with foreground\ncues extracted from the former saliency map, we obtain an improved result.\nFinally, smoothing operation is utilized to refine the foreground-based\nsaliency map to improve the contrast between salient and non-salient regions\nuntil a close to binary saliency map is reached. Experimental results show that\nthe proposed algorithm generates more accurate saliency maps and performs\nfavorably against the state-off-the-art saliency detection methods on four\npublicly available datasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 00:06:02 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Xia", "Chenxing", ""], ["Zhang", "Hanling", ""], ["Gao", "Xiuju", ""]]}, {"id": "1706.04303", "submitter": "Jia Ding", "authors": "Jia Ding, Aoxue Li, Zhiqiang Hu and Liwei Wang", "title": "Accurate Pulmonary Nodule Detection in Computed Tomography Images Using\n  Deep Convolutional Neural Networks", "comments": "MICCAI 2017 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of pulmonary cancer is the most promising way to enhance a\npatient's chance for survival. Accurate pulmonary nodule detection in computed\ntomography (CT) images is a crucial step in diagnosing pulmonary cancer. In\nthis paper, inspired by the successful use of deep convolutional neural\nnetworks (DCNNs) in natural image recognition, we propose a novel pulmonary\nnodule detection approach based on DCNNs. We first introduce a deconvolutional\nstructure to Faster Region-based Convolutional Neural Network (Faster R-CNN)\nfor candidate detection on axial slices. Then, a three-dimensional DCNN is\npresented for the subsequent false positive reduction. Experimental results of\nthe LUng Nodule Analysis 2016 (LUNA16) Challenge demonstrate the superior\ndetection performance of the proposed approach on nodule detection(average\nFROC-score of 0.891, ranking the 1st place over all submitted results).\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 03:31:04 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 05:47:37 GMT"}, {"version": "v3", "created": "Tue, 29 Aug 2017 00:26:48 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Ding", "Jia", ""], ["Li", "Aoxue", ""], ["Hu", "Zhiqiang", ""], ["Wang", "Liwei", ""]]}, {"id": "1706.04306", "submitter": "Parneet Kaur", "authors": "Parneet Kaur, Hang Zhang, Kristin J. Dana", "title": "Photo-realistic Facial Texture Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer methods have achieved significant success in recent years with\nthe use of convolutional neural networks. However, many of these methods\nconcentrate on artistic style transfer with few constraints on the output image\nappearance. We address the challenging problem of transferring face texture\nfrom a style face image to a content face image in a photorealistic manner\nwithout changing the identity of the original content image. Our framework for\nface texture transfer (FaceTex) augments the prior work of MRF-CNN with a novel\nfacial semantic regularization that incorporates a face prior regularization\nsmoothly suppressing the changes around facial meso-structures (e.g eyes, nose\nand mouth) and a facial structure loss function which implicitly preserves the\nfacial structure so that face texture can be transferred without changing the\noriginal identity. We demonstrate results on face images and compare our\napproach with recent state-of-the-art methods. Our results demonstrate superior\ntexture transfer because of the ability to maintain the identity of the\noriginal face image.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 03:45:50 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Kaur", "Parneet", ""], ["Zhang", "Hang", ""], ["Dana", "Kristin J.", ""]]}, {"id": "1706.04313", "submitter": "Huayan Wang", "authors": "Austin Stone, Huayan Wang, Michael Stark, Yi Liu, D. Scott Phoenix,\n  Dileep George", "title": "Teaching Compositionality to CNNs", "comments": "Preprint appearing in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have shown great success in computer\nvision, approaching human-level performance when trained for specific tasks via\napplication-specific loss functions. In this paper, we propose a method for\naugmenting and training CNNs so that their learned features are compositional.\nIt encourages networks to form representations that disentangle objects from\ntheir surroundings and from each other, thereby promoting better\ngeneralization. Our method is agnostic to the specific details of the\nunderlying CNN to which it is applied and can in principle be used with any\nCNN. As we show in our experiments, the learned representations lead to feature\nactivations that are more localized and improve performance over\nnon-compositional baselines in object recognition tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 04:34:59 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Stone", "Austin", ""], ["Wang", "Huayan", ""], ["Stark", "Michael", ""], ["Liu", "Yi", ""], ["Phoenix", "D. Scott", ""], ["George", "Dileep", ""]]}, {"id": "1706.04318", "submitter": "Tetsu Matsukawa", "authors": "Tetsu Matsukawa, Takahiro Okabe, Einoshin Suzuki, Yoichi Sato", "title": "Hierarchical Gaussian Descriptors with Application to Person\n  Re-Identification", "comments": "14 pages, 12 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Describing the color and textural information of a person image is one of the\nmost crucial aspects of person re-identification (re-id). In this paper, we\npresent novel meta-descriptors based on a hierarchical distribution of pixel\nfeatures. Although hierarchical covariance descriptors have been successfully\napplied to image classification, the mean information of pixel features, which\nis absent from the covariance, tends to be the major discriminative information\nfor person re-id. To solve this problem, we describe a local region in an image\nvia hierarchical Gaussian distribution in which both means and covariances are\nincluded in their parameters. More specifically, the region is modeled as a set\nof multiple Gaussian distributions in which each Gaussian represents the\nappearance of a local patch. The characteristics of the set of Gaussians are\nagain described by another Gaussian distribution. In both steps, we embed the\nparameters of the Gaussian into a point of Symmetric Positive Definite (SPD)\nmatrix manifold. By changing the way to handle mean information in this\nembedding, we develop two hierarchical Gaussian descriptors. Additionally, we\ndevelop feature norm normalization methods with the ability to alleviate the\nbiased trends that exist on the descriptors. The experimental results conducted\non five public datasets indicate that the proposed descriptors achieve\nremarkably high performance on person re-id.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 05:16:16 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Matsukawa", "Tetsu", ""], ["Okabe", "Takahiro", ""], ["Suzuki", "Einoshin", ""], ["Sato", "Yoichi", ""]]}, {"id": "1706.04372", "submitter": "Zhe Wang", "authors": "Zhe Wang, Yanxin Yin, Jianping Shi, Wei Fang, Hongsheng Li and\n  Xiaogang Wang", "title": "Zoom-in-Net: Deep Mining Lesions for Diabetic Retinopathy Detection", "comments": "accepted by MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a convolution neural network based algorithm for simultaneously\ndiagnosing diabetic retinopathy and highlighting suspicious regions. Our\ncontributions are two folds: 1) a network termed Zoom-in-Net which mimics the\nzoom-in process of a clinician to examine the retinal images. Trained with only\nimage-level supervisions, Zoomin-Net can generate attention maps which\nhighlight suspicious regions, and predicts the disease level accurately based\non both the whole image and its high resolution suspicious patches. 2) Only\nfour bounding boxes generated from the automatically learned attention maps are\nenough to cover 80% of the lesions labeled by an experienced ophthalmologist,\nwhich shows good localization ability of the attention maps. By clustering\nfeatures at high response locations on the attention maps, we discover\nmeaningful clusters which contain potential lesions in diabetic retinopathy.\nExperiments show that our algorithm outperform the state-of-the-art methods on\ntwo datasets, EyePACS and Messidor.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 09:13:52 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Wang", "Zhe", ""], ["Yin", "Yanxin", ""], ["Shi", "Jianping", ""], ["Fang", "Wei", ""], ["Li", "Hongsheng", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1706.04382", "submitter": "Shirui Li", "authors": "Hanlin Mo, Shirui Li, You Hao, Hua Li", "title": "Shape-Color Differential Moment Invariants under Affine Transformations", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the general construction formula of shape-color primitives by\nusing partial differentials of each color channel in this paper. By using all\nkinds of shape-color primitives, shape-color differential moment invariants can\nbe constructed very easily, which are invariant to the shape affine and color\naffine transforms. 50 instances of SCDMIs are obtained finally. In experiments,\nseveral commonly used color descriptors and SCDMIs are used in image\nclassification and retrieval of color images, respectively. By comparing the\nexperimental results, we find that SCDMIs get better results.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 09:42:16 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Mo", "Hanlin", ""], ["Li", "Shirui", ""], ["Hao", "You", ""], ["Li", "Hua", ""]]}, {"id": "1706.04388", "submitter": "Alexander Sagel", "authors": "Alexander Sagel and Martin Kleinsteuber", "title": "Alignment Distances on Systems of Bags", "comments": null, "journal-ref": null, "doi": "10.1109/TCSVT.2017.2715851", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in image and video recognition indicates that many visual\nprocesses can be thought of as being generated by a time-varying generative\nmodel. A nearby descriptive model for visual processes is thus a statistical\ndistribution that varies over time. Specifically, modeling visual processes as\nstreams of histograms generated by a kernelized linear dynamic system turns out\nto be efficient. We refer to such a model as a System of Bags. In this work, we\ninvestigate Systems of Bags with special emphasis on dynamic scenes and dynamic\ntextures. Parameters of linear dynamic systems suffer from ambiguities. In\norder to cope with these ambiguities in the kernelized setting, we develop a\nkernelized version of the alignment distance. For its computation, we use a\nJacobi-type method and prove its convergence to a set of critical points. We\nemploy it as a dissimilarity measure on Systems of Bags. As such, it\noutperforms other known dissimilarity measures for kernelized linear dynamic\nsystems, in particular the Martin Distance and the Maximum Singular Value\nDistance, in every tested classification setting. A considerable margin can be\nobserved in settings, where classification is performed with respect to an\nabstract mean of video sets. For this scenario, the presented approach can\noutperform state-of-the-art techniques, such as Dynamic Fractal Spectrum or\nOrthogonal Tensor Dictionary Learning.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 09:58:32 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Sagel", "Alexander", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1706.04397", "submitter": "Hinrich B Winther", "authors": "Hinrich B Winther, Christian Hundt, Bertil Schmidt, Christoph Czerner,\n  Johann Bauersachs, Frank Wacker, Jens Vogel-Claussen", "title": "$\\nu$-net: Deep Learning for Generalized Biventricular Cardiac Mass and\n  Function Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Cardiac MRI derived biventricular mass and function parameters,\nsuch as end-systolic volume (ESV), end-diastolic volume (EDV), ejection\nfraction (EF), stroke volume (SV), and ventricular mass (VM) are clinically\nwell established. Image segmentation can be challenging and time-consuming, due\nto the complex anatomy of the human heart.\n  Objectives: This study introduces $\\nu$-net (/nju:n$\\varepsilon$t/) -- a deep\nlearning approach allowing for fully-automated high quality segmentation of\nright (RV) and left ventricular (LV) endocardium and epicardium for extraction\nof cardiac function parameters.\n  Methods: A set consisting of 253 manually segmented cases has been used to\ntrain a deep neural network. Subsequently, the network has been evaluated on 4\ndifferent multicenter data sets with a total of over 1000 cases.\n  Results: For LV EF the intraclass correlation coefficient (ICC) is 98, 95,\nand 80 % (95 %), and for RV EF 96, and 87 % (80 %) on the respective data sets\n(human expert ICCs reported in parenthesis). The LV VM ICC is 95, and 94 % (84\n%), and the RV VM ICC is 83, and 83 % (54 %). This study proposes a simple\nadjustment procedure, allowing for the adaptation to distinct segmentation\nphilosophies. $\\nu$-net exhibits state of-the-art performance in terms of dice\ncoefficient.\n  Conclusions: Biventricular mass and function parameters can be determined\nreliably in high quality by applying a deep neural network for cardiac MRI\nsegmentation, especially in the anatomically complex right ventricle. Adaption\nto individual segmentation styles by applying a simple adjustment procedure is\nviable, allowing for the processing of novel data without time-consuming\nadditional training.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 10:36:30 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Winther", "Hinrich B", ""], ["Hundt", "Christian", ""], ["Schmidt", "Bertil", ""], ["Czerner", "Christoph", ""], ["Bauersachs", "Johann", ""], ["Wacker", "Frank", ""], ["Vogel-Claussen", "Jens", ""]]}, {"id": "1706.04399", "submitter": "Manh Duong Phung", "authors": "Manh Duong Phung, Cong Hoang Quach, Tran Hiep Dinh, Quang Ha", "title": "Enhanced discrete particle swarm optimization path planning for UAV\n  vision-based surface inspection", "comments": null, "journal-ref": "Automation in Construction, Vol.81, pp.25-33 (2017)", "doi": "10.1016/j.autcon.2017.04.013", "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In built infrastructure monitoring, an efficient path planning algorithm is\nessential for robotic inspection of large surfaces using computer vision. In\nthis work, we first formulate the inspection path planning problem as an\nextended travelling salesman problem (TSP) in which both the coverage and\nobstacle avoidance were taken into account. An enhanced discrete particle swarm\noptimization (DPSO) algorithm is then proposed to solve the TSP, with\nperformance improvement by using deterministic initialization, random mutation,\nand edge exchange. Finally, we take advantage of parallel computing to\nimplement the DPSO in a GPU-based framework so that the computation time can be\nsignificantly reduced while keeping the hardware requirement unchanged. To show\nthe effectiveness of the proposed algorithm, experimental results are included\nfor datasets obtained from UAV inspection of an office building and a bridge.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 10:40:19 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Phung", "Manh Duong", ""], ["Quach", "Cong Hoang", ""], ["Dinh", "Tran Hiep", ""], ["Ha", "Quang", ""]]}, {"id": "1706.04472", "submitter": "Prerana Mukherjee", "authors": "Prerana Mukherjee, Brejesh Lall, Sarvaswa Tandon", "title": "SalProp: Salient object proposals via aggregated edge cues", "comments": "5 pages, 4 figures, accepted at ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel object proposal generation scheme by\nformulating a graph-based salient edge classification framework that utilizes\nthe edge context. In the proposed method, we construct a Bayesian probabilistic\nedge map to assign a saliency value to the edgelets by exploiting low level\nedge features. A Conditional Random Field is then learned to effectively\ncombine these features for edge classification with object/non-object label. We\npropose an objectness score for the generated windows by analyzing the salient\nedge density inside the bounding box. Extensive experiments on PASCAL VOC 2007\ndataset demonstrate that the proposed method gives competitive performance\nagainst 10 popular generic object detection techniques while using fewer number\nof proposals.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 13:17:42 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Mukherjee", "Prerana", ""], ["Lall", "Brejesh", ""], ["Tandon", "Sarvaswa", ""]]}, {"id": "1706.04488", "submitter": "Manuk Akopyan", "authors": "Manuk Akopyan (1), and Eshsou Khashba (1) ((1) Institute for System\n  Programming)", "title": "Large-Scale YouTube-8M Video Understanding with Deep Neural Networks", "comments": "6 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video classification problem has been studied many years. The success of\nConvolutional Neural Networks (CNN) in image recognition tasks gives a powerful\nincentive for researchers to create more advanced video classification\napproaches. As video has a temporal content Long Short Term Memory (LSTM)\nnetworks become handy tool allowing to model long-term temporal clues. Both\napproaches need a large dataset of input data. In this paper three models\nprovided to address video classification using recently announced YouTube-8M\nlarge-scale dataset. The first model is based on frame pooling approach. Two\nother models based on LSTM networks. Mixture of Experts intermediate layer is\nused in third model allowing to increase model capacity without dramatically\nincreasing computations. The set of experiments for handling imbalanced\ntraining data has been conducted.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 13:38:43 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Akopyan", "Manuk", ""], ["Khashba", "Eshsou", ""]]}, {"id": "1706.04496", "submitter": "Haibin Huang", "authors": "Haibin Huang, Evangelos Kalogerakis, Siddhartha Chaudhuri, Duygu\n  Ceylan, Vladimir G. Kim, Ersin Yumer", "title": "Learning Local Shape Descriptors from Part Correspondences With\n  Multi-view Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new local descriptor for 3D shapes, directly applicable to a\nwide range of shape analysis problems such as point correspondences, semantic\nsegmentation, affordance prediction, and shape-to-scan matching. The descriptor\nis produced by a convolutional network that is trained to embed geometrically\nand semantically similar points close to one another in descriptor space. The\nnetwork processes surface neighborhoods around points on a shape that are\ncaptured at multiple scales by a succession of progressively zoomed out views,\ntaken from carefully selected camera positions. We leverage two extremely large\nsources of data to train our network. First, since our network processes\nrendered views in the form of 2D images, we repurpose architectures pre-trained\non massive image datasets. Second, we automatically generate a synthetic dense\npoint correspondence dataset by non-rigid alignment of corresponding shape\nparts in a large collection of segmented 3D models. As a result of these design\nchoices, our network effectively encodes multi-scale local context and\nfine-grained surface detail. Our network can be trained to produce either\ncategory-specific descriptors or more generic descriptors by learning from\nmultiple shape categories. Once trained, at test time, the network extracts\nlocal descriptors for shapes without requiring any part segmentation as input.\nOur method can produce effective local descriptors even for shapes whose\ncategory is unknown or different from the ones used while training. We\ndemonstrate through several experiments that our learned local descriptors are\nmore discriminative compared to state of the art alternatives, and are\neffective in a variety of shape analysis applications.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 13:56:07 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 03:35:20 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Huang", "Haibin", ""], ["Kalogerakis", "Evangelos", ""], ["Chaudhuri", "Siddhartha", ""], ["Ceylan", "Duygu", ""], ["Kim", "Vladimir G.", ""], ["Yumer", "Ersin", ""]]}, {"id": "1706.04508", "submitter": "Zuxuan Wu", "authors": "Yu-Gang Jiang, Zuxuan Wu, Jinhui Tang, Zechao Li, Xiangyang Xue,\n  Shih-Fu Chang", "title": "Modeling Multimodal Clues in a Hybrid Deep Learning Framework for Video\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos are inherently multimodal. This paper studies the problem of how to\nfully exploit the abundant multimodal clues for improved video categorization.\nWe introduce a hybrid deep learning framework that integrates useful clues from\nmultiple modalities, including static spatial appearance information, motion\npatterns within a short time window, audio information as well as long-range\ntemporal dynamics. More specifically, we utilize three Convolutional Neural\nNetworks (CNNs) operating on appearance, motion and audio signals to extract\ntheir corresponding features. We then employ a feature fusion network to derive\na unified representation with an aim to capture the relationships among\nfeatures. Furthermore, to exploit the long-range temporal dynamics in videos,\nwe apply two Long Short Term Memory networks with extracted appearance and\nmotion features as inputs. Finally, we also propose to refine the prediction\nscores by leveraging contextual relationships among video semantics. The hybrid\ndeep learning framework is able to exploit a comprehensive set of multimodal\nfeatures for video classification. Through an extensive set of experiments, we\ndemonstrate that (1) LSTM networks which model sequences in an explicitly\nrecurrent manner are highly complementary with CNN models; (2) the feature\nfusion network which produces a fused representation through modeling feature\nrelationships outperforms alternative fusion strategies; (3) the semantic\ncontext of video classes can help further refine the predictions for improved\nperformance. Experimental results on two challenging benchmarks, the UCF-101\nand the Columbia Consumer Videos (CCV), provide strong quantitative evidence\nthat our framework achieves promising results: $93.1\\%$ on the UCF-101 and\n$84.5\\%$ on the CCV, outperforming competing methods with clear margins.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 14:23:08 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Jiang", "Yu-Gang", ""], ["Wu", "Zuxuan", ""], ["Tang", "Jinhui", ""], ["Li", "Zechao", ""], ["Xue", "Xiangyang", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1706.04568", "submitter": "Lex Fridman", "authors": "Lex Fridman, Benedikt Jenik, Shaiyan Keshvari, Bryan Reimer, Christoph\n  Zetzsche, Ruth Rosenholtz", "title": "SideEye: A Generative Neural Network Based Simulator of Human Peripheral\n  Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foveal vision makes up less than 1% of the visual field. The other 99% is\nperipheral vision. Precisely what human beings see in the periphery is both\nobvious and mysterious in that we see it with our own eyes but can't visualize\nwhat we see, except in controlled lab experiments. Degradation of information\nin the periphery is far more complex than what might be mimicked with a radial\nblur. Rather, behaviorally-validated models hypothesize that peripheral vision\nmeasures a large number of local texture statistics in pooling regions that\noverlap and grow with eccentricity. In this work, we develop a new method for\nperipheral vision simulation by training a generative neural network on a\nbehaviorally-validated full-field synthesis model. By achieving a 21,000 fold\nreduction in running time, our approach is the first to combine realism and\nspeed of peripheral vision simulation to a degree that provides a whole new way\nto approach visual design: through peripheral visualization.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 16:18:20 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 03:40:03 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Fridman", "Lex", ""], ["Jenik", "Benedikt", ""], ["Keshvari", "Shaiyan", ""], ["Reimer", "Bryan", ""], ["Zetzsche", "Christoph", ""], ["Rosenholtz", "Ruth", ""]]}, {"id": "1706.04572", "submitter": "Miha Skalic", "authors": "Miha Skalic, Marcin Pekalski, Xingguo E. Pan", "title": "Deep Learning Methods for Efficient Large Scale Video Labeling", "comments": "7 pages, 5 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a solution to \"Google Cloud and YouTube-8M Video Understanding\nChallenge\" that ranked 5th place. The proposed model is an ensemble of three\nmodel families, two frame level and one video level. The training was performed\non augmented dataset, with cross validation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 16:24:18 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Skalic", "Miha", ""], ["Pekalski", "Marcin", ""], ["Pan", "Xingguo E.", ""]]}, {"id": "1706.04589", "submitter": "Christian Rupprecht", "authors": "Christian Rupprecht, Ansh Kapil, Nan Liu, Lamberto Ballan, Federico\n  Tombari", "title": "Learning without Prejudice: Avoiding Bias in Webly-Supervised Action\n  Recognition", "comments": "Submitted to CVIU SI: Computer Vision and the Web", "journal-ref": null, "doi": "10.1016/j.cviu.2017.08.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Webly-supervised learning has recently emerged as an alternative paradigm to\ntraditional supervised learning based on large-scale datasets with manual\nannotations. The key idea is that models such as CNNs can be learned from the\nnoisy visual data available on the web. In this work we aim to exploit web data\nfor video understanding tasks such as action recognition and detection. One of\nthe main problems in webly-supervised learning is cleaning the noisy labeled\ndata from the web. The state-of-the-art paradigm relies on training a first\nclassifier on noisy data that is then used to clean the remaining dataset. Our\nkey insight is that this procedure biases the second classifier towards samples\nthat the first one understands. Here we train two independent CNNs, a RGB\nnetwork on web images and video frames and a second network using temporal\ninformation from optical flow. We show that training the networks independently\nis vastly superior to selecting the frames for the flow classifier by using our\nRGB network. Moreover, we show benefits in enriching the training set with\ndifferent data sources from heterogeneous public web databases. We demonstrate\nthat our framework outperforms all other webly-supervised methods on two public\nbenchmarks, UCF-101 and Thumos'14.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 17:10:59 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Rupprecht", "Christian", ""], ["Kapil", "Ansh", ""], ["Liu", "Nan", ""], ["Ballan", "Lamberto", ""], ["Tombari", "Federico", ""]]}, {"id": "1706.04671", "submitter": "Madhuri Suthar", "authors": "Madhuri Suthar, Mohammad Asghari and Bahram Jalali", "title": "Feature Enhancement in Visually Impaired Images", "comments": "Submitted to PLOS One Journal on November 3, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major open problems in computer vision is detection of features in\nvisually impaired images. In this paper, we describe a potential solution using\nPhase Stretch Transform, a new computational approach for image analysis, edge\ndetection and resolution enhancement that is inspired by the physics of the\nphotonic time stretch technique. We mathematically derive the intrinsic\nnonlinear transfer function and demonstrate how it leads to (1) superior\nperformance at low contrast levels and (2) a reconfigurable operator for\nhyper-dimensional classification. We prove that the Phase Stretch Transform\nequalizes the input image brightness across the range of intensities resulting\nin a high dynamic range in visually impaired images. We also show further\nimprovement in the dynamic range by combining our method with the conventional\ntechniques. Finally, our results show a method for computation of mathematical\nderivatives via group delay dispersion operations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 21:24:26 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Suthar", "Madhuri", ""], ["Asghari", "Mohammad", ""], ["Jalali", "Bahram", ""]]}, {"id": "1706.04695", "submitter": "Ricardo Borsoi", "authors": "Ricardo Augusto Borsoi, Guilherme Holsbach Costa, Jos\\'e Carlos\n  Moreira Bermudez", "title": "A New Adaptive Video Super-Resolution Algorithm With Improved Robustness\n  to Innovations", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2866181", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new video super-resolution reconstruction (SRR) method with\nimproved robustness to outliers is proposed. Although the R-LMS is one of the\nSRR algorithms with the best reconstruction quality for its computational cost,\nand is naturally robust to registration inaccuracies, its performance is known\nto degrade severely in the presence of innovation outliers. By studying the\nproximal point cost function representation of the R-LMS iterative equation, a\nbetter understanding of its performance under different situations is attained.\nUsing statistical properties of typical innovation outliers, a new cost\nfunction is then proposed and two new algorithms are derived, which present\nimproved robustness to outliers while maintaining computational costs\ncomparable to that of R-LMS. Monte Carlo simulation results illustrate that the\nproposed method outperforms the traditional and regularized versions of LMS,\nand is competitive with state-of-the-art SRR methods at a much smaller\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 23:32:55 GMT"}, {"version": "v2", "created": "Sun, 31 Dec 2017 01:00:31 GMT"}, {"version": "v3", "created": "Fri, 17 Aug 2018 19:54:58 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Borsoi", "Ricardo Augusto", ""], ["Costa", "Guilherme Holsbach", ""], ["Bermudez", "Jos\u00e9 Carlos Moreira", ""]]}, {"id": "1706.04717", "submitter": "Zhihe Lu", "authors": "Zhihe Lu, Zhihang Li, Jie Cao, Ran He, and Zhenan Sun", "title": "Recent Progress of Face Image Synthesis", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face synthesis has been a fascinating yet challenging problem in computer\nvision and machine learning. Its main research effort is to design algorithms\nto generate photo-realistic face images via given semantic domain. It has been\na crucial prepossessing step of main-stream face recognition approaches and an\nexcellent test of AI ability to use complicated probability distributions. In\nthis paper, we provide a comprehensive review of typical face synthesis works\nthat involve traditional methods as well as advanced deep learning approaches.\nParticularly, Generative Adversarial Net (GAN) is highlighted to generate\nphoto-realistic and identity preserving results. Furthermore, the public\navailable databases and evaluation metrics are introduced in details. We end\nthe review with discussing unsolved difficulties and promising directions for\nfuture research.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 01:55:17 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Lu", "Zhihe", ""], ["Li", "Zhihang", ""], ["Cao", "Jie", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""]]}, {"id": "1706.04719", "submitter": "Yiqing Guo", "authors": "Yiqing Guo, Xiuping Jia, and David Paull", "title": "Effective Sequential Classifier Training for SVM-based Multitemporal\n  Remote Sensing Image Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2808767", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive availability of remote sensing images has challenged supervised\nclassification algorithms such as Support Vector Machines (SVM), as training\nsamples tend to be highly limited due to the expensive and laborious task of\nground truthing. The temporal correlation and spectral similarity between\nmultitemporal images have opened up an opportunity to alleviate this problem.\nIn this study, a SVM-based Sequential Classifier Training (SCT-SVM) approach is\nproposed for multitemporal remote sensing image classification. The approach\nleverages the classifiers of previous images to reduce the required number of\ntraining samples for the classifier training of an incoming image. For each\nincoming image, a rough classifier is firstly predicted based on the temporal\ntrend of a set of previous classifiers. The predicted classifier is then\nfine-tuned into a more accurate position with current training samples. This\napproach can be applied progressively to sequential image data, with only a\nsmall number of training samples being required from each image. Experiments\nwere conducted with Sentinel-2A multitemporal data over an agricultural area in\nAustralia. Results showed that the proposed SCT-SVM achieved better\nclassification accuracies compared with two state-of-the-art model transfer\nalgorithms. When training data are insufficient, the overall classification\naccuracy of the incoming image was improved from 76.18% to 94.02% with the\nproposed SCT-SVM, compared with those obtained without the assistance from\nprevious images. These results demonstrate that the leverage of a priori\ninformation from previous images can provide advantageous assistance for later\nimages in multitemporal image classification.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 02:01:44 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 02:24:47 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Guo", "Yiqing", ""], ["Jia", "Xiuping", ""], ["Paull", "David", ""]]}, {"id": "1706.04737", "submitter": "Lin Yang", "authors": "Lin Yang, Yizhe Zhang, Jianxu Chen, Siyuan Zhang, Danny Z. Chen", "title": "Suggestive Annotation: A Deep Active Learning Framework for Biomedical\n  Image Segmentation", "comments": "Accepted at MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image segmentation is a fundamental problem in biomedical image analysis.\nRecent advances in deep learning have achieved promising results on many\nbiomedical image segmentation benchmarks. However, due to large variations in\nbiomedical images (different modalities, image settings, objects, noise, etc),\nto utilize deep learning on a new application, it usually needs a new set of\ntraining data. This can incur a great deal of annotation effort and cost,\nbecause only biomedical experts can annotate effectively, and often there are\ntoo many instances in images (e.g., cells) to annotate. In this paper, we aim\nto address the following question: With limited effort (e.g., time) for\nannotation, what instances should be annotated in order to attain the best\nperformance? We present a deep active learning framework that combines fully\nconvolutional network (FCN) and active learning to significantly reduce\nannotation effort by making judicious suggestions on the most effective\nannotation areas. We utilize uncertainty and similarity information provided by\nFCN and formulate a generalized version of the maximum set cover problem to\ndetermine the most representative and uncertain areas for annotation. Extensive\nexperiments using the 2015 MICCAI Gland Challenge dataset and a lymph node\nultrasound image segmentation dataset show that, using annotation suggestions\nby our method, state-of-the-art segmentation performance can be achieved by\nusing only 50% of training data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 05:01:53 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Yang", "Lin", ""], ["Zhang", "Yizhe", ""], ["Chen", "Jianxu", ""], ["Zhang", "Siyuan", ""], ["Chen", "Danny Z.", ""]]}, {"id": "1706.04758", "submitter": "Gyeongsik Moon", "authors": "Gyeongsik Moon, Ju Yong Chang, Yumin Suh, Kyoung Mu Lee", "title": "Holistic Planimetric prediction to Local Volumetric prediction for 3D\n  Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to 3D human pose estimation from a single depth\nmap. Recently, convolutional neural network (CNN) has become a powerful\nparadigm in computer vision. Many of computer vision tasks have benefited from\nCNNs, however, the conventional approach to directly regress 3D body joint\nlocations from an image does not yield a noticeably improved performance. In\ncontrast, we formulate the problem as estimating per-voxel likelihood of key\nbody joints from a 3D occupancy grid. We argue that learning a mapping from\nvolumetric input to volumetric output with 3D convolution consistently improves\nthe accuracy when compared to learning a regression from depth map to 3D joint\ncoordinates. We propose a two-stage approach to reduce the computational\noverhead caused by volumetric representation and 3D convolution: Holistic 2D\nprediction and Local 3D prediction. In the first stage, Planimetric Network\n(P-Net) estimates per-pixel likelihood for each body joint in the holistic 2D\nspace. In the second stage, Volumetric Network (V-Net) estimates the per-voxel\nlikelihood of each body joints in the local 3D space around the 2D estimations\nof the first stage, effectively reducing the computational cost. Our model\noutperforms existing methods by a large margin in publicly available datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 07:25:34 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 17:10:18 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Moon", "Gyeongsik", ""], ["Chang", "Ju Yong", ""], ["Suh", "Yumin", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1706.04870", "submitter": "Ashraf Darwish", "authors": "Ayat Taha, Ashraf Darwish, and Aboul Ella Hassanien", "title": "Arabian Horse Identification Benchmark Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The lack of a standard muzzle print database is a challenge for conducting\nresearches in Arabian horse identification systems. Therefore, collecting a\nmuzzle print images database is a crucial decision. The dataset presented in\nthis paper is an option for the studies that need a dataset for testing and\ncomparing the algorithms under development for Arabian horse identification.\nOur collected dataset consists of 300 color images that were collected from 50\nArabian horse muzzle species. This dataset has been collected from 50 Arabian\nhorses with 6 muzzle print images each. A special care has been given to the\nquality of the collected images. The collected images cover different quality\nlevels and degradation factors such as image rotation and image partiality for\nsimulating real time identification operations. This dataset can be used to\ntest the identification of Arabian horse system including the extracted\nfeatures and the selected classifier.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 13:58:02 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Taha", "Ayat", ""], ["Darwish", "Ashraf", ""], ["Hassanien", "Aboul Ella", ""]]}, {"id": "1706.04954", "submitter": "Yawen Huang", "authors": "Yawen Huang, Ling Shao, Alejandro F. Frangi", "title": "DOTE: Dual cOnvolutional filTer lEarning for Super-Resolution and\n  Cross-Modality Synthesis in MRI", "comments": "8 pages, 5 figures To appear in MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal image synthesis is a topical problem in medical image computing.\nExisting methods for image synthesis are either tailored to a specific\napplication, require large scale training sets, or are based on partitioning\nimages into overlapping patches. In this paper, we propose a novel Dual\ncOnvolutional filTer lEarning (DOTE) approach to overcome the drawbacks of\nthese approaches. We construct a closed loop joint filter learning strategy\nthat generates informative feedback for model self-optimization. Our method can\nleverage data more efficiently thus reducing the size of the required training\nset. We extensively evaluate DOTE in two challenging tasks: image\nsuper-resolution and cross-modality synthesis. The experimental results\ndemonstrate superior performance of our method over other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 16:27:23 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Huang", "Yawen", ""], ["Shao", "Ling", ""], ["Frangi", "Alejandro F.", ""]]}, {"id": "1706.04957", "submitter": "Matthias Joachim Ehrhardt", "authors": "Antonin Chambolle, Matthias J. Ehrhardt, Peter Richt\\'arik,\n  Carola-Bibiane Sch\\\"onlieb", "title": "Stochastic Primal-Dual Hybrid Gradient Algorithm with Arbitrary Sampling\n  and Imaging Applications", "comments": "25 pages, 8 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a stochastic extension of the primal-dual hybrid gradient\nalgorithm studied by Chambolle and Pock in 2011 to solve saddle point problems\nthat are separable in the dual variable. The analysis is carried out for\ngeneral convex-concave saddle point problems and problems that are either\npartially smooth / strongly convex or fully smooth / strongly convex. We\nperform the analysis for arbitrary samplings of dual variables, and obtain\nknown deterministic results as a special case. Several variants of our\nstochastic method significantly outperform the deterministic variant on a\nvariety of imaging tasks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 16:43:16 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 12:16:17 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Chambolle", "Antonin", ""], ["Ehrhardt", "Matthias J.", ""], ["Richt\u00e1rik", "Peter", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1706.04970", "submitter": "Min Xu", "authors": "Xiangrui Zeng, Miguel Ricardo Leung, Tzviya Zeev-Ben-Mordehai, Min Xu", "title": "A convolutional autoencoder approach for mining features in cellular\n  electron cryo-tomograms and weakly supervised coarse segmentation", "comments": "Accepted by Journal of Structural Biology", "journal-ref": null, "doi": "10.1016/j.jsb.2017.12.015", "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular electron cryo-tomography enables the 3D visualization of cellular\norganization in the near-native state and at submolecular resolution. However,\nthe contents of cellular tomograms are often complex, making it difficult to\nautomatically isolate different in situ cellular components. In this paper, we\npropose a convolutional autoencoder-based unsupervised approach to provide a\ncoarse grouping of 3D small subvolumes extracted from tomograms. We demonstrate\nthat the autoencoder can be used for efficient and coarse characterization of\nfeatures of macromolecular complexes and surfaces, such as membranes. In\naddition, the autoencoder can be used to detect non-cellular features related\nto sample preparation and data collection, such as carbon edges from the grid\nand tomogram boundaries. The autoencoder is also able to detect patterns that\nmay indicate spatial interactions between cellular components. Furthermore, we\ndemonstrate that our autoencoder can be used for weakly supervised semantic\nsegmentation of cellular components, requiring a very small amount of manual\nannotation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 17:13:37 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 18:32:31 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Zeng", "Xiangrui", ""], ["Leung", "Miguel Ricardo", ""], ["Zeev-Ben-Mordehai", "Tzviya", ""], ["Xu", "Min", ""]]}, {"id": "1706.05028", "submitter": "Nelson Nauata Junior", "authors": "Nelson Nauata, Jonathan Smith, Greg Mori", "title": "Hierarchical Label Inference for Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos are a rich source of high-dimensional structured data, with a wide\nrange of interacting components at varying levels of granularity. In order to\nimprove understanding of unconstrained internet videos, it is important to\nconsider the role of labels at separate levels of abstraction. In this paper,\nwe consider the use of the Bidirectional Inference Neural Network (BINN) for\nperforming graph-based inference in label space for the task of video\nclassification. We take advantage of the inherent hierarchy between labels at\nincreasing granularity. The BINN is evaluated on the first and second release\nof the YouTube-8M large scale multilabel video dataset. Our results demonstrate\nthe effectiveness of BINN, achieving significant improvements against baseline\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 18:25:24 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 23:53:47 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Nauata", "Nelson", ""], ["Smith", "Jonathan", ""], ["Mori", "Greg", ""]]}, {"id": "1706.05029", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "M\\'onica Benito, Eduardo Garc\\'ia-Portugu\\'es, J. S. Marron, Daniel\n  Pe\\~na", "title": "Distance weighted discrimination of face images for gender\n  classification", "comments": "9 pages, 4 figures, 1 table", "journal-ref": "Stat, 6:231-240, 2017", "doi": "10.1002/sta4.151", "report-no": null, "categories": "stat.AP cs.CV stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We illustrate the advantages of distance weighted discrimination for\nclassification and feature extraction in a High Dimension Low Sample Size\n(HDLSS) situation. The HDLSS context is a gender classification problem of face\nimages in which the dimension of the data is several orders of magnitude larger\nthan the sample size. We compare distance weighted discrimination with Fisher's\nlinear discriminant, support vector machines, and principal component analysis\nby exploring their classification interpretation through insightful\nvisuanimations and by examining the classifiers' discriminant errors. This\nanalysis enables us to make new contributions to the understanding of the\ndrivers of human discrimination between males and females.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 18:25:50 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 10:14:26 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Benito", "M\u00f3nica", ""], ["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Marron", "J. S.", ""], ["Pe\u00f1a", "Daniel", ""]]}, {"id": "1706.05048", "submitter": "Ali Borji", "authors": "Ali Borji and Aysegul Dundar", "title": "Human-like Clustering with Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification and clustering have been studied separately in machine\nlearning and computer vision. Inspired by the recent success of deep learning\nmodels in solving various vision problems (e.g., object recognition, semantic\nsegmentation) and the fact that humans serve as the gold standard in assessing\nclustering algorithms, here, we advocate for a unified treatment of the two\nproblems and suggest that hierarchical frameworks that progressively build\ncomplex patterns on top of the simpler ones (e.g., convolutional neural\nnetworks) offer a promising solution. We do not dwell much on the learning\nmechanisms in these frameworks as they are still a matter of debate, with\nrespect to biological constraints. Instead, we emphasize on the\ncompositionality of the real world structures and objects. In particular, we\nshow that CNNs, trained end to end using back propagation with noisy labels,\nare able to cluster data points belonging to several overlapping shapes, and do\nso much better than the state of the art algorithms. The main takeaway lesson\nfrom our study is that mechanisms of human vision, particularly the hierarchal\norganization of the visual ventral stream should be taken into account in\nclustering algorithms (e.g., for learning representations in an unsupervised\nmanner or with minimum supervision) to reach human level clustering\nperformance. This, by no means, suggests that other methods do not hold merits.\nFor example, methods relying on pairwise affinities (e.g., spectral clustering)\nhave been very successful in many scenarios but still fail in some cases (e.g.,\noverlapping clusters).\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 19:10:50 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 23:45:26 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Borji", "Ali", ""], ["Dundar", "Aysegul", ""]]}, {"id": "1706.05067", "submitter": "Yichun Shi", "authors": "Yichun Shi, Charles Otto and Anil K. Jain", "title": "Face Clustering: Representation and Pairwise Constraints", "comments": "This second version is the same as TIFS version. Some experiment\n  results are different from v1 because we correct the protocols", "journal-ref": "IEEE Transactions on Information Forensics and Security ( Volume:\n  13, Issue: 7, July 2018 )", "doi": "10.1109/TIFS.2018.2796999", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering face images according to their identity has two important\napplications: (i) grouping a collection of face images when no external labels\nare associated with images, and (ii) indexing for efficient large scale face\nretrieval. The clustering problem is composed of two key parts: face\nrepresentation and choice of similarity for grouping faces. We first propose a\nrepresentation based on ResNet, which has been shown to perform very well in\nimage classification problems. Given this representation, we design a\nclustering algorithm, Conditional Pairwise Clustering (ConPaC), which directly\nestimates the adjacency matrix only based on the similarity between face\nimages. This allows a dynamic selection of number of clusters and retains\npairwise similarity between faces. ConPaC formulates the clustering problem as\na Conditional Random Field (CRF) model and uses Loopy Belief Propagation to\nfind an approximate solution for maximizing the posterior probability of the\nadjacency matrix. Experimental results on two benchmark face datasets (LFW and\nIJB-B) show that ConPaC outperforms well known clustering algorithms such as\nk-means, spectral clustering and approximate rank-order. Additionally, our\nalgorithm can naturally incorporate pairwise constraints to obtain a\nsemi-supervised version that leads to improved clustering performance. We also\npropose an k-NN variant of ConPaC, which has a linear time complexity given a\nk-NN graph, suitable for large datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 20:17:33 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 01:13:07 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Shi", "Yichun", ""], ["Otto", "Charles", ""], ["Jain", "Anil K.", ""]]}, {"id": "1706.05105", "submitter": "Vitaly Galinsky", "authors": "Vitaly L. Galinsky and Lawrence R. Frank", "title": "Symplectomorphic registration with phase space regularization by entropy\n  spectrum pathways", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to register image data to a common coordinate system is a\ncritical feature of virtually all imaging studies that require multiple subject\nanalysis, combining single subject data from multiple modalities, or both.\nHowever, in spite of the abundance of literature on the subject and the\nexistence of several variants of registration algorithms, their practical\nutility remains problematic, as commonly acknowledged even by developers of\nthese methods because the complexity of the problem has resisted a general,\nflexible, and robust theoretical and computational framework.\n  To address this issue, we present a new registration method that is similar\nin spirit to the current state-of-the-art technique of diffeomorphic mapping,\nbut is more general and flexible. The method utilizes a Hamiltonian formalism\nand constructs registration as a sequence of symplectomorphic maps in\nconjunction with a novel phase space regularization based on the powerful\nentropy spectrum pathways (ESP) framework.\n  The method is demonstrated on the three different magnetic resonance imaging\n(MRI) modalities routinely used for human neuroimaging applications by mapping\nbetween high resolution anatomical (HRA) volumes, medium resolution diffusion\nweighted MRI (DW-MRI) and HRA volumes, and low resolution functional MRI (fMRI)\nand HRA volumes. The typical processing time for high quality mapping ranges\nfrom less than a minute to several minutes on a modern multi core CPU for\ntypical high resolution anatomical (~256x256x256 voxels) MRI volumes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 21:46:44 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Galinsky", "Vitaly L.", ""], ["Frank", "Lawrence R.", ""]]}, {"id": "1706.05150", "submitter": "He-Da Wang", "authors": "He-Da Wang, Teng Zhang, Ji Wu", "title": "The Monkeytyping Solution to the YouTube-8M Video Understanding\n  Challenge", "comments": "Submitted to the CVPR 2017 Workshop on YouTube-8M Large-Scale Video\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the final solution of team monkeytyping, who finished\nin second place in the YouTube-8M video understanding challenge. The dataset\nused in this challenge is a large-scale benchmark for multi-label video\nclassification. We extend the work in [1] and propose several improvements for\nframe sequence modeling. We propose a network structure called Chaining that\ncan better capture the interactions between labels. Also, we report our\napproaches in dealing with multi-scale information and attention pooling. In\naddition, We find that using the output of model ensemble as a side target in\ntraining can boost single model performance. We report our experiments in\nbagging, boosting, cascade, and stacking, and propose a stacking algorithm\ncalled attention weighted stacking. Our final submission is an ensemble that\nconsists of 74 sub models, all of which are listed in the appendix.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 05:39:53 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Wang", "He-Da", ""], ["Zhang", "Teng", ""], ["Wu", "Ji", ""]]}, {"id": "1706.05157", "submitter": "Shuai Li", "authors": "Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, Yanbo Gao", "title": "A Fully Trainable Network with RNN-based Pooling", "comments": "17 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooling is an important component in convolutional neural networks (CNNs) for\naggregating features and reducing computational burden. Compared with other\ncomponents such as convolutional layers and fully connected layers which are\ncompletely learned from data, the pooling component is still handcrafted such\nas max pooling and average pooling. This paper proposes a learnable pooling\nfunction using recurrent neural networks (RNN) so that the pooling can be fully\nadapted to data and other components of the network, leading to an improved\nperformance. Such a network with learnable pooling function is referred to as a\nfully trainable network (FTN). Experimental results have demonstrated that the\nproposed RNN-based pooling can well approximate the existing pooling functions\nand improve the performance of the network. Especially for small networks, the\nproposed FTN can improve the performance by seven percentage points in terms of\nerror rate on the CIFAR-10 dataset compared with the traditional CNN.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 06:42:15 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Li", "Shuai", ""], ["Li", "Wanqing", ""], ["Cook", "Chris", ""], ["Zhu", "Ce", ""], ["Gao", "Yanbo", ""]]}, {"id": "1706.05170", "submitter": "Jerry Liu", "authors": "Jerry Liu and Fisher Yu and Thomas Funkhouser", "title": "Interactive 3D Modeling with a Generative Adversarial Network", "comments": "Published at International Conference on 3D Vision 2017\n  (http://irc.cs.sdu.edu.cn/3dv/index.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the idea of using a generative adversarial network (GAN)\nto assist a novice user in designing real-world shapes with a simple interface.\nThe user edits a voxel grid with a painting interface (like Minecraft). Yet, at\nany time, he/she can execute a SNAP command, which projects the current voxel\ngrid onto a latent shape manifold with a learned projection operator and then\ngenerates a similar, but more realistic, shape using a learned generator\nnetwork. Then the user can edit the resulting shape and snap again until he/she\nis satisfied with the result. The main advantage of this approach is that the\nprojection and generation operators assist novice users to create 3D models\ncharacteristic of a background distribution of object shapes, but without\nhaving to specify all the details. The core new research idea is to use a GAN\nto support this application. 3D GANs have previously been used for shape\ngeneration, interpolation, and completion, but never for interactive modeling.\nThe new challenge for this application is to learn a projection operator that\ntakes an arbitrary 3D voxel model and produces a latent vector on the shape\nmanifold from which a similar and realistic shape can be generated. We develop\nalgorithms for this and other steps of the SNAP processing pipeline and\nintegrate them into a simple modeling tool. Experiments with these algorithms\nand tool suggest that GANs provide a promising approach to computer-assisted\ninteractive modeling.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 08:01:09 GMT"}, {"version": "v2", "created": "Sun, 7 Jan 2018 08:55:45 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Liu", "Jerry", ""], ["Yu", "Fisher", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1706.05206", "submitter": "Jakob Verbeek", "authors": "Nitika Verma, Edmond Boyer, Jakob Verbeek", "title": "FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have massively impacted visual\nrecognition in 2D images, and are now ubiquitous in state-of-the-art\napproaches. CNNs do not easily extend, however, to data that are not\nrepresented by regular grids, such as 3D shape meshes or other graph-structured\ndata, to which traditional local convolution operators do not directly apply.\nTo address this problem, we propose a novel graph-convolution operator to\nestablish correspondences between filter weights and graph neighborhoods with\narbitrary connectivity. The key novelty of our approach is that these\ncorrespondences are dynamically computed from features learned by the network,\nrather than relying on predefined static coordinates over the graph as in\nprevious work. We obtain excellent experimental results that significantly\nimprove over previous state-of-the-art shape correspondence results. This shows\nthat our approach can learn effective shape representations from raw input\ncoordinates, without relying on shape descriptors.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 10:08:53 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 13:27:39 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Verma", "Nitika", ""], ["Boyer", "Edmond", ""], ["Verbeek", "Jakob", ""]]}, {"id": "1706.05208", "submitter": "Geoffrey French", "authors": "Geoffrey French, Michal Mackiewicz, Mark Fisher", "title": "Self-ensembling for visual domain adaptation", "comments": "20 pages, 3 figure, accepted as a poster at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of self-ensembling for visual domain adaptation\nproblems. Our technique is derived from the mean teacher variant (Tarvainen et\nal., 2017) of temporal ensembling (Laine et al;, 2017), a technique that\nachieved state of the art results in the area of semi-supervised learning. We\nintroduce a number of modifications to their approach for challenging domain\nadaptation scenarios and evaluate its effectiveness. Our approach achieves\nstate of the art results in a variety of benchmarks, including our winning\nentry in the VISDA-2017 visual domain adaptation challenge. In small image\nbenchmarks, our algorithm not only outperforms prior art, but can also achieve\naccuracy that is close to that of a classifier trained in a supervised fashion.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 10:10:42 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 08:18:23 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 09:38:42 GMT"}, {"version": "v4", "created": "Sun, 23 Sep 2018 05:50:44 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["French", "Geoffrey", ""], ["Mackiewicz", "Michal", ""], ["Fisher", "Mark", ""]]}, {"id": "1706.05249", "submitter": "Frosti Palsson", "authors": "Frosti Palsson, Johannes R. Sveinsson, Magnus O. Ulfarsson", "title": "Multispectral and Hyperspectral Image Fusion Using a 3-D-Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2017.2668299", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method using a three dimensional convolutional\nneural network (3-D-CNN) to fuse together multispectral (MS) and hyperspectral\n(HS) images to obtain a high resolution hyperspectral image. Dimensionality\nreduction of the hyperspectral image is performed prior to fusion in order to\nsignificantly reduce the computational time and make the method more robust to\nnoise. Experiments are performed on a data set simulated using a real\nhyperspectral image. The results obtained show that the proposed approach is\nvery promising when compared to conventional methods. This is especially true\nwhen the hyperspectral image is corrupted by additive noise.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 12:38:44 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Palsson", "Frosti", ""], ["Sveinsson", "Johannes R.", ""], ["Ulfarsson", "Magnus O.", ""]]}, {"id": "1706.05274", "submitter": "Jianan Li", "authors": "Jianan Li, Xiaodan Liang, Yunchao Wei, Tingfa Xu, Jiashi Feng,\n  Shuicheng Yan", "title": "Perceptual Generative Adversarial Networks for Small Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting small objects is notoriously challenging due to their low\nresolution and noisy representation. Existing object detection pipelines\nusually detect small objects through learning representations of all the\nobjects at multiple scales. However, the performance gain of such ad hoc\narchitectures is usually limited to pay off the computational cost. In this\nwork, we address the small object detection problem by developing a single\narchitecture that internally lifts representations of small objects to\n\"super-resolved\" ones, achieving similar characteristics as large objects and\nthus more discriminative for detection. For this purpose, we propose a new\nPerceptual Generative Adversarial Network (Perceptual GAN) model that improves\nsmall object detection through narrowing representation difference of small\nobjects from the large ones. Specifically, its generator learns to transfer\nperceived poor representations of the small objects to super-resolved ones that\nare similar enough to real large objects to fool a competing discriminator.\nMeanwhile its discriminator competes with the generator to identify the\ngenerated representation and imposes an additional perceptual requirement -\ngenerated representations of small objects must be beneficial for detection\npurpose - on the generator. Extensive evaluations on the challenging\nTsinghua-Tencent 100K and the Caltech benchmark well demonstrate the\nsuperiority of Perceptual GAN in detecting small objects, including traffic\nsigns and pedestrians, over well-established state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 13:41:54 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 14:38:43 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Li", "Jianan", ""], ["Liang", "Xiaodan", ""], ["Wei", "Yunchao", ""], ["Xu", "Tingfa", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1706.05340", "submitter": "David Estevez", "authors": "David Estevez, Juan G. Victores, Raul Fernandez-Fernandez and Carlos\n  Balaguer", "title": "Robotic Ironing with 3D Perception and Force/Torque Feedback in\n  Household Environments", "comments": "Accepted and to be published on the 2017 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2017) that will be held in\n  Vancouver, Canada, September 24-28, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robotic systems become more popular in household environments, the\ncomplexity of required tasks also increases. In this work we focus on a\ndomestic chore deemed dull by a majority of the population, the task of\nironing. The presented algorithm improves on the limited number of previous\nworks by joining 3D perception with force/torque sensing, with emphasis on\nfinding a practical solution with a feasible implementation in a domestic\nsetting. Our algorithm obtains a point cloud representation of the working\nenvironment. From this point cloud, the garment is segmented and a custom\nWrinkleness Local Descriptor (WiLD) is computed to determine the location of\nthe present wrinkles. Using this descriptor, the most suitable ironing path is\ncomputed and, based on it, the manipulation algorithm performs the\nforce-controlled ironing operation. Experiments have been performed with a\nhumanoid robot platform, proving that our algorithm is able to detect\nsuccessfully wrinkles present in garments and iteratively reduce the\nwrinkleness using an unmodified iron.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 16:37:54 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Estevez", "David", ""], ["Victores", "Juan G.", ""], ["Fernandez-Fernandez", "Raul", ""], ["Balaguer", "Carlos", ""]]}, {"id": "1706.05415", "submitter": "Min Liu", "authors": "Min Liu and Tobi Delbruck", "title": "Block-Matching Optical Flow for Dynamic Vision Sensor- Algorithm and\n  FPGA Implementation", "comments": "Published in ISCAS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid and low power computation of optical flow (OF) is potentially useful in\nrobotics. The dynamic vision sensor (DVS) event camera produces quick and\nsparse output, and has high dynamic range, but conventional OF algorithms are\nframe-based and cannot be directly used with event-based cameras. Previous DVS\nOF methods do not work well with dense textured input and are designed for\nimplementation in logic circuits. This paper proposes a new block-matching\nbased DVS OF algorithm which is inspired by motion estimation methods used for\nMPEG video compression. The algorithm was implemented both in software and on\nFPGA. For each event, it computes the motion direction as one of 9 directions.\nThe speed of the motion is set by the sample interval. Results show that the\nAverage Angular Error can be improved by 30\\% compared with previous methods.\nThe OF can be calculated on FPGA with 50\\,MHz clock in 0.2\\,us per event (11\nclock cycles), 20 times faster than a Java software implementation running on a\ndesktop PC. Sample data is shown that the method works on scenes dominated by\nedges, sparse features, and dense texture.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 19:45:47 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Liu", "Min", ""], ["Delbruck", "Tobi", ""]]}, {"id": "1706.05461", "submitter": "Jie Lin", "authors": "Zhe Wang, Kingsley Kuan, Mathieu Ravaut, Gaurav Manek, Sibo Song, Yuan\n  Fang, Seokhwan Kim, Nancy Chen, Luis Fernando D'Haro, Luu Anh Tuan, Hongyuan\n  Zhu, Zeng Zeng, Ngai Man Cheung, Georgios Piliouras, Jie Lin, Vijay\n  Chandrasekhar", "title": "Truly Multi-modal YouTube-8M Video Classification with Video, Audio, and\n  Text", "comments": "8 pages, Accepted to CVPR'17 Workshop on YouTube-8M Large-Scale Video\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The YouTube-8M video classification challenge requires teams to classify 0.7\nmillion videos into one or more of 4,716 classes. In this Kaggle competition,\nwe placed in the top 3% out of 650 participants using released video and audio\nfeatures. Beyond that, we extend the original competition by including text\ninformation in the classification, making this a truly multi-modal approach\nwith vision, audio and text. The newly introduced text data is termed as\nYouTube-8M-Text. We present a classification framework for the joint use of\ntext, visual and audio features, and conduct an extensive set of experiments to\nquantify the benefit that this additional mode brings. The inclusion of text\nyields state-of-the-art results, e.g. 86.7% GAP on the YouTube-8M-Text\nvalidation dataset.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 00:39:04 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 02:06:33 GMT"}, {"version": "v3", "created": "Mon, 10 Jul 2017 00:44:45 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Wang", "Zhe", ""], ["Kuan", "Kingsley", ""], ["Ravaut", "Mathieu", ""], ["Manek", "Gaurav", ""], ["Song", "Sibo", ""], ["Fang", "Yuan", ""], ["Kim", "Seokhwan", ""], ["Chen", "Nancy", ""], ["D'Haro", "Luis Fernando", ""], ["Tuan", "Luu Anh", ""], ["Zhu", "Hongyuan", ""], ["Zeng", "Zeng", ""], ["Cheung", "Ngai Man", ""], ["Piliouras", "Georgios", ""], ["Lin", "Jie", ""], ["Chandrasekhar", "Vijay", ""]]}, {"id": "1706.05507", "submitter": "Mahesh Chandra Mukkamala", "authors": "Mahesh Chandra Mukkamala, Matthias Hein", "title": "Variants of RMSProp and Adagrad with Logarithmic Regret Bounds", "comments": "ICML 2017, 16 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive gradient methods have become recently very popular, in particular as\nthey have been shown to be useful in the training of deep neural networks. In\nthis paper we have analyzed RMSProp, originally proposed for the training of\ndeep neural networks, in the context of online convex optimization and show\n$\\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and\nSC-RMSProp for which we show logarithmic regret bounds for strongly convex\nfunctions. Finally, we demonstrate in the experiments that these new variants\noutperform other adaptive gradient techniques or stochastic gradient descent in\nthe optimization of strongly convex functions as well as in training of deep\nneural networks.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 09:48:55 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 18:47:37 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Mukkamala", "Mahesh Chandra", ""], ["Hein", "Matthias", ""]]}, {"id": "1706.05534", "submitter": "Shiyuan Li", "authors": "Shiyuan Li", "title": "Rotation Invariance Neural Network", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation invariance and translation invariance have great values in image\nrecognition tasks. In this paper, we bring a new architecture in convolutional\nneural network (CNN) named cyclic convolutional layer to achieve rotation\ninvariance in 2-D symbol recognition. We can also get the position and\norientation of the 2-D symbol by the network to achieve detection purpose for\nmultiple non-overlap target. Last but not least, this architecture can achieve\none-shot learning in some cases using those invariance.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 13:33:29 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Li", "Shiyuan", ""]]}, {"id": "1706.05587", "submitter": "Liang-Chieh Chen", "authors": "Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam", "title": "Rethinking Atrous Convolution for Semantic Image Segmentation", "comments": "Add more experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we revisit atrous convolution, a powerful tool to explicitly\nadjust filter's field-of-view as well as control the resolution of feature\nresponses computed by Deep Convolutional Neural Networks, in the application of\nsemantic image segmentation. To handle the problem of segmenting objects at\nmultiple scales, we design modules which employ atrous convolution in cascade\nor in parallel to capture multi-scale context by adopting multiple atrous\nrates. Furthermore, we propose to augment our previously proposed Atrous\nSpatial Pyramid Pooling module, which probes convolutional features at multiple\nscales, with image-level features encoding global context and further boost\nperformance. We also elaborate on implementation details and share our\nexperience on training our system. The proposed `DeepLabv3' system\nsignificantly improves over our previous DeepLab versions without DenseCRF\npost-processing and attains comparable performance with other state-of-art\nmodels on the PASCAL VOC 2012 semantic image segmentation benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 22:48:57 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 23:21:29 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 18:06:21 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Chen", "Liang-Chieh", ""], ["Papandreou", "George", ""], ["Schroff", "Florian", ""], ["Adam", "Hartwig", ""]]}, {"id": "1706.05692", "submitter": "Nikolaos Passalis", "authors": "Nikolaos Passalis and Anastasios Tefas", "title": "Dimensionality Reduction using Similarity-induced Embeddings", "comments": "Accepted in IEEE Transactions on Neural Networks and Learning Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2017.2728818", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of Dimensionality Reduction (DR) techniques rely on\nsecond-order statistics to define their optimization objective. Even though\nthis provides adequate results in most cases, it comes with several\nshortcomings. The methods require carefully designed regularizers and they are\nusually prone to outliers. In this work, a new DR framework, that can directly\nmodel the target distribution using the notion of similarity instead of\ndistance, is introduced. The proposed framework, called Similarity Embedding\nFramework, can overcome the aforementioned limitations and provides a\nconceptually simpler way to express optimization targets similar to existing DR\ntechniques. Deriving a new DR technique using the Similarity Embedding\nFramework becomes simply a matter of choosing an appropriate target similarity\nmatrix. A variety of classical tasks, such as performing supervised\ndimensionality reduction and providing out-of-of-sample extensions, as well as,\nnew novel techniques, such as providing fast linear embeddings for complex\ntechniques, are demonstrated in this paper using the proposed framework. Six\ndatasets from a diverse range of domains are used to evaluate the proposed\nmethod and it is demonstrated that it can outperform many existing DR\ntechniques.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 17:03:02 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 09:00:30 GMT"}, {"version": "v3", "created": "Fri, 18 Aug 2017 07:41:27 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Passalis", "Nikolaos", ""], ["Tefas", "Anastasios", ""]]}, {"id": "1706.05721", "submitter": "Seyed Sadegh Mohseni Salehi", "authors": "Seyed Sadegh Mohseni Salehi, Deniz Erdogmus, Ali Gholipour", "title": "Tversky loss function for image segmentation using 3D fully\n  convolutional deep networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional deep neural networks carry out excellent potential for\nfast and accurate image segmentation. One of the main challenges in training\nthese networks is data imbalance, which is particularly problematic in medical\nimaging applications such as lesion segmentation where the number of lesion\nvoxels is often much lower than the number of non-lesion voxels. Training with\nunbalanced data can lead to predictions that are severely biased towards high\nprecision but low recall (sensitivity), which is undesired especially in\nmedical applications where false negatives are much less tolerable than false\npositives. Several methods have been proposed to deal with this problem\nincluding balanced sampling, two step training, sample re-weighting, and\nsimilarity loss functions. In this paper, we propose a generalized loss\nfunction based on the Tversky index to address the issue of data imbalance and\nachieve much better trade-off between precision and recall in training 3D fully\nconvolutional deep neural networks. Experimental results in multiple sclerosis\nlesion segmentation on magnetic resonance images show improved F2 score, Dice\ncoefficient, and the area under the precision-recall curve in test data. Based\non these results we suggest Tversky loss function as a generalized framework to\neffectively train deep neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 20:35:27 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Salehi", "Seyed Sadegh Mohseni", ""], ["Erdogmus", "Deniz", ""], ["Gholipour", "Ali", ""]]}, {"id": "1706.05726", "submitter": "Cemal Aker", "authors": "Cemal Aker, Sinan Kalkan", "title": "Using Deep Networks for Drone Detection", "comments": "To appear in International Workshop on Small-Drone Surveillance,\n  Detection and Counteraction Techniques organised within AVSS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drone detection is the problem of finding the smallest rectangle that\nencloses the drone(s) in a video sequence. In this study, we propose a solution\nusing an end-to-end object detection model based on convolutional neural\nnetworks. To solve the scarce data problem for training the network, we propose\nan algorithm for creating an extensive artificial dataset by combining\nbackground-subtracted real images. With this approach, we can achieve precision\nand recall values both of which are high at the same time.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 20:50:56 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Aker", "Cemal", ""], ["Kalkan", "Sinan", ""]]}, {"id": "1706.05739", "submitter": "Amirsina Torfi", "authors": "Amirsina Torfi, Seyed Mehdi Iranmanesh, Nasser M. Nasrabadi, Jeremy\n  Dawson", "title": "3D Convolutional Neural Networks for Cross Audio-Visual Matching\n  Recognition", "comments": null, "journal-ref": "IEEE Access (Year: 2017, Volume: PP, Issue: 99 )", "doi": "10.1109/ACCESS.2017.2761539", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio-visual recognition (AVR) has been considered as a solution for speech\nrecognition tasks when the audio is corrupted, as well as a visual recognition\nmethod used for speaker verification in multi-speaker scenarios. The approach\nof AVR systems is to leverage the extracted information from one modality to\nimprove the recognition ability of the other modality by complementing the\nmissing information. The essential problem is to find the correspondence\nbetween the audio and visual streams, which is the goal of this work. We\npropose the use of a coupled 3D Convolutional Neural Network (3D-CNN)\narchitecture that can map both modalities into a representation space to\nevaluate the correspondence of audio-visual streams using the learned\nmultimodal features. The proposed architecture will incorporate both spatial\nand temporal information jointly to effectively find the correlation between\ntemporal information for different modalities. By using a relatively small\nnetwork architecture and much smaller dataset for training, our proposed method\nsurpasses the performance of the existing similar methods for audio-visual\nmatching which use 3D CNNs for feature representation. We also demonstrate that\nan effective pair selection method can significantly increase the performance.\nThe proposed method achieves relative improvements over 20% on the Equal Error\nRate (EER) and over 7% on the Average Precision (AP) in comparison to the\nstate-of-the-art method.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 22:33:32 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 01:50:36 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 16:18:50 GMT"}, {"version": "v4", "created": "Mon, 7 Aug 2017 01:39:26 GMT"}, {"version": "v5", "created": "Sun, 13 Aug 2017 02:20:41 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Torfi", "Amirsina", ""], ["Iranmanesh", "Seyed Mehdi", ""], ["Nasrabadi", "Nasser M.", ""], ["Dawson", "Jeremy", ""]]}, {"id": "1706.05786", "submitter": "Denis Parra", "authors": "Pablo Messina and Vicente Dominguez and Denis Parra and Christoph\n  Trattner and Alvaro Soto", "title": "Exploring Content-based Artwork Recommendation with Metadata and Visual\n  Features", "comments": "1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to other areas, artwork recommendation has received little\nattention, despite the continuous growth of the artwork market. Previous\nresearch has relied on ratings and metadata to make artwork recommendations, as\nwell as visual features extracted with deep neural networks (DNN). However,\nthese features have no direct interpretation to explicit visual features (e.g.\nbrightness, texture) which might hinder explainability and user-acceptance. In\nthis work, we study the impact of artwork metadata as well as visual features\n(DNN-based and attractiveness-based) for physical artwork recommendation, using\nimages and transaction data from the UGallery online artwork store.\n  Our results indicate that: (i) visual features perform better than manually\ncurated data, (ii) DNN-based visual features perform better than\nattractiveness-based ones, and (iii) a hybrid approach improves the performance\nfurther. Our research can inform the development of new artwork recommenders\nrelying on diverse content data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 04:55:10 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 05:02:04 GMT"}, {"version": "v3", "created": "Mon, 23 Oct 2017 19:23:14 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Messina", "Pablo", ""], ["Dominguez", "Vicente", ""], ["Parra", "Denis", ""], ["Trattner", "Christoph", ""], ["Soto", "Alvaro", ""]]}, {"id": "1706.05791", "submitter": "Jian-Hao Luo", "authors": "Jian-Hao Luo, Jianxin Wu", "title": "An Entropy-based Pruning Method for CNN Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to simultaneously accelerate and compress off-the-shelf CNN\nmodels via filter pruning strategy. The importance of each filter is evaluated\nby the proposed entropy-based method first. Then several unimportant filters\nare discarded to get a smaller CNN model. Finally, fine-tuning is adopted to\nrecover its generalization ability which is damaged during filter pruning. Our\nmethod can reduce the size of intermediate activations, which would dominate\nmost memory footprint during model training stage but is less concerned in\nprevious compression methods. Experiments on the ILSVRC-12 benchmark\ndemonstrate the effectiveness of our method. Compared with previous filter\nimportance evaluation criteria, our entropy-based method obtains better\nperformance. We achieve 3.3x speed-up and 16.64x compression on VGG-16, 1.54x\nacceleration and 1.47x compression on ResNet-50, both with about 1% top-5\naccuracy decrease.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 05:29:29 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Luo", "Jian-Hao", ""], ["Wu", "Jianxin", ""]]}, {"id": "1706.05850", "submitter": "Michael Burke Dr", "authors": "Michael Burke, Siyabonga Mbonambi, Purity Molala, Raesetje Sefala", "title": "Rapid Probabilistic Interest Learning from Domain-Specific Pairwise\n  Image Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great deal of work aims to discover large general purpose models of image\ninterest or memorability for visual search and information retrieval. This\npaper argues that image interest is often domain and user specific, and that\nefficient mechanisms for learning about this domain-specific image interest as\nquickly as possible, while limiting the amount of data-labelling required, are\noften more useful to end-users. This work uses pairwise image comparisons to\nreduce the labelling burden on these users, and introduces an image interest\nestimation approach that performs similarly to recent data hungry deep learning\napproaches trained using pairwise ranking losses. Here, we use a Gaussian\nprocess model to interpolate image interest inferred using a Bayesian ranking\napproach over image features extracted using a pre-trained convolutional neural\nnetwork. Results show that fitting a Gaussian process in high-dimensional image\nfeature space is not only computationally feasible, but also effective across a\nbroad range of domains. The proposed probabilistic interest estimation approach\nproduces image interests paired with uncertainties that can be used to identify\nimages for which additional labelling is required and measure inference\nconvergence, allowing for sample efficient active model training. Importantly,\nthe probabilistic formulation allows for effective visual search and\ninformation retrieval when limited labelling data is available.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 09:37:29 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2019 15:54:57 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 08:12:37 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Burke", "Michael", ""], ["Mbonambi", "Siyabonga", ""], ["Molala", "Purity", ""], ["Sefala", "Raesetje", ""]]}, {"id": "1706.05864", "submitter": "Wei Zhou", "authors": "Wei Zhou and Caiwen Ma and Arjan Kuijper", "title": "Histograms of Gaussian normal distribution for feature matching in\n  clutter scenes", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D feature descriptor provide information between corresponding models and\nscenes. 3D objection recognition in cluttered scenes, however, remains a\nlargely unsolved problem. Practical applications impose several challenges\nwhich are not fully addressed by existing methods. Especially in cluttered\nscenes there are many feature mismatches between scenes and models. We\ntherefore propose Histograms of Gaussian Normal Distribution (HGND) for\nextracting salient features on a local reference frame (LRF) that enables us to\nsolve this problem. We propose a LRF on each local surface patches using the\nscatter matrix's eigenvectors. Then the HGND information of each salient point\nis calculated on the LRF, for which we use both the mesh and point data of the\ndepth image. Experiments on 45 cluttered scenes of the Bologna Dataset and 50\ncluttered scenes of the UWA Dataset are made to evaluate the robustness and\ndescriptiveness of our HGND. Experiments carried out by us demonstrate that\nHGND obtains a more reliable matching rate than state-of-the-art approaches in\ncluttered situations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 10:23:14 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Zhou", "Wei", ""], ["Ma", "Caiwen", ""], ["Kuijper", "Arjan", ""]]}, {"id": "1706.05870", "submitter": "Adel Hafiane", "authors": "Adel Hafiane, Pierre Vieyres and Alain Delbos", "title": "Deep learning with spatiotemporal consistency for nerve segmentation in\n  ultrasound images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound-Guided Regional Anesthesia (UGRA) has been gaining importance in\nthe last few years, offering numerous advantages over alternative methods of\nnerve localization (neurostimulation or paraesthesia). However, nerve detection\nis one of the most tasks that anaesthetists can encounter in the UGRA\nprocedure. Computer aided system that can detect automatically region of nerve,\nwould help practitioner to concentrate more in anaesthetic delivery. In this\npaper we propose a new method based on deep learning combined with\nspatiotemporal information to robustly segment the nerve region. The proposed\nmethod is based on two phases, localisation and segmentation. The first phase,\nconsists in using convolutional neural network combined with spatial and\ntemporal consistency to detect the nerve zone. The second phase utilises active\ncontour model to delineate the region of interest. Obtained results show the\nvalidity of the proposed approach and its robustness.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 10:36:39 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Hafiane", "Adel", ""], ["Vieyres", "Pierre", ""], ["Delbos", "Alain", ""]]}, {"id": "1706.05904", "submitter": "Eike Rehder", "authors": "Eike Rehder, Florian Wirth, Martin Lauer, Christoph Stiller", "title": "Pedestrian Prediction by Planning using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate traffic participant prediction is the prerequisite for collision\navoidance of autonomous vehicles. In this work, we predict pedestrians by\nemulating their own motion planning. From online observations, we infer a\nmixture density function for possible destinations. We use this result as the\ngoal states of a planning stage that performs motion prediction based on common\nbehavior patterns. The entire system is modeled as one monolithic neural\nnetwork and trained via inverse reinforcement learning. Experimental validation\non real world data shows the system's ability to predict both, destinations and\ntrajectories accurately.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 12:40:30 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 07:25:49 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Rehder", "Eike", ""], ["Wirth", "Florian", ""], ["Lauer", "Martin", ""], ["Stiller", "Christoph", ""]]}, {"id": "1706.05933", "submitter": "Giorgio Roffo", "authors": "Giorgio Roffo", "title": "Ranking to Learn and Learning to Rank: On the Role of Ranking in Pattern\n  Recognition Applications", "comments": "European PhD Thesis. arXiv admin note: text overlap with\n  arXiv:1601.06615, arXiv:1505.06821, arXiv:1704.02665 by other authors", "journal-ref": null, "doi": null, "report-no": "960962", "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The last decade has seen a revolution in the theory and application of\nmachine learning and pattern recognition. Through these advancements, variable\nranking has emerged as an active and growing research area and it is now\nbeginning to be applied to many new problems. The rationale behind this fact is\nthat many pattern recognition problems are by nature ranking problems. The main\nobjective of a ranking algorithm is to sort objects according to some criteria,\nso that, the most relevant items will appear early in the produced result list.\nRanking methods can be analyzed from two different methodological perspectives:\nranking to learn and learning to rank. The former aims at studying methods and\ntechniques to sort objects for improving the accuracy of a machine learning\nmodel. Enhancing a model performance can be challenging at times. For example,\nin pattern classification tasks, different data representations can complicate\nand hide the different explanatory factors of variation behind the data. In\nparticular, hand-crafted features contain many cues that are either redundant\nor irrelevant, which turn out to reduce the overall accuracy of the classifier.\nIn such a case feature selection is used, that, by producing ranked lists of\nfeatures, helps to filter out the unwanted information. Moreover, in real-time\nsystems (e.g., visual trackers) ranking approaches are used as optimization\nprocedures which improve the robustness of the system that deals with the high\nvariability of the image streams that change over time. The other way around,\nlearning to rank is necessary in the construction of ranking models for\ninformation retrieval, biometric authentication, re-identification, and\nrecommender systems. In this context, the ranking model's purpose is to sort\nobjects according to their degrees of relevance, importance, or preference as\ndefined in the specific application.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 09:15:35 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Roffo", "Giorgio", ""]]}, {"id": "1706.05952", "submitter": "Zhiyuan Shi", "authors": "Zhiyuan Shi, Timothy M. Hospedales, Tao Xiang", "title": "Bayesian Joint Modelling for Object Localisation in Weakly Labelled\n  Images", "comments": "Accepted in IEEE Transaction on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of localisation of objects as bounding boxes in images\nand videos with weak labels. This weakly supervised object localisation problem\nhas been tackled in the past using discriminative models where each object\nclass is localised independently from other classes. In this paper, a novel\nframework based on Bayesian joint topic modelling is proposed, which differs\nsignificantly from the existing ones in that: (1) All foreground object classes\nare modelled jointly in a single generative model that encodes multiple object\nco-existence so that \"explaining away\" inference can resolve ambiguity and lead\nto better learning and localisation. (2) Image backgrounds are shared across\nclasses to better learn varying surroundings and \"push out\" objects of\ninterest. (3) Our model can be learned with a mixture of weakly labelled and\nunlabelled data, allowing the large volume of unlabelled images on the Internet\nto be exploited for learning. Moreover, the Bayesian formulation enables the\nexploitation of various types of prior knowledge to compensate for the limited\nsupervision offered by weakly labelled data, as well as Bayesian domain\nadaptation for transfer learning. Extensive experiments on the PASCAL VOC,\nImageNet and YouTube-Object videos datasets demonstrate the effectiveness of\nour Bayesian joint model for weakly supervised object localisation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 13:59:48 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Shi", "Zhiyuan", ""], ["Hospedales", "Timothy M.", ""], ["Xiang", "Tao", ""]]}, {"id": "1706.05993", "submitter": "Hosnieh Sattar", "authors": "Hosnieh Sattar, Mario Fritz, Andreas Bulling", "title": "Visual Decoding of Targets During Visual Search From Human Eye Fixations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What does human gaze reveal about a users' intents and to which extend can\nthese intents be inferred or even visualized? Gaze was proposed as an implicit\nsource of information to predict the target of visual search and, more\nrecently, to predict the object class and attributes of the search target. In\nthis work, we go one step further and investigate the feasibility of combining\nrecent advances in encoding human gaze information using deep convolutional\nneural networks with the power of generative image models to visually decode,\ni.e. create a visual representation of, the search target. Such visual decoding\nis challenging for two reasons: 1) the search target only resides in the user's\nmind as a subjective visual pattern, and can most often not even be described\nverbally by the person, and 2) it is, as of yet, unclear if gaze fixations\ncontain sufficient information for this task at all. We show, for the first\ntime, that visual representations of search targets can indeed be decoded only\nfrom human gaze fixations. We propose to first encode fixations into a semantic\nrepresentation and then decode this representation into an image. We evaluate\nour method on a recent gaze dataset of 14 participants searching for clothing\nin image collages and validate the model's predictions using two human studies.\nOur results show that 62% (Chance level = 10%) of the time users were able to\nselect the categories of the decoded image right. In our second studies we show\nthe importance of a local gaze encoding for decoding visual search targets of\nuser\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 14:52:30 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 05:28:51 GMT"}, {"version": "v3", "created": "Wed, 21 Jun 2017 11:19:10 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Sattar", "Hosnieh", ""], ["Fritz", "Mario", ""], ["Bulling", "Andreas", ""]]}, {"id": "1706.06019", "submitter": "Francisco Belch\\'i Guillam\\'on", "authors": "Francisco Belch\\'i", "title": "Optimising the topological information of the $A_\\infty$-persistence\n  groups", "comments": "26 pages, 3 figures", "journal-ref": "Discrete and Computational Geometry, Volume 62(1) (2019), pages\n  29-54", "doi": "10.1007/s00454-019-00094-x", "report-no": null, "categories": "math.AT cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent homology typically studies the evolution of homology groups\n$H_p(X)$ (with coefficients in a field) along a filtration of topological\nspaces. $A_\\infty$-persistence extends this theory by analysing the evolution\nof subspaces such as $V := \\text{Ker}\\, {\\Delta_n}_{| H_p(X)} \\subseteq\nH_p(X)$, where $\\{\\Delta_m\\}_{m\\geq1}$ denotes a structure of\n$A_\\infty$-coalgebra on $H_*(X)$. In this paper we illustrate how\n$A_\\infty$-persistence can be useful beyond persistent homology by discussing\nthe topological meaning of $V$, which is the most basic form of\n$A_\\infty$-persistence group. In addition, we explore how to choose\n$A_\\infty$-coalgebras along a filtration to make the $A_\\infty$-persistence\ngroups carry more faithful information.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 15:37:53 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Belch\u00ed", "Francisco", ""]]}, {"id": "1706.06026", "submitter": "Alessia Amelio Dr.", "authors": "Alessia Amelio and Darko Brodi\\'c", "title": "The $\\mathcal{E}$-Average Common Submatrix: Approximate Searching in a\n  Restricted Neighborhood", "comments": "4 pages, 18th International Workshop on Combinatorial Image Analysis\n  (IWCIA 2017), Short Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new (dis)similarity measure for 2D arrays, extending\nthe Average Common Submatrix measure. This is accomplished by: (i) considering\nthe frequency of matching patterns, (ii) restricting the pattern matching to a\nfixed-size neighborhood, and (iii) computing a distance-based approximate\nmatching. This will achieve better performances with low execution time and\nlarger information retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 15:53:07 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Amelio", "Alessia", ""], ["Brodi\u0107", "Darko", ""]]}, {"id": "1706.06031", "submitter": "Dmitry Petrov", "authors": "Dmitry Petrov, Alexander Ivanov, Joshua Faskowitz, Boris Gutman,\n  Daniel Moyer, Julio Villalon, Neda Jahanshad and Paul Thompson", "title": "Evaluating 35 Methods to Generate Structural Connectomes Using Pairwise\n  Classification", "comments": "Accepted for MICCAI 2017, 8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is no consensus on how to construct structural brain networks from\ndiffusion MRI. How variations in pre-processing steps affect network\nreliability and its ability to distinguish subjects remains opaque. In this\nwork, we address this issue by comparing 35 structural connectome-building\npipelines. We vary diffusion reconstruction models, tractography algorithms and\nparcellations. Next, we classify structural connectome pairs as either\nbelonging to the same individual or not. Connectome weights and eight\ntopological derivative measures form our feature set. For experiments, we use\nthree test-retest datasets from the Consortium for Reliability and\nReproducibility (CoRR) comprised of a total of 105 individuals. We also compare\npairwise classification results to a commonly used parametric test-retest\nmeasure, Intraclass Correlation Coefficient (ICC).\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 16:05:11 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Petrov", "Dmitry", ""], ["Ivanov", "Alexander", ""], ["Faskowitz", "Joshua", ""], ["Gutman", "Boris", ""], ["Moyer", "Daniel", ""], ["Villalon", "Julio", ""], ["Jahanshad", "Neda", ""], ["Thompson", "Paul", ""]]}, {"id": "1706.06081", "submitter": "Jianyu Lin", "authors": "Jianyu Lin, Neil T. Clancy, Yang Hu, Ji Qi, Taran Tatla, Danail\n  Stoyanov, Lena Maier-Hein, Daniel S. Elson", "title": "Endoscopic Depth Measurement and Super-Spectral-Resolution Imaging", "comments": "accepted by MICCAI2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intra-operative measurements of tissue shape and multi/ hyperspectral\ninformation have the potential to provide surgical guidance and decision making\nsupport. We report an optical probe based system to combine sparse\nhyperspectral measurements and spectrally-encoded structured lighting (SL) for\nsurface measurements. The system provides informative signals for navigation\nwith a surgical interface. By rapidly switching between SL and white light (WL)\nmodes, SL information is combined with structure-from-motion (SfM) from white\nlight images, based on SURF feature detection and Lucas-Kanade (LK) optical\nflow to provide quasi-dense surface shape reconstruction with known scale in\nreal-time. Furthermore, \"super-spectral-resolution\" was realized, whereby the\nRGB images and sparse hyperspectral data were integrated to recover dense\npixel-level hyperspectral stacks, by using convolutional neural networks to\nupscale the wavelength dimension. Validation and demonstration of this system\nis reported on ex vivo/in vivo animal/ human experiments.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 17:49:20 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 13:13:04 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Lin", "Jianyu", ""], ["Clancy", "Neil T.", ""], ["Hu", "Yang", ""], ["Qi", "Ji", ""], ["Tatla", "Taran", ""], ["Stoyanov", "Danail", ""], ["Maier-Hein", "Lena", ""], ["Elson", "Daniel S.", ""]]}, {"id": "1706.06169", "submitter": "Vladimir Iglovikov", "authors": "Vladimir Iglovikov, Sergey Mushinskiy, Vladimir Osin", "title": "Satellite Imagery Feature Detection using Deep Convolutional Neural\n  Network: A Kaggle Competition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our approach to the DSTL Satellite Imagery Feature\nDetection challenge run by Kaggle. The primary goal of this challenge is\naccurate semantic segmentation of different classes in satellite imagery. Our\napproach is based on an adaptation of fully convolutional neural network for\nmultispectral data processing. In addition, we defined several modifications to\nthe training objective and overall training pipeline, e.g. boundary effect\nestimation, also we discuss usage of data augmentation strategies and\nreflectance indices. Our solution scored third place out of 419 entries. Its\naccuracy is comparable to the first two places, but unlike those solutions, it\ndoesn't rely on complex ensembling techniques and thus can be easily scaled for\ndeployment in production as a part of automatic feature labeling systems for\nsatellite imagery analysis.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 20:41:42 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Iglovikov", "Vladimir", ""], ["Mushinskiy", "Sergey", ""], ["Osin", "Vladimir", ""]]}, {"id": "1706.06196", "submitter": "Eyasu Mequanint Zemene", "authors": "Yonatan Tariku Tesfaye, Eyasu Zemene, Andrea Prati, Marcello Pelillo,\n  Mubarak Shah", "title": "Multi-Target Tracking in Multiple Non-Overlapping Cameras using\n  Constrained Dominant Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a unified three-layer hierarchical approach for solving\ntracking problems in multiple non-overlapping cameras is proposed. Given a\nvideo and a set of detections (obtained by any person detector), we first solve\nwithin-camera tracking employing the first two layers of our framework and,\nthen, in the third layer, we solve across-camera tracking by merging tracks of\nthe same person in all cameras in a simultaneous fashion. To best serve our\npurpose, a constrained dominant sets clustering (CDSC) technique, a\nparametrized version of standard quadratic optimization, is employed to solve\nboth tracking tasks. The tracking problem is caste as finding constrained\ndominant sets from a graph. In addition to having a unified framework that\nsimultaneously solves within- and across-camera tracking, the third layer helps\nlink broken tracks of the same person occurring during within-camera tracking.\nIn this work, we propose a fast algorithm, based on dynamics from evolutionary\ngame theory, which is efficient and salable to large-scale real-world\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 22:34:52 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Tesfaye", "Yonatan Tariku", ""], ["Zemene", "Eyasu", ""], ["Prati", "Andrea", ""], ["Pelillo", "Marcello", ""], ["Shah", "Mubarak", ""]]}, {"id": "1706.06197", "submitter": "Xu Sun", "authors": "Xu Sun, Xuancheng Ren, Shuming Ma, Houfeng Wang", "title": "meProp: Sparsified Back Propagation for Accelerated Deep Learning with\n  Reduced Overfitting", "comments": "Accepted by the 34th International Conference on Machine Learning\n  (ICML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet effective technique for neural network learning. The\nforward propagation is computed as usual. In back propagation, only a small\nsubset of the full gradient is computed to update the model parameters. The\ngradient vectors are sparsified in such a way that only the top-$k$ elements\n(in terms of magnitude) are kept. As a result, only $k$ rows or columns\n(depending on the layout) of the weight matrix are modified, leading to a\nlinear reduction ($k$ divided by the vector dimension) in the computational\ncost. Surprisingly, experimental results demonstrate that we can update only\n1-4% of the weights at each back propagation pass. This does not result in a\nlarger number of training iterations. More interestingly, the accuracy of the\nresulting models is actually improved rather than degraded, and a detailed\nanalysis is given. The code is available at https://github.com/lancopku/meProp\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 22:36:33 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 01:34:50 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 09:48:41 GMT"}, {"version": "v4", "created": "Tue, 31 Oct 2017 02:04:52 GMT"}, {"version": "v5", "created": "Mon, 11 Mar 2019 02:57:03 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Sun", "Xu", ""], ["Ren", "Xuancheng", ""], ["Ma", "Shuming", ""], ["Wang", "Houfeng", ""]]}, {"id": "1706.06208", "submitter": "William Kindel", "authors": "William F. Kindel, Elijah D. Christensen and Joel Zylberberg", "title": "Using deep learning to reveal the neural code for images in primary\n  visual cortex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Primary visual cortex (V1) is the first stage of cortical image processing,\nand a major effort in systems neuroscience is devoted to understanding how it\nencodes information about visual stimuli. Within V1, many neurons respond\nselectively to edges of a given preferred orientation: these are known as\nsimple or complex cells, and they are well-studied. Other neurons respond to\nlocalized center-surround image features. Still others respond selectively to\ncertain image stimuli, but the specific features that excite them are unknown.\nMoreover, even for the simple and complex cells-- the best-understood V1\nneurons-- it is challenging to predict how they will respond to natural image\nstimuli. Thus, there are important gaps in our understanding of how V1 encodes\nimages. To fill this gap, we train deep convolutional neural networks to\npredict the firing rates of V1 neurons in response to natural image stimuli,\nand find that 15% of these neurons are within 10% of their theoretical limit of\npredictability. For these well predicted neurons, we invert the predictor\nnetwork to identify the image features (receptive fields) that cause the V1\nneurons to spike. In addition to those with previously-characterized receptive\nfields (Gabor wavelet and center-surround), we identify neurons that respond\npredictably to higher-level textural image features that are not localized to\nany particular region of the image.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 23:13:54 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Kindel", "William F.", ""], ["Christensen", "Elijah D.", ""], ["Zylberberg", "Joel", ""]]}, {"id": "1706.06216", "submitter": "Yujia Li", "authors": "Yujia Li, Alexander Schwing, Kuan-Chieh Wang, Richard Zemel", "title": "Dualing GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial nets (GANs) are a promising technique for modeling a\ndistribution from samples. It is however well known that GAN training suffers\nfrom instability due to the nature of its maximin formulation. In this paper,\nwe explore ways to tackle the instability problem by dualizing the\ndiscriminator. We start from linear discriminators in which case conjugate\nduality provides a mechanism to reformulate the saddle point objective into a\nmaximization problem, such that both the generator and the discriminator of\nthis 'dualing GAN' act in concert. We then demonstrate how to extend this\nintuition to non-linear formulations. For GANs with linear discriminators our\napproach is able to remove the instability in training, while for GANs with\nnonlinear discriminators our approach provides an alternative to the commonly\nused GAN training algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 23:28:49 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Li", "Yujia", ""], ["Schwing", "Alexander", ""], ["Wang", "Kuan-Chieh", ""], ["Zemel", "Richard", ""]]}, {"id": "1706.06230", "submitter": "Gaurav Thakur", "authors": "Gaurav Thakur", "title": "A Bayesian algorithm for detecting identity matches and fraud in image\n  databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical algorithm for categorizing different types of matches and fraud\nin image databases is presented. The approach is based on a generative model of\na graph representing images and connections between pairs of identities,\ntrained using properties of a matching algorithm between images.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 00:43:22 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Thakur", "Gaurav", ""]]}, {"id": "1706.06247", "submitter": "Yalda Mohsenzadeh", "authors": "Erfan Zangeneh (1), Mohammad Rahmati (1), Yalda Mohsenzadeh (2) ((1)\n  Amirkabir University of Technology, (2) Massachusetts Institute of\n  Technology)", "title": "Low Resolution Face Recognition Using a Two-Branch Deep Convolutional\n  Neural Network Architecture", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel couple mappings method for low resolution face recognition\nusing deep convolutional neural networks (DCNNs). The proposed architecture\nconsists of two branches of DCNNs to map the high and low resolution face\nimages into a common space with nonlinear transformations. The branch\ncorresponding to transformation of high resolution images consists of 14 layers\nand the other branch which maps the low resolution face images to the common\nspace includes a 5-layer super-resolution network connected to a 14-layer\nnetwork. The distance between the features of corresponding high and low\nresolution images are backpropagated to train the networks. Our proposed method\nis evaluated on FERET data set and compared with state-of-the-art competing\nmethods. Our extensive experimental results show that the proposed method\nsignificantly improves the recognition performance especially for very low\nresolution probe face images (11.4% improvement in recognition accuracy).\nFurthermore, it can reconstruct a high resolution image from its corresponding\nlow resolution probe image which is comparable with state-of-the-art\nsuper-resolution methods in terms of visual quality.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 02:54:52 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Zangeneh", "Erfan", ""], ["Rahmati", "Mohammad", ""], ["Mohsenzadeh", "Yalda", ""]]}, {"id": "1706.06258", "submitter": "Chuyang Ye", "authors": "Chuyang Ye", "title": "Learning-based Ensemble Average Propagator Estimation", "comments": "Accepted by MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By capturing the anisotropic water diffusion in tissue, diffusion magnetic\nresonance imaging (dMRI) provides a unique tool for noninvasively probing the\ntissue microstructure and orientation in the human brain. The diffusion profile\ncan be described by the ensemble average propagator (EAP), which is inferred\nfrom observed diffusion signals. However, accurate EAP estimation using the\nnumber of diffusion gradients that is clinically practical can be challenging.\nIn this work, we propose a deep learning algorithm for EAP estimation, which is\nnamed learning-based ensemble average propagator estimation (LEAPE). The EAP is\ncommonly represented by a basis and its associated coefficients, and here we\nchoose the SHORE basis and design a deep network to estimate the coefficients.\nThe network comprises two cascaded components. The first component is a\nmultiple layer perceptron (MLP) that simultaneously predicts the unknown\ncoefficients. However, typical training loss functions, such as mean squared\nerrors, may not properly represent the geometry of the possibly non-Euclidean\nspace of the coefficients, which in particular causes problems for the\nextraction of directional information from the EAP. Therefore, to regularize\nthe training, in the second component we compute an auxiliary output of\napproximated fiber orientation (FO) errors with the aid of a second MLP that is\ntrained separately. We performed experiments using dMRI data that resemble\nclinically achievable $q$-space sampling, and observed promising results\ncompared with the conventional EAP estimation method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 03:55:50 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Ye", "Chuyang", ""]]}, {"id": "1706.06266", "submitter": "Longguang Wang", "authors": "Longguang Wang, Zaiping Lin, Xinpu Deng, Wei An", "title": "Multi-frame image super-resolution with fast upscaling technique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-frame image super-resolution (MISR) aims to fuse information in\nlow-resolution (LR) image sequence to compose a high-resolution (HR) one, which\nis applied extensively in many areas recently. Different with single image\nsuper-resolution (SISR), sub-pixel transitions between multiple frames\nintroduce additional information, attaching more significance to fusion\noperator to alleviate the ill-posedness of MISR. For reconstruction-based\napproaches, the inevitable projection of reconstruction errors from LR space to\nHR space is commonly tackled by an interpolation operator, however crude\ninterpolation may not fit the natural image and generate annoying blurring\nartifacts, especially after fusion operator. In this paper, we propose an\nend-to-end fast upscaling technique to replace the interpolation operator,\ndesign upscaling filters in LR space for periodic sub-locations respectively\nand shuffle the filter results to derive the final reconstruction errors in HR\nspace. The proposed fast upscaling technique not only reduce the computational\ncomplexity of the upscaling operation by utilizing shuffling operation to avoid\ncomplex operation in HR space, but also realize superior performance with fewer\nblurring artifacts. Extensive experimental results demonstrate the\neffectiveness and efficiency of the proposed technique, whilst, combining the\nproposed technique with bilateral total variation (BTV) regu-larization, the\nMISR approach outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 04:51:23 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 08:27:36 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Wang", "Longguang", ""], ["Lin", "Zaiping", ""], ["Deng", "Xinpu", ""], ["An", "Wei", ""]]}, {"id": "1706.06275", "submitter": "Satoshi Tsutsui", "authors": "Satoshi Tsutsui, David Crandall", "title": "Using Artificial Tokens to Control Languages for Multilingual Image\n  Caption Generation", "comments": "This work appears as an Extended Abstract at the 2017 CVPR Language\n  and Vision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in computer vision has yielded impressive results in\nautomatically describing images with natural language. Most of these systems\ngenerate captions in a sin- gle language, requiring multiple language-specific\nmodels to build a multilingual captioning system. We propose a very simple\ntechnique to build a single unified model across languages, using artificial\ntokens to control the language, making the captioning system more compact. We\nevaluate our approach on generating English and Japanese captions, and show\nthat a typical neural captioning architecture is capable of learning a single\nmodel that can switch between two different languages.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 05:50:49 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Tsutsui", "Satoshi", ""], ["Crandall", "David", ""]]}, {"id": "1706.06341", "submitter": "Yao Wang", "authors": "Kaidong Wang, Yao Wang, Qian Zhao, Deyu Meng and Zongben Xu", "title": "SPLBoost: An Improved Robust Boosting Algorithm Based on Self-paced\n  Learning", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that Boosting can be interpreted as a gradient descent technique\nto minimize an underlying loss function. Specifically, the underlying loss\nbeing minimized by the traditional AdaBoost is the exponential loss, which is\nproved to be very sensitive to random noise/outliers. Therefore, several\nBoosting algorithms, e.g., LogitBoost and SavageBoost, have been proposed to\nimprove the robustness of AdaBoost by replacing the exponential loss with some\ndesigned robust loss functions. In this work, we present a new way to robustify\nAdaBoost, i.e., incorporating the robust learning idea of Self-paced Learning\n(SPL) into Boosting framework. Specifically, we design a new robust Boosting\nalgorithm based on SPL regime, i.e., SPLBoost, which can be easily implemented\nby slightly modifying off-the-shelf Boosting packages. Extensive experiments\nand a theoretical characterization are also carried out to illustrate the\nmerits of the proposed SPLBoost.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 09:31:30 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 14:04:46 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Wang", "Kaidong", ""], ["Wang", "Yao", ""], ["Zhao", "Qian", ""], ["Meng", "Deyu", ""], ["Xu", "Zongben", ""]]}, {"id": "1706.06347", "submitter": "Laurent Hoeltgen", "authors": "Laurent Hoeltgen and Pascal Peter and Michael Breu{\\ss}", "title": "Clustering-Based Quantisation for PDE-Based Image Compression", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding optimal data for inpainting is a key problem in the context of\npartial differential equation based image compression. The data that yields the\nmost accurate reconstruction is real-valued. Thus, quantisation models are\nmandatory to allow an efficient encoding. These can also be understood as\nchallenging data clustering problems. Although clustering approaches are well\nsuited for this kind of compression codecs, very few works actually consider\nthem. Each pixel has a global impact on the reconstruction and optimal data\nlocations are strongly correlated with their corresponding colour values. These\nfacts make it hard to predict which feature works best.\n  In this paper we discuss quantisation strategies based on popular methods\nsuch as k-means. We are lead to the central question which kind of feature\nvectors are best suited for image compression. To this end we consider choices\nsuch as the pixel values, the histogram or the colour map.\n  Our findings show that the number of colours can be reduced significantly\nwithout impacting the reconstruction quality. Surprisingly, these benefits do\nnot directly translate to a good image compression performance. The gains in\nthe compression ratio are lost due to increased storage costs. This suggests\nthat it is integral to evaluate the clustering on both, the reconstruction\nerror and the final file size.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 10:02:05 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Hoeltgen", "Laurent", ""], ["Peter", "Pascal", ""], ["Breu\u00df", "Michael", ""]]}, {"id": "1706.06409", "submitter": "Bo Jiang", "authors": "Bo Jiang and Chris Ding", "title": "Revisiting L21-norm Robustness with Vector Outlier Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world applications, data usually contain outliers. One popular\napproach is to use L2,1 norm function as a robust error/loss function. However,\nthe robustness of L2,1 norm function is not well understood so far. In this\npaper, we propose a new Vector Outlier Regularization (VOR) framework to\nunderstand and analyze the robustness of L2,1 norm function. Our VOR function\ndefines a data point to be outlier if it is outside a threshold with respect to\na theoretical prediction, and regularize it-pull it back to the threshold line.\nWe then prove that L2,1 function is the limiting case of this VOR with the\nusual least square/L2 error function as the threshold shrinks to zero. One\ninteresting property of VOR is that how far an outlier lies away from its\ntheoretically predicted value does not affect the final regularization and\nanalysis results. This VOR property unmasks one of the most peculiar property\nof L2,1 norm function: The effects of outliers seem to be independent of how\noutlying they are-if an outlier is moved further away from the intrinsic\nmanifold/subspace, the final analysis results do not change. VOR provides a new\nway to understand and analyze the robustness of L2,1 norm function. Applying\nVOR to matrix factorization leads to a new VORPCA model. We give a\ncomprehensive comparison with trace-norm based L21-norm PCA to demonstrate the\nadvantages of VORPCA.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 13:20:11 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 07:45:27 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Jiang", "Bo", ""], ["Ding", "Chris", ""]]}, {"id": "1706.06411", "submitter": "Sharda Vashisth", "authors": "Neha Rani, Sharda Vashisth", "title": "Brain Tumor Detection and Classification with Feed Forward Back-Prop\n  Neural Network", "comments": null, "journal-ref": "International Journal of Computer Applications (0975 -- 8887),\n  Volume 146, No.12, July 2016", "doi": "10.5120/ijca2016910738", "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain is an organ that controls activities of all the parts of the body.\nRecognition of automated brain tumor in Magnetic resonance imaging (MRI) is a\ndifficult task due to complexity of size and location variability. This\nautomatic method detects all the type of cancer present in the body. Previous\nmethods for tumor are time consuming and less accurate. In the present work,\nstatistical analysis morphological and thresholding techniques are used to\nprocess the images obtained by MRI. Feed-forward back-prop neural network is\nused to classify the performance of tumors part of the image. This method\nresults high accuracy and less iterations detection which further reduces the\nconsumption time.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 12:04:40 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Rani", "Neha", ""], ["Vashisth", "Sharda", ""]]}, {"id": "1706.06419", "submitter": "Hussam Qassim Mr.", "authors": "Hussam Qassim, David Feinzimer, and Abhishek Verma", "title": "The Compressed Model of Residual CNDS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have achieved a great success in the recent\nyears. Although, the way to maximize the performance of the convolutional\nneural networks still in the beginning. Furthermore, the optimization of the\nsize and the time that need to train the convolutional neural networks is very\nfar away from reaching the researcher's ambition. In this paper, we proposed a\nnew convolutional neural network that combined several techniques to boost the\noptimization of the convolutional neural network in the aspects of speed and\nsize. As we used our previous model Residual-CNDS (ResCNDS), which solved the\nproblems of slower convergence, overfitting, and degradation, and compressed\nit. The outcome model called Residual-Squeeze-CNDS (ResSquCNDS), which we\ndemonstrated on our sold technique to add residual learning and our model of\ncompressing the convolutional neural networks. Our model of compressing adapted\nfrom the SQUEEZENET model, but our model is more generalizable, which can be\napplied almost to any neural network model, and fully integrated into the\nresidual learning, which addresses the problem of the degradation very\nsuccessfully. Our proposed model trained on very large-scale MIT\nPlaces365-Standard scene datasets, which backing our hypothesis that the new\ncompressed model inherited the best of the previous ResCNDS8 model, and almost\nget the same accuracy in the validation Top-1 and Top-5 with 87.64% smaller in\nsize and 13.33% faster in the training time.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 02:17:53 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Qassim", "Hussam", ""], ["Feinzimer", "David", ""], ["Verma", "Abhishek", ""]]}, {"id": "1706.06480", "submitter": "SeyedMajid Azimi", "authors": "Seyed Majid Azimi, Dominik Britz, Michael Engstler, Mario Fritz, Frank\n  M\\\"ucklich", "title": "Advanced Steel Microstructural Classification by Deep Learning Methods", "comments": "Published in Nature - Scientific Reports Journal", "journal-ref": null, "doi": "10.1038/s41598-018-20037-5", "report-no": null, "categories": "cs.CV cond-mat.mtrl-sci", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The inner structure of a material is called microstructure. It stores the\ngenesis of a material and determines all its physical and chemical properties.\nWhile microstructural characterization is widely spread and well known, the\nmicrostructural classification is mostly done manually by human experts, which\ngives rise to uncertainties due to subjectivity. Since the microstructure could\nbe a combination of different phases or constituents with complex substructures\nits automatic classification is very challenging and only a few prior studies\nexist. Prior works focused on designed and engineered features by experts and\nclassified microstructures separately from the feature extraction step.\nRecently, Deep Learning methods have shown strong performance in vision\napplications by learning the features from data together with the\nclassification step. In this work, we propose a Deep Learning method for\nmicrostructural classification in the examples of certain microstructural\nconstituents of low carbon steel. This novel method employs pixel-wise\nsegmentation via Fully Convolutional Neural Networks (FCNN) accompanied by a\nmax-voting scheme. Our system achieves 93.94% classification accuracy,\ndrastically outperforming the state-of-the-art method of 48.89% accuracy.\nBeyond the strong performance of our method, this line of research offers a\nmore robust and first of all objective way for the difficult task of steel\nquality appreciation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 14:29:42 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 14:30:16 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Azimi", "Seyed Majid", ""], ["Britz", "Dominik", ""], ["Engstler", "Michael", ""], ["Fritz", "Mario", ""], ["M\u00fccklich", "Frank", ""]]}, {"id": "1706.06531", "submitter": "Rene Lacher", "authors": "Rene Lacher, Francisco Vasconcelos, David Bishop, Norman Williams,\n  Mohammed Keshtgar, David Hawkes, John Hipwell, Danail Stoyanov", "title": "A comparative study of breast surface reconstruction for aesthetic\n  outcome assessment", "comments": "This paper has been accepted to MICCAI2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the most prevalent cancer type in women, and while its\nsurvival rate is generally high the aesthetic outcome is an increasingly\nimportant factor when evaluating different treatment alternatives. 3D scanning\nand reconstruction techniques offer a flexible tool for building detailed and\naccurate 3D breast models that can be used both pre-operatively for surgical\nplanning and post-operatively for aesthetic evaluation. This paper aims at\ncomparing the accuracy of low-cost 3D scanning technologies with the\nsignificantly more expensive state-of-the-art 3D commercial scanners in the\ncontext of breast 3D reconstruction. We present results from 28 synthetic and\nclinical RGBD sequences, including 12 unique patients and an anthropomorphic\nphantom demonstrating the applicability of low-cost RGBD sensors to real\nclinical cases. Body deformation and homogeneous skin texture pose challenges\nto the studied reconstruction systems. Although these should be addressed\nappropriately if higher model quality is warranted, we observe that low-cost\nsensors are able to obtain valuable reconstructions comparable to the\nstate-of-the-art within an error margin of 3 mm.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 16:10:30 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Lacher", "Rene", ""], ["Vasconcelos", "Francisco", ""], ["Bishop", "David", ""], ["Williams", "Norman", ""], ["Keshtgar", "Mohammed", ""], ["Hawkes", "David", ""], ["Hipwell", "John", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1706.06629", "submitter": "Martin R\\\"unz", "authors": "Martin R\\\"unz and Lourdes Agapito", "title": "Co-Fusion: Real-time Segmentation, Tracking and Fusion of Multiple\n  Objects", "comments": "International Conference on Robotics and Automation (ICRA) 2017,\n  http://visual.cs.ucl.ac.uk/pubs/cofusion,\n  https://github.com/martinruenz/co-fusion", "journal-ref": null, "doi": "10.1109/ICRA.2017.7989518", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce Co-Fusion, a dense SLAM system that takes a live\nstream of RGB-D images as input and segments the scene into different objects\n(using either motion or semantic cues) while simultaneously tracking and\nreconstructing their 3D shape in real time. We use a multiple model fitting\napproach where each object can move independently from the background and still\nbe effectively tracked and its shape fused over time using only the information\nfrom pixels associated with that object label. Previous attempts to deal with\ndynamic scenes have typically considered moving regions as outliers, and\nconsequently do not model their shape or track their motion over time. In\ncontrast, we enable the robot to maintain 3D models for each of the segmented\nobjects and to improve them over time through fusion. As a result, our system\ncan enable a robot to maintain a scene description at the object level which\nhas the potential to allow interactions with its working environment; even in\nthe case of dynamic scenes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 19:10:31 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["R\u00fcnz", "Martin", ""], ["Agapito", "Lourdes", ""]]}, {"id": "1706.06651", "submitter": "Nitin Khanna Dr.", "authors": "Hardik Jain, Gaurav Gupta, Sharad Joshi, Nitin Khanna", "title": "Passive Classification of Source Printer using Text-line-level Geometric\n  Distortion Signatures from Scanned Images of Printed Documents", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this digital era, one thing that still holds the convention is a printed\narchive. Printed documents find their use in many critical domains such as\ncontract papers, legal tenders and proof of identity documents. As more\nadvanced printing, scanning and image editing techniques are becoming\navailable, forgeries on these legal tenders pose a serious threat. Ability to\neasily and reliably identify source printer of a printed document can help a\nlot in reducing this menace. During printing procedure, printer hardware\nintroduces certain distortions in printed characters' locations and shapes\nwhich are invisible to naked eyes. These distortions are referred as geometric\ndistortions, their profile (or signature) is generally unique for each printer\nand can be used for printer classification purpose. This paper proposes a set\nof features for characterizing text-line-level geometric distortions, referred\nas geometric distortion signatures and presents a novel system to use them for\nidentification of the origin of a printed document. Detailed experiments\nperformed on a set of thirteen printers demonstrate that the proposed system\nachieves state of the art performance and gives much higher accuracy under\nsmall training size constraint. For four training and six test pages of three\ndifferent fonts, the proposed method gives 99\\% classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 20:11:45 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Jain", "Hardik", ""], ["Gupta", "Gaurav", ""], ["Joshi", "Sharad", ""], ["Khanna", "Nitin", ""]]}, {"id": "1706.06689", "submitter": "Garrett Goh", "authors": "Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas,\n  Nathan Baker", "title": "Chemception: A Deep Neural Network with Minimal Chemistry Knowledge\n  Matches the Performance of Expert-developed QSAR/QSPR Models", "comments": "Submitted to a chemistry peer-reviewed journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, we have seen the transformative impact of deep\nlearning in many applications, particularly in speech recognition and computer\nvision. Inspired by Google's Inception-ResNet deep convolutional neural network\n(CNN) for image classification, we have developed \"Chemception\", a deep CNN for\nthe prediction of chemical properties, using just the images of 2D drawings of\nmolecules. We develop Chemception without providing any additional explicit\nchemistry knowledge, such as basic concepts like periodicity, or advanced\nfeatures like molecular descriptors and fingerprints. We then show how\nChemception can serve as a general-purpose neural network architecture for\npredicting toxicity, activity, and solvation properties when trained on a\nmodest database of 600 to 40,000 compounds. When compared to multi-layer\nperceptron (MLP) deep neural networks trained with ECFP fingerprints,\nChemception slightly outperforms in activity and solvation prediction and\nslightly underperforms in toxicity prediction. Having matched the performance\nof expert-developed QSAR/QSPR deep learning models, our work demonstrates the\nplausibility of using deep neural networks to assist in computational chemistry\nresearch, where the feature engineering process is performed primarily by a\ndeep learning algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 22:25:57 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Goh", "Garrett B.", ""], ["Siegel", "Charles", ""], ["Vishnu", "Abhinav", ""], ["Hodas", "Nathan O.", ""], ["Baker", "Nathan", ""]]}, {"id": "1706.06694", "submitter": "Luz Mart\\'inez", "authors": "Luz Mar\\'ia Mart\\'inez and Javier Ruiz-del-Solar", "title": "Recognition of Grasp Points for Clothes Manipulation under unconstrained\n  Conditions", "comments": "Accepted in the RoboCup Symposium 2017. Final version will be\n  published at Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a system for recognizing grasp points in RGB-D images is\nproposed. This system is intended to be used by a domestic robot when deploying\nclothes lying at a random position on a table. By taking into consideration\nthat the grasp points are usually near key parts of clothing, such as the waist\nof pants or the neck of a shirt. The proposed system attempts to detect these\nkey parts first, using a local multivariate contour that adapts its shape\naccordingly. Then, the proposed system applies the Vessel Enhancement filter to\nidentify wrinkles in the clothes, allowing to compute a roughness index for the\nclothes. Finally, by mixing (i) the key part contours and (ii) the roughness\ninformation obtained by the vessel filter, the system is able to recognize\ngrasp points for unfolding a piece of clothing. The recognition system is\nvalidated using realistic RGB-D images of different cloth types.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 22:51:39 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Mart\u00ednez", "Luz Mar\u00eda", ""], ["Ruiz-del-Solar", "Javier", ""]]}, {"id": "1706.06702", "submitter": "Nicolas Cruz", "authors": "Nicol\\'as Cruz, Kenzo Lobos-Tsunekawa, and Javier Ruiz-del-Solar", "title": "Using Convolutional Neural Networks in Robots with Limited Computational\n  Resources: Detecting NAO Robots while Playing Soccer", "comments": "Accepted in the RoboCup Symposium 2017. Final version will be\n  published at Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this paper is to analyze the general problem of using\nConvolutional Neural Networks (CNNs) in robots with limited computational\ncapabilities, and to propose general design guidelines for their use. In\naddition, two different CNN based NAO robot detectors that are able to run in\nreal-time while playing soccer are proposed. One of the detectors is based on\nthe XNOR-Net and the other on the SqueezeNet. Each detector is able to process\na robot object-proposal in ~1ms, with an average number of 1.5 proposals per\nframe obtained by the upper camera of the NAO. The obtained detection rate is\n~97%.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 23:29:49 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Cruz", "Nicol\u00e1s", ""], ["Lobos-Tsunekawa", "Kenzo", ""], ["Ruiz-del-Solar", "Javier", ""]]}, {"id": "1706.06706", "submitter": "Yang Shi", "authors": "Yang Shi, Tommaso Furlanello and Anima Anandkumar", "title": "Compact Tensor Pooling for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing high level cognitive tasks requires the integration of feature\nmaps with drastically different structure. In Visual Question Answering (VQA)\nimage descriptors have spatial structures, while lexical inputs inherently\nfollow a temporal sequence. The recently proposed Multimodal Compact Bilinear\npooling (MCB) forms the outer products, via count-sketch approximation, of the\nvisual and textual representation at each spatial location. While this\nprocedure preserves spatial information locally, outer-products are taken\nindependently for each fiber of the activation tensor, and therefore do not\ninclude spatial context. In this work, we introduce multi-dimensional sketch\n({MD-sketch}), a novel extension of count-sketch to tensors. Using this new\nformulation, we propose Multimodal Compact Tensor Pooling (MCT) to fully\nexploit the global spatial context during bilinear pooling operations.\nContrarily to MCB, our approach preserves spatial context by directly\nconvolving the MD-sketch from the visual tensor features with the text vector\nfeature using higher order FFT. Furthermore we apply MCT incrementally at each\nstep of the question embedding and accumulate the multi-modal vectors with a\nsecond LSTM layer before the final answer is chosen.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 23:55:32 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Shi", "Yang", ""], ["Furlanello", "Tommaso", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1706.06718", "submitter": "Sean McMahon Mr", "authors": "Sean McMahon, Niko S\\\"underhauf, Ben Upcroft, and Michael Milford", "title": "Multi-Modal Trip Hazard Affordance Detection On Construction Sites", "comments": "9 Pages, 12 Figures, 2 Tables, Accepted to Robotics and Automation\n  Letters (RA-L)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trip hazards are a significant contributor to accidents on construction and\nmanufacturing sites, where over a third of Australian workplace injuries occur\n[1]. Current safety inspections are labour intensive and limited by human\nfallibility,making automation of trip hazard detection appealing from both a\nsafety and economic perspective. Trip hazards present an interesting challenge\nto modern learning techniques because they are defined as much by affordance as\nby object type; for example wires on a table are not a trip hazard, but can be\nif lying on the ground. To address these challenges, we conduct a comprehensive\ninvestigation into the performance characteristics of 11 different colour and\ndepth fusion approaches, including 4 fusion and one non fusion approach; using\ncolour and two types of depth images. Trained and tested on over 600 labelled\ntrip hazards over 4 floors and 2000m$\\mathrm{^{2}}$ in an active construction\nsite,this approach was able to differentiate between identical objects in\ndifferent physical configurations (see Figure 1). Outperforming a colour-only\ndetector, our multi-modal trip detector fuses colour and depth information to\nachieve a 4% absolute improvement in F1-score. These investigative results and\nthe extensive publicly available dataset moves us one step closer to assistive\nor fully automated safety inspection systems on construction sites.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 01:58:18 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["McMahon", "Sean", ""], ["S\u00fcnderhauf", "Niko", ""], ["Upcroft", "Ben", ""], ["Milford", "Michael", ""]]}, {"id": "1706.06720", "submitter": "Mohamed Loey", "authors": "Mohamed Loey, Ahmed El-Sawy, Hazem EL-Bakry", "title": "Deep Learning Autoencoder Approach for Handwritten Arabic Digits\n  Recognition", "comments": "6 pages", "journal-ref": null, "doi": "10.1007/978-3-319-48308-5_54", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new unsupervised learning approach with stacked\nautoencoder (SAE) for Arabic handwritten digits categorization. Recently,\nArabic handwritten digits recognition has been an important area due to its\napplications in several fields. This work is focusing on the recognition part\nof handwritten Arabic digits recognition that face several challenges,\nincluding the unlimited variation in human handwriting and the large public\ndatabases. Arabic digits contains ten numbers that were descended from the\nIndian digits system. Stacked autoencoder (SAE) tested and trained the MADBase\ndatabase (Arabic handwritten digits images) that contain 10000 testing images\nand 60000 training images. We show that the use of SAE leads to significant\nimprovements across different machine-learning classification algorithms. SAE\nis giving an average accuracy of 98.5%.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 02:02:31 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Loey", "Mohamed", ""], ["El-Sawy", "Ahmed", ""], ["EL-Bakry", "Hazem", ""]]}, {"id": "1706.06750", "submitter": "Ramkumar B", "authors": "Ramkumar B, R. S. Hegde, Rob Laber, Hristo Bojinov", "title": "GPGPU Acceleration of the KAZE Image Feature Extraction Algorithm", "comments": "10 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The recently proposed open-source KAZE image feature detection and\ndescription algorithm offers unprecedented performance in comparison to\nconventional ones like SIFT and SURF as it relies on nonlinear scale spaces\ninstead of Gaussian linear scale spaces. The improved performance, however,\ncomes with a significant computational cost limiting its use for many\napplications. We report a GPGPU implementation of the KAZE algorithm without\nresorting to binary descriptors for gaining speedup. For a 1920 by 1200 sized\nimage our Compute Unified Device Architecture (CUDA) C based GPU version took\naround 300 milliseconds on a NVIDIA GeForce GTX Titan X (Maxwell\nArchitecture-GM200) card in comparison to nearly 2400 milliseconds for a\nmultithreaded CPU version (16 threaded Intel(R) Xeon(R) CPU E5-2650\nprocesssor). The CUDA based parallel implementation is described in detail with\nfine-grained comparison between the GPU and CPU implementations. By achieving\nnearly 8 fold speedup without performance degradation our work expands the\napplicability of the KAZE algorithm. Additionally, the strategies described\nhere can prove useful for the GPU implementation of other nonlinear scale space\nbased methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 06:14:42 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["B", "Ramkumar", ""], ["Hegde", "R. S.", ""], ["Laber", "Rob", ""], ["Bojinov", "Hristo", ""]]}, {"id": "1706.06759", "submitter": "Chie Furusawa", "authors": "Chie Furusawa, Kazuyuki Hiroshiba, Keisuke Ogaki, Yuri Odagiri", "title": "Comicolorization: Semi-Automatic Manga Colorization", "comments": "to appear in SIGGRAPH Asia 2017 Technical Brief. Project page:\n  https://nico-opendata.jp/en/casestudy/comicolorization/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed \"Comicolorization\", a semi-automatic colorization system for\nmanga images. Given a monochrome manga and reference images as inputs, our\nsystem generates a plausible color version of the manga. This is the first work\nto address the colorization of an entire manga title (a set of manga pages).\nOur method colorizes a whole page (not a single panel) semi-automatically, with\nthe same color for the same character across multiple panels. To colorize the\ntarget character by the color from the reference image, we extract a color\nfeature from the reference and feed it to the colorization network to help the\ncolorization. Our approach employs adversarial loss to encourage the effect of\nthe color features. Optionally, our tool allows users to revise the\ncolorization result interactively. By feeding the color features to our deep\ncolorization network, we accomplish colorization of the entire manga using the\ndesired colors for each panel.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 06:52:09 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 11:28:42 GMT"}, {"version": "v3", "created": "Wed, 23 Aug 2017 08:22:42 GMT"}, {"version": "v4", "created": "Thu, 28 Sep 2017 12:58:52 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Furusawa", "Chie", ""], ["Hiroshiba", "Kazuyuki", ""], ["Ogaki", "Keisuke", ""], ["Odagiri", "Yuri", ""]]}, {"id": "1706.06768", "submitter": "Baisheng Lai", "authors": "Baisheng Lai, Xiaojin Gong", "title": "Saliency Guided End-to-End Learning for Weakly Supervised Object\n  Detection", "comments": "Accepted to appear in IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised object detection (WSOD), which is the problem of learning\ndetectors using only image-level labels, has been attracting more and more\ninterest. However, this problem is quite challenging due to the lack of\nlocation supervision. To address this issue, this paper integrates saliency\ninto a deep architecture, in which the location in- formation is explored both\nexplicitly and implicitly. Specifically, we select highly confident object pro-\nposals under the guidance of class-specific saliency maps. The location\ninformation, together with semantic and saliency information, of the selected\nproposals are then used to explicitly supervise the network by imposing two\nadditional losses. Meanwhile, a saliency prediction sub-network is built in the\narchitecture. The prediction results are used to implicitly guide the\nlocalization procedure. The entire network is trained end-to-end. Experiments\non PASCAL VOC demonstrate that our approach outperforms all state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 07:29:21 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Lai", "Baisheng", ""], ["Gong", "Xiaojin", ""]]}, {"id": "1706.06782", "submitter": "Param Rajpura", "authors": "Param S. Rajpura, Hristo Bojinov, Ravi S. Hegde", "title": "Object Detection Using Deep CNNs Trained on Synthetic Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The need for large annotated image datasets for training Convolutional Neural\nNetworks (CNNs) has been a significant impediment for their adoption in\ncomputer vision applications. We show that with transfer learning an effective\nobject detector can be trained almost entirely on synthetically rendered\ndatasets. We apply this strategy for detecting pack- aged food products\nclustered in refrigerator scenes. Our CNN trained only with 4000 synthetic\nimages achieves mean average precision (mAP) of 24 on a test set with 55\ndistinct products as objects of interest and 17 distractor objects. A further\nincrease of 12% in the mAP is obtained by adding only 400 real images to these\n4000 synthetic images in the training set. A high degree of photorealism in the\nsynthetic images was not essential in achieving this performance. We analyze\nfactors like training data set size and 3D model dictionary size for their\ninfluence on detection performance. Additionally, training strategies like\nfine-tuning with selected layers and early stopping which affect transfer\nlearning from synthetic scenes to real scenes are explored. Training CNNs with\nsynthetic datasets is a novel application of high-performance computing and a\npromising approach for object detection applications in domains where there is\na dearth of large annotated image data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 08:16:29 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 11:42:06 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Rajpura", "Param S.", ""], ["Bojinov", "Hristo", ""], ["Hegde", "Ravi S.", ""]]}, {"id": "1706.06792", "submitter": "Yujia Chen", "authors": "Yujia Chen and Ce Li", "title": "GM-Net: Learning Features with More Efficiency", "comments": "6 Pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) are capable of learning\nunprecedentedly effective features from images. Some researchers have struggled\nto enhance the parameters' efficiency using grouped convolution. However, the\nrelation between the optimal number of convolutional groups and the recognition\nperformance remains an open problem. In this paper, we propose a series of\nBasic Units (BUs) and a two-level merging strategy to construct deep CNNs,\nreferred to as a joint Grouped Merging Net (GM-Net), which can produce joint\ngrouped and reused deep features while maintaining the feature discriminability\nfor classification tasks. Our GM-Net architectures with the proposed BU_A\n(dense connection) and BU_B (straight mapping) lead to significant reduction in\nthe number of network parameters and obtain performance improvement in image\nclassification tasks. Extensive experiments are conducted to validate the\nsuperior performance of the GM-Net than the state-of-the-arts on the benchmark\ndatasets, e.g., MNIST, CIFAR-10, CIFAR-100 and SVHN.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 08:45:15 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Chen", "Yujia", ""], ["Li", "Ce", ""]]}, {"id": "1706.06873", "submitter": "Minsik Cho Dr.", "authors": "Minsik Cho, Daniel Brand", "title": "MEC: Memory-efficient Convolution for Deep Neural Network", "comments": "ICML2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution is a critical component in modern deep neural networks, thus\nseveral algorithms for convolution have been developed. Direct convolution is\nsimple but suffers from poor performance. As an alternative, multiple indirect\nmethods have been proposed including im2col-based convolution, FFT-based\nconvolution, or Winograd-based algorithm. However, all these indirect methods\nhave high memory-overhead, which creates performance degradation and offers a\npoor trade-off between performance and memory consumption. In this work, we\npropose a memory-efficient convolution or MEC with compact lowering, which\nreduces memory-overhead substantially and accelerates convolution process. MEC\nlowers the input matrix in a simple yet efficient/compact way (i.e., much less\nmemory-overhead), and then executes multiple small matrix multiplications in\nparallel to get convolution completed. Additionally, the reduced memory\nfootprint improves memory sub-system efficiency, improving performance. Our\nexperimental results show that MEC reduces memory consumption significantly\nwith good speedup on both mobile and server platforms, compared with other\nindirect convolution algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 13:00:39 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Cho", "Minsik", ""], ["Brand", "Daniel", ""]]}, {"id": "1706.06905", "submitter": "Antoine Miech", "authors": "Antoine Miech and Ivan Laptev and Josef Sivic", "title": "Learnable pooling with Context Gating for video classification", "comments": "Presented at Youtube 8M CVPR17 Workshop. Kaggle Winning model. Under\n  review for TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for video analysis often extract frame-level features using\npre-trained convolutional neural networks (CNNs). Such features are then\naggregated over time e.g., by simple temporal averaging or more sophisticated\nrecurrent neural networks such as long short-term memory (LSTM) or gated\nrecurrent units (GRU). In this work we revise existing video representations\nand study alternative methods for temporal aggregation. We first explore\nclustering-based aggregation layers and propose a two-stream architecture\naggregating audio and visual features. We then introduce a learnable non-linear\nunit, named Context Gating, aiming to model interdependencies among network\nactivations. Our experimental results show the advantage of both improvements\nfor the task of video classification. In particular, we evaluate our method on\nthe large-scale multi-modal Youtube-8M v2 dataset and outperform all other\nmethods in the Youtube 8M Large-Scale Video Understanding challenge.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 13:49:14 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 12:30:37 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Miech", "Antoine", ""], ["Laptev", "Ivan", ""], ["Sivic", "Josef", ""]]}, {"id": "1706.06917", "submitter": "Milad Niknejad", "authors": "Milad Niknejad, Jose M. Bioucas-Dias, Mario A. T. Figueiredo", "title": "Class-specific image denoising using importance sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new image denoising method, tailored to specific\nclasses of images, assuming that a dataset of clean images of the same class is\navailable. Similarly to the non-local means (NLM) algorithm, the proposed\nmethod computes a weighted average of non-local patches, which we interpret\nunder the importance sampling framework. This viewpoint introduces flexibility\nregarding the adopted priors, the noise statistics, and the computation of\nBayesian estimates. The importance sampling viewpoint is exploited to\napproximate the minimum mean squared error (MMSE) patch estimates, using the\ntrue underlying prior on image patches. The estimates thus obtained converge to\nthe true MMSE estimates, as the number of samples approaches infinity.\nExperimental results provide evidence that the proposed denoiser outperforms\nthe state-of-the-art in the specific classes of face and text images.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 14:11:29 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Niknejad", "Milad", ""], ["Bioucas-Dias", "Jose M.", ""], ["Figueiredo", "Mario A. T.", ""]]}, {"id": "1706.06918", "submitter": "Kiyoharu Aizawa Dr. Prof.", "authors": "Paulina Hensman and Kiyoharu Aizawa", "title": "cGAN-based Manga Colorization Using a Single Training Image", "comments": "8 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Japanese comic format known as Manga is popular all over the world. It is\ntraditionally produced in black and white, and colorization is time consuming\nand costly. Automatic colorization methods generally rely on greyscale values,\nwhich are not present in manga. Furthermore, due to copyright protection,\ncolorized manga available for training is scarce. We propose a manga\ncolorization method based on conditional Generative Adversarial Networks\n(cGAN). Unlike previous cGAN approaches that use many hundreds or thousands of\ntraining images, our method requires only a single colorized reference image\nfor training, avoiding the need of a large dataset. Colorizing manga using\ncGANs can produce blurry results with artifacts, and the resolution is limited.\nWe therefore also propose a method of segmentation and color-correction to\nmitigate these issues. The final results are sharp, clear, and in high\nresolution, and stay true to the character's original color scheme.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 14:11:32 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Hensman", "Paulina", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1706.06942", "submitter": "Douglas Summers Stay", "authors": "Douglas Summers-Stay", "title": "Graphcut Texture Synthesis for Single-Image Superresolution", "comments": "NYU Master's Thesis from 2006", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture synthesis has proven successful at imitating a wide variety of\ntextures. Adding additional constraints (in the form of a low-resolution\nversion of the texture to be synthesized) makes it possible to use texture\nsynthesis methods for texture superresolution.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 14:54:15 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Summers-Stay", "Douglas", ""]]}, {"id": "1706.06969", "submitter": "Robert Geirhos", "authors": "Robert Geirhos, David H. J. Janssen, Heiko H. Sch\\\"utt, Jonas Rauber,\n  Matthias Bethge, Felix A. Wichmann", "title": "Comparing deep neural networks against humans: object recognition when\n  the signal gets weaker", "comments": "updated article with reference to resulting publication (Geirhos et\n  al, NeurIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human visual object recognition is typically rapid and seemingly effortless,\nas well as largely independent of viewpoint and object orientation. Until very\nrecently, animate visual systems were the only ones capable of this remarkable\ncomputational feat. This has changed with the rise of a class of computer\nvision algorithms called deep neural networks (DNNs) that achieve human-level\nclassification performance on object recognition tasks. Furthermore, a growing\nnumber of studies report similarities in the way DNNs and the human visual\nsystem process objects, suggesting that current DNNs may be good models of\nhuman visual object recognition. Yet there clearly exist important\narchitectural and processing differences between state-of-the-art DNNs and the\nprimate visual system. The potential behavioural consequences of these\ndifferences are not well understood. We aim to address this issue by comparing\nhuman and DNN generalisation abilities towards image degradations. We find the\nhuman visual system to be more robust to image manipulations like contrast\nreduction, additive noise or novel eidolon-distortions. In addition, we find\nprogressively diverging classification error-patterns between humans and DNNs\nwhen the signal gets weaker, indicating that there may still be marked\ndifferences in the way humans and current DNNs perform visual object\nrecognition. We envision that our findings as well as our carefully measured\nand freely available behavioural datasets provide a new useful benchmark for\nthe computer vision community to improve the robustness of DNNs and a\nmotivation for neuroscientists to search for mechanisms in the brain that could\nfacilitate this robustness.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 15:46:52 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 17:06:24 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Geirhos", "Robert", ""], ["Janssen", "David H. J.", ""], ["Sch\u00fctt", "Heiko H.", ""], ["Rauber", "Jonas", ""], ["Bethge", "Matthias", ""], ["Wichmann", "Felix A.", ""]]}, {"id": "1706.06972", "submitter": "Quanming Yao", "authors": "Yaqing Wang, Quanming Yao, James T. Kwok, Lionel M. Ni", "title": "Scalable Online Convolutional Sparse Coding", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2842152", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional sparse coding (CSC) improves sparse coding by learning a\nshift-invariant dictionary from the data. However, existing CSC algorithms\noperate in the batch mode and are expensive, in terms of both space and time,\non large datasets. In this paper, we alleviate these problems by using online\nlearning. The key is a reformulation of the CSC objective so that convolution\ncan be handled easily in the frequency domain and much smaller history matrices\nare needed. We use the alternating direction method of multipliers (ADMM) to\nsolve the resulting optimization problem and the ADMM subproblems have\nefficient closed-form solutions. Theoretical analysis shows that the learned\ndictionary converges to a stationary point of the optimization problem.\nExtensive experiments show that convergence of the proposed method is much\nfaster and its reconstruction performance is also better. Moreover, while\nexisting CSC algorithms can only run on a small number of images, the proposed\nmethod can handle at least ten times more images.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 15:50:12 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 16:01:12 GMT"}, {"version": "v3", "created": "Thu, 2 Nov 2017 15:55:05 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Wang", "Yaqing", ""], ["Yao", "Quanming", ""], ["Kwok", "James T.", ""], ["Ni", "Lionel M.", ""]]}, {"id": "1706.06982", "submitter": "Matthew Tesfaldet", "authors": "Matthew Tesfaldet, Marcus A. Brubaker, Konstantinos G. Derpanis", "title": "Two-Stream Convolutional Networks for Dynamic Texture Synthesis", "comments": "In proc. CVPR 2018. Full results available at\n  https://ryersonvisionlab.github.io/two-stream-projpage/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a two-stream model for dynamic texture synthesis. Our model is\nbased on pre-trained convolutional networks (ConvNets) that target two\nindependent tasks: (i) object recognition, and (ii) optical flow prediction.\nGiven an input dynamic texture, statistics of filter responses from the object\nrecognition ConvNet encapsulate the per-frame appearance of the input texture,\nwhile statistics of filter responses from the optical flow ConvNet model its\ndynamics. To generate a novel texture, a randomly initialized input sequence is\noptimized to match the feature statistics from each stream of an example\ntexture. Inspired by recent work on image style transfer and enabled by the\ntwo-stream model, we also apply the synthesis approach to combine the texture\nappearance from one texture with the dynamics of another to generate entirely\nnovel dynamic textures. We show that our approach generates novel, high quality\nsamples that match both the framewise appearance and temporal evolution of\ninput texture. Finally, we quantitatively evaluate our texture synthesis\napproach with a thorough user study.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 16:09:28 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 18:42:02 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 23:47:29 GMT"}, {"version": "v4", "created": "Thu, 12 Apr 2018 21:39:51 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Tesfaldet", "Matthew", ""], ["Brubaker", "Marcus A.", ""], ["Derpanis", "Konstantinos G.", ""]]}, {"id": "1706.07002", "submitter": "Sara Moccia", "authors": "S. Moccia, S. J. Wirkert, H. Kenngott, A. S. Vemuri, M. Apitz, B.\n  Mayer, E. De Momi, L. S. Mattos, L. Maier-Hein", "title": "Uncertainty-Aware Organ Classification for Surgical Data Science\n  Applications in Laparoscopy", "comments": "7 pages, 6 images, 2 tables", "journal-ref": null, "doi": "10.1109/TBME.2018.2813015", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Surgical data science is evolving into a research field that aims\nto observe everything occurring within and around the treatment process to\nprovide situation-aware data-driven assistance. In the context of endoscopic\nvideo analysis, the accurate classification of organs in the field of view of\nthe camera proffers a technical challenge. Herein, we propose a new approach to\nanatomical structure classification and image tagging that features an\nintrinsic measure of confidence to estimate its own performance with high\nreliability and which can be applied to both RGB and multispectral imaging (MI)\ndata. Methods: Organ recognition is performed using a superpixel classification\nstrategy based on textural and reflectance information. Classification\nconfidence is estimated by analyzing the dispersion of class probabilities.\nAssessment of the proposed technology is performed through a comprehensive in\nvivo study with seven pigs. Results: When applied to image tagging, mean\naccuracy in our experiments increased from 65% (RGB) and 80% (MI) to 90% (RGB)\nand 96% (MI) with the confidence measure. Conclusion: Results showed that the\nconfidence measure had a significant influence on the classification accuracy,\nand MI data are better suited for anatomical structure labeling than RGB data.\nSignificance: This work significantly enhances the state of art in automatic\nlabeling of endoscopic videos by introducing the use of the confidence metric,\nand by being the first study to use MI data for in vivo laparoscopic tissue\nclassification. The data of our experiments will be released as the first in\nvivo MI dataset upon publication of this paper.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 16:49:39 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 12:38:24 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Moccia", "S.", ""], ["Wirkert", "S. J.", ""], ["Kenngott", "H.", ""], ["Vemuri", "A. S.", ""], ["Apitz", "M.", ""], ["Mayer", "B.", ""], ["De Momi", "E.", ""], ["Mattos", "L. S.", ""], ["Maier-Hein", "L.", ""]]}, {"id": "1706.07036", "submitter": "Chen-Hsuan Lin", "authors": "Chen-Hsuan Lin, Chen Kong, Simon Lucey", "title": "Learning Efficient Point Cloud Generation for Dense 3D Object\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional methods of 3D object generative modeling learn volumetric\npredictions using deep networks with 3D convolutional operations, which are\ndirect analogies to classical 2D ones. However, these methods are\ncomputationally wasteful in attempt to predict 3D shapes, where information is\nrich only on the surfaces. In this paper, we propose a novel 3D generative\nmodeling framework to efficiently generate object shapes in the form of dense\npoint clouds. We use 2D convolutional operations to predict the 3D structure\nfrom multiple viewpoints and jointly apply geometric reasoning with 2D\nprojection optimization. We introduce the pseudo-renderer, a differentiable\nmodule to approximate the true rendering operation, to synthesize novel depth\nmaps for optimization. Experimental results for single-image 3D object\nreconstruction tasks show that we outperforms state-of-the-art methods in terms\nof shape similarity and prediction density.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 17:56:59 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Lin", "Chen-Hsuan", ""], ["Kong", "Chen", ""], ["Lucey", "Simon", ""]]}, {"id": "1706.07041", "submitter": "Neel Joshi", "authors": "Brian Guenter, Neel Joshi, Richard Stoakley, Andrew Keefe, Kevin\n  Geary, Ryan Freeman, Jake Hundley, Pamela Patterson, David Hammon, Guillermo\n  Herrera, Elena Sherman, Andrew Nowak, Randall Schubert, Peter Brewer, Louis\n  Yang, Russell Mott, and Geoff McKnight", "title": "Highly curved image sensors: a practical approach for improved optical\n  performance", "comments": null, "journal-ref": "Opt. Express 25, 13010-13023 (2017)", "doi": "10.1364/OE.25.013010", "report-no": null, "categories": "physics.ins-det cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significant optical and size benefits of using a curved focal surface for\nimaging systems have been well studied yet never brought to market for lack of\na high-quality, mass-producible, curved image sensor. In this work we\ndemonstrate that commercial silicon CMOS image sensors can be thinned and\nformed into accurate, highly curved optical surfaces with undiminished\nfunctionality. Our key development is a pneumatic forming process that avoids\nrigid mechanical constraints and suppresses wrinkling instabilities. A\ncombination of forming-mold design, pressure membrane elastic properties, and\ncontrolled friction forces enables us to gradually contact the die at the\ncorners and smoothly press the sensor into a spherical shape. Allowing the die\nto slide into the concave target shape enables a threefold increase in the\nspherical curvature over prior approaches having mechanical constraints that\nresist deformation, and create a high-stress, stretch-dominated state. Our\nprocess creates a bridge between the high precision and low-cost but planar\nCMOS process, and ideal non-planar component shapes such as spherical imagers\nfor improved optical systems. We demonstrate these curved sensors in prototype\ncameras with custom lenses, measuring exceptional resolution of 3220\nline-widths per picture height at an aperture of f/1.2 and nearly 100% relative\nillumination across the field. Though we use a 1/2.3\" format image sensor in\nthis report, we also show this process is generally compatible with many state\nof the art imaging sensor formats. By example, we report photogrammetry test\ndata for an APS-C sized silicon die formed to a 30$^\\circ$ subtended spherical\nangle. These gains in sharpness and relative illumination enable a new\ngeneration of ultra-high performance, manufacturable, digital imaging systems\nfor scientific, industrial, and artistic use.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 22:37:17 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Guenter", "Brian", ""], ["Joshi", "Neel", ""], ["Stoakley", "Richard", ""], ["Keefe", "Andrew", ""], ["Geary", "Kevin", ""], ["Freeman", "Ryan", ""], ["Hundley", "Jake", ""], ["Patterson", "Pamela", ""], ["Hammon", "David", ""], ["Herrera", "Guillermo", ""], ["Sherman", "Elena", ""], ["Nowak", "Andrew", ""], ["Schubert", "Randall", ""], ["Brewer", "Peter", ""], ["Yang", "Louis", ""], ["Mott", "Russell", ""], ["McKnight", "Geoff", ""]]}, {"id": "1706.07145", "submitter": "Shuchang Zhou", "authors": "Shuchang Zhou, Yuzhi Wang, He Wen, Qinyao He and Yuheng Zou", "title": "Balanced Quantization: An Effective and Efficient Approach to Quantized\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantized Neural Networks (QNNs), which use low bitwidth numbers for\nrepresenting parameters and performing computations, have been proposed to\nreduce the computation complexity, storage size and memory usage. In QNNs,\nparameters and activations are uniformly quantized, such that the\nmultiplications and additions can be accelerated by bitwise operations.\nHowever, distributions of parameters in Neural Networks are often imbalanced,\nsuch that the uniform quantization determined from extremal values may under\nutilize available bitwidth. In this paper, we propose a novel quantization\nmethod that can ensure the balance of distributions of quantized values. Our\nmethod first recursively partitions the parameters by percentiles into balanced\nbins, and then applies uniform quantization. We also introduce computationally\ncheaper approximations of percentiles to reduce the computation overhead\nintroduced. Overall, our method improves the prediction accuracies of QNNs\nwithout introducing extra computation during inference, has negligible impact\non training speed, and is applicable to both Convolutional Neural Networks and\nRecurrent Neural Networks. Experiments on standard datasets including ImageNet\nand Penn Treebank confirm the effectiveness of our method. On ImageNet, the\ntop-5 error rate of our 4-bit quantized GoogLeNet model is 12.7\\%, which is\nsuperior to the state-of-the-arts of QNNs.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 01:25:37 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Zhou", "Shuchang", ""], ["Wang", "Yuzhi", ""], ["Wen", "He", ""], ["He", "Qinyao", ""], ["Zou", "Yuheng", ""]]}, {"id": "1706.07154", "submitter": "Daniel Lopez Martinez", "authors": "Daniel Lopez Martinez, Ognjen Rudovic, Rosalind Picard", "title": "Personalized Automatic Estimation of Self-reported Pain Intensity from\n  Facial Expressions", "comments": "Computer Vision and Pattern Recognition Conference, The 1st\n  International Workshop on Deep Affective Learning and Context Modeling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pain is a personal, subjective experience that is commonly evaluated through\nvisual analog scales (VAS). While this is often convenient and useful,\nautomatic pain detection systems can reduce pain score acquisition efforts in\nlarge-scale studies by estimating it directly from the participants' facial\nexpressions. In this paper, we propose a novel two-stage learning approach for\nVAS estimation: first, our algorithm employs Recurrent Neural Networks (RNNs)\nto automatically estimate Prkachin and Solomon Pain Intensity (PSPI) levels\nfrom face images. The estimated scores are then fed into the personalized\nHidden Conditional Random Fields (HCRFs), used to estimate the VAS, provided by\neach person. Personalization of the model is performed using a newly introduced\nfacial expressiveness score, unique for each person. To the best of our\nknowledge, this is the first approach to automatically estimate VAS from face\nimages. We show the benefits of the proposed personalized over traditional\nnon-personalized approach on a benchmark dataset for pain analysis from face\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 03:11:29 GMT"}, {"version": "v2", "created": "Sat, 24 Jun 2017 00:04:06 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Martinez", "Daniel Lopez", ""], ["Rudovic", "Ognjen", ""], ["Picard", "Rosalind", ""]]}, {"id": "1706.07156", "submitter": "Muhammad Huzaifah Md Shahrin", "authors": "M. Huzaifah", "title": "Comparison of Time-Frequency Representations for Environmental Sound\n  Classification using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent successful applications of convolutional neural networks (CNNs) to\naudio classification and speech recognition have motivated the search for\nbetter input representations for more efficient training. Visual displays of an\naudio signal, through various time-frequency representations such as\nspectrograms offer a rich representation of the temporal and spectral structure\nof the original signal. In this letter, we compare various popular signal\nprocessing methods to obtain this representation, such as short-time Fourier\ntransform (STFT) with linear and Mel scales, constant-Q transform (CQT) and\ncontinuous Wavelet transform (CWT), and assess their impact on the\nclassification performance of two environmental sound datasets using CNNs. This\nstudy supports the hypothesis that time-frequency representations are valuable\nin learning useful features for sound classification. Moreover, the actual\ntransformation used is shown to impact the classification accuracy, with\nMel-scaled STFT outperforming the other discussed methods slightly and baseline\nMFCC features to a large degree. Additionally, we observe that the optimal\nwindow size during transformation is dependent on the characteristics of the\naudio signal and architecturally, 2D convolution yielded better results in most\ncases compared to 1D.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 03:23:09 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Huzaifah", "M.", ""]]}, {"id": "1706.07157", "submitter": "Rongcui Dong", "authors": "Rongcui Dong (1), Haoxiang Wang (2 and 3) ((1) Viterbi School of\n  Engineering, University of Southern California, LA, USA, (2) Department of\n  ECE, Cornell University, NY, USA, (3) GoPerception Laboratory, NY, USA)", "title": "A Novel VHR Image Change Detection Algorithm Based on Image Fusion and\n  Fuzzy C-Means Clustering", "comments": "9 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis describes a study to perform change detection on Very High\nResolution satellite images using image fusion based on 2D Discrete Wavelet\nTransform and Fuzzy C-Means clustering algorithm. Multiple other methods are\nalso quantitatively and qualitatively compared in this study.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 03:25:48 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Dong", "Rongcui", "", "2 and 3"], ["Wang", "Haoxiang", "", "2 and 3"]]}, {"id": "1706.07178", "submitter": "Daigo Shoji", "authors": "Daigo Shoji, Rina Noguchi", "title": "Shape recognition of volcanic ash by simple convolutional neural network", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape analyses of tephra grains result in understanding eruption mechanism of\nvolcanoes. However, we have to define and select parameter set such as\nconvexity for the precise discrimination of tephra grains. Selection of the\nbest parameter set for the recognition of tephra shapes is complicated.\nActually, many shape parameters have been suggested. Recently, neural network\nhas made a great success in the field of machine learning. Convolutional neural\nnetwork can recognize the shape of images without human bias and shape\nparameters. We applied the simple convolutional neural network developed for\nthe handwritten digits to the recognition of tephra shapes. The network was\ntrained by Morphologi tephra images, and it can recognize the tephra shapes\nwith approximately 90% of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 06:53:19 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Shoji", "Daigo", ""], ["Noguchi", "Rina", ""]]}, {"id": "1706.07198", "submitter": "Asha V", "authors": "V. Asha", "title": "Synthesis of Near-regular Natural Textures", "comments": "5 Pages, 10 Figures, IJCRD-5(1), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture synthesis is widely used in the field of computer graphics, vision,\nand image processing. In the present paper, a texture synthesis algorithm is\nproposed for near-regular natural textures with the help of a representative\nperiodic pattern extracted from the input textures using distance matching\nfunction. Local texture statistics is then analyzed against global texture\nstatistics for non-overlapping windows of size same as periodic pattern size\nand a representative periodic pattern is extracted from the image and used for\ntexture synthesis, while preserving the global regularity and visual\nappearance. Validation of the algorithm based on experiments with synthetic\ntextures whose periodic pattern sizes are known and containing camouflages /\ndefects proves the strength of the algorithm for texture synthesis and its\napplication in detection of camouflages / defects in textures.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 07:58:20 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Asha", "V.", ""]]}, {"id": "1706.07251", "submitter": "Nannan Li", "authors": "Jingjia Huang and Nannan Li and Tao Zhang and Ge Li", "title": "A Self-Adaptive Proposal Model for Temporal Action Detection based on\n  Reinforcement Learning", "comments": "Deep Reinforcement Learning, Action Temporal Detection, Temporal\n  Location Regression", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing action detection algorithms usually generate action proposals\nthrough an extensive search over the video at multiple temporal scales, which\nbrings about huge computational overhead and deviates from the human perception\nprocedure. We argue that the process of detecting actions should be naturally\none of observation and refinement: observe the current window and refine the\nspan of attended window to cover true action regions. In this paper, we propose\nan active action proposal model that learns to find actions through\ncontinuously adjusting the temporal bounds in a self-adaptive way. The whole\nprocess can be deemed as an agent, which is firstly placed at a position in the\nvideo at random, adopts a sequence of transformations on the current attended\nregion to discover actions according to a learned policy. We utilize\nreinforcement learning, especially the Deep Q-learning algorithm to learn the\nagent's decision policy. In addition, we use temporal pooling operation to\nextract more effective feature representation for the long temporal window, and\ndesign a regression network to adjust the position offsets between predicted\nresults and the ground truth. Experiment results on THUMOS 2014 validate the\neffectiveness of the proposed approach, which can achieve competitive\nperformance with current action detection algorithms via much fewer proposals.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 10:59:33 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Huang", "Jingjia", ""], ["Li", "Nannan", ""], ["Zhang", "Tao", ""], ["Li", "Ge", ""]]}, {"id": "1706.07263", "submitter": "Geoff Jones", "authors": "Geoffrey Jones, Neil T Clancy, Xiaofei Du, Maria Robu, Simon Arridge,\n  Daniel S Elson, and Danail Stoyanov", "title": "Fast Estimation of Haemoglobin Concentration in Tissue Via Wavelet\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tissue oxygenation and perfusion can be an indicator for organ viability\nduring minimally invasive surgery, for example allowing real-time assessment of\ntissue perfusion and oxygen saturation. Multispectral imaging is an optical\nmodality that can inspect tissue perfusion in wide field images without\ncontact. In this paper, we present a novel, fast method for using RGB images\nfor MSI, which while limiting the spectral resolution of the modality allows\nnormal laparoscopic systems to be used. We exploit the discrete Haar\ndecomposition to separate individual video frames into low pass and directional\ncoefficients and we utilise a different multispectral estimation technique on\neach. The increase in speed is achieved by using fast Tikhonov regularisation\non the directional coefficients and more accurate Bayesian estimation on the\nlow pass component. The pipeline is implemented using a graphics processing\nunit (GPU) architecture and achieves a frame rate of approximately 15Hz. We\nvalidate the method on animal models and on human data captured using a da\nVinci stereo laparoscope.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 11:32:09 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Jones", "Geoffrey", ""], ["Clancy", "Neil T", ""], ["Du", "Xiaofei", ""], ["Robu", "Maria", ""], ["Arridge", "Simon", ""], ["Elson", "Daniel S", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1706.07342", "submitter": "Rahul Deo", "authors": "Jeffrey Zhang, Sravani Gajjala, Pulkit Agrawal, Geoffrey H. Tison,\n  Laura A. Hallock, Lauren Beussink-Nelson, Eugene Fan, Mandar A. Aras,\n  ChaRandle Jordan, Kirsten E. Fleischmann, Michelle Melisko, Atif Qasim,\n  Alexei Efros, Sanjiv J. Shah, Ruzena Bajcsy, Rahul C. Deo", "title": "A Computer Vision Pipeline for Automated Determination of Cardiac\n  Structure and Function and Detection of Disease by Two-Dimensional\n  Echocardiography", "comments": "9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated cardiac image interpretation has the potential to transform\nclinical practice in multiple ways including enabling low-cost serial\nassessment of cardiac function in the primary care and rural setting. We\nhypothesized that advances in computer vision could enable building a fully\nautomated, scalable analysis pipeline for echocardiogram (echo) interpretation.\nOur approach entailed: 1) preprocessing; 2) convolutional neural networks (CNN)\nfor view identification, image segmentation, and phasing of the cardiac cycle;\n3) quantification of chamber volumes and left ventricular mass; 4) particle\ntracking to compute longitudinal strain; and 5) targeted disease detection.\nCNNs accurately identified views (e.g. 99% for apical 4-chamber) and segmented\nindividual cardiac chambers. Cardiac structure measurements agreed with study\nreport values (e.g. mean absolute deviations (MAD) of 7.7 mL/kg/m2 for left\nventricular diastolic volume index, 2918 studies). We computed automated\nejection fraction and longitudinal strain measurements (within 2 cohorts),\nwhich agreed with commercial software-derived values [for ejection fraction,\nMAD=5.3%, N=3101 studies; for strain, MAD=1.5% (n=197) and 1.6% (n=110)], and\ndemonstrated applicability to serial monitoring of breast cancer patients for\ntrastuzumab cardiotoxicity. Overall, we found that, compared to manual\nmeasurements, automated measurements had superior performance across seven\ninternal consistency metrics with an average increase in the Spearman\ncorrelation coefficient of 0.05 (p=0.02). Finally, we developed disease\ndetection algorithms for hypertrophic cardiomyopathy and cardiac amyloidosis,\nwith C-statistics of 0.93 and 0.84, respectively. Our pipeline lays the\ngroundwork for using automated interpretation to support point-of-care handheld\ncardiac ultrasound and large-scale analysis of the millions of echos archived\nwithin healthcare systems.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 14:39:49 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 13:21:48 GMT"}, {"version": "v3", "created": "Sat, 1 Jul 2017 00:22:08 GMT"}, {"version": "v4", "created": "Fri, 7 Jul 2017 19:56:01 GMT"}, {"version": "v5", "created": "Tue, 31 Oct 2017 04:50:32 GMT"}, {"version": "v6", "created": "Mon, 13 Nov 2017 00:36:10 GMT"}, {"version": "v7", "created": "Fri, 12 Jan 2018 17:09:23 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Zhang", "Jeffrey", ""], ["Gajjala", "Sravani", ""], ["Agrawal", "Pulkit", ""], ["Tison", "Geoffrey H.", ""], ["Hallock", "Laura A.", ""], ["Beussink-Nelson", "Lauren", ""], ["Fan", "Eugene", ""], ["Aras", "Mandar A.", ""], ["Jordan", "ChaRandle", ""], ["Fleischmann", "Kirsten E.", ""], ["Melisko", "Michelle", ""], ["Qasim", "Atif", ""], ["Efros", "Alexei", ""], ["Shah", "Sanjiv J.", ""], ["Bajcsy", "Ruzena", ""], ["Deo", "Rahul C.", ""]]}, {"id": "1706.07346", "submitter": "Lingxi Xie", "authors": "Yuyin Zhou, Lingxi Xie, Elliot K. Fishman, Alan L. Yuille", "title": "Deep Supervision for Pancreatic Cyst Segmentation in Abdominal CT Scans", "comments": "Accepted to MICCAI 2017 (8 pages, 3 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of an organ and its cystic region is a prerequisite of\ncomputer-aided diagnosis. In this paper, we focus on pancreatic cyst\nsegmentation in abdominal CT scan. This task is important and very useful in\nclinical practice yet challenging due to the low contrast in boundary, the\nvariability in location, shape and the different stages of the pancreatic\ncancer. Inspired by the high relevance between the location of a pancreas and\nits cystic region, we introduce extra deep supervision into the segmentation\nnetwork, so that cyst segmentation can be improved with the help of relatively\neasier pancreas segmentation. Under a reasonable transformation function, our\napproach can be factorized into two stages, and each stage can be efficiently\noptimized via gradient back-propagation throughout the deep networks. We\ncollect a new dataset with 131 pathological samples, which, to the best of our\nknowledge, is the largest set for pancreatic cyst segmentation. Without human\nassistance, our approach reports a 63.44% average accuracy, measured by the\nDice-S{\\o}rensen coefficient (DSC), which is higher than the number (60.46%)\nwithout deep supervision.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 14:46:16 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Zhou", "Yuyin", ""], ["Xie", "Lingxi", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1706.07359", "submitter": "Athanasios Balomenos D.", "authors": "Athanasios D. Balomenos, Elias S. Manolakos", "title": "Reconstructing the Forest of Lineage Trees of Diverse Bacterial\n  Communities Using Bio-inspired Image Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell segmentation and tracking allow us to extract a plethora of cell\nattributes from bacterial time-lapse cell movies, thus promoting computational\nmodeling and simulation of biological processes down to the single-cell level.\nHowever, to analyze successfully complex cell movies, imaging multiple\ninteracting bacterial clones as they grow and merge to generate overcrowded\nbacterial communities with thousands of cells in the field of view,\nsegmentation results should be near perfect to warrant good tracking results.\nWe introduce here a fully automated closed-loop bio-inspired computational\nstrategy that exploits prior knowledge about the expected structure of a\ncolony's lineage tree to locate and correct segmentation errors in analyzed\nmovie frames. We show that this correction strategy is effective, resulting in\nimproved cell tracking and consequently trustworthy deep colony lineage trees.\nOur image analysis approach has the unique capability to keep tracking cells\neven after clonal subpopulations merge in the movie. This enables the\nreconstruction of the complete Forest of Lineage Trees (FLT) representation of\nevolving multi-clonal bacterial communities. Moreover, the percentage of valid\ncell trajectories extracted from the image analysis almost doubles after\nsegmentation correction. This plethora of trustworthy data extracted from a\ncomplex cell movie analysis enables single-cell analytics as a tool for\naddressing compelling questions for human health, such as understanding the\nrole of single-cell stochasticity in antibiotics resistance without losing site\nof the inter-cellular interactions and microenvironment effects that may shape\nit.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 15:14:08 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Balomenos", "Athanasios D.", ""], ["Manolakos", "Elias S.", ""]]}, {"id": "1706.07362", "submitter": "Athanasios Balomenos D.", "authors": "Athanasios D. Balomenos, Panagiotis Tsakanikas, and Elias S. Manolakos", "title": "Tracking Single-Cells in Overcrowded Bacterial Colonies", "comments": null, "journal-ref": "37th Annual International Conference of the IEEE Engineering in\n  Medicine and Biology Society (EMBC), 6473-6476 (2015)", "doi": "10.1109/EMBC.2015.7319875", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell tracking enables data extraction from time-lapse \"cell movies\" and\npromotes modeling biological processes at the single-cell level. We introduce a\nnew fully automated computational strategy to track accurately cells across\nframes in time-lapse movies. Our method is based on a dynamic neighborhoods\nformation and matching approach, inspired by motion estimation algorithms for\nvideo compression. Moreover, it exploits \"divide and conquer\" opportunities to\nsolve effectively the challenging cells tracking problem in overcrowded\nbacterial colonies. Using cell movies generated by different labs we\ndemonstrate that the accuracy of the proposed method remains very high (exceeds\n97%) even when analyzing large overcrowded microbial colonies.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 15:19:06 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Balomenos", "Athanasios D.", ""], ["Tsakanikas", "Panagiotis", ""], ["Manolakos", "Elias S.", ""]]}, {"id": "1706.07365", "submitter": "Alejandro Newell", "authors": "Alejandro Newell, Jia Deng", "title": "Pixels to Graphs by Associative Embedding", "comments": "Updated numbers. Code and pretrained models available at\n  https://github.com/umich-vl/px2graph", "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS 2017)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are a useful abstraction of image content. Not only can graphs\nrepresent details about individual objects in a scene but they can capture the\ninteractions between pairs of objects. We present a method for training a\nconvolutional neural network such that it takes in an input image and produces\na full graph definition. This is done end-to-end in a single stage with the use\nof associative embeddings. The network learns to simultaneously identify all of\nthe elements that make up a graph and piece them together. We benchmark on the\nVisual Genome dataset, and demonstrate state-of-the-art performance on the\nchallenging task of scene graph generation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 15:20:25 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 17:13:31 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Newell", "Alejandro", ""], ["Deng", "Jia", ""]]}, {"id": "1706.07397", "submitter": "Ting Sun", "authors": "Ting Sun, Lin Sun, Dit-Yan Yeung", "title": "Fine-Grained Categorization via CNN-Based Automatic Extraction and\n  Integration of Object-Level and Part-Level Features", "comments": "45 pages, 20 figures, accepted by Image and Vision Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained categorization can benefit from part-based features which reveal\nsubtle visual differences between object categories. Handcrafted features have\nbeen widely used for part detection and classification. Although a recent trend\nseeks to learn such features automatically using powerful deep learning models\nsuch as convolutional neural networks (CNN), their training and possibly also\ntesting require manually provided annotations which are costly to obtain. To\nrelax these requirements, we assume in this study a general problem setting in\nwhich the raw images are only provided with object-level class labels for model\ntraining with no other side information needed. Specifically, by extracting and\ninterpreting the hierarchical hidden layer features learned by a CNN, we\npropose an elaborate CNN-based system for fine-grained categorization. When\nevaluated on the Caltech-UCSD Birds-200-2011, FGVC-Aircraft, Cars and Stanford\ndogs datasets under the setting that only object-level class labels are used\nfor training and no other annotations are available for both training and\ntesting, our method achieves impressive performance that is superior or\ncomparable to the state of the art. Moreover, it sheds some light on ingenious\nuse of the hierarchical features learned by CNN which has wide applicability\nwell beyond the current fine-grained categorization task.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 16:59:16 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Sun", "Ting", ""], ["Sun", "Lin", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1706.07422", "submitter": "Nitin Khanna Dr.", "authors": "Sharad Joshi, Nitin Khanna", "title": "Single Classifier-based Passive System for Source Printer Classification\n  using Local Texture Features", "comments": "11 pages", "journal-ref": null, "doi": "10.1109/TIFS.2017.2779441", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important aspect of examining printed documents for potential forgeries\nand copyright infringement is the identification of source printer as it can be\nhelpful for ascertaining the leak and detecting forged documents. This paper\nproposes a system for classification of source printer from scanned images of\nprinted documents using all the printed letters simultaneously. This system\nuses local texture patterns based features and a single classifier for\nclassifying all the printed letters. Letters are extracted from scanned images\nusing connected component analysis followed by morphological filtering without\nthe need of using an OCR. Each letter is sub-divided into a flat region and an\nedge region, and local tetra patterns are estimated separately for these two\nregions. A strategically constructed pooling technique is used to extract the\nfinal feature vectors. The proposed method has been tested on both a publicly\navailable dataset of 10 printers and a new dataset of 18 printers scanned at a\nresolution of 600 dpi as well as 300 dpi printed in four different fonts. The\nresults indicate shape independence property in the proposed method as using a\nsingle classifier it outperforms existing handcrafted feature-based methods and\nneeds much smaller number of training pages by using all the printed letters.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 17:53:57 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Joshi", "Sharad", ""], ["Khanna", "Nitin", ""]]}, {"id": "1706.07446", "submitter": "Daniel George", "authors": "Daniel George, Hongyu Shen, E. A. Huerta", "title": "Deep Transfer Learning: A new deep learning glitch classification method\n  for advanced LIGO", "comments": null, "journal-ref": null, "doi": "10.1103/PhysRevD.97.101501", "report-no": null, "categories": "gr-qc astro-ph.IM cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exquisite sensitivity of the advanced LIGO detectors has enabled the\ndetection of multiple gravitational wave signals. The sophisticated design of\nthese detectors mitigates the effect of most types of noise. However, advanced\nLIGO data streams are contaminated by numerous artifacts known as glitches:\nnon-Gaussian noise transients with complex morphologies. Given their high rate\nof occurrence, glitches can lead to false coincident detections, obscure and\neven mimic gravitational wave signals. Therefore, successfully characterizing\nand removing glitches from advanced LIGO data is of utmost importance. Here, we\npresent the first application of Deep Transfer Learning for glitch\nclassification, showing that knowledge from deep learning algorithms trained\nfor real-world object recognition can be transferred for classifying glitches\nin time-series based on their spectrogram images. Using the Gravity Spy\ndataset, containing hand-labeled, multi-duration spectrograms obtained from\nreal LIGO data, we demonstrate that this method enables optimal use of very\ndeep convolutional neural networks for classification given small training\ndatasets, significantly reduces the time for training the networks, and\nachieves state-of-the-art accuracy above 98.8%, with perfect precision-recall\non 8 out of 22 classes. Furthermore, new types of glitches can be classified\naccurately given few labeled examples with this technique. Once trained via\ntransfer learning, we show that the convolutional neural networks can be\ntruncated and used as excellent feature extractors for unsupervised clustering\nmethods to identify new classes based on their morphology, without any labeled\nexamples. Therefore, this provides a new framework for dynamic glitch\nclassification for gravitational wave detectors, which are expected to\nencounter new types of noise as they undergo gradual improvements to attain\ndesign sensitivity.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 18:11:13 GMT"}], "update_date": "2018-07-15", "authors_parsed": [["George", "Daniel", ""], ["Shen", "Hongyu", ""], ["Huerta", "E. A.", ""]]}, {"id": "1706.07457", "submitter": "Chong Sun", "authors": "Chong Sun, Dong Wang, Huchuan Lu, Ming-Hsuan Yang", "title": "Learning Spatial-Aware Regressions for Visual Tracking", "comments": "To appear in CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the spatial information of deep features, and\npropose two complementary regressions for robust visual tracking. First, we\npropose a kernelized ridge regression model wherein the kernel value is defined\nas the weighted sum of similarity scores of all pairs of patches between two\nsamples. We show that this model can be formulated as a neural network and thus\ncan be efficiently solved. Second, we propose a fully convolutional neural\nnetwork with spatially regularized kernels, through which the filter kernel\ncorresponding to each output channel is forced to focus on a specific region of\nthe target. Distance transform pooling is further exploited to determine the\neffectiveness of each output channel of the convolution layer. The outputs from\nthe kernelized ridge regression model and the fully convolutional neural\nnetwork are combined to obtain the ultimate response. Experimental results on\ntwo benchmark datasets validate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 18:55:35 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 12:16:40 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Sun", "Chong", ""], ["Wang", "Dong", ""], ["Lu", "Huchuan", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1706.07507", "submitter": "E. M. de la Calleja Mora", "authors": "Jorge de la Calleja, Elsa M. de la Calleja, Hugo Jair Escalante", "title": "Fractal dimension analysis for automatic morphological galaxy\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we present experimental results using\n\\emph{Haussdorf-Besicovich} fractal dimension for performing morphological\ngalaxy classification. The fractal dimension is a topological, structural and\nspatial property that give us information about the space were an object lives.\nWe have calculated the fractal dimension value of the main types of galaxies:\nellipticals, spirals and irregulars; and we use it as a feature for classifying\nthem. Also, we have performed an image analysis process in order to standardize\nthe galaxy images, and we have used principal component analysis to obtain the\nmain attributes in the images. Galaxy classification was performed using\nmachine learning algorithms: C4.5, k-nearest neighbors, random forest and\nsupport vector machines. Preliminary experimental results using 10-fold\ncross-validation show that fractal dimension helps to improve classification,\nwith over 88 per cent accuracy for elliptical galaxies, 100 per cent accuracy\nfor spiral galaxies and over 40 per cent for irregular galaxies.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 22:17:26 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["de la Calleja", "Jorge", ""], ["de la Calleja", "Elsa M.", ""], ["Escalante", "Hugo Jair", ""]]}, {"id": "1706.07515", "submitter": "Denis Parra", "authors": "Vicente Dominguez and Pablo Messina and Denis Parra and Domingo Mery\n  and Christoph Trattner and Alvaro Soto", "title": "Comparing Neural and Attractiveness-based Visual Features for Artwork\n  Recommendation", "comments": "DLRS 2017 workshop, co-located at RecSys 2017", "journal-ref": null, "doi": "10.1145/3125486.3125495", "report-no": null, "categories": "cs.IR cs.AI cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in image processing and computer vision in the latest years have\nbrought about the use of visual features in artwork recommendation. Recent\nworks have shown that visual features obtained from pre-trained deep neural\nnetworks (DNNs) perform very well for recommending digital art. Other recent\nworks have shown that explicit visual features (EVF) based on attractiveness\ncan perform well in preference prediction tasks, but no previous work has\ncompared DNN features versus specific attractiveness-based visual features\n(e.g. brightness, texture) in terms of recommendation performance. In this\nwork, we study and compare the performance of DNN and EVF features for the\npurpose of physical artwork recommendation using transactional data from\nUGallery, an online store of physical paintings. In addition, we perform an\nexploratory analysis to understand if DNN embedded features have some relation\nwith certain EVF. Our results show that DNN features outperform EVF, that\ncertain EVF features are more suited for physical artwork recommendation and,\nfinally, we show evidence that certain neurons in the DNN might be partially\nencoding visual features such as brightness, providing an opportunity for\nexplaining recommendations based on visual neural models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 22:48:48 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 22:17:48 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Dominguez", "Vicente", ""], ["Messina", "Pablo", ""], ["Parra", "Denis", ""], ["Mery", "Domingo", ""], ["Trattner", "Christoph", ""], ["Soto", "Alvaro", ""]]}, {"id": "1706.07522", "submitter": "Hemanth Venkateswara", "authors": "Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, Sethuraman\n  Panchanathan", "title": "Deep Hashing Network for Unsupervised Domain Adaptation", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks have emerged as a dominant machine\nlearning tool for a wide variety of application domains. However, training a\ndeep neural network requires a large amount of labeled data, which is an\nexpensive process in terms of time, labor and human expertise. Domain\nadaptation or transfer learning algorithms address this challenge by leveraging\nlabeled data in a different, but related source domain, to develop a model for\nthe target domain. Further, the explosive growth of digital data has posed a\nfundamental challenge concerning its storage and retrieval. Due to its storage\nand retrieval efficiency, recent years have witnessed a wide application of\nhashing in a variety of computer vision applications. In this paper, we first\nintroduce a new dataset, Office-Home, to evaluate domain adaptation algorithms.\nThe dataset contains images of a variety of everyday objects from multiple\ndomains. We then propose a novel deep learning framework that can exploit\nlabeled source data and unlabeled target data to learn informative hash codes,\nto accurately classify unseen target data. To the best of our knowledge, this\nis the first research effort to exploit the feature learning capabilities of\ndeep neural networks to learn representative hash codes to address the domain\nadaptation problem. Our extensive empirical studies on multiple transfer tasks\ncorroborate the usefulness of the framework in learning efficient hash codes\nwhich outperform existing competitive baselines for unsupervised domain\nadaptation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 23:15:10 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Venkateswara", "Hemanth", ""], ["Eusebio", "Jose", ""], ["Chakraborty", "Shayok", ""], ["Panchanathan", "Sethuraman", ""]]}, {"id": "1706.07524", "submitter": "Hemanth Venkateswara", "authors": "Hemanth Venkateswara, Shayok Chakraborty, Sethuraman Panchanathan", "title": "Nonlinear Embedding Transform for Unsupervised Domain Adaptation", "comments": "ECCV Workshops 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of domain adaptation (DA) deals with adapting classifier models\ntrained on one data distribution to different data distributions. In this\npaper, we introduce the Nonlinear Embedding Transform (NET) for unsupervised DA\nby combining domain alignment along with similarity-based embedding. We also\nintroduce a validation procedure to estimate the model parameters for the NET\nalgorithm using the source data. Comprehensive evaluations on multiple vision\ndatasets demonstrate that the NET algorithm outperforms existing competitive\nprocedures for unsupervised DA.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 23:42:27 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Venkateswara", "Hemanth", ""], ["Chakraborty", "Shayok", ""], ["Panchanathan", "Sethuraman", ""]]}, {"id": "1706.07525", "submitter": "Hemanth Venkateswara", "authors": "Hemanth Venkateswara, Prasanth Lade, Jieping Ye, Sethuraman\n  Panchanathan", "title": "Coupled Support Vector Machines for Supervised Domain Adaptation", "comments": "ACM Multimedia Conference 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular domain adaptation (DA) techniques learn a classifier for the target\ndomain by sampling relevant data points from the source and combining it with\nthe target data. We present a Support Vector Machine (SVM) based supervised DA\ntechnique, where the similarity between source and target domains is modeled as\nthe similarity between their SVM decision boundaries. We couple the source and\ntarget SVMs and reduce the model to a standard single SVM. We test the\nCoupled-SVM on multiple datasets and compare our results with other popular SVM\nbased DA approaches.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 23:53:09 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Venkateswara", "Hemanth", ""], ["Lade", "Prasanth", ""], ["Ye", "Jieping", ""], ["Panchanathan", "Sethuraman", ""]]}, {"id": "1706.07530", "submitter": "Hemanth Venkateswara", "authors": "Hemanth Venkateswara, Vineeth N. Balasubramanian, Prasanth Lade,\n  Sethuraman Panchanathan", "title": "Multiresolution Match Kernels for Gesture Video Classification", "comments": "ICME 2013 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of depth imaging technologies like the Microsoft Kinect has\nrenewed interest in computational methods for gesture classification based on\nvideos. For several years now, researchers have used the Bag-of-Features (BoF)\nas a primary method for generation of feature vectors from video data for\nrecognition of gestures. However, the BoF method is a coarse representation of\nthe information in a video, which often leads to poor similarity measures\nbetween videos. Besides, when features extracted from different spatio-temporal\nlocations in the video are pooled to create histogram vectors in the BoF\nmethod, there is an intrinsic loss of their original locations in space and\ntime. In this paper, we propose a new Multiresolution Match Kernel (MMK) for\nvideo classification, which can be considered as a generalization of the BoF\nmethod. We apply this procedure to hand gesture classification based on RGB-D\nvideos of the American Sign Language(ASL) hand gestures and our results show\npromise and usefulness of this new method.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 00:23:32 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Venkateswara", "Hemanth", ""], ["Balasubramanian", "Vineeth N.", ""], ["Lade", "Prasanth", ""], ["Panchanathan", "Sethuraman", ""]]}, {"id": "1706.07536", "submitter": "Zibo Meng", "authors": "Zibo Meng, Shizhong Han, Yan Tong", "title": "Listen to Your Face: Inferring Facial Action Units from Audio Channel", "comments": "Accepted to IEEE Transactions on Affective Computing (TAFFC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensive efforts have been devoted to recognizing facial action units (AUs).\nHowever, it is still challenging to recognize AUs from spontaneous facial\ndisplays especially when they are accompanied with speech. Different from all\nprior work that utilized visual observations for facial AU recognition, this\npaper presents a novel approach that recognizes speech-related AUs exclusively\nfrom audio signals based on the fact that facial activities are highly\ncorrelated with voice during speech. Specifically, dynamic and physiological\nrelationships between AUs and phonemes are modeled through a continuous time\nBayesian network (CTBN); then AU recognition is performed by probabilistic\ninference via the CTBN model.\n  A pilot audiovisual AU-coded database has been constructed to evaluate the\nproposed audio-based AU recognition framework. The database consists of a\n\"clean\" subset with frontal and neutral faces and a challenging subset\ncollected with large head movements and occlusions. Experimental results on\nthis database show that the proposed CTBN model achieves promising recognition\nperformance for 7 speech-related AUs and outperforms the state-of-the-art\nvisual-based methods especially for those AUs that are activated at low\nintensities or \"hardly visible\" in the visual channel. Furthermore, the CTBN\nmodel yields more impressive recognition performance on the challenging subset,\nwhere the visual-based approaches suffer significantly.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 01:22:21 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 14:27:00 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Meng", "Zibo", ""], ["Han", "Shizhong", ""], ["Tong", "Yan", ""]]}, {"id": "1706.07567", "submitter": "Chao-Yuan Wu", "authors": "Chao-Yuan Wu, R. Manmatha, Alexander J. Smola, Philipp Kr\\\"ahenb\\\"uhl", "title": "Sampling Matters in Deep Embedding Learning", "comments": "Add supplementary material. Paper published in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep embeddings answer one simple question: How similar are two images?\nLearning these embeddings is the bedrock of verification, zero-shot learning,\nand visual search. The most prominent approaches optimize a deep convolutional\nnetwork with a suitable loss function, such as contrastive loss or triplet\nloss. While a rich line of work focuses solely on the loss functions, we show\nin this paper that selecting training examples plays an equally important role.\nWe propose distance weighted sampling, which selects more informative and\nstable examples than traditional approaches. In addition, we show that a simple\nmargin based loss is sufficient to outperform all other loss functions. We\nevaluate our approach on the Stanford Online Products, CAR196, and the\nCUB200-2011 datasets for image retrieval and clustering, and on the LFW dataset\nfor face verification. Our method achieves state-of-the-art performance on all\nof them.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 05:14:55 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 16:54:27 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Wu", "Chao-Yuan", ""], ["Manmatha", "R.", ""], ["Smola", "Alexander J.", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "1706.07593", "submitter": "Andrew Spek", "authors": "Thanuja Dharmasiri, Andrew Spek, Tom Drummond", "title": "Joint Prediction of Depths, Normals and Surface Curvature from RGB\n  Images using CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the 3D structure of a scene is of vital importance, when it\ncomes to developing fully autonomous robots. To this end, we present a novel\ndeep learning based framework that estimates depth, surface normals and surface\ncurvature by only using a single RGB image. To the best of our knowledge this\nis the first work to estimate surface curvature from colour using a machine\nlearning approach. Additionally, we demonstrate that by tuning the network to\ninfer well designed features, such as surface curvature, we can achieve\nimproved performance at estimating depth and normals.This indicates that\nnetwork guidance is still a useful aspect of designing and training a neural\nnetwork. We run extensive experiments where the network is trained to infer\ndifferent tasks while the model capacity is kept constant resulting in\ndifferent feature maps based on the tasks at hand. We outperform the previous\nstate-of-the-art benchmarks which jointly estimate depths and surface normals\nwhile predicting surface curvature in parallel.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 08:18:44 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Dharmasiri", "Thanuja", ""], ["Spek", "Andrew", ""], ["Drummond", "Tom", ""]]}, {"id": "1706.07649", "submitter": "Jan Egger", "authors": "Xiaojun Chen, Lu Xu, Xing Li, Jan Egger", "title": "Computer-aided implant design for the restoration of cranial defects", "comments": "10 pages, 12 figures, 20 References", "journal-ref": "Scientific Reports 7, Article number: 4199 (2017)", "doi": "10.1038/s41598-017-04454-6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patient-specific cranial implants are important and necessary in the surgery\nof cranial defect restoration. However, traditional methods of manual design of\ncranial implants are complicated and time-consuming. Our purpose is to develop\na novel software named EasyCrania to design the cranial implants conveniently\nand efficiently. The process can be divided into five steps, which are\nmirroring model, clipping surface, surface fitting, the generation of the\ninitial implant and the generation of the final implant. The main concept of\nour method is to use the geometry information of the mirrored model as the base\nto generate the final implant. The comparative studies demonstrated that the\nEasyCrania can improve the efficiency of cranial implant design significantly.\nAnd, the intra- and inter-rater reliability of the software were stable, which\nwere 87.07+/-1.6% and 87.73+/-1.4% respectively.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 11:55:46 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Chen", "Xiaojun", ""], ["Xu", "Lu", ""], ["Li", "Xing", ""], ["Egger", "Jan", ""]]}, {"id": "1706.07680", "submitter": "Moin Nabi", "authors": "Mahdyar Ravanbakhsh, Enver Sangineto, Moin Nabi, Nicu Sebe", "title": "Training Adversarial Discriminators for Cross-channel Abnormal Event\n  Detection in Crowds", "comments": "To appear at WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abnormal crowd behaviour detection attracts a large interest due to its\nimportance in video surveillance scenarios. However, the ambiguity and the lack\nof sufficient abnormal ground truth data makes end-to-end training of large\ndeep networks hard in this domain. In this paper we propose to use Generative\nAdversarial Nets (GANs), which are trained to generate only the normal\ndistribution of the data. During the adversarial GAN training, a discriminator\n(D) is used as a supervisor for the generator network (G) and vice versa. At\ntesting time we use D to solve our discriminative task (abnormality detection),\nwhere D has been trained without the need of manually-annotated abnormal data.\nMoreover, in order to prevent G learn a trivial identity function, we use a\ncross-channel approach, forcing G to transform raw-pixel data in motion\ninformation and vice versa. The quantitative results on standard benchmarks\nshow that our method outperforms previous state-of-the-art methods in both the\nframe-level and the pixel-level evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 13:06:33 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 23:54:56 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Ravanbakhsh", "Mahdyar", ""], ["Sangineto", "Enver", ""], ["Nabi", "Moin", ""], ["Sebe", "Nicu", ""]]}, {"id": "1706.07717", "submitter": "Nati Ofir", "authors": "Nati Ofir, Meirav Galun, Sharon Alpert, Achi Brandt, Boaz Nadler,\n  Ronen Basri", "title": "On Detection of Faint Edges in Noisy Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental question for edge detection in noisy images is how faint can an\nedge be and still be detected. In this paper we offer a formalism to study this\nquestion and subsequently introduce computationally efficient multiscale edge\ndetection algorithms designed to detect faint edges in noisy images. In our\nformalism we view edge detection as a search in a discrete, though potentially\nlarge, set of feasible curves. First, we derive approximate expressions for the\ndetection threshold as a function of curve length and the complexity of the\nsearch space. We then present two edge detection algorithms, one for straight\nedges, and the second for curved ones. Both algorithms efficiently search for\nedges in a large set of candidates by hierarchically constructing difference\nfilters that match the curves traced by the sought edges. We demonstrate the\nutility of our algorithms in both simulations and applications involving\nchallenging real images. Finally, based on these principles, we develop an\nalgorithm for fiber detection and enhancement. We exemplify its utility to\nreveal and enhance nerve axons in light microscopy images.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 16:37:22 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Ofir", "Nati", ""], ["Galun", "Meirav", ""], ["Alpert", "Sharon", ""], ["Brandt", "Achi", ""], ["Nadler", "Boaz", ""], ["Basri", "Ronen", ""]]}, {"id": "1706.07757", "submitter": "Mehdi Ghayoumi", "authors": "Mehdi Ghayoumi and Arvind Bansal", "title": "Improved Human Emotion Recognition Using Symmetry of Facial Key Points\n  with Dihedral Group", "comments": "7", "journal-ref": "IJASCSE Volume 6 Issue 01 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes how to deploy dihedral group theory to detect Facial\nKey Points (FKP) symmetry to recognize emotions. The method can be applied in\nmany other areas which those have the same data texture.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 14:54:14 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Ghayoumi", "Mehdi", ""], ["Bansal", "Arvind", ""]]}, {"id": "1706.07841", "submitter": "Madhuri Suthar", "authors": "Bahram Jalali, Madhuri Suthar, Mohamad Asghari and Ata Mahjoubfar", "title": "Time Stretch Inspired Computational Imaging", "comments": "This work has been published in the PHOTOPTICS 2017 - 5th\n  International Conference on Photonics, Optics and Laser Technology, Volume 1,\n  ISBN 978-989-758-223-3, pages 340-345", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an physics.optics stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that dispersive propagation of light followed by phase detection has\nproperties that can be exploited for extracting features from the waveforms.\nThis discovery is spearheading development of a new class of physics-inspired\nalgorithms for feature extraction from digital images with unique properties\nand superior dynamic range compared to conventional algorithms. In certain\ncases, these algorithms have the potential to be an energy efficient and\nscalable substitute to synthetically fashioned computational techniques in\npractice today.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 12:41:50 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Jalali", "Bahram", ""], ["Suthar", "Madhuri", ""], ["Asghari", "Mohamad", ""], ["Mahjoubfar", "Ata", ""]]}, {"id": "1706.07842", "submitter": "Yaqi Liu", "authors": "Yaqi Liu, Qingxiao Guan, Xianfeng Zhao, and Yun Cao", "title": "Image Forgery Localization Based on Multi-Scale Convolutional Neural\n  Networks", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": "10.1109/TGRS.2018.2848473", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to utilize Convolutional Neural Networks (CNNs) and\nthe segmentation-based multi-scale analysis to locate tampered areas in digital\nimages. First, to deal with color input sliding windows of different scales, a\nunified CNN architecture is designed. Then, we elaborately design the training\nprocedures of CNNs on sampled training patches. With a set of robust\nmulti-scale tampering detectors based on CNNs, complementary tampering\npossibility maps can be generated. Last but not least, a segmentation-based\nmethod is proposed to fuse the maps and generate the final decision map. By\nexploiting the benefits of both the small-scale and large-scale analyses, the\nsegmentation-based multi-scale analysis can lead to a performance leap in\nforgery localization of CNNs. Numerous experiments are conducted to demonstrate\nthe effectiveness and efficiency of our method.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 09:40:27 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 01:27:45 GMT"}, {"version": "v3", "created": "Mon, 16 Oct 2017 11:11:24 GMT"}, {"version": "v4", "created": "Wed, 7 Feb 2018 10:45:45 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Liu", "Yaqi", ""], ["Guan", "Qingxiao", ""], ["Zhao", "Xianfeng", ""], ["Cao", "Yun", ""]]}, {"id": "1706.07886", "submitter": "Mohammed Fathy", "authors": "Mohammed E. Fathy, Ashraf S. Hussein, Mohammed F. Tolba", "title": "Fundamental Matrix Estimation: A Study of Error Criteria", "comments": "15 pages, 7 figures, Pattern Recognition Letters, 2011", "journal-ref": null, "doi": "10.1016/j.patrec.2010.09.019", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental matrix (FM) describes the geometric relations that exist\nbetween two images of the same scene. Different error criteria are used for\nestimating FMs from an input set of correspondences. In this paper, the\naccuracy and efficiency aspects of the different error criteria were studied.\nWe mathematically and experimentally proved that the most popular error\ncriterion, the symmetric epipolar distance, is biased. It was also shown that\ndespite the similarity between the algebraic expressions of the symmetric\nepipolar distance and Sampson distance, they have different accuracy\nproperties. In addition, a new error criterion, Kanatani distance, was proposed\nand was proved to be the most effective for use during the outlier removal\nphase from accuracy and efficiency perspectives. To thoroughly test the\naccuracy of the different error criteria, we proposed a randomized algorithm\nfor Reprojection Error-based Correspondence Generation (RE-CG). As input, RE-CG\ntakes an FM and a desired reprojection error value $d$. As output, RE-CG\ngenerates a random correspondence having that error value. Mathematical\nanalysis of this algorithm revealed that the success probability for any given\ntrial is 1 - (2/3)^2 at best and is 1 - (6/7)^2 at worst while experiments\ndemonstrated that the algorithm often succeeds after only one trial.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 00:13:29 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Fathy", "Mohammed E.", ""], ["Hussein", "Ashraf S.", ""], ["Tolba", "Mohammed F.", ""]]}, {"id": "1706.07888", "submitter": "Sam Kriegman", "authors": "Sam Kriegman, Marcin Szubert, Josh C. Bongard, Christian Skalka", "title": "Evolving Spatially Aggregated Features from Satellite Imagery for\n  Regional Modeling", "comments": null, "journal-ref": "Parallel Problem Solving from Nature - PPSN XIV. PPSN 2016.\n  Lecture Notes in Computer Science, vol 9921. Springer, Cham", "doi": "10.1007/978-3-319-45823-6_66", "report-no": null, "categories": "stat.ML cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satellite imagery and remote sensing provide explanatory variables at\nrelatively high resolutions for modeling geospatial phenomena, yet regional\nsummaries are often desirable for analysis and actionable insight. In this\npaper, we propose a novel method of inducing spatial aggregations as a\ncomponent of the machine learning process, yielding regional model features\nwhose construction is driven by model prediction performance rather than prior\nassumptions. Our results demonstrate that Genetic Programming is particularly\nwell suited to this type of feature construction because it can automatically\nsynthesize appropriate aggregations, as well as better incorporate them into\npredictive models compared to other regression methods we tested. In our\nexperiments we consider a specific problem instance and real-world dataset\nrelevant to predicting snow properties in high-mountain Asia.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 01:25:12 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 17:42:18 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Kriegman", "Sam", ""], ["Szubert", "Marcin", ""], ["Bongard", "Josh C.", ""], ["Skalka", "Christian", ""]]}, {"id": "1706.07901", "submitter": "Wei Zhang", "authors": "Tianyi Zhao, Jun Yu, Zhenzhong Kuang, Wei Zhang, Jianping Fan", "title": "Deep Mixture of Diverse Experts for Large-Scale Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a deep mixture of diverse experts algorithm is developed for\nseamlessly combining a set of base deep CNNs (convolutional neural networks)\nwith diverse outputs (task spaces), e.g., such base deep CNNs are trained to\nrecognize different subsets of tens of thousands of atomic object classes.\nFirst, a two-layer (category layer and object class layer) ontology is\nconstructed to achieve more effective solution for task group generation, e.g.,\nassigning the semantically-related atomic object classes at the sibling leaf\nnodes into the same task group because they may share similar learning\ncomplexities. Second, one particular base deep CNNs with $M+1$ ($M \\leq 1,000$)\noutputs is learned for each task group to recognize its $M$ atomic object\nclasses effectively and identify one special class of \"not-in-group\"\nautomatically, and the network structure (numbers of layers and units in each\nlayer) of the well-designed AlexNet is directly used to configure such base\ndeep CNNs. A deep multi-task learning algorithm is developed to leverage the\ninter-class visual similarities to learn more discriminative base deep CNNs and\nmulti-task softmax for enhancing the separability of the atomic object classes\nin the same task group. Finally, all these base deep CNNs with diverse outputs\n(task spaces) are seamlessly combined to form a deep mixture of diverse experts\nfor recognizing tens of thousands of atomic object classes. Our experimental\nresults have demonstrated that our deep mixture of diverse experts algorithm\ncan achieve very competitive results on large-scale visual recognition.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 03:41:37 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Zhao", "Tianyi", ""], ["Yu", "Jun", ""], ["Kuang", "Zhenzhong", ""], ["Zhang", "Wei", ""], ["Fan", "Jianping", ""]]}, {"id": "1706.07911", "submitter": "Yi Zhu", "authors": "Yi Zhu, Sen Liu, Shawn Newsam", "title": "Large-Scale Mapping of Human Activity using Geo-Tagged Videos", "comments": "Accepted at ACM SIGSPATIAL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is the first work to perform spatio-temporal mapping of human\nactivity using the visual content of geo-tagged videos. We utilize a recent\ndeep-learning based video analysis framework, termed hidden two-stream\nnetworks, to recognize a range of activities in YouTube videos. This framework\nis efficient and can run in real time or faster which is important for\nrecognizing events as they occur in streaming video or for reducing latency in\nanalyzing already captured video. This is, in turn, important for using video\nin smart-city applications. We perform a series of experiments to show our\napproach is able to accurately map activities both spatially and temporally. We\nalso demonstrate the advantages of using the visual content over the\ntags/titles.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 05:59:37 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 18:24:56 GMT"}, {"version": "v3", "created": "Tue, 28 Nov 2017 19:05:37 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Zhu", "Yi", ""], ["Liu", "Sen", ""], ["Newsam", "Shawn", ""]]}, {"id": "1706.07929", "submitter": "Jian Zhang", "authors": "Jian Zhang, Bernard Ghanem", "title": "ISTA-Net: Interpretable Optimization-Inspired Deep Network for Image\n  Compressive Sensing", "comments": "10 pages, 6 figures, 4 Tables. To appear in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the aim of developing a fast yet accurate algorithm for compressive\nsensing (CS) reconstruction of natural images, we combine in this paper the\nmerits of two existing categories of CS methods: the structure insights of\ntraditional optimization-based methods and the speed of recent network-based\nones. Specifically, we propose a novel structured deep network, dubbed\nISTA-Net, which is inspired by the Iterative Shrinkage-Thresholding Algorithm\n(ISTA) for optimizing a general $\\ell_1$ norm CS reconstruction model. To cast\nISTA into deep network form, we develop an effective strategy to solve the\nproximal mapping associated with the sparsity-inducing regularizer using\nnonlinear transforms. All the parameters in ISTA-Net (\\eg nonlinear transforms,\nshrinkage thresholds, step sizes, etc.) are learned end-to-end, rather than\nbeing hand-crafted. Moreover, considering that the residuals of natural images\nare more compressible, an enhanced version of ISTA-Net in the residual domain,\ndubbed {ISTA-Net}$^+$, is derived to further improve CS reconstruction.\nExtensive CS experiments demonstrate that the proposed ISTA-Nets outperform\nexisting state-of-the-art optimization-based and network-based CS methods by\nlarge margins, while maintaining fast computational speed. Our source codes are\navailable: \\textsl{http://jianzhang.tech/projects/ISTA-Net}.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 09:02:21 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 09:24:34 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Zhang", "Jian", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1706.07960", "submitter": "Seil Na", "authors": "Seil Na, Youngjae Yu, Sangho Lee, Jisung Kim, Gunhee Kim", "title": "Encoding Video and Label Priors for Multi-label Video Classification on\n  YouTube-8M dataset", "comments": "accepted at Youtube-8M CVPR'17 Workshop as Oral Presentation. Kaggle\n  8th model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  YouTube-8M is the largest video dataset for multi-label video classification.\nIn order to tackle the multi-label classification on this challenging dataset,\nit is necessary to solve several issues such as temporal modeling of videos,\nlabel imbalances, and correlations between labels. We develop a deep neural\nnetwork model, which consists of four components: the frame encoder, the\nclassification layer, the label processing layer, and the loss function. We\nintroduce our newly proposed methods and discusses how existing models operate\nin the YouTube-8M Classification Task, what insights they have, and why they\nsucceed (or fail) to achieve good performance. Most of the models we proposed\nare very high compared to the baseline models, and the ensemble of the models\nwe used is 8th in the Kaggle Competition.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 13:50:41 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 05:33:50 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Na", "Seil", ""], ["Yu", "Youngjae", ""], ["Lee", "Sangho", ""], ["Kim", "Jisung", ""], ["Kim", "Gunhee", ""]]}, {"id": "1706.07966", "submitter": "Jiabin Ma", "authors": "Jiabin Ma, Wei Wang, Liang Wang", "title": "Irregular Convolutional Neural Networks", "comments": "7 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional kernels are basic and vital components of deep Convolutional\nNeural Networks (CNN). In this paper, we equip convolutional kernels with shape\nattributes to generate the deep Irregular Convolutional Neural Networks (ICNN).\nCompared to traditional CNN applying regular convolutional kernels like\n${3\\times3}$, our approach trains irregular kernel shapes to better fit the\ngeometric variations of input features. In other words, shapes are learnable\nparameters in addition to weights. The kernel shapes and weights are learned\nsimultaneously during end-to-end training with the standard back-propagation\nalgorithm. Experiments for semantic segmentation are implemented to validate\nthe effectiveness of our proposed ICNN.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 14:19:41 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Ma", "Jiabin", ""], ["Wang", "Wei", ""], ["Wang", "Liang", ""]]}, {"id": "1706.08033", "submitter": "Ruben Villegas", "authors": "Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, Honglak Lee", "title": "Decomposing Motion and Content for Natural Video Sequence Prediction", "comments": "International Conference on Learning Representations (ICLR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep neural network for the prediction of future frames in\nnatural video sequences. To effectively handle complex evolution of pixels in\nvideos, we propose to decompose the motion and content, two key components\ngenerating dynamics in videos. Our model is built upon the Encoder-Decoder\nConvolutional Neural Network and Convolutional LSTM for pixel-level prediction,\nwhich independently capture the spatial layout of an image and the\ncorresponding temporal dynamics. By independently modeling motion and content,\npredicting the next frame reduces to converting the extracted content features\ninto the next frame content by the identified motion features, which simplifies\nthe task of prediction. Our model is end-to-end trainable over multiple time\nsteps, and naturally learns to decompose motion and content without separate\ntraining. We evaluate the proposed network architecture on human activity\nvideos using KTH, Weizmann action, and UCF-101 datasets. We show\nstate-of-the-art performance in comparison to recent approaches. To the best of\nour knowledge, this is the first end-to-end trainable network architecture with\nmotion and content separation to model the spatiotemporal dynamics for\npixel-level future prediction in natural videos.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 04:18:12 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 01:21:32 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Villegas", "Ruben", ""], ["Yang", "Jimei", ""], ["Hong", "Seunghoon", ""], ["Lin", "Xunyu", ""], ["Lee", "Honglak", ""]]}, {"id": "1706.08088", "submitter": "Christophe Guyeux", "authors": "Anthony Tannoury and Rony Darazi and Christophe Guyeux and Abdallah\n  Makhoul", "title": "Efficient and accurate monitoring of the depth information in a Wireless\n  Multimedia Sensor Network based surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless Multimedia Sensor Network (WMSN) is a promising technology capturing\nrich multimedia data like audio and video, which can be useful to monitor an\nenvironment under surveillance. However, many scenarios in real time monitoring\nrequires 3D depth information. In this research work, we propose to use the\ndisparity map that is computed from two or multiple images, in order to monitor\nthe depth information in an object or event under surveillance using WMSN. Our\nsystem is based on distributed wireless sensors allowing us to notably reduce\nthe computational time needed for 3D depth reconstruction, thus permitting the\nsuccess of real time solutions. Each pair of sensors will capture images for a\ntargeted place/object and will operate a Stereo Matching in order to create a\nDisparity Map. Disparity maps will give us the ability to decrease traffic on\nthe bandwidth, because they are of low size. This will increase WMSN lifetime.\nAny event can be detected after computing the depth value for the target object\nin the scene, and also 3D scene reconstruction can be achieved with a disparity\nmap and some reference(s) image(s) taken by the node(s).\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 12:24:30 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Tannoury", "Anthony", ""], ["Darazi", "Rony", ""], ["Guyeux", "Christophe", ""], ["Makhoul", "Abdallah", ""]]}, {"id": "1706.08096", "submitter": "Adrien Coppens", "authors": "Adrien Coppens", "title": "Merging real and virtual worlds: An analysis of the state of the art and\n  practical evaluation of Microsoft Hololens", "comments": "Submitted in fulfillment of the requirements for the degree of Master\n  in Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving a symbiotic blending between reality and virtuality is a dream that\nhas been lying in the minds of many people for a long time. Advances in various\ndomains constantly bring us closer to making that dream come true. Augmented\nreality as well as virtual reality are in fact trending terms and are expected\nto further progress in the years to come.\n  This master's thesis aims to explore these areas and starts by defining\nnecessary terms such as augmented reality (AR) or virtual reality (VR). Usual\ntaxonomies to classify and compare the corresponding experiences are then\ndiscussed.\n  In order to enable those applications, many technical challenges need to be\ntackled, such as accurate motion tracking with 6 degrees of freedom (positional\nand rotational), that is necessary for compelling experiences and to prevent\nuser sickness. Additionally, augmented reality experiences typically rely on\nimage processing to position the superimposed content. To do so, \"paper\"\nmarkers or features extracted from the environment are often employed. Both\nsets of techniques are explored and common solutions and algorithms are\npresented.\n  After investigating those technical aspects, I carry out an objective\ncomparison of the existing state-of-the-art and state-of-the-practice in those\ndomains, and I discuss present and potential applications in these areas. As a\npractical validation, I present the results of an application that I have\ndeveloped using Microsoft HoloLens, one of the more advanced affordable\ntechnologies for augmented reality that is available today. Based on the\nexperience and lessons learned during this development, I discuss the\nlimitations of current technologies and present some avenues of future\nresearch.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 13:10:39 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Coppens", "Adrien", ""]]}, {"id": "1706.08098", "submitter": "Bolun Cai", "authors": "Suo Qiu, Xiangmin Xu and Bolun Cai", "title": "FReLU: Flexible Rectified Linear Units for Improving Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rectified linear unit (ReLU) is a widely used activation function for deep\nconvolutional neural networks. However, because of the zero-hard rectification,\nReLU networks miss the benefits from negative values. In this paper, we propose\na novel activation function called \\emph{flexible rectified linear unit\n(FReLU)} to further explore the effects of negative values. By redesigning the\nrectified point of ReLU as a learnable parameter, FReLU expands the states of\nthe activation output. When the network is successfully trained, FReLU tends to\nconverge to a negative value, which improves the expressiveness and thus the\nperformance. Furthermore, FReLU is designed to be simple and effective without\nexponential functions to maintain low cost computation. For being able to\neasily used in various network architectures, FReLU does not rely on strict\nassumptions by self-adaption. We evaluate FReLU on three standard image\nclassification datasets, including CIFAR-10, CIFAR-100, and ImageNet.\nExperimental results show that the proposed method achieves fast convergence\nand higher performances on both plain and residual networks.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 13:28:27 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 07:07:42 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Qiu", "Suo", ""], ["Xu", "Xiangmin", ""], ["Cai", "Bolun", ""]]}, {"id": "1706.08107", "submitter": "Michal Kepski", "authors": "Michal Kepski", "title": "Detekcja upadku i wybranych akcji na sekwencjach obraz\\'ow cyfrowych", "comments": "PhD Thesis (in Polish)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years a growing interest on action recognition is observed,\nincluding detection of fall accident for the elderly. However, despite many\nefforts undertaken, the existing technology is not widely used by elderly,\nmainly because of its flaws like low precision, large number of false alarms,\ninadequate privacy preserving during data acquisition and processing. This\nresearch work meets these expectations. The work is empirical and it is\nsituated in the field of computer vision systems. The main part of the work\nsituates itself in the area of action and behavior recognition. Efficient\nalgorithms for fall detection were developed, tested and implemented using\nimage sequences and wireless inertial sensor worn by a monitored person. A set\nof descriptors for depth maps has been elaborated to permit classification of\npose as well as the action of a person. Experimental research was carried out\nbased on the prepared data repository consisting of synchronized depth and\naccelerometric data. The study was carried out in the scenario with a static\ncamera facing the scene and an active camera observing the scene from above.\nThe experimental results showed that the developed algorithms for fall\ndetection have high sensitivity and specificity. The algorithm were designed\nwith regard to low computational demands and possibility to run on ARM\nplatforms. Several experiments including person detection, tracking and fall\ndetection in real-time were carried out to show efficiency and reliability of\nthe proposed solutions.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 13:55:55 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Kepski", "Michal", ""]]}, {"id": "1706.08124", "submitter": "Luis Carlos Garcia-Peraza-Herrera", "authors": "Lucas Fidon, Wenqi Li, Luis C. Garcia-Peraza-Herrera, Jinendra\n  Ekanayake, Neil Kitchen, Sebastien Ourselin, Tom Vercauteren", "title": "Scalable multimodal convolutional networks for brain tumour segmentation", "comments": "Paper accepted at MICCAI 2017", "journal-ref": null, "doi": "10.1007/978-3-319-66179-7_33", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain tumour segmentation plays a key role in computer-assisted surgery. Deep\nneural networks have increased the accuracy of automatic segmentation\nsignificantly, however these models tend to generalise poorly to different\nimaging modalities than those for which they have been designed, thereby\nlimiting their applications. For example, a network architecture initially\ndesigned for brain parcellation of monomodal T1 MRI can not be easily\ntranslated into an efficient tumour segmentation network that jointly utilises\nT1, T1c, Flair and T2 MRI. To tackle this, we propose a novel scalable\nmultimodal deep learning architecture using new nested structures that\nexplicitly leverage deep features within or across modalities. This aims at\nmaking the early layers of the architecture structured and sparse so that the\nfinal architecture becomes scalable to the number of modalities. We evaluate\nthe scalable architecture for brain tumour segmentation and give evidence of\nits regularisation effect compared to the conventional concatenation approach.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 15:23:54 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Fidon", "Lucas", ""], ["Li", "Wenqi", ""], ["Garcia-Peraza-Herrera", "Luis C.", ""], ["Ekanayake", "Jinendra", ""], ["Kitchen", "Neil", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1706.08126", "submitter": "Luis Carlos Garcia-Peraza-Herrera", "authors": "Luis C. Garcia-Peraza-Herrera, Wenqi Li, Lucas Fidon, Caspar\n  Gruijthuijsen, Alain Devreker, George Attilakos, Jan Deprest, Emmanuel Vander\n  Poorten, Danail Stoyanov, Tom Vercauteren, Sebastien Ourselin", "title": "ToolNet: Holistically-Nested Real-Time Segmentation of Robotic Surgical\n  Tools", "comments": "Paper accepted at IROS 2017", "journal-ref": null, "doi": "10.1109/IROS.2017.8206462", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time tool segmentation from endoscopic videos is an essential part of\nmany computer-assisted robotic surgical systems and of critical importance in\nrobotic surgical data science. We propose two novel deep learning architectures\nfor automatic segmentation of non-rigid surgical instruments. Both methods take\nadvantage of automated deep-learning-based multi-scale feature extraction while\ntrying to maintain an accurate segmentation quality at all resolutions. The two\nproposed methods encode the multi-scale constraint inside the network\narchitecture. The first proposed architecture enforces it by cascaded\naggregation of predictions and the second proposed network does it by means of\na holistically-nested architecture where the loss at each scale is taken into\naccount for the optimization process. As the proposed methods are for real-time\nsemantic labeling, both present a reduced number of parameters. We propose the\nuse of parametric rectified linear units for semantic labeling in these small\narchitectures to increase the regularization ability of the design and maintain\nthe segmentation accuracy without overfitting the training sets. We compare the\nproposed architectures against state-of-the-art fully convolutional networks.\nWe validate our methods using existing benchmark datasets, including ex vivo\ncases with phantom tissue and different robotic surgical instruments present in\nthe scene. Our results show a statistically significant improved Dice\nSimilarity Coefficient over previous instrument segmentation methods. We\nanalyze our design choices and discuss the key drivers for improving accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 15:41:25 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 19:53:12 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Garcia-Peraza-Herrera", "Luis C.", ""], ["Li", "Wenqi", ""], ["Fidon", "Lucas", ""], ["Gruijthuijsen", "Caspar", ""], ["Devreker", "Alain", ""], ["Attilakos", "George", ""], ["Deprest", "Jan", ""], ["Poorten", "Emmanuel Vander", ""], ["Stoyanov", "Danail", ""], ["Vercauteren", "Tom", ""], ["Ourselin", "Sebastien", ""]]}, {"id": "1706.08153", "submitter": "Nati Ofir", "authors": "Ofer Bartal, Nati Ofir, Yaron Lipman, Ronen Basri", "title": "Photometric Stereo by Hemispherical Metric Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photometric Stereo methods seek to reconstruct the 3d shape of an object from\nmotionless images obtained with varying illumination. Most existing methods\nsolve a restricted problem where the physical reflectance model, such as\nLambertian reflectance, is known in advance. In contrast, we do not restrict\nourselves to a specific reflectance model. Instead, we offer a method that\nworks on a wide variety of reflectances. Our approach uses a simple yet\nuncommonly used property of the problem - the sought after normals are points\non a unit hemisphere. We present a novel embedding method that maps pixels to\nnormals on the unit hemisphere. Our experiments demonstrate that this approach\noutperforms existing manifold learning methods for the task of hemisphere\nembedding. We further show successful reconstructions of objects from a wide\nvariety of reflectances including smooth, rough, diffuse and specular surfaces,\neven in the presence of significant attached shadows. Finally, we empirically\nprove that under these challenging settings we obtain more accurate shape\nreconstructions than existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 18:36:28 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Bartal", "Ofer", ""], ["Ofir", "Nati", ""], ["Lipman", "Yaron", ""], ["Basri", "Ronen", ""]]}, {"id": "1706.08189", "submitter": "Terence Brouns", "authors": "Terence Brouns", "title": "Robust Video-Based Eye Tracking Using Recursive Estimation of Pupil\n  Characteristics", "comments": "29 pages, 16 figures; fixed hyperlink in abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based eye tracking is a valuable technique in various research fields.\nNumerous open-source eye tracking algorithms have been developed in recent\nyears, primarily designed for general application with many different camera\ntypes. These algorithms do not, however, capitalize on the high frame rate of\neye tracking cameras often employed in psychophysical studies. We present a\npupil detection method that utilizes this high-speed property to obtain\nreliable predictions through recursive estimation about certain pupil\ncharacteristics in successive camera frames. These predictions are subsequently\nused to carry out novel image segmentation and classification routines to\nimprove pupil detection performance. Based on results from hand-labelled eye\nimages, our approach was found to have a greater detection rate, accuracy and\nspeed compared to other recently published open-source pupil detection\nalgorithms. The program's source code, together with a graphical user\ninterface, can be downloaded at https://github.com/tbrouns/eyestalker\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 23:22:02 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 08:23:16 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Brouns", "Terence", ""]]}, {"id": "1706.08211", "submitter": "Seong-Gyun Jeong", "authors": "Seong-Gyun Jeong, Jiwon Kim, Sujung Kim, Jaesik Min", "title": "End-to-end Learning of Image based Lane-Change Decision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an image based end-to-end learning framework that helps\nlane-change decisions for human drivers and autonomous vehicles. The proposed\nsystem, Safe Lane-Change Aid Network (SLCAN), trains a deep convolutional\nneural network to classify the status of adjacent lanes from rear view images\nacquired by cameras mounted on both sides of the vehicle. Rather than depending\non any explicit object detection or tracking scheme, SLCAN reads the whole\ninput image and directly decides whether initiation of the lane-change at the\nmoment is safe or not. We collected and annotated 77,273 rear side view images\nto train and test SLCAN. Experimental results show that the proposed framework\nachieves 96.98% classification accuracy although the test images are from\nunseen roadways. We also visualize the saliency map to understand which part of\nimage SLCAN looks at for correct decisions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 02:59:56 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Jeong", "Seong-Gyun", ""], ["Kim", "Jiwon", ""], ["Kim", "Sujung", ""], ["Min", "Jaesik", ""]]}, {"id": "1706.08218", "submitter": "Hongyuan Zhu", "authors": "Hongyuan Zhu, Romain Vial, Shijian Lu, Yonghong Tian, Xianbin Cao", "title": "YoTube: Searching Action Proposal via Recurrent and Static Regression\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2806279", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present YoTube-a novel network fusion framework for\nsearching action proposals in untrimmed videos, where each action proposal\ncorresponds to a spatialtemporal video tube that potentially locates one human\naction. Our method consists of a recurrent YoTube detector and a static YoTube\ndetector, where the recurrent YoTube explores the regression capability of RNN\nfor candidate bounding boxes predictions using learnt temporal dynamics and the\nstatic YoTube produces the bounding boxes using rich appearance cues in a\nsingle frame. Both networks are trained using rgb and optical flow in order to\nfully exploit the rich appearance, motion and temporal context, and their\noutputs are fused to produce accurate and robust proposal boxes. Action\nproposals are finally constructed by linking these boxes using dynamic\nprogramming with a novel trimming method to handle the untrimmed video\neffectively and efficiently. Extensive experiments on the challenging UCF-101\nand UCF-Sports datasets show that our proposed technique obtains superior\nperformance compared with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 03:52:15 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Zhu", "Hongyuan", ""], ["Vial", "Romain", ""], ["Lu", "Shijian", ""], ["Tian", "Yonghong", ""], ["Cao", "Xianbin", ""]]}, {"id": "1706.08227", "submitter": "Jerrin Thomas Panachakel", "authors": "Jerrin Thomas Panachakel and Jeena R.S.", "title": "Multi-level SVM Based CAD Tool for Classifying Structural MRIs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The revolutionary developments in the field of supervised machine learning\nhave paved way to the development of CAD tools for assisting doctors in\ndiagnosis. Recently, the former has been employed in the prediction of\nneurological disorders such as Alzheimer's disease. We propose a CAD (Computer\nAided Diagnosis tool for differentiating neural lesions caused by CVA\n(Cerebrovascular Accident) from the lesions caused by other neural disorders by\nusing Non-negative Matrix Factorisation (NMF) and Haralick features for feature\nextraction and SVM (Support Vector Machine) for pattern recognition. We also\nintroduce a multi-level classification system that has better classification\nefficiency, sensitivity and specificity when compared to systems using NMF or\nHaralick features alone as features for classification. Cross-validation was\nperformed using LOOCV (Leave-One-Out Cross Validation) method and our proposed\nsystem has a classification accuracy of over 86%.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 04:30:36 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Panachakel", "Jerrin Thomas", ""], ["S.", "Jeena R.", ""]]}, {"id": "1706.08249", "submitter": "Xuanyi Dong", "authors": "Xuanyi Dong, Liang Zheng, Fan Ma, Yi Yang, Deyu Meng", "title": "Few-Example Object Detection with Model Communication", "comments": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2018", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2844853", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study object detection using a large pool of unlabeled\nimages and only a few labeled images per category, named \"few-example object\ndetection\". The key challenge consists in generating trustworthy training\nsamples as many as possible from the pool. Using few training examples as\nseeds, our method iterates between model training and high-confidence sample\nselection. In training, easy samples are generated first and, then the poorly\ninitialized model undergoes improvement. As the model becomes more\ndiscriminative, challenging but reliable samples are selected. After that,\nanother round of model improvement takes place. To further improve the\nprecision and recall of the generated training samples, we embed multiple\ndetection models in our framework, which has proven to outperform the single\nmodel baseline and the model ensemble method. Experiments on PASCAL VOC'07, MS\nCOCO'14, and ILSVRC'13 indicate that by using as few as three or four samples\nselected for each category, our method produces very competitive results when\ncompared to the state-of-the-art weakly-supervised approaches using a large\nnumber of image-level labels.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 07:00:30 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 03:20:07 GMT"}, {"version": "v3", "created": "Mon, 14 Aug 2017 06:33:04 GMT"}, {"version": "v4", "created": "Wed, 16 Aug 2017 07:50:53 GMT"}, {"version": "v5", "created": "Fri, 18 Aug 2017 07:07:05 GMT"}, {"version": "v6", "created": "Wed, 14 Feb 2018 07:51:44 GMT"}, {"version": "v7", "created": "Wed, 15 Aug 2018 01:25:58 GMT"}, {"version": "v8", "created": "Tue, 30 Oct 2018 23:51:46 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Dong", "Xuanyi", ""], ["Zheng", "Liang", ""], ["Ma", "Fan", ""], ["Yang", "Yi", ""], ["Meng", "Deyu", ""]]}, {"id": "1706.08260", "submitter": "Seonghyeon Nam", "authors": "Seonghyeon Nam, Seon Joo Kim", "title": "Deep Semantics-Aware Photo Adjustment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic photo adjustment is to mimic the photo retouching style of\nprofessional photographers and automatically adjust photos to the learned\nstyle. There have been many attempts to model the tone and the color adjustment\nglobally with low-level color statistics. Also, spatially varying photo\nadjustment methods have been studied by exploiting high-level features and\nsemantic label maps. Those methods are semantics-aware since the color mapping\nis dependent on the high-level semantic context. However, their performance is\nlimited to the pre-computed hand-crafted features and it is hard to reflect\nuser's preference to the adjustment. In this paper, we propose a deep neural\nnetwork that models the semantics-aware photo adjustment. The proposed network\nexploits bilinear models that are the multiplicative interaction of the color\nand the contexual features. As the contextual features we propose the semantic\nadjustment map, which discovers the inherent photo retouching presets that are\napplied according to the scene context. The proposed method is trained using a\nrobust loss with a scene parsing task. The experimental results show that the\nproposed method outperforms the existing method both quantitatively and\nqualitatively. The proposed method also provides users a way to retouch the\nphoto by their own likings by giving customized adjustment maps.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 07:35:07 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Nam", "Seonghyeon", ""], ["Kim", "Seon Joo", ""]]}, {"id": "1706.08276", "submitter": "Amir Shahroudy", "authors": "Jun Liu, Amir Shahroudy, Dong Xu, Alex C. Kot, Gang Wang", "title": "Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network\n  with Trust Gates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton-based human action recognition has attracted a lot of research\nattention during the past few years. Recent works attempted to utilize\nrecurrent neural networks to model the temporal dependencies between the 3D\npositional configurations of human body joints for better analysis of human\nactivities in the skeletal data. The proposed work extends this idea to spatial\ndomain as well as temporal domain to better analyze the hidden sources of\naction-related information within the human skeleton sequences in both of these\ndomains simultaneously. Based on the pictorial structure of Kinect's skeletal\ndata, an effective tree-structure based traversal framework is also proposed.\nIn order to deal with the noise in the skeletal data, a new gating mechanism\nwithin LSTM module is introduced, with which the network can learn the\nreliability of the sequential data and accordingly adjust the effect of the\ninput data on the updating procedure of the long-term context representation\nstored in the unit's memory cell. Moreover, we introduce a novel multi-modal\nfeature fusion strategy within the LSTM unit in this paper. The comprehensive\nexperimental results on seven challenging benchmark datasets for human action\nrecognition demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 08:35:45 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Liu", "Jun", ""], ["Shahroudy", "Amir", ""], ["Xu", "Dong", ""], ["Kot", "Alex C.", ""], ["Wang", "Gang", ""]]}, {"id": "1706.08323", "submitter": "Ruifeng Shao", "authors": "Ruifeng Shao, Ning Xu, Xin Geng", "title": "Multi-Label Learning with Label Enhancement", "comments": "ICDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of multi-label learning is to predict a set of relevant labels for\nthe unseen instance. Traditional multi-label learning algorithms treat each\nclass label as a logical indicator of whether the corresponding label is\nrelevant or irrelevant to the instance, i.e., +1 represents relevant to the\ninstance and -1 represents irrelevant to the instance. Such label represented\nby -1 or +1 is called logical label. Logical label cannot reflect different\nlabel importance. However, for real-world multi-label learning problems, the\nimportance of each possible label is generally different. For the real\napplications, it is difficult to obtain the label importance information\ndirectly. Thus we need a method to reconstruct the essential label importance\nfrom the logical multilabel data. To solve this problem, we assume that each\nmulti-label instance is described by a vector of latent real-valued labels,\nwhich can reflect the importance of the corresponding labels. Such label is\ncalled numerical label. The process of reconstructing the numerical labels from\nthe logical multi-label data via utilizing the logical label information and\nthe topological structure in the feature space is called Label Enhancement. In\nthis paper, we propose a novel multi-label learning framework called LEMLL,\ni.e., Label Enhanced Multi-Label Learning, which incorporates regression of the\nnumerical labels and label enhancement into a unified framework. Extensive\ncomparative studies validate that the performance of multi-label learning can\nbe improved significantly with label enhancement and LEMLL can effectively\nreconstruct latent label importance information from logical multi-label data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 11:15:04 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 08:36:50 GMT"}, {"version": "v3", "created": "Wed, 9 Jan 2019 13:41:45 GMT"}, {"version": "v4", "created": "Tue, 16 Apr 2019 09:51:09 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Shao", "Ruifeng", ""], ["Xu", "Ning", ""], ["Geng", "Xin", ""]]}, {"id": "1706.08336", "submitter": "Maro\\v{s} Bl\\'aha", "authors": "Maros Blaha, Mathias Rothermel, Martin R. Oswald, Torsten Sattler,\n  Audrey Richard, Jan D. Wegner, Marc Pollefeys, Konrad Schindler", "title": "Semantically Informed Multiview Surface Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to jointly refine the geometry and semantic segmentation\nof 3D surface meshes. Our method alternates between updating the shape and the\nsemantic labels. In the geometry refinement step, the mesh is deformed with\nvariational energy minimization, such that it simultaneously maximizes\nphoto-consistency and the compatibility of the semantic segmentations across a\nset of calibrated images. Label-specific shape priors account for interactions\nbetween the geometry and the semantic labels in 3D. In the semantic\nsegmentation step, the labels on the mesh are updated with MRF inference, such\nthat they are compatible with the semantic segmentations in the input images.\nAlso, this step includes prior assumptions about the surface shape of different\nsemantic classes. The priors induce a tight coupling, where semantic\ninformation influences the shape update and vice versa. Specifically, we\nintroduce priors that favor (i) adaptive smoothing, depending on the class\nlabel; (ii) straightness of class boundaries; and (iii) semantic labels that\nare consistent with the surface orientation. The novel mesh-based\nreconstruction is evaluated in a series of experiments with real and synthetic\ndata. We compare both to state-of-the-art, voxel-based semantic 3D\nreconstruction, and to purely geometric mesh refinement, and demonstrate that\nthe proposed scheme yields improved 3D geometry as well as an improved semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 12:19:25 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Blaha", "Maros", ""], ["Rothermel", "Mathias", ""], ["Oswald", "Martin R.", ""], ["Sattler", "Torsten", ""], ["Richard", "Audrey", ""], ["Wegner", "Jan D.", ""], ["Pollefeys", "Marc", ""], ["Schindler", "Konrad", ""]]}, {"id": "1706.08355", "submitter": "Ayush Dewan", "authors": "Ayush Dewan, Gabriel L. Oliveira and Wolfram Burgard", "title": "Deep Semantic Classification for 3D LiDAR Data", "comments": "8 pages to be published in IROS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots are expected to operate autonomously in dynamic environments.\nUnderstanding the underlying dynamic characteristics of objects is a key\nenabler for achieving this goal. In this paper, we propose a method for\npointwise semantic classification of 3D LiDAR data into three classes:\nnon-movable, movable and dynamic. We concentrate on understanding these\nspecific semantics because they characterize important information required for\nan autonomous system. Non-movable points in the scene belong to unchanging\nsegments of the environment, whereas the remaining classes corresponds to the\nchanging parts of the scene. The difference between the movable and dynamic\nclass is their motion state. The dynamic points can be perceived as moving,\nwhereas movable objects can move, but are perceived as static. To learn the\ndistinction between movable and non-movable points in the environment, we\nintroduce an approach based on deep neural network and for detecting the\ndynamic points, we estimate pointwise motion. We propose a Bayes filter\nframework for combining the learned semantic cues with the motion cues to infer\nthe required semantic classification. In extensive experiments, we compare our\napproach with other methods on a standard benchmark dataset and report\ncompetitive results in comparison to the existing state-of-the-art.\nFurthermore, we show an improvement in the classification of points by\ncombining the semantic cues retrieved from the neural network with the motion\ncues.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 13:16:57 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Dewan", "Ayush", ""], ["Oliveira", "Gabriel L.", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1706.08436", "submitter": "Juan Garcia Torres", "authors": "Juan Garcia-Torres, Diana Caro-Prieto", "title": "Image Processing in Floriculture Using a robotic Mobile Platform", "comments": "4 Pages, Paper made at Fundacion Universitaria Agraria de Colombia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colombia has a privileged geographical location which makes it a cornerstone\nand equidistant point to all regional markets. The country has a great\necological diversity and it is one of the largest suppliers of flowers for US.\nColombian flower companies have made innovations in the marketing process,\nusing methods to reach all conditions for final consumers. This article\ndevelops a monitoring system for floriculture industries. The system was\nimplemented in a robotic platform. This device has the ability to be programmed\nin different programming languages. The robot takes the necessary environment\ninformation from its camera. The algorithm of the monitoring system was\ndeveloped with the image processing toolbox on Matlab. The implemented\nalgorithm acquires images through its camera, it performs a preprocessing of\nthe image, noise filter, enhancing of the color and adjusting the dimension in\norder to increase processing speed. Then, the image is segmented by color and\nwith the binarized version of the image using morphological operations (erosion\nand dilation), extract relevant features such as centroid, perimeter and area.\nThe data obtained from the image processing helps the robot with the automatic\nidentification of objectives, orientation and move towards them. Also, the\nresults generate a diagnostic quality of each object scanned.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 15:21:44 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Garcia-Torres", "Juan", ""], ["Caro-Prieto", "Diana", ""]]}, {"id": "1706.08442", "submitter": "Andrea Palazzi", "authors": "Andrea Palazzi, Guido Borghi, Davide Abati, Simone Calderara, Rita\n  Cucchiara", "title": "Learning to Map Vehicles into Bird's Eye View", "comments": "Accepted to International Conference on Image Analysis and Processing\n  (ICIAP) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Awareness of the road scene is an essential component for both autonomous\nvehicles and Advances Driver Assistance Systems and is gaining importance both\nfor the academia and car companies. This paper presents a way to learn a\nsemantic-aware transformation which maps detections from a dashboard camera\nview onto a broader bird's eye occupancy map of the scene. To this end, a huge\nsynthetic dataset featuring 1M couples of frames, taken from both car dashboard\nand bird's eye view, has been collected and automatically annotated. A\ndeep-network is then trained to warp detections from the first to the second\nview. We demonstrate the effectiveness of our model against several baselines\nand observe that is able to generalize on real-world data despite having been\ntrained solely on synthetic ones.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 15:39:53 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Palazzi", "Andrea", ""], ["Borghi", "Guido", ""], ["Abati", "Davide", ""], ["Calderara", "Simone", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1706.08474", "submitter": "Marcella Cornia", "authors": "Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, Rita Cucchiara", "title": "Paying More Attention to Saliency: Image Captioning with Saliency and\n  Context Attention", "comments": "ACM Transactions on Multimedia Computing, Communications and\n  Applications, Vol. 14, No. 2, Article 48", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning has been recently gaining a lot of attention thanks to the\nimpressive achievements shown by deep captioning architectures, which combine\nConvolutional Neural Networks to extract image representations, and Recurrent\nNeural Networks to generate the corresponding captions. At the same time, a\nsignificant research effort has been dedicated to the development of saliency\nprediction models, which can predict human eye fixations. Even though saliency\ninformation could be useful to condition an image captioning architecture, by\nproviding an indication of what is salient and what is not, research is still\nstruggling to incorporate these two techniques. In this work, we propose an\nimage captioning approach in which a generative recurrent neural network can\nfocus on different parts of the input image during the generation of the\ncaption, by exploiting the conditioning given by a saliency prediction model on\nwhich parts of the image are salient and which are contextual. We show, through\nextensive quantitative and qualitative experiments on large scale datasets,\nthat our model achieves superior performances with respect to captioning\nbaselines with and without saliency, and to different state of the art\napproaches combining saliency and captioning.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 16:45:57 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 08:28:02 GMT"}, {"version": "v3", "created": "Sat, 4 Nov 2017 12:04:01 GMT"}, {"version": "v4", "created": "Mon, 21 May 2018 09:26:27 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Cornia", "Marcella", ""], ["Baraldi", "Lorenzo", ""], ["Serra", "Giuseppe", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1706.08482", "submitter": "Samuel Schulter", "authors": "Samuel Schulter, Paul Vernaza, Wongun Choi, Manmohan Chandraker", "title": "Deep Network Flow for Multi-Object Tracking", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data association problems are an important component of many computer vision\napplications, with multi-object tracking being one of the most prominent\nexamples. A typical approach to data association involves finding a graph\nmatching or network flow that minimizes a sum of pairwise association costs,\nwhich are often either hand-crafted or learned as linear functions of fixed\nfeatures. In this work, we demonstrate that it is possible to learn features\nfor network-flow-based data association via backpropagation, by expressing the\noptimum of a smoothed network flow problem as a differentiable function of the\npairwise association costs. We apply this approach to multi-object tracking\nwith a network flow formulation. Our experiments demonstrate that we are able\nto successfully learn all cost functions for the association problem in an\nend-to-end fashion, which outperform hand-crafted costs in all settings. The\nintegration and combination of various sources of inputs becomes easy and the\ncost functions can be learned entirely from data, alleviating tedious\nhand-designing of costs.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 17:08:45 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Schulter", "Samuel", ""], ["Vernaza", "Paul", ""], ["Choi", "Wongun", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1706.08502", "submitter": "Satwik Kottur", "authors": "Satwik Kottur, Jos\\'e M.F. Moura, Stefan Lee, Dhruv Batra", "title": "Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog", "comments": "9 pages, 7 figures, 2 tables, accepted at EMNLP 2017 as short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent works have proposed techniques for end-to-end learning of\ncommunication protocols among cooperative multi-agent populations, and have\nsimultaneously found the emergence of grounded human-interpretable language in\nthe protocols developed by the agents, all learned without any human\nsupervision!\n  In this paper, using a Task and Tell reference game between two agents as a\ntestbed, we present a sequence of 'negative' results culminating in a\n'positive' one -- showing that while most agent-invented languages are\neffective (i.e. achieve near-perfect task rewards), they are decidedly not\ninterpretable or compositional.\n  In essence, we find that natural language does not emerge 'naturally',\ndespite the semblance of ease of natural-language-emergence that one may gather\nfrom recent literature. We discuss how it is possible to coax the invented\nlanguages to become more and more human-like and compositional by increasing\nrestrictions on how two agents may communicate.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 17:47:46 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 03:37:00 GMT"}, {"version": "v3", "created": "Sun, 20 Aug 2017 04:41:15 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Kottur", "Satwik", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Lee", "Stefan", ""], ["Batra", "Dhruv", ""]]}, {"id": "1706.08564", "submitter": "Garrick Brazil", "authors": "Garrick Brazil, Xi Yin, Xiaoming Liu", "title": "Illuminating Pedestrians via Simultaneous Detection & Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection is a critical problem in computer vision with\nsignificant impact on safety in urban autonomous driving. In this work, we\nexplore how semantic segmentation can be used to boost pedestrian detection\naccuracy while having little to no impact on network efficiency. We propose a\nsegmentation infusion network to enable joint supervision on semantic\nsegmentation and pedestrian detection. When placed properly, the additional\nsupervision helps guide features in shared layers to become more sophisticated\nand helpful for the downstream pedestrian detector. Using this approach, we\nfind weakly annotated boxes to be sufficient for considerable performance\ngains. We provide an in-depth analysis to demonstrate how shared layers are\nshaped by the segmentation supervision. In doing so, we show that the resulting\nfeature maps become more semantically meaningful and robust to shape and\nocclusion. Overall, our simultaneous detection and segmentation framework\nachieves a considerable gain over the state-of-the-art on the Caltech\npedestrian dataset, competitive performance on KITTI, and executes 2x faster\nthan competitive methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 19:05:52 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Brazil", "Garrick", ""], ["Yin", "Xi", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1706.08574", "submitter": "Zibo Meng", "authors": "Zibo Meng, Xiaochuan Fan, Xin Chen, Min Chen, Yan Tong", "title": "Detecting Small Signs from Large Images", "comments": "8 pages, 6 figures, accepted by IEEE Conference on Information Reuse\n  and Integration (IRI) 2017 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, Convolutional Neural Networks (CNNs) have been\ndemonstrated successful for object detections. However, the size of network\ninput is limited by the amount of memory available on GPUs. Moreover,\nperformance degrades when detecting small objects. To alleviate the memory\nusage and improve the performance of detecting small traffic signs, we proposed\nan approach for detecting small traffic signs from large images under real\nworld conditions. In particular, large images are broken into small patches as\ninput to a Small-Object-Sensitive-CNN (SOS-CNN) modified from a Single Shot\nMultibox Detector (SSD) framework with a VGG-16 network as the base network to\nproduce patch-level object detection results. Scale invariance is achieved by\napplying the SOS-CNN on an image pyramid. Then, image-level object detection is\nobtained by projecting all the patch-level detection results to the image at\nthe original scale. Experimental results on a real-world conditioned traffic\nsign dataset have demonstrated the effectiveness of the proposed method in\nterms of detection accuracy and recall, especially for those with small sizes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 19:42:33 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Meng", "Zibo", ""], ["Fan", "Xiaochuan", ""], ["Chen", "Xin", ""], ["Chen", "Min", ""], ["Tong", "Yan", ""]]}, {"id": "1706.08575", "submitter": "John McKay", "authors": "John McKay, Anne Gelb, Vishal Monga, Raghu Raj", "title": "Using Frame Theoretic Convolutional Gridding for Robust Synthetic\n  Aperture Sonar Imaging", "comments": "Accepted to OCEANS 2017 - Anchorage (Conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in synthetic aperture sonar (SAS) technology and processing\nhas led to significant advances in underwater imaging, outperforming previously\ncommon approaches in both accuracy and efficiency. There are, however, inherent\nlimitations to current SAS reconstruction methodology. In particular, popular\nand efficient Fourier domain SAS methods require a 2D interpolation which is\noften ill conditioned and inaccurate, inevitably reducing robustness with\nregard to speckle and inaccurate sound-speed estimation. To overcome these\nissues, we propose using the frame theoretic convolution gridding (FTCG)\nalgorithm to handle the non-uniform Fourier data. FTCG extends upon non-uniform\nfast Fourier transform (NUFFT) algorithms by casting the NUFFT as an\napproximation problem given Fourier frame data. The FTCG has been show to yield\nimproved accuracy at little more computational cost. Using simulated data, we\noutline how the FTCG can be used to enhance current SAS processing.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 19:42:59 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["McKay", "John", ""], ["Gelb", "Anne", ""], ["Monga", "Vishal", ""], ["Raj", "Raghu", ""]]}, {"id": "1706.08590", "submitter": "John McKay", "authors": "John McKay, Vishal Monga, Raghu G. Raj", "title": "Robust Sonar ATR Through Bayesian Pose Corrected Sparse Classification", "comments": "14 Pages, 16 Figures, Accepted TGARS", "journal-ref": null, "doi": "10.1109/TGRS.2017.2710040", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sonar imaging has seen vast improvements over the last few decades due in\npart to advances in synthetic aperture Sonar (SAS). Sophisticated\nclassification techniques can now be used in Sonar automatic target recognition\n(ATR) to locate mines and other threatening objects. Among the most promising\nof these methods is sparse reconstruction-based classification (SRC) which has\nshown an impressive resiliency to noise, blur, and occlusion. We present a\ncoherent strategy for expanding upon SRC for Sonar ATR that retains SRC's\nrobustness while also being able to handle targets with diverse geometric\narrangements, bothersome Rayleigh noise, and unavoidable background clutter.\nOur method, pose corrected sparsity (PCS), incorporates a novel interpretation\nof a spike and slab probability distribution towards use as a Bayesian prior\nfor class-specific discrimination in combination with a dictionary learning\nscheme for localized patch extractions. Additionally, PCS offers the potential\nfor anomaly detection in order to avoid false identifications of tested objects\nfrom outside the training set with no additional training required. Compelling\nresults are shown using a database provided by the United States Naval Surface\nWarfare Center.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 20:53:32 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["McKay", "John", ""], ["Monga", "Vishal", ""], ["Raj", "Raghu G.", ""]]}, {"id": "1706.08606", "submitter": "David Barrett", "authors": "Samuel Ritter, David G.T. Barrett, Adam Santoro and Matt M. Botvinick", "title": "Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved unprecedented performance on a wide\nrange of complex tasks, rapidly outpacing our understanding of the nature of\ntheir solutions. This has caused a recent surge of interest in methods for\nrendering modern neural systems more interpretable. In this work, we propose to\naddress the interpretability problem in modern DNNs using the rich history of\nproblem descriptions, theories and experimental methods developed by cognitive\npsychologists to study the human mind. To explore the potential value of these\ntools, we chose a well-established analysis from developmental psychology that\nexplains how children learn word labels for objects, and applied that analysis\nto DNNs. Using datasets of stimuli inspired by the original cognitive\npsychology experiments, we find that state-of-the-art one shot learning models\ntrained on ImageNet exhibit a similar bias to that observed in humans: they\nprefer to categorize objects according to shape rather than color. The\nmagnitude of this shape bias varies greatly among architecturally identical,\nbut differently seeded models, and even fluctuates within seeds throughout\ntraining, despite nearly equivalent classification performance. These results\ndemonstrate the capability of tools from cognitive psychology for exposing\nhidden computational properties of DNNs, while concurrently providing us with a\ncomputational model for human word learning.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 21:31:18 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 17:52:55 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Ritter", "Samuel", ""], ["Barrett", "David G. T.", ""], ["Santoro", "Adam", ""], ["Botvinick", "Matt M.", ""]]}, {"id": "1706.08616", "submitter": "Gemma Roig", "authors": "Anna Volokitin, Gemma Roig and Tomaso Poggio", "title": "Do Deep Neural Networks Suffer from Crowding?", "comments": "CBMM memo", "journal-ref": null, "doi": null, "report-no": "69", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowding is a visual effect suffered by humans, in which an object that can\nbe recognized in isolation can no longer be recognized when other objects,\ncalled flankers, are placed close to it. In this work, we study the effect of\ncrowding in artificial Deep Neural Networks for object recognition. We analyze\nboth standard deep convolutional neural networks (DCNNs) as well as a new\nversion of DCNNs which is 1) multi-scale and 2) with size of the convolution\nfilters change depending on the eccentricity wrt to the center of fixation.\nSuch networks, that we call eccentricity-dependent, are a computational model\nof the feedforward path of the primate visual cortex. Our results reveal that\nthe eccentricity-dependent model, trained on target objects in isolation, can\nrecognize such targets in the presence of flankers, if the targets are near the\ncenter of the image, whereas DCNNs cannot. Also, for all tested networks, when\ntrained on targets in isolation, we find that recognition accuracy of the\nnetworks decreases the closer the flankers are to the target and the more\nflankers there are. We find that visual similarity between the target and\nflankers also plays a role and that pooling in early layers of the network\nleads to more crowding. Additionally, we show that incorporating the flankers\ninto the images of the training set does not improve performance with crowding.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 22:25:56 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Volokitin", "Anna", ""], ["Roig", "Gemma", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1706.08629", "submitter": "Yuchao Dai Dr.", "authors": "Yuchao Dai, Huizhong Deng, Mingyi He", "title": "Dense Non-rigid Structure-from-Motion Made Easy - A Spatial-Temporal\n  Smoothness based Solution", "comments": "Accepted by ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple spatial-temporal smoothness based method for\nsolving dense non-rigid structure-from-motion (NRSfM). First, we revisit the\ntemporal smoothness and demonstrate that it can be extended to dense case\ndirectly. Second, we propose to exploit the spatial smoothness by resorting to\nthe Laplacian of the 3D non-rigid shape. Third, to handle real world noise and\noutliers in measurements, we robustify the data term by using the $L_1$ norm.\nIn this way, our method could robustly exploit both spatial and temporal\nsmoothness effectively and make dense non-rigid reconstruction easy. Our method\nis very easy to implement, which involves solving a series of least squares\nproblems. Experimental results on both synthetic and real image dense NRSfM\ntasks show that the proposed method outperforms state-of-the-art dense\nnon-rigid reconstruction methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 00:18:55 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Dai", "Yuchao", ""], ["Deng", "Huizhong", ""], ["He", "Mingyi", ""]]}, {"id": "1706.08653", "submitter": "Shafin Rahman", "authors": "Shafin Rahman, Salman H. Khan and Fatih Porikli", "title": "A Unified approach for Conventional Zero-shot, Generalized Zero-shot and\n  Few-shot Learning", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2861573", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Prevalent techniques in zero-shot learning do not generalize well to other\nrelated problem scenarios. Here, we present a unified approach for conventional\nzero-shot, generalized zero-shot and few-shot learning problems. Our approach\nis based on a novel Class Adapting Principal Directions (CAPD) concept that\nallows multiple embeddings of image features into a semantic space. Given an\nimage, our method produces one principal direction for each seen class. Then,\nit learns how to combine these directions to obtain the principal direction for\neach unseen class such that the CAPD of the test image is aligned with the\nsemantic embedding of the true class, and opposite to the other classes. This\nallows efficient and class-adaptive information transfer from seen to unseen\nclasses. In addition, we propose an automatic process for selection of the most\nuseful seen classes for each unseen class to achieve robustness in zero-shot\nlearning. Our method can update the unseen CAPD taking the advantages of few\nunseen images to work in a few-shot learning scenario. Furthermore, our method\ncan generalize the seen CAPDs by estimating seen-unseen diversity that\nsignificantly improves the performance of generalized zero-shot learning. Our\nextensive evaluations demonstrate that the proposed approach consistently\nachieves superior performance in zero-shot, generalized zero-shot and\nfew/one-shot learning problems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 02:42:55 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 22:55:39 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Rahman", "Shafin", ""], ["Khan", "Salman H.", ""], ["Porikli", "Fatih", ""]]}, {"id": "1706.08658", "submitter": "Rima Arnaout", "authors": "Ali Madani, Ramy Arnaout, Mohammad Mofrad, Rima Arnaout", "title": "Fast and accurate classification of echocardiograms using deep learning", "comments": "31 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echocardiography is essential to modern cardiology. However, human\ninterpretation limits high throughput analysis, limiting echocardiography from\nreaching its full clinical and research potential for precision medicine. Deep\nlearning is a cutting-edge machine-learning technique that has been useful in\nanalyzing medical images but has not yet been widely applied to\nechocardiography, partly due to the complexity of echocardiograms' multi view,\nmulti modality format. The essential first step toward comprehensive computer\nassisted echocardiographic interpretation is determining whether computers can\nlearn to recognize standard views. To this end, we anonymized 834,267\ntransthoracic echocardiogram (TTE) images from 267 patients (20 to 96 years, 51\npercent female, 26 percent obese) seen between 2000 and 2017 and labeled them\naccording to standard views. Images covered a range of real world clinical\nvariation. We built a multilayer convolutional neural network and used\nsupervised learning to simultaneously classify 15 standard views. Eighty\npercent of data used was randomly chosen for training and 20 percent reserved\nfor validation and testing on never seen echocardiograms. Using multiple images\nfrom each clip, the model classified among 12 video views with 97.8 percent\noverall test accuracy without overfitting. Even on single low resolution\nimages, test accuracy among 15 views was 91.7 percent versus 70.2 to 83.5\npercent for board-certified echocardiographers. Confusional matrices, occlusion\nexperiments, and saliency mapping showed that the model finds recognizable\nsimilarities among related views and classifies using clinically relevant image\nfeatures. In conclusion, deep neural networks can classify essential\nechocardiographic views simultaneously and with high accuracy. Our results\nprovide a foundation for more complex deep learning assisted echocardiographic\ninterpretation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 03:21:47 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Madani", "Ali", ""], ["Arnaout", "Ramy", ""], ["Mofrad", "Mohammad", ""], ["Arnaout", "Rima", ""]]}, {"id": "1706.08665", "submitter": "Jiayun Wang", "authors": "Peter Wang, Zhongxia Yan, Jeff Zhang", "title": "Hierarchical Model for Long-term Video Prediction", "comments": "We have made some errors in the submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video prediction has been an active topic of research in the past few years.\nMany algorithms focus on pixel-level predictions, which generates results that\nblur and disintegrate within a few frames. In this project, we use a\nhierarchical approach for long-term video prediction. We aim at estimating\nhigh-level structure in the input frame first, then predict how that structure\ngrows in the future. Finally, we use an image analogy network to recover a\nrealistic image from the predicted structure. Our method is largely adopted\nfrom the work by Villegas et al. The method is built with a combination of\nLSTMs and analogy-based convolutional auto-encoder networks. Additionally, in\norder to generate more realistic frame predictions, we also adopt adversarial\nloss. We evaluate our method on the Penn Action dataset, and demonstrate good\nresults on high-level long-term structure prediction.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 04:11:41 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 06:27:39 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Wang", "Peter", ""], ["Yan", "Zhongxia", ""], ["Zhang", "Jeff", ""]]}, {"id": "1706.08685", "submitter": "Martim Brand\\~ao", "authors": "Martim Brandao and Yukitoshi Minami Shiguematsu and Kenji Hashimoto\n  and Atsuo Takanishi", "title": "Material Recognition CNNs and Hierarchical Planning for Biped Robot\n  Locomotion on Slippery Terrain", "comments": null, "journal-ref": null, "doi": "10.1109/HUMANOIDS.2016.7803258", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the problem of visually predicting surface friction\nfor environments with diverse surfaces, and integrating this knowledge into\nbiped robot locomotion planning. The problem is essential for autonomous robot\nlocomotion since diverse surfaces with varying friction abound in the real\nworld, from wood to ceramic tiles, grass or ice, which may cause difficulties\nor huge energy costs for robot locomotion if not considered. We propose to\nestimate friction and its uncertainty from visual estimation of material\nclasses using convolutional neural networks, together with probability\ndistribution functions of friction associated with each material. We then\nrobustly integrate the friction predictions into a hierarchical (footstep and\nfull-body) planning method using chance constraints, and optimize the same\ntrajectory costs at both levels of the planning method for consistency. Our\nsolution achieves fully autonomous perception and locomotion on slippery\nterrain, which considers not only friction and its uncertainty, but also\ncollision, stability and trajectory cost. We show promising friction prediction\nresults in real pictures of outdoor scenarios, and planning experiments on a\nreal robot facing surfaces with different friction.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 06:38:53 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Brandao", "Martim", ""], ["Shiguematsu", "Yukitoshi Minami", ""], ["Hashimoto", "Kenji", ""], ["Takanishi", "Atsuo", ""]]}, {"id": "1706.08690", "submitter": "Tarik Alafif", "authors": "Tarik Alafif, Zeyad Hailat, Melih Aslan, Xuewen Chen", "title": "Large-scale Datasets: Faces with Partial Occlusions and Pose Variations\n  in the Wild", "comments": "5 pages 8 figures 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection methods have relied on face datasets for training. However,\nexisting face datasets tend to be in small scales for face learning in both\nconstrained and unconstrained environments. In this paper, we first introduce\nour large-scale image datasets, Large-scale Labeled Face (LSLF) and noisy\nLarge-scale Labeled Non-face (LSLNF). Our LSLF dataset consists of a large\nnumber of unconstrained multi-view and partially occluded faces. The faces have\nmany variations in color and grayscale, image quality, image resolution, image\nillumination, image background, image illusion, human face, cartoon face,\nfacial expression, light and severe partial facial occlusion, make up, gender,\nage, and race. Many of these faces are partially occluded with accessories such\nas tattoos, hats, glasses, sunglasses, hands, hair, beards, scarves,\nmicrophones, or other objects or persons. The LSLF dataset is currently the\nlargest labeled face image dataset in the literature in terms of the number of\nlabeled images and the number of individuals compared to other existing labeled\nface image datasets. Second, we introduce our CrowedFaces and CrowedNonFaces\nimage datasets. The crowedFaces and CrowedNonFaces datasets include faces and\nnon-faces images from crowed scenes. These datasets essentially aim for\nresearchers to provide a large number of training examples with many variations\nfor large scale face learning and face recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 07:04:51 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Alafif", "Tarik", ""], ["Hailat", "Zeyad", ""], ["Aslan", "Melih", ""], ["Chen", "Xuewen", ""]]}, {"id": "1706.08713", "submitter": "Valentina Vasco", "authors": "Valentina Vasco, Arren Glover, Elias Mueggler, Davide Scaramuzza,\n  Lorenzo Natale and Chiara Bartolozzi", "title": "Independent Motion Detection with Event-driven Cameras", "comments": "7 pages, 6 figures", "journal-ref": "International Conference on Advanced Robotics (ICAR), 2017", "doi": "10.1109/ICAR.2017.8023661", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike standard cameras that send intensity images at a constant frame rate,\nevent-driven cameras asynchronously report pixel-level brightness changes,\noffering low latency and high temporal resolution (both in the order of\nmicro-seconds). As such, they have great potential for fast and low power\nvision algorithms for robots. Visual tracking, for example, is easily achieved\neven for very fast stimuli, as only moving objects cause brightness changes.\nHowever, cameras mounted on a moving robot are typically non-stationary and the\nsame tracking problem becomes confounded by background clutter events due to\nthe robot ego-motion. In this paper, we propose a method for segmenting the\nmotion of an independently moving object for event-driven cameras. Our method\ndetects and tracks corners in the event stream and learns the statistics of\ntheir motion as a function of the robot's joint velocities when no\nindependently moving objects are present. During robot operation, independently\nmoving objects are identified by discrepancies between the predicted corner\nvelocities from ego-motion and the measured corner velocities. We validate the\nalgorithm on data collected from the neuromorphic iCub robot. We achieve a\nprecision of ~ 90 % and show that the method is robust to changes in speed of\nboth the head and the target.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 08:17:50 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 19:48:45 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Vasco", "Valentina", ""], ["Glover", "Arren", ""], ["Mueggler", "Elias", ""], ["Scaramuzza", "Davide", ""], ["Natale", "Lorenzo", ""], ["Bartolozzi", "Chiara", ""]]}, {"id": "1706.08775", "submitter": "Gabriel Oliveira", "authors": "Gabriel L. Oliveira, Noha Radwan, Wolfram Burgard, Thomas Brox", "title": "Topometric Localization with Deep Learning", "comments": "16 pages, 7 figures, ISRR 2017 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to LiDAR-based localization methods, which provide high accuracy but\nrely on expensive sensors, visual localization approaches only require a camera\nand thus are more cost-effective while their accuracy and reliability typically\nis inferior to LiDAR-based methods. In this work, we propose a vision-based\nlocalization approach that learns from LiDAR-based localization methods by\nusing their output as training data, thus combining a cheap, passive sensor\nwith an accuracy that is on-par with LiDAR-based localization. The approach\nconsists of two deep networks trained on visual odometry and topological\nlocalization, respectively, and a successive optimization to combine the\npredictions of these two networks. We evaluate the approach on a new\nchallenging pedestrian-based dataset captured over the course of six months in\nvarying weather conditions with a high degree of noise. The experiments\ndemonstrate that the localization errors are up to 10 times smaller than with\ntraditional vision-based localization methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 11:03:31 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Oliveira", "Gabriel L.", ""], ["Radwan", "Noha", ""], ["Burgard", "Wolfram", ""], ["Brox", "Thomas", ""]]}, {"id": "1706.08789", "submitter": "Tengteng Huang", "authors": "Pengyuan Lyu, Xiang Bai, Cong Yao, Zhen Zhu, Tengteng Huang, Wenyu Liu", "title": "Auto-Encoder Guided GAN for Chinese Calligraphy Synthesis", "comments": "submitted to ICADR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the Chinese calligraphy synthesis problem:\nsynthesizing Chinese calligraphy images with specified style from standard\nfont(eg. Hei font) images (Fig. 1(a)). Recent works mostly follow the stroke\nextraction and assemble pipeline which is complex in the process and limited by\nthe effect of stroke extraction. We treat the calligraphy synthesis problem as\nan image-to-image translation problem and propose a deep neural network based\nmodel which can generate calligraphy images from standard font images directly.\nBesides, we also construct a large scale benchmark that contains various styles\nfor Chinese calligraphy synthesis. We evaluate our method as well as some\nbaseline methods on the proposed dataset, and the experimental results\ndemonstrate the effectiveness of our proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 11:35:31 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Lyu", "Pengyuan", ""], ["Bai", "Xiang", ""], ["Yao", "Cong", ""], ["Zhu", "Zhen", ""], ["Huang", "Tengteng", ""], ["Liu", "Wenyu", ""]]}, {"id": "1706.08801", "submitter": "Rajendra Nagar", "authors": "Rajendra Nagar and Shanmuganathan Raman", "title": "Detecting Approximate Reflection Symmetry in a Point Set using\n  Optimization on Manifold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm to detect approximate reflection symmetry present in\na set of volumetrically distributed points belonging to $\\mathbb{R}^d$\ncontaining a distorted reflection symmetry pattern. We pose the problem of\ndetecting approximate reflection symmetry as the problem of establishing\ncorrespondences between the points which are reflections of each other and we\ndetermine the reflection symmetry transformation. We formulate an optimization\nframework in which the problem of establishing the correspondences amounts to\nsolving a linear assignment problem and the problem of determining the\nreflection symmetry transformation amounts to solving an optimization problem\non a smooth Riemannian product manifold. The proposed approach estimates the\nsymmetry from the geometry of the points and is descriptor independent. We\nevaluate the performance of the proposed approach on the standard benchmark\ndataset and achieve the state-of-the-art performance. We further show the\nrobustness of our approach by varying the amount of distortion in a perfect\nreflection symmetry pattern where we perturb each point by a different amount\nof perturbation. We demonstrate the effectiveness of the method by applying it\nto the problem of 2-D and 3-D reflection symmetry detection along with\ncomparisons.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 12:03:39 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 09:30:19 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 10:29:46 GMT"}, {"version": "v4", "created": "Wed, 10 Oct 2018 03:22:01 GMT"}, {"version": "v5", "created": "Thu, 18 Oct 2018 13:04:37 GMT"}, {"version": "v6", "created": "Tue, 15 Jan 2019 15:28:08 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Nagar", "Rajendra", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1706.08807", "submitter": "Mian Ahsan Iqbal", "authors": "Ahsan Iqbal, Alexander Richard, Hilde Kuehne, Juergen Gall", "title": "Recurrent Residual Learning for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition is a fundamental problem in computer vision with a lot of\npotential applications such as video surveillance, human computer interaction,\nand robot learning. Given pre-segmented videos, the task is to recognize\nactions happening within videos. Historically, hand crafted video features were\nused to address the task of action recognition. With the success of Deep\nConvNets as an image analysis method, a lot of extensions of standard ConvNets\nwere purposed to process variable length video data. In this work, we propose a\nnovel recurrent ConvNet architecture called recurrent residual networks to\naddress the task of action recognition. The approach extends ResNet, a state of\nthe art model for image classification. While the original formulation of\nResNet aims at learning spatial residuals in its layers, we extend the approach\nby introducing recurrent connections that allow to learn a spatio-temporal\nresidual. In contrast to fully recurrent networks, our temporal connections\nonly allow a limited range of preceding frames to contribute to the output for\nthe current frame, enabling efficient training and inference as well as\nlimiting the temporal context to a reasonable local range around each frame. On\na large-scale action recognition dataset, we show that our model improves over\nboth, the standard ResNet architecture and a ResNet extended by a fully\nrecurrent layer.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 12:08:14 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Iqbal", "Ahsan", ""], ["Richard", "Alexander", ""], ["Kuehne", "Hilde", ""], ["Gall", "Juergen", ""]]}, {"id": "1706.08917", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng, Shangxuan Wu, Fares Beainy and Kris Kitani", "title": "Rotational Rectification Network: Enabling Pedestrian Detection for\n  Mobile Vision", "comments": "published in WACV 2018. Author's homepage is\n  http://www.xinshuoweng.com/publications.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Across a majority of pedestrian detection datasets, it is typically assumed\nthat pedestrians will be standing upright with respect to the image coordinate\nsystem. This assumption, however, is not always valid for many vision-equipped\nmobile platforms such as mobile phones, UAVs or construction vehicles on rugged\nterrain. In these situations, the motion of the camera can cause images of\npedestrians to be captured at extreme angles. This can lead to very poor\npedestrian detection performance when using standard pedestrian detectors. To\naddress this issue, we propose a Rotational Rectification Network (R2N) that\ncan be inserted into any CNN-based pedestrian (or object) detector to adapt it\nto significant changes in camera rotation. The rotational rectification network\nuses a 2D rotation estimation module that passes rotational information to a\nspatial transformer network to undistort image features. To enable robust\nrotation estimation, we propose a Global Polar Pooling (GP-Pooling) operator to\ncapture rotational shifts in convolutional features. Through our experiments,\nwe show how our rotational rectification network can be used to improve the\nperformance of the state-of-the-art pedestrian detector under heavy image\nrotation by up to 45%\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 00:08:38 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 16:58:22 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 20:54:36 GMT"}, {"version": "v4", "created": "Tue, 3 Nov 2020 13:53:52 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Weng", "Xinshuo", ""], ["Wu", "Shangxuan", ""], ["Beainy", "Fares", ""], ["Kitani", "Kris", ""]]}, {"id": "1706.08924", "submitter": "Aliaa Rassem", "authors": "Aliaa Rassem, Mohammed El-Beltagy and Mohamed Saleh", "title": "Cross-Country Skiing Gears Classification using Deep Learning", "comments": "15 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Activity Recognition has witnessed a significant progress in the last\ndecade. Although a great deal of work in this field goes in recognizing normal\nhuman activities, few studies focused on identifying motion in sports.\nRecognizing human movements in different sports has high impact on\nunderstanding the different styles of humans in the play and on improving their\nperformance. As deep learning models proved to have good results in many\nclassification problems, this paper will utilize deep learning to classify\ncross-country skiing movements, known as gears, collected using a 3D\naccelerometer. It will also provide a comparison between different deep\nlearning models such as convolutional and recurrent neural networks versus\nstandard multi-layer perceptron. Results show that deep learning is more\neffective and has the highest classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 16:14:00 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Rassem", "Aliaa", ""], ["El-Beltagy", "Mohammed", ""], ["Saleh", "Mohamed", ""]]}, {"id": "1706.08948", "submitter": "Sambhav R. Jain", "authors": "Sambhav R. Jain, Kye Okabe", "title": "Training a Fully Convolutional Neural Network to Route Integrated\n  Circuits", "comments": "Code released. 8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep, fully convolutional neural network that learns to route a\ncircuit layout net with appropriate choice of metal tracks and wire class\ncombinations. Inputs to the network are the encoded layouts containing spatial\nlocation of pins to be routed. After 15 fully convolutional stages followed by\na score comparator, the network outputs 8 layout layers (corresponding to 4\nroute layers, 3 via layers and an identity-mapped pin layer) which are then\ndecoded to obtain the routed layouts. We formulate this as a binary\nsegmentation problem on a per-pixel per-layer basis, where the network is\ntrained to correctly classify pixels in each layout layer to be 'on' or 'off'.\nTo demonstrate learnability of layout design rules, we train the network on a\ndataset of 50,000 train and 10,000 validation samples that we generate based on\ncertain pre-defined layout constraints. Precision, recall and $F_1$ score\nmetrics are used to track the training progress. Our network achieves\n$F_1\\approx97\\%$ on the train set and $F_1\\approx92\\%$ on the validation set.\nWe use PyTorch for implementing our model. Code is made publicly available at\nhttps://github.com/sjain-stanford/deep-route .\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 17:20:21 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 18:37:04 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Jain", "Sambhav R.", ""], ["Okabe", "Kye", ""]]}, {"id": "1706.09077", "submitter": "Khizar Hayat", "authors": "Khizar Hayat", "title": "Super-Resolution via Deep Learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.dsp.2018.07.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent phenomenal interest in convolutional neural networks (CNNs) must\nhave made it inevitable for the super-resolution (SR) community to explore its\npotential. The response has been immense and in the last three years, since the\nadvent of the pioneering work, there appeared too many works not to warrant a\ncomprehensive survey. This paper surveys the SR literature in the context of\ndeep learning. We focus on the three important aspects of multimedia - namely\nimage, video and multi-dimensions, especially depth maps. In each case, first\nrelevant benchmarks are introduced in the form of datasets and state of the art\nSR methods, excluding deep learning. Next is a detailed analysis of the\nindividual works, each including a short description of the method and a\ncritique of the results with special reference to the benchmarking done. This\nis followed by minimum overall benchmarking in the form of comparison on some\ncommon dataset, while relying on the results reported in various works.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 00:02:18 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Hayat", "Khizar", ""]]}, {"id": "1706.09092", "submitter": "Yong Xia", "authors": "Jianpeng Zhang, Yong Xia, Qi Wu and Yutong Xie", "title": "Classification of Medical Images and Illustrations in the Biomedical\n  Literature Using Synergic Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Classification of medical images and illustrations in the literature aims\nto label a medical image according to the modality it was produced or label an\nillustration according to its production attributes. It is an essential and\nchallenging research hotspot in the area of automated literature review,\nretrieval and mining. The significant intra-class variation and inter-class\nsimilarity caused by the diverse imaging modalities and various illustration\ntypes brings a great deal of difficulties to the problem. In this paper, we\npropose a synergic deep learning (SDL) model to address this issue.\nSpecifically, a dual deep convolutional neural network with a synergic signal\nsystem is designed to mutually learn image representation. The synergic signal\nis used to verify whether the input image pair belongs to the same category and\nto give the corrective feedback if a synergic error exists. Our SDL model can\nbe trained 'end to end'. In the test phase, the class label of an input can be\npredicted by averaging the likelihood probabilities obtained by two\nconvolutional neural network components. Experimental results on the\nImageCLEF2016 Subfigure Classification Challenge suggest that our proposed SDL\nmodel achieves the state-of-the art performance in this medical image\nclassification problem and its accuracy is higher than that of the first place\nsolution on the Challenge leader board so far.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 01:15:06 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Zhang", "Jianpeng", ""], ["Xia", "Yong", ""], ["Wu", "Qi", ""], ["Xie", "Yutong", ""]]}, {"id": "1706.09119", "submitter": "Jiawei Huang", "authors": "Jiawei Huang, Zhaowen Wang", "title": "Robust Lane Tracking with Multi-mode Observation Model and Particle\n  Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic lane tracking involves estimating the underlying signal from a\nsequence of noisy signal observations. Many models and methods have been\nproposed for lane tracking, and dynamic targets tracking in general. The Kalman\nFilter is a widely used method that works well on linear Gaussian models. But\nthis paper shows that Kalman Filter is not suitable for lane tracking, because\nits Gaussian observation model cannot faithfully represent the procured\nobservations. We propose using a Particle Filter on top of a novel multiple\nmode observation model. Experiments show that our method produces superior\nperformance to a conventional Kalman Filter.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 04:08:02 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Huang", "Jiawei", ""], ["Wang", "Zhaowen", ""]]}, {"id": "1706.09138", "submitter": "Dacheng Tao", "authors": "Chaoyue Wang, Chang Xu, Chaohui Wang, Dacheng Tao", "title": "Perceptual Adversarial Networks for Image-to-Image Transformation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2836316", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a principled Perceptual Adversarial Networks (PAN)\nfor image-to-image transformation tasks. Unlike existing application-specific\nalgorithms, PAN provides a generic framework of learning mapping relationship\nbetween paired images (Fig. 1), such as mapping a rainy image to its de-rained\ncounterpart, object edges to its photo, semantic labels to a scenes image, etc.\nThe proposed PAN consists of two feed-forward convolutional neural networks\n(CNNs), the image transformation network T and the discriminative network D.\nThrough combining the generative adversarial loss and the proposed perceptual\nadversarial loss, these two networks can be trained alternately to solve\nimage-to-image transformation tasks. Among them, the hidden layers and output\nof the discriminative network D are upgraded to continually and automatically\ndiscover the discrepancy between the transformed image and the corresponding\nground-truth. Simultaneously, the image transformation network T is trained to\nminimize the discrepancy explored by the discriminative network D. Through the\nadversarial training process, the image transformation network T will\ncontinually narrow the gap between transformed images and ground-truth images.\nExperiments evaluated on several image-to-image transformation tasks (e.g.,\nimage de-raining, image inpainting, etc.) show that the proposed PAN\noutperforms many related state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 07:04:08 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 02:44:53 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Wang", "Chaoyue", ""], ["Xu", "Chang", ""], ["Wang", "Chaohui", ""], ["Tao", "Dacheng", ""]]}, {"id": "1706.09180", "submitter": "Neil Ma", "authors": "Liangzhuang Ma, Xin Kan, Qianjiang Xiao, Wenlong Liu, Peiqin Sun", "title": "Yes-Net: An effective Detector Based on Global Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new real-time object detection approach named\nYes-Net. It realizes the prediction of bounding boxes and class via single\nneural network like YOLOv2 and SSD, but owns more efficient and outstanding\nfeatures. It combines local information with global information by adding the\nRNN architecture as a packed unit in CNN model to form the basic feature\nextractor. Independent anchor boxes coming from full-dimension k-means is also\napplied in Yes-Net, it brings better average IOU than grid anchor box. In\naddition, instead of NMS, Yes-Net uses RNN as a filter to get the final boxes,\nwhich is more efficient. For 416 x 416 input, Yes-Net achieves 79.2% mAP on\nVOC2007 test at 39 FPS on an Nvidia Titan X Pascal.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 09:16:18 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 07:14:40 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Ma", "Liangzhuang", ""], ["Kan", "Xin", ""], ["Xiao", "Qianjiang", ""], ["Liu", "Wenlong", ""], ["Sun", "Peiqin", ""]]}, {"id": "1706.09193", "submitter": "Mohak Sukhwani", "authors": "Mohak Sukhwani, Ravi Kothari", "title": "A Parameterized Approach to Personalized Variable Length Summarization\n  of Soccer Matches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parameterized approach to produce personalized variable length\nsummaries of soccer matches. Our approach is based on temporally segmenting the\nsoccer video into 'plays', associating a user-specifiable 'utility' for each\ntype of play and using 'bin-packing' to select a subset of the plays that add\nup to the desired length while maximizing the overall utility (volume in\nbin-packing terms). Our approach systematically allows a user to override the\ndefault weights assigned to each type of play with individual preferences and\nthus see a highly personalized variable length summarization of soccer matches.\nWe demonstrate our approach based on the output of an end-to-end pipeline that\nwe are building to produce such summaries. Though aspects of the overall\nend-to-end pipeline are human assisted at present, the results clearly show\nthat the proposed approach is capable of producing semantically meaningful and\ncompelling summaries. Besides the obvious use of producing summaries of\nsuperior league matches for news broadcasts, we anticipate our work to promote\ngreater awareness of the local matches and junior leagues by producing\nconsumable summaries of them.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 09:57:05 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Sukhwani", "Mohak", ""], ["Kothari", "Ravi", ""]]}, {"id": "1706.09262", "submitter": "Adam Kosiorek", "authors": "Adam R. Kosiorek, Alex Bewley, Ingmar Posner", "title": "Hierarchical Attentive Recurrent Tracking", "comments": "Published as a conference paper at NIPS 2017. Code is available at\n  https://github.com/akosiorek/hart and qualitative results are available at\n  https://youtu.be/Vvkjm0FRGSs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class-agnostic object tracking is particularly difficult in cluttered\nenvironments as target specific discriminative models cannot be learned a\npriori. Inspired by how the human visual cortex employs spatial attention and\nseparate \"where\" and \"what\" processing pathways to actively suppress irrelevant\nvisual features, this work develops a hierarchical attentive recurrent model\nfor single object tracking in videos. The first layer of attention discards the\nmajority of background by selecting a region containing the object of interest,\nwhile the subsequent layers tune in on visual features particular to the\ntracked object. This framework is fully differentiable and can be trained in a\npurely data driven fashion by gradient methods. To improve training\nconvergence, we augment the loss function with terms for a number of auxiliary\ntasks relevant for tracking. Evaluation of the proposed model is performed on\ntwo datasets: pedestrian tracking on the KTH activity recognition dataset and\nthe more difficult KITTI object tracking dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 13:00:14 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 14:35:08 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Kosiorek", "Adam R.", ""], ["Bewley", "Alex", ""], ["Posner", "Ingmar", ""]]}, {"id": "1706.09274", "submitter": "Haosheng Zou", "authors": "Haosheng Zou, Kun Xu, Jialian Li, Jun Zhu", "title": "The YouTube-8M Kaggle Competition: Challenges and Methods", "comments": "accepted to CVPR'17 Workshop on YouTube-8M Large-Scale Video\n  Understanding (oral presentation); code is at\n  https://github.com/taufikxu/youtube on branches kunxu and zhs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We took part in the YouTube-8M Video Understanding Challenge hosted on\nKaggle, and achieved the 10th place within less than one month's time. In this\npaper, we present an extensive analysis and solution to the underlying\nmachine-learning problem based on frame-level data, where major challenges are\nidentified and corresponding preliminary methods are proposed. It's noteworthy\nthat, with merely the proposed strategies and uniformly-averaging multi-crop\nensemble was it sufficient for us to reach our ranking. We also report the\nmethods we believe to be promising but didn't have enough time to train to\nconvergence. We hope this paper could serve, to some extent, as a review and\nguideline of the YouTube-8M multi-label video classification benchmark,\ninspiring future attempts and research.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 13:20:51 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 05:30:37 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Zou", "Haosheng", ""], ["Xu", "Kun", ""], ["Li", "Jialian", ""], ["Zhu", "Jun", ""]]}, {"id": "1706.09302", "submitter": "Rodrigo Berriel", "authors": "Rodrigo F. Berriel, Andre Teixeira Lopes, Alberto F. de Souza, Thiago\n  Oliveira-Santos", "title": "Deep Learning Based Large-Scale Automatic Satellite Crosswalk\n  Classification", "comments": "5 pages, 3 figures, accepted by IEEE Geoscience and Remote Sensing\n  Letters", "journal-ref": null, "doi": "10.1109/LGRS.2017.2719863", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution satellite imagery have been increasingly used on remote\nsensing classification problems. One of the main factors is the availability of\nthis kind of data. Even though, very little effort has been placed on the zebra\ncrossing classification problem. In this letter, crowdsourcing systems are\nexploited in order to enable the automatic acquisition and annotation of a\nlarge-scale satellite imagery database for crosswalks related tasks. Then, this\ndataset is used to train deep-learning-based models in order to accurately\nclassify satellite images that contains or not zebra crossings. A novel dataset\nwith more than 240,000 images from 3 continents, 9 countries and more than 20\ncities was used in the experiments. Experimental results showed that freely\navailable crowdsourcing data can be used to accurately (97.11%) train robust\nmodels to perform crosswalk classification on a global scale.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 14:06:24 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 12:58:35 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Berriel", "Rodrigo F.", ""], ["Lopes", "Andre Teixeira", ""], ["de Souza", "Alberto F.", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "1706.09308", "submitter": "Eric Keiji", "authors": "Eric Keiji, Gabriel Ferreira, Claudio Silva, Roberto M. Cesar Jr", "title": "A New Urban Objects Detection Framework Using Weakly Annotated Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban informatics explore data science methods to address different urban\nissues intensively based on data. The large variety and quantity of data\navailable should be explored but this brings important challenges. For\ninstance, although there are powerful computer vision methods that may be\nexplored, they may require large annotated datasets. In this work we propose a\nnovel approach to automatically creating an object recognition system with\nminimal manual annotation. The basic idea behind the method is to use large\ninput datasets using available online cameras on large cities. A off-the-shelf\nweak classifier is used to detect an initial set of urban elements of interest\n(e.g. cars, pedestrians, bikes, etc.). Such initial dataset undergoes a quality\ncontrol procedure and it is subsequently used to fine tune a strong classifier.\nQuality control and comparative performance assessment are used as part of the\npipeline. We evaluate the method for detecting cars based on monitoring\ncameras. Experimental results using real data show that despite losing\ngenerality, the final detector provides better detection rates tailored to the\nselected cameras. The programmed robot gathered 770 video hours from 24 online\ncity cameras (\\~300GB), which has been fed to the proposed system. Our approach\nhas shown that the method nearly doubled the recall (93\\%) with respect to\nstate-of-the-art methods using off-the-shelf algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 14:16:56 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 14:38:55 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Keiji", "Eric", ""], ["Ferreira", "Gabriel", ""], ["Silva", "Claudio", ""], ["Cesar", "Roberto M.", "Jr"]]}, {"id": "1706.09317", "submitter": "Qian Wang", "authors": "Qian Wang and Ke Chen", "title": "Alternative Semantic Representations for Zero-Shot Human Action\n  Recognition", "comments": "Technical Report, School of Computer Science, The University of\n  Manchester, Accepted to ECML-PKDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A proper semantic representation for encoding side information is key to the\nsuccess of zero-shot learning. In this paper, we explore two alternative\nsemantic representations especially for zero-shot human action recognition:\ntextual descriptions of human actions and deep features extracted from still\nimages relevant to human actions. Such side information are accessible on Web\nwith little cost, which paves a new way in gaining side information for\nlarge-scale zero-shot human action recognition. We investigate different\nencoding methods to generate semantic representations for human actions from\nsuch side information. Based on our zero-shot visual recognition method, we\nconducted experiments on UCF101 and HMDB51 to evaluate two proposed semantic\nrepresentations . The results suggest that our proposed text- and image-based\nsemantic representations outperform traditional attributes and word vectors\nconsiderably for zero-shot human action recognition. In particular, the\nimage-based semantic representations yield the favourable performance even\nthough the representation is extracted from a small number of images per class.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 14:32:57 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Wang", "Qian", ""], ["Chen", "Ke", ""]]}, {"id": "1706.09318", "submitter": "Jaemin Son", "authors": "Jaemin Son, Sang Jun Park, and Kyu-Hwan Jung", "title": "Retinal Vessel Segmentation in Fundoscopic Images with Generative\n  Adversarial Networks", "comments": "9 pages, submitted to DLMIA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal vessel segmentation is an indispensable step for automatic detection\nof retinal diseases with fundoscopic images. Though many approaches have been\nproposed, existing methods tend to miss fine vessels or allow false positives\nat terminal branches. Let alone under-segmentation, over-segmentation is also\nproblematic when quantitative studies need to measure the precise width of\nvessels. In this paper, we present a method that generates the precise map of\nretinal vessels using generative adversarial training. Our methods achieve dice\ncoefficient of 0.829 on DRIVE dataset and 0.834 on STARE dataset which is the\nstate-of-the-art performance on both datasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 14:33:22 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Son", "Jaemin", ""], ["Park", "Sang Jun", ""], ["Jung", "Kyu-Hwan", ""]]}, {"id": "1706.09364", "submitter": "Paul Voigtlaender", "authors": "Paul Voigtlaender and Bastian Leibe", "title": "Online Adaptation of Convolutional Neural Networks for Video Object\n  Segmentation", "comments": "Accepted at BMVC 2017. This version contains minor changes for the\n  camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the task of semi-supervised video object segmentation, i.e.\nsegmenting the pixels belonging to an object in the video using the ground\ntruth pixel mask for the first frame. We build on the recently introduced\none-shot video object segmentation (OSVOS) approach which uses a pretrained\nnetwork and fine-tunes it on the first frame. While achieving impressive\nperformance, at test time OSVOS uses the fine-tuned network in unchanged form\nand is not able to adapt to large changes in object appearance. To overcome\nthis limitation, we propose Online Adaptive Video Object Segmentation (OnAVOS)\nwhich updates the network online using training examples selected based on the\nconfidence of the network and the spatial configuration. Additionally, we add a\npretraining step based on objectness, which is learned on PASCAL. Our\nexperiments show that both extensions are highly effective and improve the\nstate of the art on DAVIS to an intersection-over-union score of 85.7%.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 17:02:39 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 15:18:18 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Voigtlaender", "Paul", ""], ["Leibe", "Bastian", ""]]}, {"id": "1706.09430", "submitter": "Carlos Torres", "authors": "Carlos Torres, Kenneth Rose, Jeffrey C. Fried, and B. S. Manjunath", "title": "Summarization of ICU Patient Motion from Multimodal Multiview Videos", "comments": "Total of 17 pages: 1-12 (body), 13-16 (result figures), and\n  17(references) Number of figures: 21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical observations indicate that during critical care at the hospitals,\npatients sleep positioning and motion affect recovery. Unfortunately, there is\nno formal medical protocol to record, quantify, and analyze patient motion.\nThere is a small number of clinical studies, which use manual analysis of sleep\nposes and motion recordings to support medical benefits of patient positioning\nand motion monitoring. Manual processes are not scalable, are prone to human\nerrors, and strain an already taxed healthcare workforce. This study introduces\nDECU (Deep Eye-CU): an autonomous mulitmodal multiview system, which addresses\nthese issues by autonomously monitoring healthcare environments and enabling\nthe recording and analysis of patient sleep poses and motion. DECU uses three\nRGB-D cameras to monitor patient motion in a medical Intensive Care Unit (ICU).\nThe algorithms in DECU estimate pose direction at different temporal\nresolutions and use keyframes to efficiently represent pose transition\ndynamics. DECU combines deep features computed from the data with a modified\nversion of Hidden Markov Model to more flexibly model sleep pose duration,\nanalyze pose patterns, and summarize patient motion. Extensive experimental\nresults are presented. The performance of DECU is evaluated in ideal (BC:\nBright and Clear/occlusion-free) and natural (DO: Dark and Occluded) scenarios\nat two motion resolutions in a mock-up and a real ICU. The results indicate\nthat deep features allow DECU to match the classification performance of\nengineered features in BC scenes and increase the accuracy by up to 8% in DO\nscenes. In addition, the overall pose history summarization tracing accuracy\nshows an average detection rate of 85% in BC and of 76% in DO scenes. The\nproposed keyframe estimation algorithm allows DECU to reach an average 78%\ntransition classification accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 18:08:37 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Torres", "Carlos", ""], ["Rose", "Kenneth", ""], ["Fried", "Jeffrey C.", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1706.09443", "submitter": "Michal Balazia", "authors": "Michal Balazia and Petr Sojka", "title": "You Are How You Walk: Uncooperative MoCap Gait Identification for Video\n  Surveillance with Incomplete and Noisy Data", "comments": "Preprint. Full paper accepted at the IEEE/IAPR International Joint\n  Conference on Biometrics (IJCB), Denver, Colorado, USA, Oct 2017. 6 pages + 2\n  pages referenes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work offers a design of a video surveillance system based on a soft\nbiometric -- gait identification from MoCap data. The main focus is on two\nsubstantial issues of the video surveillance scenario: (1) the walkers do not\ncooperate in providing learning data to establish their identities and (2) the\ndata are often noisy or incomplete. We show that only a few examples of human\ngait cycles are required to learn a projection of raw MoCap data onto a\nlow-dimensional sub-space where the identities are well separable. Latent\nfeatures learned by Maximum Margin Criterion (MMC) method discriminate better\nthan any collection of geometric features. The MMC method is also highly robust\nto noisy data and works properly even with only a fraction of joints tracked.\nThe overall workflow of the design is directly applicable for a day-to-day\noperation based on the available MoCap technology and algorithms for gait\nanalysis. In the concept we introduce, a walker's identity is represented by a\ncluster of gait data collected at their incidents within the surveillance\nsystem: They are how they walk.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 18:47:57 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 08:57:02 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Balazia", "Michal", ""], ["Sojka", "Petr", ""]]}, {"id": "1706.09450", "submitter": "Ryan Cunningham", "authors": "Ryan J. Cunningham, Peter J. Harding, Ian D. Loram", "title": "The application of deep convolutional neural networks to ultrasound for\n  modelling of dynamic states within human skeletal muscle", "comments": "paper in preparation for submission to IEEE TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the fully automatic direct in vivo measurement of active\nand passive dynamic skeletal muscle states using ultrasound imaging. Despite\nthe long standing medical need (myopathies, neuropathies, pain, injury,\nageing), currently technology (electromyography, dynamometry, shear wave\nimaging) provides no general, non-invasive method for online estimation of\nskeletal intramuscular states. Ultrasound provides a technology in which static\nand dynamic muscle states can be observed non-invasively, yet current\ncomputational image understanding approaches are inadequate. We propose a new\napproach in which deep learning methods are used for understanding the content\nof ultrasound images of muscle in terms of its measured state. Ultrasound data\nsynchronized with electromyography of the calf muscles, with measures of joint\ntorque/angle were recorded from 19 healthy participants (6 female, ages: 30 +-\n7.7). A segmentation algorithm previously developed by our group was applied to\nextract a region of interest of the medial gastrocnemius. Then a deep\nconvolutional neural network was trained to predict the measured states (joint\nangle/torque, electromyography) directly from the segmented images. Results\nrevealed for the first time that active and passive muscle states can be\nmeasured directly from standard b-mode ultrasound images, accurately predicting\nfor a held out test participant changes in the joint angle, electromyography,\nand torque with as little error as 0.022{\\deg}, 0.0001V, 0.256Nm (root mean\nsquare error) respectively.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 19:18:04 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Cunningham", "Ryan J.", ""], ["Harding", "Peter J.", ""], ["Loram", "Ian D.", ""]]}, {"id": "1706.09498", "submitter": "Yehya Abouelnaga", "authors": "Yehya Abouelnaga, Hesham M. Eraqi, and Mohamed N. Moustafa", "title": "Real-time Distracted Driver Posture Classification", "comments": null, "journal-ref": "32nd Conference on Neural Information Processing Systems (NIPS\n  2018), Workshop on Machine Learning for Intelligent Transportation Systems", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new dataset for \"distracted driver\" posture\nestimation. In addition, we propose a novel system that achieves 95.98% driving\nposture estimation classification accuracy. The system consists of a\ngenetically-weighted ensemble of Convolutional Neural Networks (CNNs). We show\nthat a weighted ensemble of classifiers using a genetic algorithm yields in\nbetter classification confidence. We also study the effect of different visual\nelements (i.e. hands and face) in distraction detection and classification by\nmeans of face and hand localizations. Finally, we present a thinned version of\nour ensemble that could achieve a 94.29% classification accuracy and operate in\na realtime environment.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 22:13:10 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 13:14:44 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 21:50:16 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Abouelnaga", "Yehya", ""], ["Eraqi", "Hesham M.", ""], ["Moustafa", "Mohamed N.", ""]]}, {"id": "1706.09544", "submitter": "Aditya Vora", "authors": "Aditya Vora and Shanmuganathan Raman", "title": "Flow-free Video Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting foreground object from a video is a challenging task because of\nthe large deformations of the objects, occlusions, and background clutter. In\nthis paper, we propose a frame-by-frame but computationally efficient approach\nfor video object segmentation by clustering visually similar generic object\nsegments throughout the video. Our algorithm segments various object instances\nappearing in the video and then perform clustering in order to group visually\nsimilar segments into one cluster. Since the object that needs to be segmented\nappears in most part of the video, we can retrieve the foreground segments from\nthe cluster having maximum number of segments, thus filtering out noisy\nsegments that do not represent any object. We then apply a track and fill\napproach in order to localize the objects in the frames where the object\nsegmentation framework fails to segment any object. Our algorithm performs\ncomparably to the recent automatic methods for video object segmentation when\nbenchmarked on DAVIS dataset while being computationally much faster.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 02:15:15 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Vora", "Aditya", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1706.09563", "submitter": "Brendt Wohlberg", "authors": "Jialin Liu and Cristina Garcia-Cardona and Brendt Wohlberg and Wotao\n  Yin", "title": "Online Convolutional Dictionary Learning", "comments": "Accepted to be presented at ICIP 2017", "journal-ref": "Proceedings of IEEE International Conference on Image Processing\n  (ICIP), 2017, pp. 1707-1711", "doi": "10.1109/ICIP.2017.8296573", "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a number of different algorithms have recently been proposed for\nconvolutional dictionary learning, this remains an expensive problem. The\nsingle biggest impediment to learning from large training sets is the memory\nrequirements, which grow at least linearly with the size of the training set\nsince all existing methods are batch algorithms. The work reported here\naddresses this limitation by extending online dictionary learning ideas to the\nconvolutional context.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 03:25:32 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 20:14:46 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Liu", "Jialin", ""], ["Garcia-Cardona", "Cristina", ""], ["Wohlberg", "Brendt", ""], ["Yin", "Wotao", ""]]}, {"id": "1706.09579", "submitter": "Yingying Jiang", "authors": "Yingying Jiang, Xiangyu Zhu, Xiaobing Wang, Shuli Yang, Wei Li, Hua\n  Wang, Pei Fu and Zhenbo Luo", "title": "R2CNN: Rotational Region CNN for Orientation Robust Scene Text Detection", "comments": "8 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method called Rotational Region CNN (R2CNN)\nfor detecting arbitrary-oriented texts in natural scene images. The framework\nis based on Faster R-CNN [1] architecture. First, we use the Region Proposal\nNetwork (RPN) to generate axis-aligned bounding boxes that enclose the texts\nwith different orientations. Second, for each axis-aligned text box proposed by\nRPN, we extract its pooled features with different pooled sizes and the\nconcatenated features are used to simultaneously predict the text/non-text\nscore, axis-aligned box and inclined minimum area box. At last, we use an\ninclined non-maximum suppression to get the detection results. Our approach\nachieves competitive results on text detection benchmarks: ICDAR 2015 and ICDAR\n2013.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 05:00:38 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 13:01:52 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Jiang", "Yingying", ""], ["Zhu", "Xiangyu", ""], ["Wang", "Xiaobing", ""], ["Yang", "Shuli", ""], ["Li", "Wei", ""], ["Wang", "Hua", ""], ["Fu", "Pei", ""], ["Luo", "Zhenbo", ""]]}, {"id": "1706.09598", "submitter": "Dorothy Chang", "authors": "Dorothy Chang", "title": "CS591 Report: Application of siamesa network in 2D transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been extensively used various aspects of computer vision\narea. Deep learning separate itself from traditional neural network by having a\nmuch deeper and complicated network layers in its network structures.\nTraditionally, deep neural network is abundantly used in computer vision tasks\nincluding classification and detection and has achieve remarkable success and\nset up a new state of the art results in these fields. Instead of using neural\nnetwork for vision recognition and detection. I will show the ability of neural\nnetwork to do image registration, synthesis of images and image retrieval in\nthis report.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 07:13:42 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Chang", "Dorothy", ""]]}, {"id": "1706.09601", "submitter": "Li Zhang", "authors": "Li Zhang, Flood Sung, Feng Liu, Tao Xiang, Shaogang Gong, Yongxin\n  Yang, Timothy M. Hospedales", "title": "Actor-Critic Sequence Training for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating natural language descriptions of images is an important capability\nfor a robot or other visual-intelligence driven AI agent that may need to\ncommunicate with human users about what it is seeing. Such image captioning\nmethods are typically trained by maximising the likelihood of ground-truth\nannotated caption given the image. While simple and easy to implement, this\napproach does not directly maximise the language quality metrics we care about\nsuch as CIDEr. In this paper we investigate training image captioning methods\nbased on actor-critic reinforcement learning in order to directly optimise\nnon-differentiable quality metrics of interest. By formulating a per-token\nadvantage and value computation strategy in this novel reinforcement learning\nbased captioning model, we show that it is possible to achieve the state of the\nart performance on the widely used MSCOCO benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 07:26:05 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 01:32:37 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Zhang", "Li", ""], ["Sung", "Flood", ""], ["Liu", "Feng", ""], ["Xiang", "Tao", ""], ["Gong", "Shaogang", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1706.09634", "submitter": "Jan M. K\\\"ohler", "authors": "Waleed M. Gondal, Jan M. K\\\"ohler, Ren\\'e Grzeszick, Gernot A. Fink\n  and Michael Hirsch", "title": "Weakly-supervised localization of diabetic retinopathy lesions in\n  retinal fundus images", "comments": "Accepted in Proc. IEEE International Conference on Image Processing\n  (ICIP), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) show impressive performance for image\nclassification and detection, extending heavily to the medical image domain.\nNevertheless, medical experts are sceptical in these predictions as the\nnonlinear multilayer structure resulting in a classification outcome is not\ndirectly graspable. Recently, approaches have been shown which help the user to\nunderstand the discriminative regions within an image which are decisive for\nthe CNN to conclude to a certain class. Although these approaches could help to\nbuild trust in the CNNs predictions, they are only slightly shown to work with\nmedical image data which often poses a challenge as the decision for a class\nrelies on different lesion areas scattered around the entire image. Using the\nDiaretDB1 dataset, we show that on retina images different lesion areas\nfundamental for diabetic retinopathy are detected on an image level with high\naccuracy, comparable or exceeding supervised methods. On lesion level, we\nachieve few false positives with high sensitivity, though, the network is\nsolely trained on image-level labels which do not include information about\nexisting lesions. Classifying between diseased and healthy images, we achieve\nan AUC of 0.954 on the DiaretDB1.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 09:15:56 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Gondal", "Waleed M.", ""], ["K\u00f6hler", "Jan M.", ""], ["Grzeszick", "Ren\u00e9", ""], ["Fink", "Gernot A.", ""], ["Hirsch", "Michael", ""]]}, {"id": "1706.09650", "submitter": "Dong-Ju Jeong", "authors": "Dong-ju Jeong, Insung Hwang, Nam Ik Cho", "title": "Co-salient Object Detection Based on Deep Saliency Networks and Seed\n  Propagation over an Integrated Graph", "comments": "13 pages, 10 figures, 3 tables", "journal-ref": null, "doi": "10.1109/TIP.2018.2859752", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a co-salient object detection method to find common\nsalient regions in a set of images. We utilize deep saliency networks to\ntransfer co-saliency prior knowledge and better capture high-level semantic\ninformation, and the resulting initial co-saliency maps are enhanced by seed\npropagation steps over an integrated graph. The deep saliency networks are\ntrained in a supervised manner to avoid online weakly supervised learning and\nexploit them not only to extract high-level features but also to produce both\nintra- and inter-image saliency maps. Through a refinement step, the initial\nco-saliency maps can uniformly highlight co-salient regions and locate accurate\nobject boundaries. To handle input image groups inconsistent in size, we\npropose to pool multi-regional descriptors including both within-segment and\nwithin-group information. In addition, the integrated multilayer graph is\nconstructed to find the regions that the previous steps may not detect by seed\npropagation with low-level descriptors. In this work, we utilize the useful\ncomplementary components of high-, low-level information, and several\nlearning-based steps. Our experiments have demonstrated that the proposed\napproach outperforms comparable co-saliency detection methods on widely used\npublic databases and can also be directly applied to co-segmentation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 09:40:48 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Jeong", "Dong-ju", ""], ["Hwang", "Insung", ""], ["Cho", "Nam Ik", ""]]}, {"id": "1706.09719", "submitter": "Aditya Vora", "authors": "Aditya Vora and Shanmuganathan Raman", "title": "Iterative Spectral Clustering for Unsupervised Object Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of unsupervised object localization in an\nimage. Unlike previous supervised and weakly supervised algorithms that require\nbounding box or image level annotations for training classifiers in order to\nlearn features representing the object, we propose a simple yet effective\ntechnique for localization using iterative spectral clustering. This iterative\nspectral clustering approach along with appropriate cluster selection strategy\nin each iteration naturally helps in searching of object region in the image.\nIn order to estimate the final localization window, we group the proposals\nobtained from the iterative spectral clustering step based on the perceptual\nsimilarity, and average the coordinates of the proposals from the top scoring\ngroups. We benchmark our algorithm on challenging datasets like Object\nDiscovery and PASCAL VOC 2007, achieving an average CorLoc percentage of 51%\nand 35% respectively which is comparable to various other weakly supervised\nalgorithms despite being completely unsupervised.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 12:37:44 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Vora", "Aditya", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1706.09806", "submitter": "Tanushri Chakravorty", "authors": "Tanushri Chakravorty, Guillaume-Alexandre Bilodeau, Eric Granger", "title": "Robust Face Tracking using Multiple Appearance Models and Graph\n  Relational Learning", "comments": "Paper is under consideration at CVIU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of appearance matching across different\nchallenges while doing visual face tracking in real-world scenarios. In this\npaper, FaceTrack is proposed that utilizes multiple appearance models with its\nlong-term and short-term appearance memory for efficient face tracking. It\ndemonstrates robustness to deformation, in-plane and out-of-plane rotation,\nscale, distractors and background clutter. It capitalizes on the advantages of\nthe tracking-by-detection, by using a face detector that tackles drastic scale\nappearance change of a face. The detector also helps to reinitialize FaceTrack\nduring drift. A weighted score-level fusion strategy is proposed to obtain the\nface tracking output having the highest fusion score by generating candidates\naround possible face locations. The tracker showcases impressive performance\nwhen initiated automatically by outperforming many state-of-the-art trackers,\nexcept Struck by a very minute margin: 0.001 in precision and 0.017 in success\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 15:28:24 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 16:26:36 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Chakravorty", "Tanushri", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Granger", "Eric", ""]]}, {"id": "1706.09858", "submitter": "John McKay", "authors": "John McKay, Isaac Gerg, Vishal Monga, Raghu Raj", "title": "What's Mine is Yours: Pretrained CNNs for Limited Training Sonar ATR", "comments": "Accepted to OCEANS 2017 - Anchorage (Conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding mines in Sonar imagery is a significant problem with a great deal of\nrelevance for seafaring military and commercial endeavors. Unfortunately, the\nlack of enormous Sonar image data sets has prevented automatic target\nrecognition (ATR) algorithms from some of the same advances seen in other\ncomputer vision fields. Namely, the boom in convolutional neural nets (CNNs)\nwhich have been able to achieve incredible results - even surpassing human\nactors - has not been an easily feasible route for many practitioners of Sonar\nATR. We demonstrate the power of one avenue to incorporating CNNs into Sonar\nATR: transfer learning. We first show how well a straightforward, flexible CNN\nfeature-extraction strategy can be used to obtain impressive if not\nstate-of-the-art results. Secondly, we propose a way to utilize the powerful\ntransfer learning approach towards multiple instance target detection and\nidentification within a provided synthetic aperture Sonar data set.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 17:13:37 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["McKay", "John", ""], ["Gerg", "Isaac", ""], ["Monga", "Vishal", ""], ["Raj", "Raghu", ""]]}, {"id": "1706.09876", "submitter": "Zekun Hao", "authors": "Zekun Hao, Yu Liu, Hongwei Qin, Junjie Yan, Xiu Li, Xiaolin Hu", "title": "Scale-Aware Face Detection", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) based face detectors are inefficient in\nhandling faces of diverse scales. They rely on either fitting a large single\nmodel to faces across a large scale range or multi-scale testing. Both are\ncomputationally expensive. We propose Scale-aware Face Detector (SAFD) to\nhandle scale explicitly using CNN, and achieve better performance with less\ncomputation cost. Prior to detection, an efficient CNN predicts the scale\ndistribution histogram of the faces. Then the scale histogram guides the\nzoom-in and zoom-out of the image. Since the faces will be approximately in\nuniform scale after zoom, they can be detected accurately even with much\nsmaller CNN. Actually, more than 99% of the faces in AFW can be covered with\nless than two zooms per image. Extensive experiments on FDDB, MALF and AFW show\nadvantages of SAFD.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 17:40:31 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Hao", "Zekun", ""], ["Liu", "Yu", ""], ["Qin", "Hongwei", ""], ["Yan", "Junjie", ""], ["Li", "Xiu", ""], ["Hu", "Xiaolin", ""]]}, {"id": "1706.09887", "submitter": "Lacey Best-Rowden", "authors": "Lacey Best-Rowden and Anil K. Jain", "title": "Automatic Face Image Quality Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face image quality can be defined as a measure of the utility of a face image\nto automatic face recognition. In this work, we propose (and compare) two\nmethods for automatic face image quality based on target face quality values\nfrom (i) human assessments of face image quality (matcher-independent), and\n(ii) quality values computed from similarity scores (matcher-dependent). A\nsupport vector regression model trained on face features extracted using a deep\nconvolutional neural network (ConvNet) is used to predict the quality of a face\nimage. The proposed methods are evaluated on two unconstrained face image\ndatabases, LFW and IJB-A, which both contain facial variations with multiple\nquality factors. Evaluation of the proposed automatic face image quality\nmeasures shows we are able to reduce the FNMR at 1% FMR by at least 13% for two\nface matchers (a COTS matcher and a ConvNet matcher) by using the proposed face\nquality to select subsets of face images and video frames for matching\ntemplates (i.e., multiple faces per subject) in the IJB-A protocol. To our\nknowledge, this is the first work to utilize human assessments of face image\nquality in designing a predictor of unconstrained face quality that is shown to\nbe effective in cross-database evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 17:58:48 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Best-Rowden", "Lacey", ""], ["Jain", "Anil K.", ""]]}, {"id": "1706.10071", "submitter": "Hyojin Park", "authors": "Hyojin Park, Jisoo Jeong, Youngjoon Yoo, Nojun Kwak", "title": "Superpixel-based Semantic Segmentation Trained by Statistical Process\n  Control", "comments": "Accepted in British Machine Vision Conference (BMVC), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation, like other fields of computer vision, has seen a\nremarkable performance advance by the use of deep convolution neural networks.\nHowever, considering that neighboring pixels are heavily dependent on each\nother, both learning and testing of these methods have a lot of redundant\noperations. To resolve this problem, the proposed network is trained and tested\nwith only 0.37% of total pixels by superpixel-based sampling and largely\nreduced the complexity of upsampling calculation. The hypercolumn feature maps\nare constructed by pyramid module in combination with the convolution layers of\nthe base network. Since the proposed method uses a very small number of sampled\npixels, the end-to-end learning of the entire network is difficult with a\ncommon learning rate for all the layers. In order to resolve this problem, the\nlearning rate after sampling is controlled by statistical process control (SPC)\nof gradients in each layer. The proposed method performs better than or equal\nto the conventional methods that use much more samples on Pascal Context,\nSUN-RGBD dataset.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 09:13:24 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 10:35:03 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Park", "Hyojin", ""], ["Jeong", "Jisoo", ""], ["Yoo", "Youngjoon", ""], ["Kwak", "Nojun", ""]]}, {"id": "1706.10082", "submitter": "Ippei Obayashi Mr.", "authors": "Ippei Obayashi and Yasuaki Hiraoka", "title": "Persistence Diagrams with Linear Machine Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistence diagrams have been widely recognized as a compact descriptor for\ncharacterizing multiscale topological features in data. When many datasets are\navailable, statistical features embedded in those persistence diagrams can be\nextracted by applying machine learnings. In particular, the ability for\nexplicitly analyzing the inverse in the original data space from those\nstatistical features of persistence diagrams is significantly important for\npractical applications. In this paper, we propose a unified method for the\ninverse analysis by combining linear machine learning models with persistence\nimages. The method is applied to point clouds and cubical sets, showing the\nability of the statistical inverse analysis and its advantages.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 09:33:50 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 07:19:30 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Obayashi", "Ippei", ""], ["Hiraoka", "Yasuaki", ""]]}, {"id": "1706.10197", "submitter": "Zibo Meng", "authors": "Zibo Meng, Shizhong Han, Ping Liu, Yan Tong", "title": "Improving Speech Related Facial Action Unit Recognition by Audiovisual\n  Information Fusion", "comments": "arXiv admin note: text overlap with arXiv:1706.07536", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to recognize facial action unit (AU) from spontaneous\nfacial displays, especially when they are accompanied by speech. The major\nreason is that the information is extracted from a single source, i.e., the\nvisual channel, in the current practice. However, facial activity is highly\ncorrelated with voice in natural human communications.\n  Instead of solely improving visual observations, this paper presents a novel\naudiovisual fusion framework, which makes the best use of visual and acoustic\ncues in recognizing speech-related facial AUs. In particular, a dynamic\nBayesian network (DBN) is employed to explicitly model the semantic and dynamic\nphysiological relationships between AUs and phonemes as well as measurement\nuncertainty. A pilot audiovisual AU-coded database has been collected to\nevaluate the proposed framework, which consists of a \"clean\" subset containing\nfrontal faces under well controlled circumstances and a challenging subset with\nlarge head movements and occlusions. Experiments on this database have\ndemonstrated that the proposed framework yields significant improvement in\nrecognizing speech-related AUs compared to the state-of-the-art visual-based\nmethods especially for those AUs whose visual observations are impaired during\nspeech, and more importantly also outperforms feature-level fusion methods by\nexplicitly modeling and exploiting physiological relationships between AUs and\nphonemes.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 14:36:07 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Meng", "Zibo", ""], ["Han", "Shizhong", ""], ["Liu", "Ping", ""], ["Tong", "Yan", ""]]}, {"id": "1706.10217", "submitter": "Thierry Chateau Pr", "authors": "Ala Mhalla and Thierry Chateau and Houda Maamatou and Sami Gazzah and\n  Najoua Essoukri Ben Amara", "title": "SMC Faster R-CNN: Toward a scene-specialized multi-object detector", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2017.06.008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generally, the performance of a generic detector decreases significantly when\nit is tested on a specific scene due to the large variation between the source\ntraining dataset and the samples from the target scene. To solve this problem,\nwe propose a new formalism of transfer learning based on the theory of a\nSequential Monte Carlo (SMC) filter to automatically specialize a\nscene-specific Faster R-CNN detector. The suggested framework uses different\nstrategies based on the SMC filter steps to approximate iteratively the target\ndistribution as a set of samples in order to specialize the Faster R-CNN\ndetector towards a target scene. Moreover, we put forward a likelihood function\nthat combines spatio-temporal information extracted from the target video\nsequence and the confidence-score given by the output layer of the Faster\nR-CNN, to favor the selection of target samples associated with the right\nlabel. The effectiveness of the suggested framework is demonstrated through\nexperiments on several public traffic datasets. Compared with the\nstate-of-the-art specialization frameworks, the proposed framework presents\nencouraging results for both single and multi-traffic object detections.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 14:31:12 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Mhalla", "Ala", ""], ["Chateau", "Thierry", ""], ["Maamatou", "Houda", ""], ["Gazzah", "Sami", ""], ["Amara", "Najoua Essoukri Ben", ""]]}, {"id": "1706.10241", "submitter": "Jorge Calvo-Zaragoza", "authors": "Jorge Calvo-Zaragoza and Antonio-Javier Gallego", "title": "A selectional auto-encoder approach for document image binarization", "comments": "Published in Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2018.08.011", "report-no": null, "categories": "cs.CV cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Binarization plays a key role in the automatic information retrieval from\ndocument images. This process is usually performed in the first stages of\ndocuments analysis systems, and serves as a basis for subsequent steps. Hence\nit has to be robust in order to allow the full analysis workflow to be\nsuccessful. Several methods for document image binarization have been proposed\nso far, most of which are based on hand-crafted image processing strategies.\nRecently, Convolutional Neural Networks have shown an amazing performance in\nmany disparate duties related to computer vision. In this paper we discuss the\nuse of convolutional auto-encoders devoted to learning an end-to-end map from\nan input image to its selectional output, in which activations indicate the\nlikelihood of pixels to be either foreground or background. Once trained,\ndocuments can therefore be binarized by parsing them through the model and\napplying a threshold. This approach has proven to outperform existing\nbinarization strategies in a number of document domains.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 15:35:13 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 13:18:51 GMT"}, {"version": "v3", "created": "Thu, 6 Sep 2018 09:32:09 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Calvo-Zaragoza", "Jorge", ""], ["Gallego", "Antonio-Javier", ""]]}, {"id": "1706.10266", "submitter": "Paria Mehrani", "authors": "Paria Mehrani, Andrei Mouraviev, Oscar J. Avella Gonzalez, John K.\n  Tsotsos", "title": "Color-opponent mechanisms for local hue encoding in a hierarchical\n  framework", "comments": "16 pages, 15 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A biologically plausible computational model for color representation is\nintroduced. We present a mechanistic hierarchical model of neurons that not\nonly successfully encodes local hue, but also explicitly reveals how the\ncontributions of each visual cortical layer participating in the process can\nlead to a hue representation. Our proposed model benefits from studies on the\nvisual cortex and builds a network of single-opponent and hue-selective\nneurons. Local hue encoding is achieved through gradually increasing\nnonlinearity in terms of cone inputs to single-opponent cells. We demonstrate\nthat our model's single-opponent neurons have wide tuning curves, while the\nhue-selective neurons in our model V4 layer exhibit narrower tunings,\nresembling those in V4 of the primate visual system. Our simulation experiments\nsuggest that neurons in V4 or later layers have the capacity of encoding unique\nhues. Moreover, with a few examples, we present the possibility of spanning the\ninfinite space of physical hues by combining the hue-selective neurons in our\nmodel.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 16:47:01 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 18:34:05 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Mehrani", "Paria", ""], ["Mouraviev", "Andrei", ""], ["Gonzalez", "Oscar J. Avella", ""], ["Tsotsos", "John K.", ""]]}]