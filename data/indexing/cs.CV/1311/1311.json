[{"id": "1311.0053", "submitter": "Will Landecker", "authors": "Will Landecker and Rick Chartrand and Simon DeDeo", "title": "Robust Compressed Sensing and Sparse Coding with the Difference Map", "comments": "8 pages; Revised comparison to DM-ECME algorithm in Section 2.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compressed sensing, we wish to reconstruct a sparse signal $x$ from\nobserved data $y$. In sparse coding, on the other hand, we wish to find a\nrepresentation of an observed signal $y$ as a sparse linear combination, with\ncoefficients $x$, of elements from an overcomplete dictionary. While many\nalgorithms are competitive at both problems when $x$ is very sparse, it can be\nchallenging to recover $x$ when it is less sparse. We present the Difference\nMap, which excels at sparse recovery when sparseness is lower and noise is\nhigher. The Difference Map out-performs the state of the art with\nreconstruction from random measurements and natural image reconstruction via\nsparse coding.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 22:33:36 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2013 00:27:39 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Landecker", "Will", ""], ["Chartrand", "Rick", ""], ["DeDeo", "Simon", ""]]}, {"id": "1311.0119", "submitter": "Davide Eynard", "authors": "Davide Eynard, Artiom Kovnatsky, Michael M. Bronstein", "title": "Structure-preserving color transformations using Laplacian commutativity", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mappings between color spaces are ubiquitous in image processing problems\nsuch as gamut mapping, decolorization, and image optimization for color-blind\npeople. Simple color transformations often result in information loss and\nambiguities (for example, when mapping from RGB to grayscale), and one wishes\nto find an image-specific transformation that would preserve as much as\npossible the structure of the original image in the target color space. In this\npaper, we propose Laplacian colormaps, a generic framework for\nstructure-preserving color transformations between images. We use the image\nLaplacian to capture the structural information, and show that if the color\ntransformation between two images preserves the structure, the respective\nLaplacians have similar eigenvectors, or in other words, are approximately\njointly diagonalizable. Employing the relation between joint diagonalizability\nand commutativity of matrices, we use Laplacians commutativity as a criterion\nof color mapping quality and minimize it w.r.t. the parameters of a color\ntransformation to achieve optimal structure preservation. We show numerous\napplications of our approach, including color-to-gray conversion, gamut\nmapping, multispectral image fusion, and image optimization for color deficient\nviewers.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 08:48:36 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Eynard", "Davide", ""], ["Kovnatsky", "Artiom", ""], ["Bronstein", "Michael M.", ""]]}, {"id": "1311.0124", "submitter": "Andriyan Suksmono Bayu", "authors": "Andriyan B. Suksmono", "title": "Reconstruction of Complex-Valued Fractional Brownian Motion Fields Based\n  on Compressive Sampling and Its Application to PSF Interpolation in Weak\n  Lensing Survey", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new reconstruction method of complex-valued fractional Brownian motion\n(CV-fBm) field based on Compressive Sampling (CS) is proposed. The decay\nproperty of Fourier coefficients magnitude of the fBm signals/ fields indicates\nthat fBms are compressible. Therefore, a few numbers of samples will be\nsufficient for a CS based method to reconstruct the full field. The\neffectiveness of the proposed method is showed by simulating, random sampling,\nand reconstructing CV-fBm fields. Performance evaluation shows advantages of\nthe proposed method over boxcar filtering and thin plate methods. It is also\nfound that the reconstruction performance depends on both of the fBm's Hurst\nparameter and the number of samples, which in fact is consistent with the CS\nreconstruction theory. In contrast to other fBm or fractal interpolation\nmethods, the proposed CS based method does not require the knowledge of fractal\nparameters in the reconstruction process; the inherent sparsity is just\nsufficient for the CS to do the reconstruction. Potential applicability of the\nproposed method in weak gravitational lensing survey, particularly for\ninterpolating non-smooth PSF (Point Spread Function) distribution representing\ndistortion by a turbulent field is also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 09:11:55 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Suksmono", "Andriyan B.", ""]]}, {"id": "1311.0162", "submitter": "Olivier D'Hondt", "authors": "Olivier D'Hondt, St\\'ephane Guillaso and Olaf Hellwich", "title": "Iterative Bilateral Filtering of Polarimetric SAR Data", "comments": "Available:\n  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6509975", "journal-ref": "Selected Topics in Applied Earth Observations and Remote Sensing,\n  IEEE Journal of (Volume:6, Issue: 3 ) 2013", "doi": "10.1109/JSTARS.2013.2256881", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an iterative speckle filtering method for\npolarimetric SAR (PolSAR) images based on the bilateral filter. To locally\nadapt to the spatial structure of images, this filter relies on pixel\nsimilarities in both spatial and radiometric domains. To deal with polarimetric\ndata, we study the use of similarities based on a statistical distance called\nKullback-Leibler divergence as well as two geodesic distances on Riemannian\nmanifolds. To cope with speckle, we propose to progressively refine the result\nthanks to an iterative scheme. Experiments are run over synthetic and\nexperimental data. First, simulations are generated to study the effects of\nfiltering parameters in terms of polarimetric reconstruction error, edge\npreservation and smoothing of homogeneous areas. Comparison with other methods\nshows that our approach compares well to other state of the art methods in the\nextraction of polarimetric information and shows superior performance for edge\nrestoration and noise smoothing. The filter is then applied to experimental\ndata sets from ESAR and FSAR sensors (DLR) at L-band and S-band, respectively.\nThese last experiments show the ability of the filter to restore structures\nsuch as buildings and roads and to preserve boundaries between regions while\nachieving a high amount of smoothing in homogeneous areas.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 12:20:17 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["D'Hondt", "Olivier", ""], ["Guillaso", "St\u00e9phane", ""], ["Hellwich", "Olaf", ""]]}, {"id": "1311.0262", "submitter": "Suofei Zhang", "authors": "Suofei Zhang, Zhixin Sun, Xu Cheng, Zhenyang Wu", "title": "Tracking Deformable Parts via Dynamic Conditional Random Fields", "comments": "4 pages, 5 figures, the manuscript has been submitted to IEEE Signal\n  Processing Letters", "journal-ref": null, "doi": "10.1109/ICIP.2014.7025095", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of many advanced tracking methods in this area, tracking\ntargets with drastic variation of appearance such as deformation, view change\nand partial occlusion in video sequences is still a challenge in practical\napplications. In this letter, we take these serious tracking problems into\naccount simultaneously, proposing a dynamic graph based model to track object\nand its deformable parts at multiple resolutions. The method introduces well\nlearned structural object detection models into object tracking applications as\nprior knowledge to deal with deformation and view change. Meanwhile, it\nexplicitly formulates partial occlusion by integrating spatial potentials and\ntemporal potentials with an unparameterized occlusion handling mechanism in the\ndynamic conditional random field framework. Empirical results demonstrate that\nthe method outperforms state-of-the-art trackers on different challenging video\nsequences.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 09:33:55 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Zhang", "Suofei", ""], ["Sun", "Zhixin", ""], ["Cheng", "Xu", ""], ["Wu", "Zhenyang", ""]]}, {"id": "1311.0646", "submitter": "Tomas Bj\\\"orklund", "authors": "Tomas Bj\\\"orklund, Enrico Magli", "title": "A Parallel Compressive Imaging Architecture for One-Shot Acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A limitation of many compressive imaging architectures lies in the sequential\nnature of the sensing process, which leads to long sensing times. In this paper\nwe present a novel architecture that uses fewer detectors than the number of\nreconstructed pixels and is able to acquire the image in a single acquisition.\nThis paves the way for the development of video architectures that acquire\nseveral frames per second. We specifically address the diffraction problem,\nshowing that deconvolution normally used to recover diffraction blur can be\nreplaced by convolution of the sensing matrix, and how measurements of a 0/1\nphysical sensing matrix can be converted to -1/1 compressive sensing matrix\nwithout any extra acquisitions. Simulations of our architecture show that the\nimage quality is comparable to that of a classic Compressive Imaging camera,\nwhereas the proposed architecture avoids long acquisition times due to\nsequential sensing. This one-shot procedure also allows to employ a fixed\nsensing matrix instead of a complex device such as a Digital Micro Mirror array\nor Spatial Light Modulator. It also enables imaging at bandwidths where these\nare not efficient.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 11:07:47 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Bj\u00f6rklund", "Tomas", ""], ["Magli", "Enrico", ""]]}, {"id": "1311.1132", "submitter": "Jalaluddin Qureshi", "authors": "Hamed Ketabdar, Jalaluddin Qureshi, Pan Hui", "title": "Motion and audio analysis in mobile devices for remote monitoring of\n  physical activities and user authentication", "comments": null, "journal-ref": "Journal of Location Based Services, Volume 5, Issue 3-4, 2011, pp.\n  180-200, Special Issue: The social and behavioural implications of\n  location-based services", "doi": "10.1080/17489725.2011.644331", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this article we propose the use of accelerometer embedded by default in\nsmartphone as a cost-effective, reliable and efficient way to provide remote\nphysical activity monitoring for the elderly and people requiring healthcare\nservice. Mobile phones are regularly carried by users during their day-to-day\nwork routine, physical movement information can be captured by the mobile phone\naccelerometer, processed and sent to a remote server for monitoring. The\nacceleration pattern can deliver information related to the pattern of physical\nactivities the user is engaged in. We further show how this technique can be\nextended to provide implicit real-time security by analysing unexpected\nmovements captured by the phone accelerometer, and automatically locking the\nphone in such situation to prevent unauthorised access. This technique is also\nshown to provide implicit continuous user authentication, by capturing regular\nuser movements such as walking, and requesting for re-authentication whenever\nit detects a non-regular movement.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 17:30:17 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Ketabdar", "Hamed", ""], ["Qureshi", "Jalaluddin", ""], ["Hui", "Pan", ""]]}, {"id": "1311.1223", "submitter": "M HM Krishna Prasad Dr", "authors": "Srinivasa Rao Dammavalam, Seetha Maddala and M.H.M. Krishna Prasad", "title": "Quality Assessment of Pixel-Level ImageFusion Using Fuzzy Logic", "comments": "13 pages. arXiv admin note: substantial text overlap with\n  arXiv:1212.0318", "journal-ref": "International Journal on Soft Computing ( IJSC) Vol.3, No.1,\n  February 2012", "doi": "10.5121/ijsc.2012.3102", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image fusion is to reduce uncertainty and minimize redundancy in the output\nwhile maximizing relevant information from two or more images of a scene into a\nsingle composite image that is more informative and is more suitable for visual\nperception or processing tasks like medical imaging, remote sensing, concealed\nweapon detection, weather forecasting, biometrics etc. Image fusion combines\nregistered images to produce a high quality fused image with spatial and\nspectral information. The fused image with more information will improve the\nperformance of image analysis algorithms used in different applications. In\nthis paper, we proposed a fuzzy logic method to fuse images from different\nsensors, in order to enhance the quality and compared proposed method with two\nother methods i.e. image fusion using wavelet transform and weighted average\ndiscrete wavelet transform based image fusion using genetic algorithm (here\nonwards abbreviated as GA) along with quality evaluation parameters image\nquality index (IQI), mutual information measure (MIM), root mean square error\n(RMSE), peak signal to noise ratio (PSNR), fusion factor (FF), fusion symmetry\n(FS) and fusion index (FI) and entropy. The results obtained from proposed\nfuzzy based image fusion approach improves quality of fused image as compared\nto earlier reported methods, wavelet transform based image fusion and weighted\naverage discrete wavelet transform based image fusion using genetic algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 21:13:14 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Dammavalam", "Srinivasa Rao", ""], ["Maddala", "Seetha", ""], ["Prasad", "M. H. M. Krishna", ""]]}, {"id": "1311.1279", "submitter": "Sheng Huang", "authors": "Sheng Huang and Dan Yang and Fei Yang and Yongxin Ge and Xiaohong\n  Zhang and Jiwen Lu", "title": "Face Recognition via Globality-Locality Preserving Projections", "comments": "18 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present an improved Locality Preserving Projections (LPP) method, named\nGloablity-Locality Preserving Projections (GLPP), to preserve both the global\nand local geometric structures of data. In our approach, an additional\nconstraint of the geometry of classes is imposed to the objective function of\nconventional LPP for respecting some more global manifold structures. Moreover,\nwe formulate a two-dimensional extension of GLPP (2D-GLPP) as an example to\nshow how to extend GLPP with some other statistical techniques. We apply our\nworks to face recognition on four popular face databases, namely ORL, Yale,\nFERET and LFW-A databases, and extensive experimental results demonstrate that\nthe considered global manifold information can significantly improve the\nperformance of LPP and the proposed face recognition methods outperform the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 03:16:21 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Huang", "Sheng", ""], ["Yang", "Dan", ""], ["Yang", "Fei", ""], ["Ge", "Yongxin", ""], ["Zhang", "Xiaohong", ""], ["Lu", "Jiwen", ""]]}, {"id": "1311.1406", "submitter": "Martin Takac", "authors": "Martin Tak\\'a\\v{c}, Selin Damla Ahipa\\c{s}ao\\u{g}lu, Ngai-Man Cheung,\n  Peter Richt\\'arik", "title": "TOP-SPIN: TOPic discovery via Sparse Principal component INterference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel topic discovery algorithm for unlabeled images based on\nthe bag-of-words (BoW) framework. We first extract a dictionary of visual words\nand subsequently for each image compute a visual word occurrence histogram. We\nview these histograms as rows of a large matrix from which we extract sparse\nprincipal components (PCs). Each PC identifies a sparse combination of visual\nwords which co-occur frequently in some images but seldom appear in others.\nEach sparse PC corresponds to a topic, and images whose interference with the\nPC is high belong to that topic, revealing the common parts possessed by the\nimages. We propose to solve the associated sparse PCA problems using an\nAlternating Maximization (AM) method, which we modify for purpose of\nefficiently extracting multiple PCs in a deflation scheme. Our approach attacks\nthe maximization problem in sparse PCA directly and is scalable to\nhigh-dimensional data. Experiments on automatic topic discovery and category\nprediction demonstrate encouraging performance of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 19:03:31 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Tak\u00e1\u010d", "Martin", ""], ["Ahipa\u015fao\u011flu", "Selin Damla", ""], ["Cheung", "Ngai-Man", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1311.1694", "submitter": "Ankit Chadha Mr.", "authors": "Ankit Chadha, Neha Satam and Vibha Wali", "title": "Biometric Signature Processing & Recognition Using Radial Basis Function\n  Network", "comments": "CiiT International Journal of Biometrics and Bioinformatics September\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic recognition of signature is a challenging problem which has\nreceived much attention during recent years due to its many applications in\ndifferent fields. Signature has been used for long time for verification and\nauthentication purpose. Earlier methods were manual but nowadays they are\ngetting digitized. This paper provides an efficient method to signature\nrecognition using Radial Basis Function Network. The network is trained with\nsample images in database. Feature extraction is performed before using them\nfor training. For testing purpose, an image is made to undergo\nrotation-translation-scaling correction and then given to network. The network\nsuccessfully identifies the original image and gives correct output for stored\ndatabase images also. The method provides recognition rate of approximately 80%\nfor 200 samples.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 14:37:06 GMT"}], "update_date": "2013-11-08", "authors_parsed": [["Chadha", "Ankit", ""], ["Satam", "Neha", ""], ["Wali", "Vibha", ""]]}, {"id": "1311.1838", "submitter": "Yuri Boykov", "authors": "Claudia Nieuwenhuis, Eno Toeppe, Lena Gorelick, Olga Veksler, Yuri\n  Boykov", "title": "Efficient Regularization of Squared Curvature", "comments": "8 pages, 12 figures, to appear at IEEE conference on Computer Vision\n  and Pattern Recognition (CVPR), June 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curvature has received increased attention as an important alternative to\nlength based regularization in computer vision. In contrast to length, it\npreserves elongated structures and fine details. Existing approaches are either\ninefficient, or have low angular resolution and yield results with strong block\nartifacts. We derive a new model for computing squared curvature based on\nintegral geometry. The model counts responses of straight line triple cliques.\nThe corresponding energy decomposes into submodular and supermodular pairwise\npotentials. We show that this energy can be efficiently minimized even for high\nangular resolutions using the trust region framework. Our results confirm that\nwe obtain accurate and visually pleasing solutions without strong artifacts at\nreasonable run times.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 22:08:24 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 21:29:34 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Nieuwenhuis", "Claudia", ""], ["Toeppe", "Eno", ""], ["Gorelick", "Lena", ""], ["Veksler", "Olga", ""], ["Boykov", "Yuri", ""]]}, {"id": "1311.1856", "submitter": "Yuri Boykov", "authors": "Lena Gorelick, Yuri Boykov, Olga Veksler, Ismail Ben Ayed, Andrew\n  Delong", "title": "Submodularization for Quadratic Pseudo-Boolean Optimization", "comments": "8 pages, 5 figures, to appear at IEEE conference on Computer Vision\n  and Pattern Recognition (CVPR), June 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision problems require optimization of binary non-submodular\nenergies. We propose a general optimization framework based on local submodular\napproximations (LSA). Unlike standard LP relaxation methods that linearize the\nwhole energy globally, our approach iteratively approximates the energies\nlocally. On the other hand, unlike standard local optimization methods (e.g.\ngradient descent or projection techniques) we use non-linear submodular\napproximations and optimize them without leaving the domain of integer\nsolutions. We discuss two specific LSA algorithms based on \"trust region\" and\n\"auxiliary function\" principles, LSA-TR and LSA-AUX. These methods obtain\nstate-of-the-art results on a wide range of applications outperforming many\nstandard techniques such as LBP, QPBO, and TRWS. While our paper is focused on\npairwise energies, our ideas extend to higher-order problems. The code is\navailable online (http://vision.csd.uwo.ca/code/).\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 00:29:44 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 21:05:22 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Gorelick", "Lena", ""], ["Boykov", "Yuri", ""], ["Veksler", "Olga", ""], ["Ayed", "Ismail Ben", ""], ["Delong", "Andrew", ""]]}, {"id": "1311.1939", "submitter": "Kaihua Zhang", "authors": "Kaihua Zhang and Lei Zhang and Ming-Hsuan Yang and David Zhang", "title": "Fast Tracking via Spatio-Temporal Context Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we present a simple yet fast and robust algorithm which\nexploits the spatio-temporal context for visual tracking. Our approach\nformulates the spatio-temporal relationships between the object of interest and\nits local context based on a Bayesian framework, which models the statistical\ncorrelation between the low-level features (i.e., image intensity and position)\nfrom the target and its surrounding regions. The tracking problem is posed by\ncomputing a confidence map, and obtaining the best target location by\nmaximizing an object location likelihood function. The Fast Fourier Transform\nis adopted for fast learning and detection in this work. Implemented in MATLAB\nwithout code optimization, the proposed tracker runs at 350 frames per second\non an i7 machine. Extensive experimental results show that the proposed\nalgorithm performs favorably against state-of-the-art methods in terms of\nefficiency, accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 11:29:15 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Zhang", "Kaihua", ""], ["Zhang", "Lei", ""], ["Yang", "Ming-Hsuan", ""], ["Zhang", "David", ""]]}, {"id": "1311.2014", "submitter": "Yasel Garc\\'es Su\\'arez", "authors": "Roberto Rodr\\'iguez, Esley Torres, Yasel Garc\\'es, Osvaldo Pereira,\n  Humberto Sossa", "title": "A new stopping criterion for the mean shift iterative algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mean shift iterative algorithm was proposed in 2006, for using the\nentropy as a stopping criterion. From then on, a theoretical base has been\ndeveloped and a group of applications has been carried out using this\nalgorithm. This paper proposes a new stopping criterion for the mean shift\niterative algorithm, where stopping threshold via entropy is used now, but in\nanother way. Many segmentation experiments were carried out by utilizing\nstandard images and it was verified that a better segmentation was reached, and\nthat the algorithm had better stability. An analysis on the convergence,\nthrough a theorem, with the new stopping criterion was carried out. The goal of\nthis paper is to compare the new stopping criterion with the old criterion. For\nthis reason, the obtained results were not compared with other segmentation\napproaches, since with the old stopping criterion were previously carried out.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 16:27:19 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Rodr\u00edguez", "Roberto", ""], ["Torres", "Esley", ""], ["Garc\u00e9s", "Yasel", ""], ["Pereira", "Osvaldo", ""], ["Sossa", "Humberto", ""]]}, {"id": "1311.2102", "submitter": "Yuri Boykov", "authors": "Lena Gorelick, Ismail BenAyed, Frank R. Schmidt, Yuri Boykov", "title": "An Experimental Comparison of Trust Region and Level Sets", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-order (non-linear) functionals have become very popular in segmentation,\nstereo and other computer vision problems. Level sets is a well established\ngeneral gradient descent framework, which is directly applicable to\noptimization of such functionals and widely used in practice. Recently, another\ngeneral optimization approach based on trust region methodology was proposed\nfor regional non-linear functionals. Our goal is a comprehensive experimental\ncomparison of these two frameworks in regard to practical efficiency,\nrobustness to parameters, and optimality. We experiment on a wide range of\nproblems with non-linear constraints on segment volume, appearance and shape.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 22:49:07 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Gorelick", "Lena", ""], ["BenAyed", "Ismail", ""], ["Schmidt", "Frank R.", ""], ["Boykov", "Yuri", ""]]}, {"id": "1311.2191", "submitter": "Gonzalo Galiano", "authors": "Gonzalo Galiano and Juli\\'an Velasco", "title": "Neighborhood filters and the decreasing rearrangement", "comments": null, "journal-ref": null, "doi": "10.1007/s10851-014-0522-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlocal filters are simple and powerful techniques for image denoising. In\nthis paper, we give new insights into the analysis of one kind of them, the\nNeighborhood filter, by using a classical although not very common\ntransformation: the decreasing rearrangement of a function (the image).\nIndependently of the dimension of the image, we reformulate the Neighborhood\nfilter and its iterative variants as an integral operator defined in a\none-dimensional space. The simplicity of this formulation allows to perform a\ndetailed analysis of its properties. Among others, we prove that the filter\nbehaves asymptotically as a shock filter combined with a border diffusive term,\nresponsible for the staircaising effect and the loss of contrast.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2013 17:53:22 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 19:11:43 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Galiano", "Gonzalo", ""], ["Velasco", "Juli\u00e1n", ""]]}, {"id": "1311.2460", "submitter": "Xavier Alameda-Pineda", "authors": "Xavier Alameda-Pineda and Radu Horaud", "title": "Vision-Guided Robot Hearing", "comments": "26 pages, many figures and tables, journal", "journal-ref": "International Journal of Robotics Research, 34 (4-5), 437-456,\n  2015", "doi": "10.1177/0278364914548050", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural human-robot interaction in complex and unpredictable environments is\none of the main research lines in robotics. In typical real-world scenarios,\nhumans are at some distance from the robot and the acquired signals are\nstrongly impaired by noise, reverberations and other interfering sources. In\nthis context, the detection and localisation of speakers plays a key role since\nit is the pillar on which several tasks (e.g.: speech recognition and speaker\ntracking) rely. We address the problem of how to detect and localize people\nthat are both seen and heard by a humanoid robot. We introduce a hybrid\ndeterministic/probabilistic model. Indeed, the deterministic component allows\nus to map the visual information into the auditory space. By means of the\nprobabilistic component, the visual features guide the grouping of the auditory\nfeatures in order to form AV objects. The proposed model and the associated\nalgorithm are implemented in real-time (17 FPS) using a stereoscopic camera\npair and two microphones embedded into the head of the humanoid robot NAO. We\nperformed experiments on (i) synthetic data, (ii) a publicly available data set\nand (iii) data acquired using the robot. The results we obtained validate the\napproach and encourage us to further investigate how vision can help robot\nhearing.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 12:21:31 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2013 09:02:05 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Alameda-Pineda", "Xavier", ""], ["Horaud", "Radu", ""]]}, {"id": "1311.2492", "submitter": "Jean Gallier", "authors": "Jean Gallier", "title": "Notes on Elementary Spectral Graph Theory. Applications to Graph\n  Clustering Using Normalized Cuts", "comments": "76 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are notes on the method of normalized graph cuts and its applications\nto graph clustering. I provide a fairly thorough treatment of this deeply\noriginal method due to Shi and Malik, including complete proofs. I include the\nnecessary background on graphs and graph Laplacians. I then explain in detail\nhow the eigenvectors of the graph Laplacian can be used to draw a graph. This\nis an attractive application of graph Laplacians. The main thrust of this paper\nis the method of normalized cuts. I give a detailed account for K = 2 clusters,\nand also for K > 2 clusters, based on the work of Yu and Shi. Three points that\ndo not appear to have been clearly articulated before are elaborated:\n  1. The solutions of the main optimization problem should be viewed as tuples\nin the K-fold cartesian product of projective space RP^{N-1}.\n  2. When K > 2, the solutions of the relaxed problem should be viewed as\nelements of the Grassmannian G(K,N).\n  3. Two possible Riemannian distances are available to compare the closeness\nof solutions: (a) The distance on (RP^{N-1})^K. (b) The distance on the\nGrassmannian.\n  I also clarify what should be the necessary and sufficient conditions for a\nmatrix to represent a partition of the vertices of a graph to be clustered.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 16:45:03 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Gallier", "Jean", ""]]}, {"id": "1311.2524", "submitter": "Ross Girshick", "authors": "Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik", "title": "Rich feature hierarchies for accurate object detection and semantic\n  segmentation", "comments": "Extended version of our CVPR 2014 paper; latest update (v5) includes\n  results using deeper networks (see Appendix G. Changelog)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection performance, as measured on the canonical PASCAL VOC\ndataset, has plateaued in the last few years. The best-performing methods are\ncomplex ensemble systems that typically combine multiple low-level image\nfeatures with high-level context. In this paper, we propose a simple and\nscalable detection algorithm that improves mean average precision (mAP) by more\nthan 30% relative to the previous best result on VOC 2012---achieving a mAP of\n53.3%. Our approach combines two key insights: (1) one can apply high-capacity\nconvolutional neural networks (CNNs) to bottom-up region proposals in order to\nlocalize and segment objects and (2) when labeled training data is scarce,\nsupervised pre-training for an auxiliary task, followed by domain-specific\nfine-tuning, yields a significant performance boost. Since we combine region\nproposals with CNNs, we call our method R-CNN: Regions with CNN features. We\nalso compare R-CNN to OverFeat, a recently proposed sliding-window detector\nbased on a similar CNN architecture. We find that R-CNN outperforms OverFeat by\na large margin on the 200-class ILSVRC2013 detection dataset. Source code for\nthe complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 18:43:49 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 01:44:31 GMT"}, {"version": "v3", "created": "Wed, 7 May 2014 17:09:23 GMT"}, {"version": "v4", "created": "Mon, 9 Jun 2014 22:07:33 GMT"}, {"version": "v5", "created": "Wed, 22 Oct 2014 17:23:20 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Girshick", "Ross", ""], ["Donahue", "Jeff", ""], ["Darrell", "Trevor", ""], ["Malik", "Jitendra", ""]]}, {"id": "1311.2561", "submitter": "Odemir Bruno PhD", "authors": "Lucas Assirati, N\\'ubia R. da Silva, Lilian Berton, Alneu de A. Lopes\n  and Odemir M. Bruno", "title": "Performing edge detection by difference of Gaussians using q-Gaussian\n  kernels", "comments": "5 pages, 5 figures, IC-MSQUARE 2013", "journal-ref": null, "doi": "10.1088/1742-6596/490/1/012020", "report-no": null, "categories": "cs.CV physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image processing, edge detection is a valuable tool to perform the\nextraction of features from an image. This detection reduces the amount of\ninformation to be processed, since the redundant information (considered less\nrelevant) can be unconsidered. The technique of edge detection consists of\ndetermining the points of a digital image whose intensity changes sharply. This\nchanges are due to the discontinuities of the orientation on a surface for\nexample. A well known method of edge detection is the Difference of Gaussians\n(DoG). The method consists of subtracting two Gaussians, where a kernel has a\nstandard deviation smaller than the previous one. The convolution between the\nsubtraction of kernels and the input image results in the edge detection of\nthis image. This paper introduces a method of extracting edges using DoG with\nkernels based on the q-Gaussian probability distribution, derived from the\nq-statistic proposed by Constantino Tsallis. To demonstrate the method's\npotential, we compare the introduced method with the traditional DoG using\nGaussians kernels. The results showed that the proposed method can extract\nedges with more accurate details.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 20:14:11 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 02:48:51 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Assirati", "Lucas", ""], ["da Silva", "N\u00fabia R.", ""], ["Berton", "Lilian", ""], ["Lopes", "Alneu de A.", ""], ["Bruno", "Odemir M.", ""]]}, {"id": "1311.2621", "submitter": "Pedro Nogueira", "authors": "P. A Nogueira", "title": "Determining Leishmania Infection Levels by Automatic Analysis of\n  Microscopy Images", "comments": "MSc thesis, 105 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of microscopy images is one important tool in many fields of\nbiomedical research, as it allows the quantification of a multitude of\nparameters at the cellular level. However, manual counting of these images is\nboth tiring and unreliable and ultimately very time-consuming for biomedical\nresearchers. Not only does this slow down the overall research process, it also\nintroduces counting errors due to a lack of objectivity and consistency\ninherent to the researchers own human nature.\n  This thesis addresses this issue by automatically determining infection\nindexes of macrophages parasite by Leishmania in microscopy images using\ncomputer vision and pattern recognition methodologies. Initially images are\nsubmitted to a pre-processing stage that consists in a normalization of\nillumination conditions. Three algorithms are then applied in parallel to each\nimage. Algorithm A intends to detect macrophage nuclei and consists of\nsegmentation via adaptive multi-threshold, and classification of resulting\nregions using a set of collected features. Algorithm B intends to detect\nparasites and is similar to Algorithm A but the adaptive multi-threshold is\nparameterized with a different constraints vector. Algorithm C intends to\ndetect the macrophages and parasites cytoplasm and consists of a cut-off\nversion of the previous two algorithms, where the classification step is\nskipped. Regions with multiple nuclei or parasites are processed by a voting\nsystem that employs both a Support Vector Machine and a set of region features\nfor determining the number of objects present in each region. The previous vote\nis then taken into account as the number of mixtures to be used in a Gaussian\nMixture Model to decluster the said region. Finally each parasite is assigned\nto, at most, a single macrophage using minimum Euclidean distance to a cell\nnucleus, thus quantifying Leishmania infection levels.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 21:42:51 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Nogueira", "P. A", ""]]}, {"id": "1311.2626", "submitter": "Jonathan Balzer", "authors": "J. Balzer and S. Soatto", "title": "Second-order Shape Optimization for Geometric Inverse Problems in Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method for optimization in shape spaces, i.e., sets of surfaces\nmodulo re-parametrization. Unlike previously proposed gradient flows, we\nachieve superlinear convergence rates through a subtle approximation of the\nshape Hessian, which is generally hard to compute and suffers from a series of\ndegeneracies. Our analysis highlights the role of mean curvature motion in\ncomparison with first-order schemes: instead of surface area, our approach\npenalizes deformation, either by its Dirichlet energy or total variation.\nLatter regularizer sparks the development of an alternating direction method of\nmultipliers on triangular meshes. Therein, a conjugate-gradients solver enables\nus to bypass formation of the Gaussian normal equations appearing in the course\nof the overall optimization. We combine all of the aforementioned ideas in a\nversatile geometric variation-regularized Levenberg-Marquardt-type method\napplicable to a variety of shape functionals, depending on intrinsic properties\nof the surface such as normal field and curvature as well as its embedding into\nspace. Promising experimental results are reported.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 21:53:28 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2013 18:21:32 GMT"}, {"version": "v3", "created": "Sat, 7 Dec 2013 22:37:01 GMT"}, {"version": "v4", "created": "Mon, 3 Mar 2014 08:34:27 GMT"}, {"version": "v5", "created": "Sun, 13 Apr 2014 18:09:17 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Balzer", "J.", ""], ["Soatto", "S.", ""]]}, {"id": "1311.2642", "submitter": "Jonathan Balzer", "authors": "J. Balzer, M. Peters, S. Soatto", "title": "Volumetric Reconstruction Applied to Perceptual Studies of Size and\n  Weight", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the application of volumetric reconstruction from structured-light\nsensors in cognitive neuroscience, specifically in the quantification of the\nsize-weight illusion, whereby humans tend to systematically perceive smaller\nobjects as heavier. We investigate the performance of two commercial\nstructured-light scanning systems in comparison to one we developed\nspecifically for this application. Our method has two main distinct features:\nFirst, it only samples a sparse series of viewpoints, unlike other systems such\nas the Kinect Fusion. Second, instead of building a distance field for the\npurpose of points-to-surface conversion directly, we pursue a first-order\napproach: the distance function is recovered from its gradient by a screened\nPoisson reconstruction, which is very resilient to noise and yet preserves\nhigh-frequency signal components. Our experiments show that the quality of\nmetric reconstruction from structured light sensors is subject to systematic\nbiases, and highlights the factors that influence it. Our main performance\nindex rates estimates of volume (a proxy of size), for which we review a\nwell-known formula applicable to incomplete meshes. Our code and data will be\nmade publicly available upon completion of the anonymous review process.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 22:59:33 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Balzer", "J.", ""], ["Peters", "M.", ""], ["Soatto", "S.", ""]]}, {"id": "1311.2901", "submitter": "Rob  Fergus", "authors": "Matthew D Zeiler, Rob Fergus", "title": "Visualizing and Understanding Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large Convolutional Network models have recently demonstrated impressive\nclassification performance on the ImageNet benchmark. However there is no clear\nunderstanding of why they perform so well, or how they might be improved. In\nthis paper we address both issues. We introduce a novel visualization technique\nthat gives insight into the function of intermediate feature layers and the\noperation of the classifier. We also perform an ablation study to discover the\nperformance contribution from different model layers. This enables us to find\nmodel architectures that outperform Krizhevsky \\etal on the ImageNet\nclassification benchmark. We show our ImageNet model generalizes well to other\ndatasets: when the softmax classifier is retrained, it convincingly beats the\ncurrent state-of-the-art results on Caltech-101 and Caltech-256 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 20:02:22 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2013 01:48:56 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2013 23:04:01 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Zeiler", "Matthew D", ""], ["Fergus", "Rob", ""]]}, {"id": "1311.3076", "submitter": "Karthik Keyan vkk", "authors": "V.Karthikeyan and V.J.Vijayalakshmi", "title": "An Efficient Method for Recognizing the Low Quality Fingerprint\n  Verification by Means of Cross Correlation", "comments": "7 pages and 5 figures", "journal-ref": null, "doi": "10.5121/ijci.2013.2501", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient method to provide personal\nidentification using fingerprint to get better accuracy even in noisy\ncondition. The fingerprint matching based on the number of corresponding\nminutia pairings, has been in use for a long time, which is not very efficient\nfor recognizing the low quality fingerprints. To overcome this problem,\ncorrelation technique is used. The correlation-based fingerprint verification\nsystem is capable of dealing with low quality images from which no minutiae can\nbe extracted reliably and with fingerprints that suffer from non-uniform shape\ndistortions, also in case of damaged and partial images. Orientation Field\nMethodology (OFM) has been used as a preprocessing module, and it converts the\nimages into a field pattern based on the direction of the ridges, loops and\nbifurcations in the image of a fingerprint. The input image is then Cross\nCorrelated (CC) with all the images in the cluster and the highest correlated\nimage is taken as the output. The result gives a good recognition rate, as the\nproposed scheme uses Cross Correlation of Field Orientation (CCFO = OFM + CC)\nfor fingerprint identification.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 10:50:14 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Karthikeyan", "V.", ""], ["Vijayalakshmi", "V. J.", ""]]}, {"id": "1311.3269", "submitter": "Gonzalo Galiano", "authors": "Gonzalo Galiano and Juli\\'an Velasco", "title": "On a non-local spectrogram for denoising one-dimensional signals", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous works, we investigated the use of local filters based on partial\ndifferential equations (PDE) to denoise one-dimensional signals through the\nimage processing of time-frequency representations, such as the spectrogram. In\nthis image denoising algorithms, the particularity of the image was hardly\ntaken into account. We turn, in this paper, to study the performance of\nnon-local filters, like Neighborhood or Yaroslavsky filters, in the same\nproblem. We show that, for certain iterative schemes involving the Neighborhood\nfilter, the computational time is drastically reduced with respect to\nYaroslavsky or nonlinear PDE based filters, while the outputs of the filtering\nprocesses are similar. This is heuristically justified by the connection\nbetween the (fast) Neighborhood filter applied to a spectrogram and the\ncorresponding Nonlocal Means filter (accurate) applied to the Wigner-Ville\ndistribution of the signal. This correspondence holds only for time-frequency\nrepresentations of one-dimensional signals, not to usual images, and in this\nsense the particularity of the image is exploited. We compare though a series\nof experiments on synthetic and biomedical signals the performance of local and\nnon-local filters.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 19:46:06 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Galiano", "Gonzalo", ""], ["Velasco", "Juli\u00e1n", ""]]}, {"id": "1311.3318", "submitter": "Chenliang Xu", "authors": "Chenliang Xu, Richard F. Doell, Stephen Jos\\'e Hanson, Catherine\n  Hanson and Jason J. Corso", "title": "A Study of Actor and Action Semantic Retention in Video Supervoxel\n  Segmentation", "comments": "This article is in review at the International Journal of Semantic\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods in the semantic computer vision community seem unable to\ndeal with the explosion and richness of modern, open-source and social video\ncontent. Although sophisticated methods such as object detection or\nbag-of-words models have been well studied, they typically operate on low level\nfeatures and ultimately suffer from either scalability issues or a lack of\nsemantic meaning. On the other hand, video supervoxel segmentation has recently\nbeen established and applied to large scale data processing, which potentially\nserves as an intermediate representation to high level video semantic\nextraction. The supervoxels are rich decompositions of the video content: they\ncapture object shape and motion well. However, it is not yet known if the\nsupervoxel segmentation retains the semantics of the underlying video content.\nIn this paper, we conduct a systematic study of how well the actor and action\nsemantics are retained in video supervoxel segmentation. Our study has human\nobservers watching supervoxel segmentation videos and trying to discriminate\nboth actor (human or animal) and action (one of eight everyday actions). We\ngather and analyze a large set of 640 human perceptions over 96 videos in 3\ndifferent supervoxel scales. Furthermore, we conduct machine recognition\nexperiments on a feature defined on supervoxel segmentation, called supervoxel\nshape context, which is inspired by the higher order processes in human\nperception. Our ultimate findings suggest that a significant amount of\nsemantics have been well retained in the video supervoxel segmentation and can\nbe used for further video analysis.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 21:58:55 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Xu", "Chenliang", ""], ["Doell", "Richard F.", ""], ["Hanson", "Stephen Jos\u00e9", ""], ["Hanson", "Catherine", ""], ["Corso", "Jason J.", ""]]}, {"id": "1311.3405", "submitter": "Thomas Goldstein", "authors": "Tom Goldstein, Lina Xu, Kevin F. Kelly, Richard Baraniuk", "title": "The STONE Transform: Multi-Resolution Image Enhancement and Real-Time\n  Compressive Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing enables the reconstruction of high-resolution signals from\nunder-sampled data. While compressive methods simplify data acquisition, they\nrequire the solution of difficult recovery problems to make use of the\nresulting measurements. This article presents a new sensing framework that\ncombines the advantages of both conventional and compressive sensing. Using the\nproposed \\stone transform, measurements can be reconstructed instantly at\nNyquist rates at any power-of-two resolution. The same data can then be\n\"enhanced\" to higher resolutions using compressive methods that leverage\nsparsity to \"beat\" the Nyquist limit. The availability of a fast direct\nreconstruction enables compressive measurements to be processed on small\nembedded devices. We demonstrate this by constructing a real-time compressive\nvideo camera.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 07:54:28 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2013 20:03:53 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Goldstein", "Tom", ""], ["Xu", "Lina", ""], ["Kelly", "Kevin F.", ""], ["Baraniuk", "Richard", ""]]}, {"id": "1311.3618", "submitter": "Mircea Cimpoi", "authors": "Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and\n  Andrea Vedaldi", "title": "Describing Textures in the Wild", "comments": "13 pages; 12 figures Fixed misplaced affiliation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patterns and textures are defining characteristics of many natural objects: a\nshirt can be striped, the wings of a butterfly can be veined, and the skin of\nan animal can be scaly. Aiming at supporting this analytical dimension in image\nunderstanding, we address the challenging problem of describing textures with\nsemantic attributes. We identify a rich vocabulary of forty-seven texture terms\nand use them to describe a large dataset of patterns collected in the wild.The\nresulting Describable Textures Dataset (DTD) is the basis to seek for the best\ntexture representation for recognizing describable texture attributes in\nimages. We port from object recognition to texture recognition the Improved\nFisher Vector (IFV) and show that, surprisingly, it outperforms specialized\ntexture descriptors not only on our problem, but also in established material\nrecognition datasets. We also show that the describable attributes are\nexcellent texture descriptors, transferring between datasets and tasks; in\nparticular, combined with IFV, they significantly outperform the\nstate-of-the-art by more than 8 percent on both FMD and KTHTIPS-2b benchmarks.\nWe also demonstrate that they produce intuitive descriptions of materials and\nInternet images.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 19:28:35 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2013 16:14:12 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Cimpoi", "Mircea", ""], ["Maji", "Subhransu", ""], ["Kokkinos", "Iasonas", ""], ["Mohamed", "Sammy", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1311.3715", "submitter": "Sergey Karayev", "authors": "Sergey Karayev, Matthew Trentacoste, Helen Han, Aseem Agarwala, Trevor\n  Darrell, Aaron Hertzmann, Holger Winnemoeller", "title": "Recognizing Image Style", "comments": null, "journal-ref": "Proc. British Machine Vision Conference (BMVC) 2014", "doi": "10.5244/C.28.122", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The style of an image plays a significant role in how it is viewed, but style\nhas received little attention in computer vision research. We describe an\napproach to predicting style of images, and perform a thorough evaluation of\ndifferent image features for these tasks. We find that features learned in a\nmulti-layer network generally perform best -- even when trained with object\nclass (not style) labels. Our large-scale learning methods results in the best\npublished performance on an existing dataset of aesthetic ratings and\nphotographic style annotations. We present two novel datasets: 80K Flickr\nphotographs annotated with 20 curated style labels, and 85K paintings annotated\nwith 25 style/genre labels. Our approach shows excellent classification\nperformance on both datasets. We use the learned classifiers to extend\ntraditional tag-based image search to consider stylistic constraints, and\ndemonstrate cross-dataset understanding of style.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 03:37:50 GMT"}, {"version": "v2", "created": "Fri, 23 May 2014 18:14:17 GMT"}, {"version": "v3", "created": "Wed, 23 Jul 2014 07:56:20 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Karayev", "Sergey", ""], ["Trentacoste", "Matthew", ""], ["Han", "Helen", ""], ["Agarwala", "Aseem", ""], ["Darrell", "Trevor", ""], ["Hertzmann", "Aaron", ""], ["Winnemoeller", "Holger", ""]]}, {"id": "1311.3808", "submitter": "Asha V", "authors": "V.Asha, N.U.Bhajantri, P.Nagabhushan", "title": "Periodicity Extraction using Superposition of Distance Matching Function\n  and One-dimensional Haar Wavelet Transform", "comments": "12 pages, ICSCI-2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Periodicity of a texture is one of the important visual characteristics and\nis often used as a measure for textural discrimination at the structural level.\nKnowledge about periodicity of a texture is very essential in the field of\ntexture synthesis and texture compression and also in the design of frieze and\nwall papers. In this paper, we propose a method of periodicity extraction from\nnoisy images based on superposition of distance matching function (DMF) and\nwavelet decomposition without de-noising the test images. Overall DMFs are\nsubjected to single-level Haar wavelet decomposition to obtain approximate and\ndetailed coefficients. Extracted coefficients help in determination of\nperiodicities in row and column directions. We illustrate the usefulness and\nthe effectiveness of the proposed method in a texture synthesis application.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 11:02:39 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Asha", "V.", ""], ["Bhajantri", "N. U.", ""], ["Nagabhushan", "P.", ""]]}, {"id": "1311.4029", "submitter": "Joan Bruna", "authors": "Dilip Krishnan, Joan Bruna, Rob Fergus", "title": "Blind Deconvolution with Non-local Sparsity Reweighting", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind deconvolution has made significant progress in the past decade. Most\nsuccessful algorithms are classified either as Variational or Maximum\na-Posteriori ($MAP$). In spite of the superior theoretical justification of\nvariational techniques, carefully constructed $MAP$ algorithms have proven\nequally effective in practice. In this paper, we show that all successful $MAP$\nand variational algorithms share a common framework, relying on the following\nkey principles: sparsity promotion in the gradient domain, $l_2$ regularization\nfor kernel estimation, and the use of convex (often quadratic) cost functions.\nOur observations lead to a unified understanding of the principles required for\nsuccessful blind deconvolution. We incorporate these principles into a novel\nalgorithm that improves significantly upon the state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 07:34:48 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 13:17:58 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Krishnan", "Dilip", ""], ["Bruna", "Joan", ""], ["Fergus", "Rob", ""]]}, {"id": "1311.4033", "submitter": "Omprakash Patel", "authors": "Omprakash Patel, Yogendra P. S. Maravi and Sanjeev Sharma", "title": "A Comparative Study of Histogram Equalization Based Image Enhancement\n  Techniques for Brightness Preservation and Contrast Enhancement", "comments": "15 pages, 5 figures, 4 tables, Signal & Image Processing : An\n  International Journal (SIPIJ)", "journal-ref": "Signal & Image Processing : An International Journal (SIPIJ)\n  Vol.4, No.5, October 2013", "doi": "10.5121/sipij.2013.4502", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histogram Equalization is a contrast enhancement technique in the image\nprocessing which uses the histogram of image. However histogram equalization is\nnot the best method for contrast enhancement because the mean brightness of the\noutput image is significantly different from the input image. There are several\nextensions of histogram equalization has been proposed to overcome the\nbrightness preservation challenge. Contrast enhancement using brightness\npreserving bi-histogram equalization (BBHE) and Dualistic sub image histogram\nequalization (DSIHE) which divides the image histogram into two parts based on\nthe input mean and median respectively then equalizes each sub histogram\nindependently. This paper provides review of different popular histogram\nequalization techniques and experimental study based on the absolute mean\nbrightness error (AMBE), peak signal to noise ratio (PSNR), Structure\nsimilarity index (SSI) and Entropy.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 08:10:56 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Patel", "Omprakash", ""], ["Maravi", "Yogendra P. S.", ""], ["Sharma", "Sanjeev", ""]]}, {"id": "1311.4082", "submitter": "Joel Leibo", "authors": "Qianli Liao, Joel Z Leibo, Youssef Mroueh, Tomaso Poggio", "title": "Can a biologically-plausible hierarchy effectively replace face\n  detection, alignment, and recognition pipelines?", "comments": "11 Pages, 4 Figures. Mar 26, (2014): Improved exposition. Added CBMM\n  memo cover page. No substantive changes", "journal-ref": null, "doi": null, "report-no": "CBMM-003", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard approach to unconstrained face recognition in natural\nphotographs is via a detection, alignment, recognition pipeline. While that\napproach has achieved impressive results, there are several reasons to be\ndissatisfied with it, among them is its lack of biological plausibility. A\nrecent theory of invariant recognition by feedforward hierarchical networks,\nlike HMAX, other convolutional networks, or possibly the ventral stream,\nimplies an alternative approach to unconstrained face recognition. This\napproach accomplishes detection and alignment implicitly by storing\ntransformations of training images (called templates) rather than explicitly\ndetecting and aligning faces at test time. Here we propose a particular\nlocality-sensitive hashing based voting scheme which we call \"consensus of\ncollisions\" and show that it can be used to approximate the full 3-layer\nhierarchy implied by the theory. The resulting end-to-end system for\nunconstrained face recognition operates on photographs of faces taken under\nnatural conditions, e.g., Labeled Faces in the Wild (LFW), without aligning or\ncropping them, as is normally done. It achieves a drastic improvement in the\nstate of the art on this end-to-end task, reaching the same level of\nperformance as the best systems operating on aligned, closely cropped images\n(no outside training data). It also performs well on two newer datasets,\nsimilar to LFW, but more difficult: LFW-jittered (new here) and SUFR-W.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 17:49:31 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2013 10:25:29 GMT"}, {"version": "v3", "created": "Wed, 26 Mar 2014 10:11:42 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["Liao", "Qianli", ""], ["Leibo", "Joel Z", ""], ["Mroueh", "Youssef", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1311.4158", "submitter": "Fabio Anselmi", "authors": "Fabio Anselmi, Joel Z. Leibo, Lorenzo Rosasco, Jim Mutch, Andrea\n  Tacchetti, Tomaso Poggio", "title": "Unsupervised Learning of Invariant Representations in Hierarchical\n  Architectures", "comments": "23 pages, 10 figures. November 21 2013: Added acknowledgment of NSF\n  funding. No other changes. December 18 (2013): Fixed a figure. January 10\n  (2014): Fixed a figure and some math in SI. March 10 2014: modified abstract\n  and implementation section (main and SI); added a paragraph about sample\n  complexity in SI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present phase of Machine Learning is characterized by supervised learning\nalgorithms relying on large sets of labeled examples ($n \\to \\infty$). The next\nphase is likely to focus on algorithms capable of learning from very few\nlabeled examples ($n \\to 1$), like humans seem able to do. We propose an\napproach to this problem and describe the underlying theory, based on the\nunsupervised, automatic learning of a ``good'' representation for supervised\nlearning, characterized by small sample complexity ($n$). We consider the case\nof visual object recognition though the theory applies to other domains. The\nstarting point is the conjecture, proved in specific cases, that image\nrepresentations which are invariant to translations, scaling and other\ntransformations can considerably reduce the sample complexity of learning. We\nprove that an invariant and unique (discriminative) signature can be computed\nfor each image patch, $I$, in terms of empirical distributions of the\ndot-products between $I$ and a set of templates stored during unsupervised\nlearning. A module performing filtering and pooling, like the simple and\ncomplex cells described by Hubel and Wiesel, can compute such estimates.\nHierarchical architectures consisting of this basic Hubel-Wiesel moduli inherit\nits properties of invariance, stability, and discriminability while capturing\nthe compositional organization of the visual world in terms of wholes and\nparts. The theory extends existing deep learning convolutional architectures\nfor image and speech recognition. It also suggests that the main computational\ngoal of the ventral stream of visual cortex is to provide a hierarchical\nrepresentation of new objects/images which is invariant to transformations,\nstable, and discriminative for recognition---and that this representation may\nbe continuously learned in an unsupervised way during development and visual\nexperience.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 13:22:44 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2013 10:41:41 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2013 20:57:24 GMT"}, {"version": "v4", "created": "Sun, 12 Jan 2014 04:06:29 GMT"}, {"version": "v5", "created": "Tue, 11 Mar 2014 19:56:59 GMT"}], "update_date": "2014-03-12", "authors_parsed": [["Anselmi", "Fabio", ""], ["Leibo", "Joel Z.", ""], ["Rosasco", "Lorenzo", ""], ["Mutch", "Jim", ""], ["Tacchetti", "Andrea", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1311.4252", "submitter": "Odemir Bruno PhD", "authors": "Andr\\'e Ricardo Backes, Dalcimar Casanova, Odemir Martinez Bruno", "title": "Contour polygonal approximation using shortest path in networks", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": "10.1142/S0129183113500903", "report-no": null, "categories": "physics.comp-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contour polygonal approximation is a simplified representation of a contour\nby line segments, so that the main characteristics of the contour remain in a\nsmall number of line segments. This paper presents a novel method for polygonal\napproximation based on the Complex Networks theory. We convert each point of\nthe contour into a vertex, so that we model a regular network. Then we\ntransform this network into a Small-World Complex Network by applying some\ntransformations over its edges. By analyzing of network properties, especially\nthe geodesic path, we compute the polygonal approximation. The paper presents\nthe main characteristics of the method, as well as its functionality. We\nevaluate the proposed method using benchmark contours, and compare its results\nwith other polygonal approximation methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 03:16:55 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Backes", "Andr\u00e9 Ricardo", ""], ["Casanova", "Dalcimar", ""], ["Bruno", "Odemir Martinez", ""]]}, {"id": "1311.4665", "submitter": "Stefanie Wuhrer", "authors": "Pegah Kamousi, Sylvain Lazard, Anil Maheshwari, Stefanie Wuhrer", "title": "Analysis of Farthest Point Sampling for Approximating Geodesics in a\n  Graph", "comments": "13 pages, 4 figures", "journal-ref": "Computational Geometry, Elsevier, 2016, 57, pp.1-7", "doi": "10.1016/j.comgeo.2016.05.005", "report-no": null, "categories": "cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard way to approximate the distance between any two vertices $p$ and\n$q$ on a mesh is to compute, in the associated graph, a shortest path from $p$\nto $q$ that goes through one of $k$ sources, which are well-chosen vertices.\nPrecomputing the distance between each of the $k$ sources to all vertices of\nthe graph yields an efficient computation of approximate distances between any\ntwo vertices. One standard method for choosing $k$ sources, which has been used\nextensively and successfully for isometry-invariant surface processing, is the\nso-called Farthest Point Sampling (FPS), which starts with a random vertex as\nthe first source, and iteratively selects the farthest vertex from the already\nselected sources.\n  In this paper, we analyze the stretch factor $\\mathcal{F}_{FPS}$ of\napproximate geodesics computed using FPS, which is the maximum, over all pairs\nof distinct vertices, of their approximated distance over their geodesic\ndistance in the graph. We show that $\\mathcal{F}_{FPS}$ can be bounded in terms\nof the minimal value $\\mathcal{F}^*$ of the stretch factor obtained using an\noptimal placement of $k$ sources as $\\mathcal{F}_{FPS}\\leq 2 r_e^2\n\\mathcal{F}^*+ 2 r_e^2 + 8 r_e + 1$, where $r_e$ is the ratio of the lengths of\nthe longest and the shortest edges of the graph. This provides some evidence\nexplaining why farthest point sampling has been used successfully for\nisometry-invariant shape processing. Furthermore, we show that it is\nNP-complete to find $k$ sources that minimize the stretch factor.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 09:22:18 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Kamousi", "Pegah", ""], ["Lazard", "Sylvain", ""], ["Maheshwari", "Anil", ""], ["Wuhrer", "Stefanie", ""]]}, {"id": "1311.4924", "submitter": "Yipeng Liu Prof.", "authors": "Yipeng Liu", "title": "Robust Compressed Sensing Under Matrix Uncertainties", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT math.RT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (CS) shows that a signal having a sparse or compressible\nrepresentation can be recovered from a small set of linear measurements. In\nclassical CS theory, the sampling matrix and representation matrix are assumed\nto be known exactly in advance. However, uncertainties exist due to sampling\ndistortion, finite grids of the parameter space of dictionary, etc. In this\npaper, we take a generalized sparse signal model, which simultaneously\nconsiders the sampling and representation matrix uncertainties. Based on the\nnew signal model, a new optimization model for robust sparse signal\nreconstruction is proposed. This optimization model can be deduced with\nstochastic robust approximation analysis. Both convex relaxation and greedy\nalgorithms are used to solve the optimization problem. For the convex\nrelaxation method, a sufficient condition for recovery by convex relaxation is\ngiven; For the greedy algorithm, it is realized by the introduction of a\npre-processing of the sensing matrix and the measurements. In numerical\nexperiments, both simulated data and real-life ECG data based results show that\nthe proposed method has a better performance than the current methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 00:20:50 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 10:35:33 GMT"}, {"version": "v3", "created": "Wed, 19 Mar 2014 17:17:46 GMT"}, {"version": "v4", "created": "Thu, 2 Jul 2015 15:23:32 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Liu", "Yipeng", ""]]}, {"id": "1311.4963", "submitter": "Shubham Saini", "authors": "Shubham Saini, Bhavesh Kasliwal and Shraey Bhatia", "title": "Comparative Study Of Image Edge Detection Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since edge detection is in the forefront of image processing for object\ndetection, it is crucial to have a good understanding of edge detection\nalgorithms. The reason for this is that edges form the outline of an object. An\nedge is the boundary between an object and the background, and indicates the\nboundary between overlapping objects. This means that if the edges in an image\ncan be identified accurately, all of the objects can be located and basic\nproperties such as area, perimeter, and shape can be measured. Since computer\nvision involves the identification and classification of objects in an image,\nedge detection is an essential tool. We tested two edge detectors that use\ndifferent methods for detecting edges and compared their results under a\nvariety of situations to determine which detector was preferable under\ndifferent sets of conditions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 06:22:53 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2013 05:51:04 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Saini", "Shubham", ""], ["Kasliwal", "Bhavesh", ""], ["Bhatia", "Shraey", ""]]}, {"id": "1311.5290", "submitter": "Odemir Bruno PhD", "authors": "Wesley Nunes Gon\\c{c}alves, Bruno Brandoli Machado, Odemir Martinez\n  Bruno", "title": "Texture descriptor combining fractal dimension and artificial crawlers", "comments": "12 pages 9 figures. Paper in press: Physica A: Statistical Mechanics\n  and its Applications", "journal-ref": null, "doi": "10.1016/j.physa.2013.10.011", "report-no": null, "categories": "physics.data-an cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture is an important visual attribute used to describe images. There are\nmany methods available for texture analysis. However, they do not capture the\ndetails richness of the image surface. In this paper, we propose a new method\nto describe textures using the artificial crawler model. This model assumes\nthat each agent can interact with the environment and each other. Since this\nswarm system alone does not achieve a good discrimination, we developed a new\nmethod to increase the discriminatory power of artificial crawlers, together\nwith the fractal dimension theory. Here, we estimated the fractal dimension by\nthe Bouligand-Minkowski method due to its precision in quantifying structural\nproperties of images. We validate our method on two texture datasets and the\nexperimental results reveal that our method leads to highly discriminative\ntextural features. The results indicate that our method can be used in\ndifferent texture applications.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 01:51:03 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["Gon\u00e7alves", "Wesley Nunes", ""], ["Machado", "Bruno Brandoli", ""], ["Bruno", "Odemir Martinez", ""]]}, {"id": "1311.5590", "submitter": "Le Li", "authors": "Yuzhu Zhou, Le Li, Honggang Zhang", "title": "Adaptive Learning of Region-based pLSA Model for Total Scene Annotation", "comments": "Volume 2, Page 131-136. 2010 International Conference on Information\n  and Multimedia Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a region-based pLSA model to accomplish the task of\ntotal scene annotation. To be more specific, we not only properly generate a\nlist of tags for each image, but also localizing each region with its\ncorresponding tag. We integrate advantages of different existing region-based\nworks: employ efficient and powerful JSEG algorithm for segmentation so that\neach region can easily express meaningful object information; the introduction\nof pLSA model can help better capturing semantic information behind the\nlow-level features. Moreover, we also propose an adaptive padding mechanism to\nautomatically choose the optimal padding strategy for each region, which\ndirectly increases the overall system performance. Finally we conduct 3\nexperiments to verify our ideas on Corel database and demonstrate the\neffectiveness and accuracy of our system.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 21:36:23 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Zhou", "Yuzhu", ""], ["Li", "Le", ""], ["Zhang", "Honggang", ""]]}, {"id": "1311.5591", "submitter": "Ning Zhang", "authors": "Ning Zhang, Manohar Paluri, Marc'Aurelio Ranzato, Trevor Darrell,\n  Lubomir Bourdev", "title": "PANDA: Pose Aligned Networks for Deep Attribute Modeling", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for inferring human attributes (such as gender, hair\nstyle, clothes style, expression, action) from images of people under large\nvariation of viewpoint, pose, appearance, articulation and occlusion.\nConvolutional Neural Nets (CNN) have been shown to perform very well on large\nscale object recognition problems. In the context of attribute classification,\nhowever, the signal is often subtle and it may cover only a small part of the\nimage, while the image is dominated by the effects of pose and viewpoint.\nDiscounting for pose variation would require training on very large labeled\ndatasets which are not presently available. Part-based models, such as poselets\nand DPM have been shown to perform well for this problem but they are limited\nby shallow low-level features. We propose a new method which combines\npart-based models and deep learning by training pose-normalized CNNs. We show\nsubstantial improvement vs. state-of-the-art methods on challenging attribute\nclassification tasks in unconstrained settings. Experiments confirm that our\nmethod outperforms both the best part-based methods on this problem and\nconventional CNNs trained on the full bounding box of the person.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 21:43:12 GMT"}, {"version": "v2", "created": "Mon, 5 May 2014 21:32:36 GMT"}], "update_date": "2014-05-07", "authors_parsed": [["Zhang", "Ning", ""], ["Paluri", "Manohar", ""], ["Ranzato", "Marc'Aurelio", ""], ["Darrell", "Trevor", ""], ["Bourdev", "Lubomir", ""]]}, {"id": "1311.5595", "submitter": "Alon Shtern", "authors": "Alon Shtern and Ron Kimmel", "title": "On Nonrigid Shape Similarity and Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important operation in geometry processing is finding the correspondences\nbetween pairs of shapes. The Gromov-Hausdorff distance, a measure of\ndissimilarity between metric spaces, has been found to be highly useful for\nnonrigid shape comparison. Here, we explore the applicability of related shape\nsimilarity measures to the problem of shape correspondence, adopting spectral\ntype distances. We propose to evaluate the spectral kernel distance, the\nspectral embedding distance and the novel spectral quasi-conformal distance,\ncomparing the manifolds from different viewpoints. By matching the shapes in\nthe spectral domain, important attributes of surface structure are being\naligned. For the purpose of testing our ideas, we introduce a fully automatic\nframework for finding intrinsic correspondence between two shapes. The proposed\nmethod achieves state-of-the-art results on the Princeton isometric shape\nmatching protocol applied, as usual, to the TOSCA and SCAPE benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 22:08:02 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Shtern", "Alon", ""], ["Kimmel", "Ron", ""]]}, {"id": "1311.5829", "submitter": "Abdul Kadir", "authors": "Abdul Kadir, Lukito Edi Nugroho, Adhi Susanto, Paulus Insap Santosa", "title": "Neural Network Application on Foliage Plant Identification", "comments": "8 pages", "journal-ref": "International Journal of Computer Applications Volume 29 No.9,\n  September 2011", "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several researches in leaf identification did not include color information\nas features. The main reason is caused by a fact that they used green colored\nleaves as samples. However, for foliage plants, plants with colorful leaves,\nfancy patterns in their leaves, and interesting plants with unique shape, color\nand also texture could not be neglected. For example, Epipremnum pinnatum\n'Aureum' and Epipremnum pinnatum 'Marble Queen' have similar patterns, same\nshape, but different colors. Combination of shape, color, texture features, and\nother attribute contained on the leaf is very useful in leaf identification. In\nthis research, Polar Fourier Transform and three kinds of geometric features\nwere used to represent shape features, color moments that consist of mean,\nstandard deviation, skewness were used to represent color features, texture\nfeatures are extracted from GLCMs, and vein features were added to improve\nperformance of the identification system. The identification system uses\nProbabilistic Neural Network (PNN) as a classifier. The result shows that the\nsystem gives average accuracy of 93.0833% for 60 kinds of foliage plants.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 08:02:20 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Kadir", "Abdul", ""], ["Nugroho", "Lukito Edi", ""], ["Susanto", "Adhi", ""], ["Santosa", "Paulus Insap", ""]]}, {"id": "1311.5830", "submitter": "Ge Wang", "authors": "Baodong Liu, Hengyong Yu, Scott S. Verbridge, Lizhi Sun, Ge Wang", "title": "Dictionary-Learning-Based Reconstruction Method for Electron Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Electron tomography usually suffers from so called missing wedge artifacts\ncaused by limited tilt angle range. An equally sloped tomography (EST)\nacquisition scheme (which should be called the linogram sampling scheme) was\nrecently applied to achieve 2.4-angstrom resolution. On the other hand, a\ncompressive sensing-inspired reconstruction algorithm, known as adaptive\ndictionary based statistical iterative reconstruction (ADSIR), has been\nreported for x-ray computed tomography. In this paper, we evaluate the EST,\nADSIR and an ordered-subset simultaneous algebraic reconstruction technique\n(OS-SART), and compare the ES and equally angled (EA) data acquisition modes.\nOur results show that OS-SART is comparable to EST, and the ADSIR outperforms\nEST and OS-SART. Furthermore, the equally sloped projection data acquisition\nmode has no advantage over the conventional equally angled mode in the context.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 17:47:16 GMT"}], "update_date": "2013-12-16", "authors_parsed": [["Liu", "Baodong", ""], ["Yu", "Hengyong", ""], ["Verbridge", "Scott S.", ""], ["Sun", "Lizhi", ""], ["Wang", "Ge", ""]]}, {"id": "1311.5947", "submitter": "Chunhua Shen", "authors": "Guosheng Lin, Chunhua Shen, Anton van den Hengel, David Suter", "title": "Fast Training of Effective Multi-class Boosting Using Coordinate Descent\n  Optimization", "comments": "Appeared in Proc. Asian Conf. Computer Vision 2012. Code can be\n  downloaded at http://goo.gl/WluhrQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wepresentanovelcolumngenerationbasedboostingmethod for multi-class\nclassification. Our multi-class boosting is formulated in a single optimization\nproblem as in Shen and Hao (2011). Different from most existing multi-class\nboosting methods, which use the same set of weak learners for all the classes,\nwe train class specified weak learners (i.e., each class has a different set of\nweak learners). We show that using separate weak learner sets for each class\nleads to fast convergence, without introducing additional computational\noverhead in the training procedure. To further make the training more efficient\nand scalable, we also propose a fast co- ordinate descent method for solving\nthe optimization problem at each boosting iteration. The proposed coordinate\ndescent method is conceptually simple and easy to implement in that it is a\nclosed-form solution for each coordinate update. Experimental results on a\nvariety of datasets show that, compared to a range of existing multi-class\nboosting meth- ods, the proposed method has much faster convergence rate and\nbetter generalization performance in most cases. We also empirically show that\nthe proposed fast coordinate descent algorithm needs less training time than\nthe MultiBoost algorithm in Shen and Hao (2011).\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 02:30:14 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Suter", "David", ""]]}, {"id": "1311.6007", "submitter": "Mohammad  Mozumdar", "authors": "Nikunj Bajaj and Aurobinda Routray and S L Happy", "title": "Dynamic Model of Facial Expression Recognition based on Eigen-face\n  Approach", "comments": "Proceedings of Green Energy and Systems Conference 2013, November 25,\n  Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Emotions are best way of communicating information; and sometimes it carry\nmore information than words. Recently, there has been a huge interest in\nautomatic recognition of human emotion because of its wide spread application\nin security, surveillance, marketing, advertisement, and human-computer\ninteraction. To communicate with a computer in a natural way, it will be\ndesirable to use more natural modes of human communication based on voice,\ngestures and facial expressions. In this paper, a holistic approach for facial\nexpression recognition is proposed which captures the variation in facial\nfeatures in temporal domain and classifies the sequence of images in different\nemotions. The proposed method uses Haar-like features to detect face in an\nimage. The dimensionality of the eigenspace is reduced using Principal\nComponent Analysis (PCA). By projecting the subsequent face images into\nprincipal eigen directions, the variation pattern of the obtained weight vector\nis modeled to classify it into different emotions. Owing to the variations of\nexpressions for different people and its intensity, a person specific method\nfor emotion recognition is followed. Using the gray scale images of the frontal\nface, the system is able to classify four basic emotions such as happiness,\nsadness, surprise, and anger.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 15:40:37 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Bajaj", "Nikunj", ""], ["Routray", "Aurobinda", ""], ["Happy", "S L", ""]]}, {"id": "1311.6048", "submitter": "Stefano Soatto", "authors": "Jingming Dong, Jonathan Balzer, Damek Davis, Joshua Hernandez, Stefano\n  Soatto", "title": "On the Design and Analysis of Multiple View Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": "UCLA CSD TR130024, Nov. 8, 2013", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension of popular descriptors based on gradient orientation\nhistograms (HOG, computed in a single image) to multiple views. It hinges on\ninterpreting HOG as a conditional density in the space of sampled images, where\nthe effects of nuisance factors such as viewpoint and illumination are\nmarginalized. However, such marginalization is performed with respect to a very\ncoarse approximation of the underlying distribution. Our extension leverages on\nthe fact that multiple views of the same scene allow separating intrinsic from\nnuisance variability, and thus afford better marginalization of the latter. The\nresult is a descriptor that has the same complexity of single-view HOG, and can\nbe compared in the same manner, but exploits multiple views to better trade off\ninsensitivity to nuisance variability with specificity to intrinsic\nvariability. We also introduce a novel multi-view wide-baseline matching\ndataset, consisting of a mixture of real and synthetic objects with ground\ntruthed camera motion and dense three-dimensional geometry.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 20:38:50 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Dong", "Jingming", ""], ["Balzer", "Jonathan", ""], ["Davis", "Damek", ""], ["Hernandez", "Joshua", ""], ["Soatto", "Stefano", ""]]}, {"id": "1311.6049", "submitter": "Nidhal El-Abbadi", "authors": "Nidhal K. El Abbadi, Nazar Dahir, Zaid Abd Alkareem", "title": "Skin Texture Recognition Using Neural Networks", "comments": "4 pages, 6 figures, conference ACIT 2008, Tunisia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin recognition is used in many applications ranging from algorithms for\nface detection, hand gesture analysis, and to objectionable image filtering. In\nthis work a skin recognition system was developed and tested. While many skin\nsegmentation algorithms relay on skin color, our work relies on both skin color\nand texture features (features derives from the GLCM) to give a better and more\nefficient recognition accuracy of skin textures. We used feed forward neural\nnetworks to classify input textures images to be skin or non skin textures. The\nsystem gave very encouraging results during the neural network generalization\nface.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 20:52:05 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Abbadi", "Nidhal K. El", ""], ["Dahir", "Nazar", ""], ["Alkareem", "Zaid Abd", ""]]}, {"id": "1311.6079", "submitter": "Amirreza Shaban", "authors": "Amirreza Shaban, Hamid R. Rabiee and Mahyar Najibi", "title": "Local Similarities, Global Coding: An Algorithm for Feature Coding and\n  its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data coding as a building block of several image processing algorithms has\nbeen received great attention recently. Indeed, the importance of the locality\nassumption in coding approaches is studied in numerous works and several\nmethods are proposed based on this concept. We probe this assumption and claim\nthat taking the similarity between a data point and a more global set of anchor\npoints does not necessarily weaken the coding method as long as the underlying\nstructure of the anchor points are taken into account. Based on this fact, we\npropose to capture this underlying structure by assuming a random walker over\nthe anchor points. We show that our method is a fast approximate learning\nalgorithm based on the diffusion map kernel. The experiments on various\ndatasets show that making different state-of-the-art coding algorithms aware of\nthis structure boosts them in different learning tasks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2013 04:39:28 GMT"}, {"version": "v2", "created": "Wed, 5 Mar 2014 20:30:13 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Shaban", "Amirreza", ""], ["Rabiee", "Hamid R.", ""], ["Najibi", "Mahyar", ""]]}, {"id": "1311.6371", "submitter": "Antoni Chan", "authors": "Lifeng Shang and Antoni B. Chan", "title": "On Approximate Inference for Generalized Gaussian Process Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized Gaussian process model (GGPM) is a unifying framework that\nencompasses many existing Gaussian process (GP) models, such as GP regression,\nclassification, and counting. In the GGPM framework, the observation likelihood\nof the GP model is itself parameterized using the exponential family\ndistribution (EFD). In this paper, we consider efficient algorithms for\napproximate inference on GGPMs using the general form of the EFD. A particular\nGP model and its associated inference algorithms can then be formed by changing\nthe parameters of the EFD, thus greatly simplifying its creation for\ntask-specific output domains. We demonstrate the efficacy of this framework by\ncreating several new GP models for regressing to non-negative reals and to real\nintervals. We also consider a closed-form Taylor approximation for efficient\ninference on GGPMs, and elaborate on its connections with other model-specific\nheuristic closed-form approximations. Finally, we present a comprehensive set\nof experiments to compare approximate inference algorithms on a wide variety of\nGGPMs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 17:22:22 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2013 04:24:02 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2013 07:43:48 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Shang", "Lifeng", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1311.6500", "submitter": "Camille Goudeseune", "authors": "Camille Goudeseune", "title": "Stitched Panoramas from Toy Airborne Video Cameras", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Effective panoramic photographs are taken from vantage points that are high.\nHigh vantage points have recently become easier to reach as the cost of\nquadrotor helicopters has dropped to nearly disposable levels. Although cameras\ncarried by such aircraft weigh only a few grams, their low-quality video can be\nconverted into panoramas of high quality and high resolution. Also, the small\nsize of these aircraft vastly reduces the risks inherent to flight.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 20:32:50 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Goudeseune", "Camille", ""]]}, {"id": "1311.6510", "submitter": "Agata Lapedriza", "authors": "Agata Lapedriza and Hamed Pirsiavash and Zoya Bylinskii and Antonio\n  Torralba", "title": "Are all training examples equally valuable?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When learning a new concept, not all training examples may prove equally\nuseful for training: some may have higher or lower training value than others.\nThe goal of this paper is to bring to the attention of the vision community the\nfollowing considerations: (1) some examples are better than others for training\ndetectors or classifiers, and (2) in the presence of better examples, some\nexamples may negatively impact performance and removing them may be beneficial.\nIn this paper, we propose an approach for measuring the training value of an\nexample, and use it for ranking and greedily sorting examples. We test our\nmethods on different vision tasks, models, datasets and classifiers. Our\nexperiments show that the performance of current state-of-the-art detectors and\nclassifiers can be improved when training on a subset, rather than the whole\ntraining set.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 22:59:24 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Lapedriza", "Agata", ""], ["Pirsiavash", "Hamed", ""], ["Bylinskii", "Zoya", ""], ["Torralba", "Antonio", ""]]}, {"id": "1311.6740", "submitter": "Karthik Keyan vkk", "authors": "V. Karthikeyan", "title": "Hilditchs Algorithm Based Tamil Character Recognition", "comments": "7 pages 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Character identification plays a vital role in the contemporary world of\nImage processing. It can solve many composite problems and makes humans work\neasier. An instance is Handwritten Character detection. Handwritten recognition\nis not a novel expertise, but it has not gained community notice until Now. The\neventual aim of designing Handwritten Character recognition structure with an\naccurateness rate of 100% is pretty illusionary. Tamil Handwritten Character\nrecognition system uses the Neural Networks to distinguish them. Neural Network\nand structural characteristics are used to instruct and recognize written\ncharacters. After training and testing the exactness rate reached 99%. This\ncorrectness rate is extremely high. In this paper we are exploring image\nprocessing through the Hilditch algorithm foundation and structural\ncharacteristics of a character in the image. And we recognized some character\nof the Tamil language, and we are trying to identify all the character of Tamil\nIn our future works.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 08:38:50 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Karthikeyan", "V.", ""]]}, {"id": "1311.6758", "submitter": "Patrick Ott", "authors": "Patrick Ott and Mark Everingham and Jiri Matas", "title": "Detection of Partially Visible Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An \"elephant in the room\" for most current object detection and localization\nmethods is the lack of explicit modelling of partial visibility due to\nocclusion by other objects or truncation by the image boundary. Based on a\nsliding window approach, we propose a detection method which explicitly models\npartial visibility by treating it as a latent variable. A novel non-maximum\nsuppression scheme is proposed which takes into account the inferred partial\nvisibility of objects while providing a globally optimal solution. The method\ngives more detailed scene interpretations than conventional detectors in that\nwe are able to identify the visible parts of an object. We report improved\naverage precision on the PASCAL VOC 2010 dataset compared to a baseline\ndetector.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2013 16:59:19 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Ott", "Patrick", ""], ["Everingham", "Mark", ""], ["Matas", "Jiri", ""]]}, {"id": "1311.6799", "submitter": "Sabyasachi  Mukhopadhyay", "authors": "Sabyasachi Mukhopadhyay, Debadatta Dash, Swapnil Barmase, Prasanta K\n  Panigrahi", "title": "Wavelet and Fast Fourier Transform based analysis of Solar Image", "comments": "This paper has been withdrawn by the author due to some modifications\n  are required for this current paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both of Wavelet and Fast Fourier Transform are strong signal processing tools\nin the field of Data Analysis. In this paper fast fourier transform (FFT) and\nWavelet Transform are employed to observe some important features of Solar\nimage (December, 2004). We have tried to find out the periodicity and coherence\nof different sections of the solar image. We plotted the distribution of energy\nin solar surface by analyzing the solar image with scalograms and\n3D-coefficient plots.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 10:37:28 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2013 16:40:09 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Mukhopadhyay", "Sabyasachi", ""], ["Dash", "Debadatta", ""], ["Barmase", "Swapnil", ""], ["Panigrahi", "Prasanta K", ""]]}, {"id": "1311.6881", "submitter": "Abhishek Pandey", "authors": "Abhishek Pandey, Anjna Jayant Deen and Rajeev Pandey (Dept. of CSE,\n  UIT-RGPV)", "title": "Color and Shape Content Based Image Classification using RBF Network and\n  PSO Technique: A Survey", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The improvement of the accuracy of image query retrieval used image\nclassification technique. Image classification is well known technique of\nsupervised learning. The improved method of image classification increases the\nworking efficiency of image query retrieval. For the improvements of\nclassification technique we used RBF neural network function for better\nprediction of feature used in image retrieval.Colour content is represented by\npixel values in image classification using radial base function(RBF) technique.\nThis approach provides better result compare to SVM technique in image\nrepresentation.Image is represented by matrix though RBF using pixel values of\ncolour intensity of image. Firstly we using RGB colour model. In this colour\nmodel we use red, green and blue colour intensity values in matrix.SVM with\npartical swarm optimization for image classification is implemented in content\nof images which provide better Results based on the proposed approach are found\nencouraging in terms of color image classification accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 07:14:25 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Pandey", "Abhishek", "", "Dept. of CSE,\n  UIT-RGPV"], ["Deen", "Anjna Jayant", "", "Dept. of CSE,\n  UIT-RGPV"], ["Pandey", "Rajeev", "", "Dept. of CSE,\n  UIT-RGPV"]]}, {"id": "1311.6887", "submitter": "Ayan Chakrabarti", "authors": "Ayan Chakrabarti, Ying Xiong, Baochen Sun, Trevor Darrell, Daniel\n  Scharstein, Todd Zickler, Kate Saenko", "title": "Modeling Radiometric Uncertainty for Vision with Tone-mapped Color\n  Images", "comments": null, "journal-ref": "IEEE Trans. PAMI 36 (2014) 2185-2198", "doi": "10.1109/TPAMI.2014.2318713", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To produce images that are suitable for display, tone-mapping is widely used\nin digital cameras to map linear color measurements into narrow gamuts with\nlimited dynamic range. This introduces non-linear distortion that must be\nundone, through a radiometric calibration process, before computer vision\nsystems can analyze such photographs radiometrically. This paper considers the\ninherent uncertainty of undoing the effects of tone-mapping. We observe that\nthis uncertainty varies substantially across color space, making some pixels\nmore reliable than others. We introduce a model for this uncertainty and a\nmethod for fitting it to a given camera or imaging pipeline. Once fit, the\nmodel provides for each pixel in a tone-mapped digital photograph a probability\ndistribution over linear scene colors that could have induced it. We\ndemonstrate how these distributions can be useful for visual inference by\nincorporating them into estimation algorithms for a representative set of\nvision tasks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 07:39:27 GMT"}, {"version": "v2", "created": "Wed, 9 Apr 2014 18:30:40 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Chakrabarti", "Ayan", ""], ["Xiong", "Ying", ""], ["Sun", "Baochen", ""], ["Darrell", "Trevor", ""], ["Scharstein", "Daniel", ""], ["Zickler", "Todd", ""], ["Saenko", "Kate", ""]]}, {"id": "1311.6932", "submitter": "Luisa Verdoliva", "authors": "Davide Cozzolino and Diego Gragnaniello and Luisa Verdoliva", "title": "A novel framework for image forgery localization", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Image forgery localization is a very active and open research field for the\ndifficulty to handle the large variety of manipulations a malicious user can\nperform by means of more and more sophisticated image editing tools. Here, we\npropose a localization framework based on the fusion of three very different\ntools, based, respectively, on sensor noise, patch-matching, and machine\nlearning. The binary masks provided by these tools are finally fused based on\nsome suitable reliability indexes. According to preliminary experiments on the\ntraining set, the proposed framework provides often a very good localization\naccuracy and sometimes valuable clues for visual scrutiny.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 11:06:05 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Cozzolino", "Davide", ""], ["Gragnaniello", "Diego", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "1311.6934", "submitter": "Luisa Verdoliva", "authors": "Davide Cozzolino and Diego Gragnaniello and Luisa Verdoliva", "title": "Image forgery detection based on the fusion of machine learning and\n  block-matching methods", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Dense local descriptors and machine learning have been used with success in\nseveral applications, like classification of textures, steganalysis, and\nforgery detection. We develop a new image forgery detector building upon some\ndescriptors recently proposed in the steganalysis field suitably merging some\nof such descriptors, and optimizing a SVM classifier on the available training\nset. Despite the very good performance, very small forgeries are hardly ever\ndetected because they contribute very little to the descriptors. Therefore we\nalso develop a simple, but extremely specific, copy-move detector based on\nregion matching and fuse decisions so as to reduce the missing detection rate.\nOverall results appear to be extremely encouraging.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 11:17:55 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Cozzolino", "Davide", ""], ["Gragnaniello", "Diego", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "1311.7080", "submitter": "Jim Jing-Yan Wang", "authors": "Jim Jing-Yan Wang", "title": "Cross-Domain Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding has shown its power as an effective data representation method.\nHowever, up to now, all the sparse coding approaches are limited within the\nsingle domain learning problem. In this paper, we extend the sparse coding to\ncross domain learning problem, which tries to learn from a source domain to a\ntarget domain with significant different distribution. We impose the Maximum\nMean Discrepancy (MMD) criterion to reduce the cross-domain distribution\ndifference of sparse codes, and also regularize the sparse codes by the class\nlabels of the samples from both domains to increase the discriminative ability.\nThe encouraging experiment results of the proposed cross-domain sparse coding\nalgorithm on two challenging tasks --- image classification of photograph and\noil painting domains, and multiple user spam detection --- show the advantage\nof the proposed method over other cross-domain data representation methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 19:27:06 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Wang", "Jim Jing-Yan", ""]]}, {"id": "1311.7186", "submitter": "Srimal Jayawardena", "authors": "Srimal Jayawardena and Marcus Hutter and Nathan Brewer", "title": "A Novel Illumination-Invariant Loss for Monocular 3D Pose Estimation", "comments": "Digital Image Computing Techniques and Applications (DICTA), 2011\n  International Conference on", "journal-ref": null, "doi": "10.1109/DICTA.2011.15", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of identifying the 3D pose of a known object from a given 2D\nimage has important applications in Computer Vision. Our proposed method of\nregistering a 3D model of a known object on a given 2D photo of the object has\nnumerous advantages over existing methods. It does not require prior training,\nknowledge of the camera parameters, explicit point correspondences or matching\nfeatures between the image and model. Unlike techniques that estimate a partial\n3D pose (as in an overhead view of traffic or machine parts on a conveyor\nbelt), our method estimates the complete 3D pose of the object. It works on a\nsingle static image from a given view under varying and unknown lighting\nconditions. For this purpose we derive a novel illumination-invariant distance\nmeasure between the 2D photo and projected 3D model, which is then minimised to\nfind the best pose parameters. Results for vehicle pose detection in real\nphotographs are presented.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 01:54:50 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Jayawardena", "Srimal", ""], ["Hutter", "Marcus", ""], ["Brewer", "Nathan", ""]]}, {"id": "1311.7194", "submitter": "Dmitry Trifonov", "authors": "Dmitry Trifonov", "title": "Real-time High Resolution Fusion of Depth Maps on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system for live high quality surface reconstruction using a single moving\ndepth camera on a commodity hardware is presented. High accuracy and real-time\nframe rate is achieved by utilizing graphics hardware computing capabilities\nvia OpenCL and by using sparse data structure for volumetric surface\nrepresentation. Depth sensor pose is estimated by combining serial texture\nregistration algorithm with iterative closest points algorithm (ICP) aligning\nobtained depth map to the estimated scene model. Aligned surface is then fused\ninto the scene. Kalman filter is used to improve fusion quality. Truncated\nsigned distance function (TSDF) stored as block-based sparse buffer is used to\nrepresent surface. Use of sparse data structure greatly increases accuracy of\nscanned surfaces and maximum scanning area. Traditional GPU implementation of\nvolumetric rendering and fusion algorithms were modified to exploit sparsity to\nachieve desired performance. Incorporation of texture registration for sensor\npose estimation and Kalman filter for measurement integration improved accuracy\nand robustness of scanning process.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 03:17:03 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Trifonov", "Dmitry", ""]]}, {"id": "1311.7251", "submitter": "Michael Zibulevsky", "authors": "Joseph Shtok, Michael Zibulevsky and Michael Elad", "title": "Spatially-Adaptive Reconstruction in Computed Tomography using Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a supervised machine learning approach for boosting existing\nsignal and image recovery methods and demonstrate its efficacy on example of\nimage reconstruction in computed tomography. Our technique is based on a local\nnonlinear fusion of several image estimates, all obtained by applying a chosen\nreconstruction algorithm with different values of its control parameters.\nUsually such output images have different bias/variance trade-off. The fusion\nof the images is performed by feed-forward neural network trained on a set of\nknown examples. Numerical experiments show an improvement in reconstruction\nquality relatively to existing direct and iterative reconstruction methods.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 09:44:45 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Shtok", "Joseph", ""], ["Zibulevsky", "Michael", ""], ["Elad", "Michael", ""]]}, {"id": "1311.7295", "submitter": "Gerardo Aragon Camarasa", "authors": "Gerardo Aragon-Camarasa, Susanne B. Oehler, Yuan Liu, Sun Li, Paul\n  Cockshott and J. Paul Siebert", "title": "Glasgow's Stereo Image Database of Garments", "comments": "7 pages, 6 figure, image database", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  To provide insight into cloth perception and manipulation with an active\nbinocular robotic vision system, we compiled a database of 80 stereo-pair\ncolour images with corresponding horizontal and vertical disparity maps and\nmask annotations, for 3D garment point cloud rendering has been created and\nreleased. The stereo-image garment database is part of research conducted under\nthe EU-FP7 Clothes Perception and Manipulation (CloPeMa) project and belongs to\na wider database collection released through CloPeMa (www.clopema.eu). This\ndatabase is based on 16 different off-the-shelve garments. Each garment has\nbeen imaged in five different pose configurations on the project's binocular\nrobot head. A full copy of the database is made available for scientific\nresearch only at https://sites.google.com/site/ugstereodatabase/.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 12:09:28 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Aragon-Camarasa", "Gerardo", ""], ["Oehler", "Susanne B.", ""], ["Liu", "Yuan", ""], ["Li", "Sun", ""], ["Cockshott", "Paul", ""], ["Siebert", "J. Paul", ""]]}, {"id": "1311.7327", "submitter": "Sergios Petridis", "authors": "Sergios Petridis, Theodoros Giannakopoulos and Costantine D.\n  Spyropoulos", "title": "Unobtrusive Low Cost Pupil Size Measurements using Web cameras", "comments": "2nd International Workshop on Artificial Intelligence and Netmedicine\n  (NetMed'13), pages 9-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unobtrusive every day health monitoring can be of important use for the\nelderly population. In particular, pupil size may be a valuable source of\ninformation, since, apart from pathological cases, it can reveal the emotional\nstate, the fatigue and the ageing. To allow for unobtrusive monitoring to gain\nacceptance, one should seek for efficient methods of monitoring using com- mon\nlow-cost hardware. This paper describes a method for monitoring pupil sizes\nusing a common web camera in real time. Our method works by first detecting the\nface and the eyes area. Subsequently, optimal iris and sclera location and\nradius, modelled as ellipses, are found using efficient filtering. Finally, the\npupil center and radius is estimated by optimal filtering within the area of\nthe iris. Experimental result show both the efficiency and the effectiveness of\nour approach.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 14:25:43 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Petridis", "Sergios", ""], ["Giannakopoulos", "Theodoros", ""], ["Spyropoulos", "Costantine D.", ""]]}, {"id": "1311.7401", "submitter": "Eva-Maria Didden", "authors": "Eva-Maria Didden, Thordis Linda Thorarinsdottir, Alex Lenkoski,\n  Christoph Schn\\\"orr", "title": "Shape from Texture using Locally Scaled Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape from texture refers to the extraction of 3D information from 2D images\nwith irregular texture. This paper introduces a statistical framework to learn\nshape from texture where convex texture elements in a 2D image are represented\nthrough a point process. In a first step, the 2D image is preprocessed to\ngenerate a probability map corresponding to an estimate of the unnormalized\nintensity of the latent point process underlying the texture elements. The\nlatent point process is subsequently inferred from the probability map in a\nnon-parametric, model free manner. Finally, the 3D information is extracted\nfrom the point pattern by applying a locally scaled point process model where\nthe local scaling function represents the deformation caused by the projection\nof a 3D surface onto a 2D image.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 19:17:39 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Didden", "Eva-Maria", ""], ["Thorarinsdottir", "Thordis Linda", ""], ["Lenkoski", "Alex", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1311.7662", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Payman Yadollahpour, Yury Makarychev, Ruslan\n  Salakhutdinov, Nathan Srebro", "title": "The Power of Asymmetry in Binary Hashing", "comments": "Accepted to NIPS 2013, 9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When approximating binary similarity using the hamming distance between short\nbinary hashes, we show that even if the similarity is symmetric, we can have\nshorter and more accurate hashes by using two distinct code maps. I.e. by\napproximating the similarity between $x$ and $x'$ as the hamming distance\nbetween $f(x)$ and $g(x')$, for two distinct binary codes $f,g$, rather than as\nthe hamming distance between $f(x)$ and $f(x')$.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 18:53:32 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Yadollahpour", "Payman", ""], ["Makarychev", "Yury", ""], ["Salakhutdinov", "Ruslan", ""], ["Srebro", "Nathan", ""]]}]