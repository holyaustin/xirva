[{"id": "1511.00096", "submitter": "Garrick Orchard", "authors": "Garrick Orchard and Ralph Etienne-Cummings", "title": "Bioinspired Visual Motion Estimation", "comments": "16 pages, 11 figures, 1 table", "journal-ref": "Proceedings of the IEEE, vol.102, no.10, pp.1520-1536, Oct. 2014", "doi": "10.1109/JPROC.2014.2346763", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual motion estimation is a computationally intensive, but important task\nfor sighted animals. Replicating the robustness and efficiency of biological\nvisual motion estimation in artificial systems would significantly enhance the\ncapabilities of future robotic agents. 25 years ago, in this very journal,\nCarver Mead outlined his argument for replicating biological processing in\nsilicon circuits. His vision served as the foundation for the field of\nneuromorphic engineering, which has experienced a rapid growth in interest over\nrecent years as the ideas and technologies mature. Replicating biological\nvisual sensing was one of the first tasks attempted in the neuromorphic field.\nIn this paper we focus specifically on the task of visual motion estimation. We\ndescribe the task itself, present the progression of works from the early first\nattempts through to the modern day state-of-the-art, and provide an outlook for\nfuture directions in the field.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 07:46:59 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Orchard", "Garrick", ""], ["Etienne-Cummings", "Ralph", ""]]}, {"id": "1511.00098", "submitter": "Francesco Castaldo", "authors": "Francesco Castaldo, Amir Zamir, Roland Angst, Francesco Palmieri,\n  Silvio Savarese", "title": "Semantic Cross-View Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching cross-view images is challenging because the appearance and\nviewpoints are significantly different. While low-level features based on\ngradient orientations or filter responses can drastically vary with such\nchanges in viewpoint, semantic information of images however shows an invariant\ncharacteristic in this respect. Consequently, semantically labeled regions can\nbe used for performing cross-view matching. In this paper, we therefore explore\nthis idea and propose an automatic method for detecting and representing the\nsemantic information of an RGB image with the goal of performing cross-view\nmatching with a (non-RGB) geographic information system (GIS). A segmented\nimage forms the input to our system with segments assigned to semantic concepts\nsuch as traffic signs, lakes, roads, foliage, etc. We design a descriptor to\nrobustly capture both, the presence of semantic concepts and the spatial layout\nof those segments. Pairwise distances between the descriptors extracted from\nthe GIS map and the query image are then used to generate a shortlist of the\nmost promising locations with similar semantic concepts in a consistent spatial\nlayout. An experimental evaluation with challenging query images and a large\nurban area shows promising results.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 08:50:19 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Castaldo", "Francesco", ""], ["Zamir", "Amir", ""], ["Angst", "Roland", ""], ["Palmieri", "Francesco", ""], ["Savarese", "Silvio", ""]]}, {"id": "1511.00099", "submitter": "Anurag Mittal", "authors": "Sarthak Parui and Anurag Mittal", "title": "Sketch-based Image Retrieval from Millions of Images under Rotation,\n  Translation and Scale Variations", "comments": "submitted to IJCV, April 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proliferation of touch-based devices has made sketch-based image retrieval\npractical. While many methods exist for sketch-based object detection/image\nretrieval on small datasets, relatively less work has been done on large\n(web)-scale image retrieval. In this paper, we present an efficient approach\nfor image retrieval from millions of images based on user-drawn sketches.\nUnlike existing methods for this problem which are sensitive to even\ntranslation or scale variations, our method handles rotation, translation,\nscale (i.e. a similarity transformation) and small deformations. The object\nboundaries are represented as chains of connected segments and the database\nimages are pre-processed to obtain such chains that have a high chance of\ncontaining the object. This is accomplished using two approaches in this work:\na) extracting long chains in contour segment networks and b) extracting\nboundaries of segmented object proposals. These chains are then represented by\nsimilarity-invariant variable length descriptors. Descriptor similarities are\ncomputed by a fast Dynamic Programming-based partial matching algorithm. This\nmatching mechanism is used to generate a hierarchical k-medoids based indexing\nstructure for the extracted chains of all database images in an offline process\nwhich is used to efficiently retrieve a small set of possible matched images\nfor query chains. Finally, a geometric verification step is employed to test\ngeometric consistency of multiple chain matches to improve results. Qualitative\nand quantitative results clearly demonstrate superiority of the approach over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 08:50:43 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Parui", "Sarthak", ""], ["Mittal", "Anurag", ""]]}, {"id": "1511.00100", "submitter": "Garrick Orchard", "authors": "Garrick Orchard, Jacob G. Martin, R. Jacob Vogelstein, and Ralph\n  Etienne-Cummings", "title": "Fast Neuromimetic Object Recognition using FPGA Outperforms GPU\n  Implementations", "comments": "14 pages, 8 figures, 5 tables", "journal-ref": "Neural Networks and Learning Systems, IEEE Transactions on,\n  vol.24, no.8, pp.1239-1252, 2013", "doi": "10.1109/TNNLS.2013.2253563", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of objects in still images has traditionally been regarded as a\ndifficult computational problem. Although modern automated methods for visual\nobject recognition have achieved steadily increasing recognition accuracy, even\nthe most advanced computational vision approaches are unable to obtain\nperformance equal to that of humans. This has led to the creation of many\nbiologically-inspired models of visual object recognition, among them the HMAX\nmodel. HMAX is traditionally known to achieve high accuracy in visual object\nrecognition tasks at the expense of significant computational complexity.\nIncreasing complexity, in turn, increases computation time, reducing the number\nof images that can be processed per unit time. In this paper we describe how\nthe computationally intensive, biologically inspired HMAX model for visual\nobject recognition can be modified for implementation on a commercial Field\nProgrammable Gate Array, specifically the Xilinx Virtex 6 ML605 evaluation\nboard with XC6VLX240T FPGA. We show that with minor modifications to the\ntraditional HMAX model we can perform recognition on images of size 128x128\npixels at a rate of 190 images per second with a less than 1% loss in\nrecognition accuracy in both binary and multi-class visual object recognition\ntasks.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 08:59:03 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Orchard", "Garrick", ""], ["Martin", "Jacob G.", ""], ["Vogelstein", "R. Jacob", ""], ["Etienne-Cummings", "Ralph", ""]]}, {"id": "1511.00111", "submitter": "Mohammed Abdelsamea", "authors": "M. Abdelsamea", "title": "Regional Active Contours based on Variational level sets and Machine\n  Learning for Image Segmentation", "comments": "IMT PhD thesis, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation is the problem of partitioning an image into different\nsubsets, where each subset may have a different characterization in terms of\ncolor, intensity, texture, and/or other features. Segmentation is a fundamental\ncomponent of image processing, and plays a significant role in computer vision,\nobject recognition, and object tracking. Active Contour Models (ACMs)\nconstitute a powerful energy-based minimization framework for image\nsegmentation, which relies on the concept of contour evolution. Starting from\nan initial guess, the contour is evolved with the aim of approximating better\nand better the actual object boundary. Handling complex images in an efficient,\neffective, and robust way is a real challenge, especially in the presence of\nintensity inhomogeneity, overlap between the foreground/background intensity\ndistributions, objects characterized by many different intensities, and/or\nadditive noise. In this thesis, to deal with these challenges, we propose a\nnumber of image segmentation models relying on variational level set methods\nand specific kinds of neural networks, to handle complex images in both\nsupervised and unsupervised ways. Experimental results demonstrate the high\naccuracy of the segmentation results, obtained by the proposed models on\nvarious benchmark synthetic and real images compared with state-of-the-art\nactive contour models.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 11:17:15 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Abdelsamea", "M.", ""]]}, {"id": "1511.00175", "submitter": "Forrest Iandola", "authors": "Forrest N. Iandola, Khalid Ashraf, Matthew W. Moskewicz, Kurt Keutzer", "title": "FireCaffe: near-linear acceleration of deep neural network training on\n  compute clusters", "comments": "Version 2: Added results on 128 GPUs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long training times for high-accuracy deep neural networks (DNNs) impede\nresearch into new DNN architectures and slow the development of high-accuracy\nDNNs. In this paper we present FireCaffe, which successfully scales deep neural\nnetwork training across a cluster of GPUs. We also present a number of best\npractices to aid in comparing advancements in methods for scaling and\naccelerating the training of deep neural networks. The speed and scalability of\ndistributed algorithms is almost always limited by the overhead of\ncommunicating between servers; DNN training is not an exception to this rule.\nTherefore, the key consideration here is to reduce communication overhead\nwherever possible, while not degrading the accuracy of the DNN models that we\ntrain. Our approach has three key pillars. First, we select network hardware\nthat achieves high bandwidth between GPU servers -- Infiniband or Cray\ninterconnects are ideal for this. Second, we consider a number of communication\nalgorithms, and we find that reduction trees are more efficient and scalable\nthan the traditional parameter server approach. Third, we optionally increase\nthe batch size to reduce the total quantity of communication during DNN\ntraining, and we identify hyperparameters that allow us to reproduce the\nsmall-batch accuracy while training with large batch sizes. When training\nGoogLeNet and Network-in-Network on ImageNet, we achieve a 47x and 39x speedup,\nrespectively, when training on a cluster of 128 GPUs.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 21:13:30 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 06:50:36 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Iandola", "Forrest N.", ""], ["Ashraf", "Khalid", ""], ["Moskewicz", "Matthew W.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1511.00363", "submitter": "Matthieu Courbariaux", "authors": "Matthieu Courbariaux, Yoshua Bengio and Jean-Pierre David", "title": "BinaryConnect: Training Deep Neural Networks with binary weights during\n  propagations", "comments": "Accepted at NIPS 2015, 9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide\nrange of tasks, with the best results obtained with large training sets and\nlarge models. In the past, GPUs enabled these breakthroughs because of their\ngreater computational speed. In the future, faster computation at both training\nand test time is likely to be crucial for further progress and for consumer\napplications on low-power devices. As a result, there is much interest in\nresearch and development of dedicated hardware for Deep Learning (DL). Binary\nweights, i.e., weights which are constrained to only two possible values (e.g.\n-1 or 1), would bring great benefits to specialized DL hardware by replacing\nmany multiply-accumulate operations by simple accumulations, as multipliers are\nthe most space and power-hungry components of the digital implementation of\nneural networks. We introduce BinaryConnect, a method which consists in\ntraining a DNN with binary weights during the forward and backward\npropagations, while retaining precision of the stored weights in which\ngradients are accumulated. Like other dropout schemes, we show that\nBinaryConnect acts as regularizer and we obtain near state-of-the-art results\nwith BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 02:50:05 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2015 23:31:09 GMT"}, {"version": "v3", "created": "Mon, 18 Apr 2016 13:11:45 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Courbariaux", "Matthieu", ""], ["Bengio", "Yoshua", ""], ["David", "Jean-Pierre", ""]]}, {"id": "1511.00423", "submitter": "Xiaobai Li", "authors": "Xiaobai Li, Xiaopeng Hong, Antti Moilanen, Xiaohua Huang, Tomas\n  Pfister, Guoying Zhao and Matti Pietik\\\"ainen", "title": "Towards Reading Hidden Emotions: A comparative Study of Spontaneous\n  Micro-expression Spotting and Recognition Methods", "comments": null, "journal-ref": null, "doi": "10.1109/TAFFC.2017.2667642", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-expressions (MEs) are rapid, involuntary facial expressions which\nreveal emotions that people do not intend to show. Studying MEs is valuable as\nrecognizing them has many important applications, particularly in forensic\nscience and psychotherapy. However, analyzing spontaneous MEs is very\nchallenging due to their short duration and low intensity. Automatic ME\nanalysis includes two tasks: ME spotting and ME recognition. For ME spotting,\nprevious studies have focused on posed rather than spontaneous videos. For ME\nrecognition, the performance of previous studies is low. To address these\nchallenges, we make the following contributions: (i)We propose the first method\nfor spotting spontaneous MEs in long videos (by exploiting feature difference\ncontrast). This method is training free and works on arbitrary unseen videos.\n(ii)We present an advanced ME recognition framework, which outperforms previous\nwork by a large margin on two challenging spontaneous ME databases (SMIC and\nCASMEII). (iii)We propose the first automatic ME analysis system (MESR), which\ncan spot and recognize MEs from spontaneous video data. Finally, we show our\nmethod outperforms humans in the ME recognition task by a large margin, and\nachieves comparable performance to humans at the very challenging task of\nspotting and then recognizing spontaneous MEs.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 09:51:06 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 12:40:34 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Li", "Xiaobai", ""], ["Hong", "Xiaopeng", ""], ["Moilanen", "Antti", ""], ["Huang", "Xiaohua", ""], ["Pfister", "Tomas", ""], ["Zhao", "Guoying", ""], ["Pietik\u00e4inen", "Matti", ""]]}, {"id": "1511.00438", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Aniol Lidon, Marc Bola\\~nos, Mariella Dimiccoli, Petia Radeva, Maite\n  Garolera and Xavier Gir\\'o-i-Nieto", "title": "Semantic Summarization of Egocentric Photo Stream Events", "comments": "Oral paper at the ACM Multimedia 2017 Workshop on Lifelogging Tools\n  and Applications (LTA), Mountain View, California USA.\n  http://lta2017.computing.dcu.ie/", "journal-ref": null, "doi": "10.1145/3133202.3133204", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increase of users of wearable cameras in recent years and of\nthe amount of data they produce, there is a strong need for automatic retrieval\nand summarization techniques. This work addresses the problem of automatically\nsummarizing egocentric photo streams captured through a wearable camera by\ntaking an image retrieval perspective. After removing non-informative images by\na new CNN-based filter, images are ranked by relevance to ensure semantic\ndiversity and finally re-ranked by a novelty criterion to reduce redundancy. To\nassess the results, a new evaluation metric is proposed which takes into\naccount the non-uniqueness of the solution. Experimental results applied on a\ndatabase of 7,110 images from 6 different subjects and evaluated by experts\ngave 95.74% of experts satisfaction and a Mean Opinion Score of 4.57 out of\n5.0. Source code is available at\nhttps://github.com/imatge-upc/egocentric-2017-lta\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 10:41:34 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 10:23:18 GMT"}, {"version": "v3", "created": "Fri, 18 Aug 2017 16:59:38 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Lidon", "Aniol", ""], ["Bola\u00f1os", "Marc", ""], ["Dimiccoli", "Mariella", ""], ["Radeva", "Petia", ""], ["Garolera", "Maite", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""]]}, {"id": "1511.00461", "submitter": "Magnus Andersson", "authors": "Hanqing Zhang, Krister Wiklund, Magnus Andersson", "title": "Circle detection using isosceles triangles sampling", "comments": "Manuscript, 31 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of circular objects in digital images is an important problem in\nseveral vision applications. Circle detection using randomized sampling has\nbeen developed in recent years to reduce the computational intensity.\nRandomized sampling, however, is sensitive to noise that can lead to reduced\naccuracy and false-positive candidates. This paper presents a new circle\ndetection method based upon randomized isosceles triangles sampling to improve\nthe robustness of randomized circle detection in noisy conditions. It is shown\nthat the geometrical property of isosceles triangles provide a robust criterion\nto find relevant edge pixels and thereby efficiently provide an estimation of\nthe circle center and radii. The estimated results given by the isosceles\ntriangles sampling from each connected component of edge map were analyzed\nusing a simple clustering approach for efficiency. To further improve on the\naccuracy we applied a two-step refinement process using chords and linear error\ncompensation with gradient information of the edge pixels. Extensive\nexperiments using both synthetic and real images were presented and results\nwere compared to leading state-of-the-art algorithms and showed that the\nproposed algorithm: are efficient in finding circles with a low number of\niterations; has high rejection rate of false-positive circle candidates; and\nhas high robustness against noise, making it adaptive and useful in many vision\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 11:55:30 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Zhang", "Hanqing", ""], ["Wiklund", "Krister", ""], ["Andersson", "Magnus", ""]]}, {"id": "1511.00472", "submitter": "Pascal Mettes", "authors": "Pascal Mettes, Robby T. Tan, Remco C. Veltkamp", "title": "Water Detection through Spatio-Temporal Invariant Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim to segment and detect water in videos. Water detection\nis beneficial for appllications such as video search, outdoor surveillance, and\nsystems such as unmanned ground vehicles and unmanned aerial vehicles. The\nspecific problem, however, is less discussed compared to general texture\nrecognition. Here, we analyze several motion properties of water. First, we\ndescribe a video pre-processing step, to increase invariance against water\nreflections and water colours. Second, we investigate the temporal and spatial\nproperties of water and derive corresponding local descriptors. The descriptors\nare used to locally classify the presence of water and a binary water detection\nmask is generated through spatio-temporal Markov Random Field regularization of\nthe local classifications. Third, we introduce the Video Water Database,\ncontaining several hours of water and non-water videos, to validate our\nalgorithm. Experimental evaluation on the Video Water Database and the DynTex\ndatabase indicates the effectiveness of the proposed algorithm, outperforming\nmultiple algorithms for dynamic texture recognition and material recognition by\nca. 5% and 15% respectively.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 12:28:05 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2015 08:39:44 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Mettes", "Pascal", ""], ["Tan", "Robby T.", ""], ["Veltkamp", "Remco C.", ""]]}, {"id": "1511.00513", "submitter": "Martin Thoma", "authors": "Sebastian Bittel and Vitali Kaiser and Marvin Teichmann and Martin\n  Thoma", "title": "Pixel-wise Segmentation of Street with Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Pixel-wise street segmentation of photographs taken from a drivers\nperspective is important for self-driving cars and can also support other\nobject recognition tasks. A framework called SST was developed to examine the\naccuracy and execution time of different neural networks. The best neural\nnetwork achieved an $F_1$-score of 89.5% with a simple feedforward neural\nnetwork which trained to solve a regression task.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 14:23:22 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Bittel", "Sebastian", ""], ["Kaiser", "Vitali", ""], ["Teichmann", "Marvin", ""], ["Thoma", "Martin", ""]]}, {"id": "1511.00561", "submitter": "Alex Kendall", "authors": "Vijay Badrinarayanan and Alex Kendall and Roberto Cipolla", "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel and practical deep fully convolutional neural network\narchitecture for semantic pixel-wise segmentation termed SegNet. This core\ntrainable segmentation engine consists of an encoder network, a corresponding\ndecoder network followed by a pixel-wise classification layer. The architecture\nof the encoder network is topologically identical to the 13 convolutional\nlayers in the VGG16 network. The role of the decoder network is to map the low\nresolution encoder feature maps to full input resolution feature maps for\npixel-wise classification. The novelty of SegNet lies is in the manner in which\nthe decoder upsamples its lower resolution input feature map(s). Specifically,\nthe decoder uses pooling indices computed in the max-pooling step of the\ncorresponding encoder to perform non-linear upsampling. This eliminates the\nneed for learning to upsample. The upsampled maps are sparse and are then\nconvolved with trainable filters to produce dense feature maps. We compare our\nproposed architecture with the widely adopted FCN and also with the well known\nDeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory\nversus accuracy trade-off involved in achieving good segmentation performance.\n  SegNet was primarily motivated by scene understanding applications. Hence, it\nis designed to be efficient both in terms of memory and computational time\nduring inference. It is also significantly smaller in the number of trainable\nparameters than other competing architectures. We also performed a controlled\nbenchmark of SegNet and other architectures on both road scenes and SUN RGB-D\nindoor scene segmentation tasks. We show that SegNet provides good performance\nwith competitive inference time and more efficient inference memory-wise as\ncompared to other architectures. We also provide a Caffe implementation of\nSegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 15:51:03 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 13:56:56 GMT"}, {"version": "v3", "created": "Mon, 10 Oct 2016 21:11:59 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Badrinarayanan", "Vijay", ""], ["Kendall", "Alex", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1511.00758", "submitter": "Sudeep Pillai", "authors": "Sudeep Pillai, Srikumar Ramalingam, John J. Leonard", "title": "High-Performance and Tunable Stereo Reconstruction", "comments": "Accepted to International Conference on Robotics and Automation\n  (ICRA) 2016; 8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional stereo algorithms have focused their efforts on reconstruction\nquality and have largely avoided prioritizing for run time performance. Robots,\non the other hand, require quick maneuverability and effective computation to\nobserve its immediate environment and perform tasks within it. In this work, we\npropose a high-performance and tunable stereo disparity estimation method, with\na peak frame-rate of 120Hz (VGA resolution, on a single CPU-thread), that can\npotentially enable robots to quickly reconstruct their immediate surroundings\nand maneuver at high-speeds. Our key contribution is a disparity estimation\nalgorithm that iteratively approximates the scene depth via a piece-wise planar\nmesh from stereo imagery, with a fast depth validation step for semi-dense\nreconstruction. The mesh is initially seeded with sparsely matched keypoints,\nand is recursively tessellated and refined as needed (via a resampling stage),\nto provide the desired stereo disparity accuracy. The inherent simplicity and\nspeed of our approach, with the ability to tune it to a desired reconstruction\nquality and runtime performance makes it a compelling solution for applications\nin high-speed vehicles.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 02:20:56 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 17:29:14 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Pillai", "Sudeep", ""], ["Ramalingam", "Srikumar", ""], ["Leonard", "John J.", ""]]}, {"id": "1511.00871", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain", "title": "Properties of the Sample Mean in Graph Spaces and the\n  Majorize-Minimize-Mean Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most fundamental concepts in statistics is the concept of sample\nmean. Properties of the sample mean that are well-defined in Euclidean spaces\nbecome unwieldy or even unclear in graph spaces. Open problems related to the\nsample mean of graphs include: non-existence, non-uniqueness, statistical\ninconsistency, lack of convergence results of mean algorithms, non-existence of\nmidpoints, and disparity to midpoints. We present conditions to resolve all six\nproblems and propose a Majorize-Minimize-Mean (MMM) Algorithm. Experiments on\ngraph datasets representing images and molecules show that the MMM-Algorithm\nbest approximates a sample mean of graphs compared to six other mean\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 12:09:26 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Jain", "Brijnesh J.", ""]]}, {"id": "1511.01029", "submitter": "Vijay Badrinarayanan", "authors": "Vijay Badrinarayanan and Bamdev Mishra and Roberto Cipolla", "title": "Understanding symmetries in deep networks", "comments": "Accepted at the 8th NIPS Workshop on Optimization for Machine\n  Learning (OPT2015) to be held at Montreal, Canada on December 11, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have highlighted scale invariance or symmetry present in the\nweight space of a typical deep network and the adverse effect it has on the\nEuclidean gradient based stochastic gradient descent optimization. In this\nwork, we show that a commonly used deep network, which uses convolution, batch\nnormalization, reLU, max-pooling, and sub-sampling pipeline, possess more\ncomplex forms of symmetry arising from scaling-based reparameterization of the\nnetwork weights. We propose to tackle the issue of the weight space symmetry by\nconstraining the filters to lie on the unit-norm manifold. Consequently,\ntraining the network boils down to using stochastic gradient descent updates on\nthe unit-norm manifold. Our empirical evidence based on the MNIST dataset shows\nthat the proposed updates improve the test performance beyond what is achieved\nwith batch normalization and without sacrificing the computational efficiency\nof the weight updates.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 18:50:03 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Badrinarayanan", "Vijay", ""], ["Mishra", "Bamdev", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1511.01064", "submitter": "Alexandros Karargyris", "authors": "Alexandros Karargyris", "title": "Color Space Transformation Network", "comments": "Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Deep networks have become very popular over the past few years. The main\nreason for this widespread use is their excellent ability to learn and predict\nknowledge in a very easy and efficient way. Convolutional neural networks and\nauto-encoders have become the normal in the area of imaging and computer vision\nachieving unprecedented accuracy levels in many applications. The most common\nstrategy is to build and train networks with many layers by tuning their\nhyper-parameters. While this approach has proven to be a successful way to\nbuild robust deep learning schemes it suffers from high complexity. In this\npaper we introduce a module that learns color space transformations within a\nnetwork. Given a large dataset of colored images the color space transformation\nmodule tries to learn color space transformations that increase overall\nclassification accuracy. This module has shown to increase overall accuracy for\nthe same network design and to achieve faster convergence. It is part of a\nbroader family of image transformations (e.g. spatial transformer network).\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 13:25:20 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2015 21:17:27 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Karargyris", "Alexandros", ""]]}, {"id": "1511.01154", "submitter": "Stephan Saalfeld", "authors": "John A. Bogovic, Philipp Hanslovsky, Allan Wong, Stephan Saalfeld", "title": "Robust Registration of Calcium Images by Learned Contrast Synthesis", "comments": null, "journal-ref": "IEEE International Symposium on Biomedical Imaging, 2016, pages\n  1123--1126", "doi": "10.1109/ISBI.2016.7493463", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal image registration is a challenging task that is vital to fuse\ncomplementary signals for subsequent analyses. Despite much research into cost\nfunctions addressing this challenge, there exist cases in which these are\nineffective. In this work, we show that (1) this is true for the registration\nof in-vivo Drosophila brain volumes visualizing genetically encoded calcium\nindicators to an nc82 atlas and (2) that machine learning based contrast\nsynthesis can yield improvements. More specifically, the number of subjects for\nwhich the registration outright failed was greatly reduced (from 40% to 15%) by\nusing a synthesized image.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 23:08:18 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Bogovic", "John A.", ""], ["Hanslovsky", "Philipp", ""], ["Wong", "Allan", ""], ["Saalfeld", "Stephan", ""]]}, {"id": "1511.01156", "submitter": "Fabian Tschopp", "authors": "Fabian Tschopp, Marco Zorzi", "title": "Robust Large-Scale Localization in 3D Point Clouds Revisited", "comments": "6 pages; technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of getting a full 6-DOF pose estimation of a query\nimage inside a given point cloud. This technical report re-evaluates the\nalgorithms proposed by Y. Li et al. \"Worldwide Pose Estimation using 3D Point\nCloud\". Our code computes poses from 3 or 4 points, with both known and unknown\nfocal length. The results can easily be displayed and analyzed with Meshlab. We\nfound both advantages and shortcomings of the methods proposed. Furthermore,\nadditional priors and parameters for point selection, RANSAC and pose quality\nestimate (inlier test) are proposed and applied.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 23:10:47 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Tschopp", "Fabian", ""], ["Zorzi", "Marco", ""]]}, {"id": "1511.01161", "submitter": "Stephan Saalfeld", "authors": "Philipp Hanslovsky, John A. Bogovic, Stephan Saalfeld", "title": "Image-Based Correction of Continuous and Discontinuous Non-Planar Axial\n  Distortion in Serial Section Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Serial section microscopy is an established method for detailed\nanatomy reconstruction of biological specimen. During the last decade, high\nresolution electron microscopy (EM) of serial sections has become the de-facto\nstandard for reconstruction of neural connectivity at ever increasing scales\n(EM connectomics). In serial section microscopy, the axial dimension of the\nvolume is sampled by physically removing thin sections from the embedded\nspecimen and subsequently imaging either the block-face or the section series.\nThis process has limited precision leading to inhomogeneous non-planar sampling\nof the axial dimension of the volume which, in turn, results in distorted image\nvolumes. This includes that section series may be collected and imaged in\nunknown order.\n  Results: We developed methods to identify and correct these distortions\nthrough image-based signal analysis without any additional physical apparatus\nor measurements. We demonstrate the efficacy of our methods in proof of\nprinciple experiments and application to real world problems.\n  Availability and Implementation: We made our work available as libraries for\nthe ImageJ distribution Fiji and for deployment in a high performance parallel\ncomputing environment. Our sources are open and available at\nhttp://github.com/saalfeldla/section-sort,\nhttp://github.com/saalfeldlab/em-thickness-estimation, and\nhttp://github.com/saalfeldlab/z-spacing-spark.\n  Contact: saalfelds@janelia.hhmi.org\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 23:39:57 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 15:43:01 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Hanslovsky", "Philipp", ""], ["Bogovic", "John A.", ""], ["Saalfeld", "Stephan", ""]]}, {"id": "1511.01168", "submitter": "Paolo Frasconi", "authors": "Marco Paciscopi and Ludovico Silvestri and Francesco Saverio Pavone\n  and Paolo Frasconi", "title": "Cell identification in whole-brain multiview images of neural activation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable method for brain cell identification in multiview\nconfocal light sheet microscopy images. Our algorithmic pipeline includes a\nhierarchical registration approach and a novel multiview version of semantic\ndeconvolution that simultaneously enhance visibility of fluorescent cell\nbodies, equalize their contrast, and fuses adjacent views into a single 3D\nimages on which cell identification is performed with mean shift.\n  We present empirical results on a whole-brain image of an adult Arc-dVenus\nmouse acquired at 4micron resolution. Based on an annotated test volume\ncontaining 3278 cells, our algorithm achieves an $F_1$ measure of 0.89.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 00:34:29 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Paciscopi", "Marco", ""], ["Silvestri", "Ludovico", ""], ["Pavone", "Francesco Saverio", ""], ["Frasconi", "Paolo", ""]]}, {"id": "1511.01186", "submitter": "Hongyu Yang", "authors": "Hongyu Yang, Di Huang, Yunhong Wang, Heng Wang, Yuanyan Tang", "title": "Face Aging Effect Simulation using Hidden Factor Analysis Joint Sparse\n  Representation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2547587", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face aging simulation has received rising investigations nowadays, whereas it\nstill remains a challenge to generate convincing and natural age-progressed\nface images. In this paper, we present a novel approach to such an issue by\nusing hidden factor analysis joint sparse representation. In contrast to the\nmajority of tasks in the literature that handle the facial texture integrally,\nthe proposed aging approach separately models the person-specific facial\nproperties that tend to be stable in a relatively long period and the\nage-specific clues that change gradually over time. It then merely transforms\nthe age component to a target age group via sparse reconstruction, yielding\naging effects, which is finally combined with the identity component to achieve\nthe aged face. Experiments are carried out on three aging databases, and the\nresults achieved clearly demonstrate the effectiveness and robustness of the\nproposed method in rendering a face with aging effects. Additionally, a series\nof evaluations prove its validity with respect to identity preservation and\naging effect generation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 02:48:06 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Yang", "Hongyu", ""], ["Huang", "Di", ""], ["Wang", "Yunhong", ""], ["Wang", "Heng", ""], ["Tang", "Yuanyan", ""]]}, {"id": "1511.01245", "submitter": "Thierry Bouwmans", "authors": "Thierry Bouwmans, Andrews Sobral, Sajid Javed, Soon Ki Jung, El-Hadi\n  Zahzah", "title": "Decomposition into Low-rank plus Additive Matrices for\n  Background/Foreground Separation: A Review for a Comparative Evaluation with\n  a Large-Scale Dataset", "comments": "121 pages, 5 figures, submitted to Computer Science Review. arXiv\n  admin note: text overlap with arXiv:1312.7167, arXiv:1109.6297,\n  arXiv:1207.3438, arXiv:1105.2126, arXiv:1404.7592, arXiv:1210.0805,\n  arXiv:1403.8067 by other authors, Computer Science Review, November 2016", "journal-ref": null, "doi": "10.1016/j.cosrev.2016.11.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on problem formulations based on decomposition into low-rank\nplus sparse matrices shows a suitable framework to separate moving objects from\nthe background. The most representative problem formulation is the Robust\nPrincipal Component Analysis (RPCA) solved via Principal Component Pursuit\n(PCP) which decomposes a data matrix in a low-rank matrix and a sparse matrix.\nHowever, similar robust implicit or explicit decompositions can be made in the\nfollowing problem formulations: Robust Non-negative Matrix Factorization\n(RNMF), Robust Matrix Completion (RMC), Robust Subspace Recovery (RSR), Robust\nSubspace Tracking (RST) and Robust Low-Rank Minimization (RLRM). The main goal\nof these similar problem formulations is to obtain explicitly or implicitly a\ndecomposition into low-rank matrix plus additive matrices. In this context,\nthis work aims to initiate a rigorous and comprehensive review of the similar\nproblem formulations in robust subspace learning and tracking based on\ndecomposition into low-rank plus additive matrices for testing and ranking\nexisting algorithms for background/foreground separation. For this, we first\nprovide a preliminary review of the recent developments in the different\nproblem formulations which allows us to define a unified view that we called\nDecomposition into Low-rank plus Additive Matrices (DLAM). Then, we examine\ncarefully each method in each robust subspace learning/tracking frameworks with\ntheir decomposition, their loss functions, their optimization problem and their\nsolvers. Furthermore, we investigate if incremental algorithms and real-time\nimplementations can be achieved for background/foreground separation. Finally,\nexperimental results on a large-scale dataset called Background Models\nChallenge (BMC 2012) show the comparative performance of 32 different robust\nsubspace learning/tracking methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 08:51:59 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 08:35:59 GMT"}, {"version": "v3", "created": "Mon, 28 Nov 2016 12:48:44 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Bouwmans", "Thierry", ""], ["Sobral", "Andrews", ""], ["Javed", "Sajid", ""], ["Jung", "Soon Ki", ""], ["Zahzah", "El-Hadi", ""]]}, {"id": "1511.01293", "submitter": "Leonardo Parisi", "authors": "Andrea Cavagna, Chiara Creato, Lorenzo Del Castello, Stefania Melillo,\n  Leonardo Parisi, Massimiliano Viale", "title": "Towards a tracking algorithm based on the clustering of spatio-temporal\n  clouds of points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest in 3D dynamical tracking is growing in fields such as robotics,\nbiology and fluid dynamics. Recently, a major source of progress in 3D tracking\nhas been the study of collective behaviour in biological systems, where the\ntrajectories of individual animals moving within large and dense groups need to\nbe reconstructed to understand the behavioural interaction rules. Experimental\ndata in this field are generally noisy and at low spatial resolution, so that\nindividuals appear as small featureless objects and trajectories must be\nretrieved by making use of epipolar information only. Moreover, optical\nocclusions often occur: in a multi-camera system one or more objects become\nindistinguishable in one view, potentially jeopardizing the conservation of\nidentity over long-time trajectories. The most advanced 3D tracking algorithms\novercome optical occlusions making use of set-cover techniques, which however\nhave to solve NP-hard optimization problems. Moreover, current methods are not\nable to cope with occlusions arising from actual physical proximity of objects\nin 3D space. Here, we present a new method designed to work directly in 3D\nspace and time, creating (3D+1) clouds of points representing the full\nspatio-temporal evolution of the moving targets. We can then use a simple\nconnected components labeling routine, which is linear in time, to solve\noptical occlusions, hence lowering from NP to P the complexity of the problem.\nFinally, we use normalized cut spectral clustering to tackle 3D physical\nproximity.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 11:20:51 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Cavagna", "Andrea", ""], ["Creato", "Chiara", ""], ["Del Castello", "Lorenzo", ""], ["Melillo", "Stefania", ""], ["Parisi", "Leonardo", ""], ["Viale", "Massimiliano", ""]]}, {"id": "1511.01508", "submitter": "Bryan Poling", "authors": "Bryan Poling and Gilad Lerman", "title": "Enhancing Feature Tracking With Gyro Regularization", "comments": "Preprint submitted to Image and Vision Computing", "journal-ref": "Image and Vision Computing 50 (2016) 42-58", "doi": "10.1016/j.imavis.2016.01.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deeply integrated method of exploiting low-cost gyroscopes to\nimprove general purpose feature tracking. Most previous methods use gyroscopes\nto initialize and bound the search for features. In contrast, we use them to\nregularize the tracking energy function so that they can directly assist in the\ntracking of ambiguous and poor-quality features. We demonstrate that our simple\ntechnique offers significant improvements in performance over conventional\ntemplate-based tracking methods, and is in fact competitive with more complex\nand computationally expensive state-of-the-art trackers, but at a fraction of\nthe computational cost. Additionally, we show that the practice of initializing\ntemplate-based feature trackers like KLT (Kanade-Lucas-Tomasi) using\ngyro-predicted optical flow offers no advantage over using a careful\noptical-only initialization method, suggesting that some deeper level of\nintegration, like the method we propose, is needed in order to realize a\ngenuine improvement in tracking performance from these inertial sensors.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 21:04:07 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Poling", "Bryan", ""], ["Lerman", "Gilad", ""]]}, {"id": "1511.01619", "submitter": "Manjunath Narayana", "authors": "Manjunath Narayana, Allen Hanson, Erik Learned-Miller", "title": "Coherent Motion Segmentation in Moving Camera Videos using Optical Flow\n  Orientations", "comments": "8 pages, 5 figures, in 2013 IEEE International Conference on Computer\n  Vision (ICCV)", "journal-ref": null, "doi": "10.1109/ICCV.2013.199", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In moving camera videos, motion segmentation is commonly performed using the\nimage plane motion of pixels, or optical flow. However, objects that are at\ndifferent depths from the camera can exhibit different optical flows even if\nthey share the same real-world motion. This can cause a depth-dependent\nsegmentation of the scene. Our goal is to develop a segmentation algorithm that\nclusters pixels that have similar real-world motion irrespective of their depth\nin the scene. Our solution uses optical flow orientations instead of the\ncomplete vectors and exploits the well-known property that under camera\ntranslation, optical flow orientations are independent of object depth. We\nintroduce a probabilistic model that automatically estimates the number of\nobserved independent motions and results in a labeling that is consistent with\nreal-world motion in the scene. The result of our system is that static objects\nare correctly identified as one segment, even if they are at different depths.\nColor features and information from previous frames in the video sequence are\nused to correct occasional errors due to the orientation-based segmentation. We\npresent results on more than thirty videos from different benchmarks. The\nsystem is particularly robust on complex background scenes containing objects\nat significantly different depths\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 06:10:13 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Narayana", "Manjunath", ""], ["Hanson", "Allen", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "1511.01627", "submitter": "Manjunath Narayana", "authors": "Manjunath Narayana, Allen Hanson, Erik Learned-Miller", "title": "Background subtraction - separating the modeling and the inference", "comments": "19 pages, 6 figures, Machine Vision and Applications journal", "journal-ref": "Machine Vision and Applications July 2014, Volume 25, Issue 5, pp\n  1163-1174", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In its early implementations, background modeling was a process of building a\nmodel for the background of a video with a stationary camera, and identifying\npixels that did not conform well to this model. The pixels that were not\nwell-described by the background model were assumed to be moving objects. Many\nsystems today maintain models for the foreground as well as the background, and\nthese models compete to explain the pixels in a video. In this paper, we argue\nthat the logical endpoint of this evolution is to simply use Bayes' rule to\nclassify pixels. In particular, it is essential to have a background\nlikelihood, a foreground likelihood, and a prior at each pixel. A simple\napplication of Bayes' rule then gives a posterior probability over the label.\nThe only remaining question is the quality of the component models: the\nbackground likelihood, the foreground likelihood, and the prior. We describe a\nmodel for the likelihoods that is built by using not only the past observations\nat a given pixel location, but by also including observations in a spatial\nneighborhood around the location. This enables us to model the influence\nbetween neighboring pixels and is an improvement over earlier pixelwise models\nthat do not allow for such influence. Although similar in spirit to the joint\ndomain-range model, we show that our model overcomes certain deficiencies in\nthat model. We use a spatially dependent prior for the background and\nforeground. The background and foreground labels from the previous frame, after\nspatial smoothing to account for movement of objects,are used to build the\nprior for the current frame.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 06:57:38 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Narayana", "Manjunath", ""], ["Hanson", "Allen", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "1511.01631", "submitter": "Manjunath Narayana", "authors": "Manjunath Narayana, Allen Hanson, Erik Learned-Miller", "title": "Background Modeling Using Adaptive Pixelwise Kernel Variances in a\n  Hybrid Feature Space", "comments": "8 pages, 4 figures, CVPR 2012 conference paper in CVPR '12\n  Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR)", "journal-ref": null, "doi": "10.1109/CVPR.2012.6247916", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on background subtraction has shown developments on two major\nfronts. In one, there has been increasing sophistication of probabilistic\nmodels, from mixtures of Gaussians at each pixel [7], to kernel density\nestimates at each pixel [1], and more recently to joint domainrange density\nestimates that incorporate spatial information [6]. Another line of work has\nshown the benefits of increasingly complex feature representations, including\nthe use of texture information, local binary patterns, and recently\nscale-invariant local ternary patterns [4]. In this work, we use joint\ndomain-range based estimates for background and foreground scores and show that\ndynamically choosing kernel variances in our kernel estimates at each\nindividual pixel can significantly improve results. We give a heuristic method\nfor selectively applying the adaptive kernel calculations which is nearly as\naccurate as the full procedure but runs much faster. We combine these modeling\nimprovements with recently developed complex features [4] and show significant\nimprovements on a standard backgrounding benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 07:18:05 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Narayana", "Manjunath", ""], ["Hanson", "Allen", ""], ["Learned-Miller", "Erik", ""]]}, {"id": "1511.01706", "submitter": "Huilin Gao", "authors": "Huilin Gao, Wenjie Chen, and Lihua Dou", "title": "Image classification based on support vector machine and the fusion of\n  complementary features", "comments": "22 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Classification based on BOW (Bag-of-words) has broad application\nprospect in pattern recognition field but the shortcomings are existed because\nof single feature and low classification accuracy. To this end we combine three\ningredients: (i) Three features with functions of mutual complementation are\nadopted to describe the images, including PHOW (Pyramid Histogram of Words),\nPHOC (Pyramid Histogram of Color) and PHOG (Pyramid Histogram of Orientated\nGradients). (ii) The improvement of traditional BOW model is presented by using\ndense sample and an improved K-means clustering method for constructing the\nvisual dictionary. (iii) An adaptive feature-weight adjusted image\ncategorization algorithm based on the SVM and the fusion of multiple features\nis adopted. Experiments carried out on Caltech 101 database confirm the\nvalidity of the proposed approach. From the experimental results can be seen\nthat the classification accuracy rate of the proposed method is improved by\n7%-17% higher than that of the traditional BOW methods. This algorithm makes\nfull use of global, local and spatial information and has significant\nimprovements to the classification accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 11:57:28 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Gao", "Huilin", ""], ["Chen", "Wenjie", ""], ["Dou", "Lihua", ""]]}, {"id": "1511.01726", "submitter": "Ata Ur-Rehman", "authors": "Ata-ur-Rehman, Syed Mohsen Naqvi, Lyudmila Mihaylova, Jonathon\n  Chambers", "title": "Multi-Target Tracking and Occlusion Handling with Learned Variational\n  Bayesian Clusters and a Social Force Model", "comments": "19 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of multiple human target tracking in a\nsequence of video data. A solution is proposed which is able to deal with the\nchallenges of a varying number of targets, interactions and when every target\ngives rise to multiple measurements. The developed novel algorithm comprises\nvariational Bayesian clustering combined with a social force model, integrated\nwithin a particle filter with an enhanced prediction step. It performs\nmeasurement-to-target association by automatically detecting the measurement\nrelevance. The performance of the developed algorithm is evaluated over several\nsequences from publicly available data sets: AV16.3, CAVIAR and PETS2006, which\ndemonstrates that the proposed algorithm successfully initializes and tracks a\nvariable number of targets in the presence of complex occlusions. A comparison\nwith state-of-the-art techniques due to Khan et al., Laet et al. and Czyz et\nal. shows improved tracking performance.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 13:01:35 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Ata-ur-Rehman", "", ""], ["Naqvi", "Syed Mohsen", ""], ["Mihaylova", "Lyudmila", ""], ["Chambers", "Jonathon", ""]]}, {"id": "1511.01754", "submitter": "Bamdev Mishra", "authors": "Vijay Badrinarayanan and Bamdev Mishra and Roberto Cipolla", "title": "Symmetry-invariant optimization in deep networks", "comments": "Submitted to ICLR 2016. arXiv admin note: text overlap with\n  arXiv:1511.01029", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have highlighted scale invariance or symmetry that is present in\nthe weight space of a typical deep network and the adverse effect that it has\non the Euclidean gradient based stochastic gradient descent optimization. In\nthis work, we show that these and other commonly used deep networks, such as\nthose which use a max-pooling and sub-sampling layer, possess more complex\nforms of symmetry arising from scaling based reparameterization of the network\nweights. We then propose two symmetry-invariant gradient based weight updates\nfor stochastic gradient descent based learning. Our empirical evidence based on\nthe MNIST dataset shows that these updates improve the test performance without\nsacrificing the computational efficiency of the weight updates. We also show\nthe results of training with one of the proposed weight updates on an image\nsegmentation problem.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 14:17:40 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 19:01:03 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Badrinarayanan", "Vijay", ""], ["Mishra", "Bamdev", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1511.01804", "submitter": "Shuaiqi Hu", "authors": "Shuaiqi Hu, Ke Li, Xudong Bao", "title": "Wood Species Recognition Based on SIFT Keypoint Histogram", "comments": "CISP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Traditionally, only experts who are equipped with professional knowledge and\nrich experience are able to recognize different species of wood. Applying image\nprocessing techniques for wood species recognition can not only reduce the\nexpense to train qualified identifiers, but also increase the recognition\naccuracy. In this paper, a wood species recognition technique base on Scale\nInvariant Feature Transformation (SIFT) keypoint histogram is proposed. We use\nfirst the SIFT algorithm to extract keypoints from wood cross section images,\nand then k-means and k-means++ algorithms are used for clustering. Using the\nclustering results, an SIFT keypoints histogram is calculated for each wood\nimage. Furthermore, several classification models, including Artificial Neural\nNetworks (ANN), Support Vector Machine (SVM) and K-Nearest Neighbor (KNN) are\nused to verify the performance of the method. Finally, through comparing with\nother prevalent wood recognition methods such as GLCM and LBP, results show\nthat our scheme achieves higher accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 16:33:54 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2015 10:17:38 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2015 09:35:18 GMT"}, {"version": "v4", "created": "Wed, 16 Dec 2015 03:49:07 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Hu", "Shuaiqi", ""], ["Li", "Ke", ""], ["Bao", "Xudong", ""]]}, {"id": "1511.01865", "submitter": "Seyed Mostafa Kia", "authors": "Nastaran Mohammadian Rad, Andrea Bizzego, Seyed Mostafa Kia, Giuseppe\n  Jurman, Paola Venuti, Cesare Furlanello", "title": "Convolutional Neural Network for Stereotypical Motor Movement Detection\n  in Autism", "comments": "Presented at 5th NIPS Workshop on Machine Learning and Interpretation\n  in Neuroimaging (MLINI), 2015, (http://arxiv.org/html/1605.04435), Report-no:\n  MLINI/2015/13", "journal-ref": null, "doi": null, "report-no": "MLINI/2015/13", "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism Spectrum Disorders (ASDs) are often associated with specific atypical\npostural or motor behaviors, of which Stereotypical Motor Movements (SMMs) have\na specific visibility. While the identification and the quantification of SMM\npatterns remain complex, its automation would provide support to accurate\ntuning of the intervention in the therapy of autism. Therefore, it is essential\nto develop automatic SMM detection systems in a real world setting, taking care\nof strong inter-subject and intra-subject variability. Wireless accelerometer\nsensing technology can provide a valid infrastructure for real-time SMM\ndetection, however such variability remains a problem also for machine learning\nmethods, in particular whenever handcrafted features extracted from\naccelerometer signal are considered. Here, we propose to employ the deep\nlearning paradigm in order to learn discriminating features from multi-sensor\naccelerometer signals. Our results provide preliminary evidence that feature\nlearning and transfer learning embedded in the deep architecture achieve higher\naccurate SMM detectors in longitudinal scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 19:36:33 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 21:02:02 GMT"}, {"version": "v3", "created": "Tue, 7 Jun 2016 19:11:34 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Rad", "Nastaran Mohammadian", ""], ["Bizzego", "Andrea", ""], ["Kia", "Seyed Mostafa", ""], ["Jurman", "Giuseppe", ""], ["Venuti", "Paola", ""], ["Furlanello", "Cesare", ""]]}, {"id": "1511.01887", "submitter": "Vladislav Malyshkin", "authors": "Vladislav Gennadievich Malyshkin", "title": "Radon-Nikodym approximation in application to image analysis", "comments": "Images interpolated with d_x=d_y=100 are added to show the\n  practicality of high order moments calculation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an image pixel information can be converted to the moments of some basis\n$Q_k$, e.g. Fourier-Mellin, Zernike, monomials, etc. Given sufficient number of\nmoments pixel information can be completely recovered, for insufficient number\nof moments only partial information can be recovered and the image\nreconstruction is, at best, of interpolatory type. Standard approach is to\npresent interpolated value as a linear combination of basis functions, what is\nequivalent to least squares expansion. However, recent progress in numerical\nstability of moments estimation allows image information to be recovered from\nmoments in a completely different manner, applying Radon-Nikodym type of\nexpansion, what gives the result as a ratio of two quadratic forms of basis\nfunctions. In contrast with least squares the Radon-Nikodym approach has\noscillation near the boundaries very much suppressed and does not diverge\noutside of basis support. While least squares theory operate with vectors\n$<fQ_k>$, Radon-Nikodym theory operates with matrices $<fQ_jQ_k>$, what make\nthe approach much more suitable to image transforms and statistical property\nestimation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 20:43:01 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 19:20:17 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Malyshkin", "Vladislav Gennadievich", ""]]}, {"id": "1511.01954", "submitter": "Jose Oramas", "authors": "Jose Oramas M. and Tinne Tuytelaars", "title": "Recovering hard-to-find object instances by sampling context-based\n  object proposals", "comments": "Computer Vision and Image Understanding (CVIU)", "journal-ref": null, "doi": "10.1016/j.cviu.2016.08.007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on improving object detection performance in terms of\nrecall. We propose a post-detection stage during which we explore the image\nwith the objective of recovering missed detections. This exploration is\nperformed by sampling object proposals in the image. We analyze four different\nstrategies to perform this sampling, giving special attention to strategies\nthat exploit spatial relations between objects. In addition, we propose a novel\nmethod to discover higher-order relations between groups of objects.\nExperiments on the challenging KITTI dataset show that our proposed\nrelations-based proposal generation strategies can help improving recall at the\ncost of a relatively low amount of object proposals.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 23:39:19 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 14:50:24 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2016 08:30:40 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["M.", "Jose Oramas", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1511.01966", "submitter": "Ankit Parekh", "authors": "Ankit Parekh and Ivan W. Selesnick", "title": "Enhanced Low-Rank Matrix Approximation", "comments": "5 pages, 2 figures. MATLAB code available at https://goo.gl/xAi85N", "journal-ref": "IEEE Signal Processing Letters, vol. 23, no. 4, pp.493-497, Apr.\n  2016", "doi": "10.1109/LSP.2016.2535227", "report-no": null, "categories": "cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter proposes to estimate low-rank matrices by formulating a convex\noptimization problem with non-convex regularization. We employ parameterized\nnon-convex penalty functions to estimate the non-zero singular values more\naccurately than the nuclear norm. A closed-form solution for the global optimum\nof the proposed objective function (sum of data fidelity and the non-convex\nregularizer) is also derived. The solution reduces to singular value\nthresholding method as a special case. The proposed method is demonstrated for\nimage denoising.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 01:19:18 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 14:42:31 GMT"}, {"version": "v3", "created": "Tue, 22 Mar 2016 18:16:36 GMT"}, {"version": "v4", "created": "Tue, 12 Apr 2016 21:21:48 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Parekh", "Ankit", ""], ["Selesnick", "Ivan W.", ""]]}, {"id": "1511.01994", "submitter": "Julian Yarkony", "authors": "Julian Yarkony", "title": "Next Generation Multicuts for Semi-Planar Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of multicut segmentation. We introduce modified versions\nof the Semi-PlanarCC based on bounding Lagrange multipliers. We apply our work\nto natural image segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 07:18:56 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Yarkony", "Julian", ""]]}, {"id": "1511.02023", "submitter": "Mohammadamin Abbasnejad", "authors": "Mohammadamin Abbasnejad, Mohammad Ali Masnadi-Shirazi", "title": "Facial Expression Recognition Using Sparse Gaussian Conditional Random\n  Field", "comments": "http://waset.org/abstracts/computer-and-information-engineering/26245. arXiv\n  admin note: text overlap with arXiv:1509.01343 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of expression and facial Action Units (AUs) detection are very\nimportant tasks in fields of computer vision and Human Computer Interaction\n(HCI) due to the wide range of applications in human life. Many works has been\ndone during the past few years which has their own advantages and\ndisadvantages. In this work we present a new model based on Gaussian\nConditional Random Field. We solve our objective problem using ADMM and we show\nhow well the proposed model works. We train and test our work on two facial\nexpression datasets, CK+ and RU-FACS. Experimental evaluation shows that our\nproposed approach outperform state of the art expression recognition.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 10:29:09 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Abbasnejad", "Mohammadamin", ""], ["Masnadi-Shirazi", "Mohammad Ali", ""]]}, {"id": "1511.02126", "submitter": "Yahong Han", "authors": "Shichao Zhao, Yanbin Liu, Yahong Han, Richang Hong", "title": "Pooling the Convolutional Layers in Deep ConvNets for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep ConvNets have shown its good performance in image classification tasks.\nHowever it still remains as a problem in deep video representation for action\nrecognition. The problem comes from two aspects: on one hand, current video\nConvNets are relatively shallow compared with image ConvNets, which limits its\ncapability of capturing the complex video action information; on the other\nhand, temporal information of videos is not properly utilized to pool and\nencode the video sequences. Towards these issues, in this paper, we utilize two\nstate-of-the-art ConvNets, i.e., the very deep spatial net (VGGNet) and the\ntemporal net from Two-Stream ConvNets, for action representation. The\nconvolutional layers and the proposed new layer, called frame-diff layer, are\nextracted and pooled with two temporal pooling strategy: Trajectory pooling and\nline pooling. The pooled local descriptors are then encoded with VLAD to form\nthe video representations. In order to verify the effectiveness of the proposed\nframework, we conduct experiments on UCF101 and HMDB51 datasets. It achieves\nthe accuracy of 93.78\\% on UCF101 which is the state-of-the-art and the\naccuracy of 65.62\\% on HMDB51 which is comparable to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 15:51:07 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Zhao", "Shichao", ""], ["Liu", "Yanbin", ""], ["Han", "Yahong", ""], ["Hong", "Richang", ""]]}, {"id": "1511.02228", "submitter": "Radu Timofte", "authors": "Radu Timofte and Rasmus Rothe and Luc Van Gool", "title": "Seven ways to improve example-based single image super resolution", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present seven techniques that everybody should know to\nimprove example-based single image super resolution (SR): 1) augmentation of\ndata, 2) use of large dictionaries with efficient search structures, 3)\ncascading, 4) image self-similarities, 5) back projection refinement, 6)\nenhanced prediction by consistency check, and 7) context reasoning. We validate\nour seven techniques on standard SR benchmarks (i.e. Set5, Set14, B100) and\nmethods (i.e. A+, SRCNN, ANR, Zeyde, Yang) and achieve substantial\nimprovements.The techniques are widely applicable and require no changes or\nonly minor adjustments of the SR methods. Moreover, our Improved A+ (IA) method\nsets new state-of-the-art results outperforming A+ by up to 0.9dB on average\nPSNR whilst maintaining a low time complexity.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 20:58:20 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Timofte", "Radu", ""], ["Rothe", "Rasmus", ""], ["Van Gool", "Luc", ""]]}, {"id": "1511.02251", "submitter": "Armand Joulin", "authors": "Armand Joulin, Laurens van der Maaten, Allan Jabri, Nicolas Vasilache", "title": "Learning Visual Features from Large Weakly Supervised Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks trained on large supervised dataset produce visual\nfeatures which form the basis for the state-of-the-art in many computer-vision\nproblems. Further improvements of these visual features will likely require\neven larger manually labeled data sets, which severely limits the pace at which\nprogress can be made. In this paper, we explore the potential of leveraging\nmassive, weakly-labeled image collections for learning good visual features. We\ntrain convolutional networks on a dataset of 100 million Flickr photos and\ncaptions, and show that these networks produce features that perform well in a\nrange of vision problems. We also show that the networks appropriately capture\nword similarity, and learn correspondences between different languages.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 22:08:37 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Joulin", "Armand", ""], ["van der Maaten", "Laurens", ""], ["Jabri", "Allan", ""], ["Vasilache", "Nicolas", ""]]}, {"id": "1511.02274", "submitter": "Zichao Yang", "authors": "Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Smola", "title": "Stacked Attention Networks for Image Question Answering", "comments": "test-dev/standard results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents stacked attention networks (SANs) that learn to answer\nnatural language questions from images. SANs use semantic representation of a\nquestion as query to search for the regions in an image that are related to the\nanswer. We argue that image question answering (QA) often requires multiple\nsteps of reasoning. Thus, we develop a multiple-layer SAN in which we query an\nimage multiple times to infer the answer progressively. Experiments conducted\non four image QA data sets demonstrate that the proposed SANs significantly\noutperform previous state-of-the-art approaches. The visualization of the\nattention layers illustrates the progress that the SAN locates the relevant\nvisual clues that lead to the answer of the question layer-by-layer.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 00:43:32 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 20:37:49 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Yang", "Zichao", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""], ["Smola", "Alex", ""]]}, {"id": "1511.02282", "submitter": "Lianwen Jin", "authors": "Xiaorui Liu, Yichao Huang, Xin Zhang, Lianwen Jin", "title": "Fingertip in the Eye: A cascaded CNN pipeline for the real-time\n  fingertip detection in egocentric videos", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new pipeline for hand localization and fingertip detection.\nFor RGB images captured from an egocentric vision mobile camera, hand and\nfingertip detection remains a challenging problem due to factors like\nbackground complexity and hand shape variety. To address these issues\naccurately and robustly, we build a large scale dataset named Ego-Fingertip and\npropose a bi-level cascaded pipeline of convolutional neural networks, namely,\nAttention-based Hand Detector as well as Multi-point Fingertip Detector. The\nproposed method significantly tackles challenges and achieves satisfactorily\naccurate prediction and real-time performance compared to previous hand and\nfingertip detection methods.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 02:06:11 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Liu", "Xiaorui", ""], ["Huang", "Yichao", ""], ["Zhang", "Xin", ""], ["Jin", "Lianwen", ""]]}, {"id": "1511.02283", "submitter": "Junhua Mao", "authors": "Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan\n  Yuille, Kevin Murphy", "title": "Generation and Comprehension of Unambiguous Object Descriptions", "comments": "We have released the Google Refexp dataset together with a toolbox\n  for visualization and evaluation, see\n  https://github.com/mjhucla/Google_Refexp_toolbox. Camera ready version for\n  CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method that can generate an unambiguous description (known as a\nreferring expression) of a specific object or region in an image, and which can\nalso comprehend or interpret such an expression to infer which object is being\ndescribed. We show that our method outperforms previous methods that generate\ndescriptions of objects without taking into account other potentially ambiguous\nobjects in the scene. Our model is inspired by recent successes of deep\nlearning methods for image captioning, but while image captioning is difficult\nto evaluate, our task allows for easy objective evaluation. We also present a\nnew large-scale dataset for referring expressions, based on MS-COCO. We have\nreleased the dataset and a toolbox for visualization and evaluation, see\nhttps://github.com/mjhucla/Google_Refexp_toolbox\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 02:17:36 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2015 08:58:08 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2016 01:11:56 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Mao", "Junhua", ""], ["Huang", "Jonathan", ""], ["Toshev", "Alexander", ""], ["Camburu", "Oana", ""], ["Yuille", "Alan", ""], ["Murphy", "Kevin", ""]]}, {"id": "1511.02300", "submitter": "Shuran Song", "authors": "Shuran Song, Jianxiong Xiao", "title": "Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the task of amodal 3D object detection in RGB-D images, which\naims to produce a 3D bounding box of an object in metric form at its full\nextent. We introduce Deep Sliding Shapes, a 3D ConvNet formulation that takes a\n3D volumetric scene from a RGB-D image as input and outputs 3D object bounding\nboxes. In our approach, we propose the first 3D Region Proposal Network (RPN)\nto learn objectness from geometric shapes and the first joint Object\nRecognition Network (ORN) to extract geometric features in 3D and color\nfeatures in 2D. In particular, we handle objects of various sizes by training\nan amodal RPN at two different scales and an ORN to regress 3D bounding boxes.\nExperiments show that our algorithm outperforms the state-of-the-art by 13.8 in\nmAP and is 200x faster than the original Sliding Shapes. All source code and\npre-trained models will be available at GitHub.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 04:34:18 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 19:21:23 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Song", "Shuran", ""], ["Xiao", "Jianxiong", ""]]}, {"id": "1511.02319", "submitter": "Mohamad Hanif Md Saad", "authors": "Mohammad Ali Saghafi, Aini Hussain, Halimah Badioze Zaman, Mohamad\n  Hanif Md Saad", "title": "Review of Person Re-identification Techniques", "comments": "Published 2014", "journal-ref": "IET Computer Vision, 2014, 8, (6), p. 455-474", "doi": "10.1049/iet-cvi.2013.0180", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification across different surveillance cameras with disjoint\nfields of view has become one of the most interesting and challenging subjects\nin the area of intelligent video surveillance. Although several methods have\nbeen developed and proposed, certain limitations and unresolved issues remain.\nIn all of the existing re-identification approaches, feature vectors are\nextracted from segmented still images or video frames. Different similarity or\ndissimilarity measures have been applied to these vectors. Some methods have\nused simple constant metrics, whereas others have utilised models to obtain\noptimised metrics. Some have created models based on local colour or texture\ninformation, and others have built models based on the gait of people. In\ngeneral, the main objective of all these approaches is to achieve a\nhigher-accuracy rate and lowercomputational costs. This study summarises\nseveral developments in recent literature and discusses the various available\nmethods used in person re-identification. Specifically, their advantages and\ndisadvantages are mentioned and compared.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 08:26:19 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Saghafi", "Mohammad Ali", ""], ["Hussain", "Aini", ""], ["Zaman", "Halimah Badioze", ""], ["Saad", "Mohamad Hanif Md", ""]]}, {"id": "1511.02407", "submitter": "Sohini Roychowdhury", "authors": "Sohini Roychowdhury and Michelle Emmons", "title": "A Survey of the Trends in Facial and Expression Recognition Databases\n  and Methods", "comments": "16 pages, 4 figures, 3 tables, International Journal of Computer\n  Science and Engineering Survey, October, 2015", "journal-ref": "International Journal of Computer Science & Engineering Survey,\n  2015, 6, 1-19", "doi": "10.5121/ijcses.2015.6501", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated facial identification and facial expression recognition have been\ntopics of active research over the past few decades. Facial and expression\nrecognition find applications in human-computer interfaces, subject tracking,\nreal-time security surveillance systems and social networking. Several holistic\nand geometric methods have been developed to identify faces and expressions\nusing public and local facial image databases. In this work we present the\nevolution in facial image data sets and the methodologies for facial\nidentification and recognition of expressions such as anger, sadness,\nhappiness, disgust, fear and surprise. We observe that most of the earlier\nmethods for facial and expression recognition aimed at improving the\nrecognition rates for facial feature-based methods using static images.\nHowever, the recent methodologies have shifted focus towards robust\nimplementation of facial/expression recognition from large image databases that\nvary with space (gathered from the internet) and time (video recordings). The\nevolution trends in databases and methodologies for facial and expression\nrecognition can be useful for assessing the next-generation topics that may\nhave applications in security systems or personal identification systems that\ninvolve \"Quantitative face\" assessments.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 22:05:12 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2015 14:01:38 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Roychowdhury", "Sohini", ""], ["Emmons", "Michelle", ""]]}, {"id": "1511.02459", "submitter": "Lianwen Jin", "authors": "Duorui Xie, Lingyu Liang, Lianwen Jin, Jie Xu, Mengru Li", "title": "SCUT-FBP: A Benchmark Dataset for Facial Beauty Perception", "comments": "6 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel face dataset with attractiveness ratings, namely, the\nSCUT-FBP dataset, is developed for automatic facial beauty perception. This\ndataset provides a benchmark to evaluate the performance of different methods\nfor facial attractiveness prediction, including the state-of-the-art deep\nlearning method. The SCUT-FBP dataset contains face portraits of 500 Asian\nfemale subjects with attractiveness ratings, all of which have been verified in\nterms of rating distribution, standard deviation, consistency, and\nself-consistency. Benchmark evaluations for facial attractiveness prediction\nwere performed with different combinations of facial geometrical features and\ntexture features using classical statistical learning methods and the deep\nlearning method. The best Pearson correlation (0.8187) was achieved by the CNN\nmodel. Thus, the results of our experiments indicate that the SCUT-FBP dataset\nprovides a reliable benchmark for facial beauty perception.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 09:21:32 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Xie", "Duorui", ""], ["Liang", "Lingyu", ""], ["Jin", "Lianwen", ""], ["Xu", "Jie", ""], ["Li", "Mengru", ""]]}, {"id": "1511.02462", "submitter": "Steven C.H. Hoi", "authors": "Steven C.H. Hoi, Xiongwei Wu, Hantang Liu, Yue Wu, Huiqiong Wang, Hui\n  Xue, Qiang Wu", "title": "LOGO-Net: Large-scale Deep Logo Detection and Brand Recognition with\n  Deep Region-based Convolutional Networks", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logo detection from images has many applications, particularly for brand\nrecognition and intellectual property protection. Most existing studies for\nlogo recognition and detection are based on small-scale datasets which are not\ncomprehensive enough when exploring emerging deep learning techniques. In this\npaper, we introduce \"LOGO-Net\", a large-scale logo image database for logo\ndetection and brand recognition from real-world product images. To facilitate\nresearch, LOGO-Net has two datasets: (i)\"logos-18\" consists of 18 logo classes,\n10 brands, and 16,043 logo objects, and (ii) \"logos-160\" consists of 160 logo\nclasses, 100 brands, and 130,608 logo objects. We describe the ideas and\nchallenges for constructing such a large-scale database. Another key\ncontribution of this work is to apply emerging deep learning techniques for\nlogo detection and brand recognition tasks, and conduct extensive experiments\nby exploring several state-of-the-art deep region-based convolutional networks\ntechniques for object detection tasks. The LOGO-net will be released at\nhttp://logo-net.org/\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 09:44:45 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2015 12:57:05 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Hoi", "Steven C. H.", ""], ["Wu", "Xiongwei", ""], ["Liu", "Hantang", ""], ["Wu", "Yue", ""], ["Wang", "Huiqiong", ""], ["Xue", "Hui", ""], ["Wu", "Qiang", ""]]}, {"id": "1511.02465", "submitter": "Lianwen Jin", "authors": "Jie Xu, Lianwen Jin, Lingyu Liang, Ziyong Feng, Duorui Xie", "title": "A new humanlike facial attractiveness predictor with cascaded\n  fine-tuning deep learning model", "comments": "5 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a deep leaning method to address the challenging facial\nattractiveness prediction problem. The method constructs a convolutional neural\nnetwork of facial beauty prediction using a new deep cascaded fine-turning\nscheme with various face inputting channels, such as the original RGB face\nimage, the detail layer image, and the lighting layer image. With a carefully\ndesigned CNN model of deep structure, large input size and small convolutional\nkernels, we have achieved a high prediction correlation of 0.88. This result\nconvinces us that the problem of facial attractiveness prediction can be solved\nby deep learning approach, and it also shows the important roles of the facial\nsmoothness, lightness, and color information that were involved in facial\nbeauty perception, which is consistent with the result of recent psychology\nstudies. Furthermore, we analyze the high-level features learnt by CNN through\nvisualization of its hidden layers, and some interesting phenomena were\nobserved. It is found that the contours and appearance of facial features,\nespecially eyes and moth, are the most significant facial attributes for facial\nattractiveness prediction, which is also consistent with the visual perception\nintuition of human.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 09:59:04 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Xu", "Jie", ""], ["Jin", "Lianwen", ""], ["Liang", "Lingyu", ""], ["Feng", "Ziyong", ""], ["Xie", "Duorui", ""]]}, {"id": "1511.02492", "submitter": "Amirhossein Habibian", "authors": "Amirhossein Habibian, Thomas Mensink, Cees G.M. Snoek", "title": "VideoStory Embeddings Recognize Events when Examples are Scarce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims for event recognition when video examples are scarce or even\ncompletely absent. The key in such a challenging setting is a semantic video\nrepresentation. Rather than building the representation from individual\nattribute detectors and their annotations, we propose to learn the entire\nrepresentation from freely available web videos and their descriptions using an\nembedding between video features and term vectors. In our proposed embedding,\nwhich we call VideoStory, the correlations between the terms are utilized to\nlearn a more effective representation by optimizing a joint objective balancing\ndescriptiveness and predictability.We show how learning the VideoStory using a\nmultimodal predictability loss, including appearance, motion and audio\nfeatures, results in a better predictable representation. We also propose a\nvariant of VideoStory to recognize an event in video from just the important\nterms in a text query by introducing a term sensitive descriptiveness loss. Our\nexperiments on three challenging collections of web videos from the NIST\nTRECVID Multimedia Event Detection and Columbia Consumer Videos datasets\ndemonstrate: i) the advantages of VideoStory over representations using\nattributes or alternative embeddings, ii) the benefit of fusing video\nmodalities by an embedding over common strategies, iii) the complementarity of\nterm sensitive descriptiveness and multimodal predictability for event\nrecognition without examples. By it abilities to improve predictability upon\nany underlying video feature while at the same time maximizing semantic\ndescriptiveness, VideoStory leads to state-of-the-art accuracy for both few-\nand zero-example recognition of events in video.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 14:59:14 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Habibian", "Amirhossein", ""], ["Mensink", "Thomas", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1511.02500", "submitter": "Arie Rond", "authors": "Arie Rond, Raja Giryes, Michael Elad", "title": "Poisson Inverse Problems by the Plug-and-Play scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Anscombe transform offers an approximate conversion of a Poisson random\nvariable into a Gaussian one. This transform is important and appealing, as it\nis easy to compute, and becomes handy in various inverse problems with Poisson\nnoise contamination. Solution to such problems can be done by first applying\nthe Anscombe transform, then applying a Gaussian-noise-oriented restoration\nalgorithm of choice, and finally applying an inverse Anscombe transform. The\nappeal in this approach is due to the abundance of high-performance restoration\nalgorithms designed for white additive Gaussian noise (we will refer to these\nhereafter as \"Gaussian-solvers\"). This process is known to work well for high\nSNR images, where the Anscombe transform provides a rather accurate\napproximation. When the noise level is high, the above path loses much of its\neffectiveness, and the common practice is to replace it with a direct treatment\nof the Poisson distribution. Naturally, with this we lose the ability to\nleverage on vastly available Gaussian-solvers.\n  In this work we suggest a novel method for coupling Gaussian denoising\nalgorithms to Poisson noisy inverse problems, which is based on a general\napproach termed \"Plug-and-Play\". Deploying the Plug-and-Play approach to such\nproblems leads to an iterative scheme that repeats several key steps: 1) A\nconvex programming task of simple form that can be easily treated; 2) A\npowerful Gaussian denoising algorithm of choice; and 3) A simple update step.\n  Such a modular method, just like the Anscombe transform, enables other\ndevelopers to plug their own Gaussian denoising algorithms to our scheme in an\neasy way. While the proposed method bares some similarity to the Anscombe\noperation, it is in fact based on a different mathematical basis, which holds\ntrue for all SNR ranges.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 16:22:49 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Rond", "Arie", ""], ["Giryes", "Raja", ""], ["Elad", "Michael", ""]]}, {"id": "1511.02503", "submitter": "Wei Li", "authors": "Wei Li, Mingquan Qiu, Zhencai Zhu, Bo Wu, Gongbo Zhou", "title": "Bearing fault diagnosis based on spectrum images of vibration signals", "comments": null, "journal-ref": "Measurement Science and Technology, Volume 27, Number 3, 2016", "doi": "10.1088/0957-0233/27/3/035005", "report-no": null, "categories": "cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bearing fault diagnosis has been a challenge in the monitoring activities of\nrotating machinery, and it's receiving more and more attention. The\nconventional fault diagnosis methods usually extract features from the\nwaveforms or spectrums of vibration signals in order to realize fault\nclassification. In this paper, a novel feature in the form of images is\npresented, namely the spectrum images of vibration signals. The spectrum images\nare simply obtained by doing fast Fourier transformation. Such images are\nprocessed with two-dimensional principal component analysis (2DPCA) to reduce\nthe dimensions, and then a minimum distance method is applied to classify the\nfaults of bearings. The effectiveness of the proposed method is verified with\nexperimental data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 16:51:22 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 15:00:22 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2016 14:30:19 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2016 07:12:04 GMT"}, {"version": "v5", "created": "Thu, 4 Feb 2016 01:52:35 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Li", "Wei", ""], ["Qiu", "Mingquan", ""], ["Zhu", "Zhencai", ""], ["Wu", "Bo", ""], ["Zhou", "Gongbo", ""]]}, {"id": "1511.02570", "submitter": "Chunhua Shen", "authors": "Peng Wang, Qi Wu, Chunhua Shen, Anton van den Hengel, Anthony Dick", "title": "Explicit Knowledge-based Reasoning for Visual Question Answering", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for visual question answering which is capable of\nreasoning about contents of an image on the basis of information extracted from\na large-scale knowledge base. The method not only answers natural language\nquestions using concepts not contained in the image, but can provide an\nexplanation of the reasoning by which it developed its answer. The method is\ncapable of answering far more complex questions than the predominant long\nshort-term memory-based approach, and outperforms it significantly in the\ntesting. We also provide a dataset and a protocol by which to evaluate such\nmethods, thus addressing one of the key issues in general visual ques- tion\nanswering.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 05:25:57 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2015 01:10:38 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Wang", "Peng", ""], ["Wu", "Qi", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Dick", "Anthony", ""]]}, {"id": "1511.02575", "submitter": "Shiry Ginosar", "authors": "Shiry Ginosar, Kate Rakelly, Sarah Sachs, Brian Yin, Crystal Lee,\n  Philipp Krahenbuhl, Alexei A. Efros", "title": "A Century of Portraits: A Visual Historical Record of American High\n  School Yearbooks", "comments": "IEEE Transactions on Computational Imaging, September 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagery offers a rich description of our world and communicates a volume and\ntype of information that cannot be captured by text alone. Since the invention\nof the camera, an ever-increasing number of photographs document our \"visual\nculture\" complementing historical texts. But currently, this treasure trove of\nknowledge can only be analyzed manually by historians, and only at small scale.\nIn this paper we perform automated analysis on a large-scale historical image\ndataset. Our main contributions are: 1) A publicly-available dataset of 168,055\n(37,921 frontal-facing) American high school yearbook portraits. 2)\nWeakly-supervised data-driven techniques to discover historical visual trends\nin fashion and identify date-specific visual patterns. 3) A classifier to\npredict when a portrait was taken, with median error of 4 years for women and 6\nfor men. 4) A new method for discovering and displaying the visual elements\nused by the CNN-based date-prediction model to date portraits, finding that\nthey correspond to the tell-tale fashions of each era. Project page can be\nfound at:\nhttp://people.eecs.berkeley.edu/~shiry/projects/yearbooks/yearbooks.html .\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 05:44:39 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 22:50:13 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Ginosar", "Shiry", ""], ["Rakelly", "Kate", ""], ["Sachs", "Sarah", ""], ["Yin", "Brian", ""], ["Lee", "Crystal", ""], ["Krahenbuhl", "Philipp", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1511.02583", "submitter": "Yong-Sheng Chen", "authors": "Jia-Ren Chang and Yong-Sheng Chen", "title": "Batch-normalized Maxout Network in Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports a novel deep architecture referred to as Maxout network In\nNetwork (MIN), which can enhance model discriminability and facilitate the\nprocess of information abstraction within the receptive field. The proposed\nnetwork adopts the framework of the recently developed Network In Network\nstructure, which slides a universal approximator, multilayer perceptron (MLP)\nwith rectifier units, to exact features. Instead of MLP, we employ maxout MLP\nto learn a variety of piecewise linear activation functions and to mediate the\nproblem of vanishing gradients that can occur when using rectifier units.\nMoreover, batch normalization is applied to reduce the saturation of maxout\nunits by pre-conditioning the model and dropout is applied to prevent\noverfitting. Finally, average pooling is used in all pooling layers to\nregularize maxout MLP in order to facilitate information abstraction in every\nreceptive field while tolerating the change of object position. Because average\npooling preserves all features in the local patch, the proposed MIN model can\nenforce the suppression of irrelevant information during training. Our\nexperiments demonstrated the state-of-the-art classification performance when\nthe MIN model was applied to MNIST, CIFAR-10, and CIFAR-100 datasets and\ncomparable performance for SVHN dataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 07:09:57 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Chang", "Jia-Ren", ""], ["Chen", "Yong-Sheng", ""]]}, {"id": "1511.02589", "submitter": "Ioannis Pachoulakis", "authors": "Ioannis Pachoulakis, Nikolaos Papadopoulos and Cleanthe Spanaki", "title": "Parkinson's disease patient rehabilitation using gaming platforms:\n  lessons learnt", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": "10.5121/ijbes.2015.2401", "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parkinson's disease (PD) is a progressive neurodegenerative movement disorder\nwhere motor dysfunction gradually increases as the disease progress. In\naddition to administering dopaminergic PD-specific drugs, attending\nneurologists strongly recommend regular exercise combined with physiotherapy.\nHowever, because of the long-term nature of the disease, patients following\ntraditional rehabilitation programs may get bored, lose interest and eventually\ndrop out as a direct result of the repeatability and predictability of the\nprescribed exercises. Technology supported opportunities to liven up a daily\nexercise schedule have appeared in the form of character-based, virtual reality\ngames which promote physical training in a non-linear and looser fashion and\nprovide an experience that varies from one game loop the next. Such\n\"exergames\", a word that results from the amalgamation of the words \"exercise\"\nand \"game\" challenge patients into performing movements of varying complexity\nin a playful and immersive virtual environment. Today's game consoles such as\nNintendo's Wii, Sony PlayStation Eye and Microsoft's Kinect sensor present new\nopportunities to infuse motivation and variety to an otherwise mundane\nphysiotherapy routine. In this paper we present some of these approaches,\ndiscuss their suitability for these PD patients, mainly on the basis of demands\nmade on balance, agility and gesture precision, and present design principles\nthat exergame platforms must comply with in order to be suitable for PD\npatients.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 07:36:22 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Pachoulakis", "Ioannis", ""], ["Papadopoulos", "Nikolaos", ""], ["Spanaki", "Cleanthe", ""]]}, {"id": "1511.02667", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen, Francesco Tudisco, Antoine Gautier, Matthias Hein", "title": "An Efficient Multilinear Optimization Framework for Hypergraph Matching", "comments": "accepted at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (PAMI) 2016", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2574706", "report-no": null, "categories": "cs.CV cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraph matching has recently become a popular approach for solving\ncorrespondence problems in computer vision as it allows to integrate\nhigher-order geometric information. Hypergraph matching can be formulated as a\nthird-order optimization problem subject to the assignment constraints which\nturns out to be NP-hard. In recent work, we have proposed an algorithm for\nhypergraph matching which first lifts the third-order problem to a fourth-order\nproblem and then solves the fourth-order problem via optimization of the\ncorresponding multilinear form. This leads to a tensor block coordinate ascent\nscheme which has the guarantee of providing monotonic ascent in the original\nmatching score function and leads to state-of-the-art performance both in terms\nof achieved matching score and accuracy. In this paper we show that the lifting\nstep to a fourth-order problem can be avoided yielding a third-order scheme\nwith the same guarantees and performance but being two times faster. Moreover,\nwe introduce a homotopy type method which further improves the performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 13:15:10 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 19:06:19 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Nguyen", "Quynh", ""], ["Tudisco", "Francesco", ""], ["Gautier", "Antoine", ""], ["Hein", "Matthias", ""]]}, {"id": "1511.02674", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Jianbo Shi, Lorenzo Torresani", "title": "Semantic Segmentation with Boundary Neural Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art in semantic segmentation is currently represented by\nfully convolutional networks (FCNs). However, FCNs use large receptive fields\nand many pooling layers, both of which cause blurring and low spatial\nresolution in the deep layers. As a result FCNs tend to produce segmentations\nthat are poorly localized around object boundaries. Prior work has attempted to\naddress this issue in post-processing steps, for example using a color-based\nCRF on top of the FCN predictions. However, these approaches require additional\nparameters and low-level features that are difficult to tune and integrate into\nthe original network architecture. Additionally, most CRFs use color-based\npixel affinities, which are not well suited for semantic segmentation and lead\nto spatially disjoint predictions.\n  To overcome these problems, we introduce a Boundary Neural Field (BNF), which\nis a global energy model integrating FCN predictions with boundary cues. The\nboundary information is used to enhance semantic segment coherence and to\nimprove object localization. Specifically, we first show that the convolutional\nfilters of semantic FCNs provide good features for boundary detection. We then\nemploy the predicted boundaries to define pairwise potentials in our energy.\nFinally, we show that our energy decomposes semantic segmentation into multiple\nbinary problems, which can be relaxed for efficient global optimization. We\nreport extensive experiments demonstrating that minimization of our global\nboundary-based energy yields results superior to prior globalization methods,\nboth quantitatively as well as qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 13:27:30 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 23:32:10 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Bertasius", "Gedas", ""], ["Shi", "Jianbo", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1511.02680", "submitter": "Alex Kendall", "authors": "Alex Kendall and Vijay Badrinarayanan and Roberto Cipolla", "title": "Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder\n  Architectures for Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning framework for probabilistic pixel-wise semantic\nsegmentation, which we term Bayesian SegNet. Semantic segmentation is an\nimportant tool for visual scene understanding and a meaningful measure of\nuncertainty is essential for decision making. Our contribution is a practical\nsystem which is able to predict pixel-wise class labels with a measure of model\nuncertainty. We achieve this by Monte Carlo sampling with dropout at test time\nto generate a posterior distribution of pixel class labels. In addition, we\nshow that modelling uncertainty improves segmentation performance by 2-3%\nacross a number of state of the art architectures such as SegNet, FCN and\nDilation Network, with no additional parametrisation. We also observe a\nsignificant improvement in performance for smaller datasets where modelling\nuncertainty is more effective. We benchmark Bayesian SegNet on the indoor SUN\nScene Understanding and outdoor CamVid driving scenes datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 14:00:21 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 22:04:21 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Kendall", "Alex", ""], ["Badrinarayanan", "Vijay", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1511.02682", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Hyun Soo Park, Jianbo Shi", "title": "Exploiting Egocentric Object Prior for 3D Saliency Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On a minute-to-minute basis people undergo numerous fluid interactions with\nobjects that barely register on a conscious level. Recent neuroscientific\nresearch demonstrates that humans have a fixed size prior for salient objects.\nThis suggests that a salient object in 3D undergoes a consistent transformation\nsuch that people's visual system perceives it with an approximately fixed size.\nThis finding indicates that there exists a consistent egocentric object prior\nthat can be characterized by shape, size, depth, and location in the first\nperson view.\n  In this paper, we develop an EgoObject Representation, which encodes these\ncharacteristics by incorporating shape, location, size and depth features from\nan egocentric RGBD image. We empirically show that this representation can\naccurately characterize the egocentric object prior by testing it on an\negocentric RGBD dataset for three tasks: the 3D saliency detection, future\nsaliency prediction, and interaction classification. This representation is\nevaluated on our new Egocentric RGBD Saliency dataset that includes various\nactivities such as cooking, dining, and shopping. By using our EgoObject\nrepresentation, we outperform previously proposed models for saliency detection\n(relative 30% improvement for 3D saliency detection task) on our dataset.\nAdditionally, we demonstrate that this representation allows us to predict\nfuture salient objects based on the gaze cue and classify people's interactions\nwith objects.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 14:01:50 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Bertasius", "Gedas", ""], ["Park", "Hyun Soo", ""], ["Shi", "Jianbo", ""]]}, {"id": "1511.02683", "submitter": "Xiang Wu", "authors": "Xiang Wu, Ran He, Zhenan Sun, Tieniu Tan", "title": "A Light CNN for Deep Face Representation with Noisy Labels", "comments": "arXiv admin note: text overlap with arXiv:1507.04844. The models are\n  released on https://github.com/AlfredXiangWu/LightCNN, IEEE Transactions on\n  Information Forensics and Security, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volume of convolutional neural network (CNN) models proposed for face\nrecognition has been continuously growing larger to better fit large amount of\ntraining data. When training data are obtained from internet, the labels are\nlikely to be ambiguous and inaccurate. This paper presents a Light CNN\nframework to learn a compact embedding on the large-scale face data with\nmassive noisy labels. First, we introduce a variation of maxout activation,\ncalled Max-Feature-Map (MFM), into each convolutional layer of CNN. Different\nfrom maxout activation that uses many feature maps to linearly approximate an\narbitrary convex activation function, MFM does so via a competitive\nrelationship. MFM can not only separate noisy and informative signals but also\nplay the role of feature selection between two feature maps. Second, three\nnetworks are carefully designed to obtain better performance meanwhile reducing\nthe number of parameters and computational costs. Lastly, a semantic\nbootstrapping method is proposed to make the prediction of the networks more\nconsistent with noisy labels. Experimental results show that the proposed\nframework can utilize large-scale noisy data to learn a Light model that is\nefficient in computational costs and storage spaces. The learned single network\nwith a 256-D representation achieves state-of-the-art results on various face\nbenchmarks without fine-tuning. The code is released on\nhttps://github.com/AlfredXiangWu/LightCNN.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 14:02:03 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 03:51:25 GMT"}, {"version": "v3", "created": "Mon, 24 Apr 2017 01:54:33 GMT"}, {"version": "v4", "created": "Sun, 12 Aug 2018 02:35:58 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Wu", "Xiang", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""], ["Tan", "Tieniu", ""]]}, {"id": "1511.02705", "submitter": "Gabriel Peyre", "authors": "Jonathan Vacher (CEREMADE), Andrew Meso (INT), Laurent U Perrinet\n  (INT), Gabriel Peyr\\'e (CEREMADE)", "title": "Biologically Inspired Dynamic Textures for Probing Motion Perception", "comments": "Twenty-ninth Annual Conference on Neural Information Processing\n  Systems (NIPS), Dec 2015, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perception is often described as a predictive process based on an optimal\ninference with respect to a generative model. We study here the principled\nconstruction of a generative model specifically crafted to probe motion\nperception. In that context, we first provide an axiomatic, biologically-driven\nderivation of the model. This model synthesizes random dynamic textures which\nare defined by stationary Gaussian distributions obtained by the random\naggregation of warped patterns. Importantly, we show that this model can\nequivalently be described as a stochastic partial differential equation. Using\nthis characterization of motion in images, it allows us to recast motion-energy\nmodels into a principled Bayesian inference framework. Finally, we apply these\ntextures in order to psychophysically probe speed perception in humans. In this\nframework, while the likelihood is derived from the generative model, the prior\nis estimated from the observed results and accounts for the perceptual bias in\na principled fashion.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 14:50:25 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Vacher", "Jonathan", "", "CEREMADE"], ["Meso", "Andrew", "", "INT"], ["Perrinet", "Laurent U", "", "INT"], ["Peyr\u00e9", "Gabriel", "", "CEREMADE"]]}, {"id": "1511.02793", "submitter": "Elman Mansimov", "authors": "Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, Ruslan Salakhutdinov", "title": "Generating Images from Captions with Attention", "comments": "Published as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the recent progress in generative models, we introduce a model\nthat generates images from natural language descriptions. The proposed model\niteratively draws patches on a canvas, while attending to the relevant words in\nthe description. After training on Microsoft COCO, we compare our model with\nseveral baseline generative models on image generation and retrieval tasks. We\ndemonstrate that our model produces higher quality samples than other\napproaches and generates images with novel scene compositions corresponding to\npreviously unseen captions in the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 18:18:53 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 17:56:29 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Mansimov", "Elman", ""], ["Parisotto", "Emilio", ""], ["Ba", "Jimmy Lei", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1511.02799", "submitter": "Jacob Andreas", "authors": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein", "title": "Neural Module Networks", "comments": "Corrects an error in the evaluation of the NMN-only ablation\n  experiment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering is fundamentally compositional in nature---a\nquestion like \"where is the dog?\" shares substructure with questions like \"what\ncolor is the dog?\" and \"where is the cat?\" This paper seeks to simultaneously\nexploit the representational capacity of deep networks and the compositional\nlinguistic structure of questions. We describe a procedure for constructing and\nlearning *neural module networks*, which compose collections of jointly-trained\nneural \"modules\" into deep networks for question answering. Our approach\ndecomposes questions into their linguistic substructures, and uses these\nstructures to dynamically instantiate modular networks (with reusable\ncomponents for recognizing dogs, classifying colors, etc.). The resulting\ncompound networks are jointly trained. We evaluate our approach on two\nchallenging datasets for visual question answering, achieving state-of-the-art\nresults on both the VQA natural image dataset and a new dataset of complex\nquestions about abstract shapes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 18:48:39 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 06:36:22 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2016 18:26:40 GMT"}, {"version": "v4", "created": "Mon, 24 Jul 2017 17:15:06 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Andreas", "Jacob", ""], ["Rohrbach", "Marcus", ""], ["Darrell", "Trevor", ""], ["Klein", "Dan", ""]]}, {"id": "1511.02821", "submitter": "Chao Chen", "authors": "Chao Chen, Alina Zare, and J. Tory Cobb", "title": "Partial Membership Latent Dirichlet Allocation", "comments": "cut to 6 pages, add sunset results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models (e.g., pLSA, LDA, SLDA) have been widely used for segmenting\nimagery. These models are confined to crisp segmentation. Yet, there are many\nimages in which some regions cannot be assigned a crisp label (e.g., transition\nregions between a foggy sky and the ground or between sand and water at a\nbeach). In these cases, a visual word is best represented with partial\nmemberships across multiple topics. To address this, we present a partial\nmembership latent Dirichlet allocation (PM-LDA) model and associated parameter\nestimation algorithms. Experimental results on two natural image datasets and\none SONAR image dataset show that PM-LDA can produce both crisp and soft\nsemantic image segmentations; a capability existing methods do not have.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 20:04:56 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 03:59:15 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Chen", "Chao", ""], ["Zare", "Alina", ""], ["Cobb", "J. Tory", ""]]}, {"id": "1511.02825", "submitter": "Changzhe Jiao", "authors": "Changzhe Jiao, Alina Zare", "title": "Multiple Instance Dictionary Learning using Functions of Multiple\n  Instances", "comments": "Final submission to ICPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multiple instance dictionary learning method using functions of multiple\ninstances (DL-FUMI) is proposed to address target detection and two-class\nclassification problems with inaccurate training labels. Given inaccurate\ntraining labels, DL-FUMI learns a set of target dictionary atoms that describe\nthe most distinctive and representative features of the true positive class as\nwell as a set of nontarget dictionary atoms that account for the shared\ninformation found in both the positive and negative instances. Experimental\nresults show that the estimated target dictionary atoms found by DL-FUMI are\nmore representative prototypes and identify better discriminative features of\nthe true positive class than existing methods in the literature. DL-FUMI is\nshown to have significantly better performance on several target detection and\nclassification problems as compared to other multiple instance learning (MIL)\ndictionary learning algorithms on a variety of MIL problems.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 20:12:19 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 21:31:50 GMT"}, {"version": "v3", "created": "Wed, 3 Aug 2016 21:03:51 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Jiao", "Changzhe", ""], ["Zare", "Alina", ""]]}, {"id": "1511.02841", "submitter": "Galin Georgiev", "authors": "Galin Georgiev", "title": "Symmetries and control in generative neural nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study generative nets which can control and modify observations, after\nbeing trained on real-life datasets. In order to zoom-in on an object, some\nspatial, color and other attributes are learned by classifiers in specialized\nattention nets. In field-theoretical terms, these learned symmetry statistics\nform the gauge group of the data set. Plugging them in the generative layers of\nauto-classifiers-encoders (ACE) appears to be the most direct way to\nsimultaneously: i) generate new observations with arbitrary attributes, from a\ngiven class, ii) describe the low-dimensional manifold encoding the \"essence\"\nof the data, after superfluous attributes are factored out, and iii)\norganically control, i.e., move or modify objects within given observations. We\ndemonstrate the sharp improvement of the generative qualities of shallow ACE,\nwith added spatial and color symmetry statistics, on the distorted MNIST and\nCIFAR10 datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 20:49:03 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 17:49:51 GMT"}, {"version": "v3", "created": "Fri, 8 Apr 2016 21:38:31 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Georgiev", "Galin", ""]]}, {"id": "1511.02853", "submitter": "Hakan Bilen", "authors": "Hakan Bilen and Andrea Vedaldi", "title": "Weakly Supervised Deep Detection Networks", "comments": "9 pages, 3 figures, 6 tables, Conference on Computer Vision and\n  Pattern Recognition (CVPR) 2016. This version fixes the equation in Section\n  3.4, thanks to Vadim Kantorov and Sini\\v{s}a \\v{S}egvi\\'c", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised learning of object detection is an important problem in\nimage understanding that still does not have a satisfactory solution. In this\npaper, we address this problem by exploiting the power of deep convolutional\nneural networks pre-trained on large-scale image-level classification tasks. We\npropose a weakly supervised deep detection architecture that modifies one such\nnetwork to operate at the level of image regions, performing simultaneously\nregion selection and classification. Trained as an image classifier, the\narchitecture implicitly learns object detectors that are better than\nalternative weakly supervised detection systems on the PASCAL VOC data. The\nmodel, which is a simple and elegant end-to-end architecture, outperforms\nstandard data augmentation and fine-tuning techniques for the task of\nimage-level classification as well.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 20:58:30 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 15:04:03 GMT"}, {"version": "v3", "created": "Fri, 8 Apr 2016 07:38:18 GMT"}, {"version": "v4", "created": "Mon, 19 Dec 2016 09:44:29 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Bilen", "Hakan", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1511.02872", "submitter": "Hiroharu Kato", "authors": "Hiroharu Kato and Tatsuya Harada", "title": "Visual Language Modeling on CNN Image Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the naturalness of images is important to generate realistic images\nor to detect unnatural regions in images. Additionally, a method to measure\nnaturalness can be complementary to Convolutional Neural Network (CNN) based\nfeatures, which are known to be insensitive to the naturalness of images.\nHowever, most probabilistic image models have insufficient capability of\nmodeling the complex and abstract naturalness that we feel because they are\nbuilt directly on raw image pixels. In this work, we assume that naturalness\ncan be measured by the predictability on high-level features during eye\nmovement. Based on this assumption, we propose a novel method to evaluate the\nnaturalness by building a variant of Recurrent Neural Network Language Models\non pre-trained CNN representations. Our method is applied to two tasks,\ndemonstrating that 1) using our method as a regularizer enables us to generate\nmore understandable images from image features than existing approaches, and 2)\nunnaturalness maps produced by our method achieve state-of-the-art eye fixation\nprediction performance on two well-studied datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 21:00:08 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Kato", "Hiroharu", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1511.02911", "submitter": "Tal Remez", "authors": "Tal Remez and Shai Avidan", "title": "Spatially Coherent Random Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially Coherent Random Forest (SCRF) extends Random Forest to create\nspatially coherent labeling. Each split function in SCRF is evaluated based on\na traditional information gain measure that is regularized by a spatial\ncoherency term. This way, SCRF is encouraged to choose split functions that\ncluster pixels both in appearance space and in image space. In particular, we\nuse SCRF to detect contours in images, where contours are taken to be the\nboundaries between different regions. Each tree in the forest produces a\nsegmentation of the image plane and the boundaries of the segmentations of all\ntrees are aggregated to produce a final hierarchical contour map. We show that\nthis modification improves the performance of regular Random Forest by about\n10% on the standard Berkeley Segmentation Datasets. We believe that SCRF can be\nused in other settings as well.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 22:14:00 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2015 10:13:06 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Remez", "Tal", ""], ["Avidan", "Shai", ""]]}, {"id": "1511.02916", "submitter": "Zhouhan Lin", "authors": "Zhouhan Lin, Yushi Chen, Xing Zhao, Gang Wang", "title": "Spectral-Spatial Classification of Hyperspectral Image Using\n  Autoencoders", "comments": "Accepted as a conference paper at ICICS 2013, an updated version.\n  Codes published. 9 pages, 6 figures", "journal-ref": null, "doi": "10.1109/ICICS.2013.6782778", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) classification is a hot topic in the remote sensing\ncommunity. This paper proposes a new framework of spectral-spatial feature\nextraction for HSI classification, in which for the first time the concept of\ndeep learning is introduced. Specifically, the model of autoencoder is\nexploited in our framework to extract various kinds of features. First we\nverify the eligibility of autoencoder by following classical spectral\ninformation based classification and use autoencoders with different depth to\nclassify hyperspectral image. Further in the proposed framework, we combine PCA\non spectral dimension and autoencoder on the other two spatial dimensions to\nextract spectral-spatial information for classification. The experimental\nresults show that this framework achieves the highest classification accuracy\namong all methods, and outperforms classical classifiers such as SVM and\nPCA-based SVM.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 22:29:13 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Lin", "Zhouhan", ""], ["Chen", "Yushi", ""], ["Zhao", "Xing", ""], ["Wang", "Gang", ""]]}, {"id": "1511.02917", "submitter": "Vignesh Ramanathan", "authors": "Vignesh Ramanathan and Jonathan Huang and Sami Abu-El-Haija and\n  Alexander Gorban and Kevin Murphy and Li Fei-Fei", "title": "Detecting events and key actors in multi-person videos", "comments": "Accepted for publication in CVPR'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person event recognition is a challenging task, often with many people\nactive in the scene but only a small subset contributing to an actual event. In\nthis paper, we propose a model which learns to detect events in such videos\nwhile automatically \"attending\" to the people responsible for the event. Our\nmodel does not use explicit annotations regarding who or where those people are\nduring training and testing. In particular, we track people in videos and use a\nrecurrent neural network (RNN) to represent the track features. We learn\ntime-varying attention weights to combine these features at each time-instant.\nThe attended features are then processed using another RNN for event\ndetection/classification. Since most video datasets with multiple people are\nrestricted to a small number of videos, we also collected a new basketball\ndataset comprising 257 basketball games with 14K event annotations\ncorresponding to 11 event classes. Our model outperforms state-of-the-art\nmethods for both event classification and detection on this new dataset.\nAdditionally, we show that the attention mechanism is able to consistently\nlocalize the relevant players.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 22:30:19 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 00:02:03 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Ramanathan", "Vignesh", ""], ["Huang", "Jonathan", ""], ["Abu-El-Haija", "Sami", ""], ["Gorban", "Alexander", ""], ["Murphy", "Kevin", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1511.02919", "submitter": "Deepti Ghadiyaram", "authors": "Deepti Ghadiyaram and Alan C. Bovik", "title": "Massive Online Crowdsourced Study of Subjective and Objective Picture\n  Quality", "comments": "16 pages", "journal-ref": null, "doi": "10.1109/TIP.2015.2500021", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most publicly available image quality databases have been created under\nhighly controlled conditions by introducing graded simulated distortions onto\nhigh-quality photographs. However, images captured using typical real-world\nmobile camera devices are usually afflicted by complex mixtures of multiple\ndistortions, which are not necessarily well-modeled by the synthetic\ndistortions found in existing databases. The originators of existing legacy\ndatabases usually conducted human psychometric studies to obtain statistically\nmeaningful sets of human opinion scores on images in a stringently controlled\nvisual environment, resulting in small data collections relative to other kinds\nof image analysis databases. Towards overcoming these limitations, we designed\nand created a new database that we call the LIVE In the Wild Image Quality\nChallenge Database, which contains widely diverse authentic image distortions\non a large number of images captured using a representative variety of modern\nmobile devices. We also designed and implemented a new online crowdsourcing\nsystem, which we have used to conduct a very large-scale, multi-month image\nquality assessment subjective study. Our database consists of over 350000\nopinion scores on 1162 images evaluated by over 7000 unique human observers.\nDespite the lack of control over the experimental environments of the numerous\nstudy participants, we demonstrate excellent internal consistency of the\nsubjective dataset. We also evaluate several top-performing blind Image Quality\nAssessment algorithms on it and present insights on how mixtures of distortions\nchallenge both end users as well as automatic perceptual quality prediction\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 22:39:58 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Ghadiyaram", "Deepti", ""], ["Bovik", "Alan C.", ""]]}, {"id": "1511.02928", "submitter": "Reza Arablouei", "authors": "Reza Arablouei and Frank de Hoog", "title": "Hyperspectral Image Recovery via Hybrid Regularization", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2614131", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural images tend to mostly consist of smooth regions with individual\npixels having highly correlated spectra. This information can be exploited to\nrecover hyperspectral images of natural scenes from their incomplete and noisy\nmeasurements. To perform the recovery while taking full advantage of the prior\nknowledge, we formulate a composite cost function containing a square-error\ndata-fitting term and two distinct regularization terms pertaining to spatial\nand spectral domains. The regularization for the spatial domain is the sum of\ntotal-variation of the image frames corresponding to all spectral bands. The\nregularization for the spectral domain is the l1-norm of the coefficient matrix\nobtained by applying a suitable sparsifying transform to the spectra of the\npixels. We use an accelerated proximal-subgradient method to minimize the\nformulated cost function. We analyze the performance of the proposed algorithm\nand prove its convergence. Numerical simulations using real hyperspectral\nimages exhibit that the proposed algorithm offers an excellent recovery\nperformance with a number of measurements that is only a small fraction of the\nhyperspectral image data size. Simulation results also show that the proposed\nalgorithm significantly outperforms an accelerated proximal-gradient algorithm\nthat solves the classical basis-pursuit denoising problem to recover the\nhyperspectral image.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 23:31:31 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 02:27:31 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Arablouei", "Reza", ""], ["de Hoog", "Frank", ""]]}, {"id": "1511.02986", "submitter": "Li-Hao Yeh", "authors": "Li-Hao Yeh, Jonathan Dong, Jingshan Zhong, Lei Tian, Michael Chen,\n  Gongguo Tang, Mahdi Soltanolkotabi, and Laura Waller", "title": "Experimental robustness of Fourier Ptychography phase retrieval\n  algorithms", "comments": null, "journal-ref": "Opt. Express 23, 33214-33240 (2015)", "doi": "10.1364/OE.23.033214", "report-no": null, "categories": "physics.optics cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier ptychography is a new computational microscopy technique that\nprovides gigapixel-scale intensity and phase images with both wide\nfield-of-view and high resolution. By capturing a stack of low-resolution\nimages under different illumination angles, a nonlinear inverse algorithm can\nbe used to computationally reconstruct the high-resolution complex field. Here,\nwe compare and classify multiple proposed inverse algorithms in terms of\nexperimental robustness. We find that the main sources of error are noise,\naberrations and mis-calibration (i.e. model mis-match). Using simulations and\nexperiments, we demonstrate that the choice of cost function plays a critical\nrole, with amplitude-based cost functions performing better than\nintensity-based ones. The reason for this is that Fourier ptychography datasets\nconsist of images from both brightfield and darkfield illumination,\nrepresenting a large range of measured intensities. Both noise (e.g. Poisson\nnoise) and model mis-match errors are shown to scale with intensity. Hence,\nalgorithms that use an appropriate cost function will be more tolerant to both\nnoise and model mis-match. Given these insights, we propose a global Newton's\nmethod algorithm which is robust and computationally efficient. Finally, we\ndiscuss the impact of procedures for algorithmic correction of aberrations and\nmis-calibration.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 03:45:02 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2015 07:33:10 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Yeh", "Li-Hao", ""], ["Dong", "Jonathan", ""], ["Zhong", "Jingshan", ""], ["Tian", "Lei", ""], ["Chen", "Michael", ""], ["Tang", "Gongguo", ""], ["Soltanolkotabi", "Mahdi", ""], ["Waller", "Laura", ""]]}, {"id": "1511.02992", "submitter": "Mrinal Haloi", "authors": "Mrinal Haloi", "title": "Traffic Sign Classification Using Deep Inception Based Convolutional\n  Networks", "comments": "modifications: Accepted version of 2016 IEEE Intelligent Vehicles\n  Symposium (IV 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel deep network for traffic sign classification\nthat achieves outstanding performance on GTSRB surpassing all previous methods.\nOur deep network consists of spatial transformer layers and a modified version\nof inception module specifically designed for capturing local and global\nfeatures together. This features adoption allows our network to classify\nprecisely intraclass samples even under deformations. Use of spatial\ntransformer layer makes this network more robust to deformations such as\ntranslation, rotation, scaling of input images. Unlike existing approaches that\nare developed with hand-crafted features, multiple deep networks with huge\nparameters and data augmentations, our method addresses the concern of\nexploding parameters and augmentations. We have achieved the state-of-the-art\nperformance of 99.81\\% on GTSRB dataset.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 05:07:03 GMT"}, {"version": "v2", "created": "Sun, 17 Jul 2016 11:05:22 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Haloi", "Mrinal", ""]]}, {"id": "1511.02999", "submitter": "Abhishek Maity", "authors": "Abhishek Maity", "title": "Improvised Salient Object Detection and Manipulation", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In case of salient subject recognition, computer algorithms have been heavily\nrelied on scanning of images from top-left to bottom-right systematically and\napply brute-force when attempting to locate objects of interest. Thus, the\nprocess turns out to be quite time consuming. Here a novel approach and a\nsimple solution to the above problem is discussed. In this paper, we implement\nan approach to object manipulation and detection through segmentation map,\nwhich would help to desaturate or, in other words, wash out the background of\nthe image. Evaluation for the performance is carried out using the Jaccard\nindex against the well-known Ground-truth target box technique.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 06:08:42 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Maity", "Abhishek", ""]]}, {"id": "1511.03015", "submitter": "Huibin Li", "authors": "Huibin Li and Jian Sun and Dong Wang and Zongben Xu and Liming Chen", "title": "Deep Representation of Facial Geometric and Photometric Attributes for\n  Automatic 3D Facial Expression Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach to automatic 3D Facial Expression\nRecognition (FER) based on deep representation of facial 3D geometric and 2D\nphotometric attributes. A 3D face is firstly represented by its geometric and\nphotometric attributes, including the geometry map, normal maps, normalized\ncurvature map and texture map. These maps are then fed into a pre-trained deep\nconvolutional neural network to generate the deep representation. Then the\nfacial expression prediction is simplyachieved by training linear SVMs over the\ndeep representation for different maps and fusing these SVM scores. The\nvisualizations show that the deep representation provides a complete and highly\ndiscriminative coding scheme for 3D faces. Comprehensive experiments on the\nBU-3DFE database demonstrate that the proposed deep representation can\noutperform the widely used hand-crafted descriptors (i.e., LBP, SIFT, HOG,\nGabor) and the state-of-art approaches under the same experimental protocols.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 08:30:31 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Li", "Huibin", ""], ["Sun", "Jian", ""], ["Wang", "Dong", ""], ["Xu", "Zongben", ""], ["Chen", "Liming", ""]]}, {"id": "1511.03019", "submitter": "Ricardo Martin Brualla", "authors": "Ricardo Martin-Brualla, David Gallup, Steven M. Seitz", "title": "3D Time-lapse Reconstruction from Internet Photos", "comments": "To appear in ICCV'15. Supplementary video at:\n  http://grail.cs.washington.edu/projects/timelapse3d/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an Internet photo collection of a landmark, we compute a 3D time-lapse\nvideo sequence where a virtual camera moves continuously in time and space.\nWhile previous work assumed a static camera, the addition of camera motion\nduring the time-lapse creates a very compelling impression of parallax.\nAchieving this goal, however, requires addressing multiple technical\nchallenges, including solving for time-varying depth maps, regularizing 3D\npoint color profiles over time, and reconstructing high quality, hole-free\nimages at every frame from the projected profiles. Our results show\nphotorealistic time-lapses of skylines and natural scenes over many years, with\ndramatic parallax effects.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 08:42:40 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 03:21:40 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Martin-Brualla", "Ricardo", ""], ["Gallup", "David", ""], ["Seitz", "Steven M.", ""]]}, {"id": "1511.03028", "submitter": "Chang Tang", "authors": "Chang Tang, Pichao Wang, Wanqing Li", "title": "Online Action Recognition based on Incremental Learning of Weighted\n  Covariance Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from traditional action recognition based on video segments, online\naction recognition aims to recognize actions from unsegmented streams of data\nin a continuous manner. One way for online recognition is based on the evidence\naccumulation over time to make predictions from stream videos. This paper\npresents a fast yet effective method to recognize actions from stream of noisy\nskeleton data, and a novel weighted covariance descriptor is adopted to\naccumulate evidence. In particular, a fast incremental updating method for the\nweighted covariance descriptor is developed for accumulation of temporal\ninformation and online prediction. The weighted covariance descriptor takes the\nfollowing principles into consideration: past frames have less contribution for\nrecognition and recent and informative frames such as key frames contribute\nmore to the recognition. The online recognition is achieved using a simple\nnearest neighbor search against a set of offline trained action models.\nExperimental results on MSC-12 Kinect Gesture dataset and our newly constructed\nonline action recognition dataset have demonstrated the efficacy of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 09:18:30 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 02:04:51 GMT"}, {"version": "v3", "created": "Wed, 5 Apr 2017 06:45:02 GMT"}, {"version": "v4", "created": "Thu, 6 Jul 2017 11:22:38 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Tang", "Chang", ""], ["Wang", "Pichao", ""], ["Li", "Wanqing", ""]]}, {"id": "1511.03042", "submitter": "Hamed Habibi Aghdam", "authors": "Elnaz J. Heravi, Hamed H. Aghdam, Domenec Puig", "title": "Analyzing Stability of Convolutional Neural Networks in the Frequency\n  Domain", "comments": "Under review as a conference paper at ICLR2016, minor changes in the\n  text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the internal process of ConvNets is commonly done using\nvisualization techniques. However, these techniques do not usually provide a\ntool for estimating the stability of a ConvNet against noise. In this paper, we\nshow how to analyze a ConvNet in the frequency domain using a 4-dimensional\nvisualization technique. Using the frequency domain analysis, we show the\nreason that a ConvNet might be sensitive to a very low magnitude additive\nnoise. Our experiments on a few ConvNets trained on different datasets revealed\nthat convolution kernels of a trained ConvNet usually pass most of the\nfrequencies and they are not able to effectively eliminate the effect of high\nfrequencies. Our next experiments shows that a convolution kernel which has a\nmore concentrated frequency response could be more stable. Finally, we show\nthat fine-tuning a ConvNet using a training set augmented with noisy images can\nproduce more stable ConvNets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 09:54:20 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 08:42:10 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Heravi", "Elnaz J.", ""], ["Aghdam", "Hamed H.", ""], ["Puig", "Domenec", ""]]}, {"id": "1511.03055", "submitter": "Olivier Mor\\`ere", "authors": "Jie Lin, Olivier Mor\\`ere, Julie Petta, Vijay Chandrasekhar, Antoine\n  Veillard", "title": "Tiny Descriptors for Image Retrieval with Unsupervised Triplet Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical image retrieval pipeline starts with the comparison of global\ndescriptors from a large database to find a short list of candidate matches. A\ngood image descriptor is key to the retrieval pipeline and should reconcile two\ncontradictory requirements: providing recall rates as high as possible and\nbeing as compact as possible for fast matching. Following the recent successes\nof Deep Convolutional Neural Networks (DCNN) for large scale image\nclassification, descriptors extracted from DCNNs are increasingly used in place\nof the traditional hand crafted descriptors such as Fisher Vectors (FV) with\nbetter retrieval performances. Nevertheless, the dimensionality of a typical\nDCNN descriptor --extracted either from the visual feature pyramid or the\nfully-connected layers-- remains quite high at several thousands of scalar\nvalues. In this paper, we propose Unsupervised Triplet Hashing (UTH), a fully\nunsupervised method to compute extremely compact binary hashes --in the 32-256\nbits range-- from high-dimensional global descriptors. UTH consists of two\nsuccessive deep learning steps. First, Stacked Restricted Boltzmann Machines\n(SRBM), a type of unsupervised deep neural nets, are used to learn binary\nembedding functions able to bring the descriptor size down to the desired\nbitrate. SRBMs are typically able to ensure a very high compression rate at the\nexpense of loosing some desirable metric properties of the original DCNN\ndescriptor space. Then, triplet networks, a rank learning scheme based on\nweight sharing nets is used to fine-tune the binary embedding functions to\nretain as much as possible of the useful metric properties of the original\nspace. A thorough empirical evaluation conducted on multiple publicly available\ndataset using DCNN descriptors shows that our method is able to significantly\noutperform state-of-the-art unsupervised schemes in the target bit range.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 10:38:37 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Lin", "Jie", ""], ["Mor\u00e8re", "Olivier", ""], ["Petta", "Julie", ""], ["Chandrasekhar", "Vijay", ""], ["Veillard", "Antoine", ""]]}, {"id": "1511.03183", "submitter": "Hyungtae Lee", "authors": "Hyungtae Lee, Heesung Kwon, Ryan M. Robinson, William d. Nothwang, and\n  Amar M. Marathe", "title": "Dynamic Belief Fusion for Object Detection", "comments": "8 pages, 6 figures, 28 references. arXiv admin note: text overlap\n  with arXiv:1502.07643", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach for the fusion of heterogeneous object detection methods is\nproposed. In order to effectively integrate the outputs of multiple detectors,\nthe level of ambiguity in each individual detection score is estimated using\nthe precision/recall relationship of the corresponding detector. The main\ncontribution of the proposed work is a novel fusion method, called Dynamic\nBelief Fusion (DBF), which dynamically assigns probabilities to hypotheses\n(target, non-target, intermediate state (target or non-target)) based on\nconfidence levels in the detection results conditioned on the prior performance\nof individual detectors. In DBF, a joint basic probability assignment,\noptimally fusing information from all detectors, is determined by the\nDempster's combination rule, and is easily reduced to a single fused detection\nscore. Experiments on ARL and PASCAL VOC 07 datasets demonstrate that the\ndetection accuracy of DBF is considerably greater than conventional fusion\napproaches as well as individual detectors used for the fusion.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 17:03:55 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Lee", "Hyungtae", ""], ["Kwon", "Heesung", ""], ["Robinson", "Ryan M.", ""], ["Nothwang", "William d.", ""], ["Marathe", "Amar M.", ""]]}, {"id": "1511.03206", "submitter": "Soheil Kolouri", "authors": "Soheil Kolouri, Se Rim Park, and Gustavo K. Rohde", "title": "The Radon cumulative distribution transform and its application to image\n  classification", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2509419", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invertible image representation methods (transforms) are routinely employed\nas low-level image processing operations based on which feature extraction and\nrecognition algorithms are developed. Most transforms in current use (e.g.\nFourier, Wavelet, etc.) are linear transforms, and, by themselves, are unable\nto substantially simplify the representation of image classes for\nclassification. Here we describe a nonlinear, invertible, low-level image\nprocessing transform based on combining the well known Radon transform for\nimage data, and the 1D Cumulative Distribution Transform proposed earlier. We\ndescribe a few of the properties of this new transform, and with both\ntheoretical and experimental results show that it can often render certain\nproblems linearly separable in transform space.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 18:05:45 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Kolouri", "Soheil", ""], ["Park", "Se Rim", ""], ["Rohde", "Gustavo K.", ""]]}, {"id": "1511.03240", "submitter": "Andreas Geiger", "authors": "Jun Xie and Martin Kiefel and Ming-Ting Sun and Andreas Geiger", "title": "Semantic Instance Annotation of Street Scenes by 3D to 2D Label Transfer", "comments": "10 pages in Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic annotations are vital for training models for object recognition,\nsemantic segmentation or scene understanding. Unfortunately, pixelwise\nannotation of images at very large scale is labor-intensive and only little\nlabeled data is available, particularly at instance level and for street\nscenes. In this paper, we propose to tackle this problem by lifting the\nsemantic instance labeling task from 2D into 3D. Given reconstructions from\nstereo or laser data, we annotate static 3D scene elements with rough bounding\nprimitives and develop a model which transfers this information into the image\ndomain. We leverage our method to obtain 2D labels for a novel suburban video\ndataset which we have collected, resulting in 400k semantic and instance image\nannotations. A comparison of our method to state-of-the-art label transfer\nbaselines reveals that 3D information enables more efficient annotation while\nat the same time resulting in improved accuracy and time-coherent labels.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 19:56:01 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 07:08:11 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Xie", "Jun", ""], ["Kiefel", "Martin", ""], ["Sun", "Ming-Ting", ""], ["Geiger", "Andreas", ""]]}, {"id": "1511.03244", "submitter": "Ujwal Bonde", "authors": "Ujwal Bonde, Vijay Badrinarayanan, Roberto Cipolla and Minh-Tri Pham", "title": "TemplateNet for Depth-Based Object Instance Recognition", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep architecture termed templateNet for depth based\nobject instance recognition. Using an intermediate template layer we exploit\nprior knowledge of an object's shape to sparsify the feature maps. This has\nthree advantages: (i) the network is better regularised resulting in structured\nfilters; (ii) the sparse feature maps results in intuitive features been learnt\nwhich can be visualized as the output of the template layer and (iii) the\nresulting network achieves state-of-the-art performance. The network benefits\nfrom this without any additional parametrization from the template layer. We\nderive the weight updates needed to efficiently train this network in an\nend-to-end manner. We benchmark the templateNet for depth based object instance\nrecognition using two publicly available datasets. The datasets present\nmultiple challenges of clutter, large pose variations and similar looking\ndistractors. Through our experiments we show that with the addition of a\ntemplate layer, a depth based CNN is able to outperform existing\nstate-of-the-art methods in the field.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 20:03:36 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Bonde", "Ujwal", ""], ["Badrinarayanan", "Vijay", ""], ["Cipolla", "Roberto", ""], ["Pham", "Minh-Tri", ""]]}, {"id": "1511.03257", "submitter": "Fatih Cakir", "authors": "Fatih Cakir, Sarah Adel Bargal, Stan Sclaroff", "title": "Online Supervised Hashing for Ever-Growing Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised hashing methods are widely-used for nearest neighbor search in\ncomputer vision applications. Most state-of-the-art supervised hashing\napproaches employ batch-learners. Unfortunately, batch-learning strategies can\nbe inefficient when confronted with large training datasets. Moreover, with\nbatch-learners, it is unclear how to adapt the hash functions as a dataset\ncontinues to grow and diversify over time. Yet, in many practical scenarios the\ndataset grows and diversifies; thus, both the hash functions and the indexing\nmust swiftly accommodate these changes. To address these issues, we propose an\nonline hashing method that is amenable to changes and expansions of the\ndatasets. Since it is an online algorithm, our approach offers linear\ncomplexity with the dataset size. Our solution is supervised, in that we\nincorporate available label information to preserve the semantic neighborhood.\nSuch an adaptive hashing method is attractive; but it requires recomputing the\nhash table as the hash functions are updated. If the frequency of update is\nhigh, then recomputing the hash table entries may cause inefficiencies in the\nsystem, especially for large indexes. Thus, we also propose a framework to\nreduce hash table updates. We compare our method to state-of-the-art solutions\non two benchmarks and demonstrate significant improvements over previous work.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 20:37:41 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Cakir", "Fatih", ""], ["Bargal", "Sarah Adel", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1511.03292", "submitter": "Yezhou Yang", "authors": "Somak Aditya, Yezhou Yang, Chitta Baral, Cornelia Fermuller, Yiannis\n  Aloimonos", "title": "From Images to Sentences through Scene Description Graphs using\n  Commonsense Reasoning and Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the construction of linguistic descriptions of\nimages. This is achieved through the extraction of scene description graphs\n(SDGs) from visual scenes using an automatically constructed knowledge base.\nSDGs are constructed using both vision and reasoning. Specifically, commonsense\nreasoning is applied on (a) detections obtained from existing perception\nmethods on given images, (b) a \"commonsense\" knowledge base constructed using\nnatural language processing of image annotations and (c) lexical ontological\nknowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based\nevaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most\ncases, sentences auto-constructed from SDGs obtained by our method give a more\nrelevant and thorough description of an image than a recent state-of-the-art\nimage caption based approach. Our Image-Sentence Alignment Evaluation results\nare also comparable to that of the recent state-of-the art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 21:14:51 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Aditya", "Somak", ""], ["Yang", "Yezhou", ""], ["Baral", "Chitta", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1511.03296", "submitter": "Jonathan Barron", "authors": "Jonathan T. Barron, Ben Poole", "title": "The Fast Bilateral Solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the bilateral solver, a novel algorithm for edge-aware smoothing\nthat combines the flexibility and speed of simple filtering approaches with the\naccuracy of domain-specific optimization algorithms. Our technique is capable\nof matching or improving upon state-of-the-art results on several different\ncomputer vision tasks (stereo, depth superresolution, colorization, and\nsemantic segmentation) while being 10-1000 times faster than competing\napproaches. The bilateral solver is fast, robust, straightforward to generalize\nto new domains, and simple to integrate into deep learning pipelines.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 21:19:34 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 19:12:04 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Barron", "Jonathan T.", ""], ["Poole", "Ben", ""]]}, {"id": "1511.03328", "submitter": "Liang-Chieh Chen", "authors": "Liang-Chieh Chen, Jonathan T. Barron, George Papandreou, Kevin Murphy,\n  Alan L. Yuille", "title": "Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs\n  and a Discriminatively Trained Domain Transform", "comments": "14 pages. Accepted to appear at CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) are the backbone of state-of-art\nsemantic image segmentation systems. Recent work has shown that complementing\nCNNs with fully-connected conditional random fields (CRFs) can significantly\nenhance their object localization accuracy, yet dense CRF inference is\ncomputationally expensive. We propose replacing the fully-connected CRF with\ndomain transform (DT), a modern edge-preserving filtering method in which the\namount of smoothing is controlled by a reference edge map. Domain transform\nfiltering is several times faster than dense CRF inference and we show that it\nyields comparable semantic segmentation results, accurately capturing object\nboundaries. Importantly, our formulation allows learning the reference edge map\nfrom intermediate CNN features instead of using the image gradient magnitude as\nin standard DT filtering. This produces task-specific edges in an end-to-end\ntrainable system optimizing the target semantic segmentation quality.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 22:54:13 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 02:11:07 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Chen", "Liang-Chieh", ""], ["Barron", "Jonathan T.", ""], ["Papandreou", "George", ""], ["Murphy", "Kevin", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1511.03339", "submitter": "Liang-Chieh Chen", "authors": "Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, Alan L. Yuille", "title": "Attention to Scale: Scale-aware Semantic Image Segmentation", "comments": "14 pages. Accepted to appear at CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating multi-scale features in fully convolutional neural networks\n(FCNs) has been a key element to achieving state-of-the-art performance on\nsemantic image segmentation. One common way to extract multi-scale features is\nto feed multiple resized input images to a shared deep network and then merge\nthe resulting features for pixelwise classification. In this work, we propose\nan attention mechanism that learns to softly weight the multi-scale features at\neach pixel location. We adapt a state-of-the-art semantic image segmentation\nmodel, which we jointly train with multi-scale input images and the attention\nmodel. The proposed attention model not only outperforms average- and\nmax-pooling, but allows us to diagnostically visualize the importance of\nfeatures at different positions and scales. Moreover, we show that adding extra\nsupervision to the output at each scale is essential to achieving excellent\nperformance when merging multi-scale features. We demonstrate the effectiveness\nof our model with extensive experiments on three challenging datasets,\nincluding PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 23:53:57 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 02:02:21 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Chen", "Liang-Chieh", ""], ["Yang", "Yi", ""], ["Wang", "Jiang", ""], ["Xu", "Wei", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1511.03355", "submitter": "Stephen Marsland", "authors": "Stephen Marsland, Carole J Twining", "title": "Principal Autoparallel Analysis: Data Analysis in Weitzenb\\\"{o}ck Space", "comments": "9 pages, conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical analysis of data lying on a differentiable, locally\nEuclidean, manifold introduces a variety of challenges because the analogous\nmeasures to standard Euclidean statistics are local, that is only defined\nwithin a neighbourhood of each datapoint. This is because the curvature of the\nspace means that the connection of Riemannian geometry is path dependent. In\nthis paper we transfer the problem to Weitzenb\\\"{o}ck space, which has torsion,\nbut not curvature, meaning that parallel transport is path independent, and\nrather than considering geodesics, it is natural to consider autoparallels,\nwhich are `straight' in the sense that they follow the local basis vectors. We\ndemonstrate how to generate these autoparallels in a data-driven fashion, and\nshow that the resulting representation of the data is a useful space in which\nto perform further analysis.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 01:52:24 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Marsland", "Stephen", ""], ["Twining", "Carole J", ""]]}, {"id": "1511.03361", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Audrey G. Chung, Devinder Kumar, Farzad\n  Khalvati, Masoom Haider, and Alexander Wong", "title": "Discovery Radiomics via StochasticNet Sequencers for Cancer Detection", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiomics has proven to be a powerful prognostic tool for cancer detection,\nand has previously been applied in lung, breast, prostate, and head-and-neck\ncancer studies with great success. However, these radiomics-driven methods rely\non pre-defined, hand-crafted radiomic feature sets that can limit their ability\nto characterize unique cancer traits. In this study, we introduce a novel\ndiscovery radiomics framework where we directly discover custom radiomic\nfeatures from the wealth of available medical imaging data. In particular, we\nleverage novel StochasticNet radiomic sequencers for extracting custom radiomic\nfeatures tailored for characterizing unique cancer tissue phenotype. Using\nStochasticNet radiomic sequencers discovered using a wealth of lung CT data, we\nperform binary classification on 42,340 lung lesions obtained from the CT scans\nof 93 patients in the LIDC-IDRI dataset. Preliminary results show significant\nimprovement over previous state-of-the-art methods, indicating the potential of\nthe proposed discovery radiomics framework for improving cancer screening and\ndiagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 02:27:23 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Chung", "Audrey G.", ""], ["Kumar", "Devinder", ""], ["Khalvati", "Farzad", ""], ["Haider", "Masoom", ""], ["Wong", "Alexander", ""]]}, {"id": "1511.03363", "submitter": "Sohini Roychowdhury", "authors": "Sohini Roychowdhury", "title": "Facial Expression Detection using Patch-based Eigen-face Isomap Networks", "comments": "6 pages,7 figures, IJCAI-HINA 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated facial expression detection problem pose two primary challenges\nthat include variations in expression and facial occlusions (glasses, beard,\nmustache or face covers). In this paper we introduce a novel automated patch\ncreation technique that masks a particular region of interest in the face,\nfollowed by Eigen-value decomposition of the patched faces and generation of\nIsomaps to detect underlying clustering patterns among faces. The proposed\nmasked Eigen-face based Isomap clustering technique achieves 75% sensitivity\nand 66-73% accuracy in classification of faces with occlusions and smiling\nfaces in around 1 second per image. Also, betweenness centrality, Eigen\ncentrality and maximum information flow can be used as network-based measures\nto identify the most significant training faces for expression classification\ntasks. The proposed method can be used in combination with feature-based\nexpression classification methods in large data sets for improving expression\nclassification accuracies.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 02:39:26 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Roychowdhury", "Sohini", ""]]}, {"id": "1511.03369", "submitter": "Yu-Hui Chen", "authors": "Yu-Hui Chen, Roni Mittelman, Boklye Kim, Charles Meyer, and Alfred\n  Hero", "title": "Multimodal MRI Neuroimaging with Motion Compensation Based on Particle\n  Filtering", "comments": "This paper has been submitted to Transaction on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head movement during scanning impedes activation detection in fMRI studies.\nHead motion in fMRI acquired using slice-based Echo Planar Imaging (EPI) can be\nestimated and compensated by aligning the images onto a reference volume\nthrough image registration. However, registering EPI images volume to volume\nfails to consider head motion between slices, which may lead to severely biased\nhead motion estimates. Slice-to-volume registration can be used to estimate\nmotion parameters for each slice by more accurately representing the image\nacquisition sequence. However, accurate slice to volume mapping is dependent on\nthe information content of the slices: middle slices are information rich,\nwhile edge slides are information poor and more prone to distortion. In this\nwork, we propose a Gaussian particle filter based head motion tracking\nalgorithm to reduce the image misregistration errors. The algorithm uses a\ndynamic state space model of head motion with an observation equation that\nmodels continuous slice acquisition of the scanner. Under this model the\nparticle filter provides more accurate motion estimates and voxel position\nestimates. We demonstrate significant performance improvement of the proposed\napproach as compared to registration-only methods of head motion estimation and\nbrain activation detection.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 02:52:10 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Chen", "Yu-Hui", ""], ["Mittelman", "Roni", ""], ["Kim", "Boklye", ""], ["Meyer", "Charles", ""], ["Hero", "Alfred", ""]]}, {"id": "1511.03398", "submitter": "Sudeng Hu", "authors": "Sudeng Hu, Haiqiang Wang and C.-C. Jay Kuo", "title": "A GMM-Based Stair Quality Model for Human Perceived JPEG Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the notion of just noticeable differences (JND), a stair quality\nfunction (SQF) was recently proposed to model human perception on JPEG images.\nFurthermore, a k-means clustering algorithm was adopted to aggregate JND data\ncollected from multiple subjects to generate a single SQF. In this work, we\npropose a new method to derive the SQF using the Gaussian Mixture Model (GMM).\nThe newly derived SQF can be interpreted as a way to characterize the mean\nviewer experience. Furthermore, it has a lower information criterion (BIC)\nvalue than the previous one, indicating that it offers a better model. A\nspecific example is given to demonstrate the advantages of the new approach.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 06:44:31 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Hu", "Sudeng", ""], ["Wang", "Haiqiang", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1511.03416", "submitter": "Yuke Zhu", "authors": "Yuke Zhu, Oliver Groth, Michael Bernstein and Li Fei-Fei", "title": "Visual7W: Grounded Question Answering in Images", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have seen great progress in basic perceptual tasks such as object\nrecognition and detection. However, AI models still fail to match humans in\nhigh-level vision tasks due to the lack of capacities for deeper reasoning.\nRecently the new task of visual question answering (QA) has been proposed to\nevaluate a model's capacity for deep image understanding. Previous works have\nestablished a loose, global association between QA sentences and images.\nHowever, many questions and answers, in practice, relate to local regions in\nthe images. We establish a semantic link between textual descriptions and image\nregions by object-level grounding. It enables a new type of QA with visual\nanswers, in addition to textual answers used in previous work. We study the\nvisual QA tasks in a grounded setting with a large collection of 7W\nmultiple-choice QA pairs. Furthermore, we evaluate human performance and\nseveral baseline models on the QA tasks. Finally, we propose a novel LSTM model\nwith spatial attention to tackle the 7W QA tasks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 08:29:14 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 21:53:55 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2015 19:37:20 GMT"}, {"version": "v4", "created": "Sat, 9 Apr 2016 07:18:10 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Zhu", "Yuke", ""], ["Groth", "Oliver", ""], ["Bernstein", "Michael", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1511.03464", "submitter": "Rolf Jagerman", "authors": "Jan Deriu, Rolf Jagerman and Kai-En Tsay", "title": "A Directional Diffusion Algorithm for Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of inpainting involves reconstructing the missing areas of an\nimage. Inpainting has many applications, such as reconstructing old damaged\nphotographs or removing obfuscations from images. In this paper we present the\ndirectional diffusion algorithm for inpainting. Typical diffusion algorithms\nare bad at propagating edges from the image into the unknown masked regions.\nThe directional diffusion algorithm improves on the regular diffusion algorithm\nby reconstructing edges more accurately. It scores better than regular\ndiffusion when reconstructing images that are obfuscated by a text mask.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 11:38:22 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Deriu", "Jan", ""], ["Jagerman", "Rolf", ""], ["Tsay", "Kai-En", ""]]}, {"id": "1511.03466", "submitter": "Nikolaos Arvanitopoulos", "authors": "Ksenia Konyushkova, Nikolaos Arvanitopoulos, Zhargalma Dandarova\n  Robert, Pierre-Yves Brandt and Sabine S\\\"usstrunk", "title": "God(s) Know(s): Developmental and Cross-Cultural Patterns in Children\n  Drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach to data analysis designed for the\nneeds of specialists in psychology of religion. We detect developmental and\ncross-cultural patterns in children's drawings of God(s) and other supernatural\nagents. We develop methods to objectively evaluate our empirical observations\nof the drawings with respect to: (1) the gravity center, (2) the average\nintensities of the colors \\emph{green} and \\emph{yellow}, (3) the use of\ndifferent colors (palette) and (4) the visual complexity of the drawings. We\nfind statistically significant differences across ages and countries in the\ngravity centers and in the average intensities of colors. These findings\nsupport the hypotheses of the experts and raise new questions for further\ninvestigation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 11:47:58 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2016 15:28:49 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Konyushkova", "Ksenia", ""], ["Arvanitopoulos", "Nikolaos", ""], ["Robert", "Zhargalma Dandarova", ""], ["Brandt", "Pierre-Yves", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "1511.03476", "submitter": "Zhongwen Xu", "authors": "Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, Yueting Zhuang", "title": "Hierarchical Recurrent Neural Encoder for Video Representation with\n  Application to Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning approach, especially deep Convolutional Neural\nNetworks (ConvNets), have achieved overwhelming accuracy with fast processing\nspeed for image classification. Incorporating temporal structure with deep\nConvNets for video representation becomes a fundamental problem for video\ncontent analysis. In this paper, we propose a new approach, namely Hierarchical\nRecurrent Neural Encoder (HRNE), to exploit temporal information of videos.\nCompared to recent video representation inference approaches, this paper makes\nthe following three contributions. First, our HRNE is able to efficiently\nexploit video temporal structure in a longer range by reducing the length of\ninput information flow, and compositing multiple consecutive inputs at a higher\nlevel. Second, computation operations are significantly lessened while\nattaining more non-linearity. Third, HRNE is able to uncover temporal\ntransitions between frame chunks with different granularities, i.e., it can\nmodel the temporal transitions between frames as well as the transitions\nbetween segments. We apply the new method to video captioning where temporal\ninformation plays a crucial role. Experiments demonstrate that our method\noutperforms the state-of-the-art on video captioning benchmarks. Notably, even\nusing a single network with only RGB stream as input, HRNE beats all the recent\nsystems which combine multiple inputs, such as RGB ConvNet plus 3D ConvNet.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 12:38:14 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Pan", "Pingbo", ""], ["Xu", "Zhongwen", ""], ["Yang", "Yi", ""], ["Wu", "Fei", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1511.03607", "submitter": "Ju Sun", "authors": "Ju Sun, Qing Qu, John Wright", "title": "Complete Dictionary Recovery over the Sphere I: Overview and the\n  Geometric Picture", "comments": "Accepted by IEEE Transaction on Information Theory; revised according\n  to the reviewers' comments", "journal-ref": "IEEE Trans. Information Theory, 63(2): 853 - 884 (2017)", "doi": "10.1109/TIT.2016.2632162", "report-no": null, "categories": "cs.IT cs.CV math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a complete (i.e., square and\ninvertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb{R}^{n \\times p}$\nwith $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is\nsufficiently sparse. This recovery problem is central to theoretical\nunderstanding of dictionary learning, which seeks a sparse representation for a\ncollection of input signals and finds numerous applications in modern signal\nprocessing and machine learning. We give the first efficient algorithm that\nprovably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per\ncolumn, under suitable probability model for $\\mathbf X_0$. In contrast, prior\nresults based on efficient algorithms either only guarantee recovery when\n$\\mathbf X_0$ has $O(\\sqrt{n})$ zeros per column, or require multiple rounds of\nSDP relaxation to work when $\\mathbf X_0$ has $O(n^{1-\\delta})$ nonzeros per\ncolumn (for any constant $\\delta \\in (0, 1)$). }\n  Our algorithmic pipeline centers around solving a certain nonconvex\noptimization problem with a spherical constraint. In this paper, we provide a\ngeometric characterization of the objective landscape. In particular, we show\nthat the problem is highly structured: with high probability, (1) there are no\n\"spurious\" local minimizers; and (2) around all saddle points the objective has\na negative directional curvature. This distinctive structure makes the problem\namenable to efficient optimization algorithms. In a companion paper\n(arXiv:1511.04777), we design a second-order trust-region algorithm over the\nsphere that provably converges to a local minimizer from arbitrary\ninitializations, despite the presence of saddle points.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 19:09:22 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 03:48:37 GMT"}, {"version": "v3", "created": "Thu, 1 Sep 2016 17:19:08 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Sun", "Ju", ""], ["Qu", "Qing", ""], ["Wright", "John", ""]]}, {"id": "1511.03629", "submitter": "John Stuart Haberl Baxter", "authors": "John S.H. Baxter, Jonathan McLeod, and Terry M. Peters", "title": "A Continuous Max-Flow Approach to Cyclic Field Reconstruction", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of an image from noisy data using Markov Random Field theory\nhas been explored by both the graph-cuts and continuous max-flow community in\nthe form of the Potts and Ishikawa models. However, neither model takes into\naccount the particular cyclic topology of specific intensity types such as the\nhue in natural colour images, or the phase in complex valued MRI. This paper\npresents \\textit{cyclic continuous max-flow} image reconstruction which models\nthe intensity being reconstructed as having a fundamentally cyclic topology.\nThis model complements the Ishikawa model in that it is designed with image\nreconstruction in mind, having the topology of the intensity space inherent in\nthe model while being readily extendable to an arbitrary intensity resolution.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 19:55:05 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Baxter", "John S. H.", ""], ["McLeod", "Jonathan", ""], ["Peters", "Terry M.", ""]]}, {"id": "1511.03650", "submitter": "arXiv Admin", "authors": "Cheng-Yang Fu and Alexander C. Berg", "title": "Piecewise Linear Activation Functions For More Efficient Deep Networks", "comments": "Withdrawn by arXiv admins", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This submission has been withdrawn by arXiv administrators because it is\nintentionally incomplete, which is in violation of our policies.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 20:54:28 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 22:03:51 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2015 14:20:01 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Fu", "Cheng-Yang", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1511.03690", "submitter": "David Harwath", "authors": "David Harwath and James Glass", "title": "Deep Multimodal Semantic Embeddings for Speech and Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a model which takes as input a corpus of images\nwith relevant spoken captions and finds a correspondence between the two\nmodalities. We employ a pair of convolutional neural networks to model visual\nobjects and speech signals at the word level, and tie the networks together\nwith an embedding and alignment model which learns a joint semantic space over\nboth modalities. We evaluate our model using image search and annotation tasks\non the Flickr8k dataset, which we augmented by collecting a corpus of 40,000\nspoken captions using Amazon Mechanical Turk.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 21:30:10 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Harwath", "David", ""], ["Glass", "James", ""]]}, {"id": "1511.03745", "submitter": "Marcus Rohrbach", "authors": "Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, Bernt\n  Schiele", "title": "Grounding of Textual Phrases in Images by Reconstruction", "comments": "published at ECCV 2016 (oral); updated to final version", "journal-ref": null, "doi": "10.1007/978-3-319-46448-0_49", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual\ncontent is a challenging problem with many applications for human-computer\ninteraction and image-text reference resolution. Few datasets provide the\nground truth spatial localization of phrases, thus it is desirable to learn\nfrom data with no or little grounding supervision. We propose a novel approach\nwhich learns grounding by reconstructing a given phrase using an attention\nmechanism, which can be either latent or optimized directly. During training\nour approach encodes the phrase using a recurrent network language model and\nthen learns to attend to the relevant image region in order to reconstruct the\ninput phrase. At test time, the correct attention, i.e., the grounding, is\nevaluated. If grounding supervision is available it can be directly applied via\na loss over the attention mechanism. We demonstrate the effectiveness of our\napproach on the Flickr 30k Entities and ReferItGame datasets with different\nlevels of supervision, ranging from no supervision over partial supervision to\nfull supervision. Our supervised variant improves by a large margin over the\nstate-of-the-art on both datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 01:13:47 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 18:59:11 GMT"}, {"version": "v3", "created": "Fri, 18 Mar 2016 04:03:15 GMT"}, {"version": "v4", "created": "Fri, 17 Feb 2017 21:02:05 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Rohrbach", "Anna", ""], ["Rohrbach", "Marcus", ""], ["Hu", "Ronghang", ""], ["Darrell", "Trevor", ""], ["Schiele", "Bernt", ""]]}, {"id": "1511.03748", "submitter": "Joon-Young  Lee", "authors": "Joon-Young Lee, Kalyan Sunkavalli, Zhe Lin, Xiaohui Shen, In So Kweon", "title": "Automatic Content-Aware Color and Tone Stylization", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new technique that automatically generates diverse, visually\ncompelling stylizations for a photograph in an unsupervised manner. We achieve\nthis by learning style ranking for a given input using a large photo collection\nand selecting a diverse subset of matching styles for final style transfer. We\nalso propose a novel technique that transfers the global color and tone of the\nchosen exemplars to the input photograph while avoiding the common visual\nartifacts produced by the existing style transfer methods. Together, our style\nselection and transfer techniques produce compelling, artifact-free results on\na wide range of input photographs, and a user study shows that our results are\npreferred over other techniques.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 01:31:35 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Lee", "Joon-Young", ""], ["Sunkavalli", "Kalyan", ""], ["Lin", "Zhe", ""], ["Shen", "Xiaohui", ""], ["Kweon", "In So", ""]]}, {"id": "1511.03753", "submitter": "Rafael Reisenhofer", "authors": "Rafael Reisenhofer, Johannes Kiefer and Emily J. King", "title": "Shearlet-Based Detection of Flame Fronts", "comments": null, "journal-ref": "Experiments in Fluids, vol. 57(3), 41:1-41:14, 2016", "doi": "10.1007/s00348-016-2128-6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying and characterizing flame fronts is the most common task in the\ncomputer-assisted analysis of data obtained from imaging techniques such as\nplanar laser-induced fluorescence (PLIF), laser Rayleigh scattering (LRS), or\nparticle imaging velocimetry (PIV). We present a novel edge and ridge (line)\ndetection algorithm based on complex-valued wavelet-like analyzing functions --\nso-called complex shearlets -- displaying several traits useful for the\nextraction of flame fronts. In addition to providing a unified approach to the\ndetection of edges and ridges, our method inherently yields estimates of local\ntangent orientations and local curvatures. To examine the applicability for\nhigh-frequency recordings of combustion processes, the algorithm is applied to\nmock images distorted with varying degrees of noise and real-world PLIF images\nof both OH and CH radicals. Furthermore, we compare the performance of the\nnewly proposed complex shearlet-based measure to well-established edge and\nridge detection techniques such as the Canny edge detector, another\nshearlet-based edge detector, and the phase congruency measure.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 01:52:12 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 18:23:33 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Reisenhofer", "Rafael", ""], ["Kiefer", "Johannes", ""], ["King", "Emily J.", ""]]}, {"id": "1511.03776", "submitter": "Chen Sun", "authors": "Chen Sun and Manohar Paluri and Ronan Collobert and Ram Nevatia and\n  Lubomir Bourdev", "title": "ProNet: Learning to Propose Object-specific Boxes for Cascaded Neural\n  Networks", "comments": "CVPR 2016 (fixed reference issue)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to classify and locate objects accurately and efficiently,\nwithout using bounding box annotations. It is challenging as objects in the\nwild could appear at arbitrary locations and in different scales. In this\npaper, we propose a novel classification architecture ProNet based on\nconvolutional neural networks. It uses computationally efficient neural\nnetworks to propose image regions that are likely to contain objects, and\napplies more powerful but slower networks on the proposed regions. The basic\nbuilding block is a multi-scale fully-convolutional network which assigns\nobject confidence scores to boxes at different locations and scales. We show\nthat such networks can be trained effectively using image-level annotations,\nand can be connected into cascades or trees for efficient object\nclassification. ProNet outperforms previous state-of-the-art significantly on\nPASCAL VOC 2012 and MS COCO datasets for object classification and point-based\nlocalization.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 05:06:16 GMT"}, {"version": "v2", "created": "Sun, 10 Apr 2016 04:42:22 GMT"}, {"version": "v3", "created": "Wed, 13 Apr 2016 02:56:43 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Sun", "Chen", ""], ["Paluri", "Manohar", ""], ["Collobert", "Ronan", ""], ["Nevatia", "Ram", ""], ["Bourdev", "Lubomir", ""]]}, {"id": "1511.03791", "submitter": "Fangyi Zhang", "authors": "Fangyi Zhang, J\\\"urgen Leitner, Michael Milford, Ben Upcroft, Peter\n  Corke", "title": "Towards Vision-Based Deep Reinforcement Learning for Robotic Motion\n  Control", "comments": "8 pages, to appear in the proceedings of Australasian Conference on\n  Robotics and Automation (ACRA) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a machine learning based system for controlling a\nrobotic manipulator with visual perception only. The capability to autonomously\nlearn robot controllers solely from raw-pixel images and without any prior\nknowledge of configuration is shown for the first time. We build upon the\nsuccess of recent deep reinforcement learning and develop a system for learning\ntarget reaching with a three-joint robot manipulator using external visual\nobservation. A Deep Q Network (DQN) was demonstrated to perform target reaching\nafter training in simulation. Transferring the network to real hardware and\nreal observation in a naive approach failed, but experiments show that the\nnetwork works when replacing camera images with synthetic images.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 06:19:59 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2015 05:41:08 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Zhang", "Fangyi", ""], ["Leitner", "J\u00fcrgen", ""], ["Milford", "Michael", ""], ["Upcroft", "Ben", ""], ["Corke", "Peter", ""]]}, {"id": "1511.03814", "submitter": "Amir Rosenfeld", "authors": "Amir Rosenfeld, Shimon Ullman", "title": "Hand-Object Interaction and Precise Localization in Transitive Action\n  Recognition", "comments": "Minor changes: title and abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition in still images has seen major improvement in recent years\ndue to advances in human pose estimation, object recognition and stronger\nfeature representations produced by deep neural networks. However, there are\nstill many cases in which performance remains far from that of humans. A major\ndifficulty arises in distinguishing between transitive actions in which the\noverall actor pose is similar, and recognition therefore depends on details of\nthe grasp and the object, which may be largely occluded. In this paper we\ndemonstrate how recognition is improved by obtaining precise localization of\nthe action-object and consequently extracting details of the object shape\ntogether with the actor-object interaction. To obtain exact localization of the\naction object and its interaction with the actor, we employ a coarse-to-fine\napproach which combines semantic segmentation and contextual features, in\nsuccessive stages. We focus on (but are not limited) to face-related actions, a\nset of actions that includes several currently challenging categories. We\npresent an average relative improvement of 35% over state-of-the art and\nvalidate through experimentation the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 08:41:47 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 09:29:18 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Rosenfeld", "Amir", ""], ["Ullman", "Shimon", ""]]}, {"id": "1511.03853", "submitter": "Ilja Kuzborskij", "authors": "Ilja Kuzborskij, Fabio Maria Carlucci, Barbara Caputo", "title": "When Na\\\"ive Bayes Nearest Neighbours Meet Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Convolutional Neural Networks (CNNs) have become the leading learning\nparadigm in visual recognition, Naive Bayes Nearest Neighbour (NBNN)-based\nclassifiers have lost momentum in the community. This is because (1) such\nalgorithms cannot use CNN activations as input features; (2) they cannot be\nused as final layer of CNN architectures for end-to-end training , and (3) they\nare generally not scalable and hence cannot handle big data. This paper\nproposes a framework that addresses all these issues, thus bringing back NBNNs\non the map. We solve the first by extracting CNN activations from local patches\nat multiple scale levels, similarly to [1]. We address simultaneously the\nsecond and third by proposing a scalable version of Naive Bayes Non-linear\nLearning (NBNL, [2]). Results obtained using pre-trained CNNs on standard scene\nand domain adaptation databases show the strength of our approach, opening a\nnew season for NBNNs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 10:54:21 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 11:45:14 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Kuzborskij", "Ilja", ""], ["Carlucci", "Fabio Maria", ""], ["Caputo", "Barbara", ""]]}, {"id": "1511.03855", "submitter": "Wu-Jun Li", "authors": "Wu-Jun Li, Sheng Wang, and Wang-Cheng Kang", "title": "Feature Learning based Deep Supervised Hashing with Pairwise Labels", "comments": "IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed wide application of hashing for large-scale image\nretrieval. However, most existing hashing methods are based on hand-crafted\nfeatures which might not be optimally compatible with the hashing procedure.\nRecently, deep hashing methods have been proposed to perform simultaneous\nfeature learning and hash-code learning with deep neural networks, which have\nshown better performance than traditional hashing methods with hand-crafted\nfeatures. Most of these deep hashing methods are supervised whose supervised\ninformation is given with triplet labels. For another common application\nscenario with pairwise labels, there have not existed methods for simultaneous\nfeature learning and hash-code learning. In this paper, we propose a novel deep\nhashing method, called deep pairwise-supervised hashing(DPSH), to perform\nsimultaneous feature learning and hash-code learning for applications with\npairwise labels. Experiments on real datasets show that our DPSH method can\noutperform other methods to achieve the state-of-the-art performance in image\nretrieval applications.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 11:11:42 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 09:27:38 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Li", "Wu-Jun", ""], ["Wang", "Sheng", ""], ["Kang", "Wang-Cheng", ""]]}, {"id": "1511.03908", "submitter": "Natalia Neverova", "authors": "Natalia Neverova, Christian Wolf, Griffin Lacey, Lex Fridman, Deepak\n  Chandra, Brandon Barbello, Graham Taylor", "title": "Learning Human Identity from Motion Patterns", "comments": "10 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a large-scale study exploring the capability of temporal deep\nneural networks to interpret natural human kinematics and introduce the first\nmethod for active biometric authentication with mobile inertial sensors. At\nGoogle, we have created a first-of-its-kind dataset of human movements,\npassively collected by 1500 volunteers using their smartphones daily over\nseveral months. We (1) compare several neural architectures for efficient\nlearning of temporal multi-modal data representations, (2) propose an optimized\nshift-invariant dense convolutional mechanism (DCWRNN), and (3) incorporate the\ndiscriminatively-trained dynamic features in a probabilistic generative\nframework taking into account temporal characteristics. Our results demonstrate\nthat human kinematics convey important information about user identity and can\nserve as a valuable component of multi-modal authentication systems.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 14:48:53 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 15:23:06 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2015 01:59:58 GMT"}, {"version": "v4", "created": "Thu, 21 Apr 2016 16:04:00 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Neverova", "Natalia", ""], ["Wolf", "Christian", ""], ["Lacey", "Griffin", ""], ["Fridman", "Lex", ""], ["Chandra", "Deepak", ""], ["Barbello", "Brandon", ""], ["Taylor", "Graham", ""]]}, {"id": "1511.03979", "submitter": "Patrick McClure", "authors": "Patrick McClure, Nikolaus Kriegeskorte", "title": "Representational Distance Learning for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) provide useful models of visual representational\ntransformations. We present a method that enables a DNN (student) to learn from\nthe internal representational spaces of a reference model (teacher), which\ncould be another DNN or, in the future, a biological brain. Representational\nspaces of the student and the teacher are characterized by representational\ndistance matrices (RDMs). We propose representational distance learning (RDL),\na stochastic gradient descent method that drives the RDMs of the student to\napproximate the RDMs of the teacher. We demonstrate that RDL is competitive\nwith other transfer learning techniques for two publicly available benchmark\ncomputer vision datasets (MNIST and CIFAR-100), while allowing for\narchitectural differences between student and teacher. By pulling the student's\nRDMs towards those of the teacher, RDL significantly improved visual\nclassification performance when compared to baseline networks that did not use\ntransfer learning. In the future, RDL may enable combined supervised training\nof deep neural networks using task constraints (e.g. images and category\nlabels) and constraints from brain-activity measurements, so as to build models\nthat replicate the internal representational spaces of biological brains.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 17:35:03 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2015 13:58:48 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2015 17:19:03 GMT"}, {"version": "v4", "created": "Thu, 26 Nov 2015 17:10:25 GMT"}, {"version": "v5", "created": "Fri, 4 Dec 2015 20:45:15 GMT"}, {"version": "v6", "created": "Mon, 7 Nov 2016 18:37:05 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["McClure", "Patrick", ""], ["Kriegeskorte", "Nikolaus", ""]]}, {"id": "1511.03995", "submitter": "Kin Gwn Lore", "authors": "Kin Gwn Lore, Adedotun Akintayo, Soumik Sarkar", "title": "LLNet: A Deep Autoencoder Approach to Natural Low-light Image\n  Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In surveillance, monitoring and tactical reconnaissance, gathering the right\nvisual information from a dynamic environment and accurately processing such\ndata are essential ingredients to making informed decisions which determines\nthe success of an operation. Camera sensors are often cost-limited in ability\nto clearly capture objects without defects from images or videos taken in a\npoorly-lit environment. The goal in many applications is to enhance the\nbrightness, contrast and reduce noise content of such images in an on-board\nreal-time manner. We propose a deep autoencoder-based approach to identify\nsignal features from low-light images handcrafting and adaptively brighten\nimages without over-amplifying the lighter parts in images (i.e., without\nsaturation of image pixels) in high dynamic range. We show that a variant of\nthe recently proposed stacked-sparse denoising autoencoder can learn to\nadaptively enhance and denoise from synthetically darkened and noisy training\nexamples. The network can then be successfully applied to naturally low-light\nenvironment and/or hardware degraded images. Results show significant\ncredibility of deep learning based approaches both visually and by quantitative\ncomparison with various popular enhancing, state-of-the-art denoising and\nhybrid enhancing-denoising techniques.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 18:31:42 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 18:50:01 GMT"}, {"version": "v3", "created": "Fri, 15 Apr 2016 00:54:24 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Lore", "Kin Gwn", ""], ["Akintayo", "Adedotun", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1511.04003", "submitter": "Dmitry Kislyuk", "authors": "Dmitry Kislyuk, Yuchen Liu, David Liu, Eric Tzeng, Yushi Jing", "title": "Human Curation and Convnets: Powering Item-to-Item Recommendations on\n  Pinterest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Pinterest Related Pins, an item-to-item recommendation\nsystem that combines collaborative filtering with content-based ranking. We\ndemonstrate that signals derived from user curation, the activity of users\norganizing content, are highly effective when used in conjunction with\ncontent-based ranking. This paper also demonstrates the effectiveness of visual\nfeatures, such as image or object representations learned from convnets, in\nimproving the user engagement rate of our item-to-item recommendation system.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 18:55:20 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Kislyuk", "Dmitry", ""], ["Liu", "Yuchen", ""], ["Liu", "David", ""], ["Tzeng", "Eric", ""], ["Jing", "Yushi", ""]]}, {"id": "1511.04024", "submitter": "Zachary Seymour", "authors": "Zachary Seymour, Yingming Li, Zhongfei Zhang", "title": "Multimodal Skip-gram Using Convolutional Pseudowords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the representational mapping across multimodal data such\nthat given a piece of the raw data in one modality the corresponding semantic\ndescription in terms of the raw data in another modality is immediately\nobtained. Such a representational mapping can be found in a wide spectrum of\nreal-world applications including image/video retrieval, object recognition,\naction/behavior recognition, and event understanding and prediction. To that\nend, we introduce a simplified training objective for learning multimodal\nembeddings using the skip-gram architecture by introducing convolutional\n\"pseudowords:\" embeddings composed of the additive combination of distributed\nword representations and image features from convolutional neural networks\nprojected into the multimodal space. We present extensive results of the\nrepresentational properties of these embeddings on various word similarity\nbenchmarks to show the promise of this approach.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 19:32:08 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2015 19:09:38 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Seymour", "Zachary", ""], ["Li", "Yingming", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "1511.04031", "submitter": "Tal Hassner", "authors": "Yue Wu, Tal Hassner, KangGeon Kim, Gerard Medioni and Prem Natarajan", "title": "Facial Landmark Detection with Tweaked Convolutional Neural Networks", "comments": "First two authors had joint first authorship / equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel convolutional neural network (CNN) design for facial\nlandmark coordinate regression. We examine the intermediate features of a\nstandard CNN trained for landmark detection and show that features extracted\nfrom later, more specialized layers capture rough landmark locations. This\nprovides a natural means of applying differential treatment midway through the\nnetwork, tweaking processing based on facial alignment. The resulting Tweaked\nCNN model (TCNN) harnesses the robustness of CNNs for landmark detection, in an\nappearance-sensitive manner without training multi-part or multi-scale models.\nOur results on standard face landmark detection and face verification\nbenchmarks show TCNN to surpasses previously published performances by wide\nmargins.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 19:50:53 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 20:11:53 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Wu", "Yue", ""], ["Hassner", "Tal", ""], ["Kim", "KangGeon", ""], ["Medioni", "Gerard", ""], ["Natarajan", "Prem", ""]]}, {"id": "1511.04048", "submitter": "Roozbeh Mottaghi", "authors": "Roozbeh Mottaghi, Hessam Bagherinezhad, Mohammad Rastegari, Ali\n  Farhadi", "title": "Newtonian Image Understanding: Unfolding the Dynamics of Objects in\n  Static Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the challenging problem of predicting the dynamics of\nobjects in static images. Given a query object in an image, our goal is to\nprovide a physical understanding of the object in terms of the forces acting\nupon it and its long term motion as response to those forces. Direct and\nexplicit estimation of the forces and the motion of objects from a single image\nis extremely challenging. We define intermediate physical abstractions called\nNewtonian scenarios and introduce Newtonian Neural Network ($N^3$) that learns\nto map a single image to a state in a Newtonian scenario. Our experimental\nevaluations show that our method can reliably predict dynamics of a query\nobject from a single image. In addition, our approach can provide physical\nreasoning that supports the predicted dynamics in terms of velocity and force\nvectors. To spur research in this direction we compiled Visual Newtonian\nDynamics (VIND) dataset that includes 6806 videos aligned with Newtonian\nscenarios represented using game engines, and 4516 still images with their\nground truth dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 20:21:11 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Mottaghi", "Roozbeh", ""], ["Bagherinezhad", "Hessam", ""], ["Rastegari", "Mohammad", ""], ["Farhadi", "Ali", ""]]}, {"id": "1511.04056", "submitter": "Mohammad Norouzi", "authors": "Mohammad Norouzi, Maxwell D. Collins, Matthew Johnson, David J. Fleet,\n  Pushmeet Kohli", "title": "Efficient non-greedy optimization of decision trees", "comments": "in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision trees and randomized forests are widely used in computer vision and\nmachine learning. Standard algorithms for decision tree induction optimize the\nsplit functions one node at a time according to some splitting criteria. This\ngreedy procedure often leads to suboptimal trees. In this paper, we present an\nalgorithm for optimizing the split functions at all levels of the tree jointly\nwith the leaf parameters, based on a global objective. We show that the problem\nof finding optimal linear-combination (oblique) splits for decision trees is\nrelated to structured prediction with latent variables, and we formulate a\nconvex-concave upper bound on the tree's empirical loss. The run-time of\ncomputing the gradient of the proposed surrogate objective with respect to each\ntraining exemplar is quadratic in the the tree depth, and thus training deep\ntrees is feasible. The use of stochastic gradient descent for optimization\nenables effective training with large datasets. Experiments on several\nclassification benchmarks demonstrate that the resulting non-greedy decision\ntrees outperform greedy decision tree baselines.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 20:32:28 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Norouzi", "Mohammad", ""], ["Collins", "Maxwell D.", ""], ["Johnson", "Matthew", ""], ["Fleet", "David J.", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1511.04067", "submitter": "Oncel Tuzel", "authors": "Raviteja Vemulapalli and Oncel Tuzel and Ming-Yu Liu", "title": "Deep Gaussian Conditional Random Field Network: A Model-based Deep\n  Network for Discriminative Denoising", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep network architecture for image\\\\ denoising based on a\nGaussian Conditional Random Field (GCRF) model. In contrast to the existing\ndiscriminative denoising methods that train a separate model for each noise\nlevel, the proposed deep network explicitly models the input noise variance and\nhence is capable of handling a range of noise levels. Our deep network, which\nwe refer to as deep GCRF network, consists of two sub-networks: (i) a parameter\ngeneration network that generates the pairwise potential parameters based on\nthe noisy input image, and (ii) an inference network whose layers perform the\ncomputations involved in an iterative GCRF inference procedure.\\ We train the\nentire deep GCRF network (both parameter generation and inference networks)\ndiscriminatively in an end-to-end fashion by maximizing the peak\nsignal-to-noise ratio measure. Experiments on Berkeley segmentation and\nPASCALVOC datasets show that the proposed deep GCRF network outperforms\nstate-of-the-art image denoising approaches for several noise levels.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 20:49:20 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Vemulapalli", "Raviteja", ""], ["Tuzel", "Oncel", ""], ["Liu", "Ming-Yu", ""]]}, {"id": "1511.04103", "submitter": "Panqu Wang", "authors": "Panqu Wang, Garrison W. Cottrell", "title": "Basic Level Categorization Facilitates Visual Object Recognition", "comments": "ICLR 2016 submission R1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have led to significant progress in the\ncomputer vision field, especially for visual object recognition tasks. The\nfeatures useful for object classification are learned by feed-forward deep\nconvolutional neural networks (CNNs) automatically, and they are shown to be\nable to predict and decode neural representations in the ventral visual pathway\nof humans and monkeys. However, despite the huge amount of work on optimizing\nCNNs, there has not been much research focused on linking CNNs with guiding\nprinciples from the human visual cortex. In this work, we propose a network\noptimization strategy inspired by both of the developmental trajectory of\nchildren's visual object recognition capabilities, and Bar (2003), who\nhypothesized that basic level information is carried in the fast magnocellular\npathway through the prefrontal cortex (PFC) and then projected back to inferior\ntemporal cortex (IT), where subordinate level categorization is achieved. We\ninstantiate this idea by training a deep CNN to perform basic level object\ncategorization first, and then train it on subordinate level categorization. We\napply this idea to training AlexNet (Krizhevsky et al., 2012) on the ILSVRC\n2012 dataset and show that the top-5 accuracy increases from 80.13% to 82.14%,\ndemonstrating the effectiveness of the method. We also show that subsequent\ntransfer learning on smaller datasets gives superior results.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 21:41:35 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 21:47:35 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 08:26:54 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Wang", "Panqu", ""], ["Cottrell", "Garrison W.", ""]]}, {"id": "1511.04110", "submitter": "Ali Mollahosseini", "authors": "Ali Mollahosseini, David Chan, Mohammad H. Mahoor", "title": "Going Deeper in Facial Expression Recognition using Deep Neural Networks", "comments": "To be appear in IEEE Winter Conference on Applications of Computer\n  Vision (WACV), 2016 {Accepted in first round submission}", "journal-ref": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2016", "doi": "10.1109/WACV.2016.7477450", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Facial Expression Recognition (FER) has remained a challenging and\ninteresting problem. Despite efforts made in developing various methods for\nFER, existing approaches traditionally lack generalizability when applied to\nunseen images or those that are captured in wild setting. Most of the existing\napproaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where\nthe classifier's hyperparameters are tuned to give best recognition accuracies\nacross a single database, or a small collection of similar databases.\nNevertheless, the results are not significant when they are applied to novel\ndata. This paper proposes a deep neural network architecture to address the FER\nproblem across multiple well-known standard face datasets. Specifically, our\nnetwork consists of two convolutional layers each followed by max pooling and\nthen four Inception layers. The network is a single component architecture that\ntakes registered facial images as the input and classifies them into either of\nthe six basic or the neutral expressions. We conducted comprehensive\nexperiments on seven publically available facial expression databases, viz.\nMultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposed\narchitecture are comparable to or better than the state-of-the-art methods and\nbetter than traditional convolutional neural networks and in both accuracy and\ntraining time.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 22:10:46 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Mollahosseini", "Ali", ""], ["Chan", "David", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "1511.04119", "submitter": "Shikhar Sharma", "authors": "Shikhar Sharma, Ryan Kiros, Ruslan Salakhutdinov", "title": "Action Recognition using Visual Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a soft attention based model for the task of action recognition in\nvideos. We use multi-layered Recurrent Neural Networks (RNNs) with Long\nShort-Term Memory (LSTM) units which are deep both spatially and temporally.\nOur model learns to focus selectively on parts of the video frames and\nclassifies videos after taking a few glimpses. The model essentially learns\nwhich parts in the frames are relevant for the task at hand and attaches higher\nimportance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51\nand Hollywood2 datasets and analyze how the model focuses its attention\ndepending on the scene and the action being performed.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 23:06:42 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 20:46:47 GMT"}, {"version": "v3", "created": "Sun, 14 Feb 2016 17:20:19 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Sharma", "Shikhar", ""], ["Kiros", "Ryan", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1511.04136", "submitter": "Siwei Lyu", "authors": "Longyin Wen, Dawei Du, Zhaowei Cai, Zhen Lei, Ming-Ching Chang,\n  Honggang Qi, Jongwoo Lim, Ming-Hsuan Yang, Siwei Lyu", "title": "UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and\n  Tracking", "comments": "18 pages, 11 figures, accepted by CVIU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, numerous effective multi-object tracking (MOT) methods are\ndeveloped because of the wide range of applications. Existing performance\nevaluations of MOT methods usually separate the object tracking step from the\nobject detection step by using the same fixed object detection results for\ncomparisons. In this work, we perform a comprehensive quantitative study on the\neffects of object detection accuracy to the overall MOT performance, using the\nnew large-scale University at Albany DETection and tRACking (UA-DETRAC)\nbenchmark dataset. The UA-DETRAC benchmark dataset consists of 100 challenging\nvideo sequences captured from real-world traffic scenes (over 140,000 frames\nwith rich annotations, including occlusion, weather, vehicle category,\ntruncation, and vehicle bounding boxes) for object detection, object tracking\nand MOT system. We evaluate complete MOT systems constructed from combinations\nof state-of-the-art object detection and object tracking methods. Our analysis\nshows the complex effects of object detection accuracy on MOT system\nperformance. Based on these observations, we propose new evaluation tools and\nmetrics for MOT systems that consider both object detection and object tracking\nfor comprehensive analysis.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 01:37:39 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 15:08:23 GMT"}, {"version": "v3", "created": "Sun, 4 Sep 2016 01:50:10 GMT"}, {"version": "v4", "created": "Fri, 24 Jan 2020 22:57:15 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Wen", "Longyin", ""], ["Du", "Dawei", ""], ["Cai", "Zhaowei", ""], ["Lei", "Zhen", ""], ["Chang", "Ming-Ching", ""], ["Qi", "Honggang", ""], ["Lim", "Jongwoo", ""], ["Yang", "Ming-Hsuan", ""], ["Lyu", "Siwei", ""]]}, {"id": "1511.04150", "submitter": "Danica J. Sutherland", "authors": "Junier B. Oliva, Danica J. Sutherland, Barnab\\'as P\\'oczos, Jeff\n  Schneider", "title": "Deep Mean Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of distributions and high-level features from deep architecture has\nbecome commonplace in modern computer vision. Both of these methodologies have\nseparately achieved a great deal of success in many computer vision tasks.\nHowever, there has been little work attempting to leverage the power of these\nto methodologies jointly. To this end, this paper presents the Deep Mean Maps\n(DMMs) framework, a novel family of methods to non-parametrically represent\ndistributions of features in convolutional neural network models.\n  DMMs are able to both classify images using the distribution of top-level\nfeatures, and to tune the top-level features for performing this task. We show\nhow to implement DMMs using a special mean map layer composed of typical CNN\noperations, making both forward and backward propagation simple.\n  We illustrate the efficacy of DMMs at analyzing distributional patterns in\nimage data in a synthetic data experiment. We also show that we extending\nexisting deep architectures with DMMs improves the performance of existing CNNs\non several challenging real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 03:36:51 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 06:24:14 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Oliva", "Junier B.", ""], ["Sutherland", "Danica J.", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Schneider", "Jeff", ""]]}, {"id": "1511.04153", "submitter": "Yaoyi Li", "authors": "Yaoyi Li, Junxuan Chen and Hongtao Lu", "title": "Adaptive Affinity Matrix for Unsupervised Metric Learning", "comments": null, "journal-ref": null, "doi": "10.1109/ICME.2016.7552887", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is one of the most popular clustering approaches with the\ncapability to handle some challenging clustering problems. Most spectral\nclustering methods provide a nonlinear map from the data manifold to a\nsubspace. Only a little work focuses on the explicit linear map which can be\nviewed as the unsupervised distance metric learning. In practice, the selection\nof the affinity matrix exhibits a tremendous impact on the unsupervised\nlearning. While much success of affinity learning has been achieved in recent\nyears, some issues such as noise reduction remain to be addressed. In this\npaper, we propose a novel method, dubbed Adaptive Affinity Matrix (AdaAM), to\nlearn an adaptive affinity matrix and derive a distance metric from the\naffinity. We assume the affinity matrix to be positive semidefinite with\nability to quantify the pairwise dissimilarity. Our method is based on posing\nthe optimization of objective function as a spectral decomposition problem. We\nyield the affinity from both the original data distribution and the widely-used\nheat kernel. The provided matrix can be regarded as the optimal representation\nof pairwise relationship on the manifold. Extensive experiments on a number of\nreal-world data sets show the effectiveness and efficiency of AdaAM.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 03:59:14 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 13:58:06 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Li", "Yaoyi", ""], ["Chen", "Junxuan", ""], ["Lu", "Hongtao", ""]]}, {"id": "1511.04164", "submitter": "Ronghang Hu", "authors": "Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko,\n  Trevor Darrell", "title": "Natural Language Object Retrieval", "comments": "Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the task of natural language object retrieval, to\nlocalize a target object within a given image based on a natural language query\nof the object. Natural language object retrieval differs from text-based image\nretrieval task as it involves spatial information about objects within the\nscene and global scene context. To address this issue, we propose a novel\nSpatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate\nboxes for object retrieval, integrating spatial configurations and global\nscene-level contextual information into the network. Our model processes query\ntext, local image descriptors, spatial configurations and global context\nfeatures through a recurrent network, outputs the probability of the query text\nconditioned on each candidate box as a score for the box, and can transfer\nvisual-linguistic knowledge from image captioning domain to our task.\nExperimental results demonstrate that our method effectively utilizes both\nlocal and global information, outperforming previous baseline methods\nsignificantly on different datasets and scenarios, and can exploit large scale\nvision and language datasets for knowledge transfer.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 05:53:37 GMT"}, {"version": "v2", "created": "Fri, 11 Mar 2016 20:12:44 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2016 03:36:58 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Hu", "Ronghang", ""], ["Xu", "Huazhe", ""], ["Rohrbach", "Marcus", ""], ["Feng", "Jiashi", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""]]}, {"id": "1511.04166", "submitter": "Yin Li", "authors": "Yin Li and Manohar Paluri and James M. Rehg and Piotr Doll\\'ar", "title": "Unsupervised Learning of Edges", "comments": "Camera ready version for CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven approaches for edge detection have proven effective and achieve\ntop results on modern benchmarks. However, all current data-driven edge\ndetectors require manual supervision for training in the form of hand-labeled\nregion segments or object boundaries. Specifically, human annotators mark\nsemantically meaningful edges which are subsequently used for training. Is this\nform of strong, high-level supervision actually necessary to learn to\naccurately detect edges? In this work we present a simple yet effective\napproach for training edge detectors without human supervision. To this end we\nutilize motion, and more specifically, the only input to our method is noisy\nsemi-dense matches between frames. We begin with only a rudimentary knowledge\nof edges (in the form of image gradients), and alternate between improving\nmotion estimation and edge detection in turn. Using a large corpus of video\ndata, we show that edge detectors trained using our unsupervised scheme\napproach the performance of the same methods trained with full supervision\n(within 3-5%). Finally, we show that when using a deep network for the edge\ndetector, our approach provides a novel pre-training scheme for object\ndetection.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 06:09:00 GMT"}, {"version": "v2", "created": "Sun, 10 Apr 2016 21:55:43 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Li", "Yin", ""], ["Paluri", "Manohar", ""], ["Rehg", "James M.", ""], ["Doll\u00e1r", "Piotr", ""]]}, {"id": "1511.04176", "submitter": "Devendra Kumar Sahu", "authors": "Devendra Kumar Sahu and Mohak Sukhwani", "title": "Sequence to Sequence Learning for Optical Character Recognition", "comments": "9 pages (including reference), 6 figures (including subfigures), 5\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end recurrent encoder-decoder based sequence learning\napproach for printed text Optical Character Recognition (OCR). In contrast to\npresent day existing state-of-art OCR solution which uses connectionist\ntemporal classification (CTC) output layer, our approach makes minimalistic\nassumptions on the structure and length of the sequence. We use a two step\nencoder-decoder approach -- (a) A recurrent encoder reads a variable length\nprinted text word image and encodes it to a fixed dimensional embedding. (b)\nThis fixed dimensional embedding is subsequently comprehended by decoder\nstructure which converts it into a variable length text output. Our\narchitecture gives competitive performance relative to connectionist temporal\nclassification (CTC) output layer while being executed in more natural\nsettings. The learnt deep word image embedding from encoder can be used for\nprinted text based retrieval systems. The expressive fixed dimensional\nembedding for any variable length input expedites the task of retrieval and\nmakes it more efficient which is not possible with other recurrent neural\nnetwork architectures. We empirically investigate the expressiveness and the\nlearnability of long short term memory (LSTMs) in the sequence to sequence\nlearning regime by training our network for prediction tasks in segmentation\nfree printed text OCR. The utility of the proposed architecture for printed\ntext is demonstrated by quantitative and qualitative evaluation of two tasks --\nword prediction and retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 06:33:22 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2015 13:55:02 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Sahu", "Devendra Kumar", ""], ["Sukhwani", "Mohak", ""]]}, {"id": "1511.04192", "submitter": "Tianshui Chen", "authors": "Tianshui Chen, Liang Lin, Lingbo Liu, Xiaonan Luo, Xuelong Li", "title": "DISC: Deep Image Saliency Computing via Progressive Representation\n  Learning", "comments": "This manuscript is the accepted version for IEEE Transactions on\n  Neural Networks and Learning Systems (T-NNLS), 2015", "journal-ref": null, "doi": "10.1109/TNNLS.2015.2506664", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection increasingly receives attention as an important\ncomponent or step in several pattern recognition and image processing tasks.\nAlthough a variety of powerful saliency models have been intensively proposed,\nthey usually involve heavy feature (or model) engineering based on priors (or\nassumptions) about the properties of objects and backgrounds. Inspired by the\neffectiveness of recently developed feature learning, we provide a novel Deep\nImage Saliency Computing (DISC) framework for fine-grained image saliency\ncomputing. In particular, we model the image saliency from both the coarse- and\nfine-level observations, and utilize the deep convolutional neural network\n(CNN) to learn the saliency representation in a progressive manner.\nSpecifically, our saliency model is built upon two stacked CNNs. The first CNN\ngenerates a coarse-level saliency map by taking the overall image as the input,\nroughly identifying saliency regions in the global context. Furthermore, we\nintegrate superpixel-based local context information in the first CNN to refine\nthe coarse-level saliency map. Guided by the coarse saliency map, the second\nCNN focuses on the local context to produce fine-grained and accurate saliency\nmap while preserving object details. For a testing image, the two CNNs\ncollaboratively conduct the saliency computing in one shot. Our DISC framework\nis capable of uniformly highlighting the objects-of-interest from complex\nbackground while preserving well object details. Extensive experiments on\nseveral standard benchmarks suggest that DISC outperforms other\nstate-of-the-art methods and it also generalizes well across datasets without\nadditional training. The executable version of DISC is available online:\nhttp://vision.sysu.edu.cn/projects/DISC.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 07:14:13 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 13:11:23 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Chen", "Tianshui", ""], ["Lin", "Liang", ""], ["Liu", "Lingbo", ""], ["Luo", "Xiaonan", ""], ["Li", "Xuelong", ""]]}, {"id": "1511.04196", "submitter": "Zhiwei Deng", "authors": "Zhiwei Deng, Arash Vahdat, Hexiang Hu and Greg Mori", "title": "Structure Inference Machines: Recurrent Neural Networks for Analyzing\n  Relations in Group Activity Recognition", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rich semantic relations are important in a variety of visual recognition\nproblems. As a concrete example, group activity recognition involves the\ninteractions and relative spatial relations of a set of people in a scene.\nState of the art recognition methods center on deep learning approaches for\ntraining highly effective, complex classifiers for interpreting images.\nHowever, bridging the relatively low-level concepts output by these methods to\ninterpret higher-level compositional scenes remains a challenge. Graphical\nmodels are a standard tool for this task. In this paper, we propose a method to\nintegrate graphical models and deep neural networks into a joint framework.\nInstead of using a traditional inference method, we use a sequential inference\nmodeled by a recurrent neural network. Beyond this, the appropriate structure\nfor inference can be learned by imposing gates on edges between nodes.\nEmpirical results on group activity recognition demonstrate the potential of\nthis model to handle highly structured learning tasks.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 07:58:38 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 17:28:18 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Deng", "Zhiwei", ""], ["Vahdat", "Arash", ""], ["Hu", "Hexiang", ""], ["Mori", "Greg", ""]]}, {"id": "1511.04240", "submitter": "Dylan Campbell", "authors": "Dylan Campbell, Lars Petersson", "title": "An Adaptive Data Representation for Robust Point-Set Registration and\n  Merging", "comments": "Manuscript in press 2015 IEEE International Conference on Computer\n  Vision", "journal-ref": null, "doi": "10.1109/ICCV.2015.488", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework for rigid point-set registration and merging\nusing a robust continuous data representation. Our point-set representation is\nconstructed by training a one-class support vector machine with a Gaussian\nradial basis function kernel and subsequently approximating the output function\nwith a Gaussian mixture model. We leverage the representation's sparse\nparametrisation and robustness to noise, outliers and occlusions in an\nefficient registration algorithm that minimises the L2 distance between our\nsupport vector--parametrised Gaussian mixtures. In contrast, existing\ntechniques, such as Iterative Closest Point and Gaussian mixture approaches,\nmanifest a narrower region of convergence and are less robust to occlusions and\nmissing data, as demonstrated in the evaluation on a range of 2D and 3D\ndatasets. Finally, we present a novel algorithm, GMMerge, that parsimoniously\nand equitably merges aligned mixture models, allowing the framework to be used\nfor reconstruction and mapping.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 11:23:40 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Campbell", "Dylan", ""], ["Petersson", "Lars", ""]]}, {"id": "1511.04242", "submitter": "Tommaso Cavallari", "authors": "Tommaso Cavallari, Luigi Di Stefano", "title": "Volume-based Semantic Labeling with Signed Distance Functions", "comments": "Submitted to PSIVT2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research works on the two topics of Semantic Segmentation and SLAM\n(Simultaneous Localization and Mapping) have been following separate tracks.\nHere, we link them quite tightly by delineating a category label fusion\ntechnique that allows for embedding semantic information into the dense map\ncreated by a volume-based SLAM algorithm such as KinectFusion. Accordingly, our\napproach is the first to provide a semantically labeled dense reconstruction of\nthe environment from a stream of RGB-D images. We validate our proposal using a\npublicly available semantically annotated RGB-D dataset and a) employing ground\ntruth labels, b) corrupting such annotations with synthetic noise, c) deploying\na state of the art semantic segmentation algorithm based on Convolutional\nNeural Networks.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 11:25:50 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Cavallari", "Tommaso", ""], ["Di Stefano", "Luigi", ""]]}, {"id": "1511.04273", "submitter": "Kwang Yi", "authors": "Kwang Moo Yi, Yannick Verdie, Pascal Fua, Vincent Lepetit", "title": "Learning to Assign Orientations to Feature Points", "comments": "Accepted as Oral presentation in Computer Vision and Pattern\n  Recognition, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to train a Convolutional Neural Network to assign a canonical\norientation to feature points given an image patch centered on the feature\npoint. Our method improves feature point matching upon the state-of-the art and\ncan be used in conjunction with any existing rotation sensitive descriptors. To\navoid the tedious and almost impossible task of finding a target orientation to\nlearn, we propose to use Siamese networks which implicitly find the optimal\norientations during training. We also propose a new type of activation function\nfor Neural Networks that generalizes the popular ReLU, maxout, and PReLU\nactivation functions. This novel activation performs better for our task. We\nvalidate the effectiveness of our method extensively with four existing\ndatasets, including two non-planar datasets, as well as our own dataset. We\nshow that we outperform the state-of-the-art without the need of retraining for\neach dataset.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 13:23:09 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 14:03:54 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Yi", "Kwang Moo", ""], ["Verdie", "Yannick", ""], ["Fua", "Pascal", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1511.04320", "submitter": "Manuel Chica", "authors": "Manuel Chica and Pascual Campoy", "title": "Standard methods for inexpensive pollen loads authentication by means of\n  computer vision and machine learning", "comments": "24 pages. Book chapter to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a complete methodology for authenticating local bee pollen against\nfraudulent samples using image processing and machine learning techniques. The\nproposed standard methods do not need expensive equipment such as advanced\nmicroscopes and can be used for a preliminary fast rejection of unknown pollen\ntypes. The system is able to rapidly reject the non-local pollen samples with\ninexpensive hardware and without the need to send the product to the\nlaboratory. Methods are based on the color properties of bee pollen loads\nimages and the use of one-class classifiers which are appropriate to reject\nunknown pollen samples when there is limited data about them. The validation of\nthe method is carried out by authenticating Spanish bee pollen types.\nExperimentation shows that the proposed methods can obtain an overall\nauthentication accuracy of 94%. We finally illustrate the user interaction with\nthe software in some practical cases by showing the developed application\nprototype.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 15:35:22 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Chica", "Manuel", ""], ["Campoy", "Pascual", ""]]}, {"id": "1511.04377", "submitter": "Adam Harley", "authors": "Adam W. Harley, Konstantinos G. Derpanis, and Iasonas Kokkinos", "title": "Learning Dense Convolutional Embeddings for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new deep convolutional neural network (DCNN)\narchitecture that learns pixel embeddings, such that pairwise distances between\nthe embeddings can be used to infer whether or not the pixels lie on the same\nregion. That is, for any two pixels on the same object, the embeddings are\ntrained to be similar; for any pair that straddles an object boundary, the\nembeddings are trained to be dissimilar. Experimental results show that when\nthis embedding network is used in conjunction with a DCNN trained on semantic\nsegmentation, there is a systematic improvement in per-pixel classification\naccuracy. Our contributions are integrated in the popular Caffe deep learning\nframework, and consist in straightforward modifications to convolution\nroutines. As such, they can be exploited for any task involving convolution\nlayers.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 17:32:11 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 20:36:37 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 01:16:52 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Harley", "Adam W.", ""], ["Derpanis", "Konstantinos G.", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "1511.04384", "submitter": "Konstantinos Rematas", "authors": "Konstantinos Rematas, Tobias Ritschel, Mario Fritz, Efstratios Gavves,\n  Tinne Tuytelaars", "title": "Deep Reflectance Maps", "comments": "project page: http://homes.esat.kuleuven.be/~krematas/DRM/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undoing the image formation process and therefore decomposing appearance into\nits intrinsic properties is a challenging task due to the under-constraint\nnature of this inverse problem. While significant progress has been made on\ninferring shape, materials and illumination from images only, progress in an\nunconstrained setting is still limited. We propose a convolutional neural\narchitecture to estimate reflectance maps of specular materials in natural\nlighting conditions. We achieve this in an end-to-end learning formulation that\ndirectly predicts a reflectance map from the image itself. We show how to\nimprove estimates by facilitating additional supervision in an indirect scheme\nthat first predicts surface orientation and afterwards predicts the reflectance\nmap by a learning-based sparse data interpolation.\n  In order to analyze performance on this difficult task, we propose a new\nchallenge of Specular MAterials on SHapes with complex IllumiNation (SMASHINg)\nusing both synthetic and real images. Furthermore, we show the application of\nour method to a range of image-based editing tasks on real images.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 18:06:32 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Rematas", "Konstantinos", ""], ["Ritschel", "Tobias", ""], ["Fritz", "Mario", ""], ["Gavves", "Efstratios", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1511.04397", "submitter": "Ehsan Hosseini-Asl", "authors": "Ehsan Hosseini-Asl, Angshuman Guha", "title": "Similarity-based Text Recognition by Deeply Supervised Siamese Network", "comments": "Accepted for presenting at Future Technologies Conference - (FTC\n  2016) San Francisco, December 6-7, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new text recognition model based on measuring the\nvisual similarity of text and predicting the content of unlabeled texts. First\na Siamese convolutional network is trained with deep supervision on a labeled\ntraining dataset. This network projects texts into a similarity manifold. The\nDeeply Supervised Siamese network learns visual similarity of texts. Then a\nK-nearest neighbor classifier is used to predict unlabeled text based on\nsimilarity distance to labeled texts. The performance of the model is evaluated\non three datasets of machine-print and hand-written text combined. We\ndemonstrate that the model reduces the cost of human estimation by $50\\%-85\\%$.\nThe error of the system is less than $0.5\\%$. The proposed model outperform\nconventional Siamese network by finding visually-similar barely-readable and\nreadable text, e.g. machine-printed, handwritten, due to deep supervision. The\nresults also demonstrate that the predicted labels are sometimes better than\nhuman labels e.g. spelling correction.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 18:46:01 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 20:59:10 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 00:37:29 GMT"}, {"version": "v4", "created": "Sun, 3 Jul 2016 16:38:35 GMT"}, {"version": "v5", "created": "Tue, 5 Jul 2016 01:21:08 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Hosseini-Asl", "Ehsan", ""], ["Guha", "Angshuman", ""]]}, {"id": "1511.04401", "submitter": "Federico Raue", "authors": "Federico Raue, Andreas Dengel, Thomas M. Breuel, Marcus Liwicki", "title": "Symbol Grounding Association in Multimodal Sequences with Missing\n  Elements", "comments": "Under review on Journal of Artificial Intelligence Research (JAIR) --\n  Special Track on Deep Learning, Knowledge Representation, and Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend a symbolic association framework for being able to\nhandle missing elements in multimodal sequences. The general scope of the work\nis the symbolic associations of object-word mappings as it happens in language\ndevelopment in infants. In other words, two different representations of the\nsame abstract concepts can associate in both directions. This scenario has been\nlong interested in Artificial Intelligence, Psychology, and Neuroscience. In\nthis work, we extend a recent approach for multimodal sequences (visual and\naudio) to also cope with missing elements in one or both modalities. Our method\nuses two parallel Long Short-Term Memories (LSTMs) with a learning rule based\non EM-algorithm. It aligns both LSTM outputs via Dynamic Time Warping (DTW). We\npropose to include an extra step for the combination with the max operation for\nexploiting the common elements between both sequences. The motivation behind is\nthat the combination acts as a condition selector for choosing the best\nrepresentation from both LSTMs. We evaluated the proposed extension in the\nfollowing scenarios: missing elements in one modality (visual or audio) and\nmissing elements in both modalities (visual and sound). The performance of our\nextension reaches better results than the original model and similar results to\nindividual LSTM trained in each modality.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 18:59:36 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 15:59:02 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 11:36:59 GMT"}, {"version": "v4", "created": "Fri, 16 Dec 2016 14:17:02 GMT"}, {"version": "v5", "created": "Thu, 7 Dec 2017 10:14:23 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Raue", "Federico", ""], ["Dengel", "Andreas", ""], ["Breuel", "Thomas M.", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1511.04404", "submitter": "Oncel Tuzel", "authors": "Oncel Tuzel and Tim K. Marks and Salil Tambe", "title": "Robust Face Alignment Using a Mixture of Invariant Experts", "comments": "17 pages, 6 figures", "journal-ref": "Proceedings of 14th European Conference on Computer Vision (ECCV),\n  Amsterdam, The Netherlands, October 11-14, 2016, pp 825-841", "doi": "10.1007/978-3-319-46454-1_50", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face alignment, which is the task of finding the locations of a set of facial\nlandmark points in an image of a face, is useful in widespread application\nareas. Face alignment is particularly challenging when there are large\nvariations in pose (in-plane and out-of-plane rotations) and facial expression.\nTo address this issue, we propose a cascade in which each stage consists of a\nmixture of regression experts. Each expert learns a customized regression model\nthat is specialized to a different subset of the joint space of pose and\nexpressions. The system is invariant to a predefined class of transformations\n(e.g., affine), because the input is transformed to match each expert's\nprototype shape before the regression is applied. We also present a method to\ninclude deformation constraints within the discriminative alignment framework,\nwhich makes our algorithm more robust. Our algorithm significantly outperforms\nprevious methods on publicly available face alignment datasets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 19:14:51 GMT"}, {"version": "v2", "created": "Sun, 23 Oct 2016 18:31:06 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Tuzel", "Oncel", ""], ["Marks", "Tim K.", ""], ["Tambe", "Salil", ""]]}, {"id": "1511.04458", "submitter": "Xun Xu", "authors": "Xun Xu, Timothy Hospedales, Shaogang Gong", "title": "Transductive Zero-Shot Action Recognition by Word-Vector Embedding", "comments": "Accepted by IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of categories for action recognition is growing rapidly and it has\nbecome increasingly hard to label sufficient training data for learning\nconventional models for all categories. Instead of collecting ever more data\nand labelling them exhaustively for all categories, an attractive alternative\napproach is zero-shot learning\" (ZSL). To that end, in this study we construct\na mapping between visual features and a semantic descriptor of each action\ncategory, allowing new categories to be recognised in the absence of any visual\ntraining data. Existing ZSL studies focus primarily on still images, and\nattribute-based semantic representations. In this work, we explore word-vectors\nas the shared semantic space to embed videos and category labels for ZSL action\nrecognition. This is a more challenging problem than existing ZSL of still\nimages and/or attributes, because the mapping between video spacetime features\nof actions and the semantic space is more complex and harder to learn for the\npurpose of generalising over any cross-category domain shift. To solve this\ngeneralisation problem in ZSL action recognition, we investigate a series of\nsynergistic strategies to improve upon the standard ZSL pipeline. Most of these\nstrategies are transductive in nature which means access to testing data in the\ntraining phase.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 21:05:20 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 07:17:03 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Xu", "Xun", ""], ["Hospedales", "Timothy", ""], ["Gong", "Shaogang", ""]]}, {"id": "1511.04472", "submitter": "Rui Yu", "authors": "Rui Yu, Chris Russell, Lourdes Agapito", "title": "Solving Jigsaw Puzzles with Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Linear Program (LP) based formula- tion for solving jigsaw\npuzzles. We formulate jigsaw solving as a set of successive global convex\nrelaxations of the stan- dard NP-hard formulation, that can describe both\njigsaws with pieces of unknown position and puzzles of unknown po- sition and\norientation. The main contribution and strength of our approach comes from the\nLP assembly strategy. In contrast to existing greedy methods, our LP solver\nexploits all the pairwise matches simultaneously, and computes the position of\neach piece/component globally. The main ad- vantages of our LP approach\ninclude: (i) a reduced sensi- tivity to local minima compared to greedy\napproaches, since our successive approximations are global and convex and (ii)\nan increased robustness to the presence of mismatches in the pairwise matches\ndue to the use of a weighted L1 penalty. To demonstrate the effectiveness of\nour approach, we test our algorithm on public jigsaw datasets and show that it\noutperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 22:15:54 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Yu", "Rui", ""], ["Russell", "Chris", ""], ["Agapito", "Lourdes", ""]]}, {"id": "1511.04491", "submitter": "Jiwon Kim", "authors": "Jiwon Kim, Jung Kwon Lee and Kyoung Mu Lee", "title": "Deeply-Recursive Convolutional Network for Image Super-Resolution", "comments": "CVPR 2016 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an image super-resolution method (SR) using a deeply-recursive\nconvolutional network (DRCN). Our network has a very deep recursive layer (up\nto 16 recursions). Increasing recursion depth can improve performance without\nintroducing new parameters for additional convolutions. Albeit advantages,\nlearning a DRCN is very hard with a standard gradient descent method due to\nexploding/vanishing gradients. To ease the difficulty of training, we propose\ntwo extensions: recursive-supervision and skip-connection. Our method\noutperforms previous methods by a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 02:21:50 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 08:40:53 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Kim", "Jiwon", ""], ["Lee", "Jung Kwon", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1511.04510", "submitter": "Xiaodan Liang", "authors": "Xiaodan Liang and Xiaohui Shen and Donglai Xiang and Jiashi Feng and\n  Liang Lin and Shuicheng Yan", "title": "Semantic Object Parsing with Local-Global Long Short-Term Memory", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic object parsing is a fundamental task for understanding objects in\ndetail in computer vision community, where incorporating multi-level contextual\ninformation is critical for achieving such fine-grained pixel-level\nrecognition. Prior methods often leverage the contextual information through\npost-processing predicted confidence maps. In this work, we propose a novel\ndeep Local-Global Long Short-Term Memory (LG-LSTM) architecture to seamlessly\nincorporate short-distance and long-distance spatial dependencies into the\nfeature learning over all pixel positions. In each LG-LSTM layer, local\nguidance from neighboring positions and global guidance from the whole image\nare imposed on each position to better exploit complex local and global\ncontextual information. Individual LSTMs for distinct spatial dimensions are\nalso utilized to intrinsically capture various spatial layouts of semantic\nparts in the images, yielding distinct hidden and memory cells of each position\nfor each dimension. In our parsing approach, several LG-LSTM layers are stacked\nand appended to the intermediate convolutional layers to directly enhance\nvisual features, allowing network parameters to be learned in an end-to-end\nway. The long chains of sequential computation by stacked LG-LSTM layers also\nenable each pixel to sense a much larger region for inference benefiting from\nthe memorization of previous dependencies in all positions along all\ndimensions. Comprehensive evaluations on three public datasets well demonstrate\nthe significant superiority of our LG-LSTM over other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 05:42:50 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Liang", "Xiaodan", ""], ["Shen", "Xiaohui", ""], ["Xiang", "Donglai", ""], ["Feng", "Jiashi", ""], ["Lin", "Liang", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1511.04511", "submitter": "Ziming Zhang", "authors": "Ziming Zhang, Yun Liu, Xi Chen, Yanjun Zhu, Ming-Ming Cheng, Venkatesh\n  Saligrama, and Philip H.S. Torr", "title": "Sequential Optimization for Efficient High-Quality Object Proposal\n  Generation", "comments": "Accepted by TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are motivated by the need for a generic object proposal generation\nalgorithm which achieves good balance between object detection recall, proposal\nlocalization quality and computational efficiency. We propose a novel object\nproposal algorithm, BING++, which inherits the virtue of good computational\nefficiency of BING but significantly improves its proposal localization\nquality. At high level we formulate the problem of object proposal generation\nfrom a novel probabilistic perspective, based on which our BING++ manages to\nimprove the localization quality by employing edges and segments to estimate\nobject boundaries and update the proposals sequentially. We propose learning\nthe parameters efficiently by searching for approximate solutions in a\nquantized parameter space for complexity reduction. We demonstrate the\ngeneralization of BING++ with the same fixed parameters across different object\nclasses and datasets. Empirically our BING++ can run at half speed of BING on\nCPU, but significantly improve the localization quality by 18.5% and 16.7% on\nboth VOC2007 and Microhsoft COCO datasets, respectively. Compared with other\nstate-of-the-art approaches, BING++ can achieve comparable performance, but run\nsignificantly faster.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 05:45:47 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 04:35:15 GMT"}, {"version": "v3", "created": "Mon, 22 May 2017 17:23:07 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Zhang", "Ziming", ""], ["Liu", "Yun", ""], ["Chen", "Xi", ""], ["Zhu", "Yanjun", ""], ["Cheng", "Ming-Ming", ""], ["Saligrama", "Venkatesh", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1511.04512", "submitter": "Ziming Zhang", "authors": "Ziming Zhang and Venkatesh Saligrama", "title": "Zero-Shot Learning via Joint Latent Similarity Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot recognition (ZSR) deals with the problem of predicting class labels\nfor target domain instances based on source domain side information (e.g.\nattributes) of unseen classes. We formulate ZSR as a binary prediction problem.\nOur resulting classifier is class-independent. It takes an arbitrary pair of\nsource and target domain instances as input and predicts whether or not they\ncome from the same class, i.e. whether there is a match. We model the posterior\nprobability of a match since it is a sufficient statistic and propose a latent\nprobabilistic model in this context. We develop a joint discriminative learning\nframework based on dictionary learning to jointly learn the parameters of our\nmodel for both domains, which ultimately leads to our class-independent\nclassifier. Many of the existing embedding methods can be viewed as special\ncases of our probabilistic model. On ZSR our method shows 4.90\\% improvement\nover the state-of-the-art in accuracy averaged across four benchmark datasets.\nWe also adapt ZSR method for zero-shot retrieval and show 22.45\\% improvement\naccordingly in mean average precision (mAP).\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 05:53:30 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 22:14:15 GMT"}, {"version": "v3", "created": "Wed, 17 Aug 2016 16:29:51 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Zhang", "Ziming", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1511.04517", "submitter": "Xiaodan Liang", "authors": "Xiaodan Liang and Yunchao Wei and Xiaohui Shen and Zequn Jie and\n  Jiashi Feng and Liang Lin and Shuicheng Yan", "title": "Reversible Recursive Instance-level Object Segmentation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel Reversible Recursive Instance-level Object\nSegmentation (R2-IOS) framework to address the challenging instance-level\nobject segmentation task. R2-IOS consists of a reversible proposal refinement\nsub-network that predicts bounding box offsets for refining the object proposal\nlocations, and an instance-level segmentation sub-network that generates the\nforeground mask of the dominant object instance in each proposal. By being\nrecursive, R2-IOS iteratively optimizes the two sub-networks during joint\ntraining, in which the refined object proposals and improved segmentation\npredictions are alternately fed into each other to progressively increase the\nnetwork capabilities. By being reversible, the proposal refinement sub-network\nadaptively determines an optimal number of refinement iterations required for\neach proposal during both training and testing. Furthermore, to handle multiple\noverlapped instances within a proposal, an instance-aware denoising autoencoder\nis introduced into the segmentation sub-network to distinguish the dominant\nobject from other distracting instances. Extensive experiments on the\nchallenging PASCAL VOC 2012 benchmark well demonstrate the superiority of\nR2-IOS over other state-of-the-art methods. In particular, the $\\text{AP}^r$\nover $20$ classes at $0.5$ IoU achieves $66.7\\%$, which significantly\noutperforms the results of $58.7\\%$ by PFN~\\cite{PFN} and $46.3\\%$\nby~\\cite{liu2015multi}.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 06:10:41 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 06:49:18 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Liang", "Xiaodan", ""], ["Wei", "Yunchao", ""], ["Shen", "Xiaohui", ""], ["Jie", "Zequn", ""], ["Feng", "Jiashi", ""], ["Lin", "Liang", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1511.04524", "submitter": "Ziming Zhang", "authors": "Ziming Zhang, Yuting Chen and Venkatesh Saligrama", "title": "Efficient Training of Very Deep Neural Networks for Supervised Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose training very deep neural networks (DNNs) for\nsupervised learning of hash codes. Existing methods in this context train\nrelatively \"shallow\" networks limited by the issues arising in back propagation\n(e.e. vanishing gradients) as well as computational efficiency. We propose a\nnovel and efficient training algorithm inspired by alternating direction method\nof multipliers (ADMM) that overcomes some of these limitations. Our method\ndecomposes the training process into independent layer-wise local updates\nthrough auxiliary variables. Empirically we observe that our training algorithm\nalways converges and its computational complexity is linearly proportional to\nthe number of edges in the networks. Empirically we manage to train DNNs with\n64 hidden layers and 1024 nodes per layer for supervised hashing in about 3\nhours using a single GPU. Our proposed very deep supervised hashing (VDSH)\nmethod significantly outperforms the state-of-the-art on several benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 07:35:01 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 21:49:21 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Zhang", "Ziming", ""], ["Chen", "Yuting", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1511.04534", "submitter": "Zhenhua Wang", "authors": "Zhenhua Wang, Xingxing Wang, Gang Wang", "title": "Learning Fine-grained Features via a CNN Tree for Large-scale\n  Classification", "comments": "Neurocomputing 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a novel approach to enhance the discriminability of Convolutional\nNeural Networks (CNN). The key idea is to build a tree structure that could\nprogressively learn fine-grained features to distinguish a subset of classes,\nby learning features only among these classes. Such features are expected to be\nmore discriminative, compared to features learned for all the classes. We\ndevelop a new algorithm to effectively learn the tree structure from a large\nnumber of classes. Experiments on large-scale image classification tasks\ndemonstrate that our method could boost the performance of a given basic CNN\nmodel. Our method is quite general, hence it can potentially be used in\ncombination with many other deep learning models.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 09:19:44 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 07:59:12 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Wang", "Zhenhua", ""], ["Wang", "Xingxing", ""], ["Wang", "Gang", ""]]}, {"id": "1511.04587", "submitter": "Jiwon Kim", "authors": "Jiwon Kim, Jung Kwon Lee and Kyoung Mu Lee", "title": "Accurate Image Super-Resolution Using Very Deep Convolutional Networks", "comments": "CVPR 2016 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a highly accurate single-image super-resolution (SR) method. Our\nmethod uses a very deep convolutional network inspired by VGG-net used for\nImageNet classification \\cite{simonyan2015very}. We find increasing our network\ndepth shows a significant improvement in accuracy. Our final model uses 20\nweight layers. By cascading small filters many times in a deep network\nstructure, contextual information over large image regions is exploited in an\nefficient way. With very deep networks, however, convergence speed becomes a\ncritical issue during training. We propose a simple yet effective training\nprocedure. We learn residuals only and use extremely high learning rates\n($10^4$ times higher than SRCNN \\cite{dong2015image}) enabled by adjustable\ngradient clipping. Our proposed method performs better than existing methods in\naccuracy and visual improvements in our results are easily noticeable.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 17:36:45 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 08:40:47 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Kim", "Jiwon", ""], ["Lee", "Jung Kwon", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1511.04590", "submitter": "Li Yao", "authors": "Li Yao, Nicolas Ballas, Kyunghyun Cho, John R. Smith, Yoshua Bengio", "title": "Oracle performance for visual captioning", "comments": "BMVC2016 (Oral paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of associating images and videos with a natural language description\nhas attracted a great amount of attention recently. Rapid progress has been\nmade in terms of both developing novel algorithms and releasing new datasets.\nIndeed, the state-of-the-art results on some of the standard datasets have been\npushed into the regime where it has become more and more difficult to make\nsignificant improvements. Instead of proposing new models, this work\ninvestigates the possibility of empirically establishing performance upper\nbounds on various visual captioning datasets without extra data labelling\neffort or human evaluation. In particular, it is assumed that visual captioning\nis decomposed into two steps: from visual inputs to visual concepts, and from\nvisual concepts to natural language descriptions. One would be able to obtain\nan upper bound when assuming the first step is perfect and only requiring\ntraining a conditional language model for the second step. We demonstrate the\nconstruction of such bounds on MS-COCO, YouTube2Text and LSMDC (a combination\nof M-VAD and MPII-MD). Surprisingly, despite of the imperfect process we used\nfor visual concept extraction in the first step and the simplicity of the\nlanguage model for the second step, we show that current state-of-the-art\nmodels fall short when being compared with the learned upper bounds.\nFurthermore, with such a bound, we quantify several important factors\nconcerning image and video captioning: the number of visual concepts captured\nby different models, the trade-off between the amount of visual elements\ncaptured and their accuracy, and the intrinsic difficulty and blessing of\ndifferent datasets.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 18:02:39 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 04:20:08 GMT"}, {"version": "v3", "created": "Sun, 3 Jan 2016 04:55:57 GMT"}, {"version": "v4", "created": "Wed, 6 Jan 2016 23:38:25 GMT"}, {"version": "v5", "created": "Wed, 14 Sep 2016 16:55:29 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Yao", "Li", ""], ["Ballas", "Nicolas", ""], ["Cho", "Kyunghyun", ""], ["Smith", "John R.", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1511.04599", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard", "title": "DeepFool: a simple and accurate method to fool deep neural networks", "comments": "In Proceedings of IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep neural networks have achieved impressive results on\nmany image classification tasks. However, these same architectures have been\nshown to be unstable to small, well sought, perturbations of the images.\nDespite the importance of this phenomenon, no effective methods have been\nproposed to accurately compute the robustness of state-of-the-art deep\nclassifiers to such perturbations on large-scale datasets. In this paper, we\nfill this gap and propose the DeepFool algorithm to efficiently compute\nperturbations that fool deep networks, and thus reliably quantify the\nrobustness of these classifiers. Extensive experimental results show that our\napproach outperforms recent methods in the task of computing adversarial\nperturbations and making classifiers more robust.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 18:50:00 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 09:33:23 GMT"}, {"version": "v3", "created": "Mon, 4 Jul 2016 04:49:44 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Fawzi", "Alhussein", ""], ["Frossard", "Pascal", ""]]}, {"id": "1511.04601", "submitter": "Weiyang Liu", "authors": "Weiyang Liu, Zhiding Yu, Yandong Wen, Rongmei Lin, Meng Yang", "title": "Jointly Learning Non-negative Projection and Dictionary with\n  Discriminative Graph Constraints for Classification", "comments": "To appear in BMVC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding with dictionary learning (DL) has shown excellent\nclassification performance. Despite the considerable number of existing works,\nhow to obtain features on top of which dictionaries can be better learned\nremains an open and interesting question. Many current prevailing DL methods\ndirectly adopt well-performing crafted features. While such strategy may\nempirically work well, it ignores certain intrinsic relationship between\ndictionaries and features. We propose a framework where features and\ndictionaries are jointly learned and optimized. The framework, named joint\nnon-negative projection and dictionary learning (JNPDL), enables interaction\nbetween the input features and the dictionaries. The non-negative projection\nleads to discriminative parts-based object features while DL seeks a more\nsuitable representation. Discriminative graph constraints are further imposed\nto simultaneously maximize intra-class compactness and inter-class\nseparability. Experiments on both image and image set classification show the\nexcellent performance of JNPDL by outperforming several state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 19:11:13 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 07:53:07 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2016 13:02:53 GMT"}, {"version": "v4", "created": "Fri, 5 Aug 2016 10:56:20 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Liu", "Weiyang", ""], ["Yu", "Zhiding", ""], ["Wen", "Yandong", ""], ["Lin", "Rongmei", ""], ["Yang", "Meng", ""]]}, {"id": "1511.04659", "submitter": "Shailesh Panchal Mr", "authors": "Shailesh Panchal, Rajesh Thakker", "title": "Implementation and comparative quantitative assessment of different\n  multispectral image pansharpening approches", "comments": "14 pages", "journal-ref": null, "doi": "10.5121/sipij.2015.6503", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In remote sensing, images acquired by various earth observation satellites\ntend to have either a high spatial and low spectral resolution or vice versa.\nPansharpening is a technique which aims to improve spatial resolution of\nmultispectral image. The challenges involve in the pansharpening are not only\nto improve the spatial resolution but also to preserve spectral quality of the\nmultispectral image. In this paper, various pansharpening algorithms are\ndiscussed and classified based on approaches they have adopted. Using MATLAB\nimage processing toolbox, several state-of-art pan-sharpening algorithms are\nimplemented. Quality of pansharpened images are assessed visually and\nquantitatively. Correlation coefficient (CC), Root mean square error (RMSE),\nRelative average spectral error (RASE) and Universal quality index (Q) indices\nare used to easure spectral quality while to spatial-CC (SCC) quantitative\nparameter is used for spatial quality measurement. Finally, the paper is\nconcluded with useful remarks.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 04:48:17 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Panchal", "Shailesh", ""], ["Thakker", "Rajesh", ""]]}, {"id": "1511.04668", "submitter": "Dong Ki Kim", "authors": "Dong Ki Kim, Tsuhan Chen", "title": "Deep Neural Network for Real-Time Autonomous Indoor Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous indoor navigation of Micro Aerial Vehicles (MAVs) possesses many\nchallenges. One main reason is that GPS has limited precision in indoor\nenvironments. The additional fact that MAVs are not able to carry heavy weight\nor power consuming sensors, such as range finders, makes indoor autonomous\nnavigation a challenging task. In this paper, we propose a practical system in\nwhich a quadcopter autonomously navigates indoors and finds a specific target,\ni.e., a book bag, by using a single camera. A deep learning model,\nConvolutional Neural Network (ConvNet), is used to learn a controller strategy\nthat mimics an expert pilot's choice of action. We show our system's\nperformance through real-time experiments in diverse indoor locations. To\nunderstand more about our trained network, we use several visualization\ntechniques.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 07:35:10 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 07:52:46 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Kim", "Dong Ki", ""], ["Chen", "Tsuhan", ""]]}, {"id": "1511.04670", "submitter": "Zhongwen Xu", "authors": "Linchao Zhu, Zhongwen Xu, Yi Yang, Alexander G. Hauptmann", "title": "Uncovering Temporal Context for Video Question and Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce Video Question Answering in temporal domain to\ninfer the past, describe the present and predict the future. We present an\nencoder-decoder approach using Recurrent Neural Networks to learn temporal\nstructures of videos and introduce a dual-channel ranking loss to answer\nmultiple-choice questions. We explore approaches for finer understanding of\nvideo content using question form of \"fill-in-the-blank\", and managed to\ncollect 109,895 video clips with duration over 1,000 hours from TACoS, MPII-MD,\nMEDTest 14 datasets, while the corresponding 390,744 questions are generated\nfrom annotations. Extensive experiments demonstrate that our approach\nsignificantly outperforms the compared baselines.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 07:57:41 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Zhu", "Linchao", ""], ["Xu", "Zhongwen", ""], ["Yang", "Yi", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1511.04685", "submitter": "Guy Gilboa", "authors": "Guy Gilboa", "title": "Semi-Inner-Products for Convex Functionals and Their Use in Image\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-inner-products in the sense of Lumer are extended to convex functionals.\nThis yields a Hilbert-space like structure to convex functionals in Banach\nspaces. In particular, a general expression for semi-inner-products with\nrespect to one homogeneous functionals is given. Thus one can use the new\noperator for the analysis of total variation and higher order functionals like\ntotal-generalized-variation (TGV). Having a semi-inner-product, an angle\nbetween functions can be defined in a straightforward manner. It is shown that\nin the one homogeneous case the Bregman distance can be expressed in terms of\nthis newly defined angle. In addition, properties of the semi-inner-product of\nnonlinear eigenfunctions induced by the functional are derived. We use this\nconstruction to state a sufficient condition for a perfect decomposition of two\nsignals and suggest numerical measures which indicate when those conditions are\napproximately met.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 11:13:04 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Gilboa", "Guy", ""]]}, {"id": "1511.04687", "submitter": "Guy Gilboa", "authors": "Dikla Horesh and Guy Gilboa", "title": "Separation Surfaces in the Spectral TV Domain for Texture Decomposition", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2587121", "report-no": null, "categories": "cs.CV math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel notion of separation surfaces for image\ndecomposition. A surface is embedded in the spectral total-variation (TV) three\ndimensional domain and encodes a spatially-varying separation scale. The method\nallows good separation of textures with gradually varying pattern-size,\npattern-contrast or illumination. The recently proposed total variation\nspectral framework is used to decompose the image into a continuum of textural\nscales. A desired texture, within a scale range, is found by fitting a surface\nto the local maximal responses in the spectral domain. A band above and below\nthe surface, referred to as the \\textit{Texture Stratum}, defines for each\npixel the adaptive scale-range of the texture. Based on the decomposition an\napplication is proposed which can attenuate or enhance textures in the image in\na very natural and visually convincing manner.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 11:36:30 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Horesh", "Dikla", ""], ["Gilboa", "Guy", ""]]}, {"id": "1511.04777", "submitter": "Ju Sun", "authors": "Ju Sun, Qing Qu, John Wright", "title": "Complete Dictionary Recovery over the Sphere II: Recovery by Riemannian\n  Trust-region Method", "comments": "The second of two papers based on the report arXiv:1504.06785.\n  Accepted by IEEE Transaction on Information Theory; revised according to the\n  reviewers' comments", "journal-ref": "IEEE Trans. Information Theory, 63(2): 885 - 914 (2017)", "doi": "10.1109/TIT.2016.2632149", "report-no": null, "categories": "cs.IT cs.CV math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a complete (i.e., square and\ninvertible) matrix $\\mathbf A_0$, from $\\mathbf Y \\in \\mathbb{R}^{n \\times p}$\nwith $\\mathbf Y = \\mathbf A_0 \\mathbf X_0$, provided $\\mathbf X_0$ is\nsufficiently sparse. This recovery problem is central to theoretical\nunderstanding of dictionary learning, which seeks a sparse representation for a\ncollection of input signals and finds numerous applications in modern signal\nprocessing and machine learning. We give the first efficient algorithm that\nprovably recovers $\\mathbf A_0$ when $\\mathbf X_0$ has $O(n)$ nonzeros per\ncolumn, under suitable probability model for $\\mathbf X_0$.\n  Our algorithmic pipeline centers around solving a certain nonconvex\noptimization problem with a spherical constraint, and hence is naturally\nphrased in the language of manifold optimization. In a companion paper\n(arXiv:1511.03607), we have showed that with high probability our nonconvex\nformulation has no \"spurious\" local minimizers and around any saddle point the\nobjective function has a negative directional curvature. In this paper, we take\nadvantage of the particular geometric structure, and describe a Riemannian\ntrust region algorithm that provably converges to a local minimizer with from\narbitrary initializations. Such minimizers give excellent approximations to\nrows of $\\mathbf X_0$. The rows are then recovered by linear programming\nrounding and deflation.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 23:00:29 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 03:57:40 GMT"}, {"version": "v3", "created": "Thu, 1 Sep 2016 17:27:12 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Sun", "Ju", ""], ["Qu", "Qing", ""], ["Wright", "John", ""]]}, {"id": "1511.04798", "submitter": "Boyang Li", "authors": "Baohan Xu, Yanwei Fu, Yu-Gang Jiang, Boyang Li and Leonid Sigal", "title": "Heterogeneous Knowledge Transfer in Video Emotion Recognition,\n  Attribution and Summarization", "comments": "13 pages, 11 figures. Published at the IEEE Transactions on Affective\n  Computing", "journal-ref": "IEEE Transactions on Affective Computing. 2016", "doi": "10.1109/TAFFC.2016.2622690", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion is a key element in user-generated videos. However, it is difficult\nto understand emotions conveyed in such videos due to the complex and\nunstructured nature of user-generated content and the sparsity of video frames\nexpressing emotion. In this paper, for the first time, we study the problem of\ntransferring knowledge from heterogeneous external sources, including image and\ntextual data, to facilitate three related tasks in understanding video emotion:\nemotion recognition, emotion attribution and emotion-oriented summarization.\nSpecifically, our framework (1) learns a video encoding from an auxiliary\nemotional image dataset in order to improve supervised video emotion\nrecognition, and (2) transfers knowledge from an auxiliary textual corpora for\nzero-shot recognition of emotion classes unseen during training. The proposed\ntechnique for knowledge transfer facilitates novel applications of emotion\nattribution and emotion-oriented summarization. A comprehensive set of\nexperiments on multiple datasets demonstrate the effectiveness of our\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 01:40:15 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 06:02:45 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Xu", "Baohan", ""], ["Fu", "Yanwei", ""], ["Jiang", "Yu-Gang", ""], ["Li", "Boyang", ""], ["Sigal", "Leonid", ""]]}, {"id": "1511.04808", "submitter": "Mengyi Liu", "authors": "Mengyi Liu, Ruiping Wang, Shiguang Shan, Xilin Chen", "title": "Learning Mid-level Words on Riemannian Manifold for Action Recognition", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition remains a challenging task due to the various\nsources of video data and large intra-class variations. It thus becomes one of\nthe key issues in recent research to explore effective and robust\nrepresentation to handle such challenges. In this paper, we propose a novel\nrepresentation approach by constructing mid-level words in videos and encoding\nthem on Riemannian manifold. Specifically, we first conduct a global alignment\non the densely extracted low-level features to build a bank of corresponding\nfeature groups, each of which can be statistically modeled as a mid-level word\nlying on some specific Riemannian manifold. Based on these mid-level words, we\nconstruct intrinsic Riemannian codebooks by employing K-Karcher-means\nclustering and Riemannian Gaussian Mixture Model, and consequently extend the\nRiemannian manifold version of three well studied encoding methods in Euclidean\nspace, i.e. Bag of Visual Words (BoVW), Vector of Locally Aggregated\nDescriptors (VLAD), and Fisher Vector (FV), to obtain the final action video\nrepresentations. Our method is evaluated in two tasks on four popular realistic\ndatasets: action recognition on YouTube, UCF50, HMDB51 databases, and action\nsimilarity labeling on ASLAN database. In all cases, the reported results\nachieve very competitive performance with those most recent state-of-the-art\nworks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 03:18:06 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Liu", "Mengyi", ""], ["Wang", "Ruiping", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1511.04855", "submitter": "Marc Chaumont", "authors": "Lionel Pibre, Pasquet J\\'er\\^ome, Dino Ienco, Marc Chaumont", "title": "Deep learning is a good steganalysis tool when embedding key is reused\n  for different images, even if there is a cover source-mismatch", "comments": "IS&T. Media Watermarking, Security, and Forensics, Part of IS&T\n  International Symposium on Electronic Imaging, EI'2016, Feb 2015, San\n  Fransisco, United States", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the BOSS competition, in 2010, most steganalysis approaches use a\nlearning methodology involving two steps: feature extraction, such as the Rich\nModels (RM), for the image representation, and use of the Ensemble Classifier\n(EC) for the learning step. In 2015, Qian et al. have shown that the use of a\ndeep learning approach that jointly learns and computes the features, is very\npromising for the steganalysis. In this paper, we follow-up the study of Qian\net al., and show that, due to intrinsic joint minimization, the results\nobtained from a Convolutional Neural Network (CNN) or a Fully Connected Neural\nNetwork (FNN), if well parameterized, surpass the conventional use of a RM with\nan EC. First, numerous experiments were conducted in order to find the best \"\nshape \" of the CNN. Second, experiments were carried out in the clairvoyant\nscenario in order to compare the CNN and FNN to an RM with an EC. The results\nshow more than 16% reduction in the classification error with our CNN or FNN.\nThird, experiments were also performed in a cover-source mismatch setting. The\nresults show that the CNN and FNN are naturally robust to the mismatch problem.\nIn Addition to the experiments, we provide discussions on the internal\nmechanisms of a CNN, and weave links with some previously stated ideas, in\norder to understand the impressive results we obtained.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 07:59:14 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 07:49:46 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Pibre", "Lionel", ""], ["J\u00e9r\u00f4me", "Pasquet", ""], ["Ienco", "Dino", ""], ["Chaumont", "Marc", ""]]}, {"id": "1511.04891", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Scott Cohen, Walter Chang, Brian Price, Ahmed\n  Elgammal", "title": "Sherlock: Scalable Fact Learning in Images", "comments": "Jan 7 Update", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study scalable and uniform understanding of facts in images. Existing\nvisual recognition systems are typically modeled differently for each fact type\nsuch as objects, actions, and interactions. We propose a setting where all\nthese facts can be modeled simultaneously with a capacity to understand\nunbounded number of facts in a structured way. The training data comes as\nstructured facts in images, including (1) objects (e.g., $<$boy$>$), (2)\nattributes (e.g., $<$boy, tall$>$), (3) actions (e.g., $<$boy, playing$>$), and\n(4) interactions (e.g., $<$boy, riding, a horse $>$). Each fact has a semantic\nlanguage view (e.g., $<$ boy, playing$>$) and a visual view (an image with this\nfact). We show that learning visual facts in a structured way enables not only\na uniform but also generalizable visual understanding. We propose and\ninvestigate recent and strong approaches from the multiview learning literature\nand also introduce two learning representation models as potential baselines.\nWe applied the investigated methods on several datasets that we augmented with\nstructured facts and a large scale dataset of more than 202,000 facts and\n814,000 images. Our experiments show the advantage of relating facts by the\nstructure by the proposed models compared to the designed baselines on\nbidirectional fact retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 09:56:04 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 22:36:55 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 02:56:24 GMT"}, {"version": "v4", "created": "Sat, 2 Apr 2016 05:26:39 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Cohen", "Scott", ""], ["Chang", "Walter", ""], ["Price", "Brian", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1511.04898", "submitter": "Bertrand Thirion", "authors": "Bertrand Thirion (PARIETAL), Andr\\'es Hoyos-Idrobo (NEUROSPIN,\n  PARIETAL), Jonas Kahn (LPP), Gael Varoquaux (NEUROSPIN, PARIETAL)", "title": "Fast clustering for scalable statistical analysis on structured images", "comments": "ICML Workshop on Statistics, Machine Learning and Neuroscience\n  (Stamlins 2015), Jul 2015, Lille, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of brain images as markers for diseases or behavioral differences is\nchallenged by the small effects size and the ensuing lack of power, an issue\nthat has incited researchers to rely more systematically on large cohorts.\nCoupled with resolution increases, this leads to very large datasets. A\nstriking example in the case of brain imaging is that of the Human Connectome\nProject: 20 Terabytes of data and growing. The resulting data deluge poses\nsevere challenges regarding the tractability of some processing steps\n(discriminant analysis, multivariate models) due to the memory demands posed by\nthese data. In this work, we revisit dimension reduction approaches, such as\nrandom projections, with the aim of replacing costly function evaluations by\ncheaper ones while decreasing the memory requirements. Specifically, we\ninvestigate the use of alternate schemes, based on fast clustering, that are\nwell suited for signals exhibiting a strong spatial structure, such as\nanatomical and functional brain images. Our contribution is twofold: i) we\npropose a linear-time clustering scheme that bypasses the percolation issues\ninherent in these algorithms and thus provides compressions nearly as good as\ntraditional quadratic-complexity variance-minimizing clustering schemes, ii) we\nshow that cluster-based compression can have the virtuous effect of removing\nhigh-frequency noise, actually improving subsequent estimations steps. As a\nconsequence, the proposed approach yields very accurate models on several\nlarge-scale problems yet with impressive gains in computational efficiency,\nmaking it possible to analyze large datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 10:26:18 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Thirion", "Bertrand", "", "PARIETAL"], ["Hoyos-Idrobo", "Andr\u00e9s", "", "NEUROSPIN,\n  PARIETAL"], ["Kahn", "Jonas", "", "LPP"], ["Varoquaux", "Gael", "", "NEUROSPIN, PARIETAL"]]}, {"id": "1511.04901", "submitter": "Erjin Zhou", "authors": "Zhiao Huang, Erjin Zhou, Zhimin Cao", "title": "Coarse-to-fine Face Alignment with Multi-Scale Local Patch Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial landmark localization plays an important role in face recognition and\nanalysis applications. In this paper, we give a brief introduction to a\ncoarse-to-fine pipeline with neural networks and sequential regression. First,\na global convolutional network is applied to the holistic facial image to give\nan initial landmark prediction. A pyramid of multi-scale local image patches is\nthen cropped to feed to a new network for each landmark to refine the\nprediction. As the refinement network outputs a more accurate position\nestimation than the input, such procedure could be repeated several times until\nthe estimation converges. We evaluate our system on the 300-W dataset [11] and\nit outperforms the recent state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 10:31:18 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Huang", "Zhiao", ""], ["Zhou", "Erjin", ""], ["Cao", "Zhimin", ""]]}, {"id": "1511.04902", "submitter": "Yann Schoenenberger", "authors": "Yann Schoenenberger, Johan Paratte, Pierre Vandergheynst", "title": "Graph-based denoising for time-varying point clouds", "comments": "4 pages, 3 figures, 3DTV-Con 2015", "journal-ref": "3DTV-Conference: The True Vision - Capture, Transmission and\n  Display of 3D Video (3DTV-CON) (2015) 1-4", "doi": "10.1109/3DTV.2015.7169366", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Noisy 3D point clouds arise in many applications. They may be due to errors\nwhen constructing a 3D model from images or simply to imprecise depth sensors.\nPoint clouds can be given geometrical structure using graphs created from the\nsimilarity information between points. This paper introduces a technique that\nuses this graph structure and convex optimization methods to denoise 3D point\nclouds. A short discussion presents how those methods naturally generalize to\ntime-varying inputs such as 3D point cloud time series.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 10:34:25 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Schoenenberger", "Yann", ""], ["Paratte", "Johan", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1511.04906", "submitter": "Jaime Zaratiegui", "authors": "Jaime Zaratiegui, Ana Montoro and Federico Castanedo", "title": "Performing Highly Accurate Predictions Through Convolutional Networks\n  for Actual Telecommunication Challenges", "comments": "11 pages, 6 figures, accepted by IJCAI-16 Workshop on Deep Learning\n  for Artificial Intelligence (DLAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigated how the application of deep learning, specifically the use of\nconvolutional networks trained with GPUs, can help to build better predictive\nmodels in telecommunication business environments, and fill this gap. In\nparticular, we focus on the non-trivial problem of predicting customer churn in\ntelecommunication operators. Our model, called WiseNet, consists of a\nconvolutional network and a novel encoding method that transforms customer\nactivity data and Call Detail Records (CDRs) into images. Experimental\nevaluation with several machine learning classifiers supports the ability of\nWiseNet for learning features when using structured input data. For this type\nof telecommunication business problems, we found that WiseNet outperforms\nmachine learning models with hand-crafted features, and does not require the\nlabor-intensive step of feature engineering. Furthermore, the same model has\nbeen applied without retraining to a different market, achieving consistent\nresults. This confirms the generalization property of WiseNet and the ability\nto extract useful representations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 10:42:08 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2016 18:36:24 GMT"}, {"version": "v3", "created": "Thu, 14 Jul 2016 10:21:47 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Zaratiegui", "Jaime", ""], ["Montoro", "Ana", ""], ["Castanedo", "Federico", ""]]}, {"id": "1511.04934", "submitter": "Wiharto Wiharto", "authors": "Esti Suryani, Wiharto Wiharto, Nizomjon Polvonov", "title": "Identification and Counting White Blood Cells and Red Blood Cells using\n  Image Processing Case Study of Leukemia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leukemia is diagnosed with complete blood counts which is by calculating all\nblood cells and compare the number of white blood cells (White Blood Cells /\nWBC) and red blood cells (Red Blood Cells / RBC). Information obtained from a\ncomplete blood count, has become a cornerstone in the hematology laboratory for\ndiagnostic purposes and monitoring of hematological disorders. However, the\ntraditional procedure for counting blood cells manually requires effort and a\nlong time, therefore this method is one of the most expensive routine tests in\nlaboratory hematology clinic. Solution for such kind of time consuming task and\nnecessity of data tracability can be found in image processing techniques based\non blood cell morphology . This study aims to identify Acute Lymphocytic\nLeukemia (ALL) and Acute Myeloid Leukemia type M3 (AML M3) using Fuzzy Rule\nBased System based on morphology of white blood cells. Characteristic\nparameters witch extractedare WBC Area, Nucleus and Granule Ratio of white\nblood cells. Image processing algorithms such as thresholding, Canny edge\ndetection and color identification filters are used.Then for identification of\nALL, AML M3 and Healthy cells used Fuzzy Rule Based System with Sugeno method.\nIn the testing process used 104 images out of which 29 ALL - Positive, 50 AML\nM3 - Positive and 25 Healthy cells. Test results showed 83.65 % accuracy .\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 12:41:06 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Suryani", "Esti", ""], ["Wiharto", "Wiharto", ""], ["Polvonov", "Nizomjon", ""]]}, {"id": "1511.04960", "submitter": "Mohammad Najafi", "authors": "Mohammad Najafi, Sarah Taghavi Namin, Mathieu Salzmann, Lars Petersson", "title": "Sample and Filter: Nonparametric Scene Parsing via Efficient Filtering", "comments": "Please refer to the CVPR-2016 version of this manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene parsing has attracted a lot of attention in computer vision. While\nparametric models have proven effective for this task, they cannot easily\nincorporate new training data. By contrast, nonparametric approaches, which\nbypass any learning phase and directly transfer the labels from the training\ndata to the query images, can readily exploit new labeled samples as they\nbecome available. Unfortunately, because of the computational cost of their\nlabel transfer procedures, state-of-the-art nonparametric methods typically\nfilter out most training images to only keep a few relevant ones to label the\nquery. As such, these methods throw away many images that still contain\nvaluable information and generally obtain an unbalanced set of labeled samples.\nIn this paper, we introduce a nonparametric approach to scene parsing that\nfollows a sample-and-filter strategy. More specifically, we propose to sample\nlabeled superpixels according to an image similarity score, which allows us to\nobtain a balanced set of samples. We then formulate label transfer as an\nefficient filtering procedure, which lets us exploit more labeled samples than\nexisting techniques. Our experiments evidence the benefits of our approach over\nstate-of-the-art nonparametric methods on two benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 14:07:47 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 01:29:03 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Najafi", "Mohammad", ""], ["Namin", "Sarah Taghavi", ""], ["Salzmann", "Mathieu", ""], ["Petersson", "Lars", ""]]}, {"id": "1511.05045", "submitter": "Zhenzhong Lan", "authors": "Zhenzhong Lan, Shoou-I Yu, Ming Lin, Bhiksha Raj, Alexander G.\n  Hauptmann", "title": "Handcrafted Local Features are Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image and video classification research has made great progress through the\ndevelopment of handcrafted local features and learning based features. These\ntwo architectures were proposed roughly at the same time and have flourished at\noverlapping stages of history. However, they are typically viewed as distinct\napproaches. In this paper, we emphasize their structural similarities and show\nhow such a unified view helps us in designing features that balance efficiency\nand effectiveness. As an example, we study the problem of designing efficient\nvideo feature learning algorithms for action recognition.\n  We approach this problem by first showing that local handcrafted features and\nConvolutional Neural Networks (CNNs) share the same convolution-pooling network\nstructure. We then propose a two-stream Convolutional ISA (ConvISA) that adopts\nthe convolution-pooling structure of the state-of-the-art handcrafted video\nfeature with greater modeling capacities and a cost-effective training\nalgorithm. Through custom designed network structures for pixels and optical\nflow, our method also reflects distinctive characteristics of these two data\nsources.\n  Our experimental results on standard action recognition benchmarks show that\nby focusing on the structure of CNNs, rather than end-to-end training methods,\nwe are able to design an efficient and powerful video feature learning\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 17:17:28 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 20:25:09 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Lan", "Zhenzhong", ""], ["Yu", "Shoou-I", ""], ["Lin", "Ming", ""], ["Raj", "Bhiksha", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1511.05049", "submitter": "Heng Yang", "authors": "Heng Yang and Xuhui Jia and Chen Change Loy and Peter Robinson", "title": "An Empirical Study of Recent Face Alignment Methods", "comments": "under review of a conference. Project page:\n  https://www.cl.cam.ac.uk/~hy306/FaceAlignment.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of face alignment has been intensively studied in the past years.\nA large number of novel methods have been proposed and reported very good\nperformance on benchmark dataset such as 300W. However, the differences in the\nexperimental setting and evaluation metric, missing details in the description\nof the methods make it hard to reproduce the results reported and evaluate the\nrelative merits. For instance, most recent face alignment methods are built on\ntop of face detection but from different face detectors. In this paper, we\ncarry out a rigorous evaluation of these methods by making the following\ncontributions: 1) we proposes a new evaluation metric for face alignment on a\nset of images, i.e., area under error distribution curve within a threshold,\nAUC$_\\alpha$, given the fact that the traditional evaluation measure (mean\nerror) is very sensitive to big alignment error. 2) we extend the 300W database\nwith more practical face detections to make fair comparison possible. 3) we\ncarry out face alignment sensitivity analysis w.r.t. face detection, on both\nsynthetic and real data, using both off-the-shelf and re-retrained models. 4)\nwe study factors that are particularly important to achieve good performance\nand provide suggestions for practical applications. Most of the conclusions\ndrawn from our comparative analysis cannot be inferred from the original\npublications.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 17:26:27 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Yang", "Heng", ""], ["Jia", "Xuhui", ""], ["Loy", "Chen Change", ""], ["Robinson", "Peter", ""]]}, {"id": "1511.05065", "submitter": "Bumsub Ham", "authors": "Bumsub Ham, Minsu Cho, Cordelia Schmid, Jean Ponce", "title": "Proposal Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding image correspondences remains a challenging problem in the presence\nof intra-class variations and large changes in scene layout.~Semantic flow\nmethods are designed to handle images depicting different instances of the same\nobject or scene category. We introduce a novel approach to semantic flow,\ndubbed proposal flow, that establishes reliable correspondences using object\nproposals. Unlike prevailing semantic flow approaches that operate on pixels or\nregularly sampled local regions, proposal flow benefits from the\ncharacteristics of modern object proposals, that exhibit high repeatability at\nmultiple scales, and can take advantage of both local and geometric consistency\nconstraints among proposals. We also show that proposal flow can effectively be\ntransformed into a conventional dense flow field. We introduce a new dataset\nthat can be used to evaluate both general semantic flow techniques and\nregion-based approaches such as proposal flow. We use this benchmark to compare\ndifferent matching algorithms, object proposals, and region features within\nproposal flow, to the state of the art in semantic flow. This comparison, along\nwith experiments on standard datasets, demonstrates that proposal flow\nsignificantly outperforms existing semantic flow methods in various settings.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 17:54:45 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 16:19:40 GMT"}, {"version": "v3", "created": "Fri, 8 Jul 2016 18:32:37 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Ham", "Bumsub", ""], ["Cho", "Minsu", ""], ["Schmid", "Cordelia", ""], ["Ponce", "Jean", ""]]}, {"id": "1511.05067", "submitter": "Dmitrij Schlesinger", "authors": "Alexander Kirillov and Dmitrij Schlesinger and Shuai Zheng and Bogdan\n  Savchynskyy and Philip H.S. Torr and Carsten Rother", "title": "Joint Training of Generic CNN-CRF Models with Stochastic Optimization", "comments": "ACCV2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new CNN-CRF end-to-end learning framework, which is based on\njoint stochastic optimization with respect to both Convolutional Neural Network\n(CNN) and Conditional Random Field (CRF) parameters. While stochastic gradient\ndescent is a standard technique for CNN training, it was not used for joint\nmodels so far. We show that our learning method is (i) general, i.e. it applies\nto arbitrary CNN and CRF architectures and potential functions; (ii) scalable,\ni.e. it has a low memory footprint and straightforwardly parallelizes on GPUs;\n(iii) easy in implementation. Additionally, the unified CNN-CRF optimization\napproach simplifies a potential hardware implementation. We empirically\nevaluate our method on the task of semantic labeling of body parts in depth\nimages and show that it compares favorably to competing techniques.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 17:59:14 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 23:57:38 GMT"}, {"version": "v3", "created": "Wed, 14 Sep 2016 11:52:49 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Kirillov", "Alexander", ""], ["Schlesinger", "Dmitrij", ""], ["Zheng", "Shuai", ""], ["Savchynskyy", "Bogdan", ""], ["Torr", "Philip H. S.", ""], ["Rother", "Carsten", ""]]}, {"id": "1511.05084", "submitter": "Ivet Rafegas", "authors": "Ivet Rafegas and Maria Vanrell", "title": "Understanding learned CNN features through Filter Decoding with\n  Substitution", "comments": "10 pages, 7 figures (including supplementary material). Submitted for\n  review for CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In parallel with the success of CNNs to solve vision problems, there is a\ngrowing interest in developing methodologies to understand and visualize the\ninternal representations of these networks. How the responses of a trained CNN\nencode the visual information is a fundamental question both for computer and\nhuman vision research. Image representations provided by the first\nconvolutional layer as well as the resolution change provided by the\nmax-polling operation are easy to understand, however, as soon as a second and\nfurther convolutional layers are added in the representation, any intuition is\nlost. A usual way to deal with this problem has been to define deconvolutional\nnetworks that somehow allow to explore the internal representations of the most\nimportant activations towards the image space, where deconvolution is assumed\nas a convolution with the transposed filter. However, this assumption is not\nthe best approximation of an inverse convolution. In this paper we propose a\nnew assumption based on filter substitution to reverse the encoding of a\nconvolutional layer. This provides us with a new tool to directly visualize any\nCNN single neuron as a filter in the first layer, this is in terms of the image\nspace.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 18:50:24 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 10:15:48 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Rafegas", "Ivet", ""], ["Vanrell", "Maria", ""]]}, {"id": "1511.05099", "submitter": "Peng Zhang", "authors": "Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, Devi Parikh", "title": "Yin and Yang: Balancing and Answering Binary Visual Questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complex compositional structure of language makes problems at the\nintersection of vision and language challenging. But language also provides a\nstrong prior that can result in good superficial performance, without the\nunderlying models truly understanding the visual content. This can hinder\nprogress in pushing state of art in the computer vision aspects of multi-modal\nAI. In this paper, we address binary Visual Question Answering (VQA) on\nabstract scenes. We formulate this problem as visual verification of concepts\ninquired in the questions. Specifically, we convert the question to a tuple\nthat concisely summarizes the visual concept to be detected in the image. If\nthe concept can be found in the image, the answer to the question is \"yes\", and\notherwise \"no\". Abstract scenes play two roles (1) They allow us to focus on\nthe high-level semantics of the VQA task as opposed to the low-level\nrecognition problems, and perhaps more importantly, (2) They provide us the\nmodality to balance the dataset such that language priors are controlled, and\nthe role of vision is essential. In particular, we collect fine-grained pairs\nof scenes for every question, such that the answer to the question is \"yes\" for\none scene, and \"no\" for the other for the exact same question. Indeed, language\npriors alone do not perform better than chance on our balanced dataset.\nMoreover, our proposed approach matches the performance of a state-of-the-art\nVQA approach on the unbalanced dataset, and outperforms it on the balanced\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 19:38:14 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 20:54:47 GMT"}, {"version": "v3", "created": "Sun, 22 Nov 2015 20:54:35 GMT"}, {"version": "v4", "created": "Sun, 31 Jan 2016 20:58:39 GMT"}, {"version": "v5", "created": "Tue, 19 Apr 2016 19:30:00 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Zhang", "Peng", ""], ["Goyal", "Yash", ""], ["Summers-Stay", "Douglas", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1511.05122", "submitter": "Sara Sabour", "authors": "Sara Sabour, Yanshuai Cao, Fartash Faghri, David J. Fleet", "title": "Adversarial Manipulation of Deep Representations", "comments": "Accepted as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We show that the representation of an image in a deep neural network (DNN)\ncan be manipulated to mimic those of other natural images, with only minor,\nimperceptible perturbations to the original image. Previous methods for\ngenerating adversarial images focused on image perturbations designed to\nproduce erroneous class labels, while we concentrate on the internal layers of\nDNN representations. In this way our new class of adversarial images differs\nqualitatively from others. While the adversary is perceptually similar to one\nimage, its internal representation appears remarkably similar to a different\nimage, one from a different class, bearing little if any apparent similarity to\nthe input; they appear generic and consistent with the space of natural images.\nThis phenomenon raises questions about DNN representations, as well as the\nproperties of natural images themselves.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 20:48:20 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 21:00:44 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 20:56:44 GMT"}, {"version": "v4", "created": "Fri, 11 Dec 2015 21:03:14 GMT"}, {"version": "v5", "created": "Thu, 7 Jan 2016 20:59:55 GMT"}, {"version": "v6", "created": "Tue, 12 Jan 2016 20:51:51 GMT"}, {"version": "v7", "created": "Wed, 13 Jan 2016 20:57:33 GMT"}, {"version": "v8", "created": "Tue, 1 Mar 2016 20:51:06 GMT"}, {"version": "v9", "created": "Fri, 4 Mar 2016 20:21:24 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Sabour", "Sara", ""], ["Cao", "Yanshuai", ""], ["Faghri", "Fartash", ""], ["Fleet", "David J.", ""]]}, {"id": "1511.05169", "submitter": "Siyuan Huang", "authors": "Siyuan Huang, Jiwen Lu, Jie Zhou, Anil K. Jain", "title": "Nonlinear Local Metric Learning for Person Re-identification", "comments": "Submitted to CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification aims at matching pedestrians observed from\nnon-overlapping camera views. Feature descriptor and metric learning are two\nsignificant problems in person re-identification. A discriminative metric\nlearning method should be capable of exploiting complex nonlinear\ntransformations due to the large variations in feature space. In this paper, we\npropose a nonlinear local metric learning (NLML) method to improve the\nstate-of-the-art performance of person re-identification on public datasets.\nMotivated by the fact that local metric learning has been introduced to handle\nthe data which varies locally and deep neural network has presented outstanding\ncapability in exploiting the nonlinearity of samples, we utilize the merits of\nboth local metric learning and deep neural network to learn multiple sets of\nnonlinear transformations. By enforcing a margin between the distances of\npositive pedestrian image pairs and distances of negative pairs in the\ntransformed feature subspace, discriminative information can be effectively\nexploited in the developed neural networks. Our experiments show that the\nproposed NLML method achieves the state-of-the-art results on the widely used\nVIPeR, GRID, and CUHK 01 datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 21:02:31 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Huang", "Siyuan", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""], ["Jain", "Anil K.", ""]]}, {"id": "1511.05174", "submitter": "Vishwanath Saragadam Raja Venkata", "authors": "Vishwanath Saragadam, Xin Li, Aswin Sankaranarayanan", "title": "Cross-scale predictive dictionaries", "comments": "12 pages", "journal-ref": null, "doi": "10.1109/TIP.2018.2869719", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representations using data dictionaries provide an efficient model\nparticularly for signals that do not enjoy alternate analytic sparsifying\ntransformations. However, solving inverse problems with sparsifying\ndictionaries can be computationally expensive, especially when the dictionary\nunder consideration has a large number of atoms. In this paper, we incorporate\nadditional structure on to dictionary-based sparse representations for visual\nsignals to enable speedups when solving sparse approximation problems. The\nspecific structure that we endow onto sparse models is that of a multi-scale\nmodeling where the sparse representation at each scale is constrained by the\nsparse representation at coarser scales. We show that this cross-scale\npredictive model delivers significant speedups, often in the range of\n10-60$\\times$, with little loss in accuracy for linear inverse problems\nassociated with images, videos, and light fields.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 21:07:38 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 21:09:48 GMT"}, {"version": "v3", "created": "Tue, 4 Sep 2018 03:25:13 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Saragadam", "Vishwanath", ""], ["Li", "Xin", ""], ["Sankaranarayanan", "Aswin", ""]]}, {"id": "1511.05175", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Tarek El-Gaaly, Amr Bakry, Ahmed Elgammal", "title": "Convolutional Models for Joint Object Categorization and Pose Estimation", "comments": "only for workshop presentation at ICLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the task of Object Recognition, there exists a dichotomy between the\ncategorization of objects and estimating object pose, where the former\nnecessitates a view-invariant representation, while the latter requires a\nrepresentation capable of capturing pose information over different categories\nof objects. With the rise of deep architectures, the prime focus has been on\nobject category recognition. Deep learning methods have achieved wide success\nin this task. In contrast, object pose regression using these approaches has\nreceived relatively much less attention. In this paper we show how deep\narchitectures, specifically Convolutional Neural Networks (CNN), can be adapted\nto the task of simultaneous categorization and pose estimation of objects. We\ninvestigate and analyze the layers of various CNN models and extensively\ncompare between them with the goal of discovering how the layers of distributed\nrepresentations of CNNs represent object pose information and how this\ncontradicts with object category representations. We extensively experiment on\ntwo recent large and challenging multi-view datasets. Our models achieve better\nthan state-of-the-art performance on both datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 21:08:22 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 23:17:11 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 23:40:23 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2016 22:41:19 GMT"}, {"version": "v5", "created": "Mon, 22 Feb 2016 23:54:23 GMT"}, {"version": "v6", "created": "Tue, 19 Apr 2016 17:56:34 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["El-Gaaly", "Tarek", ""], ["Bakry", "Amr", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1511.05197", "submitter": "Tsung-Yu Lin", "authors": "Tsung-Yu Lin and Subhransu Maji", "title": "Visualizing and Understanding Deep Texture Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent approaches have used deep convolutional neural networks\n(CNNs) to build texture representations. Nevertheless, it is still unclear how\nthese models represent texture and invariances to categorical variations. This\nwork conducts a systematic evaluation of recent CNN-based texture descriptors\nfor recognition and attempts to understand the nature of invariances captured\nby these representations. First we show that the recently proposed bilinear CNN\nmodel [25] is an excellent general-purpose texture descriptor and compares\nfavorably to other CNN-based descriptors on various texture and scene\nrecognition benchmarks. The model is translationally invariant and obtains\nbetter accuracy on the ImageNet dataset without requiring spatial jittering of\ndata compared to corresponding models trained with spatial jittering. Based on\nrecent work [13, 28] we propose a technique to visualize pre-images, providing\na means for understanding categorical properties that are captured by these\nrepresentations. Finally, we show preliminary results on how a unified\nparametric model of texture analysis and synthesis can be used for\nattribute-based image manipulation, e.g. to make an image more swirly,\nhoneycombed, or knitted. The source code and additional visualizations are\navailable at http://vis-www.cs.umass.edu/texture\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 22:01:16 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 16:37:46 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Lin", "Tsung-Yu", ""], ["Maji", "Subhransu", ""]]}, {"id": "1511.05204", "submitter": "Mengyi Liu", "authors": "Mengyi Liu, Shiguang Shan, Ruiping Wang, Xilin Chen", "title": "Learning Expressionlets via Universal Manifold Model for Dynamic Facial\n  Expression Recognition", "comments": "12 pages", "journal-ref": null, "doi": "10.1109/TIP.2016.2615424", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression is temporally dynamic event which can be decomposed into a\nset of muscle motions occurring in different facial regions over various time\nintervals. For dynamic expression recognition, two key issues, temporal\nalignment and semantics-aware dynamic representation, must be taken into\naccount. In this paper, we attempt to solve both problems via manifold modeling\nof videos based on a novel mid-level representation, i.e.\n\\textbf{expressionlet}. Specifically, our method contains three key stages: 1)\neach expression video clip is characterized as a spatial-temporal manifold\n(STM) formed by dense low-level features; 2) a Universal Manifold Model (UMM)\nis learned over all low-level features and represented as a set of local modes\nto statistically unify all the STMs. 3) the local modes on each STM can be\ninstantiated by fitting to UMM, and the corresponding expressionlet is\nconstructed by modeling the variations in each local mode. With above strategy,\nexpression videos are naturally aligned both spatially and temporally. To\nenhance the discriminative power, the expressionlet-based STM representation is\nfurther processed with discriminant embedding. Our method is evaluated on four\npublic expression databases, CK+, MMI, Oulu-CASIA, and FERA. In all cases, our\nmethod outperforms the known state-of-the-art by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 22:19:11 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Liu", "Mengyi", ""], ["Shan", "Shiguang", ""], ["Wang", "Ruiping", ""], ["Chen", "Xilin", ""]]}, {"id": "1511.05234", "submitter": "Huijuan Xu", "authors": "Huijuan Xu and Kate Saenko", "title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for\n  Visual Question Answering", "comments": "include test-standard result on VQA full release (V1.0) dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of Visual Question Answering (VQA), which requires\njoint image and language understanding to answer a question about a given\nphotograph. Recent approaches have applied deep image captioning methods based\non convolutional-recurrent networks to this problem, but have failed to model\nspatial inference. To remedy this, we propose a model we call the Spatial\nMemory Network and apply it to the VQA task. Memory networks are recurrent\nneural networks with an explicit attention mechanism that selects certain parts\nof the information stored in memory. Our Spatial Memory Network stores neuron\nactivations from different spatial regions of the image in its memory, and uses\nthe question to choose relevant regions for computing the answer, a process of\nwhich constitutes a single \"hop\" in the network. We propose a novel spatial\nattention architecture that aligns words with image patches in the first hop,\nand obtain improved results by adding a second attention hop which considers\nthe whole question to choose visual evidence based on the results of the first\nhop. To better understand the inference process learned by the network, we\ndesign synthetic questions that specifically require spatial inference and\nvisualize the attention weights. We evaluate our model on two published visual\nquestion answering datasets, DAQUAR [1] and VQA [2], and obtain improved\nresults compared to a strong deep baseline model (iBOWIMG) which concatenates\nimage and question features to predict the answer [3].\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 01:00:04 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2016 03:06:58 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Xu", "Huijuan", ""], ["Saenko", "Kate", ""]]}, {"id": "1511.05261", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng", "title": "Robust PCA via Nonconvex Rank Approximation", "comments": "IEEE International Conference on Data Mining", "journal-ref": null, "doi": "10.1109/ICDM.2015.15", "report-no": null, "categories": "cs.CV cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous applications in data mining and machine learning require recovering\na matrix of minimal rank. Robust principal component analysis (RPCA) is a\ngeneral framework for handling this kind of problems. Nuclear norm based convex\nsurrogate of the rank function in RPCA is widely investigated. Under certain\nassumptions, it can recover the underlying true low rank matrix with high\nprobability. However, those assumptions may not hold in real-world\napplications. Since the nuclear norm approximates the rank by adding all\nsingular values together, which is essentially a $\\ell_1$-norm of the singular\nvalues, the resulting approximation error is not trivial and thus the resulting\nmatrix estimator can be significantly biased. To seek a closer approximation\nand to alleviate the above-mentioned limitations of the nuclear norm, we\npropose a nonconvex rank approximation. This approximation to the matrix rank\nis tighter than the nuclear norm. To solve the associated nonconvex\nminimization problem, we develop an efficient augmented Lagrange multiplier\nbased optimization algorithm. Experimental results demonstrate that our method\noutperforms current state-of-the-art algorithms in both accuracy and\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 03:00:30 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1511.05284", "submitter": "Lisa Anne Hendricks", "authors": "Lisa Anne Hendricks, Subhashini Venugopalan, Marcus Rohrbach, Raymond\n  Mooney, Kate Saenko, Trevor Darrell", "title": "Deep Compositional Captioning: Describing Novel Object Categories\n  without Paired Training Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent deep neural network models have achieved promising results on\nthe image captioning task, they rely largely on the availability of corpora\nwith paired image and sentence captions to describe objects in context. In this\nwork, we propose the Deep Compositional Captioner (DCC) to address the task of\ngenerating descriptions of novel objects which are not present in paired\nimage-sentence datasets. Our method achieves this by leveraging large object\nrecognition datasets and external text corpora and by transferring knowledge\nbetween semantically similar concepts. Current deep caption models can only\ndescribe objects contained in paired image-sentence corpora, despite the fact\nthat they are pre-trained with large object recognition datasets, namely\nImageNet. In contrast, our model can compose sentences that describe novel\nobjects and their interactions with other objects. We demonstrate our model's\nability to describe novel concepts by empirically evaluating its performance on\nMSCOCO and show qualitative results on ImageNet images of objects for which no\npaired image-caption data exist. Further, we extend our approach to generate\ndescriptions of objects in video clips. Our results show that DCC has distinct\nadvantages over existing image and video captioning approaches for generating\ndescriptions of new objects in context.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 06:44:48 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 23:40:55 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Hendricks", "Lisa Anne", ""], ["Venugopalan", "Subhashini", ""], ["Rohrbach", "Marcus", ""], ["Mooney", "Raymond", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""]]}, {"id": "1511.05286", "submitter": "Oren Kraus", "authors": "Oren Z. Kraus, Lei Jimmy Ba, Brendan Frey", "title": "Classifying and Segmenting Microscopy Images Using Convolutional\n  Multiple Instance Learning", "comments": null, "journal-ref": "Bioinformatics (2016) 32 (12): i52-i59", "doi": "10.1093/bioinformatics/btw252", "report-no": null, "categories": "cs.CV q-bio.SC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have achieved state of the art\nperformance on both classification and segmentation tasks. Applying CNNs to\nmicroscopy images is challenging due to the lack of datasets labeled at the\nsingle cell level. We extend the application of CNNs to microscopy image\nclassification and segmentation using multiple instance learning (MIL). We\npresent the adaptive Noisy-AND MIL pooling function, a new MIL operator that is\nrobust to outliers. Combining CNNs with MIL enables training CNNs using full\nresolution microscopy images with global labels. We base our approach on the\nsimilarity between the aggregation function used in MIL and pooling layers used\nin CNNs. We show that training MIL CNNs end-to-end outperforms several previous\nmethods on both mammalian and yeast microscopy images without requiring any\nsegmentation steps.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 06:55:58 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Kraus", "Oren Z.", ""], ["Ba", "Lei Jimmy", ""], ["Frey", "Brendan", ""]]}, {"id": "1511.05292", "submitter": "Jinghua Wang", "authors": "Jinghua Wang, Gang Wang", "title": "Hierarchical Spatial Sum-Product Networks for Action Recognition in\n  Still Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing actions from still images is popularly studied recently. In this\npaper, we model an action class as a flexible number of spatial configurations\nof body parts by proposing a new spatial SPN (Sum-Product Networks). First, we\ndiscover a set of parts in image collections via unsupervised learning. Then,\nour new spatial SPN is applied to model the spatial relationship and also the\nhigh-order correlations of parts. To learn robust networks, we further develop\na hierarchical spatial SPN method, which models pairwise spatial relationship\nbetween parts inside sub-images and models the correlation of sub-images via\nextra layers of SPN. Our method is shown to be effective on two benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 07:21:20 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 07:29:25 GMT"}, {"version": "v3", "created": "Fri, 8 Jul 2016 01:41:41 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Wang", "Jinghua", ""], ["Wang", "Gang", ""]]}, {"id": "1511.05296", "submitter": "Jinghua Wang", "authors": "Jinghua Wang, Abrar Abdul Nabi, Gang Wang, Chengde Wan, Tian-Tsong Ng", "title": "Towards Predicting the Likeability of Fashion Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for ranking fashion images to find the\nones which might be liked by more people. We collect two new datasets from\nimage sharing websites (Pinterest and Polyvore). We represent fashion images\nbased on attributes: semantic attributes and data-driven attributes. To learn\nsemantic attributes from limited training data, we use an algorithm on\nmulti-task convolutional neural networks to share visual knowledge among\ndifferent semantic attribute categories. To discover data-driven attributes\nunsupervisedly, we propose an algorithm to simultaneously discover visual\nclusters and learn fashion-specific feature representations. Given attributes\nas representations, we propose to learn a ranking SPN (sum product networks) to\nrank pairs of fashion images. The proposed ranking SPN can capture the\nhigh-order correlations of the attributes. We show the effectiveness of our\nmethod on our two newly collected datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 07:31:36 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 07:26:25 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Wang", "Jinghua", ""], ["Nabi", "Abrar Abdul", ""], ["Wang", "Gang", ""], ["Wan", "Chengde", ""], ["Ng", "Tian-Tsong", ""]]}, {"id": "1511.05298", "submitter": "Ashesh Jain", "authors": "Ashesh Jain, Amir R. Zamir, Silvio Savarese, Ashutosh Saxena", "title": "Structural-RNN: Deep Learning on Spatio-Temporal Graphs", "comments": "CVPR 2016 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Recurrent Neural Network architectures, though remarkably capable at\nmodeling sequences, lack an intuitive high-level spatio-temporal structure.\nThat is while many problems in computer vision inherently have an underlying\nhigh-level structure and can benefit from it. Spatio-temporal graphs are a\npopular tool for imposing such high-level intuitions in the formulation of real\nworld problems. In this paper, we propose an approach for combining the power\nof high-level spatio-temporal graphs and sequence learning success of Recurrent\nNeural Networks~(RNNs). We develop a scalable method for casting an arbitrary\nspatio-temporal graph as a rich RNN mixture that is feedforward, fully\ndifferentiable, and jointly trainable. The proposed method is generic and\nprincipled as it can be used for transforming any spatio-temporal graph through\nemploying a certain set of well defined steps. The evaluations of the proposed\napproach on a diverse set of problems, ranging from modeling human motion to\nobject interactions, shows improvement over the state-of-the-art with a large\nmargin. We expect this method to empower new approaches to problem formulation\nthrough high-level spatio-temporal graphs and Recurrent Neural Networks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 07:49:58 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 01:26:23 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2016 19:00:24 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Jain", "Ashesh", ""], ["Zamir", "Amir R.", ""], ["Savarese", "Silvio", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1511.05440", "submitter": "Michael Mathieu", "authors": "Michael Mathieu, Camille Couprie and Yann LeCun", "title": "Deep multi-scale video prediction beyond mean square error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to predict future images from a video sequence involves the\nconstruction of an internal representation that models the image evolution\naccurately, and therefore, to some degree, its content and dynamics. This is\nwhy pixel-space video prediction may be viewed as a promising avenue for\nunsupervised feature learning. In addition, while optical flow has been a very\nstudied problem in computer vision for a long time, future frame prediction is\nrarely approached. Still, many vision applications could benefit from the\nknowledge of the next frames of videos, that does not require the complexity of\ntracking every pixel trajectories. In this work, we train a convolutional\nnetwork to generate future frames given an input sequence. To deal with the\ninherently blurry predictions obtained from the standard Mean Squared Error\n(MSE) loss function, we propose three different and complementary feature\nlearning strategies: a multi-scale architecture, an adversarial training\nmethod, and an image gradient difference loss function. We compare our\npredictions to different published results based on recurrent neural networks\non the UCF101 dataset\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 15:36:32 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 23:21:22 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 04:58:24 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2016 21:52:53 GMT"}, {"version": "v5", "created": "Fri, 15 Jan 2016 02:09:16 GMT"}, {"version": "v6", "created": "Fri, 26 Feb 2016 22:10:30 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Mathieu", "Michael", ""], ["Couprie", "Camille", ""], ["LeCun", "Yann", ""]]}, {"id": "1511.05497", "submitter": "Suraj Srinivas", "authors": "Suraj Srinivas and R. Venkatesh Babu", "title": "Learning Neural Network Architectures using Backpropagation", "comments": "BMVC 2016 ; Title modified from 'Learning the Architecture of Deep\n  Neural Networks'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks with millions of parameters are at the heart of many\nstate of the art machine learning models today. However, recent works have\nshown that models with much smaller number of parameters can also perform just\nas well. In this work, we introduce the problem of architecture-learning, i.e;\nlearning the architecture of a neural network along with weights. We introduce\na new trainable parameter called tri-state ReLU, which helps in eliminating\nunnecessary neurons. We also propose a smooth regularizer which encourages the\ntotal number of neurons after elimination to be small. The resulting objective\nis differentiable and simple to optimize. We experimentally validate our method\non both small and large networks, and show that it can learn models with a\nconsiderably small number of parameters without affecting prediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 18:26:11 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 11:46:48 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Srinivas", "Suraj", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1511.05512", "submitter": "Florian Jug", "authors": "Florian Jug, Evgeny Levinkov, Corinna Blasse, Eugene W. Myers, Bjoern\n  Andres", "title": "Moral Lineage Tracing", "comments": "11 pages", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2016, pp. 5926-5935", "doi": "10.1109/CVPR.2016.638", "report-no": null, "categories": "cs.CV cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lineage tracing, the tracking of living cells as they move and divide, is a\ncentral problem in biological image analysis. Solutions, called lineage\nforests, are key to understanding how the structure of multicellular organisms\nemerges. We propose an integer linear program (ILP) whose feasible solutions\ndefine a decomposition of each image in a sequence into cells (segmentation),\nand a lineage forest of cells across images (tracing). Unlike previous\nformulations, we do not constrain the set of decompositions, except by\ncontracting pixels to superpixels. The main challenge, as we show, is to\nenforce the morality of lineages, i.e., the constraint that cells do not merge.\nTo enforce morality, we introduce path-cut inequalities. To find feasible\nsolutions of the NP-hard ILP, with certified bounds to the global optimum, we\ndefine efficient separation procedures and apply these as part of a\nbranch-and-cut algorithm. We show the effectiveness of this approach by\nanalyzing feasible solutions for real microscopy data in terms of bounds and\nrun-time, and by their weighted edit distance to ground truth lineage forests\ntraced by humans.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 19:18:16 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 10:42:12 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Jug", "Florian", ""], ["Levinkov", "Evgeny", ""], ["Blasse", "Corinna", ""], ["Myers", "Eugene W.", ""], ["Andres", "Bjoern", ""]]}, {"id": "1511.05526", "submitter": "Matthew Walter", "authors": "Zhengyang Wu and Mohit Bansal and Matthew R. Walter", "title": "Learning Articulated Motion Models from Visual and Lingual Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order for robots to operate effectively in homes and workplaces, they must\nbe able to manipulate the articulated objects common within environments built\nfor and by humans. Previous work learns kinematic models that prescribe this\nmanipulation from visual demonstrations. Lingual signals, such as natural\nlanguage descriptions and instructions, offer a complementary means of\nconveying knowledge of such manipulation models and are suitable to a wide\nrange of interactions (e.g., remote manipulation). In this paper, we present a\nmultimodal learning framework that incorporates both visual and lingual\ninformation to estimate the structure and parameters that define kinematic\nmodels of articulated objects. The visual signal takes the form of an RGB-D\nimage stream that opportunistically captures object motion in an unprepared\nscene. Accompanying natural language descriptions of the motion constitute the\nlingual signal. We present a probabilistic language model that uses word\nembeddings to associate lingual verbs with their corresponding kinematic\nstructures. By exploiting the complementary nature of the visual and lingual\ninput, our method infers correct kinematic structures for various multiple-part\nobjects on which the previous state-of-the-art, visual-only system fails. We\nevaluate our multimodal learning framework on a dataset comprised of a variety\nof household objects, and demonstrate a 36% improvement in model accuracy over\nthe vision-only baseline.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 19:55:34 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 14:53:28 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Wu", "Zhengyang", ""], ["Bansal", "Mohit", ""], ["Walter", "Matthew R.", ""]]}, {"id": "1511.05547", "submitter": "Baochen Sun", "authors": "Baochen Sun, Jiashi Feng, Kate Saenko", "title": "Return of Frustratingly Easy Domain Adaptation", "comments": "Fixed typos. Full paper to appear in AAAI-16. Extended Abstract of\n  the full paper to appear in TASK-CV 2015 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike human learning, machine learning often fails to handle changes between\ntraining (source) and test (target) input distributions. Such domain shifts,\ncommon in practical scenarios, severely damage the performance of conventional\nmachine learning methods. Supervised domain adaptation methods have been\nproposed for the case when the target data have labels, including some that\nperform very well despite being \"frustratingly easy\" to implement. However, in\npractice, the target domain is often unlabeled, requiring unsupervised\nadaptation. We propose a simple, effective, and efficient method for\nunsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL\nminimizes domain shift by aligning the second-order statistics of source and\ntarget distributions, without requiring any target labels. Even though it is\nextraordinarily simple--it can be implemented in four lines of Matlab\ncode--CORAL performs remarkably well in extensive evaluations on standard\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 20:53:26 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 05:39:43 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Sun", "Baochen", ""], ["Feng", "Jiashi", ""], ["Saenko", "Kate", ""]]}, {"id": "1511.05607", "submitter": "Min Li", "authors": "Min Li, Sudeep Gaddam, Xiaolin Li, Yinan Zhao, Jingzhe Ma, Jian Ge", "title": "Identifying the Absorption Bump with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pervasive interstellar dust grains provide significant insights to\nunderstand the formation and evolution of the stars, planetary systems, and the\ngalaxies, and may harbor the building blocks of life. One of the most effective\nway to analyze the dust is via their interaction with the light from background\nsources. The observed extinction curves and spectral features carry the size\nand composition information of dust. The broad absorption bump at 2175 Angstrom\nis the most prominent feature in the extinction curves. Traditionally,\nstatistical methods are applied to detect the existence of the absorption bump.\nThese methods require heavy preprocessing and the co-existence of other\nreference features to alleviate the influence from the noises. In this paper,\nwe apply Deep Learning techniques to detect the broad absorption bump. We\ndemonstrate the key steps for training the selected models and their results.\nThe success of Deep Learning based method inspires us to generalize a common\nmethodology for broader science discovery problems. We present our on-going\nwork to build the DeepDis system for such kind of applications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 22:27:05 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 14:20:46 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Li", "Min", ""], ["Gaddam", "Sudeep", ""], ["Li", "Xiaolin", ""], ["Zhao", "Yinan", ""], ["Ma", "Jingzhe", ""], ["Ge", "Jian", ""]]}, {"id": "1511.05616", "submitter": "Hexiang Hu", "authors": "Hexiang Hu, Guang-Tong Zhou, Zhiwei Deng, Zicheng Liao, Greg Mori", "title": "Learning Structured Inference Neural Networks with Label Relations", "comments": "Conference on Computer Vision and Pattern Recognition(CVPR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images of scenes have various objects as well as abundant attributes, and\ndiverse levels of visual categorization are possible. A natural image could be\nassigned with fine-grained labels that describe major components,\ncoarse-grained labels that depict high level abstraction or a set of labels\nthat reveal attributes. Such categorization at different concept layers can be\nmodeled with label graphs encoding label information. In this paper, we exploit\nthis rich information with a state-of-art deep learning framework, and propose\na generic structured model that leverages diverse label relations to improve\nimage classification performance. Our approach employs a novel stacked label\nprediction neural network, capturing both inter-level and intra-level label\nsemantics. We evaluate our method on benchmark image datasets, and empirical\nresults illustrate the efficacy of our model.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 23:22:25 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 06:13:16 GMT"}, {"version": "v3", "created": "Fri, 8 Apr 2016 05:04:52 GMT"}, {"version": "v4", "created": "Mon, 24 Oct 2016 18:20:20 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Hu", "Hexiang", ""], ["Zhou", "Guang-Tong", ""], ["Deng", "Zhiwei", ""], ["Liao", "Zicheng", ""], ["Mori", "Greg", ""]]}, {"id": "1511.05622", "submitter": "Yann Dauphin", "authors": "Yann N. Dauphin, David Grangier", "title": "Predicting distributions with Linearizing Belief Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional belief networks introduce stochastic binary variables in neural\nnetworks. Contrary to a classical neural network, a belief network can predict\nmore than the expected value of the output $Y$ given the input $X$. It can\npredict a distribution of outputs $Y$ which is useful when an input can admit\nmultiple outputs whose average is not necessarily a valid answer. Such networks\nare particularly relevant to inverse problems such as image prediction for\ndenoising, or text to speech. However, traditional sigmoid belief networks are\nhard to train and are not suited to continuous problems. This work introduces a\nnew family of networks called linearizing belief nets or LBNs. A LBN decomposes\ninto a deep linear network where each linear unit can be turned on or off by\nnon-deterministic binary latent units. It is a universal approximator of\nreal-valued conditional distributions and can be trained using gradient\ndescent. Moreover, the linear pathways efficiently propagate continuous\ninformation and they act as multiplicative skip-connections that help\noptimization by removing gradient diffusion. This yields a model which trains\nefficiently and improves the state-of-the-art on image denoising and facial\nexpression generation with the Toronto faces dataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 23:50:35 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 00:40:38 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2015 01:45:01 GMT"}, {"version": "v4", "created": "Mon, 2 May 2016 03:22:01 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Dauphin", "Yann N.", ""], ["Grangier", "David", ""]]}, {"id": "1511.05635", "submitter": "Zhibin Liao", "authors": "Zhibin Liao, Gustavo Carneiro", "title": "Competitive Multi-scale Convolution", "comments": null, "journal-ref": "Pattern Recognition 71 (2017), 94-105", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new deep convolutional neural network (ConvNet)\nmodule that promotes competition among a set of multi-scale convolutional\nfilters. This new module is inspired by the inception module, where we replace\nthe original collaborative pooling stage (consisting of a concatenation of the\nmulti-scale filter outputs) by a competitive pooling represented by a maxout\nactivation unit. This extension has the following two objectives: 1) the\nselection of the maximum response among the multi-scale filters prevents filter\nco-adaptation and allows the formation of multiple sub-networks within the same\nmodel, which has been shown to facilitate the training of complex learning\nproblems; and 2) the maxout unit reduces the dimensionality of the outputs from\nthe multi-scale filters. We show that the use of our proposed module in typical\ndeep ConvNets produces classification results that are either better than or\ncomparable to the state of the art on the following benchmark datasets: MNIST,\nCIFAR-10, CIFAR-100 and SVHN.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 01:19:00 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Liao", "Zhibin", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1511.05643", "submitter": "Md Kamrul Hasan", "authors": "Md Kamrul Hasan, Christopher J. Pal", "title": "A New Smooth Approximation to the Zero One Loss with a Probabilistic\n  Interpretation", "comments": "32 pages, 7 figures, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a new form of smooth approximation to the zero one loss in which\nlearning is performed using a reformulation of the widely used logistic\nfunction. Our approach is based on using the posterior mean of a novel\ngeneralized Beta-Bernoulli formulation. This leads to a generalized logistic\nfunction that approximates the zero one loss, but retains a probabilistic\nformulation conferring a number of useful properties. The approach is easily\ngeneralized to kernel logistic regression and easily integrated into methods\nfor structured prediction. We present experiments in which we learn such models\nusing an optimization method consisting of a combination of gradient descent\nand coordinate descent using localized grid search so as to escape from local\nminima. Our experiments indicate that optimization quality is improved when\nlearning meta-parameters are themselves optimized using a validation set. Our\nexperiments show improved performance relative to widely used logistic and\nhinge loss methods on a wide variety of problems ranging from standard UC\nIrvine and libSVM evaluation datasets to product review predictions and a\nvisual information extraction task. We observe that the approach: 1) is more\nrobust to outliers compared to the logistic and hinge losses; 2) outperforms\ncomparable logistic and max margin models on larger scale benchmark problems;\n3) when combined with Gaussian- Laplacian mixture prior on parameters the\nkernelized version of our formulation yields sparser solutions than Support\nVector Machine classifiers; and 4) when integrated into a probabilistic\nstructured prediction technique our approach provides more accurate\nprobabilities yielding improved inference and increasing information extraction\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 02:31:16 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Hasan", "Md Kamrul", ""], ["Pal", "Christopher J.", ""]]}, {"id": "1511.05666", "submitter": "Joan Bruna", "authors": "Joan Bruna, Pablo Sprechmann and Yann LeCun", "title": "Super-Resolution with Deep Convolutional Sufficient Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems in image and audio, and super-resolution in particular, can\nbe seen as high-dimensional structured prediction problems, where the goal is\nto characterize the conditional distribution of a high-resolution output given\nits low-resolution corrupted observation. When the scaling ratio is small,\npoint estimates achieve impressive performance, but soon they suffer from the\nregression-to-the-mean problem, result of their inability to capture the\nmulti-modality of this conditional distribution. Modeling high-dimensional\nimage and audio distributions is a hard task, requiring both the ability to\nmodel complex geometrical structures and textured regions. In this paper, we\npropose to use as conditional model a Gibbs distribution, where its sufficient\nstatistics are given by deep convolutional neural networks. The features\ncomputed by the network are stable to local deformation, and have reduced\nvariance when the input is a stationary texture. These properties imply that\nthe resulting sufficient statistics minimize the uncertainty of the target\nsignals given the degraded observations, while being highly informative. The\nfilters of the CNN are initialized by multiscale complex wavelets, and then we\npropose an algorithm to fine-tune them by estimating the gradient of the\nconditional log-likelihood, which bears some similarities with Generative\nAdversarial Networks. We evaluate experimentally the proposed approach in the\nimage super-resolution task, but the approach is general and could be used in\nother challenging ill-posed problems such as audio bandwidth extension.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 06:24:00 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 19:57:51 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 22:47:52 GMT"}, {"version": "v4", "created": "Tue, 1 Mar 2016 18:26:13 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Bruna", "Joan", ""], ["Sprechmann", "Pablo", ""], ["LeCun", "Yann", ""]]}, {"id": "1511.05676", "submitter": "Aiwen Jiang", "authors": "Aiwen Jiang and Fang Wang and Fatih Porikli and Yi Li", "title": "Compositional Memory for Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) emerges as one of the most fascinating topics\nin computer vision recently. Many state of the art methods naively use holistic\nvisual features with language features into a Long Short-Term Memory (LSTM)\nmodule, neglecting the sophisticated interaction between them. This coarse\nmodeling also blocks the possibilities of exploring finer-grained local\nfeatures that contribute to the question answering dynamically over time.\n  This paper addresses this fundamental problem by directly modeling the\ntemporal dynamics between language and all possible local image patches. When\ntraversing the question words sequentially, our end-to-end approach explicitly\nfuses the features associated to the words and the ones available at multiple\nlocal patches in an attention mechanism, and further combines the fused\ninformation to generate dynamic messages, which we call episode. We then feed\nthe episodes to a standard question answering module together with the\ncontextual visual information and linguistic information. Motivated by recent\npractices in deep learning, we use auxiliary loss functions during training to\nimprove the performance. Our experiments on two latest public datasets suggest\nthat our method has a superior performance. Notably, on the DARQUAR dataset we\nadvanced the state of the art by 6$\\%$, and we also evaluated our approach on\nthe most recent MSCOCO-VQA dataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 07:25:16 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Jiang", "Aiwen", ""], ["Wang", "Fang", ""], ["Porikli", "Fatih", ""], ["Li", "Yi", ""]]}, {"id": "1511.05756", "submitter": "Hyeonwoo Noh", "authors": "Hyeonwoo Noh, Paul Hongsuck Seo, Bohyung Han", "title": "Image Question Answering using Convolutional Neural Network with Dynamic\n  Parameter Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle image question answering (ImageQA) problem by learning a\nconvolutional neural network (CNN) with a dynamic parameter layer whose weights\nare determined adaptively based on questions. For the adaptive parameter\nprediction, we employ a separate parameter prediction network, which consists\nof gated recurrent unit (GRU) taking a question as its input and a\nfully-connected layer generating a set of candidate weights as its output.\nHowever, it is challenging to construct a parameter prediction network for a\nlarge number of parameters in the fully-connected dynamic parameter layer of\nthe CNN. We reduce the complexity of this problem by incorporating a hashing\ntechnique, where the candidate weights given by the parameter prediction\nnetwork are selected using a predefined hash function to determine individual\nweights in the dynamic parameter layer. The proposed network---joint network\nwith the CNN for ImageQA and the parameter prediction network---is trained\nend-to-end through back-propagation, where its weights are initialized using a\npre-trained CNN and GRU. The proposed algorithm illustrates the\nstate-of-the-art performance on all available public ImageQA benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 12:30:57 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Noh", "Hyeonwoo", ""], ["Seo", "Paul Hongsuck", ""], ["Han", "Bohyung", ""]]}, {"id": "1511.05768", "submitter": "Andreas Bulling", "authors": "Marc Tonsen, Xucong Zhang, Yusuke Sugano, Andreas Bulling", "title": "Labeled pupils in the wild: A dataset for studying pupil detection in\n  unconstrained environments", "comments": null, "journal-ref": null, "doi": "10.1145/2857491.2857520", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present labelled pupils in the wild (LPW), a novel dataset of 66\nhigh-quality, high-speed eye region videos for the development and evaluation\nof pupil detection algorithms. The videos in our dataset were recorded from 22\nparticipants in everyday locations at about 95 FPS using a state-of-the-art\ndark-pupil head-mounted eye tracker. They cover people with different\nethnicities, a diverse set of everyday indoor and outdoor illumination\nenvironments, as well as natural gaze direction distributions. The dataset also\nincludes participants wearing glasses, contact lenses, as well as make-up. We\nbenchmark five state-of-the-art pupil detection algorithms on our dataset with\nrespect to robustness and accuracy. We further study the influence of image\nresolution, vision aids, as well as recording location (indoor, outdoor) on\npupil detection performance. Our evaluations provide valuable insights into the\ngeneral pupil detection problem and allow us to identify key challenges for\nrobust pupil detection on head-mounted eye trackers.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 13:30:55 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Tonsen", "Marc", ""], ["Zhang", "Xucong", ""], ["Sugano", "Yusuke", ""], ["Bulling", "Andreas", ""]]}, {"id": "1511.05788", "submitter": "Michael Edwards", "authors": "Michael Edwards and Jingjing Deng and Xianghua Xie", "title": "From Pose to Activity: Surveying Datasets and Introducing CONVERSE", "comments": "Presentation of pose-based conversational human interaction dataset,\n  review of current appearance and depth based action recognition datasets,\n  public dataset, 38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a review on the current state of publicly available datasets\nwithin the human action recognition community; highlighting the revival of pose\nbased methods and recent progress of understanding person-person interaction\nmodeling. We categorize datasets regarding several key properties for usage as\na benchmark dataset; including the number of class labels, ground truths\nprovided, and application domain they occupy. We also consider the level of\nabstraction of each dataset; grouping those that present actions, interactions\nand higher level semantic activities. The survey identifies key appearance and\npose based datasets, noting a tendency for simplistic, emphasized, or scripted\naction classes that are often readily definable by a stable collection of\nsub-action gestures. There is a clear lack of datasets that provide closely\nrelated actions, those that are not implicitly identified via a series of poses\nand gestures, but rather a dynamic set of interactions. We therefore propose a\nnovel dataset that represents complex conversational interactions between two\nindividuals via 3D pose. 8 pairwise interactions describing 7 separate\nconversation based scenarios were collected using two Kinect depth sensors. The\nintention is to provide events that are constructed from numerous primitive\nactions, interactions and motions, over a period of time; providing a set of\nsubtle action classes that are more representative of the real world, and a\nchallenge to currently developed recognition methodologies. We believe this is\namong one of the first datasets devoted to conversational interaction\nclassification using 3D pose features and the attributed papers show this task\nis indeed possible. The full dataset is made publicly available to the research\ncommunity at www.csvision.swansea.ac.uk/converse.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 14:03:55 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 13:04:09 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Edwards", "Michael", ""], ["Deng", "Jingjing", ""], ["Xie", "Xianghua", ""]]}, {"id": "1511.05846", "submitter": "Stratis Tzoumas", "authors": "Stratis Tzoumas, Antonio Nunes, Ivan Olefir, Stefan Stangl, Panagiotis\n  Symvoulidis, Sarah Glasl, Christine Bayer, Gabriele Multhoff, Vasilis\n  Ntziachristos", "title": "Eigenspectra optoacoustic tomography achieves quantitative blood\n  oxygenation imaging deep in tissues", "comments": null, "journal-ref": null, "doi": "10.1038/ncomms12121", "report-no": null, "categories": "physics.med-ph cs.CV physics.optics q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light propagating in tissue attains a spectrum that varies with location due\nto wavelength-dependent fluence attenuation by tissue optical properties, an\neffect that causes spectral corruption. Predictions of the spectral variations\nof light fluence in tissue are challenging since the spatial distribution of\noptical properties in tissue cannot be resolved in high resolution or with high\naccuracy by current methods. Spectral corruption has fundamentally limited the\nquantification accuracy of optical and optoacoustic methods and impeded the\nlong sought-after goal of imaging blood oxygen saturation (sO2) deep in\ntissues; a critical but still unattainable target for the assessment of\noxygenation in physiological processes and disease. We discover a new principle\nunderlying light fluence in tissues, which describes the wavelength dependence\nof light fluence as an affine function of a few reference base spectra,\nindependently of the specific distribution of tissue optical properties. This\nfinding enables the introduction of a previously undocumented concept termed\neigenspectra Multispectral Optoacoustic Tomography (eMSOT) that can effectively\naccount for wavelength dependent light attenuation without explicit knowledge\nof the tissue optical properties. We validate eMSOT in more than 2000\nsimulations and with phantom and animal measurements. We find that eMSOT can\nquantitatively image tissue sO2 reaching in many occasions a better than\n10-fold improved accuracy over conventional spectral optoacoustic methods.\nThen, we show that eMSOT can spatially resolve sO2 in muscle and tumor;\nrevealing so far unattainable tissue physiology patterns. Last, we related\neMSOT readings to cancer hypoxia and found congruence between eMSOT tumor sO2\nimages and tissue perfusion and hypoxia maps obtained by correlative\nhistological analysis.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 15:56:13 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Tzoumas", "Stratis", ""], ["Nunes", "Antonio", ""], ["Olefir", "Ivan", ""], ["Stangl", "Stefan", ""], ["Symvoulidis", "Panagiotis", ""], ["Glasl", "Sarah", ""], ["Bayer", "Christine", ""], ["Multhoff", "Gabriele", ""], ["Ntziachristos", "Vasilis", ""]]}, {"id": "1511.05879", "submitter": "Giorgos Tolias", "authors": "Giorgos Tolias, Ronan Sicre and Herv\\'e J\\'egou", "title": "Particular object retrieval with integral max-pooling of CNN activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, image representation built upon Convolutional Neural Network (CNN)\nhas been shown to provide effective descriptors for image search, outperforming\npre-CNN features as short-vector representations. Yet such models are not\ncompatible with geometry-aware re-ranking methods and still outperformed, on\nsome particular object retrieval benchmarks, by traditional image search\nsystems relying on precise descriptor matching, geometric re-ranking, or query\nexpansion. This work revisits both retrieval stages, namely initial search and\nre-ranking, by employing the same primitive information derived from the CNN.\nWe build compact feature vectors that encode several image regions without the\nneed to feed multiple inputs to the network. Furthermore, we extend integral\nimages to handle max-pooling on convolutional layer activations, allowing us to\nefficiently localize matching objects. The resulting bounding box is finally\nused for image re-ranking. As a result, this paper significantly improves\nexisting CNN-based recognition pipeline: We report for the first time results\ncompeting with traditional methods on the challenging Oxford5k and Paris6k\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 17:02:59 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 15:14:34 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Tolias", "Giorgos", ""], ["Sicre", "Ronan", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1511.05904", "submitter": "Lingyu Wei", "authors": "Lingyu Wei, Qixing Huang, Duygu Ceylan, Etienne Vouga, Hao Li", "title": "Dense Human Body Correspondences Using Convolutional Networks", "comments": "CVPR 2016 oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning approach for finding dense correspondences between\n3D scans of people. Our method requires only partial geometric information in\nthe form of two depth maps or partial reconstructed surfaces, works for humans\nin arbitrary poses and wearing any clothing, does not require the two people to\nbe scanned from similar viewpoints, and runs in real time. We use a deep\nconvolutional neural network to train a feature descriptor on depth map pixels,\nbut crucially, rather than training the network to solve the shape\ncorrespondence problem directly, we train it to solve a body region\nclassification problem, modified to increase the smoothness of the learned\ndescriptors near region boundaries. This approach ensures that nearby points on\nthe human body are nearby in feature space, and vice versa, rendering the\nfeature descriptor suitable for computing dense correspondences between the\nscans. We validate our method on real and synthetic data for both clothed and\nunclothed humans, and show that our correspondences are more robust than is\npossible with state-of-the-art unsupervised methods, and more accurate than\nthose found using methods that require full watertight 3D geometry.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 18:36:54 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2016 01:06:43 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Wei", "Lingyu", ""], ["Huang", "Qixing", ""], ["Ceylan", "Duygu", ""], ["Vouga", "Etienne", ""], ["Li", "Hao", ""]]}, {"id": "1511.05914", "submitter": "Daniel Barrett", "authors": "Daniel Paul Barrett and Ran Xu and Haonan Yu and Jeffrey Mark Siskind", "title": "Collecting and Annotating the Large Continuous Action Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We make available to the community a new dataset to support\naction-recognition research. This dataset is different from prior datasets in\nseveral key ways. It is significantly larger. It contains streaming video with\nlong segments containing multiple action occurrences that often overlap in\nspace and/or time. All actions were filmed in the same collection of\nbackgrounds so that background gives little clue as to action class. We had\nfive humans replicate the annotation of temporal extent of action occurrences\nlabeled with their class and measured a surprisingly low level of intercoder\nagreement. A baseline experiment shows that recent state-of-the-art methods\nperform poorly on this dataset. This suggests that this will be a challenging\ndataset to foster advances in action-recognition research. This manuscript\nserves to describe the novel content and characteristics of the LCA dataset,\npresent the design decisions made when filming the dataset, and document the\nnovel methods employed to annotate the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 19:16:58 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Barrett", "Daniel Paul", ""], ["Xu", "Ran", ""], ["Yu", "Haonan", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1511.05960", "submitter": "Kan Chen", "authors": "Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, Ram\n  Nevatia", "title": "ABC-CNN: An Attention Based Convolutional Neural Network for Visual\n  Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel attention based deep learning architecture for visual\nquestion answering task (VQA). Given an image and an image related natural\nlanguage question, VQA generates the natural language answer for the question.\nGenerating the correct answers requires the model's attention to focus on the\nregions corresponding to the question, because different questions inquire\nabout the attributes of different image regions. We introduce an attention\nbased configurable convolutional neural network (ABC-CNN) to learn such\nquestion-guided attention. ABC-CNN determines an attention map for an\nimage-question pair by convolving the image feature map with configurable\nconvolutional kernels derived from the question's semantics. We evaluate the\nABC-CNN architecture on three benchmark VQA datasets: Toronto COCO-QA, DAQUAR,\nand VQA dataset. ABC-CNN model achieves significant improvements over\nstate-of-the-art methods on these datasets. The question-guided attention\ngenerated by ABC-CNN is also shown to reflect the regions that are highly\nrelevant to the questions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 20:59:50 GMT"}, {"version": "v2", "created": "Sun, 3 Apr 2016 22:47:38 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Chen", "Kan", ""], ["Wang", "Jiang", ""], ["Chen", "Liang-Chieh", ""], ["Gao", "Haoyuan", ""], ["Xu", "Wei", ""], ["Nevatia", "Ram", ""]]}, {"id": "1511.06015", "submitter": "Juan C. Caicedo", "authors": "Juan C. Caicedo and Svetlana Lazebnik", "title": "Active Object Localization with Deep Reinforcement Learning", "comments": "IEEE ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an active detection model for localizing objects in scenes. The\nmodel is class-specific and allows an agent to focus attention on candidate\nregions for identifying the correct location of a target object. This agent\nlearns to deform a bounding box using simple transformation actions, with the\ngoal of determining the most specific location of target objects following\ntop-down reasoning. The proposed localization agent is trained using deep\nreinforcement learning, and evaluated on the Pascal VOC 2007 dataset. We show\nthat agents guided by the proposed model are able to localize a single instance\nof an object after analyzing only between 11 and 25 regions in an image, and\nobtain the best detection results among systems that do not use object\nproposals for object localization.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 22:55:46 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Caicedo", "Juan C.", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1511.06036", "submitter": "Masayuki Ohzeki", "authors": "Masayuki Ohzeki", "title": "Stochastic gradient method with accelerated stochastic dynamics", "comments": "12 pages, proceedings for International Meeting on High-Dimensional\n  Data Driven Science (HD3-2015)\n  (http://www.sparse-modeling.jp/HD3-2015/index_e.html)", "journal-ref": null, "doi": "10.1088/1742-6596/699/1/012019", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel technique to implement stochastic gradient\nmethods, which are beneficial for learning from large datasets, through\naccelerated stochastic dynamics. A stochastic gradient method is based on\nmini-batch learning for reducing the computational cost when the amount of data\nis large. The stochasticity of the gradient can be mitigated by the injection\nof Gaussian noise, which yields the stochastic Langevin gradient method; this\nmethod can be used for Bayesian posterior sampling. However, the performance of\nthe stochastic Langevin gradient method depends on the mixing rate of the\nstochastic dynamics. In this study, we propose violating the detailed balance\ncondition to enhance the mixing rate. Recent studies have revealed that\nviolating the detailed balance condition accelerates the convergence to a\nstationary state and reduces the correlation time between the samplings. We\nimplement this violation of the detailed balance condition in the stochastic\ngradient Langevin method and test our method for a simple model to demonstrate\nits performance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 01:01:59 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Ohzeki", "Masayuki", ""]]}, {"id": "1511.06040", "submitter": "Srikanth Muralidharan", "authors": "Moustafa Ibrahim, Srikanth Muralidharan, Zhiwei Deng, Arash Vahdat,\n  Greg Mori", "title": "A Hierarchical Deep Temporal Model for Group Activity Recognition", "comments": "cs.cv Accepted to CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In group activity recognition, the temporal dynamics of the whole activity\ncan be inferred based on the dynamics of the individual people representing the\nactivity. We build a deep model to capture these dynamics based on LSTM\n(long-short term memory) models. To make use of these ob- servations, we\npresent a 2-stage deep temporal model for the group activity recognition\nproblem. In our model, a LSTM model is designed to represent action dynamics of\nin- dividual people in a sequence and another LSTM model is designed to\naggregate human-level information for whole activity understanding. We evaluate\nour model over two datasets: the collective activity dataset and a new volley-\nball dataset. Experimental results demonstrate that our proposed model improves\ngroup activity recognition perfor- mance with compared to baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 01:33:35 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 20:43:53 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Ibrahim", "Moustafa", ""], ["Muralidharan", "Srikanth", ""], ["Deng", "Zhiwei", ""], ["Vahdat", "Arash", ""], ["Mori", "Greg", ""]]}, {"id": "1511.06049", "submitter": "Deyu Meng", "authors": "Deyu Meng and Qian Zhao and Lu Jiang", "title": "What Objective Does Self-paced Learning Indeed Optimize?", "comments": "25 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Self-paced learning (SPL) is a recently raised methodology designed through\nsimulating the learning principle of humans/animals. A variety of SPL\nrealization schemes have been designed for different computer vision and\npattern recognition tasks, and empirically substantiated to be effective in\nthese applications. However, the investigation on its theoretical insight is\nstill a blank. To this issue, this study attempts to provide some new\ntheoretical understanding under the SPL scheme. Specifically, we prove that the\nsolving strategy on SPL accords with a majorization minimization algorithm\nimplemented on a latent objective function. Furthermore, we find that the loss\nfunction contained in this latent objective has a similar configuration with\nnon-convex regularized penalty (NSPR) known in statistics and machine learning.\nSuch connection inspires us discovering more intrinsic relationship between SPL\nregimes and NSPR forms, like SCAD, LOG and EXP. The robustness insight under\nSPL can then be finely explained. We also analyze the capability of SPL on its\neasy loss prior embedding property, and provide an insightful interpretation to\nthe effectiveness mechanism under previous SPL variations. Besides, we design a\ngroup-partial-order loss prior, which is especially useful to weakly labeled\nlarge-scale data processing tasks. Through applying SPL with this loss prior to\nthe FCVID dataset, which is currently one of the biggest manually annotated\nvideo dataset, our method achieves state-of-the-art performance beyond previous\nmethods, which further helps supports the proposed theoretical arguments.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 02:55:18 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 13:59:27 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Meng", "Deyu", ""], ["Zhao", "Qian", ""], ["Jiang", "Lu", ""]]}, {"id": "1511.06062", "submitter": "Yang Gao", "authors": "Yang Gao, Oscar Beijbom, Ning Zhang, Trevor Darrell", "title": "Compact Bilinear Pooling", "comments": "Camera ready version for CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilinear models has been shown to achieve impressive performance on a wide\nrange of visual tasks, such as semantic segmentation, fine grained recognition\nand face recognition. However, bilinear features are high dimensional,\ntypically on the order of hundreds of thousands to a few million, which makes\nthem impractical for subsequent analysis. We propose two compact bilinear\nrepresentations with the same discriminative power as the full bilinear\nrepresentation but with only a few thousand dimensions. Our compact\nrepresentations allow back-propagation of classification errors enabling an\nend-to-end optimization of the visual recognition system. The compact bilinear\nrepresentations are derived through a novel kernelized analysis of bilinear\npooling which provide insights into the discriminative power of bilinear\npooling, and a platform for further research in compact pooling methods.\nExperimentation illustrate the utility of the proposed representations for\nimage classification and few-shot learning across several datasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 05:34:35 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 01:59:15 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Gao", "Yang", ""], ["Beijbom", "Oscar", ""], ["Zhang", "Ning", ""], ["Darrell", "Trevor", ""]]}, {"id": "1511.06065", "submitter": "Yang Gao", "authors": "Yang Gao, Lisa Anne Hendricks, Katherine J. Kuchenbecker, Trevor\n  Darrell", "title": "Deep Learning for Tactile Understanding From Visual and Haptic Data", "comments": "Camera ready version for ICRA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots which interact with the physical world will benefit from a\nfine-grained tactile understanding of objects and surfaces. Additionally, for\ncertain tasks, robots may need to know the haptic properties of an object\nbefore touching it. To enable better tactile understanding for robots, we\npropose a method of classifying surfaces with haptic adjectives (e.g.,\ncompressible or smooth) from both visual and physical interaction data. Humans\ntypically combine visual predictions and feedback from physical interactions to\naccurately predict haptic properties and interact with the world. Inspired by\nthis cognitive pattern, we propose and explore a purely visual haptic\nprediction model. Purely visual models enable a robot to \"feel\" without\nphysical interaction. Furthermore, we demonstrate that using both visual and\nphysical interaction signals together yields more accurate haptic\nclassification. Our models take advantage of recent advances in deep neural\nnetworks by employing a unified approach to learning features for physical\ninteraction and visual observations. Even though we employ little domain\nspecific knowledge, our model still achieves better results than methods based\non hand-designed features.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 05:52:15 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 00:16:21 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Gao", "Yang", ""], ["Hendricks", "Lisa Anne", ""], ["Kuchenbecker", "Katherine J.", ""], ["Darrell", "Trevor", ""]]}, {"id": "1511.06067", "submitter": "Cheng Tai", "authors": "Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, Weinan E", "title": "Convolutional neural networks with low-rank regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large CNNs have delivered impressive performance in various computer vision\napplications. But the storage and computation requirements make it problematic\nfor deploying these models on mobile devices. Recently, tensor decompositions\nhave been used for speeding up CNNs. In this paper, we further develop the\ntensor decomposition technique. We propose a new algorithm for computing the\nlow-rank tensor decomposition for removing the redundancy in the convolution\nkernels. The algorithm finds the exact global optimizer of the decomposition\nand is more effective than iterative methods. Based on the decomposition, we\nfurther propose a new method for training low-rank constrained CNNs from\nscratch. Interestingly, while achieving a significant speedup, sometimes the\nlow-rank constrained CNNs delivers significantly better performance than their\nnon-constrained counterparts. On the CIFAR-10 dataset, the proposed low-rank\nNIN model achieves $91.31\\%$ accuracy (without data augmentation), which also\nimproves upon state-of-the-art result. We evaluated the proposed method on\nCIFAR-10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet,\nNIN, VGG and GoogleNet with success. For example, the forward time of VGG-16 is\nreduced by half while the performance is still comparable. Empirical success\nsuggests that low-rank tensor decompositions can be a very useful tool for\nspeeding up large CNNs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 06:13:55 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 23:46:17 GMT"}, {"version": "v3", "created": "Sun, 14 Feb 2016 03:46:09 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Tai", "Cheng", ""], ["Xiao", "Tong", ""], ["Zhang", "Yi", ""], ["Wang", "Xiaogang", ""], ["E", "Weinan", ""]]}, {"id": "1511.06070", "submitter": "Miaomiao Liu", "authors": "Miaomiao Liu, Mathieu Salzmann, Xuming He", "title": "Structured Depth Prediction in Challenging Monocular Video Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we tackle the problem of estimating the depth of a scene from\na monocular video sequence. In particular, we handle challenging scenarios,\nsuch as non-translational camera motion and dynamic scenes, where traditional\nstructure from motion and motion stereo methods do not apply. To this end, we\nfirst study the problem of depth estimation from a single image. In this\ncontext, we exploit the availability of a pool of images for which the depth is\nknown, and formulate monocular depth estimation as a discrete-continuous\noptimization problem, where the continuous variables encode the depth of the\nsuperpixels in the input image, and the discrete ones represent relationships\nbetween neighboring superpixels. The solution to this discrete-continuous\noptimization problem is obtained by performing inference in a graphical model\nusing particle belief propagation. To handle video sequences, we then extend\nour single image model to a two-frame one that naturally encodes short-range\ntemporal consistency and inherently handles dynamic objects. Based on the\nprediction of this model, we then introduce a fully-connected pairwise CRF that\naccounts for longer range spatio-temporal interactions throughout a video. We\ndemonstrate the effectiveness of our model in both the indoor and outdoor\nscenarios.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 06:42:45 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Liu", "Miaomiao", ""], ["Salzmann", "Mathieu", ""], ["He", "Xuming", ""]]}, {"id": "1511.06078", "submitter": "Liwei Wang", "authors": "Liwei Wang, Yin Li, Svetlana Lazebnik", "title": "Learning Deep Structure-Preserving Image-Text Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for learning joint embeddings of images and text\nusing a two-branch neural network with multiple layers of linear projections\nfollowed by nonlinearities. The network is trained using a large margin\nobjective that combines cross-view ranking constraints with within-view\nneighborhood structure preservation constraints inspired by metric learning\nliterature. Extensive experiments show that our approach gains significant\nimprovements in accuracy for image-to-text and text-to-image retrieval. Our\nmethod achieves new state-of-the-art results on the Flickr30K and MSCOCO\nimage-sentence datasets and shows promise on the new task of phrase\nlocalization on the Flickr30K Entities dataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 07:17:49 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 03:10:04 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Wang", "Liwei", ""], ["Li", "Yin", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1511.06085", "submitter": "George Toderici", "authors": "George Toderici, Sean M. O'Malley, Sung Jin Hwang, Damien Vincent,\n  David Minnen, Shumeet Baluja, Michele Covell, Rahul Sukthankar", "title": "Variable Rate Image Compression with Recurrent Neural Networks", "comments": "Under review as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large fraction of Internet traffic is now driven by requests from mobile\ndevices with relatively small screens and often stringent bandwidth\nrequirements. Due to these factors, it has become the norm for modern\ngraphics-heavy websites to transmit low-resolution, low-bytecount image\npreviews (thumbnails) as part of the initial page load process to improve\napparent page responsiveness. Increasing thumbnail compression beyond the\ncapabilities of existing codecs is therefore a current research focus, as any\nbyte savings will significantly enhance the experience of mobile device users.\nToward this end, we propose a general framework for variable-rate image\ncompression and a novel architecture based on convolutional and deconvolutional\nLSTM recurrent networks. Our models address the main issues that have prevented\nautoencoder neural networks from competing with existing image compression\nalgorithms: (1) our networks only need to be trained once (not per-image),\nregardless of input image dimensions and the desired compression rate; (2) our\nnetworks are progressive, meaning that the more bits are sent, the more\naccurate the image reconstruction; and (3) the proposed architecture is at\nleast as efficient as a standard purpose-trained autoencoder for a given number\nof bits. On a large-scale benchmark of 32$\\times$32 thumbnails, our LSTM-based\napproaches provide better visual quality than (headerless) JPEG, JPEG2000 and\nWebP, with a storage size that is reduced by 10% or more.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 07:50:46 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 01:44:51 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 02:43:40 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2016 20:57:42 GMT"}, {"version": "v5", "created": "Tue, 1 Mar 2016 22:13:44 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Toderici", "George", ""], ["O'Malley", "Sean M.", ""], ["Hwang", "Sung Jin", ""], ["Vincent", "Damien", ""], ["Minnen", "David", ""], ["Baluja", "Shumeet", ""], ["Covell", "Michele", ""], ["Sukthankar", "Rahul", ""]]}, {"id": "1511.06103", "submitter": "Pierre Baque", "authors": "Pierre Baqu\\'e, Timur Bagautdinov, Fran\\c{c}ois Fleuret and Pascal Fua", "title": "Principled Parallel Mean-Field Inference for Discrete Random Fields", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean-field variational inference is one of the most popular approaches to\ninference in discrete random fields. Standard mean-field optimization is based\non coordinate descent and in many situations can be impractical. Thus, in\npractice, various parallel techniques are used, which either rely on ad-hoc\nsmoothing with heuristically set parameters, or put strong constraints on the\ntype of models. In this paper, we propose a novel proximal gradient-based\napproach to optimizing the variational objective. It is naturally\nparallelizable and easy to implement. We prove its convergence, and then\ndemonstrate that, in practice, it yields faster convergence and often finds\nbetter optima than more traditional mean-field optimization techniques.\nMoreover, our method is less sensitive to the choice of parameters.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 09:44:20 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 10:26:03 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Baqu\u00e9", "Pierre", ""], ["Bagautdinov", "Timur", ""], ["Fleuret", "Fran\u00e7ois", ""], ["Fua", "Pascal", ""]]}, {"id": "1511.06104", "submitter": "Sheng-Yi Bai", "authors": "Sheng-Yi Bai, Sebastian Agethen, Ting-Hsuan Chao, Winston Hsu", "title": "Semi-supervised Learning for Convolutional Neural Networks via Online\n  Graph Construction", "comments": "As the original submission of iclr is withdrawn, the arxiv submission\n  should be withdrawn as well", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent promising achievements of deep learning rely on the large amount\nof labeled data. Considering the abundance of data on the web, most of them do\nnot have labels at all. Therefore, it is important to improve generalization\nperformance using unlabeled data on supervised tasks with few labeled\ninstances. In this work, we revisit graph-based semi-supervised learning\nalgorithms and propose an online graph construction technique which suits deep\nconvolutional neural network better. We consider an EM-like algorithm for\nsemi-supervised learning on deep neural networks: In forward pass, the graph is\nconstructed based on the network output, and the graph is then used for loss\ncalculation to help update the network by back propagation in the backward\npass. We demonstrate the strength of our online approach compared to the\nconventional ones whose graph is constructed on static but not robust enough\nfeature representations beforehand.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 09:44:57 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 00:56:08 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Bai", "Sheng-Yi", ""], ["Agethen", "Sebastian", ""], ["Chao", "Ting-Hsuan", ""], ["Hsu", "Winston", ""]]}, {"id": "1511.06106", "submitter": "Ting Peng", "authors": "Ting Peng, Aiping Qu, Xiaoling Wang", "title": "Quantitative Analysis of Particles Segregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segregation is a popular phenomenon. It has considerable effects on material\nperformance. To the author's knowledge, there is still no automated objective\nquantitative indicator for segregation. In order to full fill this task,\nsegregation of particles is analyzed. Edges of the particles are extracted from\nthe digital picture. Then, the whole picture of particles is splintered to\nsmall rectangles with the same shape. Statistical index of the edges in each\nrectangle is calculated. Accordingly, segregation between the indexes\ncorresponding to the rectangles is evaluated. The results show coincident with\nsubjective evaluated results. Further more, it can be implemented as an\nautomated system, which would facilitate the materials quality control\nmechanism during production process.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 09:57:14 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 03:09:58 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Peng", "Ting", ""], ["Qu", "Aiping", ""], ["Wang", "Xiaoling", ""]]}, {"id": "1511.06147", "submitter": "Abhimanyu Dubey", "authors": "Abhimanyu Dubey, Nikhil Naik, Dan Raviv, Rahul Sukthankar and Ramesh\n  Raskar", "title": "Coreset-Based Adaptive Tracking", "comments": "8 pages, 5 figures, In submission to IEEE TPAMI (Transactions on\n  Pattern Analysis and Machine Intelligence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for learning from streaming visual data using a compact,\nconstant size representation of all the data that was seen until a given\nmoment. Specifically, we construct a 'coreset' representation of streaming data\nusing a parallelized algorithm, which is an approximation of a set with\nrelation to the squared distances between this set and all other points in its\nambient space. We learn an adaptive object appearance model from the coreset\ntree in constant time and logarithmic space and use it for object tracking by\ndetection. Our method obtains excellent results for object tracking on three\nstandard datasets over more than 100 videos. The ability to summarize data\nefficiently makes our method ideally suited for tracking in long videos in\npresence of space and time constraints. We demonstrate this ability by\noutperforming a variety of algorithms on the TLD dataset with 2685 frames on\naverage. This coreset based learning approach can be applied for both real-time\nlearning of small, varied data and fast learning of big data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 12:59:20 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Dubey", "Abhimanyu", ""], ["Naik", "Nikhil", ""], ["Raviv", "Dan", ""], ["Sukthankar", "Rahul", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1511.06181", "submitter": "Andrii Maksai", "authors": "Andrii Maksai, Xinchao Wang, Pascal Fua", "title": "What Players do with the Ball: A Physically Constrained Interaction\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking the ball is critical for video-based analysis of team sports.\nHowever, it is difficult, especially in low-resolution images, due to the small\nsize of the ball, its speed that creates motion blur, and its often being\noccluded by players. In this paper, we propose a generic and principled\napproach to modeling the interaction between the ball and the players while\nalso imposing appropriate physical constraints on the ball's trajectory. We\nshow that our approach, formulated in terms of a Mixed Integer Program, is more\nrobust and more accurate than several state-of-the-art approaches on real-life\nvolleyball, basketball, and soccer sequences.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 14:25:04 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 10:56:34 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Maksai", "Andrii", ""], ["Wang", "Xinchao", ""], ["Fua", "Pascal", ""]]}, {"id": "1511.06214", "submitter": "Paul Henderson", "authors": "Paul Henderson, Vittorio Ferrari", "title": "Automatically selecting inference algorithms for discrete energy\n  minimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimisation of discrete energies defined over factors is an important\nproblem in computer vision, and a vast number of MAP inference algorithms have\nbeen proposed. Different inference algorithms perform better on factor graph\nmodels (GMs) from different underlying problem classes, and in general it is\ndifficult to know which algorithm will yield the lowest energy for a given GM.\nTo mitigate this difficulty, survey papers advise the practitioner on what\nalgorithms perform well on what classes of models. We take the next step\nforward, and present a technique to automatically select the best inference\nalgorithm for an input GM. We validate our method experimentally on an extended\nversion of the OpenGM2 benchmark, containing a diverse set of vision problems.\nOn average, our method selects an inference algorithm yielding labellings with\n96% of variables the same as the best available algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 15:45:02 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 16:56:57 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Henderson", "Paul", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1511.06233", "submitter": "Abhijit Bendale", "authors": "Abhijit Bendale, Terrance Boult", "title": "Towards Open Set Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have produced significant gains for various visual recognition\nproblems, leading to high impact academic and commercial applications. Recent\nwork in deep networks highlighted that it is easy to generate images that\nhumans would never classify as a particular object class, yet networks classify\nsuch images high confidence as that given class - deep network are easily\nfooled with images humans do not consider meaningful. The closed set nature of\ndeep networks forces them to choose from one of the known classes leading to\nsuch artifacts. Recognition in the real world is open set, i.e. the recognition\nsystem should reject unknown/unseen classes at test time. We present a\nmethodology to adapt deep networks for open set recognition, by introducing a\nnew model layer, OpenMax, which estimates the probability of an input being\nfrom an unknown class. A key element of estimating the unknown probability is\nadapting Meta-Recognition concepts to the activation patterns in the\npenultimate layer of the network. OpenMax allows rejection of \"fooling\" and\nunrelated open set images presented to the system; OpenMax greatly reduces the\nnumber of obvious errors made by a deep network. We prove that the OpenMax\nconcept provides bounded open space risk, thereby formally providing an open\nset recognition solution. We evaluate the resulting open set deep networks\nusing pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation\ndata, and thousands of fooling and open set images. The proposed OpenMax model\nsignificantly outperforms open set recognition accuracy of basic deep networks\nas well as deep networks with thresholding of SoftMax probabilities.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 16:13:55 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Bendale", "Abhijit", ""], ["Boult", "Terrance", ""]]}, {"id": "1511.06238", "submitter": "Miriam Cha", "authors": "Miriam Cha, Youngjune Gwon, H.T. Kung", "title": "Multimodal sparse representation learning and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised methods have proven effective for discriminative tasks in a\nsingle-modality scenario. In this paper, we present a multimodal framework for\nlearning sparse representations that can capture semantic correlation between\nmodalities. The framework can model relationships at a higher level by forcing\nthe shared sparse representation. In particular, we propose the use of joint\ndictionary learning technique for sparse coding and formulate the joint\nrepresentation for concision, cross-modal representations (in case of a missing\nmodality), and union of the cross-modal representations. Given the accelerated\ngrowth of multimodal data posted on the Web such as YouTube, Wikipedia, and\nTwitter, learning good multimodal features is becoming increasingly important.\nWe show that the shared representations enabled by our framework substantially\nimprove the classification performance under both unimodal and multimodal\nsettings. We further show how deep architectures built on the proposed\nframework are effective for the case of highly nonlinear correlations between\nmodalities. The effectiveness of our approach is demonstrated experimentally in\nimage denoising, multimedia event detection and retrieval on the TRECVID\ndataset (audio-video), category classification on the Wikipedia dataset\n(image-text), and sentiment classification on PhotoTweet (image-text).\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 16:26:24 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2016 23:18:09 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2016 19:22:48 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Cha", "Miriam", ""], ["Gwon", "Youngjune", ""], ["Kung", "H. T.", ""]]}, {"id": "1511.06241", "submitter": "Aysegul Dundar", "authors": "Aysegul Dundar, Jonghoon Jin and Eugenio Culurciello", "title": "Convolutional Clustering for Unsupervised Learning", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of labeling data for training deep neural networks is daunting and\ntedious, requiring millions of labels to achieve the current state-of-the-art\nresults. Such reliance on large amounts of labeled data can be relaxed by\nexploiting hierarchical features via unsupervised learning techniques. In this\nwork, we propose to train a deep convolutional network based on an enhanced\nversion of the k-means clustering algorithm, which reduces the number of\ncorrelated parameters in the form of similar filters, and thus increases test\ncategorization accuracy. We call our algorithm convolutional k-means\nclustering. We further show that learning the connection between the layers of\na deep convolutional neural network improves its ability to be trained on a\nsmaller amount of labeled data. Our experiments show that the proposed\nalgorithm outperforms other techniques that learn filters unsupervised.\nSpecifically, we obtained a test accuracy of 74.1% on STL-10 and a test error\nof 0.5% on MNIST.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 16:31:46 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 16:46:53 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Dundar", "Aysegul", ""], ["Jin", "Jonghoon", ""], ["Culurciello", "Eugenio", ""]]}, {"id": "1511.06276", "submitter": "Saurabh Sihag", "authors": "Saurabh Sihag and Pranab Kumar Dutta", "title": "Faster method for Deep Belief Network based Object classification using\n  DWT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Deep Belief Network (DBN) requires large, multiple hidden layers with high\nnumber of hidden units to learn good features from the raw pixels of large\nimages. This implies more training time as well as computational complexity. By\nintegrating DBN with Discrete Wavelet Transform (DWT), both training time and\ncomputational complexity can be reduced. The low resolution images obtained\nafter application of DWT are used to train multiple DBNs. The results obtained\nfrom these DBNs are combined using a weighted voting algorithm. The performance\nof this method is found to be competent and faster in comparison with that of\ntraditional DBNs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 17:41:08 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Sihag", "Saurabh", ""], ["Dutta", "Pranab Kumar", ""]]}, {"id": "1511.06281", "submitter": "Johannes Ball\\'e", "authors": "Johannes Ball\\'e and Valero Laparra and Eero P. Simoncelli", "title": "Density Modeling of Images using a Generalized Normalization\n  Transformation", "comments": "published as a conference paper at ICLR 2016", "journal-ref": "Int'l Conf on Learning Representations (ICLR), San Juan, Puerto\n  Rico, May 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a parametric nonlinear transformation that is well-suited for\nGaussianizing data from natural images. The data are linearly transformed, and\neach component is then normalized by a pooled activity measure, computed by\nexponentiating a weighted sum of rectified and exponentiated components and a\nconstant. We optimize the parameters of the full transformation (linear\ntransform, exponents, weights, constant) over a database of natural images,\ndirectly minimizing the negentropy of the responses. The optimized\ntransformation substantially Gaussianizes the data, achieving a significantly\nsmaller mutual information between transformed components than alternative\nmethods including ICA and radial Gaussianization. The transformation is\ndifferentiable and can be efficiently inverted, and thus induces a density\nmodel on images. We show that samples of this model are visually similar to\nsamples of natural image patches. We demonstrate the use of the model as a\nprior probability density that can be used to remove additive noise. Finally,\nwe show that the transformation can be cascaded, with each layer optimized\nusing the same Gaussianization objective, thus offering an unsupervised method\nof optimizing a deep network architecture.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 17:52:01 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 22:05:15 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2016 03:14:40 GMT"}, {"version": "v4", "created": "Mon, 29 Feb 2016 21:07:30 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Ball\u00e9", "Johannes", ""], ["Laparra", "Valero", ""], ["Simoncelli", "Eero P.", ""]]}, {"id": "1511.06292", "submitter": "Xavier Boix E", "authors": "Yan Luo, Xavier Boix, Gemma Roig, Tomaso Poggio, Qi Zhao", "title": "Foveation-based Mechanisms Alleviate Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that adversarial examples, i.e., the visually imperceptible\nperturbations that result in Convolutional Neural Networks (CNNs) fail, can be\nalleviated with a mechanism based on foveations---applying the CNN in different\nimage regions. To see this, first, we report results in ImageNet that lead to a\nrevision of the hypothesis that adversarial perturbations are a consequence of\nCNNs acting as a linear classifier: CNNs act locally linearly to changes in the\nimage regions with objects recognized by the CNN, and in other regions the CNN\nmay act non-linearly. Then, we corroborate that when the neural responses are\nlinear, applying the foveation mechanism to the adversarial example tends to\nsignificantly reduce the effect of the perturbation. This is because,\nhypothetically, the CNNs for ImageNet are robust to changes of scale and\ntranslation of the object produced by the foveation, but this property does not\ngeneralize to transformations of the perturbation. As a result, the accuracy\nafter a foveation is almost the same as the accuracy of the CNN without the\nadversarial perturbation, even if the adversarial perturbation is calculated\ntaking into account a foveation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 18:35:07 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 14:13:58 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 18:15:28 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Luo", "Yan", ""], ["Boix", "Xavier", ""], ["Roig", "Gemma", ""], ["Poggio", "Tomaso", ""], ["Zhao", "Qi", ""]]}, {"id": "1511.06306", "submitter": "Jonghoon Jin", "authors": "Jonghoon Jin, Aysegul Dundar, Eugenio Culurciello", "title": "Robust Convolutional Neural Networks under Adversarial Noise", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that Convolutional Neural Networks (CNNs) are\nvulnerable to a small perturbation of input called \"adversarial examples\". In\nthis work, we propose a new feedforward CNN that improves robustness in the\npresence of adversarial noise. Our model uses stochastic additive noise added\nto the input image and to the CNN models. The proposed model operates in\nconjunction with a CNN trained with either standard or adversarial objective\nfunction. In particular, convolution, max-pooling, and ReLU layers are modified\nto benefit from the noise model. Our feedforward model is parameterized by only\na mean and variance per pixel which simplifies computations and makes our\nmethod scalable to a deep architecture. From CIFAR-10 and ImageNet test, the\nproposed model outperforms other methods and the improvement is more evident\nfor difficult classification tasks or stronger adversarial noise.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 18:51:08 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2016 16:30:04 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Jin", "Jonghoon", ""], ["Dundar", "Aysegul", ""], ["Culurciello", "Eugenio", ""]]}, {"id": "1511.06309", "submitter": "Viorica Patraucean", "authors": "Viorica Patraucean, Ankur Handa, Roberto Cipolla", "title": "Spatio-temporal video autoencoder with differentiable memory", "comments": "The experiments section has been extended and a direct application to\n  weakly-supervised video segmentation through label propagation has been\n  included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a new spatio-temporal video autoencoder, based on a classic\nspatial image autoencoder and a novel nested temporal autoencoder. The temporal\nencoder is represented by a differentiable visual memory composed of\nconvolutional long short-term memory (LSTM) cells that integrate changes over\ntime. Here we target motion changes and use as temporal decoder a robust\noptical flow prediction module together with an image sampler serving as\nbuilt-in feedback loop. The architecture is end-to-end differentiable. At each\ntime step, the system receives as input a video frame, predicts the optical\nflow based on the current observation and the LSTM memory state as a dense\ntransformation map, and applies it to the current frame to generate the next\nframe. By minimising the reconstruction error between the predicted next frame\nand the corresponding ground truth next frame, we train the whole system to\nextract features useful for motion estimation without any supervision effort.\nWe present one direct application of the proposed framework in\nweakly-supervised semantic segmentation of videos through label propagation\nusing optical flow.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 19:06:28 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 21:07:11 GMT"}, {"version": "v3", "created": "Sat, 6 Aug 2016 16:24:58 GMT"}, {"version": "v4", "created": "Wed, 10 Aug 2016 14:46:49 GMT"}, {"version": "v5", "created": "Thu, 1 Sep 2016 11:36:40 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Patraucean", "Viorica", ""], ["Handa", "Ankur", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1511.06314", "submitter": "Stefan Lee", "authors": "Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall,\n  and Dhruv Batra", "title": "Why M Heads are Better than One: Training a Diverse Ensemble of Deep\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have achieved state-of-the-art performance on a\nwide range of tasks. Most benchmarks are led by ensembles of these powerful\nlearners, but ensembling is typically treated as a post-hoc procedure\nimplemented by averaging independently trained models with model variation\ninduced by bagging or random initialization. In this paper, we rigorously treat\nensembling as a first-class problem to explicitly address the question: what\nare the best strategies to create an ensemble? We first compare a large number\nof ensembling strategies, and then propose and evaluate novel strategies, such\nas parameter sharing (through a new family of models we call TreeNets) as well\nas training under ensemble-aware and diversity-encouraging losses. We\ndemonstrate that TreeNets can improve ensemble performance and that diverse\nensembles can be trained end-to-end under a unified loss, achieving\nsignificantly higher \"oracle\" accuracies than classical ensembles.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 19:19:58 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Lee", "Stefan", ""], ["Purushwalkam", "Senthil", ""], ["Cogswell", "Michael", ""], ["Crandall", "David", ""], ["Batra", "Dhruv", ""]]}, {"id": "1511.06316", "submitter": "Zinelabidine Boulkenafet Mr", "authors": "Zinelabidine Boulkenafet, Jukka Komulainen, Abdenour Hadid", "title": "face anti-spoofing based on color texture analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on face spoofing detection has mainly been focused on analyzing the\nluminance of the face images, hence discarding the chrominance information\nwhich can be useful for discriminating fake faces from genuine ones. In this\nwork, we propose a new face anti-spoofing method based on color texture\nanalysis. We analyze the joint color-texture information from the luminance and\nthe chrominance channels using a color local binary pattern descriptor. More\nspecifically, the feature histograms are extracted from each image band\nseparately. Extensive experiments on two benchmark datasets, namely CASIA face\nanti-spoofing and Replay-Attack databases, showed excellent results compared to\nthe state-of-the-art. Most importantly, our inter-database evaluation depicts\nthat the proposed approach showed very promising generalization capabilities.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 19:28:20 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Boulkenafet", "Zinelabidine", ""], ["Komulainen", "Jukka", ""], ["Hadid", "Abdenour", ""]]}, {"id": "1511.06335", "submitter": "Junyuan Xie", "authors": "Junyuan Xie, Ross Girshick, Ali Farhadi", "title": "Unsupervised Deep Embedding for Clustering Analysis", "comments": "icml2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is central to many data-driven application domains and has been\nstudied extensively in terms of distance functions and grouping algorithms.\nRelatively little work has focused on learning representations for clustering.\nIn this paper, we propose Deep Embedded Clustering (DEC), a method that\nsimultaneously learns feature representations and cluster assignments using\ndeep neural networks. DEC learns a mapping from the data space to a\nlower-dimensional feature space in which it iteratively optimizes a clustering\nobjective. Our experimental evaluations on image and text corpora show\nsignificant improvement over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:06:14 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 22:27:35 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Xie", "Junyuan", ""], ["Girshick", "Ross", ""], ["Farhadi", "Ali", ""]]}, {"id": "1511.06340", "submitter": "Yanwei Fu", "authors": "Yanwei Fu and De-An Huang and Leonid Sigal", "title": "Robust Classification by Pre-conditioned LASSO and Transductive\n  Diffusion Component Analysis", "comments": "we will significantly change the content of this paper which makes it\n  another paper. In order not to misleading, we decided to withdraw it. The\n  updated version can not be shared currently, for some reason. We will update\n  it once it is OK to be shared", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning-based recognition approaches require large-scale\ndatasets with large number of labelled training images. However, such datasets\nare inherently difficult and costly to collect and annotate. Hence there is a\ngreat and growing interest in automatic dataset collection methods that can\nleverage the web. % which are collected % in a cheap, efficient and yet\nunreliable way. Collecting datasets in this way, however, requires robust and\nefficient ways for detecting and excluding outliers that are common and\nprevalent. % Outliers are thus a % prominent treat of using these dataset. So\nfar, there have been a limited effort in machine learning community to directly\ndetect outliers for robust classification. Inspired by the recent work on\nPre-conditioned LASSO, this paper formulates the outlier detection task using\nPre-conditioned LASSO and employs \\red{unsupervised} transductive diffusion\ncomponent analysis to both integrate the topological structure of the data\nmanifold, from labeled and unlabeled instances, and reduce the feature\ndimensionality. Synthetic experiments as well as results on two real-world\nclassification tasks show that our framework can robustly detect the outliers\nand improve classification.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:13:51 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 02:06:46 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Fu", "Yanwei", ""], ["Huang", "De-An", ""], ["Sigal", "Leonid", ""]]}, {"id": "1511.06348", "submitter": "Synho Do", "authors": "Junghwan Cho, Kyewook Lee, Ellie Shin, Garry Choy, Synho Do", "title": "How much data is needed to train a medical image deep learning system to\n  achieve necessary high accuracy?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Convolutional Neural Networks (CNN) in natural image\nclassification systems has produced very impressive results. Combined with the\ninherent nature of medical images that make them ideal for deep-learning,\nfurther application of such systems to medical image classification holds much\npromise. However, the usefulness and potential impact of such a system can be\ncompletely negated if it does not reach a target accuracy. In this paper, we\npresent a study on determining the optimum size of the training data set\nnecessary to achieve high classification accuracy with low variance in medical\nimage classification systems. The CNN was applied to classify axial Computed\nTomography (CT) images into six anatomical classes. We trained the CNN using\nsix different sizes of training data set (5, 10, 20, 50, 100, and 200) and then\ntested the resulting system with a total of 6000 CT images. All images were\nacquired from the Massachusetts General Hospital (MGH) Picture Archiving and\nCommunication System (PACS). Using this data, we employ the learning curve\napproach to predict classification accuracy at a given training sample size.\nOur research will present a general methodology for determining the training\ndata set size necessary to achieve a certain target classification accuracy\nthat can be easily applied to other problems within such systems.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:38:43 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 21:08:10 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Cho", "Junghwan", ""], ["Lee", "Kyewook", ""], ["Shin", "Ellie", ""], ["Choy", "Garry", ""], ["Do", "Synho", ""]]}, {"id": "1511.06359", "submitter": "Bihan Wen Mr", "authors": "Bihan Wen, Saiprasad Ravishankar, and Yoram Bresler", "title": "FRIST - Flipping and Rotation Invariant Sparsifying Transform Learning\n  and Applications", "comments": "Published in Inverse Problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Features based on sparse representation, especially using the synthesis\ndictionary model, have been heavily exploited in signal processing and computer\nvision. However, synthesis dictionary learning typically involves NP-hard\nsparse coding and expensive learning steps. Recently, sparsifying transform\nlearning received interest for its cheap computation and its optimal updates in\nthe alternating algorithms. In this work, we develop a methodology for learning\nFlipping and Rotation Invariant Sparsifying Transforms, dubbed FRIST, to better\nrepresent natural images that contain textures with various geometrical\ndirections. The proposed alternating FRIST learning algorithm involves\nefficient optimal updates. We provide a convergence guarantee, and demonstrate\nthe empirical convergence behavior of the proposed FRIST learning approach.\nPreliminary experiments show the promising performance of FRIST learning for\nsparse image representation, segmentation, denoising, robust inpainting, and\ncompressed sensing-based magnetic resonance image reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:55:49 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 06:43:38 GMT"}, {"version": "v3", "created": "Tue, 17 May 2016 03:54:47 GMT"}, {"version": "v4", "created": "Mon, 16 Oct 2017 02:42:20 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Wen", "Bihan", ""], ["Ravishankar", "Saiprasad", ""], ["Bresler", "Yoram", ""]]}, {"id": "1511.06361", "submitter": "Ivan Vendrov", "authors": "Ivan Vendrov, Ryan Kiros, Sanja Fidler, Raquel Urtasun", "title": "Order-Embeddings of Images and Language", "comments": "ICLR camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypernymy, textual entailment, and image captioning can be seen as special\ncases of a single visual-semantic hierarchy over words, sentences, and images.\nIn this paper we advocate for explicitly modeling the partial order structure\nof this hierarchy. Towards this goal, we introduce a general method for\nlearning ordered representations, and show how it can be applied to a variety\nof tasks involving images and language. We show that the resulting\nrepresentations improve performance over current approaches for hypernym\nprediction and image-caption retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:56:14 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 21:19:30 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2015 04:32:53 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2016 04:58:08 GMT"}, {"version": "v5", "created": "Sun, 17 Jan 2016 03:08:20 GMT"}, {"version": "v6", "created": "Tue, 1 Mar 2016 08:23:50 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Vendrov", "Ivan", ""], ["Kiros", "Ryan", ""], ["Fidler", "Sanja", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1511.06362", "submitter": "Jonathan Huang", "authors": "Jonathan Huang and Kevin Murphy", "title": "Efficient inference in occlusion-aware generative models of images", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generative model of images based on layering, in which image\nlayers are individually generated, then composited from front to back. We are\nthus able to factor the appearance of an image into the appearance of\nindividual objects within the image --- and additionally for each individual\nobject, we can factor content from pose. Unlike prior work on layered models,\nwe learn a shape prior for each object/layer, allowing the model to tease out\nwhich object is in front by looking for a consistent shape, without needing\naccess to motion cues or any labeled data. We show that ordinary stochastic\ngradient variational bayes (SGVB), which optimizes our fully differentiable\nlower-bound on the log-likelihood, is sufficient to learn an interpretable\nrepresentation of images. Finally we present experiments demonstrating the\neffectiveness of the model for inferring foreground and background objects in\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:56:27 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 07:22:02 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Huang", "Jonathan", ""], ["Murphy", "Kevin", ""]]}, {"id": "1511.06380", "submitter": "William Lotter", "authors": "William Lotter, Gabriel Kreiman, David Cox", "title": "Unsupervised Learning of Visual Structure using Predictive Generative\n  Networks", "comments": "under review as conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to predict future states of the environment is a central pillar\nof intelligence. At its core, effective prediction requires an internal model\nof the world and an understanding of the rules by which the world changes.\nHere, we explore the internal models developed by deep neural networks trained\nusing a loss based on predicting future frames in synthetic video sequences,\nusing a CNN-LSTM-deCNN framework. We first show that this architecture can\nachieve excellent performance in visual sequence prediction tasks, including\nstate-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever\net al., 2009). Using a weighted mean-squared error and adversarial loss\n(Goodfellow et al., 2014), the same architecture successfully extrapolates\nout-of-the-plane rotations of computer-generated faces. Furthermore, despite\nbeing trained end-to-end to predict only pixel-level information, our\nPredictive Generative Networks learn a representation of the latent structure\nof the underlying three-dimensional objects themselves. Importantly, we find\nthat this representation is naturally tolerant to object transformations, and\ngeneralizes well to new tasks, such as classification of static images. Similar\nmodels trained solely with a reconstruction loss fail to generalize as\neffectively. We argue that prediction can serve as a powerful unsupervised loss\nfor learning rich internal representations of high-level object features.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:10:17 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 05:50:46 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Lotter", "William", ""], ["Kreiman", "Gabriel", ""], ["Cox", "David", ""]]}, {"id": "1511.06381", "submitter": "Taehoon Lee", "authors": "Taehoon Lee, Minsuk Choi, and Sungroh Yoon", "title": "Manifold Regularized Deep Neural Networks using Adversarial Examples", "comments": "Figure 2, 5, 7, and several descriptions revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning meaningful representations using deep neural networks involves\ndesigning efficient training schemes and well-structured networks. Currently,\nthe method of stochastic gradient descent that has a momentum with dropout is\none of the most popular training protocols. Based on that, more advanced\nmethods (i.e., Maxout and Batch Normalization) have been proposed in recent\nyears, but most still suffer from performance degradation caused by small\nperturbations, also known as adversarial examples. To address this issue, we\npropose manifold regularized networks (MRnet) that utilize a novel training\nobjective function that minimizes the difference between multi-layer embedding\nresults of samples and those adversarial. Our experimental results demonstrated\nthat MRnet is more resilient to adversarial examples and helps us to generalize\nrepresentations on manifolds. Furthermore, combining MRnet and dropout allowed\nus to achieve competitive classification performances for three well-known\nbenchmarks: MNIST, CIFAR-10, and SVHN.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:10:29 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2016 16:35:11 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Lee", "Taehoon", ""], ["Choi", "Minsuk", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1511.06394", "submitter": "Olivier  H\\'enaff", "authors": "Olivier J. H\\'enaff and Eero P. Simoncelli", "title": "Geodesics of learned representations", "comments": "Published as a conference paper at ICLR 2016", "journal-ref": "Presented at: Int'l Conf on Learning Representations (ICLR), San\n  Juan, Puerto Rico, May 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new method for visualizing and refining the invariances of\nlearned representations. Specifically, we test for a general form of\ninvariance, linearization, in which the action of a transformation is confined\nto a low-dimensional subspace. Given two reference images (typically, differing\nby some transformation), we synthesize a sequence of images lying on a path\nbetween them that is of minimal length in the space of the representation (a\n\"representational geodesic\"). If the transformation relating the two reference\nimages is linearized by the representation, this sequence should follow the\ngradual evolution of this transformation. We use this method to assess the\ninvariance properties of a state-of-the-art image classification network and\nfind that geodesics generated for image pairs differing by translation,\nrotation, and dilation do not evolve according to their associated\ntransformations. Our method also suggests a remedy for these failures, and\nfollowing this prescription, we show that the modified representation is able\nto linearize a variety of geometric image transformations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:40:13 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 21:10:58 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 21:05:40 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2016 17:42:25 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["H\u00e9naff", "Olivier J.", ""], ["Simoncelli", "Eero P.", ""]]}, {"id": "1511.06408", "submitter": "Grace Lindsay", "authors": "Grace W. Lindsay", "title": "Feature-based Attention in Convolutional Neural Networks", "comments": "9 pages (plus 3 page Appendix), 7 figures total, submitted to ICLR\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolutional neural networks (CNNs) have proven effective for image\nprocessing tasks, such as object recognition and classification. Recently, CNNs\nhave been enhanced with concepts of attention, similar to those found in\nbiology. Much of this work on attention has focused on effective serial spatial\nprocessing. In this paper, I introduce a simple procedure for applying\nfeature-based attention (FBA) to CNNs and compare multiple implementation\noptions. FBA is a top-down signal applied globally to an input image which\naides in detecting chosen objects in cluttered or noisy settings. The concept\nof FBA and the implementation details tested here were derived from what is\nknown (and debated) about biological object- and feature-based attention. The\nimplementations of FBA described here increase performance on challenging\nobject detection tasks using a procedure that is simple, fast, and does not\nrequire additional iterative training. Furthermore, the comparisons performed\nhere suggest that a proposed model of biological FBA (the \"feature similarity\ngain model\") is effective in increasing performance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:57:27 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 19:31:12 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Lindsay", "Grace W.", ""]]}, {"id": "1511.06409", "submitter": "Jake Snell", "authors": "Jake Snell, Karl Ridgeway, Renjie Liao, Brett D. Roads, Michael C.\n  Mozer, Richard S. Zemel", "title": "Learning to Generate Images with Perceptual Similarity Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks are increasingly being applied to problems involving image\nsynthesis, e.g., generating images from textual descriptions and reconstructing\nan input image from a compact representation. Supervised training of\nimage-synthesis networks typically uses a pixel-wise loss (PL) to indicate the\nmismatch between a generated image and its corresponding target image. We\npropose instead to use a loss function that is better calibrated to human\nperceptual judgments of image quality: the multiscale structural-similarity\nscore (MS-SSIM). Because MS-SSIM is differentiable, it is easily incorporated\ninto gradient-descent learning. We compare the consequences of using MS-SSIM\nversus PL loss on training deterministic and stochastic autoencoders. For three\ndifferent architectures, we collected human judgments of the quality of image\nreconstructions. Observers reliably prefer images synthesized by\nMS-SSIM-optimized models over those synthesized by PL-optimized models, for two\ndistinct PL measures ($\\ell_1$ and $\\ell_2$ distances). We also explore the\neffect of training objective on image encoding and analyze conditions under\nwhich perceptually-optimized representations yield better performance on image\nclassification. Finally, we demonstrate the superiority of\nperceptually-optimized networks for super-resolution imaging. Just as computer\nvision has advanced through the use of convolutional architectures that mimic\nthe structure of the mammalian visual system, we argue that significant\nadditional advances can be made in modeling images through the use of training\nobjectives that are well aligned to characteristics of human perception.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:57:46 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 17:21:56 GMT"}, {"version": "v3", "created": "Tue, 24 Jan 2017 02:03:41 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Snell", "Jake", ""], ["Ridgeway", "Karl", ""], ["Liao", "Renjie", ""], ["Roads", "Brett D.", ""], ["Mozer", "Michael C.", ""], ["Zemel", "Richard S.", ""]]}, {"id": "1511.06412", "submitter": "M\\'elanie Ducoffe", "authors": "Melanie Ducoffe and Frederic Precioso", "title": "QBDC: Query by dropout committee for training deep supervised\n  architecture", "comments": "Submitted to ICLR2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the current trend is to increase the depth of neural networks to\nincrease their performance, the size of their training database has to grow\naccordingly. We notice an emergence of tremendous databases, although providing\nlabels to build a training set still remains a very expensive task. We tackle\nthe problem of selecting the samples to be labelled in an online fashion. In\nthis paper, we present an active learning strategy based on query by committee\nand dropout technique to train a Convolutional Neural Network (CNN). We derive\na commmittee of partial CNNs resulting from batchwise dropout runs on the\ninitial CNN. We evaluate our active learning strategy for CNN on MNIST\nbenchmark, showing in particular that selecting less than 30 % from the\nannotated database is enough to get similar error rate as using the full\ntraining set on MNIST. We also studied the robustness of our method against\nadversarial examples.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:03:14 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 14:19:01 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Ducoffe", "Melanie", ""], ["Precioso", "Frederic", ""]]}, {"id": "1511.06421", "submitter": "Jacob Gardner", "authors": "Jacob R. Gardner, Paul Upchurch, Matt J. Kusner, Yixuan Li, Kilian Q.\n  Weinberger, Kavita Bala, John E. Hopcroft", "title": "Deep Manifold Traversal: Changing Labels with Convolutional Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in computer vision can be cast as a \"label changing\" problem,\nwhere the goal is to make a semantic change to the appearance of an image or\nsome subject in an image in order to alter the class membership. Although\nsuccessful task-specific methods have been developed for some label changing\napplications, to date no general purpose method exists. Motivated by this we\npropose deep manifold traversal, a method that addresses the problem in its\nmost general form: it first approximates the manifold of natural images then\nmorphs a test image along a traversal path away from a source class and towards\na target class while staying near the manifold throughout. The resulting\nalgorithm is surprisingly effective and versatile. It is completely data\ndriven, requiring only an example set of images from the desired source and\ntarget domains. We demonstrate deep manifold traversal on highly diverse label\nchanging tasks: changing an individual's appearance (age and hair color),\nchanging the season of an outdoor image, and transforming a city skyline\ntowards nighttime.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:17:20 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 01:37:02 GMT"}, {"version": "v3", "created": "Thu, 17 Mar 2016 17:57:55 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Gardner", "Jacob R.", ""], ["Upchurch", "Paul", ""], ["Kusner", "Matt J.", ""], ["Li", "Yixuan", ""], ["Weinberger", "Kilian Q.", ""], ["Bala", "Kavita", ""], ["Hopcroft", "John E.", ""]]}, {"id": "1511.06425", "submitter": "KyungHyun Cho", "authors": "Quan Gan, Qipeng Guo, Zheng Zhang, Kyunghyun Cho", "title": "First Step toward Model-Free, Anonymous Object Tracking with Recurrent\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and study a novel visual object tracking approach\nbased on convolutional networks and recurrent networks. The proposed approach\nis distinct from the existing approaches to visual object tracking, such as\nfiltering-based ones and tracking-by-detection ones, in the sense that the\ntracking system is explicitly trained off-line to track anonymous objects in a\nnoisy environment. The proposed visual tracking model is end-to-end trainable,\nminimizing any adversarial effect from mismatches in object representation and\nbetween the true underlying dynamics and learning dynamics. We empirically show\nthat the proposed tracking approach works well in various scenarios by\ngenerating artificial video sequences with varying conditions; the number of\nobjects, amount of noise and the match between the training shapes and test\nshapes.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:24:15 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 19:44:15 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Gan", "Quan", ""], ["Guo", "Qipeng", ""], ["Zhang", "Zheng", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1511.06428", "submitter": "Kelvin Xu", "authors": "Marcin Moczulski, Kelvin Xu, Aaron Courville, Kyunghyun Cho", "title": "A Controller-Recognizer Framework: How necessary is recognition for\n  control?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been growing interest in building active visual object\nrecognizers, as opposed to the usual passive recognizers which classifies a\ngiven static image into a predefined set of object categories. In this paper we\npropose to generalize these recently proposed end-to-end active visual\nrecognizers into a controller-recognizer framework. A model in the\ncontroller-recognizer framework consists of a controller, which interfaces with\nan external manipulator, and a recognizer which classifies the visual input\nadjusted by the manipulator. We describe two most recently proposed\ncontroller-recognizer models: recurrent attention model and spatial transformer\nnetwork as representative examples of controller-recognizer models. Based on\nthis description we observe that most existing end-to-end\ncontroller-recognizers tightly, or completely, couple a controller and\nrecognizer. We ask a question whether this tight coupling is necessary, and try\nto answer this empirically by building a controller-recognizer model with a\ndecoupled controller and recognizer. Our experiments revealed that it is not\nalways necessary to tightly couple them and that by decoupling a controller and\nrecognizer, there is a possibility of building a generic controller that is\npretrained and works together with any subsequent recognizer.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:38:53 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 03:51:33 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2015 18:47:15 GMT"}, {"version": "v4", "created": "Tue, 9 Feb 2016 20:58:21 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Moczulski", "Marcin", ""], ["Xu", "Kelvin", ""], ["Courville", "Aaron", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1511.06432", "submitter": "Nicolas Ballas", "authors": "Nicolas Ballas, Li Yao, Chris Pal, Aaron Courville", "title": "Delving Deeper into Convolutional Networks for Learning Video\n  Representations", "comments": "ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to learn spatio-temporal features in videos from\nintermediate visual representations we call \"percepts\" using\nGated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts\nthat are extracted from all level of a deep convolutional network trained on\nthe large ImageNet dataset. While high-level percepts contain highly\ndiscriminative information, they tend to have a low-spatial resolution.\nLow-level percepts, on the other hand, preserve a higher spatial resolution\nfrom which we can model finer motion patterns. Using low-level percepts can\nleads to high-dimensionality video representations. To mitigate this effect and\ncontrol the model number of parameters, we introduce a variant of the GRU model\nthat leverages the convolution operations to enforce sparse connectivity of the\nmodel units and share parameters across the input spatial locations.\n  We empirically validate our approach on both Human Action Recognition and\nVideo Captioning tasks. In particular, we achieve results equivalent to\nstate-of-art on the YouTube2Text dataset using a simpler text-decoder model and\nwithout extra 3D CNN features.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:46:13 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 02:46:54 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 19:43:19 GMT"}, {"version": "v4", "created": "Tue, 1 Mar 2016 18:54:11 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Ballas", "Nicolas", ""], ["Yao", "Li", ""], ["Pal", "Chris", ""], ["Courville", "Aaron", ""]]}, {"id": "1511.06434", "submitter": "Alec Radford", "authors": "Alec Radford, Luke Metz, and Soumith Chintala", "title": "Unsupervised Representation Learning with Deep Convolutional Generative\n  Adversarial Networks", "comments": "Under review as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and\nunsupervised learning. We introduce a class of CNNs called deep convolutional\ngenerative adversarial networks (DCGANs), that have certain architectural\nconstraints, and demonstrate that they are a strong candidate for unsupervised\nlearning. Training on various image datasets, we show convincing evidence that\nour deep convolutional adversarial pair learns a hierarchy of representations\nfrom object parts to scenes in both the generator and discriminator.\nAdditionally, we use the learned features for novel tasks - demonstrating their\napplicability as general image representations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:50:32 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 23:09:39 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Radford", "Alec", ""], ["Metz", "Luke", ""], ["Chintala", "Soumith", ""]]}, {"id": "1511.06437", "submitter": "Rodrigo Benenson", "authors": "Jan Hosang, Rodrigo Benenson, Bernt Schiele", "title": "A convnet for non-maximum suppression", "comments": "Included comments from reviewers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-maximum suppression (NMS) is used in virtually all state-of-the-art\nobject detection pipelines. While essential object detection ingredients such\nas features, classifiers, and proposal methods have been extensively researched\nsurprisingly little work has aimed to systematically address NMS. The de-facto\nstandard for NMS is based on greedy clustering with a fixed distance threshold,\nwhich forces to trade-off recall versus precision. We propose a convnet\ndesigned to perform NMS of a given set of detections. We report experiments on\na synthetic setup, and results on crowded pedestrian detection scenes. Our\napproach overcomes the intrinsic limitations of greedy NMS, obtaining better\nrecall and precision.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:56:18 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 08:16:33 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 00:00:21 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Hosang", "Jan", ""], ["Benenson", "Rodrigo", ""], ["Schiele", "Bernt", ""]]}, {"id": "1511.06442", "submitter": "Henry Gouk", "authors": "Henry Gouk, Bernhard Pfahringer, Michael Cree", "title": "Fast Metric Learning For Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity metrics are a core component of many information retrieval and\nmachine learning systems. In this work we propose a method capable of learning\na similarity metric from data equipped with a binary relation. By considering\nonly the similarity constraints, and initially ignoring the features, we are\nable to learn target vectors for each instance using one of several\nappropriately designed loss functions. A regression model can then be\nconstructed that maps novel feature vectors to the same target vector space,\nresulting in a feature extractor that computes vectors for which a predefined\nmetric is a meaningful measure of similarity. We present results on both\nmulticlass and multi-label classification datasets that demonstrate\nconsiderably faster convergence, as well as higher accuracy on the majority of\nthe intrinsic evaluation tasks and all extrinsic evaluation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 23:10:00 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 06:05:30 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2015 15:27:11 GMT"}, {"version": "v4", "created": "Wed, 17 Feb 2016 02:11:00 GMT"}, {"version": "v5", "created": "Tue, 5 Apr 2016 07:29:48 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Gouk", "Henry", ""], ["Pfahringer", "Bernhard", ""], ["Cree", "Michael", ""]]}, {"id": "1511.06448", "submitter": "Pouya Bashivan", "authors": "Pouya Bashivan, Irina Rish, Mohammed Yeasin, Noel Codella", "title": "Learning Representations from EEG with Deep Recurrent-Convolutional\n  Neural Networks", "comments": "To be published as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in modeling cognitive events from electroencephalogram\n(EEG) data is finding representations that are invariant to inter- and\nintra-subject differences, as well as to inherent noise associated with such\ndata. Herein, we propose a novel approach for learning such representations\nfrom multi-channel EEG time-series, and demonstrate its advantages in the\ncontext of mental load classification task. First, we transform EEG activities\ninto a sequence of topology-preserving multi-spectral images, as opposed to\nstandard EEG analysis techniques that ignore such spatial information. Next, we\ntrain a deep recurrent-convolutional network inspired by state-of-the-art video\nclassification to learn robust representations from the sequence of images. The\nproposed approach is designed to preserve the spatial, spectral, and temporal\nstructure of EEG which leads to finding features that are less sensitive to\nvariations and distortions within each dimension. Empirical evaluation on the\ncognitive load classification task demonstrated significant improvements in\nclassification accuracy over current state-of-the-art approaches in this field.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 23:29:55 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 22:04:23 GMT"}, {"version": "v3", "created": "Mon, 29 Feb 2016 21:33:45 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Bashivan", "Pouya", ""], ["Rish", "Irina", ""], ["Yeasin", "Mohammed", ""], ["Codella", "Noel", ""]]}, {"id": "1511.06449", "submitter": "Eunbyung Park", "authors": "Eunbyung Park, Alexander C. Berg", "title": "Learning to decompose for object detection and instance segmentation", "comments": "ICLR 2016 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep convolutional neural networks(CNNs) have achieved remarkable\nresults on object detection and segmentation, pre- and post-processing steps\nsuch as region proposals and non-maximum suppression(NMS), have been required.\nThese steps result in high computational complexity and sensitivity to\nhyperparameters, e.g. thresholds for NMS. In this work, we propose a novel\nend-to-end trainable deep neural network architecture, which consists of\nconvolutional and recurrent layers, that generates the correct number of object\ninstances and their bounding boxes (or segmentation masks) given an image,\nusing only a single network evaluation without any pre- or post-processing\nsteps. We have tested on detecting digits in multi-digit images synthesized\nusing MNIST, automatically segmenting digits in these images, and detecting\ncars in the KITTI benchmark dataset. The proposed approach outperforms a strong\nCNN baseline on the synthesized digits datasets and shows promising results on\nKITTI car detection.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 23:30:06 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 06:07:28 GMT"}, {"version": "v3", "created": "Wed, 11 May 2016 02:55:29 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Park", "Eunbyung", ""], ["Berg", "Alexander C.", ""]]}, {"id": "1511.06452", "submitter": "Hyun Oh Song", "authors": "Hyun Oh Song, Yu Xiang, Stefanie Jegelka, Silvio Savarese", "title": "Deep Metric Learning via Lifted Structured Feature Embedding", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the distance metric between pairs of examples is of great importance\nfor learning and visual recognition. With the remarkable success from the state\nof the art convolutional neural networks, recent works have shown promising\nresults on discriminatively training the networks to learn semantic feature\nembeddings where similar examples are mapped close to each other and dissimilar\nexamples are mapped farther apart. In this paper, we describe an algorithm for\ntaking full advantage of the training batches in the neural network training by\nlifting the vector of pairwise distances within the batch to the matrix of\npairwise distances. This step enables the algorithm to learn the state of the\nart feature embedding by optimizing a novel structured prediction objective on\nthe lifted problem. Additionally, we collected Online Products dataset: 120k\nimages of 23k classes of online products for metric learning. Our experiments\non the CUB-200-2011, CARS196, and Online Products datasets demonstrate\nsignificant improvement over existing deep feature embedding methods on all\nexperimented embedding sizes with the GoogLeNet network.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 23:41:11 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Song", "Hyun Oh", ""], ["Xiang", "Yu", ""], ["Jegelka", "Stefanie", ""], ["Savarese", "Silvio", ""]]}, {"id": "1511.06457", "submitter": "Peng Wang", "authors": "Peng Wang and Alan Yuille", "title": "DOC: Deep OCclusion Estimation From a Single Image", "comments": "Accepted to ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the occlusion relationships between objects is a fundamental human\nvisual ability which yields important information about the 3D world. In this\npaper we propose a deep network architecture, called DOC, which acts on a\nsingle image, detects object boundaries and estimates the border ownership\n(i.e. which side of the boundary is foreground and which is background). We\nrepresent occlusion relations by a binary edge map, to indicate the object\nboundary, and an occlusion orientation variable which is tangential to the\nboundary and whose direction specifies border ownership by a left-hand rule. We\ntrain two related deep convolutional neural networks, called DOC, which exploit\nlocal and non-local image cues to estimate this representation and hence\nrecover occlusion relations. In order to train and test DOC we construct a\nlarge-scale instance occlusion boundary dataset using PASCAL VOC images, which\nwe call the PASCAL instance occlusion dataset (PIOD). This contains 10,000\nimages and hence is two orders of magnitude larger than existing occlusion\ndatasets for outdoor images. We test two variants of DOC on PIOD and on the\nBSDS occlusion dataset and show they outperform state-of-the-art methods.\nFinally, we perform numerous experiments investigating multiple settings of DOC\nand transfer between BSDS and PIOD, which provides more insights for further\nstudy of occlusion estimation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 00:04:06 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 00:49:47 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 06:46:26 GMT"}, {"version": "v4", "created": "Sun, 24 Jul 2016 07:16:54 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Wang", "Peng", ""], ["Yuille", "Alan", ""]]}, {"id": "1511.06489", "submitter": "Xiaohan Fei", "authors": "Xiaohan Fei, Konstantine Tsotsos, Stefano Soatto", "title": "A Simple Hierarchical Pooling Data Structure for Loop Closure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data structure obtained by hierarchically averaging bag-of-word\ndescriptors during a sequence of views that achieves average speedups in\nlarge-scale loop closure applications ranging from 4 to 20 times on benchmark\ndatasets. Although simple, the method works as well as sophisticated\nagglomerative schemes at a fraction of the cost with minimal loss of\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 04:56:47 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 22:04:42 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Fei", "Xiaohan", ""], ["Tsotsos", "Konstantine", ""], ["Soatto", "Stefano", ""]]}, {"id": "1511.06494", "submitter": "Ali Mollahosseini", "authors": "Ali Mollahosseini, Mohammad H. Mahoor", "title": "Bidirectional Warping of Active Appearance Model", "comments": null, "journal-ref": "2013 IEEE Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW)", "doi": "10.1109/CVPRW.2013.129", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Appearance Model (AAM) is a commonly used method for facial image\nanalysis with applications in face identification and facial expression\nrecognition. This paper proposes a new approach based on image alignment for\nAAM fitting called bidirectional warping. Previous approaches warp either the\ninput image or the appearance template. We propose to warp both the input\nimage, using incremental update by an affine transformation, and the appearance\ntemplate, using an inverse compositional approach. Our experimental results on\nMulti-PIE face database show that the bidirectional approach outperforms\nstate-of-the-art inverse compositional fitting approaches in extracting\nlandmark points of faces with shape and pose variations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 05:23:12 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Mollahosseini", "Ali", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "1511.06522", "submitter": "Yan Zhang", "authors": "Yan Zhang, Mete Ozay, Xing Liu, Takayuki Okatani", "title": "Integrating Deep Features for Material Recognition", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for integration of features extracted using deep\nrepresentations of Convolutional Neural Networks (CNNs) each of which is\nlearned using a different image dataset of objects and materials for material\nrecognition. Given a set of representations of multiple pre-trained CNNs, we\nfirst compute activations of features using the representations on the images\nto select a set of samples which are best represented by the features. Then, we\nmeasure the uncertainty of the features by computing the entropy of class\ndistributions for each sample set. Finally, we compute the contribution of each\nfeature to representation of classes for feature selection and integration. We\nexamine the proposed method on three benchmark datasets for material\nrecognition. Experimental results show that the proposed method achieves\nstate-of-the-art performance by integrating deep features. Additionally, we\nintroduce a new material dataset called EFMD by extending Flickr Material\nDatabase (FMD). By the employment of the EFMD with transfer learning for\nupdating the learned CNN models, we achieve 84.0%+/-1.8% accuracy on the FMD\ndataset which is close to human performance that is 84.9%.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 08:31:00 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2015 14:21:28 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2015 13:39:24 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2016 14:36:36 GMT"}, {"version": "v5", "created": "Tue, 5 Apr 2016 09:18:49 GMT"}, {"version": "v6", "created": "Thu, 21 Apr 2016 10:19:56 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Zhang", "Yan", ""], ["Ozay", "Mete", ""], ["Liu", "Xing", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1511.06523", "submitter": "Shuo Yang", "authors": "Shuo Yang, Ping Luo, Chen Change Loy, and Xiaoou Tang", "title": "WIDER FACE: A Face Detection Benchmark", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection is one of the most studied topics in the computer vision\ncommunity. Much of the progresses have been made by the availability of face\ndetection benchmark datasets. We show that there is a gap between current face\ndetection performance and the real world requirements. To facilitate future\nface detection research, we introduce the WIDER FACE dataset, which is 10 times\nlarger than existing datasets. The dataset contains rich annotations, including\nocclusions, poses, event categories, and face bounding boxes. Faces in the\nproposed dataset are extremely challenging due to large variations in scale,\npose and occlusion, as shown in Fig. 1. Furthermore, we show that WIDER FACE\ndataset is an effective training source for face detection. We benchmark\nseveral representative detection systems, providing an overview of\nstate-of-the-art performance and propose a solution to deal with large scale\nvariation. Finally, we discuss common failure cases that worth to be further\ninvestigated. Dataset can be downloaded at:\nmmlab.ie.cuhk.edu.hk/projects/WIDERFace\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 08:33:57 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Yang", "Shuo", ""], ["Luo", "Ping", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1511.06530", "submitter": "Yong-Deok Kim", "authors": "Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang,\n  Dongjun Shin", "title": "Compression of Deep Convolutional Neural Networks for Fast and Low Power\n  Mobile Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the latest high-end smartphone has powerful CPU and GPU, running\ndeeper convolutional neural networks (CNNs) for complex tasks such as ImageNet\nclassification on mobile devices is challenging. To deploy deep CNNs on mobile\ndevices, we present a simple and effective scheme to compress the entire CNN,\nwhich we call one-shot whole network compression. The proposed scheme consists\nof three steps: (1) rank selection with variational Bayesian matrix\nfactorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning\nto recover accumulated loss of accuracy, and each step can be easily\nimplemented using publicly available tools. We demonstrate the effectiveness of\nthe proposed scheme by testing the performance of various compressed CNNs\n(AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significant\nreductions in model size, runtime, and energy consumption are obtained, at the\ncost of small loss in accuracy. In addition, we address the important\nimplementation level issue on 1?1 convolution, which is a key operation of\ninception module of GoogLeNet as well as CNNs compressed by our proposed\nscheme.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 09:20:08 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 11:52:12 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Kim", "Yong-Deok", ""], ["Park", "Eunhyeok", ""], ["Yoo", "Sungjoo", ""], ["Choi", "Taelim", ""], ["Yang", "Lu", ""], ["Shin", "Dongjun", ""]]}, {"id": "1511.06545", "submitter": "Souradeep Chakraborty", "authors": "Souradeep Chakraborty, Pabitra Mitra", "title": "A dense subgraph based algorithm for compact salient image region\n  detection", "comments": "33 pages, 18 figures, Single column manuscript pre-print, Accepted at\n  Computer Vision and Image Understanding, Elsevier", "journal-ref": null, "doi": "10.1016/j.cviu.2015.12.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for graph based saliency computation that utilizes\nthe underlying dense subgraphs in finding visually salient regions in an image.\nTo compute the salient regions, the model first obtains a saliency map using\nrandom walks on a Markov chain. Next, k-dense subgraphs are detected to further\nenhance the salient regions in the image. Dense subgraphs convey more\ninformation about local graph structure than simple centrality measures. To\ngenerate the Markov chain, intensity and color features of an image in addition\nto region compactness is used. For evaluating the proposed model, we do\nextensive experiments on benchmark image data sets. The proposed method\nperforms comparable to well-known algorithms in salient region detection.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 10:09:13 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2015 12:31:16 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Chakraborty", "Souradeep", ""], ["Mitra", "Pabitra", ""]]}, {"id": "1511.06566", "submitter": "Tuomo Valkonen", "authors": "Tuomo Valkonen, Thomas Pock", "title": "Acceleration of the PDHGM on strongly convex subspaces", "comments": null, "journal-ref": null, "doi": "10.1007/s10851-016-0692-2", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose several variants of the primal-dual method due to Chambolle and\nPock. Without requiring full strong convexity of the objective functions, our\nmethods are accelerated on subspaces with strong convexity. This yields mixed\nrates, $O(1/N^2)$ with respect to initialisation and $O(1/N)$ with respect to\nthe dual sequence, and the residual part of the primal sequence. We demonstrate\nthe efficacy of the proposed methods on image processing problems lacking\nstrong convexity, such as total generalised variation denoising and total\nvariation deblurring.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 11:59:11 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 23:24:35 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Valkonen", "Tuomo", ""], ["Pock", "Thomas", ""]]}, {"id": "1511.06575", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl, Thiago C. Santini, Thomas Kuebler, Enkelejda Kasneci", "title": "ElSe: Ellipse Selection for Robust Pupil Detection in Real-World\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and robust pupil detection is an essential prerequisite for video-based\neye-tracking in real-world settings. Several algorithms for image-based pupil\ndetection have been proposed, their applicability is mostly limited to\nlaboratory conditions. In realworld scenarios, automated pupil detection has to\nface various challenges, such as illumination changes, reflections (on\nglasses), make-up, non-centered eye recording, and physiological eye\ncharacteristics. We propose ElSe, a novel algorithm based on ellipse evaluation\nof a filtered edge image. We aim at a robust, resource-saving approach that can\nbe integrated in embedded architectures e.g. driving. The proposed algorithm\nwas evaluated against four state-of-the-art methods on over 93,000 hand-labeled\nimages from which 55,000 are new images contributed by this work. On average,\nthe proposed method achieved a 14.53% improvement on the detection rate\nrelative to the best state-of-the-art performer.\ndownload:ftp://emmapupildata@messor.informatik.unituebingen. de\n(password:eyedata).\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 12:49:20 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 06:56:10 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Santini", "Thiago C.", ""], ["Kuebler", "Thomas", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "1511.06586", "submitter": "Chee Seng Chan", "authors": "Ven Jyn Kok, Mei Kuan Lim, Chee Seng Chan", "title": "Crowd Behavior Analysis: A Review where Physics meets Biology", "comments": "Accepted in Neurocomputing, 31 pages, 180 references", "journal-ref": "Neurocomputing 177 (2016) 342-362", "doi": "10.1016/j.neucom.2015.11.021", "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the traits emerged in a mass gathering are often non-deliberative,\nthe act of mass impulse may lead to irre- vocable crowd disasters. The two-fold\nincrease of carnage in crowd since the past two decades has spurred significant\nadvances in the field of computer vision, towards effective and proactive crowd\nsurveillance. Computer vision stud- ies related to crowd are observed to\nresonate with the understanding of the emergent behavior in physics (complex\nsystems) and biology (animal swarm). These studies, which are inspired by\nbiology and physics, share surprisingly common insights, and interesting\ncontradictions. However, this aspect of discussion has not been fully explored.\nTherefore, this survey provides the readers with a review of the\nstate-of-the-art methods in crowd behavior analysis from the physics and\nbiologically inspired perspectives. We provide insights and comprehensive\ndiscussions for a broader understanding of the underlying prospect of blending\nphysics and biology studies in computer vision.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 13:19:44 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Kok", "Ven Jyn", ""], ["Lim", "Mei Kuan", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1511.06624", "submitter": "Pui Tung Choi", "authors": "Ting Wei Meng, Gary Pui-Tung Choi, Lok Ming Lui", "title": "TEMPO: Feature-Endowed Teichm\\\"uller Extremal Mappings of Point Clouds", "comments": null, "journal-ref": "SIAM Journal on Imaging Sciences 9, 1922-1962 (2016)", "doi": "10.1137/15M1049117", "report-no": null, "categories": "cs.CG cs.CV cs.GR math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent decades, the use of 3D point clouds has been widespread in computer\nindustry. The development of techniques in analyzing point clouds is\nincreasingly important. In particular, mapping of point clouds has been a\nchallenging problem. In this paper, we develop a discrete analogue of the\nTeichm\\\"{u}ller extremal mappings, which guarantee uniform conformality\ndistortions, on point cloud surfaces. Based on the discrete analogue, we\npropose a novel method called TEMPO for computing Teichm\\\"{u}ller extremal\nmappings between feature-endowed point clouds. Using our proposed method, the\nTeichm\\\"{u}ller metric is introduced for evaluating the dissimilarity of point\nclouds. Consequently, our algorithm enables accurate recognition and\nclassification of point clouds. Experimental results demonstrate the\neffectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 14:57:05 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 12:37:02 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Meng", "Ting Wei", ""], ["Choi", "Gary Pui-Tung", ""], ["Lui", "Lok Ming", ""]]}, {"id": "1511.06627", "submitter": "Zhu Shizhan", "authors": "Shizhan Zhu, Cheng Li, Chen Change Loy, Xiaoou Tang", "title": "Towards Arbitrary-View Face Alignment by Recommendation Trees", "comments": "This is our original submission to ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to simultaneously handle face alignment of arbitrary views, e.g.\nfrontal and profile views, appears to be more challenging than we thought. The\ndifficulties lay in i) accommodating the complex appearance-shape relations\nexhibited in different views, and ii) encompassing the varying landmark point\nsets due to self-occlusion and different landmark protocols. Most existing\nstudies approach this problem via training multiple viewpoint-specific models,\nand conduct head pose estimation for model selection. This solution is\nintuitive but the performance is highly susceptible to inaccurate head pose\nestimation. In this study, we address this shortcoming through learning an\nEnsemble of Model Recommendation Trees (EMRT), which is capable of selecting\noptimal model configuration without prior head pose estimation. The unified\nframework seamlessly handles different viewpoints and landmark protocols, and\nit is trained by optimising directly on landmark locations, thus yielding\nsuperior results on arbitrary-view face alignment. This is the first study that\nperforms face alignment on the full AFLWdataset with faces of different views\nincluding profile view. State-of-the-art performances are also reported on\nMultiPIE and AFW datasets containing both frontaland profile-view faces.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 15:01:21 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Zhu", "Shizhan", ""], ["Li", "Cheng", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1511.06631", "submitter": "Matthias Joachim Ehrhardt", "authors": "Matthias J. Ehrhardt and Marta M. Betcke", "title": "Multi-Contrast MRI Reconstruction with Structure-Guided Total Variation", "comments": "18 pages, 16 figures", "journal-ref": null, "doi": "10.1137/15M1047325", "report-no": null, "categories": "math.NA cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) is a versatile imaging technique that allows\ndifferent contrasts depending on the acquisition parameters. Many clinical\nimaging studies acquire MRI data for more than one of these contrasts---such as\nfor instance T1 and T2 weighted images---which makes the overall scanning\nprocedure very time consuming. As all of these images show the same underlying\nanatomy one can try to omit unnecessary measurements by taking the similarity\ninto account during reconstruction. We will discuss two modifications of total\nvariation---based on i) location and ii) direction---that take structural a\npriori knowledge into account and reduce to total variation in the degenerate\ncase when no structural knowledge is available. We solve the resulting convex\nminimization problem with the alternating direction method of multipliers that\nseparates the forward operator from the prior. For both priors the\ncorresponding proximal operator can be implemented as an extension of the fast\ngradient projection method on the dual problem for total variation. We tested\nthe priors on six data sets that are based on phantoms and real MRI images. In\nall test cases exploiting the structural information from the other contrast\nyields better results than separate reconstruction with total variation in\nterms of standard metrics like peak signal-to-noise ratio and structural\nsimilarity index. Furthermore, we found that exploiting the two dimensional\ndirectional information results in images with well defined edges, superior to\nthose reconstructed solely using a priori information about the edge location.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 15:08:17 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Ehrhardt", "Matthias J.", ""], ["Betcke", "Marta M.", ""]]}, {"id": "1511.06645", "submitter": "Leonid Pishchulin", "authors": "Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres,\n  Mykhaylo Andriluka, Peter Gehler, Bernt Schiele", "title": "DeepCut: Joint Subset Partition and Labeling for Multi Person Pose\n  Estimation", "comments": "Accepted at IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the task of articulated human pose estimation of\nmultiple people in real world images. We propose an approach that jointly\nsolves the tasks of detection and pose estimation: it infers the number of\npersons in a scene, identifies occluded body parts, and disambiguates body\nparts between people in close proximity of each other. This joint formulation\nis in contrast to previous strategies, that address the problem by first\ndetecting people and subsequently estimating their body pose. We propose a\npartitioning and labeling formulation of a set of body-part hypotheses\ngenerated with CNN-based part detectors. Our formulation, an instance of an\ninteger linear program, implicitly performs non-maximum suppression on the set\nof part candidates and groups them to form configurations of body parts\nrespecting geometric and appearance constraints. Experiments on four different\ndatasets demonstrate state-of-the-art results for both single person and multi\nperson pose estimation. Models and code available at\nhttp://pose.mpi-inf.mpg.de.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 15:37:55 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 04:26:29 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Pishchulin", "Leonid", ""], ["Insafutdinov", "Eldar", ""], ["Tang", "Siyu", ""], ["Andres", "Bjoern", ""], ["Andriluka", "Mykhaylo", ""], ["Gehler", "Peter", ""], ["Schiele", "Bernt", ""]]}, {"id": "1511.06653", "submitter": "F\\'elix G. Harvey", "authors": "F\\'elix G. Harvey, Julien Roy, David Kanaa, Christopher Pal", "title": "Recurrent Semi-supervised Classification and Constrained Adversarial\n  Generation with Motion Capture Data", "comments": "IVC Journal Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore recurrent encoder multi-decoder neural network architectures for\nsemi-supervised sequence classification and reconstruction. We find that the\nuse of multiple reconstruction modules helps models generalize in a\nclassification task when only a small amount of labeled data is available,\nwhich is often the case in practice. Such models provide useful high-level\nrepresentations of motions allowing clustering, searching and faster labeling\nof new sequences. We also propose a new, realistic partitioning of a\nwell-known, high quality motion-capture dataset for better evaluations. We\nfurther explore a novel formulation for future-predicting decoders based on\nconditional recurrent generative adversarial networks, for which we propose\nboth soft and hard constraints for transition generation derived from desired\nphysical properties of synthesized future movements and desired animation\ngoals. We find that using such constraints allow to stabilize the training of\nrecurrent adversarial architectures for animation generation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 15:47:55 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 01:03:26 GMT"}, {"version": "v3", "created": "Mon, 25 Jul 2016 18:13:00 GMT"}, {"version": "v4", "created": "Fri, 26 May 2017 18:31:48 GMT"}, {"version": "v5", "created": "Mon, 5 Jun 2017 13:54:26 GMT"}, {"version": "v6", "created": "Tue, 6 Jun 2017 12:54:48 GMT"}, {"version": "v7", "created": "Wed, 21 Feb 2018 15:04:43 GMT"}, {"version": "v8", "created": "Wed, 11 Jul 2018 12:25:55 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Harvey", "F\u00e9lix G.", ""], ["Roy", "Julien", ""], ["Kanaa", "David", ""], ["Pal", "Christopher", ""]]}, {"id": "1511.06654", "submitter": "Bing Wang", "authors": "Bing Wang, Gang Wang, Kap Luk Chan and Li Wang", "title": "Tracklet Association by Online Target-Specific Metric Learning and\n  Coherent Dynamics Estimation", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence, in\n  press, 2016", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2551245", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel method based on online target-specific\nmetric learning and coherent dynamics estimation for tracklet (track fragment)\nassociation by network flow optimization in long-term multi-person tracking.\nOur proposed framework aims to exploit appearance and motion cues to prevent\nidentity switches during tracking and to recover missed detections.\nFurthermore, target-specific metrics (appearance cue) and motion dynamics\n(motion cue) are proposed to be learned and estimated online, i.e. during the\ntracking process. Our approach is effective even when such cues fail to\nidentify or follow the target due to occlusions or object-to-object\ninteractions. We also propose to learn the weights of these two tracking cues\nto handle the difficult situations, such as severe occlusions and\nobject-to-object interactions effectively. Our method has been validated on\nseveral public datasets and the experimental results show that it outperforms\nseveral state-of-the-art tracking methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 15:48:21 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 03:53:35 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Wang", "Bing", ""], ["Wang", "Gang", ""], ["Chan", "Kap Luk", ""], ["Wang", "Li", ""]]}, {"id": "1511.06674", "submitter": "Marius Leordeanu", "authors": "Anirudh Goyal and Marius Leordeanu", "title": "Stories in the Eye: Contextual Visual Interactions for Efficient Video\n  to Language Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating higher level visual and linguistic interpretations is at the\nheart of human intelligence. As automatic visual category recognition in images\nis approaching human performance, the high level understanding in the dynamic\nspatiotemporal domain of videos and its translation into natural language is\nstill far from being solved. While most works on vision-to-text translations\nuse pre-learned or pre-established computational linguistic models, in this\npaper we present an approach that uses vision alone to efficiently learn how to\ntranslate into language the video content. We discover, in simple form, the\nstory played by main actors, while using only visual cues for representing\nobjects and their interactions. Our method learns in a hierarchical manner\nhigher level representations for recognizing subjects, actions and objects\ninvolved, their relevant contextual background and their interaction to one\nanother over time. We have a three stage approach: first we take in\nconsideration features of the individual entities at the local level of\nappearance, then we consider the relationship between these objects and actions\nand their video background, and third, we consider their spatiotemporal\nrelations as inputs to classifiers at the highest level of interpretation.\nThus, our approach finds a coherent linguistic description of videos in the\nform of a subject, verb and object based on their role played in the overall\nvisual story learned directly from training data, without using a known\nlanguage model. We test the efficiency of our approach on a large scale dataset\ncontaining YouTube clips taken in the wild and demonstrate state-of-the-art\nperformance, often superior to current approaches that use more complex,\npre-learned linguistic knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 16:33:13 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Goyal", "Anirudh", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1511.06676", "submitter": "James Charles", "authors": "James Charles and Tomas Pfister and Derek Magee and David Hogg and\n  Andrew Zisserman", "title": "Personalizing Human Video Pose Estimation", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a personalized ConvNet pose estimator that automatically adapts\nitself to the uniqueness of a person's appearance to improve pose estimation in\nlong videos. We make the following contributions: (i) we show that given a few\nhigh-precision pose annotations, e.g. from a generic ConvNet pose estimator,\nadditional annotations can be generated throughout the video using a\ncombination of image-based matching for temporally distant frames, and dense\noptical flow for temporally local frames; (ii) we develop an occlusion aware\nself-evaluation model that is able to automatically select the high-quality and\nreject the erroneous additional annotations; and (iii) we demonstrate that\nthese high-quality annotations can be used to fine-tune a ConvNet pose\nestimator and thereby personalize it to lock on to key discriminative features\nof the person's appearance. The outcome is a substantial improvement in the\npose estimates for the target video using the personalized ConvNet compared to\nthe original generic ConvNet. Our method outperforms the state of the art\n(including top ConvNet methods) by a large margin on two standard benchmarks,\nas well as on a new challenging YouTube video dataset. Furthermore, we show\nthat training from the automatically generated annotations can be used to\nimprove the performance of a generic ConvNet on other benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 16:34:42 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2016 11:05:05 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Charles", "James", ""], ["Pfister", "Tomas", ""], ["Magee", "Derek", ""], ["Hogg", "David", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1511.06681", "submitter": "Du Tran", "authors": "Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, Manohar\n  Paluri", "title": "Deep End2End Voxel2Voxel Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years deep learning methods have emerged as one of the most\nprominent approaches for video analysis. However, so far their most successful\napplications have been in the area of video classification and detection, i.e.,\nproblems involving the prediction of a single class label or a handful of\noutput variables per video. Furthermore, while deep networks are commonly\nrecognized as the best models to use in these domains, there is a widespread\nperception that in order to yield successful results they often require\ntime-consuming architecture search, manual tweaking of parameters and\ncomputationally intensive pre-processing or post-processing methods.\n  In this paper we challenge these views by presenting a deep 3D convolutional\narchitecture trained end to end to perform voxel-level prediction, i.e., to\noutput a variable at every voxel of the video. Most importantly, we show that\nthe same exact architecture can be used to achieve competitive results on three\nwidely different voxel-prediction tasks: video semantic segmentation, optical\nflow estimation, and video coloring. The three networks learned on these\nproblems are trained from raw video without any form of preprocessing and their\noutputs do not require post-processing to achieve outstanding performance.\nThus, they offer an efficient alternative to traditional and much more\ncomputationally expensive methods in these video domains.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 16:42:37 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Tran", "Du", ""], ["Bourdev", "Lubomir", ""], ["Fergus", "Rob", ""], ["Torresani", "Lorenzo", ""], ["Paluri", "Manohar", ""]]}, {"id": "1511.06683", "submitter": "Maksim Lapin", "authors": "Maksim Lapin, Matthias Hein and Bernt Schiele", "title": "Top-k Multiclass SVM", "comments": "NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class ambiguity is typical in image classification problems with a large\nnumber of classes. When classes are difficult to discriminate, it makes sense\nto allow k guesses and evaluate classifiers based on the top-k error instead of\nthe standard zero-one loss. We propose top-k multiclass SVM as a direct method\nto optimize for top-k performance. Our generalization of the well-known\nmulticlass SVM is based on a tight convex upper bound of the top-k error. We\npropose a fast optimization scheme based on an efficient projection onto the\ntop-k simplex, which is of its own interest. Experiments on five datasets show\nconsistent improvements in top-k accuracy compared to various baselines.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 16:49:33 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Lapin", "Maksim", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1511.06692", "submitter": "Bugra Tekin", "authors": "Bugra Tekin, Artem Rozantsev, Vincent Lepetit, Pascal Fua", "title": "Direct Prediction of 3D Body Poses from Motion Compensated Sequences", "comments": "Published in CVPR 2016. supersedes arXiv:1504.08200", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient approach to exploiting motion information from\nconsecutive frames of a video sequence to recover the 3D pose of people.\nPrevious approaches typically compute candidate poses in individual frames and\nthen link them in a post-processing step to resolve ambiguities. By contrast,\nwe directly regress from a spatio-temporal volume of bounding boxes to a 3D\npose in the central frame.\n  We further show that, for this approach to achieve its full potential, it is\nessential to compensate for the motion in consecutive frames so that the\nsubject remains centered. This then allows us to effectively overcome\nambiguities and improve upon the state-of-the-art by a large margin on the\nHuman3.6m, HumanEva, and KTH Multiview Football 3D human pose estimation\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 17:08:18 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2015 06:50:43 GMT"}, {"version": "v3", "created": "Tue, 2 Aug 2016 07:05:26 GMT"}, {"version": "v4", "created": "Fri, 2 Sep 2016 09:38:08 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Tekin", "Bugra", ""], ["Rozantsev", "Artem", ""], ["Lepetit", "Vincent", ""], ["Fua", "Pascal", ""]]}, {"id": "1511.06702", "submitter": "Maxim Tatarchenko", "authors": "Maxim Tatarchenko, Alexey Dosovitskiy, Thomas Brox", "title": "Multi-view 3D Models from Single Images with a Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a convolutional network capable of inferring a 3D representation\nof a previously unseen object given a single image of this object. Concretely,\nthe network can predict an RGB image and a depth map of the object as seen from\nan arbitrary view. Several of these depth maps fused together give a full point\ncloud of the object. The point cloud can in turn be transformed into a surface\nmesh. The network is trained on renderings of synthetic 3D models of cars and\nchairs. It successfully deals with objects on cluttered background and\ngenerates reasonable predictions for real images of cars.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 17:34:17 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 10:14:07 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Tatarchenko", "Maxim", ""], ["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1511.06704", "submitter": "Ot\\'avio Penatti", "authors": "Ot\\'avio A. B. Penatti and Sandra Avila and Eduardo Valle and Ricardo\n  da S. Torres", "title": "Semantic Diversity versus Visual Diversity in Visual Dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual dictionaries are a critical component for image\nclassification/retrieval systems based on the bag-of-visual-words (BoVW) model.\nDictionaries are usually learned without supervision from a training set of\nimages sampled from the collection of interest. However, for large,\ngeneral-purpose, dynamic image collections (e.g., the Web), obtaining a\nrepresentative sample in terms of semantic concepts is not straightforward. In\nthis paper, we evaluate the impact of semantics in the dictionary quality,\naiming at verifying the importance of semantic diversity in relation visual\ndiversity for visual dictionaries. In the experiments, we vary the amount of\nclasses used for creating the dictionary and then compute different BoVW\ndescriptors, using multiple codebook sizes and different coding and pooling\nmethods (standard BoVW and Fisher Vectors). Results for image classification\nshow that as visual dictionaries are based on low-level visual appearances,\nvisual diversity is more important than semantic diversity. Our conclusions\nopen the opportunity to alleviate the burden in generating visual dictionaries\nas we need only a visually diverse set of images instead of the whole\ncollection to create a good dictionary.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 17:38:15 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Penatti", "Ot\u00e1vio A. B.", ""], ["Avila", "Sandra", ""], ["Valle", "Eduardo", ""], ["Torres", "Ricardo da S.", ""]]}, {"id": "1511.06728", "submitter": "Natalia Neverova", "authors": "Natalia Neverova, Christian Wolf, Florian Nebout, Graham Taylor", "title": "Hand Pose Estimation through Semi-Supervised and Weakly-Supervised\n  Learning", "comments": "13 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for hand pose estimation based on a deep regressor\ntrained on two different kinds of input. Raw depth data is fused with an\nintermediate representation in the form of a segmentation of the hand into\nparts. This intermediate representation contains important topological\ninformation and provides useful cues for reasoning about joint locations. The\nmapping from raw depth to segmentation maps is learned in a\nsemi/weakly-supervised way from two different datasets: (i) a synthetic dataset\ncreated through a rendering pipeline including densely labeled ground truth\n(pixelwise segmentations); and (ii) a dataset with real images for which ground\ntruth joint positions are available, but not dense segmentations. Loss for\ntraining on real images is generated from a patch-wise restoration process,\nwhich aligns tentative segmentation maps with a large dictionary of synthetic\nposes. The underlying premise is that the domain shift between synthetic and\nreal data is smaller in the intermediate representation, where labels carry\ngeometric and topological meaning, than in the raw input domain. Experiments on\nthe NYU dataset show that the proposed training method decreases error on\njoints over direct regression of joints from depth data by 15.7%.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 19:19:00 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 13:31:05 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 06:08:54 GMT"}, {"version": "v4", "created": "Fri, 15 Sep 2017 09:24:57 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Neverova", "Natalia", ""], ["Wolf", "Christian", ""], ["Nebout", "Florian", ""], ["Taylor", "Graham", ""]]}, {"id": "1511.06739", "submitter": "Varun Jampani", "authors": "Raghudeep Gadde and Varun Jampani and Martin Kiefel and Daniel Kappler\n  and Peter V. Gehler", "title": "Superpixel Convolutional Networks using Bilateral Inceptions", "comments": "European Conference on Computer Vision (ECCV), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a CNN architecture for semantic image segmentation.\nWe introduce a new 'bilateral inception' module that can be inserted in\nexisting CNN architectures and performs bilateral filtering, at multiple\nfeature-scales, between superpixels in an image. The feature spaces for\nbilateral filtering and other parameters of the module are learned end-to-end\nusing standard backpropagation techniques. The bilateral inception module\naddresses two issues that arise with general CNN segmentation architectures.\nFirst, this module propagates information between (super) pixels while\nrespecting image edges, thus using the structured information of the problem\nfor improved results. Second, the layer recovers a full resolution segmentation\nresult from the lower resolution solution of a CNN. In the experiments, we\nmodify several existing CNN architectures by inserting our inception module\nbetween the last CNN (1x1 convolution) layers. Empirical results on three\ndifferent datasets show reliable improvements not only in comparison to the\nbaseline networks, but also in comparison to several dense-pixel prediction\ntechniques such as CRFs, while being competitive in time.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 19:58:38 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2015 10:43:52 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 09:10:31 GMT"}, {"version": "v4", "created": "Fri, 5 Aug 2016 09:14:18 GMT"}, {"version": "v5", "created": "Mon, 8 Aug 2016 15:31:14 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Gadde", "Raghudeep", ""], ["Jampani", "Varun", ""], ["Kiefel", "Martin", ""], ["Kappler", "Daniel", ""], ["Gehler", "Peter V.", ""]]}, {"id": "1511.06744", "submitter": "Yani Ioannou", "authors": "Yani Ioannou, Duncan Robertson, Jamie Shotton, Roberto Cipolla,\n  Antonio Criminisi", "title": "Training CNNs with Low-Rank Filters for Efficient Image Classification", "comments": "Published as a conference paper at ICLR 2016. v3: updated ICLR\n  status. v2: Incorporated reviewer's feedback including: Amend Fig. 2 and 5\n  descriptions to explain that there are no ReLUs within the figures. Fix\n  headings of Table 5 - Fix typo in the sentence at bottom of page 6. Add ref.\n  to Predicting Parameters in Deep Learning. Fix Table 6, GMP-LR and GMP-LR-2x\n  had incorrect numbers of filters", "journal-ref": "International Conference on Learning Representations (ICLR), San\n  Juan, Puerto Rico, 2-4 May 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for creating computationally efficient convolutional\nneural networks (CNNs) by using low-rank representations of convolutional\nfilters. Rather than approximating filters in previously-trained networks with\nmore efficient versions, we learn a set of small basis filters from scratch;\nduring training, the network learns to combine these basis filters into more\ncomplex filters that are discriminative for image classification. To train such\nnetworks, a novel weight initialization scheme is used. This allows effective\ninitialization of connection weights in convolutional layers composed of groups\nof differently-shaped filters. We validate our approach by applying it to\nseveral existing CNN architectures and training these networks from scratch\nusing the CIFAR, ILSVRC and MIT Places datasets. Our results show similar or\nhigher accuracy than conventional CNNs with much less compute. Applying our\nmethod to an improved version of VGG-11 network using global max-pooling, we\nachieve comparable validation accuracy using 41% less compute and only 24% of\nthe original VGG-11 model parameters; another variant of our method gives a 1\npercentage point increase in accuracy over our improved VGG-11 model, giving a\ntop-5 center-crop validation accuracy of 89.7% while reducing computation by\n16% relative to the original VGG-11 model. Applying our method to the GoogLeNet\narchitecture for ILSVRC, we achieved comparable accuracy with 26% less compute\nand 41% fewer model parameters. Applying our method to a near state-of-the-art\nnetwork for CIFAR, we achieved comparable accuracy with 46% less compute and\n55% fewer parameters.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 20:14:28 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2016 17:07:02 GMT"}, {"version": "v3", "created": "Sun, 7 Feb 2016 21:23:19 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Ioannou", "Yani", ""], ["Robertson", "Duncan", ""], ["Shotton", "Jamie", ""], ["Cipolla", "Roberto", ""], ["Criminisi", "Antonio", ""]]}, {"id": "1511.06746", "submitter": "Kamelia Aryafar", "authors": "Corey Lynch, Kamelia Aryafar, Josh Attenberg", "title": "Images Don't Lie: Transferring Deep Visual Semantic Features to\n  Large-Scale Multimodal Learning to Rank", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search is at the heart of modern e-commerce. As a result, the task of ranking\nsearch results automatically (learning to rank) is a multibillion dollar\nmachine learning problem. Traditional models optimize over a few\nhand-constructed features based on the item's text. In this paper, we introduce\na multimodal learning to rank model that combines these traditional features\nwith visual semantic features transferred from a deep convolutional neural\nnetwork. In a large scale experiment using data from the online marketplace\nEtsy, we verify that moving to a multimodal representation significantly\nimproves ranking quality. We show how image features can capture fine-grained\nstyle information not available in a text-only representation. In addition, we\nshow concrete examples of how image information can successfully disentangle\npairs of highly different items that are ranked similarly by a text-only model.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 20:26:26 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Lynch", "Corey", ""], ["Aryafar", "Kamelia", ""], ["Attenberg", "Josh", ""]]}, {"id": "1511.06783", "submitter": "Katsunori Ohnishi", "authors": "Katsunori Ohnishi, Atsushi Kanehira, Asako Kanezaki, Tatsuya Harada", "title": "Recognizing Activities of Daily Living with a Wrist-mounted Camera", "comments": "CVPR2016 spotlight presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel dataset and a novel algorithm for recognizing activities\nof daily living (ADL) from a first-person wearable camera. Handled objects are\ncrucially important for egocentric ADL recognition. For specific examination of\nobjects related to users' actions separately from other objects in an\nenvironment, many previous works have addressed the detection of handled\nobjects in images captured from head-mounted and chest-mounted cameras.\nNevertheless, detecting handled objects is not always easy because they tend to\nappear small in images. They can be occluded by a user's body. As described\nherein, we mount a camera on a user's wrist. A wrist-mounted camera can capture\nhandled objects at a large scale, and thus it enables us to skip object\ndetection process. To compare a wrist-mounted camera and a head-mounted camera,\nwe also develop a novel and publicly available dataset that includes videos and\nannotations of daily activities captured simultaneously by both cameras.\nAdditionally, we propose a discriminative video representation that retains\nspatial and temporal information after encoding frame descriptors extracted by\nConvolutional Neural Networks (CNN).\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 22:02:09 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 04:39:03 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Ohnishi", "Katsunori", ""], ["Kanehira", "Atsushi", ""], ["Kanezaki", "Asako", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1511.06789", "submitter": "Jonathan Krause", "authors": "Jonathan Krause, Benjamin Sapp, Andrew Howard, Howard Zhou, Alexander\n  Toshev, Tom Duerig, James Philbin, Li Fei-Fei", "title": "The Unreasonable Effectiveness of Noisy Data for Fine-Grained\n  Recognition", "comments": "ECCV 2016, data is released", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches for fine-grained recognition do the following: First,\nrecruit experts to annotate a dataset of images, optionally also collecting\nmore structured data in the form of part annotations and bounding boxes.\nSecond, train a model utilizing this data. Toward the goal of solving\nfine-grained recognition, we introduce an alternative approach, leveraging\nfree, noisy data from the web and simple, generic methods of recognition. This\napproach has benefits in both performance and scalability. We demonstrate its\nefficacy on four fine-grained datasets, greatly exceeding existing state of the\nart without the manual collection of even a single label, and furthermore show\nfirst results at scaling to more than 10,000 fine-grained categories.\nQuantitatively, we achieve top-1 accuracies of 92.3% on CUB-200-2011, 85.4% on\nBirdsnap, 93.4% on FGVC-Aircraft, and 80.8% on Stanford Dogs without using\ntheir annotated training sets. We compare our approach to an active learning\napproach for expanding fine-grained datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 22:40:30 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 08:22:52 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2016 18:35:31 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Krause", "Jonathan", ""], ["Sapp", "Benjamin", ""], ["Howard", "Andrew", ""], ["Zhou", "Howard", ""], ["Toshev", "Alexander", ""], ["Duerig", "Tom", ""], ["Philbin", "James", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1511.06811", "submitter": "Phillip Isola", "authors": "Phillip Isola, Daniel Zoran, Dilip Krishnan, Edward H. Adelson", "title": "Learning visual groups from co-occurrences in space and time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-supervised framework that learns to group visual entities\nbased on their rate of co-occurrence in space and time. To model statistical\ndependencies between the entities, we set up a simple binary classification\nproblem in which the goal is to predict if two visual primitives occur in the\nsame spatial or temporal context. We apply this framework to three domains:\nlearning patch affinities from spatial adjacency in images, learning frame\naffinities from temporal adjacency in videos, and learning photo affinities\nfrom geospatial proximity in image collections. We demonstrate that in each\ncase the learned affinities uncover meaningful semantic groupings. From patch\naffinities we generate object proposals that are competitive with\nstate-of-the-art supervised methods. From frame affinities we generate movie\nscene segmentations that correlate well with DVD chapter structure. Finally,\nfrom geospatial affinities we learn groups that relate well to semantic place\ncategories.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 01:33:12 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Isola", "Phillip", ""], ["Zoran", "Daniel", ""], ["Krishnan", "Dilip", ""], ["Adelson", "Edward H.", ""]]}, {"id": "1511.06815", "submitter": "Ju Shen Dr.", "authors": "Xinzhong Lu, Ju Shen, Saverio Perugini, Jianjun Yang", "title": "An Immersive Telepresence System using RGB-D Sensors and Head Mounted\n  Display", "comments": "IEEE International Symposium on Multimedia 2015", "journal-ref": null, "doi": "10.1109/ISM.2015.108", "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a tele-immersive system that enables people to interact with each\nother in a virtual world using body gestures in addition to verbal\ncommunication. Beyond the obvious applications, including general online\nconversations and gaming, we hypothesize that our proposed system would be\nparticularly beneficial to education by offering rich visual contents and\ninteractivity. One distinct feature is the integration of egocentric pose\nrecognition that allows participants to use their gestures to demonstrate and\nmanipulate virtual objects simultaneously. This functionality enables the\ninstructor to ef- fectively and efficiently explain and illustrate complex\nconcepts or sophisticated problems in an intuitive manner. The highly\ninteractive and flexible environment can capture and sustain more student\nattention than the traditional classroom setting and, thus, delivers a\ncompelling experience to the students. Our main focus here is to investigate\npossible solutions for the system design and implementation and devise\nstrategies for fast, efficient computation suitable for visual data processing\nand network transmission. We describe the technique and experiments in details\nand provide quantitative performance results, demonstrating our system can be\nrun comfortably and reliably for different application scenarios. Our\npreliminary results are promising and demonstrate the potential for more\ncompelling directions in cyberlearning.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 01:57:47 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lu", "Xinzhong", ""], ["Shen", "Ju", ""], ["Perugini", "Saverio", ""], ["Yang", "Jianjun", ""]]}, {"id": "1511.06830", "submitter": "Xuan Dong", "authors": "Xuan Dong, Boyan Bonev, Weixin Li, Weichao Qiu, Xianjie Chen, Alan\n  Yuille", "title": "Ground-truth dataset and baseline evaluations for image base-detail\n  separation algorithms", "comments": "This paper has been withdrawn by the author due to some un-proper\n  examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Base-detail separation is a fundamental computer vision problem consisting of\nmodeling a smooth base layer with the coarse structures, and a detail layer\ncontaining the texture-like structures. One of the challenges of estimating the\nbase is to preserve sharp boundaries between objects or parts to avoid halo\nartifacts. Many methods have been proposed to address this problem, but there\nis no ground-truth dataset of real images for quantitative evaluation. We\nproposed a procedure to construct such a dataset, and provide two datasets:\nPascal Base-Detail and Fashionista Base-Detail, containing 1000 and 250 images,\nrespectively. Our assumption is that the base is piecewise smooth and we label\nthe appearance of each piece by a polynomial model. The pieces are objects and\nparts of objects, obtained from human annotations. Finally, we proposed a way\nto evaluate methods with our base-detail ground-truth and we compared the\nperformances of seven state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 04:04:39 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 22:59:13 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Dong", "Xuan", ""], ["Bonev", "Boyan", ""], ["Li", "Weixin", ""], ["Qiu", "Weichao", ""], ["Chen", "Xianjie", ""], ["Yuille", "Alan", ""]]}, {"id": "1511.06834", "submitter": "Xuan Dong", "authors": "Xuan Dong, Yu Zhu, Weixin Li, Lingxi Xie, Alex Wong, Alan Yuille", "title": "Fidelity-Naturalness Evaluation of Single Image Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of evaluating super resolution methods. Traditional\nevaluation methods usually judge the quality of super resolved images based on\na single measure of their difference with the original high resolution images.\nIn this paper, we proposed to use both fidelity (the difference with original\nimages) and naturalness (human visual perception of super resolved images) for\nevaluation. For fidelity evaluation, a new metric is proposed to solve the bias\nproblem of traditional evaluation. For naturalness evaluation, we let humans\nlabel preference of super resolution results using pair-wise comparison, and\ntest the correlation between human labeling results and image quality\nassessment metrics' outputs. Experimental results show that our\nfidelity-naturalness method is better than the traditional evaluation method\nfor super resolution methods, which could help future research on single-image\nsuper resolution.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 04:40:59 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Dong", "Xuan", ""], ["Zhu", "Yu", ""], ["Li", "Weixin", ""], ["Xie", "Lingxi", ""], ["Wong", "Alex", ""], ["Yuille", "Alan", ""]]}, {"id": "1511.06838", "submitter": "Stella Yu", "authors": "Takuya Narihira, Damian Borth, Stella X. Yu, Karl Ni, Trevor Darrell", "title": "Mapping Images to Sentiment Adjective Noun Pairs with Factorized Neural\n  Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the visual sentiment task of mapping an image to an adjective\nnoun pair (ANP) such as \"cute baby\". To capture the two-factor structure of our\nANP semantics as well as to overcome annotation noise and ambiguity, we propose\na novel factorized CNN model which learns separate representations for\nadjectives and nouns but optimizes the classification performance over their\nproduct. Our experiments on the publicly available SentiBank dataset show that\nour model significantly outperforms not only independent ANP classifiers on\nunseen ANPs and on retrieving images of novel ANPs, but also image captioning\nmodels which capture word semantics from co-occurrence of natural text; the\nlatter turn out to be surprisingly poor at capturing the sentiment evoked by\npure visual experience. That is, our factorized ANP CNN not only trains better\nfrom noisy labels, generalizes better to new images, but can also expands the\nANP vocabulary on its own.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 04:58:46 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Narihira", "Takuya", ""], ["Borth", "Damian", ""], ["Yu", "Stella X.", ""], ["Ni", "Karl", ""], ["Darrell", "Trevor", ""]]}, {"id": "1511.06853", "submitter": "Yichao Xu", "authors": "Yichao Xu, Hajime Nagahara, Atsushi Shimada, Rin-ichiro Taniguchi", "title": "TransCut: Transparent Object Segmentation from a Light-Field Image", "comments": "9 pages, 14 figures, 2 tables, ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segmentation of transparent objects can be very useful in computer vision\napplications. However, because they borrow texture from their background and\nhave a similar appearance to their surroundings, transparent objects are not\nhandled well by regular image segmentation methods. We propose a method that\novercomes these problems using the consistency and distortion properties of a\nlight-field image. Graph-cut optimization is applied for the pixel labeling\nproblem. The light-field linearity is used to estimate the likelihood of a\npixel belonging to the transparent object or Lambertian background, and the\nocclusion detector is used to find the occlusion boundary. We acquire a light\nfield dataset for the transparent object, and use this dataset to evaluate our\nmethod. The results demonstrate that the proposed method successfully segments\ntransparent objects from the background.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 08:33:18 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Xu", "Yichao", ""], ["Nagahara", "Hajime", ""], ["Shimada", "Atsushi", ""], ["Taniguchi", "Rin-ichiro", ""]]}, {"id": "1511.06855", "submitter": "Jianyu Wang", "authors": "Jianyu Wang, Zhishuai Zhang, Cihang Xie, Vittal Premachandran, Alan\n  Yuille", "title": "Unsupervised learning of object semantic parts from internal states of\n  CNNs by population encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the key question of how object part representations can be found\nfrom the internal states of CNNs that are trained for high-level tasks, such as\nobject classification. This work provides a new unsupervised method to learn\nsemantic parts and gives new understanding of the internal representations of\nCNNs. Our technique is based on the hypothesis that semantic parts are\nrepresented by populations of neurons rather than by single filters. We propose\na clustering technique to extract part representations, which we call Visual\nConcepts. We show that visual concepts are semantically coherent in that they\nrepresent semantic parts, and visually coherent in that corresponding image\npatches appear very similar. Also, visual concepts provide full spatial\ncoverage of the parts of an object, rather than a few sparse parts as is\ntypically found in keypoint annotations. Furthermore, We treat single visual\nconcept as part detector and evaluate it for keypoint detection using the\nPASCAL3D+ dataset and for part detection using our newly annotated ImageNetPart\ndataset. The experiments demonstrate that visual concepts can be used to detect\nparts. We also show that some visual concepts respond to several semantic\nparts, provided these parts are visually similar. Thus visual concepts have the\nessential properties: semantic meaning and detection capability. Note that our\nImageNetPart dataset gives rich part annotations which cover the whole object,\nmaking it useful for other part-related applications.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 09:02:21 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 22:10:52 GMT"}, {"version": "v3", "created": "Sat, 12 Nov 2016 13:37:07 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Wang", "Jianyu", ""], ["Zhang", "Zhishuai", ""], ["Xie", "Cihang", ""], ["Premachandran", "Vittal", ""], ["Yuille", "Alan", ""]]}, {"id": "1511.06856", "submitter": "Philipp Kr\\\"ahenb\\\"uhl", "authors": "Philipp Kr\\\"ahenb\\\"uhl, Carl Doersch, Jeff Donahue, Trevor Darrell", "title": "Data-dependent Initializations of Convolutional Neural Networks", "comments": "ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks spread through computer vision like a wildfire,\nimpacting almost all visual tasks imaginable. Despite this, few researchers\ndare to train their models from scratch. Most work builds on one of a handful\nof ImageNet pre-trained models, and fine-tunes or adapts these for specific\ntasks. This is in large part due to the difficulty of properly initializing\nthese networks from scratch. A small miscalibration of the initial weights\nleads to vanishing or exploding gradients, as well as poor convergence\nproperties. In this work we present a fast and simple data-dependent\ninitialization procedure, that sets the weights of a network such that all\nunits in the network train at roughly the same rate, avoiding vanishing or\nexploding gradients. Our initialization matches the current state-of-the-art\nunsupervised or self-supervised pre-training methods on standard computer\nvision tasks, such as image classification and object detection, while being\nroughly three orders of magnitude faster. When combined with pre-training\nmethods, our initialization significantly outperforms prior work, narrowing the\ngap between supervised and unsupervised pre-training.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 09:07:08 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2016 03:36:16 GMT"}, {"version": "v3", "created": "Thu, 22 Sep 2016 22:14:17 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Kr\u00e4henb\u00fchl", "Philipp", ""], ["Doersch", "Carl", ""], ["Donahue", "Jeff", ""], ["Darrell", "Trevor", ""]]}, {"id": "1511.06860", "submitter": "Canyi Lu", "authors": "Canyi Lu, Shuicheng Yan, Zhouchen Lin", "title": "Convex Sparse Spectral Clustering: Single-view to Multi-view", "comments": null, "journal-ref": "IEEE Transactions on Image Processing (TIP), vol. 25, pp.\n  2833-2843, 2016", "doi": "10.1109/TIP.2016.2553459", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral Clustering (SC) is one of the most widely used methods for data\nclustering. It first finds a low-dimensonal embedding $U$ of data by computing\nthe eigenvectors of the normalized Laplacian matrix, and then performs k-means\non $U^\\top$ to get the final clustering result. In this work, we observe that,\nin the ideal case, $UU^\\top$ should be block diagonal and thus sparse.\nTherefore we propose the Sparse Spectral Clustering (SSC) method which extends\nSC with sparse regularization on $UU^\\top$. To address the computational issue\nof the nonconvex SSC model, we propose a novel convex relaxation of SSC based\non the convex hull of the fixed rank projection matrices. Then the convex SSC\nmodel can be efficiently solved by the Alternating Direction Method of\n\\canyi{Multipliers} (ADMM). Furthermore, we propose the Pairwise Sparse\nSpectral Clustering (PSSC) which extends SSC to boost the clustering\nperformance by using the multi-view information of data. Experimental\ncomparisons with several baselines on real-world datasets testify to the\nefficacy of our proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 09:38:31 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 12:45:01 GMT"}, {"version": "v3", "created": "Sun, 27 May 2018 06:02:40 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Lu", "Canyi", ""], ["Yan", "Shuicheng", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1511.06881", "submitter": "Fangting Xia", "authors": "Fangting Xia, Peng Wang, Liang-Chieh Chen, Alan L. Yuille", "title": "Zoom Better to See Clearer: Human and Object Parsing with Hierarchical\n  Auto-Zoom Net", "comments": "A shortened version has been submitted to ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing articulated objects, e.g. humans and animals, into semantic parts\n(e.g. body, head and arms, etc.) from natural images is a challenging and\nfundamental problem for computer vision. A big difficulty is the large\nvariability of scale and location for objects and their corresponding parts.\nEven limited mistakes in estimating scale and location will degrade the parsing\noutput and cause errors in boundary details. To tackle these difficulties, we\npropose a \"Hierarchical Auto-Zoom Net\" (HAZN) for object part parsing which\nadapts to the local scales of objects and parts. HAZN is a sequence of two\n\"Auto-Zoom Net\" (AZNs), each employing fully convolutional networks that\nperform two tasks: (1) predict the locations and scales of object instances\n(the first AZN) or their parts (the second AZN); (2) estimate the part scores\nfor predicted object instance or part regions. Our model can adaptively \"zoom\"\n(resize) predicted image regions into their proper scales to refine the\nparsing.\n  We conduct extensive experiments over the PASCAL part datasets on humans,\nhorses, and cows. For humans, our approach significantly outperforms the\nstate-of-the-arts by 5% mIOU and is especially better at segmenting small\ninstances and small parts. We obtain similar improvements for parsing cows and\nhorses over alternative methods. In summary, our strategy of first zooming into\nobjects and then zooming into parts is very effective. It also enables us to\nprocess different regions of the image at different scales adaptively so that,\nfor example, we do not need to waste computational resources scaling the entire\nimage.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 13:32:26 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 00:39:14 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 02:32:33 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2016 23:48:34 GMT"}, {"version": "v5", "created": "Mon, 28 Mar 2016 21:53:31 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Xia", "Fangting", ""], ["Wang", "Peng", ""], ["Chen", "Liang-Chieh", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1511.06911", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Amirali Abdolrashidi and Yao Wang", "title": "Screen Content Image Segmentation Using Sparse-Smooth Decomposition", "comments": "Asilomar Conference on Signals, Systems and Computers, IEEE, 2015,\n  (to Appear)", "journal-ref": null, "doi": "10.1109/ACSSC.2015.7421331", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse decomposition has been extensively used for different applications\nincluding signal compression and denoising and document analysis. In this\npaper, sparse decomposition is used for image segmentation. The proposed\nalgorithm separates the background and foreground using a sparse-smooth\ndecomposition technique such that the smooth and sparse components correspond\nto the background and foreground respectively. This algorithm is tested on\nseveral test images from HEVC test sequences and is shown to have superior\nperformance over other methods, such as the hierarchical k-means clustering in\nDjVu. This segmentation algorithm can also be used for text extraction, video\ncompression and medical image segmentation.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 17:55:14 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Minaee", "Shervin", ""], ["Abdolrashidi", "Amirali", ""], ["Wang", "Yao", ""]]}, {"id": "1511.06919", "submitter": "Philipp Kainz", "authors": "Philipp Kainz, Michael Pfeiffer, and Martin Urschler", "title": "Semantic Segmentation of Colon Glands with Deep Convolutional Neural\n  Networks and Total Variation Segmentation", "comments": "An extended version of this work has been published in PeerJ\n  (https://doi.org/10.7717/peerj.3874), so please cite our journal version\n  instead of this preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of histopathology sections is an ubiquitous requirement in\ndigital pathology and due to the large variability of biological tissue,\nmachine learning techniques have shown superior performance over standard image\nprocessing methods. As part of the GlaS@MICCAI2015 colon gland segmentation\nchallenge, we present a learning-based algorithm to segment glands in tissue of\nbenign and malignant colorectal cancer. Images are preprocessed according to\nthe Hematoxylin-Eosin staining protocol and two deep convolutional neural\nnetworks (CNN) are trained as pixel classifiers. The CNN predictions are then\nregularized using a figure-ground segmentation based on weighted total\nvariation to produce the final segmentation result. On two test sets, our\napproach achieves a tissue classification accuracy of 98% and 94%, making use\nof the inherent capability of our system to distinguish between benign and\nmalignant tissue.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 20:13:24 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 11:44:54 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Kainz", "Philipp", ""], ["Pfeiffer", "Michael", ""], ["Urschler", "Martin", ""]]}, {"id": "1511.06936", "submitter": "Mohammad Sabokrou", "authors": "Mohammad Sabokrou, Mahmood Fathy, Mojtaba Hosseini, Reinhard Klette", "title": "Real-Time Anomaly Detection and Localization in Crowded Scenes", "comments": "CVPRw 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we propose a method for real-time anomaly detection and\nlocalization in crowded scenes. Each video is defined as a set of\nnon-overlapping cubic patches, and is described using two local and global\ndescriptors. These descriptors capture the video properties from different\naspects. By incorporating simple and cost-effective Gaussian classifiers, we\ncan distinguish normal activities and anomalies in videos. The local and global\nfeatures are based on structure similarity between adjacent patches and the\nfeatures learned in an unsupervised way, using a sparse auto- encoder.\nExperimental results show that our algorithm is comparable to a\nstate-of-the-art procedure on UCSD ped2 and UMN benchmarks, but even more\ntime-efficient. The experiments confirm that our system can reliably detect and\nlocalize anomalies as soon as they happen in a video.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 23:17:55 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Sabokrou", "Mohammad", ""], ["Fathy", "Mahmood", ""], ["Hosseini", "Mojtaba", ""], ["Klette", "Reinhard", ""]]}, {"id": "1511.06951", "submitter": "Leslie Smith", "authors": "Leslie N. Smith, Emily M. Hand, Timothy Doster", "title": "Gradual DropIn of Layers to Train Very Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of dynamically growing a neural network during\ntraining. In particular, an untrainable deep network starts as a trainable\nshallow network and newly added layers are slowly, organically added during\ntraining, thereby increasing the network's depth. This is accomplished by a new\nlayer, which we call DropIn. The DropIn layer starts by passing the output from\na previous layer (effectively skipping over the newly added layers), then\nincreasingly including units from the new layers for both feedforward and\nbackpropagation. We show that deep networks, which are untrainable with\nconventional methods, will converge with DropIn layers interspersed in the\narchitecture. In addition, we demonstrate that DropIn provides regularization\nduring training in an analogous way as dropout. Experiments are described with\nthe MNIST dataset and various expanded LeNet architectures, CIFAR-10 dataset\nwith its architecture expanded from 3 to 11 layers, and on the ImageNet dataset\nwith the AlexNet architecture expanded to 13 layers and the VGG 16-layer\narchitecture.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 02:33:08 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Smith", "Leslie N.", ""], ["Hand", "Emily M.", ""], ["Doster", "Timothy", ""]]}, {"id": "1511.06973", "submitter": "Chunhua Shen", "authors": "Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, Anton van den Hengel", "title": "Ask Me Anything: Free-form Visual Question Answering Based on Knowledge\n  from External Sources", "comments": "Accepted to IEEE Conf. Computer Vision and Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for visual question answering which combines an internal\nrepresentation of the content of an image with information extracted from a\ngeneral knowledge base to answer a broad range of image-based questions. This\nallows more complex questions to be answered using the predominant neural\nnetwork-based approach than has previously been possible. It particularly\nallows questions to be asked about the contents of an image, even when the\nimage itself does not contain the whole answer. The method constructs a textual\nrepresentation of the semantic content of an image, and merges it with textual\ninformation sourced from a knowledge base, to develop a deeper understanding of\nthe scene viewed. Priming a recurrent neural network with this combined\ninformation, and the submitted question, leads to a very flexible visual\nquestion answering approach. We are specifically able to answer questions posed\nin natural language, that refer to information not contained in the image. We\ndemonstrate the effectiveness of our model on two publicly available datasets,\nToronto COCO-QA and MS COCO-VQA and show that it produces the best reported\nresults in both cases.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 07:08:14 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 08:09:08 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Wu", "Qi", ""], ["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Dick", "Anthony", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1511.06984", "submitter": "Serena Yeung", "authors": "Serena Yeung, Olga Russakovsky, Greg Mori, Li Fei-Fei", "title": "End-to-end Learning of Action Detection from Frame Glimpses in Videos", "comments": "Update to version in CVPR 2016 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a fully end-to-end approach for action detection in\nvideos that learns to directly predict the temporal bounds of actions. Our\nintuition is that the process of detecting actions is naturally one of\nobservation and refinement: observing moments in video, and refining hypotheses\nabout when an action is occurring. Based on this insight, we formulate our\nmodel as a recurrent neural network-based agent that interacts with a video\nover time. The agent observes video frames and decides both where to look next\nand when to emit a prediction. Since backpropagation is not adequate in this\nnon-differentiable setting, we use REINFORCE to learn the agent's decision\npolicy. Our model achieves state-of-the-art results on the THUMOS'14 and\nActivityNet datasets while observing only a fraction (2% or less) of the video\nframes.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 09:41:50 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 07:33:15 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Yeung", "Serena", ""], ["Russakovsky", "Olga", ""], ["Mori", "Greg", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1511.06988", "submitter": "Haitian Zheng", "authors": "Haitian Zheng, Yebin Liu, Mengqi Ji, Feng Wu, Lu Fang", "title": "Learning High-level Prior with Convolutional Neural Networks for\n  Semantic Segmentation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a convolutional neural network that can fuse high-level\nprior for semantic image segmentation. Motivated by humans' vision recognition\nsystem, our key design is a three-layer generative structure consisting of\nhigh-level coding, middle-level segmentation and low-level image to introduce\nglobal prior for semantic segmentation. Based on this structure, we proposed a\ngenerative model called conditional variational auto-encoder (CVAE) that can\nbuild up the links behind these three layers. These important links include an\nimage encoder that extracts high level info from image, a segmentation encoder\nthat extracts high level info from segmentation, and a hybrid decoder that\noutputs semantic segmentation from the high level prior and input image. We\ntheoretically derive the semantic segmentation as an optimization problem\nparameterized by these links. Finally, the optimization problem enables us to\ntake advantage of state-of-the-art fully convolutional network structure for\nthe implementation of the above encoders and decoder. Experimental results on\nseveral representative datasets demonstrate our supreme performance for\nsemantic segmentation.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 10:25:02 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Zheng", "Haitian", ""], ["Liu", "Yebin", ""], ["Ji", "Mengqi", ""], ["Wu", "Feng", ""], ["Fang", "Lu", ""]]}, {"id": "1511.07041", "submitter": "Ankur Handa", "authors": "Ankur Handa, Viorica Patraucean, Vijay Badrinarayanan, Simon Stent,\n  Roberto Cipolla", "title": "SceneNet: Understanding Real World Indoor Scenes With Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scene understanding is a prerequisite to many high level tasks for any\nautomated intelligent machine operating in real world environments. Recent\nattempts with supervised learning have shown promise in this direction but also\nhighlighted the need for enormous quantity of supervised data --- performance\nincreases in proportion to the amount of data used. However, this quickly\nbecomes prohibitive when considering the manual labour needed to collect such\ndata. In this work, we focus our attention on depth based semantic per-pixel\nlabelling as a scene understanding problem and show the potential of computer\ngraphics to generate virtually unlimited labelled data from synthetic 3D\nscenes. By carefully synthesizing training data with appropriate noise models\nwe show comparable performance to state-of-the-art RGBD systems on NYUv2\ndataset despite using only depth data as input and set a benchmark on\ndepth-based segmentation on SUN RGB-D dataset. Additionally, we offer a route\nto generating synthesized frame or video data, and understanding of different\nfactors influencing performance gains.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 17:59:49 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 22:09:09 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Handa", "Ankur", ""], ["Patraucean", "Viorica", ""], ["Badrinarayanan", "Vijay", ""], ["Stent", "Simon", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1511.07053", "submitter": "Francesco Visin", "authors": "Francesco Visin, Marco Ciccone, Adriana Romero, Kyle Kastner,\n  Kyunghyun Cho, Yoshua Bengio, Matteo Matteucci, Aaron Courville", "title": "ReSeg: A Recurrent Neural Network-based Model for Semantic Segmentation", "comments": "In CVPR Deep Vision Workshop, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a structured prediction architecture, which exploits the local\ngeneric features extracted by Convolutional Neural Networks and the capacity of\nRecurrent Neural Networks (RNN) to retrieve distant dependencies. The proposed\narchitecture, called ReSeg, is based on the recently introduced ReNet model for\nimage classification. We modify and extend it to perform the more challenging\ntask of semantic segmentation. Each ReNet layer is composed of four RNN that\nsweep the image horizontally and vertically in both directions, encoding\npatches or activations, and providing relevant global information. Moreover,\nReNet layers are stacked on top of pre-trained convolutional layers, benefiting\nfrom generic local features. Upsampling layers follow ReNet layers to recover\nthe original image resolution in the final predictions. The proposed ReSeg\narchitecture is efficient, flexible and suitable for a variety of semantic\nsegmentation tasks. We evaluate ReSeg on several widely-used semantic\nsegmentation datasets: Weizmann Horse, Oxford Flower, and CamVid; achieving\nstate-of-the-art performance. Results show that ReSeg can act as a suitable\narchitecture for semantic segmentation tasks, and may have further applications\nin other structured prediction problems. The source code and model\nhyperparameters are available on https://github.com/fvisin/reseg.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 19:25:27 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 14:41:56 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 15:55:41 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Visin", "Francesco", ""], ["Ciccone", "Marco", ""], ["Romero", "Adriana", ""], ["Kastner", "Kyle", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""], ["Matteucci", "Matteo", ""], ["Courville", "Aaron", ""]]}, {"id": "1511.07063", "submitter": "Ning Zhang", "authors": "Ning Zhang, Evan Shelhamer, Yang Gao, Trevor Darrell", "title": "Fine-grained pose prediction, normalization, and recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose variation and subtle differences in appearance are key challenges to\nfine-grained classification. While deep networks have markedly improved general\nrecognition, many approaches to fine-grained recognition rely on anchoring\nnetworks to parts for better accuracy. Identifying parts to find correspondence\ndiscounts pose variation so that features can be tuned to appearance. To this\nend previous methods have examined how to find parts and extract\npose-normalized features. These methods have generally separated fine-grained\nrecognition into stages which first localize parts using hand-engineered and\ncoarsely-localized proposal features, and then separately learn deep\ndescriptors centered on inferred part positions. We unify these steps in an\nend-to-end trainable network supervised by keypoint locations and class labels\nthat localizes parts by a fully convolutional network to focus the learning of\nfeature representations for the fine-grained classification task. Experiments\non the popular CUB200 dataset show that our method is state-of-the-art and\nsuggest a continuing role for strong supervision.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 20:32:45 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Zhang", "Ning", ""], ["Shelhamer", "Evan", ""], ["Gao", "Yang", ""], ["Darrell", "Trevor", ""]]}, {"id": "1511.07067", "submitter": "Satwik Kottur", "authors": "Satwik Kottur, Ramakrishna Vedantam, Jos\\'e M. F. Moura, Devi Parikh", "title": "Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings\n  Using Abstract Scenes", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model to learn visually grounded word embeddings (vis-w2v) to\ncapture visual notions of semantic relatedness. While word embeddings trained\nusing text have been extremely successful, they cannot uncover notions of\nsemantic relatedness implicit in our visual world. For instance, although\n\"eats\" and \"stares at\" seem unrelated in text, they share semantics visually.\nWhen people are eating something, they also tend to stare at the food.\nGrounding diverse relations like \"eats\" and \"stares at\" into vision remains\nchallenging, despite recent progress in vision. We note that the visual\ngrounding of words depends on semantics, and not the literal pixels. We thus\nuse abstract scenes created from clipart to provide the visual grounding. We\nfind that the embeddings we learn capture fine-grained, visually grounded\nnotions of semantic relatedness. We show improvements over text-only word\nembeddings (word2vec) on three tasks: common-sense assertion classification,\nvisual paraphrasing and text-based image retrieval. Our code and datasets are\navailable online.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 20:46:42 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 18:15:25 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Kottur", "Satwik", ""], ["Vedantam", "Ramakrishna", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Parikh", "Devi", ""]]}, {"id": "1511.07069", "submitter": "Samaneh Azadi", "authors": "Samaneh Azadi, Jiashi Feng, Stefanie Jegelka, Trevor Darrell", "title": "Auxiliary Image Regularization for Deep CNNs with Noisy Labels", "comments": "Published as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precisely-labeled data sets with sufficient amount of samples are very\nimportant for training deep convolutional neural networks (CNNs). However, many\nof the available real-world data sets contain erroneously labeled samples and\nthose errors substantially hinder the learning of very accurate CNN models. In\nthis work, we consider the problem of training a deep CNN model for image\nclassification with mislabeled training samples - an issue that is common in\nreal image data sets with tags supplied by amateur users. To solve this\nproblem, we propose an auxiliary image regularization technique, optimized by\nthe stochastic Alternating Direction Method of Multipliers (ADMM) algorithm,\nthat automatically exploits the mutual context information among training\nimages and encourages the model to select reliable images to robustify the\nlearning process. Comprehensive experiments on benchmark data sets clearly\ndemonstrate our proposed regularized CNN model is resistant to label noise in\ntraining data.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 20:59:19 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 07:21:08 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Azadi", "Samaneh", ""], ["Feng", "Jiashi", ""], ["Jegelka", "Stefanie", ""], ["Darrell", "Trevor", ""]]}, {"id": "1511.07106", "submitter": "Michael Salvato", "authors": "Michael Salvato, Ross Finman, and John Leonard", "title": "Multi-Volume High Resolution RGB-D Mapping with Dynamic Volume Placement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel RGB-D mapping system for generating 3D maps over spatially\nextended regions with higher resolution than current methods using multiple,\ndynamically placed mapping volumes. Our method takes in RGB-D frames and\ndynamically assigns multiple mapping volumes to the environment, exchanging\nmapping volumes between the CPU and GPU. Mapping volumes are added or removed\nas needed to allow for spatially extended, high resolution mapping. Our system\nis designed to maximize the resolution possible for such volumetric methods,\nwhile working on an unbounded space.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 04:01:00 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Salvato", "Michael", ""], ["Finman", "Ross", ""], ["Leonard", "John", ""]]}, {"id": "1511.07111", "submitter": "Eric Tzeng", "authors": "Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Pieter Abbeel,\n  Sergey Levine, Kate Saenko, Trevor Darrell", "title": "Adapting Deep Visuomotor Representations with Weak Pairwise Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world robotics problems often occur in domains that differ significantly\nfrom the robot's prior training environment. For many robotic control tasks,\nreal world experience is expensive to obtain, but data is easy to collect in\neither an instrumented environment or in simulation. We propose a novel domain\nadaptation approach for robot perception that adapts visual representations\nlearned on a large easy-to-obtain source dataset (e.g. synthetic images) to a\ntarget real-world domain, without requiring expensive manual data annotation of\nreal world data before policy search. Supervised domain adaptation methods\nminimize cross-domain differences using pairs of aligned images that contain\nthe same object or scene in both the source and target domains, thus learning a\ndomain-invariant representation. However, they require manual alignment of such\nimage pairs. Fully unsupervised adaptation methods rely on minimizing the\ndiscrepancy between the feature distributions across domains. We propose a\nnovel, more powerful combination of both distribution and pairwise image\nalignment, and remove the requirement for expensive annotation by using weakly\naligned pairs of images in the source and target domains. Focusing on adapting\nfrom simulation to real world data using a PR2 robot, we evaluate our approach\non a manipulation task and show that by using weakly paired images, our method\ncompensates for domain shift more effectively than previous techniques,\nenabling better robot performance in the real world.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 05:07:15 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 19:55:50 GMT"}, {"version": "v3", "created": "Tue, 12 Apr 2016 22:05:27 GMT"}, {"version": "v4", "created": "Mon, 21 Nov 2016 21:37:58 GMT"}, {"version": "v5", "created": "Thu, 25 May 2017 21:51:55 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Tzeng", "Eric", ""], ["Devin", "Coline", ""], ["Hoffman", "Judy", ""], ["Finn", "Chelsea", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""]]}, {"id": "1511.07122", "submitter": "Fisher Yu", "authors": "Fisher Yu and Vladlen Koltun", "title": "Multi-Scale Context Aggregation by Dilated Convolutions", "comments": "Published as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art models for semantic segmentation are based on adaptations of\nconvolutional networks that had originally been designed for image\nclassification. However, dense prediction and image classification are\nstructurally different. In this work, we develop a new convolutional network\nmodule that is specifically designed for dense prediction. The presented module\nuses dilated convolutions to systematically aggregate multi-scale contextual\ninformation without losing resolution. The architecture is based on the fact\nthat dilated convolutions support exponential expansion of the receptive field\nwithout loss of resolution or coverage. We show that the presented context\nmodule increases the accuracy of state-of-the-art semantic segmentation\nsystems. In addition, we examine the adaptation of image classification\nnetworks to dense prediction and show that simplifying the adapted network can\nincrease accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 07:32:14 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 01:48:42 GMT"}, {"version": "v3", "created": "Sat, 30 Apr 2016 18:19:37 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Yu", "Fisher", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1511.07125", "submitter": "Patrick Gallagher", "authors": "Patrick W. Gallagher, Shuai Tang, Zhuowen Tu", "title": "What Happened to My Dog in That Network: Unraveling Top-down Generators\n  in Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-down information plays a central role in human perception, but plays\nrelatively little role in many current state-of-the-art deep networks, such as\nConvolutional Neural Networks (CNNs). This work seeks to explore a path by\nwhich top-down information can have a direct impact within current deep\nnetworks. We explore this path by learning and using \"generators\" corresponding\nto the network internal effects of three types of transformation (each a\nrestriction of a general affine transformation): rotation, scaling, and\ntranslation. We demonstrate how these learned generators can be used to\ntransfer top-down information to novel settings, as mediated by the \"feature\nflows\" that the transformations (and the associated generators) correspond to\ninside the network. Specifically, we explore three aspects: 1) using generators\nas part of a method for synthesizing transformed images --- given a previously\nunseen image, produce versions of that image corresponding to one or more\nspecified transformations, 2) \"zero-shot learning\" --- when provided with a\nfeature flow corresponding to the effect of a transformation of unknown amount,\nleverage learned generators as part of a method by which to perform an accurate\ncategorization of the amount of transformation, even for amounts never observed\nduring training, and 3) (inside-CNN) \"data augmentation\" --- improve the\nclassification performance of an existing network by using the learned\ngenerators to directly provide additional training \"inside the CNN\".\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 07:48:01 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Gallagher", "Patrick W.", ""], ["Tang", "Shuai", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1511.07131", "submitter": "Jun Zhu", "authors": "Jun Zhu and Xianjie Chen and Alan L. Yuille", "title": "DeePM: A Deep Part-Based Model for Object Detection and Semantic Part\n  Localization", "comments": "the final revision to ICLR 2016, in which some color errors in the\n  figures are fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep part-based model (DeePM) for symbiotic\nobject detection and semantic part localization. For this purpose, we annotate\nsemantic parts for all 20 object categories on the PASCAL VOC 2012 dataset,\nwhich provides information on object pose, occlusion, viewpoint and\nfunctionality. DeePM is a latent graphical model based on the state-of-the-art\nR-CNN framework, which learns an explicit representation of the object-part\nconfiguration with flexible type sharing (e.g., a sideview horse head can be\nshared by a fully-visible sideview horse and a highly truncated sideview horse\nwith head and neck only). For comparison, we also present an end-to-end\nObject-Part (OP) R-CNN which learns an implicit feature representation for\njointly mapping an image ROI to the object and part bounding boxes. We evaluate\nthe proposed methods for both the object and part detection performance on\nPASCAL VOC 2012, and show that DeePM consistently outperforms OP R-CNN in\ndetecting objects and parts. In addition, it obtains superior performance to\nFast and Faster R-CNNs in object detection.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 08:24:18 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 15:25:38 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2016 09:14:31 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Zhu", "Jun", ""], ["Chen", "Xianjie", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1511.07212", "submitter": "XIangyu Zhu", "authors": "Xiangyu Zhu, Zhen Lei, Xiaoming Liu, Hailin Shi, Stan Z. Li", "title": "Face Alignment Across Large Poses: A 3D Solution", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2778152", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face alignment, which fits a face model to an image and extracts the semantic\nmeanings of facial pixels, has been an important topic in CV community.\nHowever, most algorithms are designed for faces in small to medium poses (below\n45 degree), lacking the ability to align faces in large poses up to 90 degree.\nThe challenges are three-fold: Firstly, the commonly used landmark-based face\nmodel assumes that all the landmarks are visible and is therefore not suitable\nfor profile views. Secondly, the face appearance varies more dramatically\nacross large poses, ranging from frontal view to profile view. Thirdly,\nlabelling landmarks in large poses is extremely challenging since the invisible\nlandmarks have to be guessed. In this paper, we propose a solution to the three\nproblems in an new alignment framework, called 3D Dense Face Alignment (3DDFA),\nin which a dense 3D face model is fitted to the image via convolutional neutral\nnetwork (CNN). We also propose a method to synthesize large-scale training\nsamples in profile views to solve the third problem of data labelling.\nExperiments on the challenging AFLW database show that our approach achieves\nsignificant improvements over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 13:23:19 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Zhu", "Xiangyu", ""], ["Lei", "Zhen", ""], ["Liu", "Xiaoming", ""], ["Shi", "Hailin", ""], ["Li", "Stan Z.", ""]]}, {"id": "1511.07247", "submitter": "Relja Arandjelovi\\'c", "authors": "Relja Arandjelovi\\'c, Petr Gronat, Akihiko Torii, Tomas Pajdla, Josef\n  Sivic", "title": "NetVLAD: CNN architecture for weakly supervised place recognition", "comments": "Appears in: IEEE Computer Vision and Pattern Recognition (CVPR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of large scale visual place recognition, where the task\nis to quickly and accurately recognize the location of a given query\nphotograph. We present the following three principal contributions. First, we\ndevelop a convolutional neural network (CNN) architecture that is trainable in\nan end-to-end manner directly for the place recognition task. The main\ncomponent of this architecture, NetVLAD, is a new generalized VLAD layer,\ninspired by the \"Vector of Locally Aggregated Descriptors\" image representation\ncommonly used in image retrieval. The layer is readily pluggable into any CNN\narchitecture and amenable to training via backpropagation. Second, we develop a\ntraining procedure, based on a new weakly supervised ranking loss, to learn\nparameters of the architecture in an end-to-end manner from images depicting\nthe same places over time downloaded from Google Street View Time Machine.\nFinally, we show that the proposed architecture significantly outperforms\nnon-learnt image representations and off-the-shelf CNN descriptors on two\nchallenging place recognition benchmarks, and improves over current\nstate-of-the-art compact image representations on standard image retrieval\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 14:51:51 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 17:19:39 GMT"}, {"version": "v3", "created": "Mon, 2 May 2016 15:42:41 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Arandjelovi\u0107", "Relja", ""], ["Gronat", "Petr", ""], ["Torii", "Akihiko", ""], ["Pajdla", "Tomas", ""], ["Sivic", "Josef", ""]]}, {"id": "1511.07299", "submitter": "Thomas C. K\\\"ubler", "authors": "Thomas C. K\\\"ubler and Tobias Rittig and Judith Ungewiss and Christina\n  Krauss and Enkelejda Kasneci", "title": "Rendering refraction and reflection of eyeglasses for synthetic eye\n  tracker images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While for the evaluation of robustness of eye tracking algorithms the use of\nreal-world data is essential, there are many applications where simulated,\nsynthetic eye images are of advantage. They can generate labelled ground-truth\ndata for appearance based gaze estimation algorithms or enable the development\nof model based gaze estimation techniques by showing the influence on gaze\nestimation error of different model factors that can then be simplified or\nextended. We extend the generation of synthetic eye images by a simulation of\nrefraction and reflection for eyeglasses. On the one hand this allows for the\ntesting of pupil and glint detection algorithms under different illumination\nand reflection conditions, on the other hand the error of gaze estimation\nroutines can be estimated in conjunction with different eyeglasses. We show how\na polynomial function fitting calibration performs equally well with and\nwithout eyeglasses, and how a geometrical eye model behaves when exposed to\nglasses.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 16:28:14 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 15:01:57 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["K\u00fcbler", "Thomas C.", ""], ["Rittig", "Tobias", ""], ["Ungewiss", "Judith", ""], ["Krauss", "Christina", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "1511.07347", "submitter": "Karl Zipser", "authors": "Karl Zipser", "title": "Node Specificity in Convolutional Deep Nets Depends on Receptive Field\n  Position and Size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In convolutional deep neural networks, receptive field (RF) size increases\nwith hierarchical depth. When RF size approaches full coverage of the input\nimage, different RF positions result in RFs with different specificity, as\nportions of the RF fall out of the input space. This leads to a departure from\nthe convolutional concept of positional invariance and opens the possibility\nfor complex forms of context specificity.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 18:15:13 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Zipser", "Karl", ""]]}, {"id": "1511.07356", "submitter": "Sina Honari", "authors": "Sina Honari, Jason Yosinski, Pascal Vincent, Christopher Pal", "title": "Recombinator Networks: Learning Coarse-to-Fine Feature Aggregation", "comments": "accepted in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks with alternating convolutional, max-pooling and\ndecimation layers are widely used in state of the art architectures for\ncomputer vision. Max-pooling purposefully discards precise spatial information\nin order to create features that are more robust, and typically organized as\nlower resolution spatial feature maps. On some tasks, such as whole-image\nclassification, max-pooling derived features are well suited; however, for\ntasks requiring precise localization, such as pixel level prediction and\nsegmentation, max-pooling destroys exactly the information required to perform\nwell. Precise localization may be preserved by shallow convnets without pooling\nbut at the expense of robustness. Can we have our max-pooled multi-layered cake\nand eat it too? Several papers have proposed summation and concatenation based\nmethods for combining upsampled coarse, abstract features with finer features\nto produce robust pixel level predictions. Here we introduce another model ---\ndubbed Recombinator Networks --- where coarse features inform finer features\nearly in their formation such that finer features can make use of several\nlayers of computation in deciding how to use coarse features. The model is\ntrained once, end-to-end and performs better than summation-based\narchitectures, reducing the error from the previous state of the art on two\nfacial keypoint datasets, AFW and AFLW, by 30\\% and beating the current\nstate-of-the-art on 300W without using extra data. We improve performance even\nfurther by adding a denoising prediction model based on a novel convnet\nformulation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 18:42:36 GMT"}, {"version": "v2", "created": "Sun, 17 Apr 2016 23:29:25 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Honari", "Sina", ""], ["Yosinski", "Jason", ""], ["Vincent", "Pascal", ""], ["Pal", "Christopher", ""]]}, {"id": "1511.07376", "submitter": "Matin Hashemi", "authors": "Seyyed Salar Latifi Oskouei, Hossein Golestani, Matin Hashemi, Soheil\n  Ghiasi", "title": "CNNdroid: GPU-Accelerated Execution of Trained Deep Convolutional Neural\n  Networks on Android", "comments": null, "journal-ref": "Proceedings of the 2016 ACM Multimedia Conference, Open Source\n  Software Track, pages 1201-1205, October 2016", "doi": "10.1145/2964284.2973801", "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many mobile applications running on smartphones and wearable devices would\npotentially benefit from the accuracy and scalability of deep CNN-based machine\nlearning algorithms. However, performance and energy consumption limitations\nmake the execution of such computationally intensive algorithms on mobile\ndevices prohibitive. We present a GPU-accelerated library, dubbed CNNdroid, for\nexecution of trained deep CNNs on Android-based mobile devices. Empirical\nevaluations show that CNNdroid achieves up to 60X speedup and 130X energy\nsaving on current mobile devices. The CNNdroid open source library is available\nfor download at https://github.com/ENCP/CNNdroid\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 19:32:37 GMT"}, {"version": "v2", "created": "Sat, 15 Oct 2016 19:22:46 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Oskouei", "Seyyed Salar Latifi", ""], ["Golestani", "Hossein", ""], ["Hashemi", "Matin", ""], ["Ghiasi", "Soheil", ""]]}, {"id": "1511.07386", "submitter": "Iasonas Kokkinos", "authors": "Iasonas Kokkinos", "title": "Pushing the Boundaries of Boundary Detection using Deep Learning", "comments": "The previous version reported large improvements w.r.t. the LPO\n  region proposal baseline, which turned out to be due to a wrong computation\n  for the baseline. The improvements are currently less important, and are\n  omitted. We are sorry if the reported results caused any confusion. We have\n  also integrated reviewer feedback regarding human performance on the BSD\n  benchmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we show that adapting Deep Convolutional Neural Network training\nto the task of boundary detection can result in substantial improvements over\nthe current state-of-the-art in boundary detection.\n  Our contributions consist firstly in combining a careful design of the loss\nfor boundary detection training, a multi-resolution architecture and training\nwith external data to improve the detection accuracy of the current state of\nthe art. When measured on the standard Berkeley Segmentation Dataset, we\nimprove theoptimal dataset scale F-measure from 0.780 to 0.808 - while human\nperformance is at 0.803. We further improve performance to 0.813 by combining\ndeep learning with grouping, integrating the Normalized Cuts technique within a\ndeep network.\n  We also examine the potential of our boundary detector in conjunction with\nthe task of semantic segmentation and demonstrate clear improvements over\nstate-of-the-art systems. Our detector is fully integrated in the popular Caffe\nframework and processes a 320x420 image in less than a second.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 19:54:09 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2016 15:31:32 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Kokkinos", "Iasonas", ""]]}, {"id": "1511.07394", "submitter": "Kevin Shih", "authors": "Kevin J. Shih, Saurabh Singh, Derek Hoiem", "title": "Where To Look: Focus Regions for Visual Question Answering", "comments": "Submitted to CVPR2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that learns to answer visual questions by selecting image\nregions relevant to the text-based query. Our method exhibits significant\nimprovements in answering questions such as \"what color,\" where it is necessary\nto evaluate a specific location, and \"what room,\" where it selectively\nidentifies informative image regions. Our model is tested on the VQA dataset\nwhich is the largest human-annotated visual question answering dataset to our\nknowledge.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 20:17:18 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2016 13:26:23 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Shih", "Kevin J.", ""], ["Singh", "Saurabh", ""], ["Hoiem", "Derek", ""]]}, {"id": "1511.07404", "submitter": "Katerina Fragkiadaki", "authors": "Katerina Fragkiadaki and Pulkit Agrawal and Sergey Levine and Jitendra\n  Malik", "title": "Learning Visual Predictive Models of Physics for Playing Billiards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to plan and execute goal specific actions in varied, unexpected\nsettings is a central requirement of intelligent agents. In this paper, we\nexplore how an agent can be equipped with an internal model of the dynamics of\nthe external world, and how it can use this model to plan novel actions by\nrunning multiple internal simulations (\"visual imagination\"). Our models\ndirectly process raw visual input, and use a novel object-centric prediction\nformulation based on visual glimpses centered on objects (fixations) to enforce\ntranslational invariance of the learned physical laws. The agent gathers\ntraining data through random interaction with a collection of different\nenvironments, and the resulting model can then be used to plan goal-directed\nactions in novel environments that the agent has not seen before. We\ndemonstrate that our agent can accurately plan actions for playing a simulated\nbilliards game, which requires pushing a ball into a target position or into\ncollision with another ball.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 20:27:48 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 20:44:39 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 20:58:24 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Fragkiadaki", "Katerina", ""], ["Agrawal", "Pulkit", ""], ["Levine", "Sergey", ""], ["Malik", "Jitendra", ""]]}, {"id": "1511.07409", "submitter": "Saining Xie", "authors": "Saining Xie, Xun Huang and Zhuowen Tu", "title": "Top-Down Learning for Structured Labeling with Convolutional Pseudoprior", "comments": "To appear in ECCV 2016, 16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current practice in convolutional neural networks (CNN) remains largely\nbottom-up and the role of top-down process in CNN for pattern analysis and\nvisual inference is not very clear. In this paper, we propose a new method for\nstructured labeling by developing convolutional pseudo-prior (ConvPP) on the\nground-truth labels. Our method has several interesting properties: (1)\ncompared with classical machine learning algorithms like CRFs and Structural\nSVM, ConvPP automatically learns rich convolutional kernels to capture both\nshort- and long- range contexts; (2) compared with cascade classifiers like\nAuto-Context, ConvPP avoids the iterative steps of learning a series of\ndiscriminative classifiers and automatically learns contextual configurations;\n(3) compared with recent efforts combing CNN models with CRFs and RNNs, ConvPP\nlearns convolution in the labeling space with much improved modeling capability\nand less manual specification; (4) compared with Bayesian models like MRFs,\nConvPP capitalizes on the rich representation power of convolution by\nautomatically learning priors built on convolutional filters. We accomplish our\ntask using pseudo-likelihood approximation to the prior under a novel\nfixed-point network structure that facilitates an end-to-end learning process.\nWe show state-of-the-art results on sequential labeling and image labeling\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 20:43:14 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 05:25:29 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Xie", "Saining", ""], ["Huang", "Xun", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1511.07425", "submitter": "Mohammad Sabokrou", "authors": "Mohammad Sabokrou, Mahmood Fathy, Mojtaba Hosseini", "title": "Real-Time Anomalous Behavior Detection and Localization in Crowded\n  Scenes", "comments": "This paper has been withdrawn by the author due to some error in\n  experimental result. There are some mistakes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an accurate and real-time anomaly detection and\nlocalization in crowded scenes, and two descriptors for representing anomalous\nbehavior in video are proposed. We consider a video as being a set of cubic\npatches. Based on the low likelihood of an anomaly occurrence, and the\nredundancy of structures in normal patches in videos, two (global and local)\nviews are considered for modeling the video. Our algorithm has two components,\nfor (1) representing the patches using local and global descriptors, and for\n(2) modeling the training patches using a new representation. We have two\nGaussian models for all training patches respect to global and local\ndescriptors. The local and global features are based on structure similarity\nbetween adjacent patches and the features that are learned in an unsupervised\nway. We propose a fusion strategy to combine the two descriptors as the output\nof our system. Experimental results show that our algorithm performs like a\nstate-of-the-art method on several standard datasets, but even is more\ntime-efficient.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 22:42:53 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2016 06:10:47 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Sabokrou", "Mohammad", ""], ["Fathy", "Mahmood", ""], ["Hosseini", "Mojtaba", ""]]}, {"id": "1511.07497", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Philipp Kr\\\"ahenb\\\"uhl, Stella X. Yu, Trevor Darrell", "title": "Constrained Structured Regression with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have recently emerged as the dominant\nmodel in computer vision. If provided with enough training data, they predict\nalmost any visual quantity. In a discrete setting, such as classification, CNNs\nare not only able to predict a label but often predict a confidence in the form\nof a probability distribution over the output space. In continuous regression\ntasks, such a probability estimate is often lacking. We present a regression\nframework which models the output distribution of neural networks. This output\ndistribution allows us to infer the most likely labeling following a set of\nphysical or modeling constraints. These constraints capture the intricate\ninterplay between different input and output variables, and complement the\noutput of a CNN. However, they may not hold everywhere. Our setup further\nallows to learn a confidence with which a constraint holds, in the form of a\ndistribution of the constrain satisfaction. We evaluate our approach on the\nproblem of intrinsic image decomposition, and show that constrained structured\nregression significantly increases the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 22:43:37 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Pathak", "Deepak", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""], ["Yu", "Stella X.", ""], ["Darrell", "Trevor", ""]]}, {"id": "1511.07545", "submitter": "Hailin Shi", "authors": "Hailin Shi and Xiangyu Zhu and Shengcai Liao and Zhen Lei and Yang\n  Yang and Stan Z. Li", "title": "Constrained Deep Metric Learning for Person Re-identification", "comments": "11 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification aims to re-identify the probe image from a given set\nof images under different camera views. It is challenging due to large\nvariations of pose, illumination, occlusion and camera view. Since the\nconvolutional neural networks (CNN) have excellent capability of feature\nextraction, certain deep learning methods have been recently applied in person\nre-identification. However, in person re-identification, the deep networks\noften suffer from the over-fitting problem. In this paper, we propose a novel\nCNN-based method to learn a discriminative metric with good robustness to the\nover-fitting problem in person re-identification. Firstly, a novel deep\narchitecture is built where the Mahalanobis metric is learned with a weight\nconstraint. This weight constraint is used to regularize the learning, so that\nthe learned metric has a better generalization ability. Secondly, we find that\nthe selection of intra-class sample pairs is crucial for learning but has\nreceived little attention. To cope with the large intra-class variations in\npedestrian images, we propose a novel training strategy named moderate positive\nmining to prevent the training process from over-fitting to the extreme samples\nin intra-class pairs. Experiments show that our approach significantly\noutperforms state-of-the-art methods on several benchmarks of person\nre-identification.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 02:46:35 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Shi", "Hailin", ""], ["Zhu", "Xiangyu", ""], ["Liao", "Shengcai", ""], ["Lei", "Zhen", ""], ["Yang", "Yang", ""], ["Li", "Stan Z.", ""]]}, {"id": "1511.07571", "submitter": "Justin Johnson", "authors": "Justin Johnson and Andrej Karpathy and Li Fei-Fei", "title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the dense captioning task, which requires a computer vision\nsystem to both localize and describe salient regions in images in natural\nlanguage. The dense captioning task generalizes object detection when the\ndescriptions consist of a single word, and Image Captioning when one predicted\nregion covers the full image. To address the localization and description task\njointly we propose a Fully Convolutional Localization Network (FCLN)\narchitecture that processes an image with a single, efficient forward pass,\nrequires no external regions proposals, and can be trained end-to-end with a\nsingle round of optimization. The architecture is composed of a Convolutional\nNetwork, a novel dense localization layer, and Recurrent Neural Network\nlanguage model that generates the label sequences. We evaluate our network on\nthe Visual Genome dataset, which comprises 94,000 images and 4,100,000\nregion-grounded captions. We observe both speed and accuracy improvements over\nbaselines based on current state of the art approaches in both generation and\nretrieval settings.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 05:13:54 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Johnson", "Justin", ""], ["Karpathy", "Andrej", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1511.07607", "submitter": "Rahul Anand Sharma Mr.", "authors": "Rahul Anand Sharma, Pramod Sankar K and CV Jawahar", "title": "Fine-Grain Annotation of Cricket Videos", "comments": "ACPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of human activities is one of the key problems in video\nunderstanding. Action recognition is challenging even for specific categories\nof videos, such as sports, that contain only a small set of actions.\nInterestingly, sports videos are accompanied by detailed commentaries available\nonline, which could be used to perform action annotation in a weakly-supervised\nsetting. For the specific case of Cricket videos, we address the challenge of\ntemporal segmentation and annotation of ctions with semantic descriptions. Our\nsolution consists of two stages. In the first stage, the video is segmented\ninto \"scenes\", by utilizing the scene category information extracted from\ntext-commentary. The second stage consists of classifying video-shots as well\nas the phrases in the textual description into various categories. The relevant\nphrases are then suitably mapped to the video-shots. The novel aspect of this\nwork is the fine temporal scale at which semantic information is assigned to\nthe video. As a result of our approach, we enable retrieval of specific actions\nthat last only a few seconds, from several hours of video. This solution yields\na large number of labeled exemplars, with no manual effort, that could be used\nby machine learning algorithms to learn complex actions.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 08:34:20 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 10:48:11 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Sharma", "Rahul Anand", ""], ["K", "Pramod Sankar", ""], ["Jawahar", "CV", ""]]}, {"id": "1511.07608", "submitter": "Janne V. Kujala", "authors": "Janne V. Kujala, Tuomas J. Lukka, and Harri Holopainen", "title": "Picking a Conveyor Clean by an Autonomously Learning Robot", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a research picking prototype related to our company's industrial\nwaste sorting application. The goal of the prototype is to be as autonomous as\npossible and it both calibrates itself and improves its picking with minimal\nhuman intervention. The system learns to pick objects better based on a\nfeedback sensor in its gripper and uses machine learning to choosing the best\nproposal from a random sample produced by simple hard-coded geometric models.\nWe show experimentally the system improving its picking autonomously by\nmeasuring the pick success rate as function of time. We also show how this\nsystem can pick a conveyor belt clean, depositing 70 out of 80 objects in a\ndifficult to manipulate pile of novel objects into the correct chute. We\ndiscuss potential improvements and next steps in this direction.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 08:35:49 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Kujala", "Janne V.", ""], ["Lukka", "Tuomas J.", ""], ["Holopainen", "Harri", ""]]}, {"id": "1511.07611", "submitter": "Li Cheng", "authors": "Ashwin Nanjappa, Li Cheng, Wei Gao, Chi Xu, Adam Claridge-Chang, Zoe\n  Bichler", "title": "Mouse Pose Estimation From Depth Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the challenging problem of efficient mouse 3D pose estimation\nbased on static images, and especially single depth images. We introduce an\napproach to discriminatively train the split nodes of trees in random forest to\nimprove their performance on estimation of 3D joint positions of mouse. Our\nalgorithm is capable of working with different types of rodents and with\ndifferent types of depth cameras and imaging setups. In particular, it is\ndemonstrated in this paper that when a top-mounted depth camera is combined\nwith a bottom-mounted color camera, the final system is capable of delivering\nfull-body pose estimation including four limbs and the paws. Empirical\nexaminations on synthesized and real-world depth images confirm the\napplicability of our approach on mouse pose estimation, as well as the closely\nrelated task of part-based labeling of mouse.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 08:42:01 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Nanjappa", "Ashwin", ""], ["Cheng", "Li", ""], ["Gao", "Wei", ""], ["Xu", "Chi", ""], ["Claridge-Chang", "Adam", ""], ["Bichler", "Zoe", ""]]}, {"id": "1511.07710", "submitter": "Varun Nagaraja", "authors": "Varun K. Nagaraja, Vlad I. Morariu, Larry S. Davis", "title": "Searching for Objects using Structure in Indoor Scenes", "comments": "Appeared in British Machine Vision Conference (BMVC) 2015", "journal-ref": null, "doi": "10.5244/C.29.53", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To identify the location of objects of a particular class, a passive computer\nvision system generally processes all the regions in an image to finally output\nfew regions. However, we can use structure in the scene to search for objects\nwithout processing the entire image. We propose a search technique that\nsequentially processes image regions such that the regions that are more likely\nto correspond to the query class object are explored earlier. We frame the\nproblem as a Markov decision process and use an imitation learning algorithm to\nlearn a search strategy. Since structure in the scene is essential for search,\nwe work with indoor scene images as they contain both unary scene context\ninformation and object-object context in the scene. We perform experiments on\nthe NYU-depth v2 dataset and show that the unary scene context features alone\ncan achieve a significantly high average precision while processing only\n20-25\\% of the regions for classes like bed and sofa. By considering\nobject-object context along with the scene context features, the performance is\nfurther improved for classes like counter, lamp, pillow and sofa.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 14:05:28 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Nagaraja", "Varun K.", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1511.07732", "submitter": "Thiago Santini", "authors": "Thiago Santini, Wolfgang Fuhl, Thomas K\\\"ubler, and Enkelejda Kasneci", "title": "Bayesian Identification of Fixations, Saccades, and Smooth Pursuits", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smooth pursuit eye movements provide meaningful insights and information on\nsubject's behavior and health and may, in particular situations, disturb the\nperformance of typical fixation/saccade classification algorithms. Thus, an\nautomatic and efficient algorithm to identify these eye movements is paramount\nfor eye-tracking research involving dynamic stimuli. In this paper, we propose\nthe Bayesian Decision Theory Identification (I-BDT) algorithm, a novel\nalgorithm for ternary classification of eye movements that is able to reliably\nseparate fixations, saccades, and smooth pursuits in an online fashion, even\nfor low-resolution eye trackers. The proposed algorithm is evaluated on four\ndatasets with distinct mixtures of eye movements, including fixations,\nsaccades, as well as straight and circular smooth pursuits; data was collected\nwith a sample rate of 30 Hz from six subjects, totaling 24 evaluation datasets.\nThe algorithm exhibits high and consistent performance across all datasets and\nmovements relative to a manual annotation by a domain expert (recall: \\mu =\n91.42%, \\sigma = 9.52%; precision: \\mu = 95.60%, \\sigma = 5.29%; specificity\n\\mu = 95.41%, \\sigma = 7.02%) and displays a significant improvement when\ncompared to I-VDT, an state-of-the-art algorithm (recall: \\mu = 87.67%, \\sigma\n= 14.73%; precision: \\mu = 89.57%, \\sigma = 8.05%; specificity \\mu = 92.10%,\n\\sigma = 11.21%). For algorithm implementation and annotated datasets, please\ncontact the first author.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 14:40:05 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Santini", "Thiago", ""], ["Fuhl", "Wolfgang", ""], ["K\u00fcbler", "Thomas", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "1511.07763", "submitter": "Spyros Gidaris", "authors": "Spyros Gidaris, Nikos Komodakis", "title": "LocNet: Improving Localization Accuracy for Object Detection", "comments": "Extended technical report -- short version to appear as oral paper on\n  CVPR 2016. Code: https://github.com/gidariss/LocNet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel object localization methodology with the purpose of\nboosting the localization accuracy of state-of-the-art object detection\nsystems. Our model, given a search region, aims at returning the bounding box\nof an object of interest inside this region. To accomplish its goal, it relies\non assigning conditional probabilities to each row and column of this region,\nwhere these probabilities provide useful information regarding the location of\nthe boundaries of the object inside the search region and allow the accurate\ninference of the object bounding box under a simple probabilistic framework.\n  For implementing our localization model, we make use of a convolutional\nneural network architecture that is properly adapted for this task, called\nLocNet. We show experimentally that LocNet achieves a very significant\nimprovement on the mAP for high IoU thresholds on PASCAL VOC2007 test set and\nthat it can be very easily coupled with recent state-of-the-art object\ndetection systems, helping them to boost their performance. Finally, we\ndemonstrate that our detection approach can achieve high detection accuracy\neven when it is given as input a set of sliding windows, thus proving that it\nis independent of box proposal methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 15:42:01 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 15:09:15 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Gidaris", "Spyros", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1511.07803", "submitter": "Anna Khoreva", "authors": "Anna Khoreva, Rodrigo Benenson, Mohamed Omran, Matthias Hein and Bernt\n  Schiele", "title": "Weakly Supervised Object Boundaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art learning based boundary detection methods require extensive\ntraining data. Since labelling object boundaries is one of the most expensive\ntypes of annotations, there is a need to relax the requirement to carefully\nannotate images to make both the training more affordable and to extend the\namount of training data. In this paper we propose a technique to generate\nweakly supervised annotations and show that bounding box annotations alone\nsuffice to reach high-quality object boundaries without using any\nobject-specific boundary annotations. With the proposed weak supervision\ntechniques we achieve the top performance on the object boundary detection\ntask, outperforming by a large margin the current fully supervised\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 16:54:58 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Khoreva", "Anna", ""], ["Benenson", "Rodrigo", ""], ["Omran", "Mohamed", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1511.07845", "submitter": "Shubham Tulsiani", "authors": "Shubham Tulsiani, Abhishek Kar, Qixing Huang, Jo\\~ao Carreira and\n  Jitendra Malik", "title": "Shape and Symmetry Induction for 3D Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actions as simple as grasping an object or navigating around it require a\nrich understanding of that object's 3D shape from a given viewpoint. In this\npaper we repurpose powerful learning machinery, originally developed for object\nclassification, to discover image cues relevant for recovering the 3D shape of\npotentially unfamiliar objects. We cast the problem as one of local prediction\nof surface normals and global detection of 3D reflection symmetry planes, which\nopen the door for extrapolating occluded surfaces from visible ones. We\ndemonstrate that our method is able to recover accurate 3D shape information\nfor classes of objects it was not trained on, in both synthetic and real\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 19:48:42 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 01:43:51 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Tulsiani", "Shubham", ""], ["Kar", "Abhishek", ""], ["Huang", "Qixing", ""], ["Carreira", "Jo\u00e3o", ""], ["Malik", "Jitendra", ""]]}, {"id": "1511.07917", "submitter": "Anton Osokin", "authors": "Tuan-Hung Vu, Anton Osokin, Ivan Laptev", "title": "Context-aware CNNs for person head detection", "comments": "To appear in International Conference on Computer Vision (ICCV), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person detection is a key problem for many computer vision tasks. While face\ndetection has reached maturity, detecting people under a full variation of\ncamera view-points, human poses, lighting conditions and occlusions is still a\ndifficult challenge. In this work we focus on detecting human heads in natural\nscenes. Starting from the recent local R-CNN object detector, we extend it with\ntwo types of contextual cues. First, we leverage person-scene relations and\npropose a Global CNN model trained to predict positions and scales of heads\ndirectly from the full image. Second, we explicitly model pairwise relations\namong objects and train a Pairwise CNN model using a structured-output\nsurrogate loss. The Local, Global and Pairwise models are combined into a joint\nCNN framework. To train and test our full model, we introduce a large dataset\ncomposed of 369,846 human heads annotated in 224,740 movie frames. We evaluate\nour method and demonstrate improvements of person head detection against\nseveral recent baselines in three datasets. We also show improvements of the\ndetection speed provided by our model.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 23:23:18 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Vu", "Tuan-Hung", ""], ["Osokin", "Anton", ""], ["Laptev", "Ivan", ""]]}, {"id": "1511.07927", "submitter": "Cheng Sang", "authors": "Hong Sun, Cheng-Wei Sang, Chen-Guang Liu", "title": "Principal Basis Analysis in Sparse Representation", "comments": "The text propose a Principal Basis Analysis in Sparse Representation\n  and apply the principal basis analysis to image denoising corrupted by\n  Gaussian and non-Gaussian noises, showing better performances than some\n  reference methods at suppressing strong noise and at preserving signal\n  details;including 8 pages, 4 figures prepared using pdf according to the\n  instructions to Authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a new signal analysis method, which can be\ninterpreted as a principal component analysis in sparse decomposition of the\nsignal. The method, called principal basis analysis, is based on a novel\ncriterion: reproducibility of component which is an intrinsic characteristic of\nregularity in natural signals. We show how to measure reproducibility. Then we\npresent the principal basis analysis method, which chooses, in a sparse\nrepresentation of the signal, the components optimizing the reproducibility\ndegree to build the so-called principal basis. With this principal basis, we\nshow that the underlying signal pattern could be effectively extracted from\ncorrupted data. As illustration, we apply the principal basis analysis to image\ndenoising corrupted by Gaussian and non-Gaussian noises, showing better\nperformances than some reference methods at suppressing strong noise and at\npreserving signal details.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 01:11:24 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Sun", "Hong", ""], ["Sang", "Cheng-Wei", ""], ["Liu", "Chen-Guang", ""]]}, {"id": "1511.07940", "submitter": "Li Wang", "authors": "Li Wang, Ting Liu, Gang Wang, Kap Luk Chan and Qingxiong Yang", "title": "Video Tracking Using Learned Hierarchical Features", "comments": "12 pages, 7 figures", "journal-ref": "IEEE Transactions on Image Processing, vol. 24, no. 4, April 2015", "doi": "10.1109/TIP.2015.2403231", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an approach to learn hierarchical features for\nvisual object tracking. First, we offline learn features robust to diverse\nmotion patterns from auxiliary video sequences. The hierarchical features are\nlearned via a two-layer convolutional neural network. Embedding the temporal\nslowness constraint in the stacked architecture makes the learned features\nrobust to complicated motion transformations, which is important for visual\nobject tracking. Then, given a target video sequence, we propose a domain\nadaptation module to online adapt the pre-learned features according to the\nspecific target object. The adaptation is conducted in both layers of the deep\nfeature learning module so as to include appearance information of the specific\ntarget object. As a result, the learned hierarchical features can be robust to\nboth complicated motion transformations and appearance changes of target\nobjects. We integrate our feature learning algorithm into three tracking\nmethods. Experimental results demonstrate that significant improvement can be\nachieved using our learned hierarchical features, especially on video sequences\nwith complicated motion transformations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 02:58:42 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Wang", "Li", ""], ["Liu", "Ting", ""], ["Wang", "Gang", ""], ["Chan", "Kap Luk", ""], ["Yang", "Qingxiong", ""]]}, {"id": "1511.07951", "submitter": "Vittal Premachandran", "authors": "Vittal Premachandran, Boyan Bonev, Alan L. Yuille", "title": "PASCAL Boundaries: A Class-Agnostic Semantic Boundary Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the boundary detection task motivated by the\nambiguities in current definition of edge detection. To this end, we generate a\nlarge database consisting of more than 10k images (which is 20x bigger than\nexisting edge detection databases) along with ground truth boundaries between\n459 semantic classes including both foreground objects and different types of\nbackground, and call it the PASCAL Boundaries dataset, which will be released\nto the community. In addition, we propose a novel deep network-based\nmulti-scale semantic boundary detector and name it Multi-scale Deep Semantic\nBoundary Detector (M-DSBD). We provide baselines using models that were trained\non edge detection and show that they transfer reasonably to the task of\nboundary detection. Finally, we point to various important research problems\nthat this dataset can be used for.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 05:12:38 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Premachandran", "Vittal", ""], ["Bonev", "Boyan", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1511.07963", "submitter": "Oleg Titov", "authors": "Elena Legchekova, Oleg Titov", "title": "Calculate distance to object in the area where car, using video analysis", "comments": "5 pages, in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of using video cameras installed on the car, to calculate the\ndistance to the object in its area of movement.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 06:09:42 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Legchekova", "Elena", ""], ["Titov", "Oleg", ""]]}, {"id": "1511.08032", "submitter": "Christos Tzelepis", "authors": "Christos Tzelepis, Damianos Galanopoulos, Vasileios Mezaris, Ioannis\n  Patras", "title": "Learning to detect video events from zero or very few video examples", "comments": "Image and Vision Computing Journal, Elsevier, 2015, accepted for\n  publication", "journal-ref": "Image and Vision Computing Journal, Elsevier, 2015", "doi": "10.1016/j.imavis.2015.09.005", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we deal with the problem of high-level event detection in video.\nSpecifically, we study the challenging problems of i) learning to detect video\nevents from solely a textual description of the event, without using any\npositive video examples, and ii) additionally exploiting very few positive\ntraining samples together with a small number of ``related'' videos. For\nlearning only from an event's textual description, we first identify a general\nlearning framework and then study the impact of different design choices for\nvarious stages of this framework. For additionally learning from example\nvideos, when true positive training samples are scarce, we employ an extension\nof the Support Vector Machine that allows us to exploit ``related'' event\nvideos by automatically introducing different weights for subsets of the videos\nin the overall training set. Experimental evaluations performed on the\nlarge-scale TRECVID MED 2014 video dataset provide insight on the effectiveness\nof the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 12:17:50 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Tzelepis", "Christos", ""], ["Galanopoulos", "Damianos", ""], ["Mezaris", "Vasileios", ""], ["Patras", "Ioannis", ""]]}, {"id": "1511.08058", "submitter": "Yanwei Pang", "authors": "Jiale Cao, Yanwei Pang, and Xuelong Li", "title": "Pedestrian Detection Inspired by Appearance Constancy and Shape Symmetry", "comments": "9 pages,17 figures", "journal-ref": null, "doi": "10.1109/TIP.2016.2609807", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrimination and simplicity of features are very important for\neffective and efficient pedestrian detection. However, most state-of-the-art\nmethods are unable to achieve good tradeoff between accuracy and efficiency.\nInspired by some simple inherent attributes of pedestrians (i.e., appearance\nconstancy and shape symmetry), we propose two new types of non-neighboring\nfeatures (NNF): side-inner difference features (SIDF) and symmetrical\nsimilarity features (SSF). SIDF can characterize the difference between the\nbackground and pedestrian and the difference between the pedestrian contour and\nits inner part. SSF can capture the symmetrical similarity of pedestrian shape.\nHowever, it's difficult for neighboring features to have such above\ncharacterization abilities. Finally, we propose to combine both non-neighboring\nand neighboring features for pedestrian detection. It's found that\nnon-neighboring features can further decrease the average miss rate by 4.44%.\nExperimental results on INRIA and Caltech pedestrian datasets demonstrate the\neffectiveness and efficiency of the proposed method. Compared to the\nstate-of-the-art methods without using CNN, our method achieves the best\ndetection performance on Caltech, outperforming the second best method (i.e.,\nCheckboards) by 1.63%.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 13:49:13 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Cao", "Jiale", ""], ["Pang", "Yanwei", ""], ["Li", "Xuelong", ""]]}, {"id": "1511.08119", "submitter": "Anurag Arnab", "authors": "Anurag Arnab, Sadeep Jayasumana, Shuai Zheng, Philip Torr", "title": "Higher Order Conditional Random Fields in Deep Neural Networks", "comments": "ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of semantic segmentation using deep learning. Most\nsegmentation systems include a Conditional Random Field (CRF) to produce a\nstructured output that is consistent with the image's visual features. Recent\ndeep learning approaches have incorporated CRFs into Convolutional Neural\nNetworks (CNNs), with some even training the CRF end-to-end with the rest of\nthe network. However, these approaches have not employed higher order\npotentials, which have previously been shown to significantly improve\nsegmentation performance. In this paper, we demonstrate that two types of\nhigher order potential, based on object detections and superpixels, can be\nincluded in a CRF embedded within a deep network. We design these higher order\npotentials to allow inference with the differentiable mean field algorithm. As\na result, all the parameters of our richer CRF model can be learned end-to-end\nwith our pixelwise CNN classifier. We achieve state-of-the-art segmentation\nperformance on the PASCAL VOC benchmark with these trainable higher order\npotentials.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 17:02:31 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2015 13:43:24 GMT"}, {"version": "v3", "created": "Wed, 30 Mar 2016 21:43:45 GMT"}, {"version": "v4", "created": "Fri, 29 Jul 2016 18:16:18 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Arnab", "Anurag", ""], ["Jayasumana", "Sadeep", ""], ["Zheng", "Shuai", ""], ["Torr", "Philip", ""]]}, {"id": "1511.08131", "submitter": "Adriana Romero", "authors": "Adriana Romero, Carlo Gatta and Gustau Camps-Valls", "title": "Unsupervised Deep Feature Extraction for Remote Sensing Image\n  Classification", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, Volume:PP ,\n  Issue: 99, 2015", "doi": "10.1109/TGRS.2015.2478379", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the use of single layer and deep convolutional networks\nfor remote sensing data analysis. Direct application to multi- and\nhyper-spectral imagery of supervised (shallow or deep) convolutional networks\nis very challenging given the high input data dimensionality and the relatively\nsmall amount of available labeled data. Therefore, we propose the use of greedy\nlayer-wise unsupervised pre-training coupled with a highly efficient algorithm\nfor unsupervised learning of sparse features. The algorithm is rooted on sparse\nrepresentations and enforces both population and lifetime sparsity of the\nextracted features, simultaneously. We successfully illustrate the expressive\npower of the extracted representations in several scenarios: classification of\naerial scenes, as well as land-use classification in very high resolution\n(VHR), or land-cover classification from multi- and hyper-spectral images. The\nproposed algorithm clearly outperforms standard Principal Component Analysis\n(PCA) and its kernel counterpart (kPCA), as well as current state-of-the-art\nalgorithms of aerial classification, while being extremely computationally\nefficient at learning representations of data. Results show that single layer\nconvolutional networks can extract powerful discriminative features only when\nthe receptive field accounts for neighboring pixels, and are preferred when the\nclassification requires high resolution and detailed results. However, deep\narchitectures significantly outperform single layers variants, capturing\nincreasing levels of abstraction and complexity throughout the feature\nhierarchy.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 17:36:28 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Romero", "Adriana", ""], ["Gatta", "Carlo", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "1511.08166", "submitter": "Chandrayee Basu", "authors": "Chandrayee Basu, Anthony Rowe", "title": "Tracking Motion and Proxemics using Thermal-sensor Array", "comments": "6 pages, 6 figures, Machine Learning for Signal Processing Class\n  project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor tracking has all-pervasive applications beyond mere surveillance, for\nexample in education, health monitoring, marketing, energy management and so\non. Image and video based tracking systems are intrusive. Thermal array sensors\non the other hand can provide coarse-grained tracking while preserving privacy\nof the subjects. The goal of the project is to facilitate motion detection and\ngroup proxemics modeling using an 8 x 8 infrared sensor array. Each of the 8 x\n8 pixels is a temperature reading in Fahrenheit. We refer to each 8 x 8 matrix\nas a scene. We collected approximately 902 scenes with different configurations\nof human groups and different walking directions. We infer direction of motion\nof a subject across a set of scenes as left-to-right, right-to-left, up-to-down\nand down-to-up using cross-correlation analysis. We used features from\nconnected component analysis of each background subtracted scene and performed\nSupport Vector Machine classification to estimate number of instances of human\nsubjects in the scene.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 19:14:49 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Basu", "Chandrayee", ""], ["Rowe", "Anthony", ""]]}, {"id": "1511.08177", "submitter": "Saurabh Gupta", "authors": "Saurabh Gupta, Bharath Hariharan, Jitendra Malik", "title": "Exploring Person Context and Local Scene Context for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore two ways of using context for object detection. The\nfirst model focusses on people and the objects they commonly interact with,\nsuch as fashion and sports accessories. The second model considers more general\nobject detection and uses the spatial relationships between objects and between\nobjects and scenes. Our models are able to capture precise spatial\nrelationships between the context and the object of interest, and make\neffective use of the appearance of the contextual region. On the newly released\nCOCO dataset, our models provide relative improvements of up to 5% over\nCNN-based state-of-the-art detectors, with the gains concentrated on hard cases\nsuch as small objects (10% relative improvement).\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 19:45:03 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Gupta", "Saurabh", ""], ["Hariharan", "Bharath", ""], ["Malik", "Jitendra", ""]]}, {"id": "1511.08250", "submitter": "Bernardino Romera-Paredes", "authors": "Bernardino Romera-Paredes, Philip H. S. Torr", "title": "Recurrent Instance Segmentation", "comments": "14 pages (main paper). 24 pages including references and appendix", "journal-ref": "ECCV 2016. 14th European Conference on Computer Vision", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation is the problem of detecting and delineating each\ndistinct object of interest appearing in an image. Current instance\nsegmentation approaches consist of ensembles of modules that are trained\nindependently of each other, thus missing opportunities for joint learning.\nHere we propose a new instance segmentation paradigm consisting in an\nend-to-end method that learns how to segment instances sequentially. The model\nis based on a recurrent neural network that sequentially finds objects and\ntheir segmentations one at a time. This net is provided with a spatial memory\nthat keeps track of what pixels have been explained and allows occlusion\nhandling. In order to train the model we designed a principled loss function\nthat accurately represents the properties of the instance segmentation problem.\nIn the experiments carried out, we found that our method outperforms recent\napproaches on multiple person segmentation, and all state of the art approaches\non the Plant Phenotyping dataset for leaf counting.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 23:28:14 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 22:45:04 GMT"}, {"version": "v3", "created": "Mon, 24 Oct 2016 23:57:19 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Romera-Paredes", "Bernardino", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1511.08418", "submitter": "Maria Oliver", "authors": "Maria Oliver, Gloria Haro, Mariella Dimiccoli, Baptiste Mazin and\n  Coloma Ballester", "title": "A Computational Model for Amodal Completion", "comments": null, "journal-ref": null, "doi": "10.1007/s10851-016-0652-x", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a computational model to recover the most likely\ninterpretation of the 3D scene structure from a planar image, where some\nobjects may occlude others. The estimated scene interpretation is obtained by\nintegrating some global and local cues and provides both the complete\ndisoccluded objects that form the scene and their ordering according to depth.\nOur method first computes several distal scenes which are compatible with the\nproximal planar image. To compute these different hypothesized scenes, we\npropose a perceptually inspired object disocclusion method, which works by\nminimizing the Euler's elastica as well as by incorporating the relatability of\npartially occluded contours and the convexity of the disoccluded objects. Then,\nto estimate the preferred scene we rely on a Bayesian model and define\nprobabilities taking into account the global complexity of the objects in the\nhypothesized scenes as well as the effort of bringing these objects in their\nrelative position in the planar image, which is also measured by an Euler's\nelastica-based quantity. The model is illustrated with numerical experiments\non, both, synthetic and real images showing the ability of our model to\nreconstruct the occluded objects and the preferred perceptual order among them.\nWe also present results on images of the Berkeley dataset with provided\nfigure-ground ground-truth labeling.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 15:25:46 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 14:49:41 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Oliver", "Maria", ""], ["Haro", "Gloria", ""], ["Dimiccoli", "Mariella", ""], ["Mazin", "Baptiste", ""], ["Ballester", "Coloma", ""]]}, {"id": "1511.08446", "submitter": "Xu Jia", "authors": "Amir Ghodrati and Xu Jia and Marco Pedersoli and Tinne Tuytelaars", "title": "Towards Automatic Image Editing: Learning to See another You", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the distribution of images in order to generate new samples is a\nchallenging task due to the high dimensionality of the data and the highly\nnon-linear relations that are involved. Nevertheless, some promising results\nhave been reported in the literature recently,building on deep network\narchitectures. In this work, we zoom in on a specific type of image generation:\ngiven an image and knowing the category of objects it belongs to (e.g. faces),\nour goal is to generate a similar and plausible image, but with some altered\nattributes. This is particularly challenging, as the model needs to learn to\ndisentangle the effect of each attribute and to apply a desired attribute\nchange to a given input image, while keeping the other attributes and overall\nobject appearance intact. To this end, we learn a convolutional network, where\nthe desired attribute information is encoded then merged with the encoded image\nat feature map level. We show promising results, both qualitatively as well as\nquantitatively, in the context of a retrieval experiment, on two face datasets\n(MultiPie and CAS-PEAL-R1).\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 16:33:10 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Ghodrati", "Amir", ""], ["Jia", "Xu", ""], ["Pedersoli", "Marco", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1511.08458", "submitter": "Keiron O'Shea", "authors": "Keiron O'Shea and Ryan Nash", "title": "An Introduction to Convolutional Neural Networks", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of machine learning has taken a dramatic twist in recent times,\nwith the rise of the Artificial Neural Network (ANN). These biologically\ninspired computational models are able to far exceed the performance of\nprevious forms of artificial intelligence in common machine learning tasks. One\nof the most impressive forms of ANN architecture is that of the Convolutional\nNeural Network (CNN). CNNs are primarily used to solve difficult image-driven\npattern recognition tasks and with their precise yet simple architecture,\noffers a simplified method of getting started with ANNs.\n  This document provides a brief introduction to CNNs, discussing recently\npublished papers and newly formed techniques in developing these brilliantly\nfantastic image recognition models. This introduction assumes you are familiar\nwith the fundamentals of ANNs and machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 17:45:01 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 18:06:03 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["O'Shea", "Keiron", ""], ["Nash", "Ryan", ""]]}, {"id": "1511.08478", "submitter": "Ives Rey-Otero", "authors": "Ives Rey-Otero and Jean-Michel Morel and Mauricio Delbracio", "title": "An analysis of the factors affecting keypoint stability in scale-space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most popular image matching algorithm SIFT, introduced by D. Lowe a\ndecade ago, has proven to be sufficiently scale invariant to be used in\nnumerous applications. In practice, however, scale invariance may be weakened\nby various sources of error inherent to the SIFT implementation affecting the\nstability and accuracy of keypoint detection. The density of the sampling of\nthe Gaussian scale-space and the level of blur in the input image are two of\nthese sources. This article presents a numerical analysis of their impact on\nthe extracted keypoints stability. Such an analysis has both methodological and\npractical implications, on how to compare feature detectors and on how to\nimprove SIFT. We show that even with a significantly oversampled scale-space\nnumerical errors prevent from achieving perfect stability. Usual strategies to\nfilter out unstable detections are shown to be inefficient. We also prove that\nthe effect of the error in the assumption on the initial blur is asymmetric and\nthat the method is strongly degraded in presence of aliasing or without a\ncorrect assumption on the camera blur.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 19:09:11 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Rey-Otero", "Ives", ""], ["Morel", "Jean-Michel", ""], ["Delbracio", "Mauricio", ""]]}, {"id": "1511.08498", "submitter": "Ke Li", "authors": "Ke Li, Bharath Hariharan, Jitendra Malik", "title": "Iterative Instance Segmentation", "comments": "13 pages, 10 figures; IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for pixel-wise labelling tasks generally disregard the\nunderlying structure of labellings, often leading to predictions that are\nvisually implausible. While incorporating structure into the model should\nimprove prediction quality, doing so is challenging - manually specifying the\nform of structural constraints may be impractical and inference often becomes\nintractable even if structural constraints are given. We sidestep this problem\nby reducing structured prediction to a sequence of unconstrained prediction\nproblems and demonstrate that this approach is capable of automatically\ndiscovering priors on shape, contiguity of region predictions and smoothness of\nregion contours from data without any a priori specification. On the instance\nsegmentation task, this method outperforms the state-of-the-art, achieving a\nmean $\\mathrm{AP}^{r}$ of 63.6% at 50% overlap and 43.3% at 70% overlap.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 20:51:17 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 19:55:31 GMT"}, {"version": "v3", "created": "Fri, 10 Jun 2016 18:48:10 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Li", "Ke", ""], ["Hariharan", "Bharath", ""], ["Malik", "Jitendra", ""]]}, {"id": "1511.08522", "submitter": "Mohak Sukhwani", "authors": "Mohak Sukhwani, C.V. Jawahar", "title": "TennisVid2Text: Fine-grained Descriptions for Domain Specific Videos", "comments": "BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically describing videos has ever been fascinating. In this work, we\nattempt to describe videos from a specific domain - broadcast videos of lawn\ntennis matches. Given a video shot from a tennis match, we intend to generate a\ntextual commentary similar to what a human expert would write on a sports\nwebsite. Unlike many recent works that focus on generating short captions, we\nare interested in generating semantically richer descriptions. This demands a\ndetailed low-level analysis of the video content, specially the actions and\ninteractions among subjects. We address this by limiting our domain to the game\nof lawn tennis. Rich descriptions are generated by leveraging a large corpus of\nhuman created descriptions harvested from Internet. We evaluate our method on a\nnewly created tennis video data set. Extensive analysis demonstrate that our\napproach addresses both semantic correctness as well as readability aspects\ninvolved in the task.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 22:21:44 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Sukhwani", "Mohak", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1511.08531", "submitter": "Chunhua Shen", "authors": "Sakrapee Paisitkriangkrai, Lin Wu, Chunhua Shen, Anton van den Hengel", "title": "Structured learning of metric ensembles with application to person\n  re-identification", "comments": "16 pages. Extended version of \"Learning to Rank in Person\n  Re-Identification With Metric Ensembles\", at\n  http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Paisitkriangkrai_Learning_to_Rank_2015_CVPR_paper.html.\n  arXiv admin note: text overlap with arXiv:1503.01543", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching individuals across non-overlapping camera networks, known as person\nre-identification, is a fundamentally challenging problem due to the large\nvisual appearance changes caused by variations of viewpoints, lighting, and\nocclusion. Approaches in literature can be categoried into two streams: The\nfirst stream is to develop reliable features against realistic conditions by\ncombining several visual features in a pre-defined way; the second stream is to\nlearn a metric from training data to ensure strong inter-class differences and\nintra-class similarities. However, seeking an optimal combination of visual\nfeatures which is generic yet adaptive to different benchmarks is a unsoved\nproblem, and metric learning models easily get over-fitted due to the scarcity\nof training data in person re-identification. In this paper, we propose two\neffective structured learning based approaches which explore the adaptive\neffects of visual features in recognizing persons in different benchmark data\nsets. Our framework is built on the basis of multiple low-level visual features\nwith an optimal ensemble of their metrics. We formulate two optimization\nalgorithms, CMCtriplet and CMCstruct, which directly optimize evaluation\nmeasures commonly used in person re-identification, also known as the\nCumulative Matching Characteristic (CMC) curve.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 00:10:59 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 04:48:59 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Paisitkriangkrai", "Sakrapee", ""], ["Wu", "Lin", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1511.08861", "submitter": "Orazio Gallo", "authors": "Hang Zhao, Orazio Gallo, Iuri Frosio, Jan Kautz", "title": "Loss Functions for Neural Networks for Image Processing", "comments": "This paper was published in IEEE Transactions on Computational\n  Imaging on December 23, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are becoming central in several areas of computer vision and\nimage processing and different architectures have been proposed to solve\nspecific problems. The impact of the loss layer of neural networks, however,\nhas not received much attention in the context of image processing: the default\nand virtually only choice is L2. In this paper, we bring attention to\nalternative choices for image restoration. In particular, we show the\nimportance of perceptually-motivated losses when the resulting image is to be\nevaluated by a human observer. We compare the performance of several losses,\nand propose a novel, differentiable error function. We show that the quality of\nthe results improves significantly with better loss functions, even when the\nnetwork architecture is left unchanged.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2015 02:02:44 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 21:35:48 GMT"}, {"version": "v3", "created": "Fri, 20 Apr 2018 22:54:19 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Zhao", "Hang", ""], ["Gallo", "Orazio", ""], ["Frosio", "Iuri", ""], ["Kautz", "Jan", ""]]}, {"id": "1511.08886", "submitter": "Roy Or - El", "authors": "Roy Or - El, Rom Hershkovitz, Aaron Wetzler, Guy Rosman, Alfred M.\n  Bruckstein, Ron Kimmel", "title": "Real-Time Depth Refinement for Specular Objects", "comments": "Camera-Ready version for CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of consumer RGB-D scanners set off a major boost in 3D\ncomputer vision research. Yet, the precision of existing depth scanners is not\naccurate enough to recover fine details of a scanned object. While modern\nshading based depth refinement methods have been proven to work well with\nLambertian objects, they break down in the presence of specularities. We\npresent a novel shape from shading framework that addresses this issue and\nenhances both diffuse and specular objects' depth profiles. We take advantage\nof the built-in monochromatic IR projector and IR images of the RGB-D scanners\nand present a lighting model that accounts for the specular regions in the\ninput image. Using this model, we reconstruct the depth map in real-time. Both\nquantitative tests and visual evaluations prove that the proposed method\nproduces state of the art depth reconstruction results.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2015 10:13:48 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2016 14:27:20 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["El", "Roy Or -", ""], ["Hershkovitz", "Rom", ""], ["Wetzler", "Aaron", ""], ["Rosman", "Guy", ""], ["Bruckstein", "Alfred M.", ""], ["Kimmel", "Ron", ""]]}, {"id": "1511.08899", "submitter": "Mohamed Moustafa", "authors": "Mohamed Moustafa", "title": "Applying deep learning to classify pornographic images and videos", "comments": "PSIVT 2015, the final publication is available at link.springer.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is no secret that pornographic material is now a one-click-away from\neveryone, including children and minors. General social media networks are\nstriving to isolate adult images and videos from normal ones. Intelligent image\nanalysis methods can help to automatically detect and isolate questionable\nimages in media. Unfortunately, these methods require vast experience to design\nthe classifier including one or more of the popular computer vision feature\ndescriptors. We propose to build a classifier based on one of the recently\nflourishing deep learning techniques. Convolutional neural networks contain\nmany layers for both automatic features extraction and classification. The\nbenefit is an easier system to build (no need for hand-crafting features and\nclassifiers). Additionally, our experiments show that it is even more accurate\nthan the state of the art methods on the most recent benchmark dataset.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2015 13:55:25 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Moustafa", "Mohamed", ""]]}, {"id": "1511.08913", "submitter": "Qi Guo", "authors": "Qi Guo, Le Dan, Dong Yin and Xiangyang Ji", "title": "Sliding-Window Optimization on an Ambiguity-Clearness Graph for\n  Multi-object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-object tracking remains challenging due to frequent occurrence of\nocclusions and outliers. In order to handle this problem, we propose an\nApproximation-Shrink Scheme for sequential optimization. This scheme is\nrealized by introducing an Ambiguity-Clearness Graph to avoid conflicts and\nmaintain sequence independent, as well as a sliding window optimization\nframework to constrain the size of state space and guarantee convergence. Based\non this window-wise framework, the states of targets are clustered in a\nself-organizing manner. Moreover, we show that the traditional online and batch\ntracking methods can be embraced by the window-wise framework. Experiments\nindicate that with only a small window, the optimization performance can be\nmuch better than online methods and approach to batch methods.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2015 17:02:39 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Guo", "Qi", ""], ["Dan", "Le", ""], ["Yin", "Dong", ""], ["Ji", "Xiangyang", ""]]}, {"id": "1511.08951", "submitter": "Basura Fernando", "authors": "Basura Fernando, Efstratios Gavves, Damien Muselet, Tinne Tuytelaars", "title": "MidRank: Learning to rank based on subsequences", "comments": "To appear in ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a supervised learning to rank algorithm that effectively orders\nimages by exploiting the structure in image sequences. Most often in the\nsupervised learning to rank literature, ranking is approached either by\nanalyzing pairs of images or by optimizing a list-wise surrogate loss function\non full sequences. In this work we propose MidRank, which learns from\nmoderately sized sub-sequences instead. These sub-sequences contain useful\nstructural ranking information that leads to better learnability during\ntraining and better generalization during testing. By exploiting sub-sequences,\nthe proposed MidRank improves ranking accuracy considerably on an extensive\narray of image ranking applications and datasets.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 00:47:19 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Fernando", "Basura", ""], ["Gavves", "Efstratios", ""], ["Muselet", "Damien", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1511.08956", "submitter": "Naveed Akhtar Mr.", "authors": "Naveed Akhtar and Faisal Shafait and Ajmal Mian", "title": "Sparseness helps: Sparsity Augmented Collaborative Representation for\n  Classification", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classification approaches first represent a test sample using the\ntraining samples of all the classes. This collaborative representation is then\nused to label the test sample. It was a common belief that sparseness of the\nrepresentation is the key to success for this classification scheme. However,\nmore recently, it has been claimed that it is the collaboration and not the\nsparseness that makes the scheme effective. This claim is attractive as it\nallows to relinquish the computationally expensive sparsity constraint over the\nrepresentation. In this paper, we first extend the analysis supporting this\nclaim and then show that sparseness explicitly contributes to improved\nclassification, hence it should not be completely ignored for computational\ngains. Inspired by this result, we augment a dense collaborative representation\nwith a sparse representation and propose an efficient classification method\nthat capitalizes on the resulting representation. The augmented representation\nand the classification method work together meticulously to achieve higher\naccuracy and lower computational time compared to state-of-the-art\ncollaborative representation based classification approaches. Experiments on\nbenchmark face, object and action databases show the efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 01:59:34 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Akhtar", "Naveed", ""], ["Shafait", "Faisal", ""], ["Mian", "Ajmal", ""]]}, {"id": "1511.09030", "submitter": "Martin Thoma", "authors": "Martin Thoma", "title": "On-line Recognition of Handwritten Mathematical Symbols", "comments": null, "journal-ref": null, "doi": "10.5445/IR/1000048047", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Finding the name of an unknown symbol is often hard, but writing the symbol\nis easy. This bachelor's thesis presents multiple systems that use the pen\ntrajectory to classify handwritten symbols. Five preprocessing steps, one data\naugmentation algorithm, five features and five variants for multilayer\nPerceptron training were evaluated using 166898 recordings which were collected\nwith two crowdsourcing projects. The evaluation results of these 21 experiments\nwere used to create an optimized recognizer which has a TOP1 error of less than\n17.5% and a TOP3 error of 4.0%. This is an improvement of 18.5% for the TOP1\nerror and 29.7% for the TOP3 error.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 15:52:00 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Thoma", "Martin", ""]]}, {"id": "1511.09033", "submitter": "Lior Wolf", "authors": "Etai Littwin and Lior Wolf", "title": "The Multiverse Loss for Robust Transfer Learning", "comments": "In the second version, whitening was applied in the CIFAR-100\n  experiments in order to improve results. Figure 2 in [v1] had a duplicate\n  subfigure which is now fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques are renowned for supporting effective transfer\nlearning. However, as we demonstrate, the transferred representations support\nonly a few modes of separation and much of its dimensionality is unutilized. In\nthis work, we suggest to learn, in the source domain, multiple orthogonal\nclassifiers. We prove that this leads to a reduced rank representation, which,\nhowever, supports more discriminative directions. Interestingly, the softmax\nprobabilities produced by the multiple classifiers are likely to be identical.\nExperimental results, on CIFAR-100 and LFW, further demonstrate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 16:10:47 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 14:00:59 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Littwin", "Etai", ""], ["Wolf", "Lior", ""]]}, {"id": "1511.09067", "submitter": "Mohamed Elawady", "authors": "Mohamed Elawady", "title": "Sparse Coral Classification Using Deep Convolutional Neural Networks", "comments": "Thesis Submitted for the Degree of MSc Erasmus Mundus in Vision and\n  Robotics (VIBOT 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous repair of deep-sea coral reefs is a recent proposed idea to\nsupport the oceans ecosystem in which is vital for commercial fishing, tourism\nand other species. This idea can be operated through using many small\nautonomous underwater vehicles (AUVs) and swarm intelligence techniques to\nlocate and replace chunks of coral which have been broken off, thus enabling\nre-growth and maintaining the habitat. The aim of this project is developing\nmachine vision algorithms to enable an underwater robot to locate a coral reef\nand a chunk of coral on the seabed and prompt the robot to pick it up. Although\nthere is no literature on this particular problem, related work on fish\ncounting may give some insight into the problem. The technical challenges are\nprincipally due to the potential lack of clarity of the water and platform\nstabilization as well as spurious artifacts (rocks, fish, and crabs). We\npresent an efficient sparse classification for coral species using supervised\ndeep learning method called Convolutional Neural Networks (CNNs). We compute\nWeber Local Descriptor (WLD), Phase Congruency (PC), and Zero Component\nAnalysis (ZCA) Whitening to extract shape and texture feature descriptors,\nwhich are employed to be supplementary channels (feature-based maps) besides\nbasic spatial color channels (spatial-based maps) of coral input image, we also\nexperiment state-of-art preprocessing underwater algorithms for image\nenhancement and color normalization and color conversion adjustment. Our\nproposed coral classification method is developed under MATLAB platform, and\nevaluated by two different coral datasets (University of California San Diego's\nMoorea Labeled Corals, and Heriot-Watt University's Atlantic Deep Sea).\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 19:18:36 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Elawady", "Mohamed", ""]]}, {"id": "1511.09120", "submitter": "Soliman Nasser", "authors": "Soliman Nasser, Ibrahim Jubran, Dan Feldman", "title": "Coresets for Kinematic Data: From Theorems to Real-Time Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A coreset (or core-set) of a dataset is its semantic compression with respect\nto a set of queries, such that querying the (small) coreset provably yields an\napproximate answer to querying the original (full) dataset. In the last decade,\ncoresets provided breakthroughs in theoretical computer science for\napproximation algorithms, and more recently, in the machine learning community\nfor learning \"Big data\". However, we are not aware of real-time systems that\ncompute coresets in a rate of dozens of frames per second. In this paper we\nsuggest a framework to turn theorems to such systems using coresets. We begin\nwith a proof of independent interest, that any set of $n$ matrices in\n$\\mathbb{R}^{d\\times d}$ whose sum is $S$, has a positively weighted subset\nwhose sum has the same center of mass (mean) and orientation (left+right\nsingular vectors) as $S$, and consists of $O(dr)$ matrices (independent of\n$n$), where $r\\leq d$ is the rank of $S$. We provide an algorithm that computes\nthis (core) set in one pass over possibly infinite stream of matrices in\n$d^{O(1)}$ time per matrix insertion. By maintaining such a coreset for\nkinematic (moving) set of $n$ points, we can run pose-estimation algorithms,\nsuch as Kabsch or PnP, on the small coresets, instead of the $n$ points, in\nreal-time using weak devices, while obtaining the same results. This enabled us\nto implement a low-cost ($<\\$100$) IoT wireless system that tracks a toy (and\nharmless) quadcopter which guides guests to a desired room (in a hospital,\nmall, hotel, museum, etc.) with no help of additional human or remote\ncontroller. We hope that our framework will encourage researchers outside the\ntheoretical community to design and use coresets in future systems and papers.\nTo this end, we provide extensive experimental results on both synthetic and\nreal data, as well as a link to the open code of our system and algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 00:44:41 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 17:40:31 GMT"}, {"version": "v3", "created": "Fri, 22 Jul 2016 12:43:28 GMT"}, {"version": "v4", "created": "Mon, 18 Dec 2017 14:04:48 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Nasser", "Soliman", ""], ["Jubran", "Ibrahim", ""], ["Feldman", "Dan", ""]]}, {"id": "1511.09123", "submitter": "Ka-Chun Wong", "authors": "Ka-Chun Wong", "title": "A Short Survey on Data Clustering Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapidly increasing data, clustering algorithms are important tools for\ndata analytics in modern research. They have been successfully applied to a\nwide range of domains; for instance, bioinformatics, speech recognition, and\nfinancial analysis. Formally speaking, given a set of data instances, a\nclustering algorithm is expected to divide the set of data instances into the\nsubsets which maximize the intra-subset similarity and inter-subset\ndissimilarity, where a similarity measure is defined beforehand. In this work,\nthe state-of-the-arts clustering algorithms are reviewed from design concept to\nmethodology; Different clustering paradigms are discussed. Advanced clustering\nalgorithms are also discussed. After that, the existing clustering evaluation\nmetrics are reviewed. A summary with future insights is provided at the end.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 08:02:37 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Wong", "Ka-Chun", ""]]}, {"id": "1511.09150", "submitter": "Rahul Rama Varior Mr.", "authors": "Rahul Rama Varior, Gang Wang", "title": "Hierarchical Invariant Feature Learning with Marginalization for Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of matching pedestrians across multiple\ncamera views, known as person re-identification. Variations in lighting\nconditions, environment and pose changes across camera views make\nre-identification a challenging problem. Previous methods address these\nchallenges by designing specific features or by learning a distance function.\nWe propose a hierarchical feature learning framework that learns invariant\nrepresentations from labeled image pairs. A mapping is learned such that the\nextracted features are invariant for images belonging to same individual across\nviews. To learn robust representations and to achieve better generalization to\nunseen data, the system has to be trained with a large amount of data.\nCritically, most of the person re-identification datasets are small. Manually\naugmenting the dataset by partial corruption of input data introduces\nadditional computational burden as it requires several training epochs to\nconverge. We propose a hierarchical network which incorporates a\nmarginalization technique that can reap the benefits of training on large\ndatasets without explicit augmentation. We compare our approach with several\nbaseline algorithms as well as popular linear and non-linear metric learning\nalgorithms and demonstrate improved performance on challenging publicly\navailable datasets, VIPeR, CUHK01, CAVIAR4REID and iLIDS. Our approach also\nachieves the stateof-the-art results on these datasets.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 04:05:21 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Varior", "Rahul Rama", ""], ["Wang", "Gang", ""]]}, {"id": "1511.09207", "submitter": "Cong Yao", "authors": "Cong Yao, Jianan Wu, Xinyu Zhou, Chi Zhang, Shuchang Zhou, Zhimin Cao,\n  Qi Yin", "title": "Incidental Scene Text Understanding: Recent Progresses on ICDAR 2015\n  Robust Reading Competition Challenge 4", "comments": "3 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from focused texts present in natural images, which are captured\nwith user's intention and intervention, incidental texts usually exhibit much\nmore diversity, variability and complexity, thus posing significant\ndifficulties and challenges for scene text detection and recognition\nalgorithms. The ICDAR 2015 Robust Reading Competition Challenge 4 was launched\nto assess the performance of existing scene text detection and recognition\nmethods on incidental texts as well as to stimulate novel ideas and solutions.\nThis report is dedicated to briefly introduce our strategies for this\nchallenging problem and compare them with prior arts in this field.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 09:08:02 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 07:26:43 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Yao", "Cong", ""], ["Wu", "Jianan", ""], ["Zhou", "Xinyu", ""], ["Zhang", "Chi", ""], ["Zhou", "Shuchang", ""], ["Cao", "Zhimin", ""], ["Yin", "Qi", ""]]}, {"id": "1511.09209", "submitter": "Zongyuan Ge", "authors": "ZongYuan Ge and Alex Bewley and Christopher McCool and Ben Upcroft and\n  Peter Corke and Conrad Sanderson", "title": "Fine-Grained Classification via Mixture of Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep convolutional neural network (DCNN) system for\nfine-grained image classification, called a mixture of DCNNs (MixDCNN). The\nfine-grained image classification problem is characterised by large intra-class\nvariations and small inter-class variations. To overcome these problems our\nproposed MixDCNN system partitions images into K subsets of similar images and\nlearns an expert DCNN for each subset. The output from each of the K DCNNs is\ncombined to form a single classification decision. In contrast to previous\ntechniques, we provide a formulation to perform joint end-to-end training of\nthe K DCNNs simultaneously. Extensive experiments, on three datasets using two\nnetwork structures (AlexNet and GoogLeNet), show that the proposed MixDCNN\nsystem consistently outperforms other methods. It provides a relative\nimprovement of 12.7% and achieves state-of-the-art results on two datasets.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 09:14:10 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Ge", "ZongYuan", ""], ["Bewley", "Alex", ""], ["McCool", "Christopher", ""], ["Upcroft", "Ben", ""], ["Corke", "Peter", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1511.09231", "submitter": "Zhun Sun", "authors": "Zhun Sun, Mete Ozay, Takayuki Okatani", "title": "Design of Kernels in Convolutional Neural Networks for Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the effectiveness of Convolutional Neural Networks (CNNs) for image\nclassification, our understanding of the relationship between shape of\nconvolution kernels and learned representations is limited. In this work, we\nexplore and employ the relationship between shape of kernels which define\nReceptive Fields (RFs) in CNNs for learning of feature representations and\nimage classification. For this purpose, we first propose a feature\nvisualization method for visualization of pixel-wise classification score maps\nof learned features. Motivated by our experimental results, and observations\nreported in the literature for modeling of visual systems, we propose a novel\ndesign of shape of kernels for learning of representations in CNNs. In the\nexperimental results, we achieved a state-of-the-art classification performance\ncompared to a base CNN model [28] by reducing the number of parameters and\ncomputational time of the model using the ILSVRC-2012 dataset [24]. The\nproposed models also outperform the state-of-the-art models employed on the\nCIFAR-10/100 datasets [12] for image classification. Additionally, we analyzed\nthe robustness of the proposed method to occlusion for classification of\npartially occluded images compared with the state-of-the-art methods. Our\nresults indicate the effectiveness of the proposed approach. The code is\navailable in github.com/minogame/caffe-qhconv.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 10:30:35 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 11:59:08 GMT"}, {"version": "v3", "created": "Tue, 29 Nov 2016 04:11:58 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Sun", "Zhun", ""], ["Ozay", "Mete", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1511.09319", "submitter": "Luca Del Pero", "authors": "Luca Del Pero, Susanna Ricco, Rahul Sukthankar, Vittorio Ferrari", "title": "Behavior Discovery and Alignment of Articulated Object Classes from\n  Unstructured Video", "comments": "19 pages, 19 figure, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:1411.7883", "journal-ref": "International Journal of Computer Vision (IJCV), July 2016", "doi": "10.1007/S11263-016-0939-9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automatic system for organizing the content of a collection of\nunstructured videos of an articulated object class (e.g. tiger, horse). By\nexploiting the recurring motion patterns of the class across videos, our\nsystem: 1) identifies its characteristic behaviors; and 2) recovers\npixel-to-pixel alignments across different instances. Our system can be useful\nfor organizing video collections for indexing and retrieval. Moreover, it can\nbe a platform for learning the appearance or behaviors of object classes from\nInternet video. Traditional supervised techniques cannot exploit this wealth of\ndata directly, as they require a large amount of time-consuming manual\nannotations.\n  The behavior discovery stage generates temporal video intervals, each\nautomatically trimmed to one instance of the discovered behavior, clustered by\ntype. It relies on our novel motion representation for articulated motion based\non the displacement of ordered pairs of trajectories (PoTs). The alignment\nstage aligns hundreds of instances of the class to a great accuracy despite\nconsiderable appearance variations (e.g. an adult tiger and a cub). It uses a\nflexible Thin Plate Spline deformation model that can vary through time. We\ncarefully evaluate each step of our system on a new, fully annotated dataset.\nOn behavior discovery, we outperform the state-of-the-art Improved DTF\ndescriptor. On spatial alignment, we outperform the popular SIFT Flow\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 14:22:52 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 01:29:20 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Del Pero", "Luca", ""], ["Ricco", "Susanna", ""], ["Sukthankar", "Rahul", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1511.09439", "submitter": "Xiaowei Zhou", "authors": "Xiaowei Zhou, Menglong Zhu, Spyridon Leonardos, Kosta Derpanis, Kostas\n  Daniilidis", "title": "Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video", "comments": "Published in CVPR2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenge of 3D full-body human pose estimation from\na monocular image sequence. Here, two cases are considered: (i) the image\nlocations of the human joints are provided and (ii) the image locations of\njoints are unknown. In the former case, a novel approach is introduced that\nintegrates a sparsity-driven 3D geometric prior and temporal smoothness. In the\nlatter case, the former case is extended by treating the image locations of the\njoints as latent variables. A deep fully convolutional network is trained to\npredict the uncertainty maps of the 2D joint locations. The 3D pose estimates\nare realized via an Expectation-Maximization algorithm over the entire\nsequence, where it is shown that the 2D joint location uncertainties can be\nconveniently marginalized out during inference. Empirical evaluation on the\nHuman3.6M dataset shows that the proposed approaches achieve greater 3D pose\nestimation accuracy over state-of-the-art baselines. Further, the proposed\napproach outperforms a publicly available 2D pose estimation baseline on the\nchallenging PennAction dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 19:41:06 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 14:53:43 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Zhou", "Xiaowei", ""], ["Zhu", "Menglong", ""], ["Leonardos", "Spyridon", ""], ["Derpanis", "Kosta", ""], ["Daniilidis", "Kostas", ""]]}]