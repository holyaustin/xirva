[{"id": "1804.00021", "submitter": "Xishuang Dong", "authors": "Xishuang Dong, Hsiang-Huang Wu, Yuzhong Yan, Lijun Qian", "title": "Hierarchical Transfer Convolutional Neural Networks for Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the issue of how to enhance the generalization\nperformance of convolutional neural networks (CNN) in the early learning stage\nfor image classification. This is motivated by real-time applications that\nrequire the generalization performance of CNN to be satisfactory within limited\ntraining time. In order to achieve this, a novel hierarchical transfer CNN\nframework is proposed. It consists of a group of shallow CNNs and a cloud CNN,\nwhere the shallow CNNs are trained firstly and then the first layers of the\ntrained shallow CNNs are used to initialize the first layer of the cloud CNN.\nThis method will boost the generalization performance of the cloud CNN\nsignificantly, especially during the early stage of training. Experiments using\nCIFAR-10 and ImageNet datasets are performed to examine the proposed method.\nResults demonstrate the improvement of testing accuracy is 12% on average and\nas much as 20% for the CIFAR-10 case while 5% testing accuracy improvement for\nthe ImageNet case during the early stage of learning. It is also shown that\nuniversal improvements of testing accuracy are obtained across different\nsettings of dropout and number of shallow CNNs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 18:19:32 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 19:38:24 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Dong", "Xishuang", ""], ["Wu", "Hsiang-Huang", ""], ["Yan", "Yuzhong", ""], ["Qian", "Lijun", ""]]}, {"id": "1804.00060", "submitter": "Varun Manjunatha", "authors": "Varun Manjunatha and Srikumar Ramalingam and Tim K. Marks and Larry\n  Davis", "title": "Class Subset Selection for Transfer Learning using Submodularity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, it is common practice to extract fully-connected layer (fc)\nfeatures that were learned while performing image classification on a source\ndataset, such as ImageNet, and apply them generally to a wide range of other\ntasks. The general usefulness of some large training datasets for transfer\nlearning is not yet well understood, and raises a number of questions. For\nexample, in the context of transfer learning, what is the role of a specific\nclass in the source dataset, and how is the transferability of fc features\naffected when they are trained using various subsets of the set of all classes\nin the source dataset? In this paper, we address the question of how to select\nan optimal subset of the set of classes, subject to a budget constraint, that\nwill more likely generate good features for other tasks. To accomplish this, we\nuse a submodular set function to model the accuracy achievable on a new task\nwhen the features have been learned on a given subset of classes of the source\ndataset. An optimal subset is identified as the set that maximizes this\nsubmodular function. The maximization can be accomplished using an efficient\ngreedy algorithm that comes with guarantees on the optimality of the solution.\nWe empirically validate our submodular model by successfully identifying\nsubsets of classes that produce good features for new tasks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 21:36:59 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Manjunatha", "Varun", ""], ["Ramalingam", "Srikumar", ""], ["Marks", "Tim K.", ""], ["Davis", "Larry", ""]]}, {"id": "1804.00064", "submitter": "Jyh-Jing Hwang", "authors": "Jyh-Jing Hwang, Sergei Azernikov, Alexei A. Efros, and Stella X. Yu", "title": "Learning Beyond Human Expertise with Generative Models for Dental\n  Restorations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has advanced significantly that many discriminative\napproaches such as object recognition are now widely used in real applications.\nWe present another exciting development that utilizes generative models for the\nmass customization of medical products such as dental crowns. In the dental\nindustry, it takes a technician years of training to design synthetic crowns\nthat restore the function and integrity of missing teeth. Each crown must be\ncustomized to individual patients, and it requires human expertise in a\ntime-consuming and labor-intensive process, even with computer-assisted design\nsoftware. We develop a fully automatic approach that learns not only from human\ndesigns of dental crowns, but also from natural spatial profiles between\nopposing teeth. The latter is hard to account for by technicians but important\nfor proper biting and chewing functions. Built upon a Generative Adversar-ial\nNetwork architecture (GAN), our deep learning model predicts the customized\ncrown-filled depth scan from the crown-missing depth scan and opposing depth\nscan. We propose to incorporate additional space constraints and statistical\ncompatibility into learning. Our automatic designs exceed human technicians'\nstandards for good morphology and functionality, and our algorithm is being\ntested for production use.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 21:56:38 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Hwang", "Jyh-Jing", ""], ["Azernikov", "Sergei", ""], ["Efros", "Alexei A.", ""], ["Yu", "Stella X.", ""]]}, {"id": "1804.00090", "submitter": "Jiaye Wu", "authors": "Chen Liu and Jiaye Wu and Yasutaka Furukawa", "title": "FloorNet: A Unified Framework for Floorplan Reconstruction from 3D Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ultimate goal of this indoor mapping research is to automatically\nreconstruct a floorplan simply by walking through a house with a smartphone in\na pocket. This paper tackles this problem by proposing FloorNet, a novel deep\nneural architecture. The challenge lies in the processing of RGBD streams\nspanning a large 3D space. FloorNet effectively processes the data through\nthree neural network branches: 1) PointNet with 3D points, exploiting the 3D\ninformation; 2) CNN with a 2D point density image in a top-down view, enhancing\nthe local spatial reasoning; and 3) CNN with RGB images, utilizing the full\nimage information. FloorNet exchanges intermediate features across the branches\nto exploit the best of all the architectures. We have created a benchmark for\nfloorplan reconstruction by acquiring RGBD video streams for 155 residential\nhouses or apartments with Google Tango phones and annotating complete floorplan\ninformation. Our qualitative and quantitative evaluations demonstrate that the\nfusion of three branches effectively improves the reconstruction quality. We\nhope that the paper together with the benchmark will be an important step\ntowards solving a challenging vector-graphics reconstruction problem. Code and\ndata are available at https://github.com/art-programmer/FloorNet.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 00:22:27 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Liu", "Chen", ""], ["Wu", "Jiaye", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "1804.00092", "submitter": "Yisen Wang", "authors": "Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le\n  Song, Shu-Tao Xia", "title": "Iterative Learning with Open-set Noisy Labels", "comments": "CVPR2018-Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale datasets possessing clean label annotations are crucial for\ntraining Convolutional Neural Networks (CNNs). However, labeling large-scale\ndata can be very costly and error-prone, and even high-quality datasets are\nlikely to contain noisy (incorrect) labels. Existing works usually employ a\nclosed-set assumption, whereby the samples associated with noisy labels possess\na true class contained within the set of known classes in the training data.\nHowever, such an assumption is too restrictive for many applications, since\nsamples associated with noisy labels might in fact possess a true class that is\nnot present in the training data. We refer to this more complex scenario as the\n\\textbf{open-set noisy label} problem and show that it is nontrivial in order\nto make accurate predictions. To address this problem, we propose a novel\niterative learning framework for training CNNs on datasets with open-set noisy\nlabels. Our approach detects noisy labels and learns deep discriminative\nfeatures in an iterative fashion. To benefit from the noisy label detection, we\ndesign a Siamese network to encourage clean labels and noisy labels to be\ndissimilar. A reweighting module is also applied to simultaneously emphasize\nthe learning from clean labels and reduce the effect caused by noisy labels.\nExperiments on CIFAR-10, ImageNet and real-world noisy (web-search) datasets\ndemonstrate that our proposed model can robustly train CNNs in the presence of\na high proportion of open-set as well as closed-set noisy labels.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 00:27:30 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Wang", "Yisen", ""], ["Liu", "Weiyang", ""], ["Ma", "Xingjun", ""], ["Bailey", "James", ""], ["Zha", "Hongyuan", ""], ["Song", "Le", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "1804.00097", "submitter": "Alexey Kurakin", "authors": "Alexey Kurakin, Ian Goodfellow, Samy Bengio, Yinpeng Dong, Fangzhou\n  Liao, Ming Liang, Tianyu Pang, Jun Zhu, Xiaolin Hu, Cihang Xie, Jianyu Wang,\n  Zhishuai Zhang, Zhou Ren, Alan Yuille, Sangxia Huang, Yao Zhao, Yuzhe Zhao,\n  Zhonglin Han, Junjiajia Long, Yerkebulan Berdibekov, Takuya Akiba, Seiya\n  Tokui, Motoki Abe", "title": "Adversarial Attacks and Defences Competition", "comments": "36 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To accelerate research on adversarial examples and robustness of machine\nlearning classifiers, Google Brain organized a NIPS 2017 competition that\nencouraged researchers to develop new methods to generate adversarial examples\nas well as to develop new ways to defend against them. In this chapter, we\ndescribe the structure and organization of the competition and the solutions\ndeveloped by several of the top-placing teams.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 00:52:20 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kurakin", "Alexey", ""], ["Goodfellow", "Ian", ""], ["Bengio", "Samy", ""], ["Dong", "Yinpeng", ""], ["Liao", "Fangzhou", ""], ["Liang", "Ming", ""], ["Pang", "Tianyu", ""], ["Zhu", "Jun", ""], ["Hu", "Xiaolin", ""], ["Xie", "Cihang", ""], ["Wang", "Jianyu", ""], ["Zhang", "Zhishuai", ""], ["Ren", "Zhou", ""], ["Yuille", "Alan", ""], ["Huang", "Sangxia", ""], ["Zhao", "Yao", ""], ["Zhao", "Yuzhe", ""], ["Han", "Zhonglin", ""], ["Long", "Junjiajia", ""], ["Berdibekov", "Yerkebulan", ""], ["Akiba", "Takuya", ""], ["Tokui", "Seiya", ""], ["Abe", "Motoki", ""]]}, {"id": "1804.00100", "submitter": "Jingwen Wang", "authors": "Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, Yong Xu", "title": "Bidirectional Attentive Fusion with Context Gating for Dense Video\n  Captioning", "comments": "CVPR2018 spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Dense video captioning is a newly emerging task that aims at both localizing\nand describing all events in a video. We identify and tackle two challenges on\nthis task, namely, (1) how to utilize both past and future contexts for\naccurate event proposal predictions, and (2) how to construct informative input\nto the decoder for generating natural event descriptions. First, previous works\npredominantly generate temporal event proposals in the forward direction, which\nneglects future video context. We propose a bidirectional proposal method that\neffectively exploits both past and future contexts to make proposal\npredictions. Second, different events ending at (nearly) the same time are\nindistinguishable in the previous works, resulting in the same captions. We\nsolve this problem by representing each event with an attentive fusion of\nhidden states from the proposal module and video contents (e.g., C3D features).\nWe further propose a novel context gating mechanism to balance the\ncontributions from the current event and its surrounding contexts dynamically.\nWe empirically show that our attentively fused event representation is superior\nto the proposal hidden states or video contents alone. By coupling proposal and\ncaptioning modules into one unified framework, our model outperforms the\nstate-of-the-arts on the ActivityNet Captions dataset with a relative gain of\nover 100% (Meteor score increases from 4.82 to 9.65).\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 01:08:33 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 08:29:53 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Wang", "Jingwen", ""], ["Jiang", "Wenhao", ""], ["Ma", "Lin", ""], ["Liu", "Wei", ""], ["Xu", "Yong", ""]]}, {"id": "1804.00103", "submitter": "Xiangyu Yue", "authors": "Xiangyu Yue, Bichen Wu, Sanjit A. Seshia, Kurt Keutzer and Alberto L.\n  Sangiovanni-Vincentelli", "title": "A LiDAR Point Cloud Generator: from a Virtual World to Autonomous\n  Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D LiDAR scanners are playing an increasingly important role in autonomous\ndriving as they can generate depth information of the environment. However,\ncreating large 3D LiDAR point cloud datasets with point-level labels requires a\nsignificant amount of manual annotation. This jeopardizes the efficient\ndevelopment of supervised deep learning algorithms which are often data-hungry.\nWe present a framework to rapidly create point clouds with accurate point-level\nlabels from a computer game. The framework supports data collection from both\nauto-driving scenes and user-configured scenes. Point clouds from auto-driving\nscenes can be used as training data for deep learning algorithms, while point\nclouds from user-configured scenes can be used to systematically test the\nvulnerability of a neural network, and use the falsifying examples to make the\nneural network more robust through retraining. In addition, the scene images\ncan be captured simultaneously in order for sensor fusion tasks, with a method\nproposed to do automatic calibration between the point clouds and captured\nscene images. We show a significant improvement in accuracy (+9%) in point\ncloud segmentation by augmenting the training dataset with the generated\nsynthesized data. Our experiments also show by testing and retraining the\nnetwork using point clouds from user-configured scenes, the weakness/blind\nspots of the neural network can be fixed.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 01:32:11 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Yue", "Xiangyu", ""], ["Wu", "Bichen", ""], ["Seshia", "Sanjit A.", ""], ["Keutzer", "Kurt", ""], ["Sangiovanni-Vincentelli", "Alberto L.", ""]]}, {"id": "1804.00105", "submitter": "Qingxing Cao", "authors": "Qingxing Cao, Xiaodan Liang, Bailing Li, Guanbin Li, Liang Lin", "title": "Visual Question Reasoning on General Dependency Tree", "comments": "Accepted as spotlight at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collaborative reasoning for understanding each image-question pair is\nvery critical but under-explored for an interpretable Visual Question Answering\n(VQA) system. Although very recent works also tried the explicit compositional\nprocesses to assemble multiple sub-tasks embedded in the questions, their\nmodels heavily rely on the annotations or hand-crafted rules to obtain valid\nreasoning layout, leading to either heavy labor or poor performance on\ncomposition reasoning. In this paper, to enable global context reasoning for\nbetter aligning image and language domains in diverse and unrestricted cases,\nwe propose a novel reasoning network called Adversarial Composition Modular\nNetwork (ACMN). This network comprises of two collaborative modules: i) an\nadversarial attention module to exploit the local visual evidence for each word\nparsed from the question; ii) a residual composition module to compose the\npreviously mined evidence. Given a dependency parse tree for each question, the\nadversarial attention module progressively discovers salient regions of one\nword by densely combining regions of child word nodes in an adversarial manner.\nThen residual composition module merges the hidden representations of an\narbitrary number of children through sum pooling and residual connection. Our\nACMN is thus capable of building an interpretable VQA system that gradually\ndives the image cues following a question-driven reasoning route and makes\nglobal reasoning by incorporating the learned knowledge of all attention\nmodules in a principled manner. Experiments on relational datasets demonstrate\nthe superiority of our ACMN and visualization results show the explainable\ncapability of our reasoning system.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 01:48:27 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Cao", "Qingxing", ""], ["Liang", "Xiaodan", ""], ["Li", "Bailing", ""], ["Li", "Guanbin", ""], ["Lin", "Liang", ""]]}, {"id": "1804.00112", "submitter": "Steven Chen", "authors": "Steven Chen and Kristen Grauman", "title": "Compare and Contrast: Learning Prominent Visual Differences", "comments": "In Proceedings of the IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relative attribute models can compare images in terms of all detected\nproperties or attributes, exhaustively predicting which image is fancier, more\nnatural, and so on without any regard to ordering. However, when humans compare\nimages, certain differences will naturally stick out and come to mind first.\nThese most noticeable differences, or prominent differences, are likely to be\ndescribed first. In addition, many differences, although present, may not be\nmentioned at all. In this work, we introduce and model prominent differences, a\nrich new functionality for comparing images. We collect instance-level\nannotations of most noticeable differences, and build a model trained on\nrelative attribute features that predicts prominent differences for unseen\npairs. We test our model on the challenging UT-Zap50K shoes and LFW10 faces\ndatasets, and outperform an array of baseline methods. We then demonstrate how\nour prominence model improves two vision tasks, image search and description\ngeneration, enabling more natural communication between people and vision\nsystems.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 03:20:18 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 18:44:09 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Chen", "Steven", ""], ["Grauman", "Kristen", ""]]}, {"id": "1804.00113", "submitter": "Baoyuan Wu", "authors": "Baoyuan Wu, Weidong Chen, Peng Sun, Wei Liu, Bernard Ghanem, Siwei Lyu", "title": "Tagging like Humans: Diverse and Distinct Image Annotation", "comments": "Accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a new automatic image annotation model, dubbed {\\bf\ndiverse and distinct image annotation} (D2IA). The generative model D2IA is\ninspired by the ensemble of human annotations, which create semantically\nrelevant, yet distinct and diverse tags. In D2IA, we generate a relevant and\ndistinct tag subset, in which the tags are relevant to the image contents and\nsemantically distinct to each other, using sequential sampling from a\ndeterminantal point process (DPP) model. Multiple such tag subsets that cover\ndiverse semantic aspects or diverse semantic levels of the image contents are\ngenerated by randomly perturbing the DPP sampling process. We leverage a\ngenerative adversarial network (GAN) model to train D2IA. Extensive experiments\nincluding quantitative and qualitative comparisons, as well as human subject\nstudies, on two benchmark datasets demonstrate that the proposed model can\nproduce more diverse and distinct tags than the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 03:22:50 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Wu", "Baoyuan", ""], ["Chen", "Weidong", ""], ["Sun", "Peng", ""], ["Liu", "Wei", ""], ["Ghanem", "Bernard", ""], ["Lyu", "Siwei", ""]]}, {"id": "1804.00117", "submitter": "Baoyuan Wu", "authors": "Baoyuan Wu, Fan Jia, Wei Liu, Bernard Ghanem, Siwei Lyu", "title": "Multi-label Learning with Missing Labels using Mixed Dependency Graphs", "comments": "Published in International Journal of Computer Vision, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the problem of multi-label learning with missing labels\n(MLML), which aims to label each test instance with multiple class labels given\ntraining instances that have an incomplete/partial set of these labels. The key\npoint to handle missing labels is propagating the label information from\nprovided labels to missing labels, through a dependency graph that each label\nof each instance is treated as a node. We build this graph by utilizing\ndifferent types of label dependencies. Specifically, the instance-level\nsimilarity is served as undirected edges to connect the label nodes across\ndifferent instances and the semantic label hierarchy is used as directed edges\nto connect different classes. This base graph is referred to as the mixed\ndependency graph, as it includes both undirected and directed edges.\nFurthermore, we present another two types of label dependencies to connect the\nlabel nodes across different classes. One is the class co-occurrence, which is\nalso encoded as undirected edges. Combining with the base graph, we obtain a\nnew mixed graph, called MG-CO (mixed graph with co-occurrence). The other is\nthe sparse and low rank decomposition of the whole label matrix, to embed\nhigh-order dependencies over all labels. Combining with the base graph, the new\nmixed graph is called as MG-SL (mixed graph with sparse and low rank\ndecomposition). Based on MG-CO and MG-SL, we propose two convex transductive\nformulations of the MLML problem, denoted as MLMG-CO and MLMG-SL, respectively.\nTwo important applications, including image annotation and tag based image\nretrieval, can be jointly handled using our proposed methods. Experiments on\nbenchmark datasets show that our methods give significant improvements in\nperformance and robustness to missing labels over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 04:15:11 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Wu", "Baoyuan", ""], ["Jia", "Fan", ""], ["Liu", "Wei", ""], ["Ghanem", "Bernard", ""], ["Lyu", "Siwei", ""]]}, {"id": "1804.00118", "submitter": "Mao-Chuang Yeh", "authors": "Mao-Chuang Yeh, Shuai Tang, Anand Bhattad, and D. A. Forsyth", "title": "Quantitative Evaluation of Style Transfer", "comments": "30 pages, including supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer methods produce a transferred image which is a rendering of a\ncontent image in the manner of a style image. There is a rich literature of\nvariant methods. However, evaluation procedures are qualitative, mostly\ninvolving user studies. We describe a novel quantitative evaluation procedure.\nOne plots effectiveness (a measure of the extent to which the style was\ntransferred) against coherence (a measure of the extent to which the\ntransferred image decomposes into objects in the same way that the content\nimage does) to obtain an EC plot.\n  We construct EC plots comparing a number of recent style transfer methods.\nMost methods control within-layer gram matrices, but we also investigate a\nmethod that controls cross-layer gram matrices. These EC plots reveal a number\nof intriguing properties of recent style transfer methods. The style used has a\nstrong effect on the outcome, for all methods. Using large style weights does\nnot necessarily improve effectiveness, and can produce worse results.\nCross-layer gram matrices easily beat all other methods, but some styles remain\ndifficult for all methods. Ensemble methods show real promise. It is likely\nthat, for current methods, each style requires a different choice of weights to\nobtain the best results, so that automated weight setting methods are\ndesirable. Finally, we show evidence comparing our EC evaluations to human\nevaluations.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 04:25:05 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Yeh", "Mao-Chuang", ""], ["Tang", "Shuai", ""], ["Bhattad", "Anand", ""], ["Forsyth", "D. A.", ""]]}, {"id": "1804.00126", "submitter": "Bo Xiong", "authors": "Bo Xiong and Kristen Grauman", "title": "Snap Angle Prediction for 360$^{\\circ}$ Panoramas", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  360$^{\\circ}$ panoramas are a rich medium, yet notoriously difficult to\nvisualize in the 2D image plane. We explore how intelligent rotations of a\nspherical image may enable content-aware projection with fewer perceptible\ndistortions. Whereas existing approaches assume the viewpoint is fixed,\nintuitively some viewing angles within the sphere preserve high-level objects\nbetter than others. To discover the relationship between these optimal snap\nangles and the spherical panorama's content, we develop a reinforcement\nlearning approach for the cubemap projection model. Implemented as a deep\nrecurrent neural network, our method selects a sequence of rotation actions and\nreceives reward for avoiding cube boundaries that overlap with important\nforeground objects. We show our approach creates more visually pleasing\npanoramas while using 5x less computation than the baseline.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 06:09:30 GMT"}, {"version": "v2", "created": "Sun, 12 Aug 2018 01:00:39 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Xiong", "Bo", ""], ["Grauman", "Kristen", ""]]}, {"id": "1804.00174", "submitter": "Song Feng", "authors": "Song Feng, Linhua Deng, Guofeng Shu, Feng Wang, Hui Deng and Kaifan Ji", "title": "A Subpixel Registration Algorithm for Low PSNR Images", "comments": "in 2012 IEEE 5th Int. Conf. on Advanced Computational Intelligence\n  (ICACI) (New York: IEEE), 626", "journal-ref": null, "doi": "10.1109/ICACI.2012.6463241", "report-no": null, "categories": "cs.CV astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fast algorithm for obtaining high-accuracy subpixel\ntranslation of low PSNR images. Instead of locating the maximum point on the\nupsampled images or fitting the peak of correlation surface, the proposed\nalgorithm is based on the measurement of centroid on the cross correlation\nsurface by Modified Moment method. Synthetic images, real solar images and\nstandard testing images with white Gaussian noise added were tested, and the\nresults show that the accuracies of our algorithm are comparable with other\nsubpixel registration techniques and the processing speed is higher. The\ndrawback is also discussed at the end of this paper.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 14:00:32 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Feng", "Song", ""], ["Deng", "Linhua", ""], ["Shu", "Guofeng", ""], ["Wang", "Feng", ""], ["Deng", "Hui", ""], ["Ji", "Kaifan", ""]]}, {"id": "1804.00175", "submitter": "Yi Li", "authors": "Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, Dieter Fox", "title": "DeepIM: Deep Iterative Matching for 6D Pose Estimation", "comments": "submitted to IJCV, update results on YCB_Video, add depth-based\n  results", "journal-ref": null, "doi": "10.1007/s11263-019-01250-9", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the 6D pose of objects from images is an important problem in\nvarious applications such as robot manipulation and virtual reality. While\ndirect regression of images to object poses has limited accuracy, matching\nrendered images of an object against the observed image can produce accurate\nresults. In this work, we propose a novel deep neural network for 6D pose\nmatching named DeepIM. Given an initial pose estimation, our network is able to\niteratively refine the pose by matching the rendered image against the observed\nimage. The network is trained to predict a relative pose transformation using\nan untangled representation of 3D location and 3D orientation and an iterative\ntraining process. Experiments on two commonly used benchmarks for 6D pose\nestimation demonstrate that DeepIM achieves large improvements over\nstate-of-the-art methods. We furthermore show that DeepIM is able to match\npreviously unseen objects.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 14:02:25 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 16:28:50 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 13:25:49 GMT"}, {"version": "v4", "created": "Wed, 2 Oct 2019 00:54:47 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Li", "Yi", ""], ["Wang", "Gu", ""], ["Ji", "Xiangyang", ""], ["Xiang", "Yu", ""], ["Fox", "Dieter", ""]]}, {"id": "1804.00177", "submitter": "Fernando Navarro", "authors": "Fernando Navarro, Sailesh Conjeti, Federico Tombari, and Nassir Navab", "title": "Webly Supervised Learning for Skin Lesion Classification", "comments": "Accepted to International Conference on Medical Image Computing and\n  Computer-Assisted Intervention 2018 Added Acknowledgements section, rest is\n  unchanged. In MICCAI 2018. Springer, Cham", "journal-ref": null, "doi": "10.1007/978-3-030-00934-2_45", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within medical imaging, manual curation of sufficient well-labeled samples is\ncost, time and scale-prohibitive. To improve the representativeness of the\ntraining dataset, for the first time, we present an approach to utilize large\namounts of freely available web data through web-crawling. To handle noise and\nweak nature of web annotations, we propose a two-step transfer learning based\ntraining process with a robust loss function, termed as Webly Supervised\nLearning (WSL) to train deep models for the task. We also leverage search by\nimage to improve the search specificity of our web-crawling and reduce\ncross-domain noise. Within WSL, we explicitly model the noise structure between\nclasses and incorporate it to selectively distill knowledge from the web data\nduring model training. To demonstrate improved performance due to WSL, we\nbenchmarked on a publicly available 10-class fine-grained skin lesion\nclassification dataset and report a significant improvement of top-1\nclassification accuracy from 71.25 % to 80.53 % due to the incorporation of\nweb-supervision.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 14:13:43 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 08:14:55 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Navarro", "Fernando", ""], ["Conjeti", "Sailesh", ""], ["Tombari", "Federico", ""], ["Navab", "Nassir", ""]]}, {"id": "1804.00194", "submitter": "Adam Czajka", "authors": "Adam Czajka and Kevin W. Bowyer", "title": "Presentation Attack Detection for Iris Recognition: An Assessment of the\n  State of the Art", "comments": "Pre-print accepted for publication in ACM Computing Surveys on June\n  13, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris recognition is increasingly used in large-scale applications. As a\nresult, presentation attack detection for iris recognition takes on fundamental\nimportance. This survey covers the diverse research literature on this topic.\nDifferent categories of presentation attack are described and placed in an\napplication-relevant framework, and the state of the art in detecting each\ncategory of attack is summarized. One conclusion from this is that presentation\nattack detection for iris recognition is not yet a solved problem. Datasets\navailable for research are described, research directions for the near- and\nmedium-term future are outlined, and a short list of recommended readings are\nsuggested.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 17:17:50 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 11:28:52 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 17:08:39 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Czajka", "Adam", ""], ["Bowyer", "Kevin W.", ""]]}, {"id": "1804.00213", "submitter": "Jiawei Zhang", "authors": "Wenqi Ren, Lin Ma, Jiawei Zhang, Jinshan Pan, Xiaochun Cao, Wei Liu,\n  Ming-Hsuan Yang", "title": "Gated Fusion Network for Single Image Dehazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient algorithm to directly restore a clear\nimage from a hazy input. The proposed algorithm hinges on an end-to-end\ntrainable neural network that consists of an encoder and a decoder. The encoder\nis exploited to capture the context of the derived input images, while the\ndecoder is employed to estimate the contribution of each input to the final\ndehazed result using the learned representations attributed to the encoder. The\nconstructed network adopts a novel fusion-based strategy which derives three\ninputs from an original hazy image by applying White Balance (WB), Contrast\nEnhancing (CE), and Gamma Correction (GC). We compute pixel-wise confidence\nmaps based on the appearance differences between these different inputs to\nblend the information of the derived inputs and preserve the regions with\npleasant visibility. The final dehazed image is yielded by gating the important\nfeatures of the derived inputs. To train the network, we introduce a\nmulti-scale approach such that the halo artifacts can be avoided. Extensive\nexperimental results on both synthetic and real-world images demonstrate that\nthe proposed algorithm performs favorably against the state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 20:33:11 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Ren", "Wenqi", ""], ["Ma", "Lin", ""], ["Zhang", "Jiawei", ""], ["Pan", "Jinshan", ""], ["Cao", "Xiaochun", ""], ["Liu", "Wei", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1804.00216", "submitter": "Mahdi Kalayeh", "authors": "Mahdi M. Kalayeh, Emrah Basaran, Muhittin Gokmen, Mustafa E. Kamasak,\n  Mubarak Shah", "title": "Human Semantic Parsing for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is a challenging task mainly due to factors such as\nbackground clutter, pose, illumination and camera point of view variations.\nThese elements hinder the process of extracting robust and discriminative\nrepresentations, hence preventing different identities from being successfully\ndistinguished. To improve the representation learning, usually, local features\nfrom human body parts are extracted. However, the common practice for such a\nprocess has been based on bounding box part detection. In this paper, we\npropose to adopt human semantic parsing which, due to its pixel-level accuracy\nand capability of modeling arbitrary contours, is naturally a better\nalternative. Our proposed SPReID integrates human semantic parsing in person\nre-identification and not only considerably outperforms its counter baseline,\nbut achieves state-of-the-art performance. We also show that by employing a\n\\textit{simple} yet effective training strategy, standard popular deep\nconvolutional architectures such as Inception-V3 and ResNet-152, with no\nmodification, while operating solely on full image, can dramatically outperform\ncurrent state-of-the-art. Our proposed methods improve state-of-the-art person\nre-identification on: Market-1501 by ~17% in mAP and ~6% in rank-1, CUHK03 by\n~4% in rank-1 and DukeMTMC-reID by ~24% in mAP and ~10% in rank-1.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 21:13:07 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kalayeh", "Mahdi M.", ""], ["Basaran", "Emrah", ""], ["Gokmen", "Muhittin", ""], ["Kamasak", "Mustafa E.", ""], ["Shah", "Mubarak", ""]]}, {"id": "1804.00227", "submitter": "Milad Mozafari", "authors": "Milad Mozafari, Mohammad Ganjtabesh, Abbas Nowzari-Dalini, Simon J.\n  Thorpe, Timoth\\'ee Masquelier", "title": "Bio-inspired digit recognition using reward-modulated\n  spike-timing-dependent plasticity in deep convolutional networks", "comments": "Pattern Recognition (2019)", "journal-ref": null, "doi": "10.1016/j.patcog.2019.05.015", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primate visual system has inspired the development of deep artificial\nneural networks, which have revolutionized the computer vision domain. Yet\nthese networks are much less energy-efficient than their biological\ncounterparts, and they are typically trained with backpropagation, which is\nextremely data-hungry. To address these limitations, we used a deep\nconvolutional spiking neural network (DCSNN) and a latency-coding scheme. We\ntrained it using a combination of spike-timing-dependent plasticity (STDP) for\nthe lower layers and reward-modulated STDP (R-STDP) for the higher ones. In\nshort, with R-STDP a correct (resp. incorrect) decision leads to STDP (resp.\nanti-STDP). This approach led to an accuracy of $97.2\\%$ on MNIST, without\nrequiring an external classifier. In addition, we demonstrated that R-STDP\nextracts features that are diagnostic for the task at hand, and discards the\nother ones, whereas STDP extracts any feature that repeats. Finally, our\napproach is biologically plausible, hardware friendly, and energy-efficient.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 23:35:12 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 09:26:35 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 19:33:54 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Mozafari", "Milad", ""], ["Ganjtabesh", "Mohammad", ""], ["Nowzari-Dalini", "Abbas", ""], ["Thorpe", "Simon J.", ""], ["Masquelier", "Timoth\u00e9e", ""]]}, {"id": "1804.00236", "submitter": "Andreas K\\\"olsch", "authors": "Andreas K\\\"olsch, Ashutosh Mishra, Saurabh Varshneya, Muhammad Zeshan\n  Afzal, Marcus Liwicki", "title": "Recognizing Challenging Handwritten Annotations with Fully Convolutional\n  Networks", "comments": null, "journal-ref": "16th International Conference on Frontiers in Handwriting\n  Recognition 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a very challenging dataset of historic German documents\nand evaluates Fully Convolutional Neural Network (FCNN) based methods to locate\nhandwritten annotations of any kind in these documents. The handwritten\nannotations can appear in form of underlines and text by using various writing\ninstruments, e.g., the use of pencils makes the data more challenging. We train\nand evaluate various end-to-end semantic segmentation approaches and report the\nresults. The task is to classify the pixels of documents into two classes:\nbackground and handwritten annotation. The best model achieves a mean\nIntersection over Union (IoU) score of 95.6% on the test documents of the\npresented dataset. We also present a comparison of different strategies used\nfor data augmentation and training on our presented dataset. For evaluation, we\nuse the Layout Analysis Evaluator for the ICDAR 2017 Competition on Layout\nAnalysis for Challenging Medieval Manuscripts.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 00:56:02 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 12:40:23 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["K\u00f6lsch", "Andreas", ""], ["Mishra", "Ashutosh", ""], ["Varshneya", "Saurabh", ""], ["Afzal", "Muhammad Zeshan", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1804.00242", "submitter": "Heng Fan", "authors": "Qin Zhou, Heng Fan, Shibao Zheng, Hang Su, Xinzhe Li, Shuang Wu,\n  Haibin Ling", "title": "Graph Correspondence Transfer for Person Re-identification", "comments": "Accepted to AAAI'18 (Oral). The code is available at\n  http://www.dabi.temple.edu/~hbling/code/gct.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a graph correspondence transfer (GCT) approach for\nperson re-identification. Unlike existing methods, the GCT model formulates\nperson re-identification as an off-line graph matching and on-line\ncorrespondence transferring problem. In specific, during training, the GCT\nmodel aims to learn off-line a set of correspondence templates from positive\ntraining pairs with various pose-pair configurations via patch-wise graph\nmatching. During testing, for each pair of test samples, we select a few\ntraining pairs with the most similar pose-pair configurations as references,\nand transfer the correspondences of these references to test pair for feature\ndistance calculation. The matching score is derived by aggregating distances\nfrom different references. For each probe image, the gallery image with the\nhighest matching score is the re-identifying result. Compared to existing\nalgorithms, our GCT can handle spatial misalignment caused by large variations\nin view angles and human poses owing to the benefits of patch-wise graph\nmatching. Extensive experiments on five benchmarks including VIPeR, Road,\nPRID450S, 3DPES and CUHK01 evidence the superior performance of GCT model over\nother state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 01:39:17 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Zhou", "Qin", ""], ["Fan", "Heng", ""], ["Zheng", "Shibao", ""], ["Su", "Hang", ""], ["Li", "Xinzhe", ""], ["Wu", "Shuang", ""], ["Ling", "Haibin", ""]]}, {"id": "1804.00248", "submitter": "Qi Chen", "authors": "Qi Chen, Weichao Qiu, Yi Zhang, Lingxi Xie, Alan Yuille", "title": "SampleAhead: Online Classifier-Sampler Communication for Learning from\n  Synthesized Data", "comments": "BMVC 2018 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art techniques of artificial intelligence, in particular deep\nlearning, are mostly data-driven. However, collecting and manually labeling a\nlarge scale dataset is both difficult and expensive. A promising alternative is\nto introduce synthesized training data, so that the dataset size can be\nsignificantly enlarged with little human labor. But, this raises an important\nproblem in active vision: given an {\\bf infinite} data space, how to\neffectively sample a {\\bf finite} subset to train a visual classifier? This\npaper presents an approach for learning from synthesized data effectively. The\nmotivation is straightforward -- increasing the probability of seeing difficult\ntraining data. We introduce a module named {\\bf SampleAhead} to formulate the\nlearning process into an online communication between a {\\em classifier} and a\n{\\em sampler}, and update them iteratively. In each round, we adjust the\nsampling distribution according to the classification results, and train the\nclassifier using the data sampled from the updated distribution. Experiments\nare performed by introducing synthesized images rendered from ShapeNet models\nto assist PASCAL3D+ classification. Our approach enjoys higher classification\naccuracy, especially in the scenario of a limited number of training samples.\nThis demonstrates its efficiency in exploring the infinite data space.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 02:12:41 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 00:42:40 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Chen", "Qi", ""], ["Qiu", "Weichao", ""], ["Zhang", "Yi", ""], ["Xie", "Lingxi", ""], ["Yuille", "Alan", ""]]}, {"id": "1804.00256", "submitter": "Jiaxin Gu", "authors": "Baochang Zhang, Jiaxin Gu, Chen Chen, Jungong Han, Xiangbo Su, Xianbin\n  Cao, Jianzhuang Liu", "title": "One-Two-One Networks for Compression Artifacts Reduction in Remote\n  Sensing", "comments": null, "journal-ref": null, "doi": "10.1016/j.isprsjprs.2018.01.003", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression artifacts reduction (CAR) is a challenging problem in the field\nof remote sensing. Most recent deep learning based methods have demonstrated\nsuperior performance over the previous hand-crafted methods. In this paper, we\npropose an end-to-end one-two-one (OTO) network, to combine different deep\nmodels, i.e., summation and difference models, to solve the CAR problem.\nParticularly, the difference model motivated by the Laplacian pyramid is\ndesigned to obtain the high frequency information, while the summation model\naggregates the low frequency information. We provide an in-depth investigation\ninto our OTO architecture based on the Taylor expansion, which shows that these\ntwo kinds of information can be fused in a nonlinear scheme to gain more\ncapacity of handling complicated image compression artifacts, especially the\nblocking effect in compression. Extensive experiments are conducted to\ndemonstrate the superior performance of the OTO networks, as compared to the\nstate-of-the-arts on remote sensing datasets and other benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 04:44:13 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Zhang", "Baochang", ""], ["Gu", "Jiaxin", ""], ["Chen", "Chen", ""], ["Han", "Jungong", ""], ["Su", "Xiangbo", ""], ["Cao", "Xianbin", ""], ["Liu", "Jianzhuang", ""]]}, {"id": "1804.00257", "submitter": "Quang-Hieu Pham", "authors": "Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, Sai-Kit Yeung", "title": "Real-time Progressive 3D Semantic Segmentation for Indoor Scene", "comments": "WACV 2019. More information at https://pqhieu.github.io/wacv19.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The widespread adoption of autonomous systems such as drones and assistant\nrobots has created a need for real-time high-quality semantic scene\nsegmentation. In this paper, we propose an efficient yet robust technique for\non-the-fly dense reconstruction and semantic segmentation of 3D indoor scenes.\nTo guarantee (near) real-time performance, our method is built atop an\nefficient super-voxel clustering method and a conditional random field with\nhigher-order constraints from structural and object cues, enabling progressive\ndense semantic segmentation without any precomputation. We extensively evaluate\nour method on different indoor scenes including kitchens, offices, and bedrooms\nin the SceneNN and ScanNet datasets and show that our technique consistently\nproduces state-of-the-art segmentation results in both qualitative and\nquantitative experiments.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 05:09:08 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 06:12:34 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 16:13:47 GMT"}, {"version": "v4", "created": "Mon, 1 Apr 2019 11:22:50 GMT"}, {"version": "v5", "created": "Fri, 5 Apr 2019 14:09:02 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Pham", "Quang-Hieu", ""], ["Hua", "Binh-Son", ""], ["Nguyen", "Duc Thanh", ""], ["Yeung", "Sai-Kit", ""]]}, {"id": "1804.00292", "submitter": "Ronald Kemker", "authors": "Ronald Kemker and Utsav B. Gewali and Christopher Kanan", "title": "EarthMapper: A Tool Box for the Semantic Segmentation of Remote Sensing\n  Imagery", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning continues to push state-of-the-art performance for the semantic\nsegmentation of color (i.e., RGB) imagery; however, the lack of annotated data\nfor many remote sensing sensors (i.e. hyperspectral imagery (HSI)) prevents\nresearchers from taking advantage of this recent success. Since generating\nsensor specific datasets is time intensive and cost prohibitive, remote sensing\nresearchers have embraced deep unsupervised feature extraction. Although these\nmethods have pushed state-of-the-art performance on current HSI benchmarks,\nmany of these tools are not readily accessible to many researchers. In this\nletter, we introduce a software pipeline, which we call EarthMapper, for the\nsemantic segmentation of non-RGB remote sensing imagery. It includes\nself-taught spatial-spectral feature extraction, various standard and deep\nlearning classifiers, and undirected graphical models for post-processing. We\nevaluated EarthMapper on the Indian Pines and Pavia University datasets and\nhave released this code for public use.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 12:44:20 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kemker", "Ronald", ""], ["Gewali", "Utsav B.", ""], ["Kanan", "Christopher", ""]]}, {"id": "1804.00298", "submitter": "Badri Narayana Patro", "authors": "Badri Patro, Vinay P. Namboodiri", "title": "Differential Attention for Visual Question Answering", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we aim to answer questions based on images when provided with a\ndataset of question-answer pairs for a number of images during training. A\nnumber of methods have focused on solving this problem by using image based\nattention. This is done by focusing on a specific part of the image while\nanswering the question. Humans also do so when solving this problem. However,\nthe regions that the previous systems focus on are not correlated with the\nregions that humans focus on. The accuracy is limited due to this drawback. In\nthis paper, we propose to solve this problem by using an exemplar based method.\nWe obtain one or more supporting and opposing exemplars to obtain a\ndifferential attention region. This differential attention is closer to human\nattention than other image based attention methods. It also helps in obtaining\nimproved accuracy when answering questions. The method is evaluated on\nchallenging benchmark datasets. We perform better than other image based\nattention methods and are competitive with other state of the art methods that\nfocus on both image and questions.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 13:52:55 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 06:30:19 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Patro", "Badri", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1804.00304", "submitter": "Karen L\\'opez-Linares Rom\\'an", "authors": "Karen L\\'opez-Linares, Nerea Aranjuelo, Luis Kabongo, Gregory Maclair,\n  Nerea Lete, Mario Ceresa, Ainhoa Garc\\'ia-Familiar, Iv\\'an Mac\\'ia, Miguel A.\n  Gonz\\'alez Ballester", "title": "Fully automatic detection and segmentation of abdominal aortic thrombus\n  in post-operative CTA images using deep convolutional neural networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2018.03.010", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computerized Tomography Angiography (CTA) based follow-up of Abdominal Aortic\nAneurysms (AAA) treated with Endovascular Aneurysm Repair (EVAR) is essential\nto evaluate the progress of the patient and detect complications. In this\ncontext, accurate quantification of post-operative thrombus volume is required.\nHowever, a proper evaluation is hindered by the lack of automatic, robust and\nreproducible thrombus segmentation algorithms. We propose a new fully automatic\napproach based on Deep Convolutional Neural Networks (DCNN) for robust and\nreproducible thrombus region of interest detection and subsequent fine thrombus\nsegmentation. The DetecNet detection network is adapted to perform region of\ninterest extraction from a complete CTA and a new segmentation network\narchitecture, based on Fully Convolutional Networks and a Holistically-Nested\nEdge Detection Network, is presented. These networks are trained, validated and\ntested in 13 post-operative CTA volumes of different patients using a 4-fold\ncross-validation approach to provide more robustness to the results. Our\npipeline achieves a Dice score of more than 82% for post-operative thrombus\nsegmentation and provides a mean relative volume difference between ground\ntruth and automatic segmentation that lays within the experienced human\nobserver variance without the need of human intervention in most common cases.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 15:26:32 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["L\u00f3pez-Linares", "Karen", ""], ["Aranjuelo", "Nerea", ""], ["Kabongo", "Luis", ""], ["Maclair", "Gregory", ""], ["Lete", "Nerea", ""], ["Ceresa", "Mario", ""], ["Garc\u00eda-Familiar", "Ainhoa", ""], ["Mac\u00eda", "Iv\u00e1n", ""], ["Ballester", "Miguel A. Gonz\u00e1lez", ""]]}, {"id": "1804.00307", "submitter": "Xu Liu", "authors": "Xu Liu, Steven W. Chen, Shreyas Aditya, Nivedha Sivakumar, Sandeep\n  Dcunha, Chao Qu, Camillo J. Taylor, Jnaneshwar Das, and Vijay Kumar", "title": "Robust Fruit Counting: Combining Deep Learning, Tracking, and Structure\n  from Motion", "comments": "Accepted in IROS 2018 (2018 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel fruit counting pipeline that combines deep segmentation,\nframe to frame tracking, and 3D localization to accurately count visible fruits\nacross a sequence of images. Our pipeline works on image streams from a\nmonocular camera, both in natural light, as well as with controlled\nillumination at night. We first train a Fully Convolutional Network (FCN) and\nsegment video frame images into fruit and non-fruit pixels. We then track\nfruits across frames using the Hungarian Algorithm where the objective cost is\ndetermined from a Kalman Filter corrected Kanade-Lucas-Tomasi (KLT) Tracker. In\norder to correct the estimated count from tracking process, we combine tracking\nresults with a Structure from Motion (SfM) algorithm to calculate relative 3D\nlocations and size estimates to reject outliers and double counted fruit\ntracks. We evaluate our algorithm by comparing with ground-truth\nhuman-annotated visual counts. Our results demonstrate that our pipeline is\nable to accurately and reliably count fruits across image sequences, and the\ncorrection step can significantly improve the counting accuracy and robustness.\nAlthough discussed in the context of fruit counting, our work can extend to\ndetection, tracking, and counting of a variety of other stationary features of\ninterest such as leaf-spots, wilt, and blossom.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 15:44:58 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 04:35:07 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Liu", "Xu", ""], ["Chen", "Steven W.", ""], ["Aditya", "Shreyas", ""], ["Sivakumar", "Nivedha", ""], ["Dcunha", "Sandeep", ""], ["Qu", "Chao", ""], ["Taylor", "Camillo J.", ""], ["Das", "Jnaneshwar", ""], ["Kumar", "Vijay", ""]]}, {"id": "1804.00326", "submitter": "Arsha Nagrani", "authors": "Arsha Nagrani, Samuel Albanie and Andrew Zisserman", "title": "Seeing Voices and Hearing Faces: Cross-modal biometric matching", "comments": "To appear in: IEEE Computer Vision and Pattern Recognition (CVPR),\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a seemingly impossible task: given only an audio clip of someone\nspeaking, decide which of two face images is the speaker. In this paper we\nstudy this, and a number of related cross-modal tasks, aimed at answering the\nquestion: how much can we infer from the voice about the face and vice versa?\nWe study this task \"in the wild\", employing the datasets that are now publicly\navailable for face recognition from static images (VGGFace) and speaker\nidentification from audio (VoxCeleb). These provide training and testing\nscenarios for both static and dynamic testing of cross-modal matching. We make\nthe following contributions: (i) we introduce CNN architectures for both binary\nand multi-way cross-modal face and audio matching, (ii) we compare dynamic\ntesting (where video information is available, but the audio is not from the\nsame video) with static testing (where only a single still image is available),\nand (iii) we use human testing as a baseline to calibrate the difficulty of the\ntask. We show that a CNN can indeed be trained to solve this task in both the\nstatic and dynamic scenarios, and is even well above chance on 10-way\nclassification of the face given the voice. The CNN matches human performance\non easy examples (e.g. different gender across faces) but exceeds human\nperformance on more challenging examples (e.g. faces with the same gender, age\nand nationality).\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 18:02:41 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 10:55:59 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Nagrani", "Arsha", ""], ["Albanie", "Samuel", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1804.00347", "submitter": "Yedid Hoshen", "authors": "Yedid Hoshen, Lior Wolf", "title": "Unsupervised Correlation Analysis", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linking between two data sources is a basic building block in numerous\ncomputer vision problems. In this paper, we set to answer a fundamental\ncognitive question: are prior correspondences necessary for linking between\ndifferent domains?\n  One of the most popular methods for linking between domains is Canonical\nCorrelation Analysis (CCA). All current CCA algorithms require correspondences\nbetween the views. We introduce a new method Unsupervised Correlation Analysis\n(UCA), which requires no prior correspondences between the two domains. The\ncorrelation maximization term in CCA is replaced by a combination of a\nreconstruction term (similar to autoencoders), full cycle loss, orthogonality\nand multiple domain confusion terms. Due to lack of supervision, the\noptimization leads to multiple alternative solutions with similar scores and we\ntherefore introduce a consensus-based mechanism that is often able to recover\nthe desired solution. Remarkably, this suffices in order to link remote domains\nsuch as text and images. We also present results on well accepted CCA\nbenchmarks, showing that performance far exceeds other unsupervised baselines,\nand approaches supervised performance in some cases.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 21:14:06 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Hoshen", "Yedid", ""], ["Wolf", "Lior", ""]]}, {"id": "1804.00376", "submitter": "Zhenwei He", "authors": "Zhenwei He, Lei Zhang, Wei Jia", "title": "End-to-End Detection and Re-identification Integrated Net for Person\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a pedestrian detection and re-identification (re-id)\nintegration net (I-Net) in an end-to-end learning framework. The I-Net is used\nin real-world video surveillance scenarios, where the target person needs to be\nsearched in the whole scene videos, while the annotations of pedestrian\nbounding boxes are unavailable. By comparing to the OIM which is a work for\njoint detection and re-id, we have three distinct contributions. First, we\nintroduce a Siamese architecture of I-Net instead of 1 stream, such that a\nverification task can be implemented. Second, we propose a novel on-line\npairing loss (OLP) and hard example priority softmax loss (HEP), such that only\nthe hard negatives are posed much attention in loss computation. Third, an\non-line dictionary for negative samples storage is designed in I-Net without\nrecording the positive samples. We show our result on person search datasets,\nthe gap between detection and re-identification is narrowed. The superior\nperformance can be achieved.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 02:34:35 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["He", "Zhenwei", ""], ["Zhang", "Lei", ""], ["Jia", "Wei", ""]]}, {"id": "1804.00382", "submitter": "Wonsik Kim", "authors": "Wonsik Kim, Bhavya Goyal, Kunal Chawla, Jungmin Lee, Keunjoo Kwon", "title": "Attention-based Ensemble for Deep Metric Learning", "comments": "ECCV 2018 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning aims to learn an embedding function, modeled as deep\nneural network. This embedding function usually puts semantically similar\nimages close while dissimilar images far from each other in the learned\nembedding space. Recently, ensemble has been applied to deep metric learning to\nyield state-of-the-art results. As one important aspect of ensemble, the\nlearners should be diverse in their feature embeddings. To this end, we propose\nan attention-based ensemble, which uses multiple attention masks, so that each\nlearner can attend to different parts of the object. We also propose a\ndivergence loss, which encourages diversity among the learners. The proposed\nmethod is applied to the standard benchmarks of deep metric learning and\nexperimental results show that it outperforms the state-of-the-art methods by a\nsignificant margin on image retrieval tasks.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 03:23:06 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 09:12:37 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Kim", "Wonsik", ""], ["Goyal", "Bhavya", ""], ["Chawla", "Kunal", ""], ["Lee", "Jungmin", ""], ["Kwon", "Keunjoo", ""]]}, {"id": "1804.00389", "submitter": "Yule Li", "authors": "Yule Li, Jianping Shi, Dahua Lin", "title": "Low-Latency Video Semantic Segmentation", "comments": "Accepted by CVPR 2018 as Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen remarkable progress in semantic segmentation. Yet, it\nremains a challenging task to apply segmentation techniques to video-based\napplications. Specifically, the high throughput of video streams, the sheer\ncost of running fully convolutional networks, together with the low-latency\nrequirements in many real-world applications, e.g. autonomous driving, present\na significant challenge to the design of the video segmentation framework. To\ntackle this combined challenge, we develop a framework for video semantic\nsegmentation, which incorporates two novel components: (1) a feature\npropagation module that adaptively fuses features over time via spatially\nvariant convolution, thus reducing the cost of per-frame computation; and (2)\nan adaptive scheduler that dynamically allocate computation based on accuracy\nprediction. Both components work together to ensure low latency while\nmaintaining high segmentation quality. On both Cityscapes and CamVid, the\nproposed framework obtained competitive performance compared to the state of\nthe art, while substantially reducing the latency, from 360 ms to 119 ms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 03:47:51 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Li", "Yule", ""], ["Shi", "Jianping", ""], ["Lin", "Dahua", ""]]}, {"id": "1804.00392", "submitter": "Lingxi Xie", "authors": "Yingda Xia, Lingxi Xie, Fengze Liu, Zhuotun Zhu, Elliot K. Fishman,\n  Alan L. Yuille", "title": "Bridging the Gap Between 2D and 3D Organ Segmentation with Volumetric\n  Fusion Net", "comments": "8 pages, 2 figures, accepted to MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a debate on whether to use 2D or 3D deep neural networks for\nvolumetric organ segmentation. Both 2D and 3D models have their advantages and\ndisadvantages. In this paper, we present an alternative framework, which trains\n2D networks on different viewpoints for segmentation, and builds a 3D\nVolumetric Fusion Net (VFN) to fuse the 2D segmentation results. VFN is\nrelatively shallow and contains much fewer parameters than most 3D networks,\nmaking our framework more efficient at integrating 3D information for\nsegmentation. We train and test the segmentation and fusion modules\nindividually, and propose a novel strategy, named cross-cross-augmentation, to\nmake full use of the limited training data. We evaluate our framework on\nseveral challenging abdominal organs, and verify its superiority in\nsegmentation accuracy and stability over existing 2D and 3D approaches.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 03:57:14 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 15:46:44 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Xia", "Yingda", ""], ["Xie", "Lingxi", ""], ["Liu", "Fengze", ""], ["Zhu", "Zhuotun", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1804.00393", "submitter": "Narita Pandhe Ms", "authors": "Narita Pandhe, Balazs Rada, Shannon Quinn", "title": "Generative Spatiotemporal Modeling Of Neutrophil Behavior", "comments": "4 pages, Accepted to 2018 IEEE International Symposium on Biomedical\n  Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell motion and appearance have a strong correlation with cell cycle and\ndisease progression. Many contemporary efforts in machine learning utilize\nspatio-temporal models to predict a cell's physical state and, consequently,\nthe advancement of disease. Alternatively, generative models learn the\nunderlying distribution of the data, creating holistic representations that can\nbe used in learning. In this work, we propose an aggregate model that combine\nGenerative Adversarial Networks (GANs) and Autoregressive (AR) models to\npredict cell motion and appearance in human neutrophils imaged by differential\ninterference contrast (DIC) microscopy. We bifurcate the task of learning cell\nstatistics by leveraging GANs for the spatial component and AR models for the\ntemporal component. The aggregate model learned results offer a promising\ncomputational environment for studying changes in organellar shape, quantity,\nand spatial distribution over large sequences.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 04:07:11 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Pandhe", "Narita", ""], ["Rada", "Balazs", ""], ["Quinn", "Shannon", ""]]}, {"id": "1804.00410", "submitter": "Wen-Cheng Chen", "authors": "Wen-Cheng Chen, Chien-Wen Chen, Min-Chun Hu", "title": "SyncGAN: Synchronize the Latent Space of Cross-modal Generative\n  Adversarial Networks", "comments": "9 pages, Part of this work is accepted by IEEE International\n  Conference on Multimedia Expo 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial network (GAN) has achieved impressive success on\ncross-domain generation, but it faces difficulty in cross-modal generation due\nto the lack of a common distribution between heterogeneous data. Most existing\nmethods of conditional based cross-modal GANs adopt the strategy of\none-directional transfer and have achieved preliminary success on text-to-image\ntransfer. Instead of learning the transfer between different modalities, we aim\nto learn a synchronous latent space representing the cross-modal common\nconcept. A novel network component named synchronizer is proposed in this work\nto judge whether the paired data is synchronous/corresponding or not, which can\nconstrain the latent space of generators in the GANs. Our GAN model, named as\nSyncGAN, can successfully generate synchronous data (e.g., a pair of image and\nsound) from identical random noise. For transforming data from one modality to\nanother, we recover the latent code by inverting the mappings of a generator\nand use it to generate data of different modality. In addition, the proposed\nmodel can achieve semi-supervised learning, which makes our model more flexible\nfor practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 06:27:50 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Chen", "Wen-Cheng", ""], ["Chen", "Chien-Wen", ""], ["Hu", "Min-Chun", ""]]}, {"id": "1804.00413", "submitter": "Lijie Fan", "authors": "Lijie Fan, Wenbing Huang, Chuang Gan, Stefano Ermon, Boqing Gong,\n  Junzhou Huang", "title": "End-to-End Learning of Motion Representation for Video Understanding", "comments": "CVPR 2018 spotlight. The first two authors contributed equally to\n  this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of end-to-end learned representations,\nhand-crafted optical flow features are still widely used in video analysis\ntasks. To fill this gap, we propose TVNet, a novel end-to-end trainable neural\nnetwork, to learn optical-flow-like features from data. TVNet subsumes a\nspecific optical flow solver, the TV-L1 method, and is initialized by unfolding\nits optimization iterations as neural layers. TVNet can therefore be used\ndirectly without any extra learning. Moreover, it can be naturally concatenated\nwith other task-specific networks to formulate an end-to-end architecture, thus\nmaking our method more efficient than current multi-stage approaches by\navoiding the need to pre-compute and store features on disk. Finally, the\nparameters of the TVNet can be further fine-tuned by end-to-end training. This\nenables TVNet to learn richer and task-specific patterns beyond exact optical\nflow. Extensive experiments on two action recognition benchmarks verify the\neffectiveness of the proposed approach. Our TVNet achieves better accuracies\nthan all compared methods, while being competitive with the fastest counterpart\nin terms of features extraction time.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 06:40:37 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Fan", "Lijie", ""], ["Huang", "Wenbing", ""], ["Gan", "Chuang", ""], ["Ermon", "Stefano", ""], ["Gong", "Boqing", ""], ["Huang", "Junzhou", ""]]}, {"id": "1804.00428", "submitter": "Hao Wang", "authors": "Hao Wang, Qilong Wang, Mingqi Gao, Peihua Li, Wangmeng Zuo", "title": "Multi-scale Location-aware Kernel Representation for Object Detection", "comments": "Accepted by CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although Faster R-CNN and its variants have shown promising performance in\nobject detection, they only exploit simple first-order representation of object\nproposals for final classification and regression. Recent classification\nmethods demonstrate that the integration of high-order statistics into deep\nconvolutional neural networks can achieve impressive improvement, but their\ngoal is to model whole images by discarding location information so that they\ncannot be directly adopted to object detection. In this paper, we make an\nattempt to exploit high-order statistics in object detection, aiming at\ngenerating more discriminative representations for proposals to enhance the\nperformance of detectors. To this end, we propose a novel Multi-scale\nLocation-aware Kernel Representation (MLKP) to capture high-order statistics of\ndeep features in proposals. Our MLKP can be efficiently computed on a modified\nmulti-scale feature map using a low-dimensional polynomial kernel\napproximation.Moreover, different from existing orderless global\nrepresentations based on high-order statistics, our proposed MLKP is location\nretentive and sensitive so that it can be flexibly adopted to object detection.\nThrough integrating into Faster R-CNN schema, the proposed MLKP achieves very\ncompetitive performance with state-of-the-art methods, and improves Faster\nR-CNN by 4.9% (mAP), 4.7% (mAP) and 5.0% (AP at IOU=[0.5:0.05:0.95]) on PASCAL\nVOC 2007, VOC 2012 and MS COCO benchmarks, respectively. Code is available at:\nhttps://github.com/Hwang64/MLKP.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 08:27:40 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Wang", "Hao", ""], ["Wang", "Qilong", ""], ["Gao", "Mingqi", ""], ["Li", "Peihua", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1804.00429", "submitter": "Mehmet Guzel", "authors": "Abdullah Asim Yilmaz, Mehmet Serdar Guzel, Iman Askerbeyli, Erkan\n  Bostanci", "title": "A Vehicle Detection Approach using Deep Learning Methodologies", "comments": "7 pages, 8 Figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to successfully train our vehicle detector using\nR-CNN, Faster R-CNN deep learning methods on a sample vehicle data sets and to\noptimize the success rate of the trained detector by providing efficient\nresults for vehicle detection by testing the trained vehicle detector on the\ntest data. The working method consists of six main stages. These are\nrespectively; loading the data set, the design of the convolutional neural\nnetwork, configuration of training options, training of the Faster R-CNN object\ndetector and evaluation of trained detector. In addition, in the scope of the\nstudy, Faster R-CNN, R-CNN deep learning methods were mentioned and\nexperimental analysis comparisons were made with the results obtained from\nvehicle detection.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 08:34:38 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Yilmaz", "Abdullah Asim", ""], ["Guzel", "Mehmet Serdar", ""], ["Askerbeyli", "Iman", ""], ["Bostanci", "Erkan", ""]]}, {"id": "1804.00432", "submitter": "Jong Chul Ye", "authors": "Dongwook Lee, Jaejun Yoo, Sungho Tak and Jong Chul Ye", "title": "Deep Residual Learning for Accelerated MRI using Magnitude and Phase\n  Networks", "comments": "This paper will appear in IEEE Trans. Biomedical Engineering, Special\n  Section on Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated magnetic resonance (MR) scan acquisition with compressed sensing\n(CS) and parallel imaging is a powerful method to reduce MR imaging scan time.\nHowever, many reconstruction algorithms have high computational costs. To\naddress this, we investigate deep residual learning networks to remove aliasing\nartifacts from artifact corrupted images. The proposed deep residual learning\nnetworks are composed of magnitude and phase networks that are separately\ntrained. If both phase and magnitude information are available, the proposed\nalgorithm can work as an iterative k-space interpolation algorithm using\nframelet representation. When only magnitude data is available, the proposed\napproach works as an image domain post-processing algorithm. Even with strong\ncoherent aliasing artifacts, the proposed network successfully learned and\nremoved the aliasing artifacts, whereas current parallel and CS reconstruction\nmethods were unable to remove these artifacts. Comparisons using single and\nmultiple coil show that the proposed residual network provides good\nreconstruction results with orders of magnitude faster computational time than\nexisting compressed sensing methods. The proposed deep learning framework may\nhave a great potential for accelerated MR reconstruction by generating accurate\nresults immediately.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 09:08:02 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Lee", "Dongwook", ""], ["Yoo", "Jaejun", ""], ["Tak", "Sungho", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1804.00433", "submitter": "Xiaowei Hu", "authors": "Xiaowei Hu, Xuemiao Xu, Yongjie Xiao, Hao Chen, Shengfeng He, Jing Qin\n  and Pheng-Ann Heng", "title": "SINet: A Scale-insensitive Convolutional Neural Network for Fast Vehicle\n  Detection", "comments": "Accepted by IEEE Transactions on Intelligent Transportation Systems\n  (T-ITS)", "journal-ref": "IEEE Transactions on Intelligent Transportation Systems, vol. 20,\n  no. 3, pp. 1010-1019, 2019", "doi": "10.1109/TITS.2018.2838132", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based vehicle detection approaches achieve incredible success in\nrecent years with the development of deep convolutional neural network (CNN).\nHowever, existing CNN based algorithms suffer from the problem that the\nconvolutional features are scale-sensitive in object detection task but it is\ncommon that traffic images and videos contain vehicles with a large variance of\nscales. In this paper, we delve into the source of scale sensitivity, and\nreveal two key issues: 1) existing RoI pooling destroys the structure of small\nscale objects, 2) the large intra-class distance for a large variance of scales\nexceeds the representation capability of a single network. Based on these\nfindings, we present a scale-insensitive convolutional neural network (SINet)\nfor fast detecting vehicles with a large variance of scales. First, we present\na context-aware RoI pooling to maintain the contextual information and original\nstructure of small scale objects. Second, we present a multi-branch decision\nnetwork to minimize the intra-class distance of features. These lightweight\ntechniques bring zero extra time complexity but prominent detection accuracy\nimprovement. The proposed techniques can be equipped with any deep network\narchitectures and keep them trained end-to-end. Our SINet achieves\nstate-of-the-art performance in terms of accuracy and speed (up to 37 FPS) on\nthe KITTI benchmark and a new highway dataset, which contains a large variance\nof scales and extremely small objects.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 09:27:09 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 09:05:29 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Hu", "Xiaowei", ""], ["Xu", "Xuemiao", ""], ["Xiao", "Yongjie", ""], ["Chen", "Hao", ""], ["He", "Shengfeng", ""], ["Qin", "Jing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1804.00435", "submitter": "C\\'eline Craye", "authors": "Celine Craye, Timothee Lesort, David Filliat and Jean-Francois Goudou", "title": "Exploring to learn visual saliency: The RL-IAC approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of object localization and recognition on autonomous mobile\nrobots is still an active topic. In this context, we tackle the problem of\nlearning a model of visual saliency directly on a robot. This model, learned\nand improved on-the-fly during the robot's exploration provides an efficient\ntool for localizing relevant objects within their environment. The proposed\napproach includes two intertwined components. On the one hand, we describe a\nmethod for learning and incrementally updating a model of visual saliency from\na depth-based object detector. This model of saliency can also be exploited to\nproduce bounding box proposals around objects of interest. On the other hand,\nwe investigate an autonomous exploration technique to efficiently learn such a\nsaliency model. The proposed exploration, called Reinforcement\nLearning-Intelligent Adaptive Curiosity (RL-IAC) is able to drive the robot's\nexploration so that samples selected by the robot are likely to improve the\ncurrent model of saliency. We then demonstrate that such a saliency model\nlearned directly on a robot outperforms several state-of-the-art saliency\ntechniques, and that RL-IAC can drastically decrease the required time for\nlearning a reliable saliency model.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 09:39:22 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Craye", "Celine", ""], ["Lesort", "Timothee", ""], ["Filliat", "David", ""], ["Goudou", "Jean-Francois", ""]]}, {"id": "1804.00448", "submitter": "Luiz Gustavo Hafemann", "authors": "Luiz G. Hafemann, Robert Sabourin, Luiz S. Oliveira", "title": "Fixed-sized representation learning from Offline Handwritten Signatures\n  of different sizes", "comments": "This is a pre-print of an article published in the International\n  Journal on Document Analysis and Recognition", "journal-ref": null, "doi": "10.1007/s10032-018-0301-6", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for learning feature representations for Offline Handwritten\nSignature Verification have been successfully proposed in recent literature,\nusing Deep Convolutional Neural Networks to learn representations from\nsignature pixels. Such methods reported large performance improvements compared\nto handcrafted feature extractors. However, they also introduced an important\nconstraint: the inputs to the neural networks must have a fixed size, while\nsignatures vary significantly in size between different users. In this paper we\npropose addressing this issue by learning a fixed-sized representation from\nvariable-sized signatures by modifying the network architecture, using Spatial\nPyramid Pooling. We also investigate the impact of the resolution of the images\nused for training, and the impact of adapting (fine-tuning) the representations\nto new operating conditions (different acquisition protocols, such as writing\ninstruments and scan resolution). On the GPDS dataset, we achieve results\ncomparable with the state-of-the-art, while removing the constraint of having a\nmaximum size for the signatures to be processed. We also show that using higher\nresolutions (300 or 600dpi) can improve performance when skilled forgeries from\na subset of users are available for feature learning, but lower resolutions\n(around 100dpi) can be used if only genuine signatures are used. Lastly, we\nshow that fine-tuning can improve performance when the operating conditions\nchange.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 11:07:01 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 17:06:46 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Hafemann", "Luiz G.", ""], ["Sabourin", "Robert", ""], ["Oliveira", "Luiz S.", ""]]}, {"id": "1804.00490", "submitter": "Masayuki Tanaka", "authors": "Masayuki Tanaka", "title": "Learnable Image Encryption", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The network-based machine learning algorithm is very powerful tools. However,\nit requires huge training dataset. Researchers often meet privacy issues when\nthey collect image dataset especially for surveillance applications. A\nlearnable image encryption scheme is introduced. The key idea of this scheme is\nto encrypt images, so that human cannot understand images but the network can\nbe train with encrypted images. This scheme allows us to train the network\nwithout the privacy issues. In this paper, a simple learnable image encryption\nalgorithm is proposed. Then, the proposed algorithm is validated with cifar\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 05:44:53 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Tanaka", "Masayuki", ""]]}, {"id": "1804.00492", "submitter": "Shruti Mittal", "authors": "Shruti Mittal and Dattaraj Rao", "title": "Regional Priority Based Anomaly Detection using Autoencoders", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": "2018TDS0001", "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent times, autoencoders, besides being used for compression, have\nbeen proven quite useful even for regenerating similar images or help in image\ndenoising. They have also been explored for anomaly detection in a few cases.\nHowever, due to location invariance property of convolutional neural network,\nautoencoders tend to learn from or search for learned features in the complete\nimage. This creates issues when all the items in the image are not equally\nimportant and their location matters. For such cases, a semi supervised\nsolution - regional priority based autoencoder (RPAE) has been proposed. In\nthis model, similar to object detection models, a region proposal network\nidentifies the relevant areas in the images as belonging to one of the\npredefined categories and then those bounding boxes are fed into appropriate\ndecoder based on the category they belong to. Finally, the error scores from\nall the decoders are combined based on their importance to provide total\nreconstruction error.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 13:49:01 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Mittal", "Shruti", ""], ["Rao", "Dattaraj", ""]]}, {"id": "1804.00494", "submitter": "Shervan Fekri-Ershad", "authors": "Shervan Fekri-Ershad", "title": "A Review on Image Texture Analysis Methods", "comments": "in Persian", "journal-ref": "International Online Journal of Image Processing and Pattern\n  Recognition, Vol. 1, No. 1, pp. 1-63, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Texture classification is an active topic in image processing which plays an\nimportant role in many applications such as image retrieval, inspection\nsystems, face recognition, medical image processing, etc. There are many\napproaches extracting texture features in gray-level images such as local\nbinary patterns, gray level co-occurrence matrices, statistical features,\nskeleton, scale invariant feature transform, etc. The texture analysis methods\ncan be categorized in 4 groups titles: statistical methods, structural methods,\nfilter-based and model based approaches. In many related researches, authors\nhave tried to extract color and texture features jointly. In this respect,\ncombined methods are considered as efficient image analysis descriptors. Mostly\nimportant challenges in image texture analysis are rotation sensitivity, gray\nscale variations, noise sensitivity, illumination and brightness conditions,\netc. In this paper, we review most efficient and state-of-the-art image texture\nanalysis methods. Also, some texture classification approaches are survived.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 23:04:12 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Fekri-Ershad", "Shervan", ""]]}, {"id": "1804.00495", "submitter": "Macheng Shen", "authors": "Macheng Shen, Golnaz Habibi, Jonathan P. How", "title": "Transferable Pedestrian Motion Prediction Models at Intersections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One desirable capability of autonomous cars is to accurately predict the\npedestrian motion near intersections for safe and efficient trajectory\nplanning. We are interested in developing transfer learning algorithms that can\nbe trained on the pedestrian trajectories collected at one intersection and yet\nstill provide accurate predictions of the trajectories at another, previously\nunseen intersection. We first discussed the feature selection for transferable\npedestrian motion models in general. Following this discussion, we developed\none transferable pedestrian motion prediction algorithm based on Inverse\nReinforcement Learning (IRL) that infers pedestrian intentions and predicts\nfuture trajectories based on observed trajectory. We evaluated our algorithm on\na dataset collected at two intersections, trained at one intersection and\ntested at the other intersection. We used the accuracy of augmented\nsemi-nonnegative sparse coding (ASNSC), trained and tested at the same\nintersection as a baseline. The result shows that the proposed algorithm\nimproves the baseline accuracy by 40% in the non-transfer task, and 16% in the\ntransfer task.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 23:58:19 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 23:51:54 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Shen", "Macheng", ""], ["Habibi", "Golnaz", ""], ["How", "Jonathan P.", ""]]}, {"id": "1804.00497", "submitter": "Alexander Wong", "authors": "Alexander Wong, Mohammad Javad Shafiee, and Michael St. Jules", "title": "MicronNet: A Highly Compact Deep Convolutional Neural Network\n  Architecture for Real-time Embedded Traffic Sign Classification", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic sign recognition is a very important computer vision task for a\nnumber of real-world applications such as intelligent transportation\nsurveillance and analysis. While deep neural networks have been demonstrated in\nrecent years to provide state-of-the-art performance traffic sign recognition,\na key challenge for enabling the widespread deployment of deep neural networks\nfor embedded traffic sign recognition is the high computational and memory\nrequirements of such networks. As a consequence, there are significant benefits\nin investigating compact deep neural network architectures for traffic sign\nrecognition that are better suited for embedded devices. In this paper, we\nintroduce MicronNet, a highly compact deep convolutional neural network for\nreal-time embedded traffic sign recognition designed based on macroarchitecture\ndesign principles (e.g., spectral macroarchitecture augmentation, parameter\nprecision optimization, etc.) as well as numerical microarchitecture\noptimization strategies. The resulting overall architecture of MicronNet is\nthus designed with as few parameters and computations as possible while\nmaintaining recognition performance, leading to optimized information density\nof the proposed network. The resulting MicronNet possesses a model size of just\n~1MB and ~510,000 parameters (~27x fewer parameters than state-of-the-art)\nwhile still achieving a human performance level top-1 accuracy of 98.9% on the\nGerman traffic sign recognition benchmark. Furthermore, MicronNet requires just\n~10 million multiply-accumulate operations to perform inference, and has a\ntime-to-compute of just 32.19 ms on a Cortex-A53 high efficiency processor.\nThese experimental results show that highly compact, optimized deep neural\nnetwork architectures can be designed for real-time traffic sign recognition\nthat are well-suited for embedded scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 01:32:59 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2018 00:43:55 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 16:11:25 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Wong", "Alexander", ""], ["Shafiee", "Mohammad Javad", ""], ["Jules", "Michael St.", ""]]}, {"id": "1804.00498", "submitter": "Xin Zhang", "authors": "Xin Zhang, Bingfang Wu, Liang Zhu, Fuyou Tian, Miao Zhang and Yuanzeng", "title": "Land use mapping in the Three Gorges Reservoir Area based on semantic\n  segmentation deep learning method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Three Gorges Dam, a massive cross-century project spans the Yangtze River\nby the town of Sandouping, located in Yichang, Hubei province, China, was built\nto provide great power, improve the River shipping, control floods in the upper\nreaches of the Yangtze River, and increase the dry season flow in the middle\nand lower reaches of the Yangtze River. Benefits are enormous and\ncomprehensive. However, the social and environmental impacts are also immense\nand far-reaching to its surrounding areas. Mapping land use /land cover changed\n(LUCC) is critical for tracking the impacts. Remote sensing has been proved to\nbe an effective way to map and monitor land use change in real time and in\nlarge areas such as the Three Gorges Reservoir Area(TGRA) by using pixel based\nor oriented based classifier in different resolution. In this paper, we first\ntest the state of the art semantic segmentation deep learning classifiers for\nLUCC mapping with 7 categories in the TGRA area with rapideye 5m resolution\ndata. The topographic information was also added for better accuracy in\nmountain area. By compared with the pixel-based classifier, the semantic\nsegmentation deep learning method has better accuracy and robustness at 5m\nresolution level.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 13:30:49 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Zhang", "Xin", ""], ["Wu", "Bingfang", ""], ["Zhu", "Liang", ""], ["Tian", "Fuyou", ""], ["Zhang", "Miao", ""], ["Yuanzeng", "", ""]]}, {"id": "1804.00499", "submitter": "Hossein Hosseini", "authors": "Hossein Hosseini and Radha Poovendran", "title": "Semantic Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are known to be vulnerable to adversarial examples,\ni.e., images that are maliciously perturbed to fool the model. Generating\nadversarial examples has been mostly limited to finding small perturbations\nthat maximize the model prediction error. Such images, however, contain\nartificial perturbations that make them somewhat distinguishable from natural\nimages. This property is used by several defense methods to counter adversarial\nexamples by applying denoising filters or training the model to be robust to\nsmall perturbations.\n  In this paper, we introduce a new class of adversarial examples, namely\n\"Semantic Adversarial Examples,\" as images that are arbitrarily perturbed to\nfool the model, but in such a way that the modified image semantically\nrepresents the same object as the original image. We formulate the problem of\ngenerating such images as a constrained optimization problem and develop an\nadversarial transformation based on the shape bias property of human cognitive\nsystem. In our method, we generate adversarial images by first converting the\nRGB image into the HSV (Hue, Saturation and Value) color space and then\nrandomly shifting the Hue and Saturation components, while keeping the Value\ncomponent the same. Our experimental results on CIFAR10 dataset show that the\naccuracy of VGG16 network on adversarial color-shifted images is 5.7%.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 18:02:14 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Hosseini", "Hossein", ""], ["Poovendran", "Radha", ""]]}, {"id": "1804.00501", "submitter": "Leonardo Scabini F S", "authors": "Leonardo F S Scabini, Rayner H M Condori, Wesley N Gon\\c{c}alves,\n  Odemir M Bruno", "title": "Multilayer Complex Network Descriptors for Color-Texture\n  Characterization", "comments": "20 pages, 7 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method based on complex networks is proposed for color-texture\nanalysis. The proposal consists on modeling the image as a multilayer complex\nnetwork where each color channel is a layer, and each pixel (in each color\nchannel) is represented as a network vertex. The network dynamic evolution is\naccessed using a set of modeling parameters (radii and thresholds), and new\ncharacterization techniques are introduced to capt information regarding within\nand between color channel spatial interaction. An automatic and adaptive\napproach for threshold selection is also proposed. We conduct classification\nexperiments on 5 well-known datasets: Vistex, Usptex, Outex13, CURet and MBT.\nResults among various literature methods are compared, including deep\nconvolutional neural networks with pre-trained architectures. The proposed\nmethod presented the highest overall performance over the 5 datasets, with 97.7\nof mean accuracy against 97.0 achieved by the ResNet convolutional neural\nnetwork with 50 layers.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 13:55:43 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Scabini", "Leonardo F S", ""], ["Condori", "Rayner H M", ""], ["Gon\u00e7alves", "Wesley N", ""], ["Bruno", "Odemir M", ""]]}, {"id": "1804.00504", "submitter": "Magdalini Paschali", "authors": "Magdalini Paschali, Sailesh Conjeti, Fernando Navarro, Nassir Navab", "title": "Generalizability vs. Robustness: Adversarial Examples for Medical\n  Imaging", "comments": "Under Review for MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, for the first time, we propose an evaluation method for deep\nlearning models that assesses the performance of a model not only in an unseen\ntest scenario, but also in extreme cases of noise, outliers and ambiguous input\ndata. To this end, we utilize adversarial examples, images that fool machine\nlearning models, while looking imperceptibly different from original data, as a\nmeasure to evaluate the robustness of a variety of medical imaging models.\nThrough extensive experiments on skin lesion classification and whole brain\nsegmentation with state-of-the-art networks such as Inception and UNet, we show\nthat models that achieve comparable performance regarding generalizability may\nhave significant variations in their perception of the underlying data\nmanifold, leading to an extensive performance gap in their robustness.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 10:43:16 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Paschali", "Magdalini", ""], ["Conjeti", "Sailesh", ""], ["Navarro", "Fernando", ""], ["Navab", "Nassir", ""]]}, {"id": "1804.00506", "submitter": "Mengnan Du", "authors": "Mengnan Du, Ninghao Liu, Qingquan Song, Xia Hu", "title": "Towards Explanation of DNN-based Prediction with Guided Feature\n  Inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks (DNN) have become an effective computational tool,\nthe prediction results are often criticized by the lack of interpretability,\nwhich is essential in many real-world applications such as health informatics.\nExisting attempts based on local interpretations aim to identify relevant\nfeatures contributing the most to the prediction of DNN by monitoring the\nneighborhood of a given input. They usually simply ignore the intermediate\nlayers of the DNN that might contain rich information for interpretation. To\nbridge the gap, in this paper, we propose to investigate a guided feature\ninversion framework for taking advantage of the deep architectures towards\neffective interpretation. The proposed framework not only determines the\ncontribution of each feature in the input but also provides insights into the\ndecision-making process of DNN models. By further interacting with the neuron\nof the target category at the output layer of the DNN, we enforce the\ninterpretation result to be class-discriminative. We apply the proposed\ninterpretation model to different CNN architectures to provide explanations for\nimage data and conduct extensive experiments on ImageNet and PASCAL VOC07\ndatasets. The interpretation results demonstrate the effectiveness of our\nproposed framework in providing class-discriminative interpretation for\nDNN-based prediction.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 17:35:26 GMT"}, {"version": "v2", "created": "Sat, 26 May 2018 04:47:32 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Du", "Mengnan", ""], ["Liu", "Ninghao", ""], ["Song", "Qingquan", ""], ["Hu", "Xia", ""]]}, {"id": "1804.00508", "submitter": "Norma Ang\\'elica \\'Alvarez Torres", "authors": "Rivas P. Pedro E., Velarde-Anaya Omar, Gonzalez-Lopez Samuel, Rivas P.\n  Pablo, Alvarez-Torres Norma Angelica", "title": "Entrenamiento de una red neuronal para el reconocimiento de imagenes de\n  lengua de senas capturadas con sensores de profundidad", "comments": "5 pages, in Spanish, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the growth of the population with hearing problems, devices have been\ndeveloped that facilitate the inclusion of deaf people in society, using\ntechnology as a communication tool, such as vision systems. Then, a solution to\nthis problem is presented using neural networks and autoencoders for the\nclassification of American Sign Language images. As a result, 99.5% accuracy\nand an error of 0.01684 were obtained for image classification\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 05:40:28 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["E.", "Rivas P. Pedro", ""], ["Omar", "Velarde-Anaya", ""], ["Samuel", "Gonzalez-Lopez", ""], ["Pablo", "Rivas P.", ""], ["Angelica", "Alvarez-Torres Norma", ""]]}, {"id": "1804.00512", "submitter": "Emmanouil Tsardoulias", "authors": "Panagiotis G. Mousouliotis, Konstantinos L. Panayiotou, Emmanouil G.\n  Tsardoulias, Loukas P. Petrou, Andreas L. Symeonidis", "title": "Expanding a robot's life: Low power object recognition via FPGA-based\n  DCNN deployment", "comments": "Accepted in MOCAST 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGAs are commonly used to accelerate domain-specific algorithmic\nimplementations, as they can achieve impressive performance boosts, are\nreprogrammable and exhibit minimal power consumption. In this work, the\nSqueezeNet DCNN is accelerated using an SoC FPGA in order for the offered\nobject recognition resource to be employed in a robotic application.\nExperiments are conducted to investigate the performance and power consumption\nof the implementation in comparison to deployment on other widely-used\ncomputational systems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 09:44:44 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Mousouliotis", "Panagiotis G.", ""], ["Panayiotou", "Konstantinos L.", ""], ["Tsardoulias", "Emmanouil G.", ""], ["Petrou", "Loukas P.", ""], ["Symeonidis", "Andreas L.", ""]]}, {"id": "1804.00516", "submitter": "Anabel G\\'omez-R\\'ios", "authors": "Anabel G\\'omez-R\\'ios, Siham Tabik, Juli\\'an Luengo, ASM Shihavuddin,\n  Bartosz Krawczyk and Francisco Herrera", "title": "Towards Highly Accurate Coral Texture Images Classification Using Deep\n  Convolutional Neural Networks and Data Augmentation", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of coral species based on underwater texture images pose a\nsignificant difficulty for machine learning algorithms, due to the three\nfollowing challenges embedded in the nature of this data: 1) datasets do not\ninclude information about the global structure of the coral; 2) several species\nof coral have very similar characteristics; and 3) defining the spatial borders\nbetween classes is difficult as many corals tend to appear together in groups.\nFor this reason, the classification of coral species has always required an aid\nfrom a domain expert. The objective of this paper is to develop an accurate\nclassification model for coral texture images. Current datasets contain a large\nnumber of imbalanced classes, while the images are subject to inter-class\nvariation. We have analyzed 1) several Convolutional Neural Network (CNN)\narchitectures, 2) data augmentation techniques and 3) transfer learning. We\nhave achieved the state-of-the art accuracies using different variations of\nResNet on the two current coral texture datasets, EILAT and RSMAS.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 12:05:12 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["G\u00f3mez-R\u00edos", "Anabel", ""], ["Tabik", "Siham", ""], ["Luengo", "Juli\u00e1n", ""], ["Shihavuddin", "ASM", ""], ["Krawczyk", "Bartosz", ""], ["Herrera", "Francisco", ""]]}, {"id": "1804.00518", "submitter": "Dawei Du", "authors": "Dawei Du, Yuankai Qi, Hongyang Yu, Yifan Yang, Kaiwen Duan, Guorong\n  Li, Weigang Zhang, Qingming Huang, Qi Tian", "title": "The Unmanned Aerial Vehicle Benchmark: Object Detection and Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advantage of high mobility, Unmanned Aerial Vehicles (UAVs) are used\nto fuel numerous important applications in computer vision, delivering more\nefficiency and convenience than surveillance cameras with fixed camera angle,\nscale and view. However, very limited UAV datasets are proposed, and they focus\nonly on a specific task such as visual tracking or object detection in\nrelatively constrained scenarios. Consequently, it is of great importance to\ndevelop an unconstrained UAV benchmark to boost related researches. In this\npaper, we construct a new UAV benchmark focusing on complex scenarios with new\nlevel challenges. Selected from 10 hours raw videos, about 80,000\nrepresentative frames are fully annotated with bounding boxes as well as up to\n14 kinds of attributes (e.g., weather condition, flying altitude, camera view,\nvehicle category, and occlusion) for three fundamental computer vision tasks:\nobject detection, single object tracking, and multiple object tracking. Then, a\ndetailed quantitative study is performed using most recent state-of-the-art\nalgorithms for each task. Experimental results show that the current\nstate-of-the-art methods perform relative worse on our dataset, due to the new\nchallenges appeared in UAV based real scenes, e.g., high density, small object,\nand camera motion. To our knowledge, our work is the first time to explore such\nissues in unconstrained scenes comprehensively.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 15:07:09 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Du", "Dawei", ""], ["Qi", "Yuankai", ""], ["Yu", "Hongyang", ""], ["Yang", "Yifan", ""], ["Duan", "Kaiwen", ""], ["Li", "Guorong", ""], ["Zhang", "Weigang", ""], ["Huang", "Qingming", ""], ["Tian", "Qi", ""]]}, {"id": "1804.00521", "submitter": "Raunak Dey", "authors": "Raunak Dey, Yi Hong", "title": "CompNet: Complementary Segmentation Network for Brain MRI Extraction", "comments": "8 pages, Accepted to MICCAI 2018", "journal-ref": null, "doi": "10.1007/978-3-030-00931-1_72", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain extraction is a fundamental step for most brain imaging studies. In\nthis paper, we investigate the problem of skull stripping and propose\ncomplementary segmentation networks (CompNets) to accurately extract the brain\nfrom T1-weighted MRI scans, for both normal and pathological brain images. The\nproposed networks are designed in the framework of encoder-decoder networks and\nhave two pathways to learn features from both the brain tissue and its\ncomplementary part located outside of the brain. The complementary pathway\nextracts the features in the non-brain region and leads to a robust solution to\nbrain extraction from MRIs with pathologies, which do not exist in our training\ndataset. We demonstrate the effectiveness of our networks by evaluating them on\nthe OASIS dataset, resulting in the state of the art performance under the\ntwo-fold cross-validation setting. Moreover, the robustness of our networks is\nverified by testing on images with introduced pathologies and by showing its\ninvariance to unseen brain pathologies. In addition, our complementary network\ndesign is general and can be extended to address other image segmentation\nproblems with better generalization.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 03:26:22 GMT"}, {"version": "v2", "created": "Sun, 17 Jun 2018 04:28:20 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Dey", "Raunak", ""], ["Hong", "Yi", ""]]}, {"id": "1804.00525", "submitter": "Ismail Elezi", "authors": "Lukas Tuggener, Ismail Elezi, J\\\"urgen Schmidhuber, Marcello Pelillo\n  and Thilo Stadelmann", "title": "DeepScores -- A Dataset for Segmentation, Detection and Classification\n  of Tiny Objects", "comments": "6 pages, accepted on IEEE International Conference on Pattern\n  Recognition 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the DeepScores dataset with the goal of advancing the\nstate-of-the-art in small objects recognition, and by placing the question of\nobject recognition in the context of scene understanding. DeepScores contains\nhigh quality images of musical scores, partitioned into 300,000 sheets of\nwritten music that contain symbols of different shapes and sizes. With close to\na hundred millions of small objects, this makes our dataset not only unique,\nbut also the largest public dataset. DeepScores comes with ground truth for\nobject classification, detection and semantic segmentation. DeepScores thus\nposes a relevant challenge for computer vision in general, beyond the scope of\noptical music recognition (OMR) research. We present a detailed statistical\nanalysis of the dataset, comparing it with other computer vision datasets like\nCaltech101/256, PASCAL VOC, SUN, SVHN, ImageNet, MS-COCO, smaller computer\nvision datasets, as well as with other OMR datasets. Finally, we provide\nbaseline performances for object classification and give pointers to future\nresearch based on this dataset.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 14:44:45 GMT"}, {"version": "v2", "created": "Sat, 26 May 2018 21:12:59 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Tuggener", "Lukas", ""], ["Elezi", "Ismail", ""], ["Schmidhuber", "J\u00fcrgen", ""], ["Pelillo", "Marcello", ""], ["Stadelmann", "Thilo", ""]]}, {"id": "1804.00527", "submitter": "Najoua Essoukri Ben Amara", "authors": "Imen Abroug Ben Abdelghani, Najwa Essoukri Ben Amara", "title": "A Neuronal Planar Modeling for Handwriting Signature based on Automatic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with offline handwriting signature verification.We propose a\nplanar neuronal model of signature image. Planarmodelsare generally based on\ndelimiting homogenous zones ofimages; we propose in this paper an automatic\nsegmentationapproach into bands of signature images. Signature image ismodeled\nby a planar neuronal model with horizontal secondarymodels and a\nverticalprincipal model. The proposed methodhas been tested on two databases.\nThe first is the one we havecollected; it includes 6000 signaturescorresponding\nto 60writers. The second is the public GPDS-300 database including16200\nsignature corresponding to 300 persons. The achievedresults are promising.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 16:50:35 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Abdelghani", "Imen Abroug Ben", ""], ["Amara", "Najwa Essoukri Ben", ""]]}, {"id": "1804.00528", "submitter": "Najoua Essoukri Ben Amara", "authors": "Anouar Ben Khalifa, Sami Gazzah, Najoua Essoukri Ben Amara", "title": "Multimodal Biometric Authentication Using Choquet Integral and Genetic\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Choquet integral is a tool for the information fusion that is very\neffective in the case where fuzzy measures associated with it are well chosen.\nIn this paper,we propose a new approach for calculating fuzzy measures\nassociated with the Choquet integral in a context of data fusion in multimodal\nbiometrics. The proposed approach is based on genetic algorithms. It has been\nvalidated in two databases: the first base is relative to synthetic scores and\nthe second one is biometrically relating to the face, fingerprintand palmprint.\nThe results achieved attest the robustness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 18:03:22 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Khalifa", "Anouar Ben", ""], ["Gazzah", "Sami", ""], ["Amara", "Najoua Essoukri Ben", ""]]}, {"id": "1804.00532", "submitter": "Zhou Xing Dr", "authors": "Zhou Xing, Fei Xiao", "title": "Predictions of short-term driving intention using recurrent neural\n  network on sequential data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictions of driver's intentions and their behaviors using the road is of\ngreat importance for planning and decision making processes of autonomous\ndriving vehicles. In particular, relatively short-term driving intentions are\nthe fundamental units that constitute more sophisticated driving goals,\nbehaviors, such as overtaking the slow vehicle in front, exit or merge onto a\nhigh way, etc. While it is not uncommon that most of the time human driver can\nrationalize, in advance, various on-road behaviors, intentions, as well as the\nassociated risks, aggressiveness, reciprocity characteristics, etc., such\nreasoning skills can be challenging and difficult for an autonomous driving\nsystem to learn. In this article, we demonstrate a disciplined methodology that\ncan be used to build and train a predictive drive system, therefore to learn\nthe on-road characteristics aforementioned.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 03:14:50 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Xing", "Zhou", ""], ["Xiao", "Fei", ""]]}, {"id": "1804.00533", "submitter": "Wenhan Luo", "authors": "Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Wei Liu, Hongdong Li", "title": "Adversarial Spatio-Temporal Learning for Video Deblurring", "comments": "To appear in IEEE Transactions on Image Processing (TIP)", "journal-ref": null, "doi": "10.1109/TIP.2018.2867733", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera shake or target movement often leads to undesired blur effects in\nvideos captured by a hand-held camera. Despite significant efforts having been\ndevoted to video-deblur research, two major challenges remain: 1) how to model\nthe spatio-temporal characteristics across both the spatial domain (i.e., image\nplane) and temporal domain (i.e., neighboring frames), and 2) how to restore\nsharp image details w.r.t. the conventionally adopted metric of pixel-wise\nerrors. In this paper, to address the first challenge, we propose a DeBLuRring\nNetwork (DBLRNet) for spatial-temporal learning by applying a modified 3D\nconvolution to both spatial and temporal domains. Our DBLRNet is able to\ncapture jointly spatial and temporal information encoded in neighboring frames,\nwhich directly contributes to improved video deblur performance. To tackle the\nsecond challenge, we leverage the developed DBLRNet as a generator in the GAN\n(generative adversarial network) architecture, and employ a content loss in\naddition to an adversarial loss for efficient adversarial training. The\ndeveloped network, which we name as DeBLuRring Generative Adversarial Network\n(DBLRGAN), is tested on two standard benchmarks and achieves the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 10:25:18 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 04:14:18 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Zhang", "Kaihao", ""], ["Luo", "Wenhan", ""], ["Zhong", "Yiran", ""], ["Ma", "Lin", ""], ["Liu", "Wei", ""], ["Li", "Hongdong", ""]]}, {"id": "1804.00558", "submitter": "Aral Sarrafi", "authors": "Aral Sarrafi, Zhu Mao, Christopher Niezrecki, Peyman Poozesh", "title": "Vibration-Based Damage Detection in Wind Turbine Blades using\n  Phase-Based Motion Estimation and Motion Magnification", "comments": null, "journal-ref": "Journal of Sound and Vibration, 421 (2018) 300-318", "doi": "10.1016/j.jsv.2018.01.050", "report-no": "10.1016/j.jsv.2018.01.050", "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vibration-based Structural Health Monitoring (SHM) techniques are among the\nmost common approaches for structural damage identification. The presence of\ndamage in structures may be identified by monitoring the changes in dynamic\nbehavior subject to external loading, and is typically performed by using\nexperimental modal analysis (EMA) or operational modal analysis (OMA). These\ntools for SHM normally require a limited number of physically attached\ntransducers (e.g. accelerometers) in order to record the response of the\nstructure for further analysis. Signal conditioners, wires, wireless receivers\nand a data acquisition system (DAQ) are also typical components of traditional\nsensing systems used in vibration-based SHM. However, instrumentation of\nlightweight structures with contact sensors such as accelerometers may induce\nmass-loading effects, and for large-scale structures, the instrumentation is\nlabor intensive and time consuming. Achieving high spatial measurement\nresolution for a large-scale structure is not always feasible while working\nwith traditional contact sensors, and there is also the potential for a lack of\nreliability associated with fixed contact sensors in outliving the life-span of\nthe host structure. Among the state-of-the-art non-contact measurements,\ndigital video cameras are able to rapidly collect high-density spatial\ninformation from structures remotely. In this paper, the subtle motions from\nrecorded video (i.e. a sequence of images) are extracted by means of\nPhase-based Motion Estimation (PME) and the extracted information is used to\nconduct damage identification on a 2.3-meter long Skystream wind turbine blade\n(WTB). The PME and phased-based motion magnification approach estimates the\nstructural motion from the captured sequence of images for both a baseline and\ndamaged test cases on a wind turbine blade.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 16:03:35 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Sarrafi", "Aral", ""], ["Mao", "Zhu", ""], ["Niezrecki", "Christopher", ""], ["Poozesh", "Peyman", ""]]}, {"id": "1804.00582", "submitter": "Zhengqi Li", "authors": "Zhengqi Li and Noah Snavely", "title": "Learning Intrinsic Image Decomposition from Watching the World", "comments": "CVPR, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-view intrinsic image decomposition is a highly ill-posed problem, and\nso a promising approach is to learn from large amounts of data. However, it is\ndifficult to collect ground truth training data at scale for intrinsic images.\nIn this paper, we explore a different approach to learning intrinsic images:\nobserving image sequences over time depicting the same scene under changing\nillumination, and learning single-view decompositions that are consistent with\nthese changes. This approach allows us to learn without ground truth\ndecompositions, and to instead exploit information available from multiple\nimages when training. Our trained model can then be applied at test time to\nsingle views. We describe a new learning framework based on this idea,\nincluding new loss functions that can be efficiently evaluated over entire\nsequences. While prior learning-based methods achieve good performance on\nspecific benchmarks, we show that our approach generalizes well to several\ndiverse datasets, including MIT intrinsic images, Intrinsic Images in the Wild\nand Shading Annotations in the Wild.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 15:06:11 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Li", "Zhengqi", ""], ["Snavely", "Noah", ""]]}, {"id": "1804.00586", "submitter": "Jianwen Xie", "authors": "Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu,\n  Ying Nian Wu", "title": "Learning Descriptor Networks for 3D Shape Synthesis and Analysis", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a 3D shape descriptor network, which is a deep\nconvolutional energy-based model, for modeling volumetric shape patterns. The\nmaximum likelihood training of the model follows an \"analysis by synthesis\"\nscheme and can be interpreted as a mode seeking and mode shifting process. The\nmodel can synthesize 3D shape patterns by sampling from the probability\ndistribution via MCMC such as Langevin dynamics. The model can be used to train\na 3D generator network via MCMC teaching. The conditional version of the 3D\nshape descriptor net can be used for 3D object recovery and 3D object\nsuper-resolution. Experiments demonstrate that the proposed model can generate\nrealistic 3D shape patterns and can be useful for 3D shape analysis.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 15:15:34 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Xie", "Jianwen", ""], ["Zheng", "Zilong", ""], ["Gao", "Ruiqi", ""], ["Wang", "Wenguan", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1804.00607", "submitter": "Zhengqi Li", "authors": "Zhengqi Li and Noah Snavely", "title": "MegaDepth: Learning Single-View Depth Prediction from Internet Photos", "comments": "updated paper for 'MegaDepth: Learning Single-View Depth Prediction\n  from Internet Photos', CVPR, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-view depth prediction is a fundamental problem in computer vision.\nRecently, deep learning methods have led to significant progress, but such\nmethods are limited by the available training data. Current datasets based on\n3D sensors have key limitations, including indoor-only images (NYU), small\nnumbers of training examples (Make3D), and sparse sampling (KITTI). We propose\nto use multi-view Internet photo collections, a virtually unlimited data\nsource, to generate training data via modern structure-from-motion and\nmulti-view stereo (MVS) methods, and present a large depth dataset called\nMegaDepth based on this idea. Data derived from MVS comes with its own\nchallenges, including noise and unreconstructable objects. We address these\nchallenges with new data cleaning methods, as well as automatically augmenting\nour data with ordinal depth relations generated using semantic segmentation. We\nvalidate the use of large amounts of Internet data by showing that models\ntrained on MegaDepth exhibit strong generalization-not only to novel scenes,\nbut also to other diverse datasets including Make3D, KITTI, and DIW, even when\nno images from those datasets are seen during training.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 16:03:34 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 02:06:36 GMT"}, {"version": "v3", "created": "Sun, 11 Nov 2018 21:57:44 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 01:12:43 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Li", "Zhengqi", ""], ["Snavely", "Noah", ""]]}, {"id": "1804.00623", "submitter": "Renzo Andri", "authors": "Renzo Andri, Lukas Cavigelli, Davide Rossi, Luca Benini", "title": "Hyperdrive: A Multi-Chip Systolically Scalable Binary-Weight CNN\n  Inference Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved impressive results in computer vision and\nmachine learning. Unfortunately, state-of-the-art networks are extremely\ncompute and memory intensive which makes them unsuitable for mW-devices such as\nIoT end-nodes. Aggressive quantization of these networks dramatically reduces\nthe computation and memory footprint. Binary-weight neural networks (BWNs)\nfollow this trend, pushing weight quantization to the limit. Hardware\naccelerators for BWNs presented up to now have focused on core efficiency,\ndisregarding I/O bandwidth and system-level efficiency that are crucial for\ndeployment of accelerators in ultra-low power devices. We present Hyperdrive: a\nBWN accelerator dramatically reducing the I/O bandwidth exploiting a novel\nbinary-weight streaming approach, which can be used for arbitrarily sized\nconvolutional neural network architecture and input resolution by exploiting\nthe natural scalability of the compute units both at chip-level and\nsystem-level by arranging Hyperdrive chips systolically in a 2D mesh while\nprocessing the entire feature map together in parallel. Hyperdrive achieves 4.3\nTOp/s/W system-level efficiency (i.e., including I/Os)---3.1x higher than\nstate-of-the-art BWN accelerators, even if its core uses resource-intensive\nFP16 arithmetic for increased robustness.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 17:35:42 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 13:25:50 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 11:15:37 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Andri", "Renzo", ""], ["Cavigelli", "Lukas", ""], ["Rossi", "Davide", ""], ["Benini", "Luca", ""]]}, {"id": "1804.00630", "submitter": "Hesam Pakdaman", "authors": "Hesam Pakdaman", "title": "Updating the generator in PPGN-h with gradients flowing through the\n  encoder", "comments": "Master's thesis, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Generative Adversarial Network framework has shown success in implicitly\nmodeling data distributions and is able to generate realistic samples. Its\narchitecture is comprised of a generator, which produces fake data that\nsuperficially seem to belong to the real data distribution, and a discriminator\nwhich is to distinguish fake from genuine samples. The Noiseless Joint Plug &\nPlay model offers an extension to the framework by simultaneously training\nautoencoders. This model uses a pre-trained encoder as a feature extractor,\nfeeding the generator with global information. Using the Plug & Play network as\nbaseline, we design a new model by adding discriminators to the Plug & Play\narchitecture. These additional discriminators are trained to discern real and\nfake latent codes, which are the output of the encoder using genuine and\ngenerated inputs, respectively. We proceed to investigate whether this approach\nis viable. Experiments conducted for the MNIST manifold show that this indeed\nis the case.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 17:22:00 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Pakdaman", "Hesam", ""]]}, {"id": "1804.00637", "submitter": "Carolina Raposo", "authors": "Carolina Raposo and Joao P. Barreto", "title": "3D Registration of Curves and Surfaces using Local Differential\n  Information", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents for the first time a global method for registering 3D\ncurves with 3D surfaces without requiring an initialization. The algorithm\nworks with 2-tuples point+vector that consist in pairs of points augmented with\nthe information of their tangents or normals. A closed-form solution for\ndetermining the alignment transformation from a pair of matching 2-tuples is\nproposed. In addition, the set of necessary conditions for two 2-tuples to\nmatch is derived. This allows fast search of correspondences that are used in\nan hypothesise-and-test framework for accomplishing global registration.\nComparative experiments demonstrate that the proposed algorithm is the first\neffective solution for curve vs surface registration, with the method achieving\naccurate alignment in situations of small overlap and large percentage of\noutliers in a fraction of a second. The proposed framework is extended to the\ncases of curve vs curve and surface vs surface registration, with the former\nbeing particularly relevant since it is also a largely unsolved problem.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 17:33:15 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Raposo", "Carolina", ""], ["Barreto", "Joao P.", ""]]}, {"id": "1804.00645", "submitter": "Aravind Srinivas", "authors": "Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, Chelsea\n  Finn", "title": "Universal Planning Networks", "comments": "Videos available at https://sites.google.com/view/upn-public/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in complex visuomotor control is learning abstract\nrepresentations that are effective for specifying goals, planning, and\ngeneralization. To this end, we introduce universal planning networks (UPN).\nUPNs embed differentiable planning within a goal-directed policy. This planning\ncomputation unrolls a forward model in a latent space and infers an optimal\naction plan through gradient descent trajectory optimization. The\nplan-by-gradient-descent process and its underlying representations are learned\nend-to-end to directly optimize a supervised imitation learning objective. We\nfind that the representations learned are not only effective for goal-directed\nvisual imitation via gradient-based trajectory optimization, but can also\nprovide a metric for specifying goals using images. The learned representations\ncan be leveraged to specify distance-based rewards to reach new target states\nfor model-free reinforcement learning, resulting in substantially more\neffective learning when solving new tasks described via image-based goals. We\nwere able to achieve successful transfer of visuomotor planning strategies\nacross robots with significantly different morphologies and actuation\ncapabilities.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 17:51:53 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 17:36:36 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Srinivas", "Aravind", ""], ["Jabri", "Allan", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""], ["Finn", "Chelsea", ""]]}, {"id": "1804.00650", "submitter": "Jia-Bin Huang", "authors": "Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, Jia-Bin\n  Huang", "title": "DeepMVS: Learning Multi-view Stereopsis", "comments": "CVPR 2018. Project page: https://phuang17.github.io/DeepMVS/ Code:\n  https://github.com/phuang17/DeepMVS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepMVS, a deep convolutional neural network (ConvNet) for\nmulti-view stereo reconstruction. Taking an arbitrary number of posed images as\ninput, we first produce a set of plane-sweep volumes and use the proposed\nDeepMVS network to predict high-quality disparity maps. The key contributions\nthat enable these results are (1) supervised pretraining on a photorealistic\nsynthetic dataset, (2) an effective method for aggregating information across a\nset of unordered images, and (3) integrating multi-layer feature activations\nfrom the pre-trained VGG-19 network. We validate the efficacy of DeepMVS using\nthe ETH3D Benchmark. Our results show that DeepMVS compares favorably against\nstate-of-the-art conventional MVS algorithms and other ConvNet based methods,\nparticularly for near-textureless regions and thin structures.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 17:58:45 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Huang", "Po-Han", ""], ["Matzen", "Kevin", ""], ["Kopf", "Johannes", ""], ["Ahuja", "Narendra", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "1804.00651", "submitter": "Cairong Zhang", "authors": "Cairong Zhang, Guijin Wang, Hengkai Guo, Xinghao Chen, Fei Qiao,\n  Huazhong Yang", "title": "Interactive Hand Pose Estimation: Boosting accuracy in localizing\n  extended finger joints", "comments": "Original publication available on\n  https://doi.org/10.2352/ISSN.2470-1173.2018.2.VIPC-251", "journal-ref": "Electronic Imaging, Visual Information Processing and\n  Communication IX (2018), pp. 251-1-251-6(6)", "doi": "10.2352/ISSN.2470-1173.2018.2.VIPC-251", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate 3D hand pose estimation plays an important role in Human Machine\nInteraction (HMI). In the reality of HMI, joints in fingers stretching out,\nespecially corresponding fingertips, are much more important than other joints.\nWe propose a novel method to refine stretching-out finger joint locations after\nobtaining rough hand pose estimation. It first detects which fingers are\nstretching out, then neighbor pixels of certain joint vote for its new location\nbased on random forests. The algorithm is tested on two public datasets: MSRA15\nand ICVL. After the refinement stage of stretching-out fingers, errors of\npredicted HMI finger joint locations are significantly reduced. Mean error of\nall fingertips reduces around 5mm (relatively more than 20%). Stretching-out\nfingertip locations are even more precise, which in MSRA15 reduces 10.51mm\n(relatively 41.4%).\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 17:59:38 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 15:31:40 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Zhang", "Cairong", ""], ["Wang", "Guijin", ""], ["Guo", "Hengkai", ""], ["Chen", "Xinghao", ""], ["Qiao", "Fei", ""], ["Yang", "Huazhong", ""]]}, {"id": "1804.00657", "submitter": "Yuval Bahat", "authors": "Yuval Bahat and Gregory Shakhnarovich", "title": "Confidence from Invariance to Image Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a technique for automatically detecting the classification errors\nof a pre-trained visual classifier. Our method is agnostic to the form of the\nclassifier, requiring access only to classifier responses to a set of inputs.\nWe train a parametric binary classifier (error/correct) on a representation\nderived from a set of classifier responses generated from multiple copies of\nthe same input, each subject to a different natural image transformation. Thus,\nwe establish a measure of confidence in classifier's decision by analyzing the\ninvariance of its decision under various transformations. In experiments with\nmultiple data sets (STL-10,CIFAR-100,ImageNet) and classifiers, we demonstrate\nnew state of the art for the error detection task. In addition, we apply our\ntechnique to novelty detection scenarios, where we also demonstrate state of\nthe art results.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 20:38:52 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Bahat", "Yuval", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "1804.00722", "submitter": "Kibok Lee", "authors": "Kibok Lee, Kimin Lee, Kyle Min, Yuting Zhang, Jinwoo Shin, Honglak Lee", "title": "Hierarchical Novelty Detection for Visual Object Recognition", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved impressive success in large-scale visual\nobject recognition tasks with a predefined set of classes. However, recognizing\nobjects of novel classes unseen during training still remains challenging. The\nproblem of detecting such novel classes has been addressed in the literature,\nbut most prior works have focused on providing simple binary or regressive\ndecisions, e.g., the output would be \"known,\" \"novel,\" or corresponding\nconfidence intervals. In this paper, we study more informative novelty\ndetection schemes based on a hierarchical classification framework. For an\nobject of a novel class, we aim for finding its closest super class in the\nhierarchical taxonomy of known classes. To this end, we propose two different\napproaches termed top-down and flatten methods, and their combination as well.\nThe essential ingredients of our methods are confidence-calibrated classifiers,\ndata relabeling, and the leave-one-out strategy for modeling novel classes\nunder the hierarchical taxonomy. Furthermore, our method can generate a\nhierarchical embedding that leads to improved generalized zero-shot learning\nperformance in combination with other commonly-used semantic embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 20:36:43 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 10:18:15 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Lee", "Kibok", ""], ["Lee", "Kimin", ""], ["Min", "Kyle", ""], ["Zhang", "Yuting", ""], ["Shin", "Jinwoo", ""], ["Lee", "Honglak", ""]]}, {"id": "1804.00775", "submitter": "Duy-Kien Nguyen", "authors": "Duy-Kien Nguyen and Takayuki Okatani", "title": "Improved Fusion of Visual and Language Representations by Dense\n  Symmetric Co-Attention for Visual Question Answering", "comments": "In Proceeding of CVPR'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key solution to visual question answering (VQA) exists in how to fuse\nvisual and language features extracted from an input image and question. We\nshow that an attention mechanism that enables dense, bi-directional\ninteractions between the two modalities contributes to boost accuracy of\nprediction of answers. Specifically, we present a simple architecture that is\nfully symmetric between visual and language representations, in which each\nquestion word attends on image regions and each image region attends on\nquestion words. It can be stacked to form a hierarchy for multi-step\ninteractions between an image-question pair. We show through experiments that\nthe proposed architecture achieves a new state-of-the-art on VQA and VQA 2.0\ndespite its small size. We also present qualitative evaluation, demonstrating\nhow the proposed attention mechanism can generate reasonable attention maps on\nimages and questions, which leads to the correct answer prediction.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 01:24:23 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 08:12:22 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Nguyen", "Duy-Kien", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1804.00782", "submitter": "Jiajun Wu", "authors": "Jiajun Wu, Tianfan Xue, Joseph J. Lim, Yuandong Tian, Joshua B.\n  Tenenbaum, Antonio Torralba, William T. Freeman", "title": "3D Interpreter Networks for Viewer-Centered Wireframe Modeling", "comments": "Journal preprint of arXiv:1604.08685 (IJCV, 2018). The first two\n  authors contributed equally to this work. Project page:\n  http://3dinterpreter.csail.mit.edu", "journal-ref": "International Journal of Computer Vision, Volume 126, Issue 9, pp\n  1009-1026, 2018", "doi": "10.1007/s11263-018-1074-6", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding 3D object structure from a single image is an important but\nchallenging task in computer vision, mostly due to the lack of 3D object\nannotations to real images. Previous research tackled this problem by either\nsearching for a 3D shape that best explains 2D annotations, or training purely\non synthetic data with ground truth 3D information. In this work, we propose 3D\nINterpreter Networks (3D-INN), an end-to-end trainable framework that\nsequentially estimates 2D keypoint heatmaps and 3D object skeletons and poses.\nOur system learns from both 2D-annotated real images and synthetic 3D data.\nThis is made possible mainly by two technical innovations. First, heatmaps of\n2D keypoints serve as an intermediate representation to connect real and\nsynthetic data. 3D-INN is trained on real images to estimate 2D keypoint\nheatmaps from an input image; it then predicts 3D object structure from\nheatmaps using knowledge learned from synthetic 3D shapes. By doing so, 3D-INN\nbenefits from the variation and abundance of synthetic 3D objects, without\nsuffering from the domain difference between real and synthesized images, often\ndue to imperfect rendering. Second, we propose a Projection Layer, mapping\nestimated 3D structure back to 2D. During training, it ensures 3D-INN to\npredict 3D structure whose projection is consistent with the 2D annotations to\nreal images. Experiments show that the proposed system performs well on both 2D\nkeypoint estimation and 3D structure recovery. We also demonstrate that the\nrecovered 3D information has wide vision applications, such as image retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 01:55:31 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 23:14:56 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Wu", "Jiajun", ""], ["Xue", "Tianfan", ""], ["Lim", "Joseph J.", ""], ["Tian", "Yuandong", ""], ["Tenenbaum", "Joshua B.", ""], ["Torralba", "Antonio", ""], ["Freeman", "William T.", ""]]}, {"id": "1804.00787", "submitter": "Lingxi Xie", "authors": "Yan Wang, Lingxi Xie, Siyuan Qiao, Ya Zhang, Wenjun Zhang, Alan L.\n  Yuille", "title": "Multi-Scale Spatially-Asymmetric Recalibration for Image Classification", "comments": "17 pages, 5 figures, submitted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution is spatially-symmetric, i.e., the visual features are independent\nof its position in the image, which limits its ability to utilize contextual\ncues for visual recognition. This paper addresses this issue by introducing a\nrecalibration process, which refers to the surrounding region of each neuron,\ncomputes an importance value and multiplies it to the original neural response.\nOur approach is named multi-scale spatially-asymmetric recalibration (MS-SAR),\nwhich extracts visual cues from surrounding regions at multiple scales, and\ndesigns a weighting scheme which is asymmetric in the spatial domain. MS-SAR is\nimplemented in an efficient way, so that only small fractions of extra\nparameters and computations are required. We apply MS-SAR to several popular\nbuilding blocks, including the residual block and the densely-connected block,\nand demonstrate its superior performance in both CIFAR and ILSVRC2012\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 02:09:14 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Wang", "Yan", ""], ["Xie", "Lingxi", ""], ["Qiao", "Siyuan", ""], ["Zhang", "Ya", ""], ["Zhang", "Wenjun", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1804.00792", "submitter": "Wenqian Ronny Huang", "authors": "Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph\n  Studer, Tudor Dumitras, Tom Goldstein", "title": "Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks", "comments": "Presented at the NIPS 2018 conference. 11 pages, 4 figures, with a\n  supplementary section of 7 pages, 7 figures. First two authors contributed\n  equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data poisoning is an attack on machine learning models wherein the attacker\nadds examples to the training set to manipulate the behavior of the model at\ntest time. This paper explores poisoning attacks on neural nets. The proposed\nattacks use \"clean-labels\"; they don't require the attacker to have any control\nover the labeling of training data. They are also targeted; they control the\nbehavior of the classifier on a $\\textit{specific}$ test instance without\ndegrading overall classifier performance. For example, an attacker could add a\nseemingly innocuous image (that is properly labeled) to a training set for a\nface recognition engine, and control the identity of a chosen person at test\ntime. Because the attacker does not need to control the labeling function,\npoisons could be entered into the training set simply by leaving them on the\nweb and waiting for them to be scraped by a data collection bot.\n  We present an optimization-based method for crafting poisons, and show that\njust one single poison image can control classifier behavior when transfer\nlearning is used. For full end-to-end training, we present a \"watermarking\"\nstrategy that makes poisoning reliable using multiple ($\\approx$50) poisoned\ntraining instances. We demonstrate our method by generating poisoned frog\nimages from the CIFAR dataset and using them to manipulate image classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 02:24:31 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2018 15:37:17 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Shafahi", "Ali", ""], ["Huang", "W. Ronny", ""], ["Najibi", "Mahyar", ""], ["Suciu", "Octavian", ""], ["Studer", "Christoph", ""], ["Dumitras", "Tudor", ""], ["Goldstein", "Tom", ""]]}, {"id": "1804.00796", "submitter": "Zequn Jie", "authors": "Zequn Jie, Pengfei Wang, Yonggen Ling, Bo Zhao, Yunchao Wei, Jiashi\n  Feng, Wei Liu", "title": "Left-Right Comparative Recurrent Model for Stereo Matching", "comments": "Accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging the disparity information from both left and right views is\ncrucial for stereo disparity estimation. Left-right consistency check is an\neffective way to enhance the disparity estimation by referring to the\ninformation from the opposite view. However, the conventional left-right\nconsistency check is an isolated post-processing step and heavily hand-crafted.\nThis paper proposes a novel left-right comparative recurrent model to perform\nleft-right consistency checking jointly with disparity estimation. At each\nrecurrent step, the model produces disparity results for both views, and then\nperforms online left-right comparison to identify the mismatched regions which\nmay probably contain erroneously labeled pixels. A soft attention mechanism is\nintroduced, which employs the learned error maps for better guiding the model\nto selectively focus on refining the unreliable regions at the next recurrent\nstep. In this way, the generated disparity maps are progressively improved by\nthe proposed recurrent model. Extensive evaluations on KITTI 2015, Scene Flow\nand Middlebury benchmarks validate the effectiveness of our model,\ndemonstrating that state-of-the-art stereo disparity estimation results can be\nachieved by this new model.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 02:50:26 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Jie", "Zequn", ""], ["Wang", "Pengfei", ""], ["Ling", "Yonggen", ""], ["Zhao", "Bo", ""], ["Wei", "Yunchao", ""], ["Feng", "Jiashi", ""], ["Liu", "Wei", ""]]}, {"id": "1804.00819", "submitter": "Luowei Zhou", "authors": "Luowei Zhou, Yingbo Zhou, Jason J. Corso, Richard Socher, and Caiming\n  Xiong", "title": "End-to-End Dense Video Captioning with Masked Transformer", "comments": "To appear at CVPR18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense video captioning aims to generate text descriptions for all events in\nan untrimmed video. This involves both detecting and describing events.\nTherefore, all previous methods on dense video captioning tackle this problem\nby building two models, i.e. an event proposal and a captioning model, for\nthese two sub-problems. The models are either trained separately or in\nalternation. This prevents direct influence of the language description to the\nevent proposal, which is important for generating accurate descriptions. To\naddress this problem, we propose an end-to-end transformer model for dense\nvideo captioning. The encoder encodes the video into appropriate\nrepresentations. The proposal decoder decodes from the encoding with different\nanchors to form video event proposals. The captioning decoder employs a masking\nnetwork to restrict its attention to the proposal event over the encoding\nfeature. This masking network converts the event proposal to a differentiable\nmask, which ensures the consistency between the proposal and captioning during\ntraining. In addition, our model employs a self-attention mechanism, which\nenables the use of efficient non-recurrent structure during encoding and leads\nto performance improvements. We demonstrate the effectiveness of this\nend-to-end model on ActivityNet Captions and YouCookII datasets, where we\nachieved 10.12 and 6.58 METEOR score, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 04:11:00 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Zhou", "Luowei", ""], ["Zhou", "Yingbo", ""], ["Corso", "Jason J.", ""], ["Socher", "Richard", ""], ["Xiong", "Caiming", ""]]}, {"id": "1804.00858", "submitter": "Aamir Mustafa", "authors": "Amanjot Kaur, Aamir Mustafa, Love Mehta, Abhinav Dhall", "title": "Prediction and Localization of Student Engagement in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new dataset for student engagement detection\nand localization. Digital revolution has transformed the traditional teaching\nprocedure and a result analysis of the student engagement in an e-learning\nenvironment would facilitate effective task accomplishment and learning. Well\nknown social cues of engagement/disengagement can be inferred from facial\nexpressions, body movements and gaze pattern. In this paper, student's response\nto various stimuli videos are recorded and important cues are extracted to\nestimate variations in engagement level. In this paper, we study the\nassociation of a subject's behavioral cues with his/her engagement level, as\nannotated by labelers. We then localize engaging/non-engaging parts in the\nstimuli videos using a deep multiple instance learning based framework, which\ncan give useful insight into designing Massive Open Online Courses (MOOCs)\nvideo material. Recognizing the lack of any publicly available dataset in the\ndomain of user engagement, a new `in the wild' dataset is created to study the\nsubject engagement problem. The dataset contains 195 videos captured from 78\nsubjects which is about 16.5 hours of recording. We present detailed baseline\nresults using different classifiers ranging from traditional machine learning\nto deep learning based approaches. The subject independent analysis is\nperformed so that it can be generalized to new users. The problem of engagement\nprediction is modeled as a weakly supervised learning problem. The dataset is\nmanually annotated by different labelers for four levels of engagement\nindependently and the correlation studies between annotated and predicted\nlabels of videos by different classifiers is reported. This dataset creation is\nan effort to facilitate research in various e-learning environments such as\nintelligent tutoring systems, MOOCs, and others.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 07:42:15 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 07:05:07 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 05:11:56 GMT"}, {"version": "v4", "created": "Tue, 26 Jun 2018 21:32:04 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Kaur", "Amanjot", ""], ["Mustafa", "Aamir", ""], ["Mehta", "Love", ""], ["Dhall", "Abhinav", ""]]}, {"id": "1804.00861", "submitter": "Dianqi Li", "authors": "Dianqi Li, Qiuyuan Huang, Xiaodong He, Lei Zhang, Ming-Ting Sun", "title": "Generating Diverse and Accurate Visual Captions by Comparative\n  Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to generate captions that are not only accurate in describing an\nimage but also discriminative across different images. The problem is both\nfundamental and interesting, as most machine-generated captions, despite\nphenomenal research progresses in the past several years, are expressed in a\nvery monotonic and featureless format. While such captions are normally\naccurate, they often lack important characteristics in human languages -\ndistinctiveness for each caption and diversity for different images. To address\nthis problem, we propose a novel conditional generative adversarial network for\ngenerating diverse captions across images. Instead of estimating the quality of\na caption solely on one image, the proposed comparative adversarial learning\nframework better assesses the quality of captions by comparing a set of\ncaptions within the image-caption joint space. By contrasting with\nhuman-written captions and image-mismatched captions, the caption generator\neffectively exploits the inherent characteristics of human languages, and\ngenerates more discriminative captions. We show that our proposed network is\ncapable of producing accurate and diverse captions across images.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 08:06:33 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 08:05:47 GMT"}, {"version": "v3", "created": "Sun, 10 Mar 2019 07:01:55 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Li", "Dianqi", ""], ["Huang", "Qiuyuan", ""], ["He", "Xiaodong", ""], ["Zhang", "Lei", ""], ["Sun", "Ming-Ting", ""]]}, {"id": "1804.00863", "submitter": "Maxim Maximov", "authors": "Maxim Maximov, Laura Leal-Taix\\'e, Mario Fritz, Tobias Ritschel", "title": "Deep Appearance Maps", "comments": null, "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep representation of appearance, i. e., the relation of color,\nsurface orientation, viewer position, material and illumination. Previous\napproaches have useddeep learning to extract classic appearance\nrepresentationsrelating to reflectance model parameters (e. g., Phong)\norillumination (e. g., HDR environment maps). We suggest todirectly represent\nappearance itself as a network we call aDeep Appearance Map (DAM). This is a 4D\ngeneralizationover 2D reflectance maps, which held the view direction fixed.\nFirst, we show how a DAM can be learned from images or video frames and later\nbe used to synthesize appearance, given new surface orientations and viewer\npositions. Second, we demonstrate how another network can be used to map from\nan image or video frames to a DAM network to reproduce this appearance, without\nusing a lengthy optimization such as stochastic gradient descent\n(learning-to-learn). Finally, we show the example of an appearance\nestimation-and-segmentation task, mapping from an image showingmultiple\nmaterials to multiple deep appearance maps.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 08:17:38 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 13:15:36 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 06:31:33 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Maximov", "Maxim", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Fritz", "Mario", ""], ["Ritschel", "Tobias", ""]]}, {"id": "1804.00872", "submitter": "Yanwei Pang", "authors": "Jiale Cao, Yanwei Pang, and Xuelong Li", "title": "Exploring Multi-Branch and High-Level Semantic Networks for Improving\n  Pedestrian Detection", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To better detect pedestrians of various scales, deep multi-scale methods\nusually detect pedestrians of different scales by different in-network layers.\nHowever, the semantic levels of features from different layers are usually\ninconsistent. In this paper, we propose a multi-branch and high-level semantic\nnetwork by gradually splitting a base network into multiple different branches.\nAs a result, the different branches have the same depth and the output features\nof different branches have similarly high-level semantics. Due to the\ndifference of receptive fields, the different branches are suitable to detect\npedestrians of different scales. Meanwhile, the multi-branch network does not\nintroduce additional parameters by sharing convolutional weights of different\nbranches. To further improve detection performance, skip-layer connections\namong different branches are used to add context to the branch of relatively\nsmall receptive filed, and dilated convolution is incorporated into part\nbranches to enlarge the resolutions of output feature maps. When they are\nembedded into Faster RCNN architecture, the weighted scores of proposal\ngeneration network and proposal classification network are further proposed.\nExperiments on KITTI dataset, Caltech pedestrian dataset, and Citypersons\ndataset demonstrate the effectiveness of proposed method. On these pedestrian\ndatasets, the proposed method achieves state-of-the-art detection performance.\nMoreover, experiments on COCO benchmark show the proposed method is also\nsuitable for general object detection.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 08:52:50 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Cao", "Jiale", ""], ["Pang", "Yanwei", ""], ["Li", "Xuelong", ""]]}, {"id": "1804.00874", "submitter": "Michael Bloesch", "authors": "Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger,\n  Andrew J. Davison", "title": "CodeSLAM - Learning a Compact, Optimisable Representation for Dense\n  Visual SLAM", "comments": "Published in Proceedings of the IEEE Conference on Computer Vision\n  and Pattern Recognition (CVPR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The representation of geometry in real-time 3D perception systems continues\nto be a critical research issue. Dense maps capture complete surface shape and\ncan be augmented with semantic labels, but their high dimensionality makes them\ncomputationally costly to store and process, and unsuitable for rigorous\nprobabilistic inference. Sparse feature-based representations avoid these\nproblems, but capture only partial scene information and are mainly useful for\nlocalisation only.\n  We present a new compact but dense representation of scene geometry which is\nconditioned on the intensity data from a single image and generated from a code\nconsisting of a small number of parameters. We are inspired by work both on\nlearned depth from images, and auto-encoders. Our approach is suitable for use\nin a keyframe-based monocular dense SLAM system: While each keyframe with a\ncode can produce a depth map, the code can be optimised efficiently jointly\nwith pose variables and together with the codes of overlapping keyframes to\nattain global consistency. Conditioning the depth map on the image allows the\ncode to only represent aspects of the local geometry which cannot directly be\npredicted from the image. We explain how to learn our code representation, and\ndemonstrate its advantageous properties in monocular SLAM.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 09:00:42 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 11:54:05 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Bloesch", "Michael", ""], ["Czarnowski", "Jan", ""], ["Clark", "Ronald", ""], ["Leutenegger", "Stefan", ""], ["Davison", "Andrew J.", ""]]}, {"id": "1804.00880", "submitter": "Yanzhao Zhou", "authors": "Yanzhao Zhou, Yi Zhu, Qixiang Ye, Qiang Qiu and Jianbin Jiao", "title": "Weakly Supervised Instance Segmentation using Class Peak Response", "comments": "Accepted in CVPR 2018 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised instance segmentation with image-level labels, instead of\nexpensive pixel-level masks, remains unexplored. In this paper, we tackle this\nchallenging problem by exploiting class peak responses to enable a\nclassification network for instance mask extraction. With image labels\nsupervision only, CNN classifiers in a fully convolutional manner can produce\nclass response maps, which specify classification confidence at each image\nlocation. We observed that local maximums, i.e., peaks, in a class response map\ntypically correspond to strong visual cues residing inside each instance.\nMotivated by this, we first design a process to stimulate peaks to emerge from\na class response map. The emerged peaks are then back-propagated and\neffectively mapped to highly informative regions of each object instance, such\nas instance boundaries. We refer to the above maps generated from class peak\nresponses as Peak Response Maps (PRMs). PRMs provide a fine-detailed\ninstance-level representation, which allows instance masks to be extracted even\nwith some off-the-shelf methods. To the best of our knowledge, we for the first\ntime report results for the challenging image-level supervised instance\nsegmentation task. Extensive experiments show that our method also boosts\nweakly supervised pointwise localization as well as semantic segmentation\nperformance, and reports state-of-the-art results on popular benchmarks,\nincluding PASCAL VOC 2012 and MS COCO.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 09:29:30 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Zhou", "Yanzhao", ""], ["Zhu", "Yi", ""], ["Ye", "Qixiang", ""], ["Qiu", "Qiang", ""], ["Jiao", "Jianbin", ""]]}, {"id": "1804.00884", "submitter": "Simone Meyer", "authors": "Simone Meyer, Abdelaziz Djelouah, Brian McWilliams, Alexander\n  Sorkine-Hornung, Markus Gross, Christopher Schroers", "title": "PhaseNet for Video Frame Interpolation", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most approaches for video frame interpolation require accurate dense\ncorrespondences to synthesize an in-between frame. Therefore, they do not\nperform well in challenging scenarios with e.g. lighting changes or motion\nblur. Recent deep learning approaches that rely on kernels to represent motion\ncan only alleviate these problems to some extent. In those cases, methods that\nuse a per-pixel phase-based motion representation have been shown to work well.\nHowever, they are only applicable for a limited amount of motion. We propose a\nnew approach, PhaseNet, that is designed to robustly handle challenging\nscenarios while also coping with larger motion. Our approach consists of a\nneural network decoder that directly estimates the phase decomposition of the\nintermediate frame. We show that this is superior to the hand-crafted\nheuristics previously used in phase-based methods and also compares favorably\nto recent deep learning based approaches for video frame interpolation on\nchallenging datasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 09:41:36 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Meyer", "Simone", ""], ["Djelouah", "Abdelaziz", ""], ["McWilliams", "Brian", ""], ["Sorkine-Hornung", "Alexander", ""], ["Gross", "Markus", ""], ["Schroers", "Christopher", ""]]}, {"id": "1804.00887", "submitter": "Wenhao Jiang", "authors": "Wenhao Jiang, Lin Ma, Xinpeng Chen, Hanwang Zhang, Wei Liu", "title": "Learning to Guide Decoding for Image Captioning", "comments": "AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, much advance has been made in image captioning, and an\nencoder-decoder framework has achieved outstanding performance for this task.\nIn this paper, we propose an extension of the encoder-decoder framework by\nadding a component called guiding network. The guiding network models the\nattribute properties of input images, and its output is leveraged to compose\nthe input of the decoder at each time step. The guiding network can be plugged\ninto the current encoder-decoder framework and trained in an end-to-end manner.\nHence, the guiding vector can be adaptively learned according to the signal\nfrom the decoder, making itself to embed information from both image and\nlanguage. Additionally, discriminative supervision can be employed to further\nimprove the quality of guidance. The advantages of our proposed approach are\nverified by experiments carried out on the MS COCO dataset.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 09:50:06 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Jiang", "Wenhao", ""], ["Ma", "Lin", ""], ["Chen", "Xinpeng", ""], ["Zhang", "Hanwang", ""], ["Liu", "Wei", ""]]}, {"id": "1804.00892", "submitter": "Yazan Abu Farha", "authors": "Yazan Abu Farha, Alexander Richard, Juergen Gall", "title": "When will you do what? - Anticipating Temporal Occurrences of Activities", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing human actions in videos has gained increased attention recently.\nWhile most works focus on classifying and labeling observed video frames or\nanticipating the very recent future, making long-term predictions over more\nthan just a few seconds is a task with many practical applications that has not\nyet been addressed. In this paper, we propose two methods to predict a\nconsiderably large amount of future actions and their durations. Both, a CNN\nand an RNN are trained to learn future video labels based on previously seen\ncontent. We show that our methods generate accurate predictions of the future\neven for long videos with a huge amount of different actions and can even deal\nwith noisy or erroneous input information.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 09:59:46 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Farha", "Yazan Abu", ""], ["Richard", "Alexander", ""], ["Gall", "Juergen", ""]]}, {"id": "1804.00908", "submitter": "Andr\\'e Klein", "authors": "Andr\\'e Klein, Jan Warszawski, Jens Hillenga{\\ss}, Klaus H. Maier-Hein", "title": "Towards whole-body CT Bone Segmentation", "comments": "Accepted conference paper at BVM 2018", "journal-ref": null, "doi": "10.1007/978-3-662-56537-7_59", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bone segmentation from CT images is a task that has been worked on for\ndecades. It is an important ingredient to several diagnostics or treatment\nplanning approaches and relevant to various diseases. As high-quality manual\nand semi-automatic bone segmentation is very time-consuming, a reliable and\nfully automatic approach would be of great interest in many scenarios. In this\npublication, we propose a UNet inspired architecture to address the task using\nDeep Learning. We evaluated the approach on whole-body CT scans of patients\nsuffering from multiple myeloma. As the disease decomposes the bone, an\naccurate segmentation is of utmost importance for the evaluation of bone\ndensity, disease staging and localization of focal lesions. The method was\nevaluated on an in-house data-set of 6000 2D image slices taken from 15\nwhole-body CT scans, achieving a dice score of 0.96 and an IOU of 0.94.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 11:07:50 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Klein", "Andr\u00e9", ""], ["Warszawski", "Jan", ""], ["Hillenga\u00df", "Jens", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1804.00931", "submitter": "Yu-Syuan Xu", "authors": "Yu-Syuan Xu, Tsu-Jui Fu, Hsuan-Kung Yang, and Chun-Yi Lee", "title": "Dynamic Video Segmentation Network", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a detailed design of dynamic video segmentation\nnetwork (DVSNet) for fast and efficient semantic video segmentation. DVSNet\nconsists of two convolutional neural networks: a segmentation network and a\nflow network. The former generates highly accurate semantic segmentations, but\nis deeper and slower. The latter is much faster than the former, but its output\nrequires further processing to generate less accurate semantic segmentations.\nWe explore the use of a decision network to adaptively assign different frame\nregions to different networks based on a metric called expected confidence\nscore. Frame regions with a higher expected confidence score traverse the flow\nnetwork. Frame regions with a lower expected confidence score have to pass\nthrough the segmentation network. We have extensively performed experiments on\nvarious configurations of DVSNet, and investigated a number of variants for the\nproposed decision network. The experimental results show that our DVSNet is\nable to achieve up to 70.4% mIoU at 19.8 fps on the Cityscape dataset. A high\nspeed version of DVSNet is able to deliver an fps of 30.4 with 63.2% mIoU on\nthe same dataset. DVSNet is also able to reduce up to 95% of the computational\nworkloads.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 12:36:14 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 12:11:48 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Xu", "Yu-Syuan", ""], ["Fu", "Tsu-Jui", ""], ["Yang", "Hsuan-Kung", ""], ["Lee", "Chun-Yi", ""]]}, {"id": "1804.00946", "submitter": "Wenjie Pei", "authors": "Wenjie Pei, David M.J. Tax", "title": "Unsupervised Learning of Sequence Representations by Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence data is challenging for machine learning approaches, because the\nlengths of the sequences may vary between samples. In this paper, we present an\nunsupervised learning model for sequence data, called the Integrated Sequence\nAutoencoder (ISA), to learn a fixed-length vectorial representation by\nminimizing the reconstruction error. Specifically, we propose to integrate two\nclassical mechanisms for sequence reconstruction which takes into account both\nthe global silhouette information and the local temporal dependencies.\nFurthermore, we propose a stop feature that serves as a temporal stamp to guide\nthe reconstruction process, which results in a higher-quality representation.\nThe learned representation is able to effectively summarize not only the\napparent features, but also the underlying and high-level style information.\nTake for example a speech sequence sample: our ISA model can not only recognize\nthe spoken text (apparent feature), but can also discriminate the speaker who\nutters the audio (more high-level style). One promising application of the ISA\nmodel is that it can be readily used in the semi-supervised learning scenario,\nin which a large amount of unlabeled data is leveraged to extract high-quality\nsequence representations and thus to improve the performance of the subsequent\nsupervised learning tasks on limited labeled data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 13:12:45 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 22:31:09 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Pei", "Wenjie", ""], ["Tax", "David M. J.", ""]]}, {"id": "1804.00970", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl, Thiago Santini, Thomas Kuebler, Nora Castner, Wolfgang\n  Rosenstiel, Enkelejda Kasneci", "title": "Eye movement simulation and detector creation to reduce laborious\n  parameter adjustments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movements hold information about human perception, intention and\ncognitive state. Various algorithms have been proposed to identify and\ndistinguish eye movements, particularly fixations, saccades, and smooth\npursuits. A major drawback of existing algorithms is that they rely on accurate\nand constant sampling rates, impeding straightforward adaptation to new\nmovements such as micro saccades. We propose a novel eye movement simulator\nthat i) probabilistically simulates saccade movements as gamma distributions\nconsidering different peak velocities and ii) models smooth pursuit onsets with\nthe sigmoid function. This simulator is combined with a machine learning\napproach to create detectors for general and specific velocity profiles.\nAdditionally, our approach is capable of using any sampling rate, even with\nfluctuations. The machine learning approach consists of different binary\npatterns combined using conditional distributions. The simulation is evaluated\nagainst publicly available real data using a squared error, and the detectors\nare evaluated against state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 06:48:37 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Santini", "Thiago", ""], ["Kuebler", "Thomas", ""], ["Castner", "Nora", ""], ["Rosenstiel", "Wolfgang", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "1804.01005", "submitter": "Xiangyu Zhu", "authors": "Xiangyu Zhu, Xiaoming Liu, Zhen Lei, Stan Z. Li", "title": "Face Alignment in Full Pose Range: A 3D Total Solution", "comments": "Published by IEEE TPAMI in 28 November 2017. arXiv admin note: text\n  overlap with arXiv:1511.07212", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2778152", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face alignment, which fits a face model to an image and extracts the semantic\nmeanings of facial pixels, has been an important topic in the computer vision\ncommunity. However, most algorithms are designed for faces in small to medium\nposes (yaw angle is smaller than 45 degrees), which lack the ability to align\nfaces in large poses up to 90 degrees. The challenges are three-fold. Firstly,\nthe commonly used landmark face model assumes that all the landmarks are\nvisible and is therefore not suitable for large poses. Secondly, the face\nappearance varies more drastically across large poses, from the frontal view to\nthe profile view. Thirdly, labelling landmarks in large poses is extremely\nchallenging since the invisible landmarks have to be guessed. In this paper, we\npropose to tackle these three challenges in an new alignment framework termed\n3D Dense Face Alignment (3DDFA), in which a dense 3D Morphable Model (3DMM) is\nfitted to the image via Cascaded Convolutional Neural Networks. We also utilize\n3D information to synthesize face images in profile views to provide abundant\nsamples for training. Experiments on the challenging AFLW database show that\nthe proposed approach achieves significant improvements over the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 07:49:19 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Zhu", "Xiangyu", ""], ["Liu", "Xiaoming", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "1804.01050", "submitter": "Garoe Dorta", "authors": "Garoe Dorta, Sara Vicente, Lourdes Agapito, Neill D.F. Campbell, Ivor\n  Simpson", "title": "Training VAEs Under Structured Residuals", "comments": "Simplified training methodology, added more results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational auto-encoders (VAEs) are a popular and powerful deep generative\nmodel. Previous works on VAEs have assumed a factorized likelihood model,\nwhereby the output uncertainty of each pixel is assumed to be independent. This\napproximation is clearly limited as demonstrated by observing a residual image\nfrom a VAE reconstruction, which often possess a high level of structure. This\npaper demonstrates a novel scheme to incorporate a structured Gaussian\nlikelihood prediction network within the VAE that allows the residual\ncorrelations to be modeled. Our novel architecture, with minimal increase in\ncomplexity, incorporates the covariance matrix prediction within the VAE. We\nalso propose a new mechanism for allowing structured uncertainty on color\nimages. Furthermore, we provide a scheme for effectively training this model,\nand include some suggestions for improving performance in terms of efficiency\nor modeling longer range correlations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 16:04:22 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 19:00:03 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 16:53:19 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Dorta", "Garoe", ""], ["Vicente", "Sara", ""], ["Agapito", "Lourdes", ""], ["Campbell", "Neill D. F.", ""], ["Simpson", "Ivor", ""]]}, {"id": "1804.01077", "submitter": "Krishna Kumar Singh", "authors": "Krishna Kumar Singh, Santosh Divvala, Ali Farhadi, Yong Jae Lee", "title": "DOCK: Detecting Objects by transferring Common-sense Knowledge", "comments": null, "journal-ref": "ECCV, 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable approach for Detecting Objects by transferring\nCommon-sense Knowledge (DOCK) from source to target categories. In our setting,\nthe training data for the source categories have bounding box annotations,\nwhile those for the target categories only have image-level annotations.\nCurrent state-of-the-art approaches focus on image-level visual or semantic\nsimilarity to adapt a detector trained on the source categories to the new\ntarget categories. In contrast, our key idea is to (i) use similarity not at\nthe image-level, but rather at the region-level, and (ii) leverage richer\ncommon-sense (based on attribute, spatial, etc.) to guide the algorithm towards\nlearning the correct detections. We acquire such common-sense cues\nautomatically from readily-available knowledge bases without any extra human\neffort. On the challenging MS COCO dataset, we find that common-sense knowledge\ncan substantially improve detection performance over existing transfer-learning\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 17:41:53 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 06:42:30 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Singh", "Krishna Kumar", ""], ["Divvala", "Santosh", ""], ["Farhadi", "Ali", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1804.01110", "submitter": "Helge Rhodin", "authors": "Helge Rhodin and Mathieu Salzmann and Pascal Fua", "title": "Unsupervised Geometry-Aware Representation for 3D Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern 3D human pose estimation techniques rely on deep networks, which\nrequire large amounts of training data. While weakly-supervised methods require\nless supervision, by utilizing 2D poses or multi-view imagery without\nannotations, they still need a sufficiently large set of samples with 3D\nannotations for learning to succeed.\n  In this paper, we propose to overcome this problem by learning a\ngeometry-aware body representation from multi-view images without annotations.\nTo this end, we use an encoder-decoder that predicts an image from one\nviewpoint given an image from another viewpoint. Because this representation\nencodes 3D geometry, using it in a semi-supervised setting makes it easier to\nlearn a mapping from it to 3D human pose. As evidenced by our experiments, our\napproach significantly outperforms fully-supervised methods given the same\namount of labeled data, and improves over other semi-supervised methods while\nusing as little as 1% of the labeled data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 18:01:54 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Rhodin", "Helge", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1804.01117", "submitter": "Christian A. Mueller", "authors": "Christian A. Mueller and Andreas Birk", "title": "Visual Object Categorization Based on Hierarchical Shape Motifs Learned\n  From Noisy Point Cloud Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object shape is a key cue that contributes to the semantic understanding of\nobjects. In this work we focus on the categorization of real-world object point\nclouds to particular shape types. Therein surface description and\nrepresentation of object shape structure have significant influence on shape\ncategorization accuracy, when dealing with real-world scenes featuring noisy,\npartial and occluded object observations. An unsupervised hierarchical learning\nprocedure is utilized here to symbolically describe surface characteristics on\nmultiple semantic levels. Furthermore, a constellation model is proposed that\nhierarchically decomposes objects. The decompositions are described as\nconstellations of symbols (shape motifs) in a gradual order, hence reflecting\nshape structure from local to global, i.e., from parts over groups of parts to\nentire objects. The combination of this multi-level description of surfaces and\nthe hierarchical decomposition of shapes leads to a representation which allows\nto conceptualize shapes. An object discrimination has been observed in\nexperiments with seven categories featuring instances with sensor noise,\nocclusions as well as inter-category and intra-category similarities.\nExperiments include the evaluation of the proposed description and shape\ndecomposition approach, and comparisons to Fast Point Feature Histograms, a\nVocabulary Tree and a neural network-based Deep Learning method. Furthermore,\nexperiments are conducted with alternative datasets which analyze the\ngeneralization capability of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 18:21:31 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Mueller", "Christian A.", ""], ["Birk", "Andreas", ""]]}, {"id": "1804.01118", "submitter": "Yaroslav Ganin", "authors": "Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S.M. Ali Eslami,\n  Oriol Vinyals", "title": "Synthesizing Programs for Images using Reinforced Adversarial Learning", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep generative networks have led to impressive results in recent\nyears. Nevertheless, such models can often waste their capacity on the minutiae\nof datasets, presumably due to weak inductive biases in their decoders. This is\nwhere graphics engines may come in handy since they abstract away low-level\ndetails and represent images as high-level programs. Current methods that\ncombine deep learning and renderers are limited by hand-crafted likelihood or\ndistance functions, a need for large amounts of supervision, or difficulties in\nscaling their inference algorithms to richer datasets. To mitigate these\nissues, we present SPIRAL, an adversarially trained agent that generates a\nprogram which is executed by a graphics engine to interpret and sample images.\nThe goal of this agent is to fool a discriminator network that distinguishes\nbetween real and rendered data, trained with a distributed reinforcement\nlearning setup without any supervision. A surprising finding is that using the\ndiscriminator's output as a reward signal is the key to allow the agent to make\nmeaningful progress at matching the desired output rendering. To the best of\nour knowledge, this is the first demonstration of an end-to-end, unsupervised\nand adversarial inverse graphics agent on challenging real world (MNIST,\nOmniglot, CelebA) and synthetic 3D datasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 18:25:42 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Ganin", "Yaroslav", ""], ["Kulkarni", "Tejas", ""], ["Babuschkin", "Igor", ""], ["Eslami", "S. M. Ali", ""], ["Vinyals", "Oriol", ""]]}, {"id": "1804.01142", "submitter": "Anas Al-Oraiqat Dr.", "authors": "Anas M. Al-Oraiqat, Natalya S. Kostyukova", "title": "A Modified Image Comparison Algorithm Using Histogram Features", "comments": "8 pages, 7 figures", "journal-ref": "International Journal of advanced studies in Computer Science and\n  Engineering, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discuss the problem of color image content comparison.\nParticularly, methods of image content comparison are analyzed, restrictions of\ncolor histogram are described and a modified method of images content\ncomparison is proposed. This method uses the color histograms and considers\ncolor locations. Testing and analyzing of based and modified algorithms are\nperformed. The modified method shows 97% average precision for a collection\ncontaining about 700 images without loss of the advantages of based method,\ni.e. scale and rotation invariant.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 19:41:00 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Al-Oraiqat", "Anas M.", ""], ["Kostyukova", "Natalya S.", ""]]}, {"id": "1804.01159", "submitter": "Rajeev Ranjan", "authors": "Rajeev Ranjan, Ankan Bansal, Hongyu Xu, Swami Sankaranarayanan,\n  Jun-Cheng Chen, Carlos D. Castillo and Rama Chellappa", "title": "Crystal Loss and Quality Pooling for Unconstrained Face Verification and\n  Recognition", "comments": "Previously portions of this work appeared in arXiv:1703.09507, which\n  was a conference version. This version is an extended journal version of it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the performance of face verification and recognition systems\nbased on deep convolutional neural networks (DCNNs) has significantly improved.\nA typical pipeline for face verification includes training a deep network for\nsubject classification with softmax loss, using the penultimate layer output as\nthe feature descriptor, and generating a cosine similarity score given a pair\nof face images or videos. The softmax loss function does not optimize the\nfeatures to have higher similarity score for positive pairs and lower\nsimilarity score for negative pairs, which leads to a performance gap. In this\npaper, we propose a new loss function, called Crystal Loss, that restricts the\nfeatures to lie on a hypersphere of a fixed radius. The loss can be easily\nimplemented using existing deep learning frameworks. We show that integrating\nthis simple step in the training pipeline significantly improves the\nperformance of face verification and recognition systems. We achieve\nstate-of-the-art performance for face verification and recognition on\nchallenging LFW, IJB-A, IJB-B and IJB-C datasets over a large range of false\nalarm rates (10-1 to 10-7).\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 20:30:25 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 03:13:42 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Ranjan", "Rajeev", ""], ["Bansal", "Ankan", ""], ["Xu", "Hongyu", ""], ["Sankaranarayanan", "Swami", ""], ["Chen", "Jun-Cheng", ""], ["Castillo", "Carlos D.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1804.01174", "submitter": "Srujana Gattupalli", "authors": "Srujana Gattupalli, Ashwin Ramesh Babu, James Robert Brady, Fillia\n  Makedon, Vassilis Athitsos", "title": "Towards Deep Learning based Hand Keypoints Detection for Rapid\n  Sequential Movements from RGB Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand keypoints detection and pose estimation has numerous applications in\ncomputer vision, but it is still an unsolved problem in many aspects. An\napplication of hand keypoints detection is in performing cognitive assessments\nof a subject by observing the performance of that subject in physical tasks\ninvolving rapid finger motion. As a part of this work, we introduce a novel\nhand key-points benchmark dataset that consists of hand gestures recorded\nspecifically for cognitive behavior monitoring. We explore the state of the art\nmethods in hand keypoint detection and we provide quantitative evaluations for\nthe performance of these methods on our dataset. In future, these results and\nour dataset can serve as a useful benchmark for hand keypoint recognition for\nrapid finger movements.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 21:28:16 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Gattupalli", "Srujana", ""], ["Babu", "Ashwin Ramesh", ""], ["Brady", "James Robert", ""], ["Makedon", "Fillia", ""], ["Athitsos", "Vassilis", ""]]}, {"id": "1804.01176", "submitter": "Kevan Yuen", "authors": "Kevan Yuen and Mohan M. Trivedi", "title": "Looking at Hands in Autonomous Vehicles: A ConvNet Approach using Part\n  Affinity Fields", "comments": "11 pages, 8 figures, 1 table. Submitted to \"IEEE Transactions on\n  Intelligent Vehicles\" (under review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of autonomous driving, where humans may need to take over in\nthe event where the computer may issue a takeover request, a key step towards\ndriving safety is the monitoring of the hands to ensure the driver is ready for\nsuch a request. This work, focuses on the first step of this process, which is\nto locate the hands. Such a system must work in real-time and under varying\nharsh lighting conditions. This paper introduces a fast ConvNet approach, based\non the work of original work of OpenPose for full body joint estimation. The\nnetwork is modified with fewer parameters and retrained using our own day-time\nnaturalistic autonomous driving dataset to estimate joint and affinity heatmaps\nfor driver & passenger's wrist and elbows, for a total of 8 joint classes and\npart affinity fields between each wrist-elbow pair. The approach runs real-time\non real-world data at 40 fps on multiple drivers and passengers. The system is\nextensively evaluated both quantitatively and qualitatively, showing at least\n95% detection performance on joint localization and arm-angle estimation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 21:45:14 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Yuen", "Kevan", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1804.01194", "submitter": "Pichao Wang", "authors": "Pichao Wang and Wanqing Li and Zhimin Gao and Chang Tang and Philip\n  Ogunbona", "title": "Depth Pooling Based Large-scale 3D Action Recognition with Convolutional\n  Neural Networks", "comments": "arXiv admin note: text overlap with arXiv:1701.01814,\n  arXiv:1608.06338", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes three simple, compact yet effective representations of\ndepth sequences, referred to respectively as Dynamic Depth Images (DDI),\nDynamic Depth Normal Images (DDNI) and Dynamic Depth Motion Normal Images\n(DDMNI), for both isolated and continuous action recognition. These dynamic\nimages are constructed from a segmented sequence of depth maps using\nhierarchical bidirectional rank pooling to effectively capture the\nspatial-temporal information. Specifically, DDI exploits the dynamics of\npostures over time and DDNI and DDMNI exploit the 3D structural information\ncaptured by depth maps. Upon the proposed representations, a ConvNet based\nmethod is developed for action recognition. The image-based representations\nenable us to fine-tune the existing Convolutional Neural Network (ConvNet)\nmodels trained on image data without training a large number of parameters from\nscratch. The proposed method achieved the state-of-art results on three large\ndatasets, namely, the Large-scale Continuous Gesture Recognition Dataset (means\nJaccard index 0.4109), the Large-scale Isolated Gesture Recognition Dataset\n(59.21%), and the NTU RGB+D Dataset (87.08% cross-subject and 84.22%\ncross-view) even though only the depth modality was used.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 23:30:34 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 23:46:47 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Wang", "Pichao", ""], ["Li", "Wanqing", ""], ["Gao", "Zhimin", ""], ["Tang", "Chang", ""], ["Ogunbona", "Philip", ""]]}, {"id": "1804.01203", "submitter": "Min Xu", "authors": "Yixiu Zhao, Xiangrui Zeng, Qiang Guo, Min Xu", "title": "An integration of fast alignment and maximum-likelihood methods for\n  electron subtomogram averaging and classification", "comments": "17 pages", "journal-ref": "Intelligent Systems for Molecular Biology (ISMB) 2018,\n  Bioinformatics", "doi": "10.1093/bioinformatics/bty267", "report-no": null, "categories": "q-bio.QM cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Cellular Electron CryoTomography (CECT) is an emerging 3D imaging\ntechnique that visualizes subcellular organization of single cells at\nsubmolecular resolution and in near-native state. CECT captures large numbers\nof macromolecular complexes of highly diverse structures and abundances.\nHowever, the structural complexity and imaging limits complicate the systematic\nde novo structural recovery and recognition of these macromolecular complexes.\nEfficient and accurate reference-free subtomogram averaging and classification\nrepresent the most critical tasks for such analysis. Existing subtomogram\nalignment based methods are prone to the missing wedge effects and low\nsignal-to-noise ratio (SNR). Moreover, existing maximum-likelihood based\nmethods rely on integration operations, which are in principle computationally\ninfeasible for accurate calculation.\n  Results: Built on existing works, we propose an integrated method, Fast\nAlignment Maximum Likelihood method (FAML), which uses fast subtomogram\nalignment to sample sub-optimal rigid transformations. The transformations are\nthen used to approximate integrals for maximum-likelihood update of subtomogram\naverages through expectation-maximization algorithm. Our tests on simulated and\nexperimental subtomograms showed that, compared to our previously developed\nfast alignment method (FA), FAML is significantly more robust to noise and\nmissing wedge effects with moderate increases of computation cost.Besides, FAML\nperforms well with significantly fewer input subtomograms when the FA method\nfails. Therefore, FAML can serve as a key component for improved construction\nof initial structural models from macromolecules captured by CECT.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 01:16:20 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Zhao", "Yixiu", ""], ["Zeng", "Xiangrui", ""], ["Guo", "Qiang", ""], ["Xu", "Min", ""]]}, {"id": "1804.01210", "submitter": "Xinghao Ding", "authors": "Zhiwen Fan, Liyan Sun, Xinghao Ding, Yue Huang, Congbo Cai, John\n  Paisley", "title": "A Segmentation-aware Deep Fusion Network for Compressed Sensing MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing MRI is a classic inverse problem in the field of\ncomputational imaging, accelerating the MR imaging by measuring less k-space\ndata. The deep neural network models provide the stronger representation\nability and faster reconstruction compared with \"shallow\" optimization-based\nmethods. However, in the existing deep-based CS-MRI models, the high-level\nsemantic supervision information from massive segmentation-labels in MRI\ndataset is overlooked. In this paper, we proposed a segmentation-aware deep\nfusion network called SADFN for compressed sensing MRI. The multilayer feature\naggregation (MLFA) method is introduced here to fuse all the features from\ndifferent layers in the segmentation network. Then, the aggregated feature maps\ncontaining semantic information are provided to each layer in the\nreconstruction network with a feature fusion strategy. This guarantees the\nreconstruction network is aware of the different regions in the image it\nreconstructs, simplifying the function mapping. We prove the utility of the\ncross-layer and cross-task information fusion strategy by comparative study.\nExtensive experiments on brain segmentation benchmark MRBrainS validated that\nthe proposed SADFN model achieves state-of-the-art accuracy in compressed\nsensing MRI. This paper provides a novel approach to guide the low-level visual\ntask using the information from mid- or high-level task.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 02:10:58 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Fan", "Zhiwen", ""], ["Sun", "Liyan", ""], ["Ding", "Xinghao", ""], ["Huang", "Yue", ""], ["Cai", "Congbo", ""], ["Paisley", "John", ""]]}, {"id": "1804.01223", "submitter": "Dacheng Tao", "authors": "Chao Li and Cheng Deng and Ning Li and Wei Liu and Xinbo Gao and\n  Dacheng Tao", "title": "Self-Supervised Adversarial Hashing Networks for Cross-Modal Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the success of deep learning, cross-modal retrieval has made\nsignificant progress recently. However, there still remains a crucial\nbottleneck: how to bridge the modality gap to further enhance the retrieval\naccuracy. In this paper, we propose a self-supervised adversarial hashing\n(\\textbf{SSAH}) approach, which lies among the early attempts to incorporate\nadversarial learning into cross-modal hashing in a self-supervised fashion. The\nprimary contribution of this work is that two adversarial networks are\nleveraged to maximize the semantic correlation and consistency of the\nrepresentations between different modalities. In addition, we harness a\nself-supervised semantic network to discover high-level semantic information in\nthe form of multi-label annotations. Such information guides the feature\nlearning process and preserves the modality relationships in both the common\nsemantic space and the Hamming space. Extensive experiments carried out on\nthree benchmark datasets validate that the proposed SSAH surpasses the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 03:03:47 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Li", "Chao", ""], ["Deng", "Cheng", ""], ["Li", "Ning", ""], ["Liu", "Wei", ""], ["Gao", "Xinbo", ""], ["Tao", "Dacheng", ""]]}, {"id": "1804.01233", "submitter": "Liu Liu", "authors": "Liu Liu, Hairong Qi", "title": "Discriminative Cross-View Binary Representation Learning", "comments": "Published in WACV2018. Code will be available soon", "journal-ref": "WACV2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning compact representation is vital and challenging for large scale\nmultimedia data. Cross-view/cross-modal hashing for effective binary\nrepresentation learning has received significant attention with exponentially\ngrowing availability of multimedia content. Most existing cross-view hashing\nalgorithms emphasize the similarities in individual views, which are then\nconnected via cross-view similarities. In this work, we focus on the\nexploitation of the discriminative information from different views, and\npropose an end-to-end method to learn semantic-preserving and discriminative\nbinary representation, dubbed Discriminative Cross-View Hashing (DCVH), in\nlight of learning multitasking binary representation for various tasks\nincluding cross-view retrieval, image-to-image retrieval, and image\nannotation/tagging. The proposed DCVH has the following key components. First,\nit uses convolutional neural network (CNN) based nonlinear hashing functions\nand multilabel classification for both images and texts simultaneously. Such\nhashing functions achieve effective continuous relaxation during training\nwithout explicit quantization loss by using Direct Binary Embedding (DBE)\nlayers. Second, we propose an effective view alignment via Hamming distance\nminimization, which is efficiently accomplished by bit-wise XOR operation.\nExtensive experiments on two image-text benchmark datasets demonstrate that\nDCVH outperforms state-of-the-art cross-view hashing algorithms as well as\nsingle-view image hashing algorithms. In addition, DCVH can provide competitive\nperformance for image annotation/tagging.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 04:17:52 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Liu", "Liu", ""], ["Qi", "Hairong", ""]]}, {"id": "1804.01259", "submitter": "ZhiYuan Li", "authors": "Zhiyuan Li, Nanjun Teng, Min Jin, Huaxiang Lu", "title": "Building Efficient CNN Architecture for Offline Handwritten Chinese\n  Character Recognition", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional networks based methods have brought great breakthrough in\nimages classification, which provides an end-to-end solution for handwritten\nChinese character recognition(HCCR) problem through learning discriminative\nfeatures automatically. Nevertheless, state-of-the-art CNNs appear to incur\nhuge computation cost, and require the storage of a large number of parameters\nespecially in fully connected layers, which is difficult to deploy such\nnetworks into alternative hardware device with the limit of computation amount.\nTo solve the storage problem, we propose a novel technique called Global\nWeighted Arverage Pooling for reducing the parameters in fully connected layer\nwithout loss in accuracy. Besides, we implement a cascaded model in single CNN\nby adding mid output layer to complete recognition as early as possible, which\nreduces average inference time significantly. Experiments were performed on the\nICDAR-2013 offline HCCR dataset, and it is found that the proposed approach\nonly needs 6.9ms for classfying a chracter image on average, and achieves the\nstate-of-the-art accuracy of 97.1% while requiring only 3.3MB for storage.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 06:58:03 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 02:11:01 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Li", "Zhiyuan", ""], ["Teng", "Nanjun", ""], ["Jin", "Min", ""], ["Lu", "Huaxiang", ""]]}, {"id": "1804.01296", "submitter": "Benjam\\'in Guti\\'errez Becker", "authors": "Benjamin Gutierrez Becker, Tassilo Klein, Christian Wachinger", "title": "Gaussian Process Uncertainty in Age Estimation as a Measure of Brain\n  Abnormality", "comments": "Paper accepted in Neuroimage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate regression models for age estimation are a powerful tool for\nassessing abnormal brain morphology associated to neuropathology. Age\nprediction models are built on cohorts of healthy subjects and are built to\nreflect normal aging patterns. The application of these multivariate models to\ndiseased subjects usually results in high prediction errors, under the\nhypothesis that neuropathology presents a similar degenerative pattern as that\nof accelerated aging. In this work, we propose an alternative to the idea that\npathology follows a similar trajectory than normal aging. Instead, we propose\nthe use of metrics which measure deviations from the mean aging trajectory. We\npropose to measure these deviations using two different metrics: uncertainty in\na Gaussian process regression model and a newly proposed age weighted\nuncertainty measure. Consequently, our approach assumes that pathologic brain\npatterns are different to those of normal aging. We present results for\nsubjects with autism, mild cognitive impairment and Alzheimer's disease to\nhighlight the versatility of the approach to different diseases and age ranges.\nWe evaluate volume, thickness, and VBM features for quantifying brain\nmorphology. Our evaluations are performed on a large number of images obtained\nfrom a variety of publicly available neuroimaging databases. Across all\nfeatures, our uncertainty based measurements yield a better separation between\ndiseased subjects and healthy individuals than the prediction error. Finally,\nwe illustrate differences in the disease pattern to normal aging, supporting\nthe application of uncertainty as a measure of neuropathology.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 08:38:07 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Becker", "Benjamin Gutierrez", ""], ["Klein", "Tassilo", ""], ["Wachinger", "Christian", ""]]}, {"id": "1804.01306", "submitter": "Guillermo Gallego", "authors": "Guillermo Gallego, Henri Rebecq, Davide Scaramuzza", "title": "A Unifying Contrast Maximization Framework for Event Cameras, with\n  Applications to Motion, Depth, and Optical Flow Estimation", "comments": "16 pages, 16 figures. Video: https://youtu.be/KFMZFhi-9Aw", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  Salt Lake City, 2018", "doi": "10.1109/CVPR.2018.00407", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unifying framework to solve several computer vision problems\nwith event cameras: motion, depth and optical flow estimation. The main idea of\nour framework is to find the point trajectories on the image plane that are\nbest aligned with the event data by maximizing an objective function: the\ncontrast of an image of warped events. Our method implicitly handles data\nassociation between the events, and therefore, does not rely on additional\nappearance information about the scene. In addition to accurately recovering\nthe motion parameters of the problem, our framework produces motion-corrected\nedge-like images with high dynamic range that can be used for further scene\nanalysis. The proposed method is not only simple, but more importantly, it is,\nto the best of our knowledge, the first method that can be successfully applied\nto such a diverse set of important vision tasks with event cameras.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 08:59:57 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Gallego", "Guillermo", ""], ["Rebecq", "Henri", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1804.01307", "submitter": "Anjany Kumar Sekuboyina", "authors": "Anjany Sekuboyina, Markus Rempfler, Jan Kuka\\v{c}ka, Giles Tetteh,\n  Alexander Valentinitsch, Jan S. Kirschke, and Bjoern H. Menze", "title": "Btrfly Net: Vertebrae Labelling with Energy-based Adversarial Learning\n  of Local Spine Prior", "comments": "Published as conference paper in Medical Image Computing and Computer\n  Assisted Intervention - MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust localisation and identification of vertebrae is essential for\nautomated spine analysis. The contribution of this work to the task is\ntwo-fold: (1) Inspired by the human expert, we hypothesise that a sagittal and\ncoronal reformation of the spine contain sufficient information for labelling\nthe vertebrae. Thereby, we propose a butterfly-shaped network architecture\n(termed Btrfly Net) that efficiently combines the information across\nreformations. (2) Underpinning the Btrfly net, we present an energy-based\nadversarial training regime that encodes local spine structure as an anatomical\nprior into the network, thereby enabling it to achieve state-of-art performance\nin all standard metrics on a benchmark dataset of 302 scans without any\npost-processing during inference.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 09:00:33 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 16:20:52 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Sekuboyina", "Anjany", ""], ["Rempfler", "Markus", ""], ["Kuka\u010dka", "Jan", ""], ["Tetteh", "Giles", ""], ["Valentinitsch", "Alexander", ""], ["Kirschke", "Jan S.", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "1804.01310", "submitter": "Guillermo Gallego", "authors": "Ana I. Maqueda, Antonio Loquercio, Guillermo Gallego, Narciso Garcia,\n  Davide Scaramuzza", "title": "Event-based Vision meets Deep Learning on Steering Prediction for\n  Self-driving Cars", "comments": "9 pages, 8 figures, 6 tables. Video: https://youtu.be/_r_bsjkJTHA", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  Salt Lake City, 2018", "doi": "10.1109/CVPR.2018.00568", "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are bio-inspired vision sensors that naturally capture the\ndynamics of a scene, filtering out redundant information. This paper presents a\ndeep neural network approach that unlocks the potential of event cameras on a\nchallenging motion-estimation task: prediction of a vehicle's steering angle.\nTo make the best out of this sensor-algorithm combination, we adapt\nstate-of-the-art convolutional architectures to the output of event sensors and\nextensively evaluate the performance of our approach on a publicly available\nlarge scale event-camera dataset (~1000 km). We present qualitative and\nquantitative explanations of why event cameras allow robust steering prediction\neven in cases where traditional cameras fail, e.g. challenging illumination\nconditions and fast motion. Finally, we demonstrate the advantages of\nleveraging transfer learning from traditional to event-based vision, and show\nthat our approach outperforms state-of-the-art algorithms based on standard\ncameras.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 09:05:41 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Maqueda", "Ana I.", ""], ["Loquercio", "Antonio", ""], ["Gallego", "Guillermo", ""], ["Garcia", "Narciso", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1804.01322", "submitter": "Alina Marcu M.Sc", "authors": "Alina Marcu, Dragos Costea, Emil Slusanschi and Marius Leordeanu", "title": "A Multi-Stage Multi-Task Neural Network for Aerial Scene Interpretation\n  and Geolocalization", "comments": "23 pages, 11 figures. Under review at the 15th European Conference on\n  Computer Vision (ECCV 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation and vision-based geolocalization in aerial images are\nchallenging tasks in computer vision. Due to the advent of deep convolutional\nnets and the availability of relatively low cost UAVs, they are currently\ngenerating a growing attention in the field. We propose a novel multi-task\nmulti-stage neural network that is able to handle the two problems at the same\ntime, in a single forward pass. The first stage of our network predicts\npixelwise class labels, while the second stage provides a precise location\nusing two branches. One branch uses a regression network, while the other is\nused to predict a location map trained as a segmentation task. From a\nstructural point of view, our architecture uses encoder-decoder modules at each\nstage, having the same encoder structure re-used. Furthermore, its size is\nlimited to be tractable on an embedded GPU. We achieve commercial GPS-level\nlocalization accuracy from satellite images with spatial resolution of 1 square\nmeter per pixel in a city-wide area of interest. On the task of semantic\nsegmentation, we obtain state-of-the-art results on two challenging datasets,\nthe Inria Aerial Image Labeling dataset and Massachusetts Buildings.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 09:42:47 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Marcu", "Alina", ""], ["Costea", "Dragos", ""], ["Slusanschi", "Emil", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1804.01346", "submitter": "Federico Perazzi", "authors": "Meng Tang and Abdelaziz Djelouah and Federico Perazzi and Yuri Boykov\n  and Christopher Schroers", "title": "Normalized Cut Loss for Weakly-supervised CNN Segmentation", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent semantic segmentation methods train deep convolutional neural\nnetworks with fully annotated masks requiring pixel-accuracy for good quality\ntraining. Common weakly-supervised approaches generate full masks from partial\ninput (e.g. scribbles or seeds) using standard interactive segmentation methods\nas preprocessing. But, errors in such masks result in poorer training since\nstandard loss functions (e.g. cross-entropy) do not distinguish seeds from\npotentially mislabeled other pixels. Inspired by the general ideas in\nsemi-supervised learning, we address these problems via a new principled loss\nfunction evaluating network output with criteria standard in \"shallow\"\nsegmentation, e.g. normalized cut. Unlike prior work, the cross entropy part of\nour loss evaluates only seeds where labels are known while normalized cut\nsoftly evaluates consistency of all pixels. We focus on normalized cut loss\nwhere dense Gaussian kernel is efficiently implemented in linear time by fast\nBilateral filtering. Our normalized cut loss approach to segmentation brings\nthe quality of weakly-supervised training significantly closer to fully\nsupervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 11:18:21 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Tang", "Meng", ""], ["Djelouah", "Abdelaziz", ""], ["Perazzi", "Federico", ""], ["Boykov", "Yuri", ""], ["Schroers", "Christopher", ""]]}, {"id": "1804.01373", "submitter": "Xinpeng Chen", "authors": "Xinpeng Chen and Jingyuan Chen and Lin Ma and Jian Yao and Wei Liu and\n  Jiebo Luo and Tong Zhang", "title": "Fine-grained Video Attractiveness Prediction Using Multimodal Deep\n  Learning on a Large Real-world Dataset", "comments": "Accepted by WWW 2018 The Big Web Track", "journal-ref": null, "doi": "10.1145/3184558.3186584", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, billions of videos are online ready to be viewed and shared. Among\nan enormous volume of videos, some popular ones are widely viewed by online\nusers while the majority attract little attention. Furthermore, within each\nvideo, different segments may attract significantly different numbers of views.\nThis phenomenon leads to a challenging yet important problem, namely\nfine-grained video attractiveness prediction. However, one major obstacle for\nsuch a challenging problem is that no suitable benchmark dataset currently\nexists. To this end, we construct the first fine-grained video attractiveness\ndataset, which is collected from one of the most popular video websites in the\nworld. In total, the constructed FVAD consists of 1,019 drama episodes with\n780.6 hours covering different categories and a wide variety of video contents.\nApart from the large amount of videos, hundreds of millions of user behaviors\nduring watching videos are also included, such as \"view counts\",\n\"fast-forward\", \"fast-rewind\", and so on, where \"view counts\" reflects the\nvideo attractiveness while other engagements capture the interactions between\nthe viewers and videos. First, we demonstrate that video attractiveness and\ndifferent engagements present different relationships. Second, FVAD provides us\nan opportunity to study the fine-grained video attractiveness prediction\nproblem. We design different sequential models to perform video attractiveness\nprediction by relying solely on video contents. The sequential models exploit\nthe multimodal relationships between visual and audio components of the video\ncontents at different levels. Experimental results demonstrate the\neffectiveness of our proposed sequential models with different visual and audio\nrepresentations, the necessity of incorporating the two modalities, and the\ncomplementary behaviors of the sequential prediction models at different\nlevels.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 12:44:43 GMT"}, {"version": "v2", "created": "Sat, 7 Apr 2018 01:45:29 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Chen", "Xinpeng", ""], ["Chen", "Jingyuan", ""], ["Ma", "Lin", ""], ["Yao", "Jian", ""], ["Liu", "Wei", ""], ["Luo", "Jiebo", ""], ["Zhang", "Tong", ""]]}, {"id": "1804.01396", "submitter": "Tarique Anwer", "authors": "Jahanzaib Shabbir, and Tarique Anwer", "title": "Artificial Intelligence and its Role in Near Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI technology has a long history which is actively and constantly changing\nand growing. It focuses on intelligent agents, which contain devices that\nperceive the environment and based on which takes actions in order to maximize\ngoal success chances. In this paper, we will explain the modern AI basics and\nvarious representative applications of AI. In the context of the modern\ndigitalized world, AI is the property of machines, computer programs, and\nsystems to perform the intellectual and creative functions of a person,\nindependently find ways to solve problems, be able to draw conclusions and make\ndecisions. Most artificial intelligence systems have the ability to learn,\nwhich allows people to improve their performance over time. The recent research\non AI tools, including machine learning, deep learning and predictive analysis\nintended toward increasing the planning, learning, reasoning, thinking and\naction taking ability. Based on which, the proposed research intends towards\nexploring on how the human intelligence differs from the artificial\nintelligence. Moreover, we critically analyze what AI of today is capable of\ndoing, why it still cannot reach human intelligence and what are the open\nchallenges existing in front of AI to reach and outperform human level of\nintelligence. Furthermore, it will explore the future predictions for\nartificial intelligence and based on which potential solution will be\nrecommended to solve it within next decades.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 23:12:30 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Shabbir", "Jahanzaib", ""], ["Anwer", "Tarique", ""]]}, {"id": "1804.01401", "submitter": "Peng Xu", "authors": "Peng Xu, Yongye Huang, Tongtong Yuan, Kaiyue Pang, Yi-Zhe Song, Tao\n  Xiang, Timothy M. Hospedales, Zhanyu Ma, Jun Guo", "title": "SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval", "comments": "Accepted by CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep hashing framework for sketch retrieval that, for the first\ntime, works on a multi-million scale human sketch dataset. Leveraging on this\nlarge dataset, we explore a few sketch-specific traits that were otherwise\nunder-studied in prior literature. Instead of following the conventional sketch\nrecognition task, we introduce the novel problem of sketch hashing retrieval\nwhich is not only more challenging, but also offers a better testbed for\nlarge-scale sketch analysis, since: (i) more fine-grained sketch feature\nlearning is required to accommodate the large variations in style and\nabstraction, and (ii) a compact binary code needs to be learned at the same\ntime to enable efficient retrieval. Key to our network design is the embedding\nof unique characteristics of human sketch, where (i) a two-branch CNN-RNN\narchitecture is adapted to explore the temporal ordering of strokes, and (ii) a\nnovel hashing loss is specifically designed to accommodate both the temporal\nand abstract traits of sketches. By working with a 3.8M sketch dataset, we show\nthat state-of-the-art hashing models specifically engineered for static images\nfail to perform well on temporal sketch data. Our network on the other hand not\nonly offers the best retrieval performance on various code sizes, but also\nyields the best generalization performance under a zero-shot setting and when\nre-purposed for sketch recognition. Such superior performances effectively\ndemonstrate the benefit of our sketch-specific design.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 13:39:26 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Xu", "Peng", ""], ["Huang", "Yongye", ""], ["Yuan", "Tongtong", ""], ["Pang", "Kaiyue", ""], ["Song", "Yi-Zhe", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy M.", ""], ["Ma", "Zhanyu", ""], ["Guo", "Jun", ""]]}, {"id": "1804.01417", "submitter": "Lingfeng Zhang", "authors": "Lingfeng Zhang, Pengfei Dou, Ioannis A Kakadiaris", "title": "Patch-based Face Recognition using a Hierarchical Multi-label Matcher", "comments": "accepted in IVC: Biometrics in the Wild. arXiv admin note: text\n  overlap with arXiv:1803.09359", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a hierarchical multi-label matcher for patch-based face\nrecognition. In signature generation, a face image is iteratively divided into\nmulti-level patches. Two different types of patch divisions and signatures are\nintroduced for 2D facial image and texture-lifted image, respectively. The\nmatcher training consists of three steps. First, local classifiers are built to\nlearn the local matching of each patch. Second, the hierarchical relationships\ndefined between local patches are used to learn the global matching of each\npatch. Three ways are introduced to learn the global matching: majority voting,\nl1-regularized weighting, and decision rule. Last, the global matchings of\ndifferent levels are combined as the final matching. Experimental results on\ndifferent face recognition tasks demonstrate the effectiveness of the proposed\nmatcher at the cost of gallery generalization. Compared with the UR2D system,\nthe proposed matcher improves the Rank-1 accuracy significantly by 3% and 0.18%\non the UHDB31 dataset and IJB-A dataset, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 04:00:37 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Zhang", "Lingfeng", ""], ["Dou", "Pengfei", ""], ["Kakadiaris", "Ioannis A", ""]]}, {"id": "1804.01422", "submitter": "Jian Xu", "authors": "Jian Xu, Chunheng Wang, Chengzuo Qi, Cunzhao Shi, and Baihua Xiao", "title": "Unsupervised Semantic-based Aggregation of Deep Convolutional Features", "comments": "10 pages. arXiv admin note: text overlap with arXiv:1705.01247", "journal-ref": null, "doi": "10.1109/TIP.2018.2867104", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple but effective semantic-based aggregation\n(SBA) method. The proposed SBA utilizes the discriminative filters of deep\nconvolutional layers as semantic detectors. Moreover, we propose the effective\nunsupervised strategy to select some semantic detectors to generate the\n\"probabilistic proposals\", which highlight certain discriminative pattern of\nobjects and suppress the noise of background. The final global SBA\nrepresentation could then be acquired by aggregating the regional\nrepresentations weighted by the selected \"probabilistic proposals\"\ncorresponding to various semantic content. Our unsupervised SBA is easy to\ngeneralize and achieves excellent performance on various tasks. We conduct\ncomprehensive experiments and show that our unsupervised SBA outperforms the\nstate-of-the-art unsupervised and supervised aggregation methods on image\nretrieval, place recognition and cloud classification.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 07:43:05 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Xu", "Jian", ""], ["Wang", "Chunheng", ""], ["Qi", "Chengzuo", ""], ["Shi", "Cunzhao", ""], ["Xiao", "Baihua", ""]]}, {"id": "1804.01429", "submitter": "Ruichi Yu", "authors": "Ruichi Yu, Hongcheng Wang, Ang Li, Jingxiao Zheng, Vlad I. Morariu,\n  Larry S. Davis", "title": "Layout-induced Video Representation for Recognizing Agent-in-Place\n  Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the recognition of agent-in-place actions, which are associated\nwith agents who perform them and places where they occur, in the context of\noutdoor home surveillance. We introduce a representation of the geometry and\ntopology of scene layouts so that a network can generalize from the layouts\nobserved in the training set to unseen layouts in the test set. This\nLayout-Induced Video Representation (LIVR) abstracts away low-level appearance\nvariance and encodes geometric and topological relationships of places in a\nspecific scene layout. LIVR partitions the semantic features of a video clip\ninto different places to force the network to learn place-based feature\ndescriptions; to predict the confidence of each action, LIVR aggregates\nfeatures from the place associated with an action and its adjacent places on\nthe scene layout. We introduce the Agent-in-Place Action dataset to show that\nour method allows neural network models to generalize significantly better to\nunseen scenes.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 14:25:04 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 16:34:39 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 04:46:05 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Yu", "Ruichi", ""], ["Wang", "Hongcheng", ""], ["Li", "Ang", ""], ["Zheng", "Jingxiao", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1804.01438", "submitter": "Guanshuo Wang", "authors": "Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, Xi Zhou", "title": "Learning Discriminative Features with Multiple Granularities for Person\n  Re-Identification", "comments": "9 pages, 5 figures. To appear in ACM Multimedia 2018", "journal-ref": null, "doi": "10.1145/3240508.3240552", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of global and partial features has been an essential solution\nto improve discriminative performances in person re-identification (Re-ID)\ntasks. Previous part-based methods mainly focus on locating regions with\nspecific pre-defined semantics to learn local representations, which increases\nlearning difficulty but not efficient or robust to scenarios with large\nvariances. In this paper, we propose an end-to-end feature learning strategy\nintegrating discriminative information with various granularities. We carefully\ndesign the Multiple Granularity Network (MGN), a multi-branch deep network\narchitecture consisting of one branch for global feature representations and\ntwo branches for local feature representations. Instead of learning on semantic\nregions, we uniformly partition the images into several stripes, and vary the\nnumber of parts in different local branches to obtain local feature\nrepresentations with multiple granularities. Comprehensive experiments\nimplemented on the mainstream evaluation datasets including Market-1501,\nDukeMTMC-reid and CUHK03 indicate that our method has robustly achieved\nstate-of-the-art performances and outperformed any existing approaches by a\nlarge margin. For example, on Market-1501 dataset in single query mode, we\nachieve a state-of-the-art result of Rank-1/mAP=96.6%/94.2% after re-ranking.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 14:36:01 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 07:27:07 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 06:43:29 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Wang", "Guanshuo", ""], ["Yuan", "Yufeng", ""], ["Chen", "Xiong", ""], ["Li", "Jiwei", ""], ["Zhou", "Xi", ""]]}, {"id": "1804.01452", "submitter": "David Harwath", "authors": "David Harwath, Adri\\`a Recasens, D\\'idac Sur\\'is, Galen Chuang,\n  Antonio Torralba, and James Glass", "title": "Jointly Discovering Visual Objects and Spoken Words from Raw Sensory\n  Input", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore neural network models that learn to associate\nsegments of spoken audio captions with the semantically relevant portions of\nnatural images that they refer to. We demonstrate that these audio-visual\nassociative localizations emerge from network-internal representations learned\nas a by-product of training to perform an image-audio retrieval task. Our\nmodels operate directly on the image pixels and speech waveform, and do not\nrely on any conventional supervision in the form of labels, segmentations, or\nalignments between the modalities during training. We perform analysis using\nthe Places 205 and ADE20k datasets demonstrating that our models implicitly\nlearn semantically-coupled object and word detectors.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 15:03:08 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Harwath", "David", ""], ["Recasens", "Adri\u00e0", ""], ["Sur\u00eds", "D\u00eddac", ""], ["Chuang", "Galen", ""], ["Torralba", "Antonio", ""], ["Glass", "James", ""]]}, {"id": "1804.01495", "submitter": "Felix J\\\"aremo Lawin", "authors": "Felix J\\\"aremo Lawin, Martin Danelljan, Fahad Shahbaz Khan, Per-Erik\n  Forss\\'en, Michael Felsberg", "title": "Density Adaptive Point Set Registration", "comments": "CVPR 2018 (Oral)", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic methods for point set registration have demonstrated\ncompetitive results in recent years. These techniques estimate a probability\ndistribution model of the point clouds. While such a representation has shown\npromise, it is highly sensitive to variations in the density of 3D points. This\nfundamental problem is primarily caused by changes in the sensor location\nacross point sets. We revisit the foundations of the probabilistic registration\nparadigm. Contrary to previous works, we model the underlying structure of the\nscene as a latent probability distribution, and thereby induce invariance to\npoint set density changes. Both the probabilistic model of the scene and the\nregistration parameters are inferred by minimizing the Kullback-Leibler\ndivergence in an Expectation Maximization based framework. Our density-adaptive\nregistration successfully handles severe density variations commonly\nencountered in terrestrial Lidar applications. We perform extensive experiments\non several challenging real-world Lidar datasets. The results demonstrate that\nour approach outperforms state-of-the-art probabilistic methods for multi-view\nregistration, without the need of re-sampling. Code is available at\nhttps://github.com/felja633/DARE.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 16:28:50 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 08:48:09 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Lawin", "Felix J\u00e4remo", ""], ["Danelljan", "Martin", ""], ["Khan", "Fahad Shahbaz", ""], ["Forss\u00e9n", "Per-Erik", ""], ["Felsberg", "Michael", ""]]}, {"id": "1804.01508", "submitter": "Ole-Christoffer Granmo", "authors": "Ole-Christoffer Granmo", "title": "The Tsetlin Machine -- A Game Theoretic Bandit Driven Approach to\n  Optimal Pattern Recognition with Propositional Logic", "comments": "42 pages, 14 figures, further formalizing key concepts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although simple individually, artificial neurons provide state-of-the-art\nperformance when interconnected in deep networks. Arguably, the Tsetlin\nAutomaton is an even simpler and more versatile learning mechanism, capable of\nsolving the multi-armed bandit problem. Merely by means of a single integer as\nmemory, it learns the optimal action in stochastic environments through\nincrement and decrement operations. In this paper, we introduce the Tsetlin\nMachine, which solves complex pattern recognition problems with propositional\nformulas, composed by a collective of Tsetlin Automata. To eliminate the\nlongstanding problem of vanishing signal-to-noise ratio, the Tsetlin Machine\norchestrates the automata using a novel game. Further, both inputs, patterns,\nand outputs are expressed as bits, while recognition and learning rely on bit\nmanipulation, simplifying computation. Our theoretical analysis establishes\nthat the Nash equilibria of the game align with the propositional formulas that\nprovide optimal pattern recognition accuracy. This translates to learning\nwithout local optima, only global ones. In five benchmarks, the Tsetlin Machine\nprovides competitive accuracy compared with SVMs, Decision Trees, Random\nForests, Naive Bayes Classifier, Logistic Regression, and Neural Networks. We\nfurther demonstrate how the propositional formulas facilitate interpretation.\nIn conclusion, we believe the combination of high accuracy, interpretability,\nand computational simplicity makes the Tsetlin Machine a promising tool for a\nwide range of domains.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 16:52:34 GMT"}, {"version": "v10", "created": "Tue, 15 Jan 2019 17:29:40 GMT"}, {"version": "v11", "created": "Mon, 4 Feb 2019 12:00:26 GMT"}, {"version": "v12", "created": "Thu, 23 Apr 2020 16:17:31 GMT"}, {"version": "v13", "created": "Thu, 11 Jun 2020 08:09:55 GMT"}, {"version": "v14", "created": "Thu, 3 Dec 2020 13:02:37 GMT"}, {"version": "v15", "created": "Sat, 2 Jan 2021 00:51:08 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 15:19:24 GMT"}, {"version": "v3", "created": "Mon, 9 Apr 2018 14:33:17 GMT"}, {"version": "v4", "created": "Tue, 10 Apr 2018 16:42:33 GMT"}, {"version": "v5", "created": "Wed, 11 Apr 2018 16:26:27 GMT"}, {"version": "v6", "created": "Mon, 16 Apr 2018 13:33:49 GMT"}, {"version": "v7", "created": "Mon, 23 Apr 2018 12:51:28 GMT"}, {"version": "v8", "created": "Mon, 7 Jan 2019 13:01:31 GMT"}, {"version": "v9", "created": "Thu, 10 Jan 2019 17:11:21 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Granmo", "Ole-Christoffer", ""]]}, {"id": "1804.01523", "submitter": "Alex Lee", "authors": "Alex X. Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea\n  Finn, Sergey Levine", "title": "Stochastic Adversarial Video Prediction", "comments": "Website: https://alexlee-gk.github.io/video_prediction/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to predict what may happen in the future requires an in-depth\nunderstanding of the physical and causal rules that govern the world. A model\nthat is able to do so has a number of appealing applications, from robotic\nplanning to representation learning. However, learning to predict raw future\nobservations, such as frames in a video, is exceedingly challenging -- the\nambiguous nature of the problem can cause a naively designed model to average\ntogether possible futures into a single, blurry prediction. Recently, this has\nbeen addressed by two distinct approaches: (a) latent variational variable\nmodels that explicitly model underlying stochasticity and (b)\nadversarially-trained models that aim to produce naturalistic images. However,\na standard latent variable model can struggle to produce realistic results, and\na standard adversarially-trained model underutilizes latent variables and fails\nto produce diverse predictions. We show that these distinct methods are in fact\ncomplementary. Combining the two produces predictions that look more realistic\nto human raters and better cover the range of possible futures. Our method\noutperforms prior and concurrent work in these aspects.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 17:55:40 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Lee", "Alex X.", ""], ["Zhang", "Richard", ""], ["Ebert", "Frederik", ""], ["Abbeel", "Pieter", ""], ["Finn", "Chelsea", ""], ["Levine", "Sergey", ""]]}, {"id": "1804.01527", "submitter": "Jos\\'e Carlos Aradillas Jaramillo", "authors": "Jos\\'e Carlos Aradillas, Juan Jos\\'e Murillo-Fuentes, Pablo M. Olmos", "title": "Boosting Handwriting Text Recognition in Small Databases with Transfer\n  Learning", "comments": "ICFHR 2018 Conference", "journal-ref": null, "doi": "10.1109/ICFHR-2018.2018.00081", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we deal with the offline handwriting text recognition (HTR)\nproblem with reduced training datasets. Recent HTR solutions based on\nartificial neural networks exhibit remarkable solutions in referenced\ndatabases. These deep learning neural networks are composed of both\nconvolutional (CNN) and long short-term memory recurrent units (LSTM). In\naddition, connectionist temporal classification (CTC) is the key to avoid\nsegmentation at character level, greatly facilitating the labeling task. One of\nthe main drawbacks of the CNNLSTM-CTC (CLC) solutions is that they need a\nconsiderable part of the text to be transcribed for every type of calligraphy,\ntypically in the order of a few thousands of lines. Furthermore, in some\nscenarios the text to transcribe is not that long, e.g. in the Washington\ndatabase. The CLC typically overfits for this reduced number of training\nsamples. Our proposal is based on the transfer learning (TL) from the\nparameters learned with a bigger database. We first investigate, for a reduced\nand fixed number of training samples, 350 lines, how the learning from a large\ndatabase, the IAM, can be transferred to the learning of the CLC of a reduced\ndatabase, Washington. We focus on which layers of the network could be not\nre-trained. We conclude that the best solution is to re-train the whole CLC\nparameters initialized to the values obtained after the training of the CLC\nfrom the larger database. We also investigate results when the training size is\nfurther reduced. The differences in the CER are more remarkable when training\nwith just 350 lines, a CER of 3.3% is achieved with TL while we have a CER of\n18.2% when training from scratch. As a byproduct, the learning times are quite\nreduced. Similar good results are obtained from the Parzival database when\ntrained with this reduced number of lines and this new approach.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 11:20:28 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Aradillas", "Jos\u00e9 Carlos", ""], ["Murillo-Fuentes", "Juan Jos\u00e9", ""], ["Olmos", "Pablo M.", ""]]}, {"id": "1804.01552", "submitter": "David Novotn\\'y", "authors": "David Novotny, Samuel Albanie, Diane Larlus, Andrea Vedaldi", "title": "Self-supervised Learning of Geometrically Stable Features Through\n  Probabilistic Introspection", "comments": "In 2018 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervision can dramatically cut back the amount of manually-labelled\ndata required to train deep neural networks. While self-supervision has usually\nbeen considered for tasks such as image classification, in this paper we aim at\nextending it to geometry-oriented tasks such as semantic matching and part\ndetection. We do so by building on several recent ideas in unsupervised\nlandmark detection. Our approach learns dense distinctive visual descriptors\nfrom an unlabelled dataset of images using synthetic image transformations. It\ndoes so by means of a robust probabilistic formulation that can introspectively\ndetermine which image regions are likely to result in stable image matching. We\nshow empirically that a network pre-trained in this manner requires\nsignificantly less supervision to learn semantic object parts compared to\nnumerous pre-training alternatives. We also show that the pre-trained\nrepresentation is excellent for semantic object matching.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 18:15:17 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Novotny", "David", ""], ["Albanie", "Samuel", ""], ["Larlus", "Diane", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1804.01565", "submitter": "Alireza Sedghi", "authors": "Alireza Sedghi, Jie Luo, Alireza Mehrtash, Steve Pieper, Clare M.\n  Tempany, Tina Kapur, Parvin Mousavi, William M. Wells III", "title": "Semi-Supervised Deep Metrics for Image Registration", "comments": "Under Review for MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metrics have been shown effective as similarity measures in multi-modal\nimage registration; however, the metrics are currently constructed from aligned\nimage pairs in the training data. In this paper, we propose a strategy for\nlearning such metrics from roughly aligned training data. Symmetrizing the data\ncorrects bias in the metric that results from misalignment in the data (at the\nexpense of increased variance), while random perturbations to the data, i.e.\ndithering, ensures that the metric has a single mode, and is amenable to\nregistration by optimization. Evaluation is performed on the task of\nregistration on separate unseen test image pairs. The results demonstrate the\nfeasibility of learning a useful deep metric from substantially misaligned\ntraining data, in some cases the results are significantly better than from\nMutual Information. Data augmentation via dithering is, therefore, an effective\nstrategy for discharging the need for well-aligned training data; this brings\ndeep metric registration from the realm of supervised to semi-supervised\nmachine learning.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 18:51:51 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Sedghi", "Alireza", ""], ["Luo", "Jie", ""], ["Mehrtash", "Alireza", ""], ["Pieper", "Steve", ""], ["Tempany", "Clare M.", ""], ["Kapur", "Tina", ""], ["Mousavi", "Parvin", ""], ["Wells", "William M.", "III"]]}, {"id": "1804.01601", "submitter": "Shadi Albarqouni", "authors": "M Tarek Shaban, Christoph Baur, Nassir Navab, Shadi Albarqouni", "title": "StainGAN: Stain Style Transfer for Digital Histological Images", "comments": "Submitted to MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitized Histological diagnosis is in increasing demand. However, color\nvariations due to various factors are imposing obstacles to the diagnosis\nprocess. The problem of stain color variations is a well-defined problem with\nmany proposed solutions. Most of these solutions are highly dependent on a\nreference template slide. We propose a deep-learning solution inspired by\nCycleGANs that is trained end-to-end, eliminating the need for an expert to\npick a representative reference slide. Our approach showed superior results\nquantitatively and qualitatively against the state of the art methods (10%\nimprovement visually using SSIM). We further validated our method on a clinical\nuse-case, namely Breast Cancer tumor classification, showing 12% increase in\nAUC. The code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 20:34:53 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Shaban", "M Tarek", ""], ["Baur", "Christoph", ""], ["Navab", "Nassir", ""], ["Albarqouni", "Shadi", ""]]}, {"id": "1804.01611", "submitter": "Onay Urfalioglu", "authors": "Fahd Bouzaraa, Ibrahim Halfaoui, Onay Urfalioglu", "title": "Learnable Exposure Fusion for Dynamic Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on Exposure Fusion (EF) [ExposFusi2] for dynamic\nscenes. The task is to fuse multiple images obtained by exposure bracketing to\ncreate an image which comprises a high level of details. Typically, such images\nare not possible to obtain directly from a camera due to hardware limitations,\ne.g., a limited dynamic range of the sensor. A major problem of such tasks is\nthat the images may not be spatially aligned due to scene motion or camera\nmotion. It is known that the required alignment by image registration problems\nis ill-posed. In this case, the images to be aligned vary in their intensity\nrange, which makes the problem even more difficult.\n  To address the mentioned problems, we propose an end-to-end\n\\emph{Convolutional Neural Network} (CNN) based approach to learn to estimate\nexposure fusion from $2$ and $3$ Low Dynamic Range (LDR) images depicting\ndifferent scene contents. To the best of our knowledge, no efficient and robust\nCNN-based end-to-end approach can be found in the literature for this kind of\nproblem. The idea is to create a dataset with perfectly aligned LDR images to\nobtain ground-truth exposure fusion images. At the same time, we obtain\nadditional LDR images with some motion, having the same exposure fusion\nground-truth as the perfectly aligned LDR images. This way, we can train an\nend-to-end CNN having misaligned LDR input images, but with a proper ground\ntruth exposure fusion image. We propose a specific CNN-architecture to solve\nthis problem. In various experiments, we show that the proposed approach yields\nexcellent results.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 21:36:31 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Bouzaraa", "Fahd", ""], ["Halfaoui", "Ibrahim", ""], ["Urfalioglu", "Onay", ""]]}, {"id": "1804.01622", "submitter": "Justin Johnson", "authors": "Justin Johnson, Agrim Gupta, Li Fei-Fei", "title": "Image Generation from Scene Graphs", "comments": "To appear at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To truly understand the visual world our models should be able not only to\nrecognize images but also generate them. To this end, there has been exciting\nrecent progress on generating images from natural language descriptions. These\nmethods give stunning results on limited domains such as descriptions of birds\nor flowers, but struggle to faithfully reproduce complex sentences with many\nobjects and relationships. To overcome this limitation we propose a method for\ngenerating images from scene graphs, enabling explicitly reasoning about\nobjects and their relationships. Our model uses graph convolution to process\ninput graphs, computes a scene layout by predicting bounding boxes and\nsegmentation masks for objects, and converts the layout to an image with a\ncascaded refinement network. The network is trained adversarially against a\npair of discriminators to ensure realistic outputs. We validate our approach on\nVisual Genome and COCO-Stuff, where qualitative results, ablations, and user\nstudies demonstrate our method's ability to generate complex images with\nmultiple objects.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 22:59:08 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Johnson", "Justin", ""], ["Gupta", "Agrim", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1804.01624", "submitter": "Roger Gomez Nieto", "authors": "Roger Gomez Nieto, H.D. Benitez-Restrepo, Ivan Mauricio Cabezas", "title": "Evaluation of Object Trackers in Distorted Surveillance Videos", "comments": "5 pages, 8 figures, presented in SPSWSIVA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Object tracking in realistic scenarios is a difficult problem affected by\nvarious image factors such as occlusion, clutter, confusion, object shape,\nunstable speed, and zooming. While these conditions do affect tracking\nperformance, there is no clear distinction between the scene dependent\nchallenges like occlusion, clutter, etc., and the challenges imposed by\ntraditional notions of impairments from capture, compression, processing, and\ntransmission. This paper is concerned with the latter interpretation of quality\nas it affects video tracking performance. In this work we aim to evaluate two\nstate-of-the-art trackers (STRUCK and TLD) systematically and experimentally in\nsurveillance videos affected by in-capture distortions such as under-exposure\nand defocus. We evaluate these trackers with the area under curve (AUC) values\nof success plots and precision curves. In spite of the fact that STRUCK and TLD\nhave ranked high in video tracking surveys. This study concludes that incapture\ndistortions severely affect the performance of these trackers. For this reason,\nthe design and construction of a robust tracker with respect to these\ndistortions remains an open question that can be answered by creating\nalgorithms that makes use of perceptual features to compensate the degradations\nprovided by these distortions.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 23:34:17 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Nieto", "Roger Gomez", ""], ["Benitez-Restrepo", "H. D.", ""], ["Cabezas", "Ivan Mauricio", ""]]}, {"id": "1804.01635", "submitter": "Neale Ratzlaff", "authors": "Neale Ratzlaff, Li Fuxin", "title": "Unifying Bilateral Filtering and Adversarial Training for Robust Neural\n  Networks", "comments": "9 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent analysis of deep neural networks has revealed their vulnerability to\ncarefully structured adversarial examples. Many effective algorithms exist to\ncraft these adversarial examples, but performant defenses seem to be far away.\nIn this work, we explore the use of edge-aware bilateral filtering as a\nprojection back to the space of natural images. We show that bilateral\nfiltering is an effective defense in multiple attack settings, where the\nstrength of the adversary gradually increases. In the case of an adversary who\nhas no knowledge of the defense, bilateral filtering can remove more than 90%\nof adversarial examples from a variety of different attacks. To evaluate\nagainst an adversary with complete knowledge of our defense, we adapt the\nbilateral filter as a trainable layer in a neural network and show that adding\nthis layer makes ImageNet images significantly more robust to attacks. When\ntrained under a framework of adversarial training, we show that the resulting\nmodel is hard to fool with even the best attack methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 00:40:25 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 22:55:20 GMT"}, {"version": "v3", "created": "Mon, 8 Oct 2018 17:36:59 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Ratzlaff", "Neale", ""], ["Fuxin", "Li", ""]]}, {"id": "1804.01646", "submitter": "Daniel Morris", "authors": "Daniel D. Morris", "title": "A Pyramid CNN for Dense-Leaves Segmentation", "comments": "To appear in Computer and Robot Vision, Toronto, May 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection and segmentation of overlapping leaves in dense foliage\ncan be a difficult task, particularly for leaves with strong textures and high\nocclusions. We present Dense-Leaves, an image dataset with ground truth\nsegmentation labels that can be used to train and quantify algorithms for leaf\nsegmentation in the wild. We also propose a pyramid convolutional neural\nnetwork with multi-scale predictions that detects and discriminates leaf\nboundaries from interior textures. Using these detected boundaries,\nclosed-contour boundaries around individual leaves are estimated with a\nwatershed-based algorithm. The result is an instance segmenter for dense\nleaves. Promising segmentation results for leaves in dense foliage are\nobtained.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 01:38:42 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Morris", "Daniel D.", ""]]}, {"id": "1804.01653", "submitter": "Rong Zhang", "authors": "Rong Zhang, Weiping Li, Tong Mo", "title": "Review of Deep Learning", "comments": "In Chinese. Have been published in the journal \"Information and\n  Control\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, China, the United States and other countries, Google and\nother high-tech companies have increased investment in artificial intelligence.\nDeep learning is one of the current artificial intelligence research's key\nareas. This paper analyzes and summarizes the latest progress and future\nresearch directions of deep learning. Firstly, three basic models of deep\nlearning are outlined, including multilayer perceptrons, convolutional neural\nnetworks, and recurrent neural networks. On this basis, we further analyze the\nemerging new models of convolution neural networks and recurrent neural\nnetworks. This paper then summarizes deep learning's applications in many areas\nof artificial intelligence, including speech processing, computer vision,\nnatural language processing and so on. Finally, this paper discusses the\nexisting problems of deep learning and gives the corresponding possible\nsolutions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 02:23:59 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 15:34:03 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Zhang", "Rong", ""], ["Li", "Weiping", ""], ["Mo", "Tong", ""]]}, {"id": "1804.01654", "submitter": "Nanyang Wang", "authors": "Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, Yu-Gang\n  Jiang", "title": "Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images", "comments": null, "journal-ref": "ECCV2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end deep learning architecture that produces a 3D shape\nin triangular mesh from a single color image. Limited by the nature of deep\nneural network, previous methods usually represent a 3D shape in volume or\npoint cloud, and it is non-trivial to convert them to the more ready-to-use\nmesh model. Unlike the existing methods, our network represents 3D mesh in a\ngraph-based convolutional neural network and produces correct geometry by\nprogressively deforming an ellipsoid, leveraging perceptual features extracted\nfrom the input image. We adopt a coarse-to-fine strategy to make the whole\ndeformation procedure stable, and define various of mesh related losses to\ncapture properties of different levels to guarantee visually appealing and\nphysically accurate 3D geometry. Extensive experiments show that our method not\nonly qualitatively produces mesh model with better details, but also achieves\nhigher 3D shape estimation accuracy compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 02:24:03 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 08:52:33 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Wang", "Nanyang", ""], ["Zhang", "Yinda", ""], ["Li", "Zhuwen", ""], ["Fu", "Yanwei", ""], ["Liu", "Wei", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "1804.01661", "submitter": "Xin Yu", "authors": "Xin Yu, Zhiding Yu, Srikumar Ramalingam", "title": "Learning Strict Identity Mappings in Deep Residual Networks", "comments": "Make title consistent with the CVPR version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family of super deep networks, referred to as residual networks or ResNet,\nachieved record-beating performance in various visual tasks such as image\nrecognition, object detection, and semantic segmentation. The ability to train\nvery deep networks naturally pushed the researchers to use enormous resources\nto achieve the best performance. Consequently, in many applications super deep\nresidual networks were employed for just a marginal improvement in performance.\nIn this paper, we propose epsilon-ResNet that allows us to automatically\ndiscard redundant layers, which produces responses that are smaller than a\nthreshold epsilon, with a marginal or no loss in performance. The\nepsilon-ResNet architecture can be achieved using a few additional rectified\nlinear units in the original ResNet. Our method does not use any additional\nvariables nor numerous trials like other hyper-parameter optimization\ntechniques. The layer selection is achieved using a single training process and\nthe evaluation is performed on CIFAR-10, CIFAR-100, SVHN, and ImageNet\ndatasets. In some instances, we achieve about 80% reduction in the number of\nparameters.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 03:19:53 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 05:13:34 GMT"}, {"version": "v3", "created": "Wed, 16 May 2018 20:03:44 GMT"}, {"version": "v4", "created": "Sun, 18 Nov 2018 23:43:33 GMT"}, {"version": "v5", "created": "Sun, 16 Jun 2019 00:03:19 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Yu", "Xin", ""], ["Yu", "Zhiding", ""], ["Ramalingam", "Srikumar", ""]]}, {"id": "1804.01665", "submitter": "Ruohan Gao", "authors": "Ruohan Gao, Rogerio Feris, Kristen Grauman", "title": "Learning to Separate Object Sounds by Watching Unlabeled Video", "comments": "Published in ECCV 2018; Project Page:\n  http://vision.cs.utexas.edu/projects/separating_object_sounds/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceiving a scene most fully requires all the senses. Yet modeling how\nobjects look and sound is challenging: most natural scenes and events contain\nmultiple objects, and the audio track mixes all the sound sources together. We\npropose to learn audio-visual object models from unlabeled video, then exploit\nthe visual context to perform audio source separation in novel videos. Our\napproach relies on a deep multi-instance multi-label learning framework to\ndisentangle the audio frequency bases that map to individual visual objects,\neven without observing/hearing those objects in isolation. We show how the\nrecovered disentangled bases can be used to guide audio source separation to\nobtain better-separated, object-level sounds. Our work is the first to learn\naudio source separation from large-scale \"in the wild\" videos containing\nmultiple audio sources per video. We obtain state-of-the-art results on\nvisually-aided audio source separation and audio denoising. Our video results:\nhttp://vision.cs.utexas.edu/projects/separating_object_sounds/\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 04:06:46 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 04:46:24 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Gao", "Ruohan", ""], ["Feris", "Rogerio", ""], ["Grauman", "Kristen", ""]]}, {"id": "1804.01670", "submitter": "Takao Murakami", "authors": "Takao Murakami, Tetsushi Ohki, Yosuke Kaga, Masakazu Fujio, Kenta\n  Takahashi", "title": "Cancelable Indexing Based on Low-rank Approximation of\n  Correlation-invariant Random Filtering for Fast and Secure Biometric\n  Identification", "comments": "Accepted to Pattern Recognition Letters (Special Issue on Robustness,\n  Security and Regulation Aspects in Current Biometric Systems), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cancelable biometric scheme called correlation-invariant random filtering\n(CIRF) is known as a promising template protection scheme. This scheme\ntransforms a biometric feature represented as an image via the 2D number\ntheoretic transform (NTT) and random filtering. CIRF has perfect secrecy in\nthat the transformed feature leaks no information about the original feature.\nHowever, CIRF cannot be applied to large-scale biometric identification, since\nthe 2D inverse NTT in the matching phase requires high computational time.\nFurthermore, existing biometric indexing schemes cannot be used in conjunction\nwith template protection schemes to speed up biometric identification, since a\nbiometric index leaks some information about the original feature. In this\npaper, we propose a novel indexing scheme called \"cancelable indexing\" to speed\nup CIRF without losing its security properties. The proposed scheme is based on\nfast computation of CIRF via low-rank approximation of biometric images and via\na minimum spanning tree representation of low-rank matrices in the Fourier\ndomain. We prove that the transformed index leaks no information about the\noriginal index and the original biometric feature (i.e., perfect secrecy), and\nthoroughly discuss the security of the proposed scheme. We also demonstrate\nthat it significantly reduces the one-to-many matching time using a finger-vein\ndataset that includes six fingers from 505 subjects.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 04:56:44 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Murakami", "Takao", ""], ["Ohki", "Tetsushi", ""], ["Kaga", "Yosuke", ""], ["Fujio", "Masakazu", ""], ["Takahashi", "Kenta", ""]]}, {"id": "1804.01681", "submitter": "Junyi Lin", "authors": "Kwan-Yee Lin and Guanxiang Wang", "title": "Hallucinated-IQA: No-Reference Image Quality Assessment via Adversarial\n  Learning", "comments": "Accepted to CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  No-reference image quality assessment (NR-IQA) is a fundamental yet\nchallenging task in low-level computer vision community. The difficulty is\nparticularly pronounced for the limited information, for which the\ncorresponding reference for comparison is typically absent. Although various\nfeature extraction mechanisms have been leveraged from natural scene statistics\nto deep neural networks in previous methods, the performance bottleneck still\nexists. In this work, we propose a hallucination-guided quality regression\nnetwork to address the issue. We firstly generate a hallucinated reference\nconstrained on the distorted image, to compensate the absence of the true\nreference. Then, we pair the information of hallucinated reference with the\ndistorted image, and forward them to the regressor to learn the perceptual\ndiscrepancy with the guidance of an implicit ranking relationship within the\ngenerator, and therefore produce the precise quality prediction. To demonstrate\nthe effectiveness of our approach, comprehensive experiments are evaluated on\nfour popular image quality assessment benchmarks. Our method significantly\noutperforms all the previous state-of-the-art methods by large margins. The\ncode and model will be publicly available on the project page\nhttps://kwanyeelin.github.io/projects/HIQA/HIQA.html.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 06:33:21 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Lin", "Kwan-Yee", ""], ["Wang", "Guanxiang", ""]]}, {"id": "1804.01708", "submitter": "Benjamin Busam", "authors": "Benjamin Busam, Patrick Ruhkamp, Salvatore Virga, Beatrice Lentes,\n  Julia Rackerseder, Nassir Navab, Christoph Hennersperger", "title": "Markerless Inside-Out Tracking for Interventional Applications", "comments": "Medical Image Computing and Computer Assisted Interventions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking of rotation and translation of medical instruments plays a\nsubstantial role in many modern interventions. Traditional external optical\ntracking systems are often subject to line-of-sight issues, in particular when\nthe region of interest is difficult to access or the procedure allows only for\nlimited rigid body markers. The introduction of inside-out tracking systems\naims to overcome these issues. We propose a marker-less tracking system based\non visual SLAM to enable tracking of instruments in an interventional scenario.\nTo achieve this goal, we mount a miniature multi-modal (monocular, stereo,\nactive depth) vision system on the object of interest and relocalize its pose\nwithin an adaptive map of the operating room. We compare state-of-the-art\nalgorithmic pipelines and apply the idea to transrectal 3D Ultrasound (TRUS)\ncompounding of the prostate. Obtained volumes are compared to reconstruction\nusing a commercial optical tracking system as well as a robotic manipulator.\nFeature-based binocular SLAM is identified as the most promising method and is\ntested extensively in challenging clinical environment under severe occlusion\nand for the use case of prostate US biopsies.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 07:47:08 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 17:35:18 GMT"}, {"version": "v3", "created": "Tue, 12 Jun 2018 09:35:31 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Busam", "Benjamin", ""], ["Ruhkamp", "Patrick", ""], ["Virga", "Salvatore", ""], ["Lentes", "Beatrice", ""], ["Rackerseder", "Julia", ""], ["Navab", "Nassir", ""], ["Hennersperger", "Christoph", ""]]}, {"id": "1804.01720", "submitter": "Martin Engilberge", "authors": "Martin Engilberge, Louis Chevallier, Patrick P\\'erez, Matthieu Cord", "title": "Finding beans in burgers: Deep semantic-visual embedding with\n  localization", "comments": "Accepted to CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several works have proposed to learn a two-path neural network that maps\nimages and texts, respectively, to a same shared Euclidean space where geometry\ncaptures useful semantic relationships. Such a multi-modal embedding can be\ntrained and used for various tasks, notably image captioning. In the present\nwork, we introduce a new architecture of this type, with a visual path that\nleverages recent space-aware pooling mechanisms. Combined with a textual path\nwhich is jointly trained from scratch, our semantic-visual embedding offers a\nversatile model. Once trained under the supervision of captioned images, it\nyields new state-of-the-art performance on cross-modal retrieval. It also\nallows the localization of new concepts from the embedding space into any input\nimage, delivering state-of-the-art result on the visual grounding of phrases.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 08:13:37 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 14:04:35 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Engilberge", "Martin", ""], ["Chevallier", "Louis", ""], ["P\u00e9rez", "Patrick", ""], ["Cord", "Matthieu", ""]]}, {"id": "1804.01728", "submitter": "Michele Alberti", "authors": "Vinaychandran Pondenkandath and Michele Alberti and Nicole\n  Eichenberger and Rolf Ingold and Marcus Liwicki", "title": "Identifying Cross-Depicted Historical Motifs", "comments": "6 pages, 6 figures", "journal-ref": "16th International Conference on Frontiers in Handwriting\n  Recognition (Vol. 16, pp. 333-338), IEEE, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-depiction is the problem of identifying the same object even when it is\ndepicted in a variety of manners. This is a common problem in handwritten\nhistorical documents image analysis, for instance when the same letter or motif\nis depicted in several different ways. It is a simple task for humans yet\nconventional heuristic computer vision methods struggle to cope with it. In\nthis paper we address this problem using state-of-the-art deep learning\ntechniques on a dataset of historical watermarks containing images created with\ndifferent methods of reproduction, such as hand tracing, rubbing, and\nradiography. To study the robustness of deep learning based approaches to the\ncross-depiction problem, we measure their performance on two different tasks:\nclassification and similarity rankings. For the former we achieve a\nclassification accuracy of 96% using deep convolutional neural networks. For\nthe latter we have a false positive rate at 95% true positive rate of 0.11.\nThese results outperform state-of-the-art methods by a significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 08:28:00 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 19:56:10 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Pondenkandath", "Vinaychandran", ""], ["Alberti", "Michele", ""], ["Eichenberger", "Nicole", ""], ["Ingold", "Rolf", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1804.01736", "submitter": "Tatsuya Yokota", "authors": "Tatsuya Yokota, Burak Erem, Seyhmus Guler, Simon K. Warfield, Hidekata\n  Hontani", "title": "Missing Slice Recovery for Tensors Using a Low-rank Model in Embedded\n  Space", "comments": "accepted for CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let us consider a case where all of the elements in some continuous slices\nare missing in tensor data.\n  In this case, the nuclear-norm and total variation regularization methods\nusually fail to recover the missing elements.\n  The key problem is capturing some delay/shift-invariant structure.\n  In this study, we consider a low-rank model in an embedded space of a tensor.\n  For this purpose, we extend a delay embedding for a time series to a\n\"multi-way delay-embedding transform\" for a tensor, which takes a given\nincomplete tensor as the input and outputs a higher-order incomplete Hankel\ntensor.\n  The higher-order tensor is then recovered by Tucker-based low-rank tensor\nfactorization.\n  Finally, an estimated tensor can be obtained by using the inverse multi-way\ndelay embedding transform of the recovered higher-order tensor.\n  Our experiments showed that the proposed method successfully recovered\nmissing slices for some color images and functional magnetic resonance images.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 08:44:01 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Yokota", "Tatsuya", ""], ["Erem", "Burak", ""], ["Guler", "Seyhmus", ""], ["Warfield", "Simon K.", ""], ["Hontani", "Hidekata", ""]]}, {"id": "1804.01753", "submitter": "Saurav Jha", "authors": "Saurav Jha, Nikhil Agarwal, Suneeta Agarwal", "title": "Bringing Cartoons to Life: Towards Improved Cartoon Face Detection and\n  Recognition Systems", "comments": "8 pages, 5 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the recent deep learning advancements in face detection and recognition\ntechniques for human faces, this paper answers the question \"how well would\nthey work for cartoons'?\" - a domain that remains largely unexplored until\nrecently, mainly due to the unavailability of large scale datasets and the\nfailure of traditional methods on these. Our work studies and extends multiple\nframeworks for the aforementioned tasks. For face detection, we incorporate the\nMulti-task Cascaded Convolutional Network (MTCNN) architecture and contrast it\nwith conventional methods. For face recognition, our two-fold contributions\ninclude: (i) an inductive transfer learning approach combining the feature\nlearning capability of the Inception v3 network and the feature recognizing\ncapability of Support Vector Machines (SVMs), (ii) a proposed Hybrid\nConvolutional Neural Network (HCNN) framework trained over a fusion of pixel\nvalues and 15 manually located facial keypoints. All the methods are evaluated\non the Cartoon Faces in the Wild (IIIT-CFW) database. We demonstrate that the\nHCNN model offers stability superior to that of Inception+SVM over larger input\nvariations, and explore the plausible architectural principles. We show that\nthe Inception+SVM model establishes a state-of-the-art F1 score on the task of\ngender recognition of cartoon faces. Further, we introduce a small database\nhosting location coordinates of 15 points on the cartoon faces belonging to 50\npublic figures of the IIIT-CFW database.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 09:59:00 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 10:00:23 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Jha", "Saurav", ""], ["Agarwal", "Nikhil", ""], ["Agarwal", "Suneeta", ""]]}, {"id": "1804.01771", "submitter": "Elena Burceanu", "authors": "Elena Burceanu, Marius Leordeanu", "title": "Learning a Robust Society of Tracking Parts using Co-occurrence\n  Constraints", "comments": "17+3 pages, 5 figures, European Conference on Computer Vision (ECCV),\n  Visual Object Tracking workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking is an essential problem in computer vision that has been\nresearched for several decades. One of the main challenges in tracking is to\nadapt to object appearance changes over time and avoiding drifting to\nbackground clutter. We address this challenge by proposing a deep neural\nnetwork composed of different parts, which functions as a society of tracking\nparts. They work in conjunction according to a certain policy and learn from\neach other in a robust manner, using co-occurrence constraints that ensure\nrobust inference and learning. From a structural point of view, our network is\ncomposed of two main pathways. One pathway is more conservative. It carefully\nmonitors a large set of simple tracker parts learned as linear filters over\ndeep feature activation maps. It assigns the parts different roles. It promotes\nthe reliable ones and removes the inconsistent ones. We learn these filters\nsimultaneously in an efficient way, with a single closed-form formulation, for\nwhich we propose novel theoretical properties. The second pathway is more\nprogressive. It is learned completely online and thus it is able to better\nmodel object appearance changes. In order to adapt in a robust manner, it is\nlearned only on highly confident frames, which are decided using co-occurrences\nwith the first pathway. Thus, our system has the full benefit of two main\napproaches in tracking. The larger set of simpler filter parts offers\nrobustness, while the full deep network learned online provides adaptability to\nchange. As shown in the experimental section, our approach achieves state of\nthe art performance on the challenging VOT17 benchmark, outperforming the\npublished methods both on the general EAO metric and in the number of fails, by\na significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 10:43:35 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 09:40:46 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Burceanu", "Elena", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1804.01793", "submitter": "Saumya Jetley", "authors": "Saumya Jetley, Naila Murray, Eleonora Vig", "title": "End-to-End Saliency Mapping via Probability Distribution Prediction", "comments": null, "journal-ref": "Proceedings of IEEE Conference on Computer Vision and Pattern\n  Recognition 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most saliency estimation methods aim to explicitly model low-level\nconspicuity cues such as edges or blobs and may additionally incorporate\ntop-down cues using face or text detection. Data-driven methods for training\nsaliency models using eye-fixation data are increasingly popular, particularly\nwith the introduction of large-scale datasets and deep architectures. However,\ncurrent methods in this latter paradigm use loss functions designed for\nclassification or regression tasks whereas saliency estimation is evaluated on\ntopographical maps. In this work, we introduce a new saliency map model which\nformulates a map as a generalized Bernoulli distribution. We then train a deep\narchitecture to predict such maps using novel loss functions which pair the\nsoftmax activation function with measures designed to compute distances between\nprobability distributions. We show in extensive experiments the effectiveness\nof such loss functions over standard ones on four public benchmark datasets,\nand demonstrate improved performance over state-of-the-art saliency methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 11:59:01 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Jetley", "Saumya", ""], ["Murray", "Naila", ""], ["Vig", "Eleonora", ""]]}, {"id": "1804.01824", "submitter": "Victor Escorcia", "authors": "Victor Escorcia and Cuong D. Dao and Mihir Jain and Bernard Ghanem and\n  Cees Snoek", "title": "Guess Where? Actor-Supervision for Spatiotemporal Action Localization", "comments": "cvpr version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper addresses the problem of spatiotemporal localization of actions in\nvideos. Compared to leading approaches, which all learn to localize based on\ncarefully annotated boxes on training video frames, we adhere to a\nweakly-supervised solution that only requires a video class label. We introduce\nan actor-supervised architecture that exploits the inherent compositionality of\nactions in terms of actor transformations, to localize actions. We make two\ncontributions. First, we propose actor proposals derived from a detector for\nhuman and non-human actors intended for images, which is linked over time by\nSiamese similarity matching to account for actor deformations. Second, we\npropose an actor-based attention mechanism that enables the localization of the\nactions from action class labels and actor proposals and is end-to-end\ntrainable. Experiments on three human and non-human action datasets show actor\nsupervision is state-of-the-art for weakly-supervised action localization and\nis even competitive to some fully-supervised alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 13:08:25 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Escorcia", "Victor", ""], ["Dao", "Cuong D.", ""], ["Jain", "Mihir", ""], ["Ghanem", "Bernard", ""], ["Snoek", "Cees", ""]]}, {"id": "1804.01900", "submitter": "Davood Zabihzadeh", "authors": "Baida Hamdan, Davood Zabihzadeh, Monsefi Reza", "title": "Large Scale Local Online Similarity/Distance Learning Framework based on\n  Passive/Aggressive", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity/Distance measures play a key role in many machine learning,\npattern recognition, and data mining algorithms, which leads to the emergence\nof metric learning field. Many metric learning algorithms learn a global\ndistance function from data that satisfy the constraints of the problem.\nHowever, in many real-world datasets that the discrimination power of features\nvaries in the different regions of input space, a global metric is often unable\nto capture the complexity of the task. To address this challenge, local metric\nlearning methods are proposed that learn multiple metrics across the different\nregions of input space. Some advantages of these methods are high flexibility\nand the ability to learn a nonlinear mapping but typically achieves at the\nexpense of higher time requirement and overfitting problem. To overcome these\nchallenges, this research presents an online multiple metric learning\nframework. Each metric in the proposed framework is composed of a global and a\nlocal component learned simultaneously. Adding a global component to a local\nmetric efficiently reduce the problem of overfitting. The proposed framework is\nalso scalable with both sample size and the dimension of input data. To the\nbest of our knowledge, this is the first local online similarity/distance\nlearning framework based on PA (Passive/Aggressive). In addition, for\nscalability with the dimension of input data, DRP (Dual Random Projection) is\nextended for local online learning in the present work. It enables our methods\nto be run efficiently on high-dimensional datasets, while maintains their\npredictive performance. The proposed framework provides a straightforward local\nextension to any global online similarity/distance learning algorithm based on\nPA.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 15:11:11 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Hamdan", "Baida", ""], ["Zabihzadeh", "Davood", ""], ["Reza", "Monsefi", ""]]}, {"id": "1804.01901", "submitter": "Stojan Trajanovski", "authors": "Stojan Trajanovski, Dimitrios Mavroeidis, Christine Leon Swisher,\n  Binyam Gebrekidan Gebre, Bastiaan S. Veeling, Rafael Wiemker, Tobias Klinder,\n  Amir Tahmasebi, Shawn M. Regis, Christoph Wald, Brady J. McKee, Sebastian\n  Flacke, Heber MacMahon, Homer Pien", "title": "Towards radiologist-level cancer risk assessment in CT lung screening\n  using deep learning", "comments": "Submitted for publication. 11 pages", "journal-ref": "Computerized Medical Imaging and Graphics, Elsevier, 2021", "doi": "10.1016/j.compmedimag.2021.101883", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance: Lung cancer is the leading cause of cancer mortality in the US,\nresponsible for more deaths than breast, prostate, colon and pancreas cancer\ncombined and it has been recently demonstrated that low-dose computed\ntomography (CT) screening of the chest can significantly reduce this death\nrate.\n  Objective: To compare the performance of a deep learning model to\nstate-of-the-art automated algorithms and radiologists as well as assessing the\nrobustness of the algorithm in heterogeneous datasets.\n  Design, Setting, and Participants: Three low-dose CT lung cancer screening\ndatasets from heterogeneous sources were used, including National Lung\nScreening Trial (NLST, n=3410), Lahey Hospital and Medical Center (LHMC,\nn=3174) data, Kaggle competition data (from both stages, n=1595+505) and the\nUniversity of Chicago data (UCM, a subset of NLST, annotated by radiologists,\nn=197). Relevant works on automated methods for Lung Cancer malignancy\nestimation have used significantly less data in size and diversity. At the\nfirst stage, our framework employs a nodule detector; while in the second\nstage, we use both the image area around the nodules and nodule features as\ninputs to a neural network that estimates the malignancy risk for the entire CT\nscan. We trained our two-stage algorithm on a part of the NLST dataset, and\nvalidated it on the other datasets.\n  Results, Conclusions, and Relevance: The proposed deep learning model: (a)\ngeneralizes well across all three data sets, achieving AUC between 86% to 94%;\n(b) has better performance than the widely accepted PanCan Risk Model,\nachieving 11% better AUC score; (c) has improved performance compared to the\nstate-of-the-art represented by the winners of the Kaggle Data Science Bowl\n2017 competition on lung cancer screening; (d) has comparable performance to\nradiologists in estimating cancer risk at a patient level.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 15:12:33 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 21:38:42 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Trajanovski", "Stojan", ""], ["Mavroeidis", "Dimitrios", ""], ["Swisher", "Christine Leon", ""], ["Gebre", "Binyam Gebrekidan", ""], ["Veeling", "Bastiaan S.", ""], ["Wiemker", "Rafael", ""], ["Klinder", "Tobias", ""], ["Tahmasebi", "Amir", ""], ["Regis", "Shawn M.", ""], ["Wald", "Christoph", ""], ["McKee", "Brady J.", ""], ["Flacke", "Sebastian", ""], ["MacMahon", "Heber", ""], ["Pien", "Homer", ""]]}, {"id": "1804.01910", "submitter": "Marie Piraud", "authors": "Marie Piraud and Anjany Sekuboyina and Bjoern H. Menze", "title": "Multi-level Activation for Segmentation of Hierarchically-nested Classes", "comments": "Accepted for the BioImage Computing 2018 workshop, ECCV conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many biological image segmentation tasks, including topological\nknowledge, such as the nesting of classes, can greatly improve results.\nHowever, most `out-of-the-box' CNN models are still blind to such prior\ninformation. In this paper, we propose a novel approach to encode this\ninformation, through a multi-level activation layer and three compatible\nlosses. We benchmark all of them on nuclei segmentation in bright-field\nmicroscopy cell images from the 2018 Data Science Bowl challenge, offering an\nexemplary segmentation task with cells and nested subcellular structures. Our\nscheme greatly speeds up learning, and outperforms standard multi-class\nclassification with soft-max activation and a previously proposed method\nstemming from it, improving the Dice score significantly (p-values<0.007). Our\napproach is conceptually simple, easy to implement and can be integrated in any\nCNN architecture. It can be generalized to a higher number of classes, with or\nwithout further relations of containment.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 15:27:02 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 09:59:59 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Piraud", "Marie", ""], ["Sekuboyina", "Anjany", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "1804.01961", "submitter": "Lucia Ballerini", "authors": "Enrico Pellegrini, Lucia Ballerini, Maria del C. Valdes Hernandez,\n  Francesca M. Chappell, Victor Gonz\\'alez-Castro, Devasuda Anblagan, Samuel\n  Danso, Susana Mu\\~noz Maniega, Dominic Job, Cyril Pernet, Grant Mair, Tom\n  MacGillivray, Emanuele Trucco, Joanna Wardlaw", "title": "Machine learning of neuroimaging to diagnose cognitive impairment and\n  dementia: a systematic review and comparative analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  INTRODUCTION: Advanced machine learning methods might help to identify\ndementia risk from neuroimaging, but their accuracy to date is unclear.\n  METHODS: We systematically reviewed the literature, 2006 to late 2016, for\nmachine learning studies differentiating healthy ageing through to dementia of\nvarious types, assessing study quality, and comparing accuracy at different\ndisease boundaries.\n  RESULTS: Of 111 relevant studies, most assessed Alzheimer's disease (AD) vs\nhealthy controls, used ADNI data, support vector machines and only T1-weighted\nsequences. Accuracy was highest for differentiating AD from healthy controls,\nand poor for differentiating healthy controls vs MCI vs AD, or MCI converters\nvs non-converters. Accuracy increased using combined data types, but not by\ndata source, sample size or machine learning method.\n  DISCUSSION: Machine learning does not differentiate clinically-relevant\ndisease categories yet. More diverse datasets, combinations of different types\nof data, and close clinical integration of machine learning would help to\nadvance the field.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 17:17:39 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 22:01:51 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Pellegrini", "Enrico", ""], ["Ballerini", "Lucia", ""], ["Hernandez", "Maria del C. Valdes", ""], ["Chappell", "Francesca M.", ""], ["Gonz\u00e1lez-Castro", "Victor", ""], ["Anblagan", "Devasuda", ""], ["Danso", "Samuel", ""], ["Maniega", "Susana Mu\u00f1oz", ""], ["Job", "Dominic", ""], ["Pernet", "Cyril", ""], ["Mair", "Grant", ""], ["MacGillivray", "Tom", ""], ["Trucco", "Emanuele", ""], ["Wardlaw", "Joanna", ""]]}, {"id": "1804.01962", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz, Adam Czajka, Piotr Maciejewicz", "title": "Iris Recognition After Death", "comments": "Paper accepted for publication in the IEEE Transactions on\n  Information Forensics and Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a comprehensive study of post-mortem human iris\nrecognition carried out for 1,200 near-infrared and 1,787 visible-light samples\ncollected from 37 deceased individuals kept in the mortuary conditions. We used\nfour independent iris recognition methods (three commercial and one academic)\nto analyze genuine and impostor comparison scores and check the dynamics of\niris quality decay over a period of up to 814 hours after death. This study\nshows that post-mortem iris recognition may be close-to-perfect approximately 5\nto 7 hours after death and occasionally is still viable even 21 days after\ndeath. These conclusions contradict the statements present in past literature\nthat the iris is unusable as a biometrics shortly after death, and show that\nthe dynamics of post-mortem changes to the iris that are important for\nbiometric identification are more moderate than previously hypothesized. The\npaper contains a thorough medical commentary that helps to understand which\npost-mortem metamorphoses of the eye may impact the performance of automatic\niris recognition. We also show that post-mortem iris recognition works equally\nwell for images taken in near-infrared and when the red channel of\nvisible-light sample is used. However, cross-wavelength matching presents\nsignificantly worse performance. This paper conforms to reproducible research\nand the database used in this study is made publicly available to facilitate\nresearch of post-mortem iris recognition. To our knowledge, this paper offers\nthe most comprehensive evaluation of post-mortem iris recognition and the\nlargest database of post-mortem iris images.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 17:19:26 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 10:37:06 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Maciejewicz", "Piotr", ""]]}, {"id": "1804.01967", "submitter": "Konstantinos Batsos", "authors": "Konstantinos Batsos, Changjiang Cai and Philippos Mordohai", "title": "CBMV: A Coalesced Bidirectional Matching Volume for Disparity Estimation", "comments": "Accepted to Computer Vision and Pattern Recognition (CVPR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a paradigm shift in stereo matching with\nlearning-based methods achieving the best results on all popular benchmarks.\nThe success of these methods is due to the availability of training data with\nground truth; training learning-based systems on these datasets has allowed\nthem to surpass the accuracy of conventional approaches based on heuristics and\nassumptions. Many of these assumptions, however, had been validated extensively\nand hold for the majority of possible inputs. In this paper, we generate a\nmatching volume leveraging both data with ground truth and conventional wisdom.\nWe accomplish this by coalescing diverse evidence from a bidirectional matching\nprocess via random forest classifiers. We show that the resulting matching\nvolume estimation method achieves similar accuracy to purely data-driven\nalternatives on benchmarks and that it generalizes to unseen data much better.\nIn fact, the results we submitted to the KITTI and ETH3D benchmarks were\ngenerated using a classifier trained on the Middlebury 2014 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 17:30:56 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Batsos", "Konstantinos", ""], ["Cai", "Changjiang", ""], ["Mordohai", "Philippos", ""]]}, {"id": "1804.01983", "submitter": "Longhao Yuan", "authors": "Longhao Yuan, Qibin Zhao, Lihua Gui and Jianting Cao", "title": "High-dimension Tensor Completion via Gradient-based Optimization Under\n  Tensor-train Format", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor train (TT) decomposition has drawn people's attention due to its\npowerful representation ability and performance stability in high-order\ntensors. In this paper, we propose a novel approach to recover the missing\nentries of incomplete data represented by higher-order tensors. We attempt to\nfind the low-rank TT decomposition of the incomplete data which captures the\nlatent features of the whole data and then reconstruct the missing entries. By\napplying gradient descent algorithms, tensor completion problem is efficiently\nsolved by optimization models. We propose two TT-based algorithms: Tensor Train\nWeighted Optimization (TT-WOPT) and Tensor Train Stochastic Gradient Descent\n(TT-SGD) to optimize TT decomposition factors. In addition, a method named\nVisual Data Tensorization (VDT) is proposed to transform visual data into\nhigher-order tensors, resulting in the performance improvement of our\nalgorithms. The experiments in synthetic data and visual data show high\nefficiency and performance of our algorithms compared to the state-of-the-art\ncompletion algorithms, especially in high-order, high missing rate, and\nlarge-scale tensor completion situations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 02:06:28 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 09:03:23 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2018 03:21:55 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Yuan", "Longhao", ""], ["Zhao", "Qibin", ""], ["Gui", "Lihua", ""], ["Cao", "Jianting", ""]]}, {"id": "1804.01984", "submitter": "Liang Lin", "authors": "Xiaodan Liang and Ke Gong and Xiaohui Shen and Liang Lin", "title": "Look into Person: Joint Body Parsing & Pose Estimation Network and A New\n  Benchmark", "comments": "We proposed the most comprehensive dataset around the world for\n  human-centric analysis! (Accepted By T-PAMI 2018) The dataset, code and\n  models are available at http://www.sysu-hcp.net/lip/ . arXiv admin note:\n  substantial text overlap with arXiv:1703.05446", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human parsing and pose estimation have recently received considerable\ninterest due to their substantial application potentials. However, the existing\ndatasets have limited numbers of images and annotations and lack a variety of\nhuman appearances and coverage of challenging cases in unconstrained\nenvironments. In this paper, we introduce a new benchmark named \"Look into\nPerson (LIP)\" that provides a significant advancement in terms of scalability,\ndiversity, and difficulty, which are crucial for future developments in\nhuman-centric analysis. This comprehensive dataset contains over 50,000\nelaborately annotated images with 19 semantic part labels and 16 body joints,\nwhich are captured from a broad range of viewpoints, occlusions, and background\ncomplexities. Using these rich annotations, we perform detailed analyses of the\nleading human parsing and pose estimation approaches, thereby obtaining\ninsights into the successes and failures of these methods. To further explore\nand take advantage of the semantic correlation of these two tasks, we propose a\nnovel joint human parsing and pose estimation network to explore efficient\ncontext modeling, which can simultaneously predict parsing and pose with\nextremely high quality. Furthermore, we simplify the network to solve human\nparsing by exploring a novel self-supervised structure-sensitive learning\napproach, which imposes human pose structures into the parsing results without\nresorting to extra supervision. The dataset, code and models are available at\nhttp://www.sysu-hcp.net/lip/.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 07:41:15 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Liang", "Xiaodan", ""], ["Gong", "Ke", ""], ["Shen", "Xiaohui", ""], ["Lin", "Liang", ""]]}, {"id": "1804.02009", "submitter": "Michael Maire", "authors": "Mohammadreza Mostajabi, Michael Maire, Gregory Shakhnarovich", "title": "Regularizing Deep Networks by Modeling and Predicting Label Structure", "comments": "to appear at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct custom regularization functions for use in supervised training\nof deep neural networks. Our technique is applicable when the ground-truth\nlabels themselves exhibit internal structure; we derive a regularizer by\nlearning an autoencoder over the set of annotations. Training thereby becomes a\ntwo-phase procedure. The first phase models labels with an autoencoder. The\nsecond phase trains the actual network of interest by attaching an auxiliary\nbranch that must predict output via a hidden layer of the autoencoder. After\ntraining, we discard this auxiliary branch.\n  We experiment in the context of semantic segmentation, demonstrating this\nregularization strategy leads to consistent accuracy boosts over baselines,\nboth when training from scratch, or in combination with ImageNet pretraining.\nGains are also consistent over different choices of convolutional network\narchitecture. As our regularizer is discarded after training, our method has\nzero cost at test time; the performance improvements are essentially free. We\nare simply able to learn better network weights by building an abstract model\nof the label space, and then training the network to understand this\nabstraction alongside the original task.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 18:17:18 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Mostajabi", "Mohammadreza", ""], ["Maire", "Michael", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "1804.02032", "submitter": "Li Sulimowicz Mrs.", "authors": "Li Sulimowicz, Ishfaq Ahmad, Alexander Aved", "title": "A Multi-Layer Approach to Superpixel-based Higher-order Conditional\n  Random Field for Semantic Image Segmentation", "comments": "7pages, 6 figures, 3 tables, an updated version from original\n  submission to ICME2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Superpixel-based Higher-order Conditional random fields (SP-HO-CRFs) are\nknown for their effectiveness in enforcing both short and long spatial\ncontiguity for pixelwise labelling in computer vision. However, their\nhigher-order potentials are usually too complex to learn and often incur a high\ncomputational cost in performing inference. We propose an new approximation\napproach to SP-HO-CRFs that resolves these problems. Our approach is a\nmulti-layer CRF framework that inherits the simplicity from pairwise CRFs by\nformulating both the higher-order and pairwise cues into the same pairwise\npotentials in the first layer. Essentially, this approach provides accuracy\nenhancement on the basis of pairwise CRFs without training by reusing their\npre-trained parameters and/or weights. The proposed multi-layer approach\nperforms especially well in delineating the boundary details (boarders) of\nobject categories such as \"trees\" and \"bushes\". Multiple sets of experiments\nconducted on dataset MSRC-21 and PASCAL VOC 2012 validate the effectiveness and\nefficiency of the proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 19:31:03 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Sulimowicz", "Li", ""], ["Ahmad", "Ishfaq", ""], ["Aved", "Alexander", ""]]}, {"id": "1804.02047", "submitter": "Yu Cheng", "authors": "Xi Ouyang, Yu Cheng, Yifan Jiang, Chun-Liang Li, Pan Zhou", "title": "Pedestrian-Synthesis-GAN: Generating Pedestrian Data in Real Scene and\n  Beyond", "comments": "v2.0,adding supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art pedestrian detection models have achieved great success in\nmany benchmarks. However, these models require lots of annotation information\nand the labeling process usually takes much time and efforts. In this paper, we\npropose a method to generate labeled pedestrian data and adapt them to support\nthe training of pedestrian detectors. The proposed framework is built on the\nGenerative Adversarial Network (GAN) with multiple discriminators, trying to\nsynthesize realistic pedestrians and learn the background context\nsimultaneously. To handle the pedestrians of different sizes, we adopt the\nSpatial Pyramid Pooling (SPP) layer in the discriminator. We conduct\nexperiments on two benchmarks. The results show that our framework can smoothly\nsynthesize pedestrians on background images of variations and different levels\nof details. To quantitatively evaluate our approach, we add the generated\nsamples into training data of the baseline pedestrian detectors and show the\nsynthetic images are able to improve the detectors' performance.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 20:22:01 GMT"}, {"version": "v2", "created": "Sat, 14 Apr 2018 07:19:28 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Ouyang", "Xi", ""], ["Cheng", "Yu", ""], ["Jiang", "Yifan", ""], ["Li", "Chun-Liang", ""], ["Zhou", "Pan", ""]]}, {"id": "1804.02051", "submitter": "Shiv Ram Dubey", "authors": "Shiv Ram Dubey, Soumendu Chakraborty", "title": "Average Biased ReLU Based CNN Descriptor for Improved Face Retrieval", "comments": "9 Pages, 3 Figures, 8 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional neural networks (CNN) like AlexNet, GoogleNet, VGGNet, etc.\nhave been proven as the very discriminative feature descriptor for many\ncomputer vision problems. The trained CNN model over one dataset performs\nreasonably well over another dataset of similar type and outperforms the\nhand-designed feature descriptor. The Rectified Linear Unit (ReLU) layer\ndiscards some information in order to introduce the non-linearity. In this\npaper, it is proposed that the discriminative ability of deep image\nrepresentation using trained model can be improved by Average Biased ReLU\n(AB-ReLU) at last few layers. Basically, AB-ReLU improves the discriminative\nability by two ways: 1) it also exploits some of the discriminative and\ndiscarded negative information of ReLU and 2) it kills the irrelevant and\npositive information used by ReLU. The VGGFace model already trained in\nMatConvNet over the VGG-Face dataset is used as the feature descriptor for face\nretrieval over other face datasets. The proposed approach is tested over six\nchallenging unconstrained and robust face datasets like PubFig, LFW, PaSC, AR,\netc. in retrieval framework. It is observed that AB-ReLU is consistently\nperformed better than ReLU using VGGFace pretrained model over face datasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 15:03:02 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Dubey", "Shiv Ram", ""], ["Chakraborty", "Soumendu", ""]]}, {"id": "1804.02056", "submitter": "Xiaojiang Du", "authors": "Sijia Chen, Bin Song, Jie Guo, Xiaojiang Du, Mohsen Guizani", "title": "FPAN: Fine-grained and Progressive Attention Localization Network for\n  Data Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Localization of the target object for data retrieval is a key issue in\nthe Intelligent and Connected Transportation Systems (ICTS). However, due to\nlack of intelligence in the traditional transportation system, it can take\ntremendous resources to manually retrieve and locate the queried objects among\na large number of images. In order to solve this issue, we propose an effective\nmethod to query-based object localization that uses artificial intelligence\ntechniques to automatically locate the queried object in the complex\nbackground. The presented method is termed as Fine-grained and Progressive\nAttention Localization Network (FPAN), which uses an image and a queried object\nas input to accurately locate the target object in the image. Specifically, the\nfine-grained attention module is naturally embedded into each layer of the\nconvolution neural network (CNN), thereby gradually suppressing the regions\nthat are irrelevant to the queried object and eventually shrinking attention to\nthe target area. We further employ top-down attentions fusion algorithm\noperated by a learnable cascade up-sampling structure to establish the\nconnection between the attention map and the exact location of the queried\nobject in the original image. Furthermore, the FPAN is trained by multi-task\nlearning with box segmentation loss and cosine loss. At last, we conduct\ncomprehensive experiments on both queried-based digit localization and object\ntracking with synthetic and benchmark datasets, respectively. The experimental\nresults show that our algorithm is far superior to other algorithms in the\nsynthesis datasets and outperforms most existing trackers on the OTB and VOT\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 20:59:36 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Chen", "Sijia", ""], ["Song", "Bin", ""], ["Guo", "Jie", ""], ["Du", "Xiaojiang", ""], ["Guizani", "Mohsen", ""]]}, {"id": "1804.02062", "submitter": "James Theiler", "authors": "James Theiler, Beate Zimmer, Amanda Ziemann", "title": "Closed-form detector for solid sub-pixel targets in multivariate\n  t-distributed background clutter", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": "LA-UR-18-20163", "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized likelihood ratio test (GLRT) is used to derive a detector for\nsolid sub-pixel targets in hyperspectral imagery. A closed-form solution is\nobtained that optimizes the replacement target model when the background is a\nfat-tailed elliptically-contoured multivariate t-distribution. This generalizes\nGLRT-based detectors that have previously been derived for the replacement\ntarget model with Gaussian background, and for the additive target model with\nan elliptically-contoured background. Experiments with simulated hyperspectral\ndata illustrate the performance of this detector in various parameter regimes.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 21:28:51 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 19:12:42 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Theiler", "James", ""], ["Zimmer", "Beate", ""], ["Ziemann", "Amanda", ""]]}, {"id": "1804.02077", "submitter": "Dmytro Bobkov", "authors": "Dmytro Bobkov, Sili Chen, Ruiqing Jian, Muhammad Iqbal, Eckehard\n  Steinbach", "title": "Noise-resistant Deep Learning for Object Classification in 3D Point\n  Clouds Using a Point Pair Descriptor", "comments": "8 pages", "journal-ref": "IEEE Robotics and Automation Letters 2018 Volume 3, Issue 2 IEEE\n  Robotics and Automation Letters IEEE Robotics and Automation Letters", "doi": "10.1109/LRA.2018.2792681", "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object retrieval and classification in point cloud data is challenged by\nnoise, irregular sampling density and occlusion. To address this issue, we\npropose a point pair descriptor that is robust to noise and occlusion and\nachieves high retrieval accuracy. We further show how the proposed descriptor\ncan be used in a 4D convolutional neural network for the task of object\nclassification. We propose a novel 4D convolutional layer that is able to learn\nclass-specific clusters in the descriptor histograms. Finally, we provide\nexperimental validation on 3 benchmark datasets, which confirms the superiority\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 23:19:55 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Bobkov", "Dmytro", ""], ["Chen", "Sili", ""], ["Jian", "Ruiqing", ""], ["Iqbal", "Muhammad", ""], ["Steinbach", "Eckehard", ""]]}, {"id": "1804.02085", "submitter": "Jiaqi Yang", "authors": "Jiaqi Yang, Ke Xian, Yang Xiao, Zhiguo Cao", "title": "Performance Evaluation of 3D Correspondence Grouping Algorithms", "comments": "Accepted to 3DV 2017, (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a thorough evaluation of several widely-used 3D\ncorrespondence grouping algorithms, motived by their significance in vision\ntasks relying on correct feature correspondences. A good correspondence\ngrouping algorithm is desired to retrieve as many as inliers from initial\nfeature matches, giving a rise in both precision and recall. Towards this rule,\nwe deploy the experiments on three benchmarks respectively addressing shape\nretrieval, 3D object recognition and point cloud registration scenarios. The\nvariety in application context brings a rich category of nuisances including\nnoise, varying point densities, clutter, occlusion and partial overlaps. It\nalso results to different ratios of inliers and correspondence distributions\nfor comprehensive evaluation. Based on the quantitative outcomes, we give a\nsummarization of the merits/demerits of the evaluated algorithms from both\nperformance and efficiency perspectives.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 00:10:18 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Yang", "Jiaqi", ""], ["Xian", "Ke", ""], ["Xiao", "Yang", ""], ["Cao", "Zhiguo", ""]]}, {"id": "1804.02088", "submitter": "Yang Shi", "authors": "Yang Shi and Tommaso Furlanello and Sheng Zha and Animashree\n  Anandkumar", "title": "Question Type Guided Attention in Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) requires integration of feature maps with\ndrastically different structures and focus of the correct regions. Image\ndescriptors have structures at multiple spatial scales, while lexical inputs\ninherently follow a temporal sequence and naturally cluster into semantically\ndifferent question types. A lot of previous works use complex models to extract\nfeature representations but neglect to use high-level information summary such\nas question types in learning. In this work, we propose Question Type-guided\nAttention (QTA). It utilizes the information of question type to dynamically\nbalance between bottom-up and top-down visual features, respectively extracted\nfrom ResNet and Faster R-CNN networks. We experiment with multiple VQA\narchitectures with extensive input ablation studies over the TDIUC dataset and\nshow that QTA systematically improves the performance by more than 5% across\nmultiple question type categories such as \"Activity Recognition\", \"Utility\" and\n\"Counting\" on TDIUC dataset. By adding QTA on the state-of-art model MCB, we\nachieve 3% improvement for overall accuracy. Finally, we propose a multi-task\nextension to predict question types which generalizes QTA to applications that\nlack of question type, with minimal performance loss.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 00:28:57 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 20:43:42 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Shi", "Yang", ""], ["Furlanello", "Tommaso", ""], ["Zha", "Sheng", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1804.02119", "submitter": "Micha{\\l} Byra", "authors": "Michal Byra, Tomasz Sznajder, Danijel Korzinek, Hanna\n  Piotrzkowska-Wroblewska, Katarzyna Dobruch-Sobczak, Andrzej Nowicki and\n  Krzysztof Marasek", "title": "Impact of ultrasound image reconstruction method on breast lesion\n  classification with neural transfer learning", "comments": "6 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms, especially convolutional neural networks, have\nbecome a methodology of choice in medical image analysis. However, recent\nstudies in computer vision show that even a small modification of input image\nintensities may cause a deep learning model to classify the image differently.\nIn medical imaging, the distribution of image intensities is related to applied\nimage reconstruction algorithm. In this paper we investigate the impact of\nultrasound image reconstruction method on breast lesion classification with\nneural transfer learning. Due to high dynamic range raw ultrasonic signals are\ncommonly compressed in order to reconstruct B-mode images. Based on raw data\nacquired from breast lesions, we reconstruct B-mode images using different\ncompression levels. Next, transfer learning is applied for classification.\nDifferently reconstructed images are employed for training and evaluation. We\nshow that the modification of the reconstruction algorithm leads to decrease of\nclassification performance. As a remedy, we propose a method of data\naugmentation. We show that the augmentation of the training set with\ndifferently reconstructed B-mode images leads to a more robust and efficient\nclassification. Our study suggests that it is important to take into account\nimage reconstruction algorithms implemented in medical scanners during\ndevelopment of computer aided diagnosis systems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 03:07:09 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Byra", "Michal", ""], ["Sznajder", "Tomasz", ""], ["Korzinek", "Danijel", ""], ["Piotrzkowska-Wroblewska", "Hanna", ""], ["Dobruch-Sobczak", "Katarzyna", ""], ["Nowicki", "Andrzej", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1804.02142", "submitter": "Xun Xu", "authors": "Xun Xu, Loong-Fah Cheong, Zhuwen Li", "title": "Motion Segmentation by Exploiting Complementary Geometric Models", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world sequences cannot be conveniently categorized as general or\ndegenerate; in such cases, imposing a false dichotomy in using the fundamental\nmatrix or homography model for motion segmentation would lead to difficulty.\nEven when we are confronted with a general scene-motion, the fundamental matrix\napproach as a model for motion segmentation still suffers from several defects,\nwhich we discuss in this paper. The full potential of the fundamental matrix\napproach could only be realized if we judiciously harness information from the\nsimpler homography model. From these considerations, we propose a multi-view\nspectral clustering framework that synergistically combines multiple models\ntogether. We show that the performance can be substantially improved in this\nway. We perform extensive testing on existing motion segmentation datasets,\nachieving state-of-the-art performance on all of them; we also put forth a more\nrealistic and challenging dataset adapted from the KITTI benchmark, containing\nreal-world effects such as strong perspectives and strong forward translations\nnot seen in the traditional datasets.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 06:04:45 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Xu", "Xun", ""], ["Cheong", "Loong-Fah", ""], ["Li", "Zhuwen", ""]]}, {"id": "1804.02152", "submitter": "Franziska Schirrmacher", "authors": "Franziska Schirrmacher, Thomas K\u007f\\\"ohler and Christian Riess", "title": "Adaptive Quantile Sparse Image (AQuaSI) Prior for Inverse Imaging\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems play a central role for many classical computer vision and\nimage processing tasks. Many inverse problems are ill-posed, and hence require\na prior to regularize the solution space. However, many of the existing priors,\nlike total variation, are based on ad-hoc assumptions that have difficulties to\nrepresent the actual distribution of natural images. Thus, a key challenge in\nresearch on image processing is to find better suited priors to represent\nnatural images.\n  In this work, we propose the Adaptive Quantile Sparse Image (AQuaSI) prior.\nIt is based on a quantile filter, can be used as a joint filter on guidance\ndata, and be readily plugged into a wide range of numerical optimization\nalgorithms. We demonstrate the efficacy of the proposed prior in joint\nRGB/depth upsampling, on RGB/NIR image restoration, and in a comparison with\nrelated regularization by denoising approaches.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 07:18:54 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 16:18:03 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Schirrmacher", "Franziska", ""], ["K\u007f\u00f6hler", "Thomas", ""], ["Riess", "Christian", ""]]}, {"id": "1804.02156", "submitter": "Ben Talbot", "authors": "Ben Talbot, Sourav Garg, and Michael Milford", "title": "OpenSeqSLAM2.0: An Open Source Toolbox for Visual Place Recognition\n  Under Changing Conditions", "comments": "8 pages, Submitted to IROS 2018 (2018 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems), see\n  http://www.michaelmilford.com/seqslam for access to the software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually recognising a traversed route - regardless of whether seen during\nthe day or night, in clear or inclement conditions, or in summer or winter - is\nan important capability for navigating robots. Since SeqSLAM was introduced in\n2012, a large body of work has followed exploring how robotic systems can use\nthe algorithm to meet the challenges posed by navigation in changing\nenvironmental conditions. The following paper describes OpenSeqSLAM2.0, a fully\nopen source toolbox for visual place recognition under changing conditions.\nBeyond the benefits of open access to the source code, OpenSeqSLAM2.0 provides\na number of tools to facilitate exploration of the visual place recognition\nproblem and interactive parameter tuning. Using the new open source platform,\nit is shown for the first time how comprehensive parameter characterisations\nprovide new insights into many of the system components previously presented in\nad hoc ways and provide users with a guide to what system component options\nshould be used under what circumstances and why.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 07:37:45 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 06:25:05 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Talbot", "Ben", ""], ["Garg", "Sourav", ""], ["Milford", "Michael", ""]]}, {"id": "1804.02176", "submitter": "Chenyang Lu", "authors": "Chenyang Lu, Marinus Jacobus Gerardus van de Molengraft, Gijs\n  Dubbelman", "title": "Monocular Semantic Occupancy Grid Mapping with Convolutional Variational\n  Encoder-Decoder Networks", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2019.2891028", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we research and evaluate end-to-end learning of monocular\nsemantic-metric occupancy grid mapping from weak binocular ground truth. The\nnetwork learns to predict four classes, as well as a camera to bird's eye view\nmapping. At the core, it utilizes a variational encoder-decoder network that\nencodes the front-view visual information of the driving scene and subsequently\ndecodes it into a 2-D top-view Cartesian coordinate system. The evaluations on\nCityscapes show that the end-to-end learning of semantic-metric occupancy grids\noutperforms the deterministic mapping approach with flat-plane assumption by\nmore than 12% mean IoU. Furthermore, we show that the variational sampling with\na relatively small embedding vector brings robustness against vehicle dynamic\nperturbations, and generalizability for unseen KITTI data. Our network achieves\nreal-time inference rates of approx. 35 Hz for an input image with a resolution\nof 256x512 pixels and an output map with 64x64 occupancy grid cells using a\nTitan V GPU.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 09:38:35 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 07:39:12 GMT"}, {"version": "v3", "created": "Mon, 31 Dec 2018 18:23:05 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Lu", "Chenyang", ""], ["van de Molengraft", "Marinus Jacobus Gerardus", ""], ["Dubbelman", "Gijs", ""]]}, {"id": "1804.02199", "submitter": "Luis Herranz", "authors": "Yaxing Wang, Joost van de Weijer, Luis Herranz", "title": "Mix and match networks: encoder-decoder alignment for zero-pair image\n  translation", "comments": "Accepted CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of image translation between domains or modalities for\nwhich no direct paired data is available (i.e. zero-pair translation). We\npropose mix and match networks, based on multiple encoders and decoders aligned\nin such a way that other encoder-decoder pairs can be composed at test time to\nperform unseen image translation tasks between domains or modalities for which\nexplicit paired samples were not seen during training. We study the impact of\nautoencoders, side information and losses in improving the alignment and\ntransferability of trained pairwise translation models to unseen translations.\nWe show our approach is scalable and can perform colorization and style\ntransfer between unseen combinations of domains. We evaluate our system in a\nchallenging cross-modal setting where semantic segmentation is estimated from\ndepth images, without explicit access to any depth-semantic segmentation\ntraining pairs. Our model outperforms baselines based on pix2pix and CycleGAN\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 10:53:07 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Wang", "Yaxing", ""], ["van de Weijer", "Joost", ""], ["Herranz", "Luis", ""]]}, {"id": "1804.02201", "submitter": "Dengxin Dai", "authors": "Dengxin Dai, Wen Li, Till Kroeger, Luc Van Gool", "title": "Ensemble Manifold Segmentation for Model Distillation and\n  Semi-supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold theory has been the central concept of many learning methods.\nHowever, learning modern CNNs with manifold structures has not raised due\nattention, mainly because of the inconvenience of imposing manifold structures\nonto the architecture of the CNNs. In this paper we present ManifoldNet, a\nnovel method to encourage learning of manifold-aware representations. Our\napproach segments the input manifold into a set of fragments. By assigning the\ncorresponding segmentation id as a pseudo label to every sample, we convert the\nproblem of preserving the local manifold structure into a point-wise\nclassification task. Due to its unsupervised nature, the segmentation tends to\nbe noisy. We mitigate this by introducing ensemble manifold segmentation (EMS).\nEMS accounts for the manifold structure by dividing the training data into an\nensemble of classification training sets that contain samples of local\nproximity. CNNs are trained on these ensembles under a multi-task learning\nframework to conform to the manifold. ManifoldNet can be trained with only the\npseudo labels or together with task-specific labels. We evaluate ManifoldNet on\ntwo different tasks: network imitation (distillation) and semi-supervised\nlearning. Our experiments show that the manifold structures are effectively\nutilized for both unsupervised and semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 10:55:16 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Dai", "Dengxin", ""], ["Li", "Wen", ""], ["Kroeger", "Till", ""], ["Van Gool", "Luc", ""]]}, {"id": "1804.02205", "submitter": "Matthias Zeppelzauer", "authors": "Matthias Zeppelzauer, Miroslav Despotovic, Muntaha Sakeena, David\n  Koch, Mario D\\\"oller", "title": "Automatic Prediction of Building Age from Photographs", "comments": "Preprint of paper to appear in ACM International Conference on\n  Multimedia Retrieval (ICMR) 2018 Conference", "journal-ref": null, "doi": "10.1145/3206025.3206060", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a first method for the automated age estimation of buildings from\nunconstrained photographs. To this end, we propose a two-stage approach that\nfirstly learns characteristic visual patterns for different building epochs at\npatch-level and then globally aggregates patch-level age estimates over the\nbuilding. We compile evaluation datasets from different sources and perform an\ndetailed evaluation of our approach, its sensitivity to parameters, and the\ncapabilities of the employed deep networks to learn characteristic visual\nage-related patterns. Results show that our approach is able to estimate\nbuilding age at a surprisingly high level that even outperforms human\nevaluators and thereby sets a new performance baseline. This work represents a\nfirst step towards the automated assessment of building parameters for\nautomated price prediction.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 11:06:43 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 17:45:38 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Zeppelzauer", "Matthias", ""], ["Despotovic", "Miroslav", ""], ["Sakeena", "Muntaha", ""], ["Koch", "David", ""], ["D\u00f6ller", "Mario", ""]]}, {"id": "1804.02307", "submitter": "Ganesh Sundaramoorthi", "authors": "Ganesh Sundaramoorthi and Anthony Yezzi", "title": "Accelerated Optimization in the PDE Framework: Formulations for the\n  Manifold of Diffeomorphisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of optimization of cost functionals on the\ninfinite-dimensional manifold of diffeomorphisms. We present a new class of\noptimization methods, valid for any optimization problem setup on the space of\ndiffeomorphisms by generalizing Nesterov accelerated optimization to the\nmanifold of diffeomorphisms. While our framework is general for infinite\ndimensional manifolds, we specifically treat the case of diffeomorphisms,\nmotivated by optical flow problems in computer vision. This is accomplished by\nbuilding on a recent variational approach to a general class of accelerated\noptimization methods by Wibisono, Wilson and Jordan, which applies in finite\ndimensions. We generalize that approach to infinite dimensional manifolds. We\nderive the surprisingly simple continuum evolution equations, which are partial\ndifferential equations, for accelerated gradient descent, and relate it to\nsimple mechanical principles from fluid mechanics. Our approach has natural\nconnections to the optimal mass transport problem. This is because one can\nthink of our approach as an evolution of an infinite number of particles\nendowed with mass (represented with a mass density) that moves in an energy\nlandscape. The mass evolves with the optimization variable, and endows the\nparticles with dynamics. This is different than the finite dimensional case\nwhere only a single particle moves and hence the dynamics does not depend on\nthe mass. We derive the theory, compute the PDEs for accelerated optimization,\nand illustrate the behavior of these new accelerated optimization schemes.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 19:58:03 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 21:38:57 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Sundaramoorthi", "Ganesh", ""], ["Yezzi", "Anthony", ""]]}, {"id": "1804.02343", "submitter": "Juan Terven", "authors": "Diana-Margarita C\\'ordova-Esparza, Juan Terven, Hugo\n  Jim\\'enez-Hern\\'andez, Ana Herrera-Navarro, Alberto V\\'azquez-Cervantes\n  Juan-M. Garc\\'ia-Huerta", "title": "Telepresence System based on Simulated Holographic Display", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a telepresence system based on a custom-made simulated holographic\ndisplay that produces a full 3D model of the remote participants using\ncommodity depth sensors. Our display is composed of a video projector and a\nquadrangular pyramid made of acrylic, that allows the user to experience an\nomnidirectional visualization of a remote person without the need for\nhead-mounted displays. To obtain a precise representation of the participants,\nwe fuse together multiple views extracted using a deep background subtraction\nmethod. Our system represents an attempt to democratize high-fidelity 3D\ntelepresence using off-the-shelf components.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 16:14:34 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["C\u00f3rdova-Esparza", "Diana-Margarita", ""], ["Terven", "Juan", ""], ["Jim\u00e9nez-Hern\u00e1ndez", "Hugo", ""], ["Herrera-Navarro", "Ana", ""], ["Garc\u00eda-Huerta", "Alberto V\u00e1zquez-Cervantes Juan-M.", ""]]}, {"id": "1804.02367", "submitter": "Bailey Kong", "authors": "Bailey Kong, James Supancic, Deva Ramanan, Charless C. Fowlkes", "title": "Cross-Domain Image Matching with Deep Feature Maps", "comments": null, "journal-ref": null, "doi": "10.1007/s11263-018-01143-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of automatically determining what type of shoe\nleft an impression found at a crime scene. This recognition problem is made\ndifficult by the variability in types of crime scene evidence (ranging from\ntraces of dust or oil on hard surfaces to impressions made in soil) and the\nlack of comprehensive databases of shoe outsole tread patterns. We find that\nmid-level features extracted by pre-trained convolutional neural nets are\nsurprisingly effective descriptors for this specialized domains. However, the\nchoice of similarity measure for matching exemplars to a query image is\nessential to good performance. For matching multi-channel deep features, we\npropose the use of multi-channel normalized cross-correlation and analyze its\neffectiveness. Our proposed metric significantly improves performance in\nmatching crime scene shoeprints to laboratory test impressions. We also show\nits effectiveness in other cross-domain image retrieval problems: matching\nfacade images to segmentation labels and aerial photos to map images. Finally,\nwe introduce a discriminatively trained variant and fine-tune our system\nthrough our proposed metric, obtaining state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 17:35:43 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 04:16:17 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Kong", "Bailey", ""], ["Supancic", "James", ""], ["Ramanan", "Deva", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1804.02379", "submitter": "Changha Shin", "authors": "Changha Shin, Hae-Gon Jeon, Youngjin Yoon, In So Kweon, Seon Joo Kim", "title": "EPINET: A Fully-Convolutional Neural Network Using Epipolar Geometry for\n  Depth from Light Field Images", "comments": "Accepted to CVPR 2018, Total 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field cameras capture both the spatial and the angular properties of\nlight rays in space. Due to its property, one can compute the depth from light\nfields in uncontrolled lighting environments, which is a big advantage over\nactive sensing devices. Depth computed from light fields can be used for many\napplications including 3D modelling and refocusing. However, light field images\nfrom hand-held cameras have very narrow baselines with noise, making the depth\nestimation difficult. any approaches have been proposed to overcome these\nlimitations for the light field depth estimation, but there is a clear\ntrade-off between the accuracy and the speed in these methods. In this paper,\nwe introduce a fast and accurate light field depth estimation method based on a\nfully-convolutional neural network. Our network is designed by considering the\nlight field geometry and we also overcome the lack of training data by\nproposing light field specific data augmentation methods. We achieved the top\nrank in the HCI 4D Light Field Benchmark on most metrics, and we also\ndemonstrate the effectiveness of the proposed method on real-world light-field\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 17:52:38 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Shin", "Changha", ""], ["Jeon", "Hae-Gon", ""], ["Yoon", "Youngjin", ""], ["Kweon", "In So", ""], ["Kim", "Seon Joo", ""]]}, {"id": "1804.02391", "submitter": "Saumya Jetley", "authors": "Saumya Jetley, Nicholas A. Lord, Namhoon Lee, Philip H.S. Torr", "title": "Learn To Pay Attention", "comments": "International Conference on Learning Representations 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end-trainable attention module for convolutional neural\nnetwork (CNN) architectures built for image classification. The module takes as\ninput the 2D feature vector maps which form the intermediate representations of\nthe input image at different stages in the CNN pipeline, and outputs a 2D\nmatrix of scores for each map. Standard CNN architectures are modified through\nthe incorporation of this module, and trained under the constraint that a\nconvex combination of the intermediate 2D feature vectors, as parameterised by\nthe score matrices, must \\textit{alone} be used for classification.\nIncentivised to amplify the relevant and suppress the irrelevant or misleading,\nthe scores thus assume the role of attention values. Our experimental\nobservations provide clear evidence to this effect: the learned attention maps\nneatly highlight the regions of interest while suppressing background clutter.\nConsequently, the proposed function is able to bootstrap standard CNN\narchitectures for the task of image classification, demonstrating superior\ngeneralisation over 6 unseen benchmark datasets. When binarised, our attention\nmaps outperform other CNN-based attention maps, traditional saliency maps, and\ntop object proposals for weakly supervised segmentation as demonstrated on the\nObject Discovery dataset. We also demonstrate improved robustness against the\nfast gradient sign method of adversarial attack.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 10:47:26 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 14:33:56 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Jetley", "Saumya", ""], ["Lord", "Nicholas A.", ""], ["Lee", "Namhoon", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1804.02419", "submitter": "Shervin Minaee", "authors": "Shervin Minaee", "title": "Image Segmentation Using Subspace Representation and Sparse\n  Decomposition", "comments": "PhD Dissertation, NYU, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image foreground extraction is a classical problem in image processing and\nvision, with a large range of applications. In this dissertation, we focus on\nthe extraction of text and graphics in mixed-content images, and design novel\napproaches for various aspects of this problem.\n  We first propose a sparse decomposition framework, which models the\nbackground by a subspace containing smooth basis vectors, and foreground as a\nsparse and connected component. We then formulate an optimization framework to\nsolve this problem, by adding suitable regularizations to the cost function to\npromote the desired characteristics of each component. We present two\ntechniques to solve the proposed optimization problem, one based on alternating\ndirection method of multipliers (ADMM), and the other one based on robust\nregression. Promising results are obtained for screen content image\nsegmentation using the proposed algorithm.\n  We then propose a robust subspace learning algorithm for the representation\nof the background component using training images that could contain both\nbackground and foreground components, as well as noise. With the learnt\nsubspace for the background, we can further improve the segmentation results,\ncompared to using a fixed subspace. Lastly, we investigate a different class of\nsignal/image decomposition problem, where only one signal component is active\nat each signal element. In this case, besides estimating each component, we\nneed to find their supports, which can be specified by a binary mask. We\npropose a mixed-integer programming problem, that jointly estimates the two\ncomponents and their supports through an alternating optimization scheme. We\nshow the application of this algorithm on various problems, including image\nsegmentation, video motion segmentation, and also separation of text from\ntextured images.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 18:36:53 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Minaee", "Shervin", ""]]}, {"id": "1804.02445", "submitter": "Noah Siegel", "authors": "Noah Siegel, Nicholas Lourie, Russell Power, Waleed Ammar", "title": "Extracting Scientific Figures with Distantly Supervised Neural Networks", "comments": "10 pages, 5 figures, paper accepted at JCDL 2018", "journal-ref": null, "doi": "10.1145/3197026.3197040", "report-no": null, "categories": "cs.DL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-textual components such as charts, diagrams and tables provide key\ninformation in many scientific documents, but the lack of large labeled\ndatasets has impeded the development of data-driven methods for scientific\nfigure extraction. In this paper, we induce high-quality training labels for\nthe task of figure extraction in a large number of scientific documents, with\nno human intervention. To accomplish this we leverage the auxiliary data\nprovided in two large web collections of scientific documents (arXiv and\nPubMed) to locate figures and their associated captions in the rasterized PDF.\nWe share the resulting dataset of over 5.5 million induced labels---4,000 times\nlarger than the previous largest figure extraction dataset---with an average\nprecision of 96.8%, to enable the development of modern data-driven methods for\nthis task. We use this dataset to train a deep neural network for end-to-end\nfigure detection, yielding a model that can be more easily extended to new\ndomains compared to previous work. The model was successfully deployed in\nSemantic Scholar, a large-scale academic search engine, and used to extract\nfigures in 13 million scientific documents.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 20:22:47 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 19:13:53 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Siegel", "Noah", ""], ["Lourie", "Nicholas", ""], ["Power", "Russell", ""], ["Ammar", "Waleed", ""]]}, {"id": "1804.02463", "submitter": "Lucas Beyer", "authors": "Lucas Beyer, Alexander Hermans, Timm Linder, Kai O. Arras, Bastian\n  Leibe", "title": "Deep Person Detection in 2D Range Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting humans is a key skill for mobile robots and intelligent vehicles in\na large variety of applications. While the problem is well studied for certain\nsensory modalities such as image data, few works exist that address this\ndetection task using 2D range data. However, a widespread sensory setup for\nmany mobile robots in service and domestic applications contains a horizontally\nmounted 2D laser scanner. Detecting people from 2D range data is challenging\ndue to the speed and dynamics of human leg motion and the high levels of\nocclusion and self-occlusion particularly in crowds of people. While previous\napproaches mostly relied on handcrafted features, we recently developed the\ndeep learning based wheelchair and walker detector DROW. In this paper, we show\nthe generalization to people, including small modifications that significantly\nboost DROW's performance. Additionally, by providing a small, fully online\ntemporal window in our network, we further boost our score. We extend the DROW\ndataset with person annotations, making this the largest dataset of person\nannotations in 2D range data, recorded during several days in a real-world\nenvironment with high diversity. Extensive experiments with three current\nbaseline methods indicate it is a challenging dataset, on which our improved\nDROW detector beats the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 21:39:55 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Beyer", "Lucas", ""], ["Hermans", "Alexander", ""], ["Linder", "Timm", ""], ["Arras", "Kai O.", ""], ["Leibe", "Bastian", ""]]}, {"id": "1804.02470", "submitter": "Ashwin Dani", "authors": "Gang Yao, Ashwin Dani", "title": "Visual Tracking Using Sparse Coding and Earth Mover's Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient iterative Earth Mover's Distance (iEMD) algorithm for visual\ntracking is proposed in this paper. The Earth Mover's Distance (EMD) is used as\nthe similarity measure to search for the optimal template candidates in\nfeature-spatial space in a video sequence. The computation of the EMD is\nformulated as the transportation problem from linear programming. The\nefficiency of the EMD optimization problem limits its use for visual tracking.\nTo alleviate this problem, a transportation-simplex method is used for EMD\noptimization and a monotonically convergent iterative optimization algorithm is\ndeveloped. The local sparse representation is used as the appearance models for\nthe iEMD tracker. The maximum-alignment-pooling method is used for constructing\na sparse coding histogram which reduces the computational complexity of the EMD\noptimization. The template update algorithm based on the EMD is also presented.\nThe iEMD tracking algorithm assumes small inter-frame movement in order to\nguarantee convergence. When the camera is mounted on a moving robot, e.g., a\nflying quadcopter, the camera could experience a sudden and rapid motion\nleading to large inter-frame movements. To ensure that the tracking algorithm\nconverges, a gyro-aided extension of the iEMD tracker is presented, where\nsynchronized gyroscope information is utilized to compensate for the rotation\nof the camera. The iEMD algorithm's performance is evaluated using eight\npublicly available datasets. The performance of the iEMD algorithm is compared\nwith seven state-of-the-art tracking algorithms based on relative percentage\noverlap. The robustness of this algorithm for large inter-frame displacements\nis also illustrated.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 22:07:47 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Yao", "Gang", ""], ["Dani", "Ashwin", ""]]}, {"id": "1804.02505", "submitter": "Yao Yao None", "authors": "Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, Long Quan", "title": "MVSNet: Depth Inference for Unstructured Multi-view Stereo", "comments": "Accepted to European Conference on Computer Vision (ECCV 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end deep learning architecture for depth map inference\nfrom multi-view images. In the network, we first extract deep visual image\nfeatures, and then build the 3D cost volume upon the reference camera frustum\nvia the differentiable homography warping. Next, we apply 3D convolutions to\nregularize and regress the initial depth map, which is then refined with the\nreference image to generate the final output. Our framework flexibly adapts\narbitrary N-view inputs using a variance-based cost metric that maps multiple\nfeatures into one cost feature. The proposed MVSNet is demonstrated on the\nlarge-scale indoor DTU dataset. With simple post-processing, our method not\nonly significantly outperforms previous state-of-the-arts, but also is several\ntimes faster in runtime. We also evaluate MVSNet on the complex outdoor Tanks\nand Temples dataset, where our method ranks first before April 18, 2018 without\nany fine-tuning, showing the strong generalization ability of MVSNet.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 03:57:00 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 12:44:13 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Yao", "Yao", ""], ["Luo", "Zixin", ""], ["Li", "Shiwei", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "1804.02508", "submitter": "Christoph Adami", "authors": "Ali Tehrani-Saleh, Thomas LaBar and Christoph Adami (Michigan State\n  University)", "title": "Evolution leads to a diversity of motion-detection neuronal circuits", "comments": "8 pages, 8 figures, Artificial Life Conference (2018), to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.CV cs.NE nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central goal of evolutionary biology is to explain the origins and\ndistribution of diversity across life. Beyond species or genetic diversity, we\nalso observe diversity in the circuits (genetic or otherwise) underlying\ncomplex functional traits. However, while the theory behind the origins and\nmaintenance of genetic and species diversity has been studied for decades,\ntheory concerning the origin of diverse functional circuits is still in its\ninfancy. It is not known how many different circuit structures can implement\nany given function, which evolutionary factors lead to different circuits, and\nwhether the evolution of a particular circuit was due to adaptive or\nnon-adaptive processes. Here, we use digital experimental evolution to study\nthe diversity of neural circuits that encode motion detection in digital\n(artificial) brains. We find that evolution leads to an enormous diversity of\npotential neural architectures encoding motion detection circuits, even for\ncircuits encoding the exact same function. Evolved circuits vary in both\nredundancy and complexity (as previously found in genetic circuits) suggesting\nthat similar evolutionary principles underlie circuit formation using any\nsubstrate. We also show that a simple (designed) motion detection circuit that\nis optimally-adapted gains in complexity when evolved further, and that\nselection for mutational robustness led this gain in complexity.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 04:26:21 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 14:06:08 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Tehrani-Saleh", "Ali", "", "Michigan State\n  University"], ["LaBar", "Thomas", "", "Michigan State\n  University"], ["Adami", "Christoph", "", "Michigan State\n  University"]]}, {"id": "1804.02516", "submitter": "Antoine Miech", "authors": "Antoine Miech, Ivan Laptev, Josef Sivic", "title": "Learning a Text-Video Embedding from Incomplete and Heterogeneous Data", "comments": "The paper had a major update in January 2020 after a bug we found in\n  the codebase that affected many results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint understanding of video and language is an active research area with\nmany applications. Prior work in this domain typically relies on learning\ntext-video embeddings. One difficulty with this approach, however, is the lack\nof large-scale annotated video-caption datasets for training. To address this\nissue, we aim at learning text-video embeddings from heterogeneous data\nsources. To this end, we propose a Mixture-of-Embedding-Experts (MEE) model\nwith ability to handle missing input modalities during training. As a result,\nour framework can learn improved text-video embeddings simultaneously from\nimage and video datasets. We also show the generalization of MEE to other input\nmodalities such as face descriptors. We evaluate our method on the task of\nvideo retrieval and report results for the MPII Movie Description and MSR-VTT\ndatasets. The proposed MEE model demonstrates significant improvements and\noutperforms previously reported methods on both text-to-video and video-to-text\nretrieval tasks. Code is available at:\nhttps://github.com/antoine77340/Mixture-of-Embedding-Experts\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 06:59:45 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 13:18:58 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Miech", "Antoine", ""], ["Laptev", "Ivan", ""], ["Sivic", "Josef", ""]]}, {"id": "1804.02541", "submitter": "Anil Bas", "authors": "Anil Bas, William A. P. Smith", "title": "Statistical transformer networks: learning shape and appearance models\n  via self supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalise Spatial Transformer Networks (STN) by replacing the parametric\ntransformation of a fixed, regular sampling grid with a deformable, statistical\nshape model which is itself learnt. We call this a Statistical Transformer\nNetwork (StaTN). By training a network containing a StaTN end-to-end for a\nparticular task, the network learns the optimal nonrigid alignment of the input\ndata for the task. Moreover, the statistical shape model is learnt with no\ndirect supervision (such as landmarks) and can be reused for other tasks.\nBesides training for a specific task, we also show that a StaTN can learn a\nshape model using generic loss functions. This includes a loss inspired by the\nminimum description length principle in which an appearance model is also\nlearnt from scratch. In this configuration, our model learns an active\nappearance model and a means to fit the model from scratch with no supervision\nat all, even identity labels.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 10:18:15 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Bas", "Anil", ""], ["Smith", "William A. P.", ""]]}, {"id": "1804.02543", "submitter": "Egor Illarionov", "authors": "Egor Illarionov and Roman Khudorozhkov", "title": "Not quite unreasonable effectiveness of machine learning algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art machine learning algorithms demonstrate close to absolute\nperformance in selected challenges. We provide arguments that the reason can be\nin low variability of the samples and high effectiveness in learning typical\npatterns. Due to this fact, standard performance metrics do not reveal model\ncapacity and new metrics are required for the better understanding of\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 10:24:04 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Illarionov", "Egor", ""], ["Khudorozhkov", "Roman", ""]]}, {"id": "1804.02554", "submitter": "Hossein Ziaei Nafchi", "authors": "Hossein Ziaei Nafchi and Mohamed Cheriet", "title": "Efficient No-Reference Quality Assessment and Classification Model for\n  Contrast Distorted Images", "comments": "6 pages, 4 figures, 4 tables", "journal-ref": null, "doi": "10.1109/TBC.2018.2818402", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an efficient Minkowski Distance based Metric (MDM) for\nno-reference (NR) quality assessment of contrast distorted images is proposed.\nIt is shown that higher orders of Minkowski distance and entropy provide\naccurate quality prediction for the contrast distorted images. The proposed\nmetric performs predictions by extracting only three features from the\ndistorted images followed by a regression analysis. Furthermore, the proposed\nfeatures are able to classify type of the contrast distorted images with a high\naccuracy. Experimental results on four datasets CSIQ, TID2013, CCID2014, and\nSIQAD show that the proposed metric with a very low complexity provides better\nquality predictions than the state-of-the-art NR metrics. The MATLAB source\ncode of the proposed metric is available to public at\nhttp://www.synchromedia.ca/system/files/MDM.zip.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 12:52:33 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Nafchi", "Hossein Ziaei", ""], ["Cheriet", "Mohamed", ""]]}, {"id": "1804.02555", "submitter": "Hirokatsu Kataoka", "authors": "Hirokatsu Kataoka, Teppei Suzuki, Shoko Oikawa, Yasuhiro Matsui,\n  Yutaka Satoh", "title": "Drive Video Analysis for the Detection of Traffic Near-Miss Incidents", "comments": "Accepted to ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of their recent introduction, self-driving cars and advanced driver\nassistance system (ADAS) equipped vehicles have had little opportunity to\nlearn, the dangerous traffic (including near-miss incident) scenarios that\nprovide normal drivers with strong motivation to drive safely. Accordingly, as\na means of providing learning depth, this paper presents a novel traffic\ndatabase that contains information on a large number of traffic near-miss\nincidents that were obtained by mounting driving recorders in more than 100\ntaxis over the course of a decade. The study makes the following two main\ncontributions: (i) In order to assist automated systems in detecting near-miss\nincidents based on database instances, we created a large-scale traffic\nnear-miss incident database (NIDB) that consists of video clip of dangerous\nevents captured by monocular driving recorders. (ii) To illustrate the\napplicability of NIDB traffic near-miss incidents, we provide two primary\ndatabase-related improvements: parameter fine-tuning using various near-miss\nscenes from NIDB, and foreground/background separation into motion\nrepresentation. Then, using our new database in conjunction with a monocular\ndriving recorder, we developed a near-miss recognition method that provides\nautomated systems with a performance level that is comparable to a human-level\nunderstanding of near-miss incidents (64.5% vs. 68.4% at near-miss recognition,\n61.3% vs. 78.7% at near-miss detection).\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 12:56:40 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Kataoka", "Hirokatsu", ""], ["Suzuki", "Teppei", ""], ["Oikawa", "Shoko", ""], ["Matsui", "Yasuhiro", ""], ["Satoh", "Yutaka", ""]]}, {"id": "1804.02574", "submitter": "Omid Haji Maghsoudi", "authors": "Omid Haji Maghsoudi, Annie Vahedipour, Benjamin Robertson, Andrew\n  Spence", "title": "Application of Superpixels to Segment Several Landmarks in Running\n  Rodents", "comments": "This paper has been accepted by the Journal of Pattern Recognition\n  and Image Analysis (PRIA), being published by July 2018", "journal-ref": null, "doi": "10.1134/S1054661818030082", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Examining locomotion has improved our basic understanding of motor control\nand aided in treating motor impairment. Mice and rats are the model system of\nchoice for basic neuroscience studies of human disease. High frame rates are\nneeded to quantify the kinematics of running rodents, due to their high stride\nfrequency. Manual tracking, especially for multiple body landmarks, becomes\nextremely time-consuming. To overcome these limitations, we proposed the use of\nsuperpixels based image segmentation as superpixels utilized both spatial and\ncolor information for segmentation. We segmented some parts of body and tested\nthe success of segmentation as a function of color space and SLIC segment size.\nWe used a simple merging function to connect the segmented regions considered\nas neighbor and having the same intensity value range. In addition, 28 features\nwere extracted, and t-SNE was used to demonstrate how much the methods are\ncapable to differentiate the regions. Finally, we compared the segmented\nregions to a manually outlined region. The results showed for segmentation,\nusing the RGB image was slightly better compared to the hue channel. For merg-\ning and classification, however, the hue representation was better as it\ncaptures the relevant color information in a single channel.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 16:46:15 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Maghsoudi", "Omid Haji", ""], ["Vahedipour", "Annie", ""], ["Robertson", "Benjamin", ""], ["Spence", "Andrew", ""]]}, {"id": "1804.02576", "submitter": "Marcel Sheeny de Moraes", "authors": "Marcel Sheeny, Andrew Wallace, Mehryar Emambakhsh, Sen Wang, Barry\n  Connor", "title": "POL-LWIR Vehicle Detection: Convolutional Neural Networks Meet Polarised\n  Infrared Sensors", "comments": "Computer Vision and Pattern Recognition Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For vehicle autonomy, driver assistance and situational awareness, it is\nnecessary to operate at day and night, and in all weather conditions. In\nparticular, long wave infrared (LWIR) sensors that receive predominantly\nemitted radiation have the capability to operate at night as well as during the\nday. In this work, we employ a polarised LWIR (POL-LWIR) camera to acquire data\nfrom a mobile vehicle, to compare and contrast four different convolutional\nneural network (CNN) configurations to detect other vehicles in video\nsequences. We evaluate two distinct and promising approaches, two-stage\ndetection (Faster-RCNN) and one-stage detection (SSD), in four different\nconfigurations. We also employ two different image decompositions: the first\nbased on the polarisation ellipse and the second on the Stokes parameters\nthemselves. To evaluate our approach, the experimental trials were quantified\nby mean average precision (mAP) and processing time, showing a clear trade-off\nbetween the two factors. For example, the best mAP result of 80.94% was\nachieved using Faster-RCNN, but at a frame rate of 6.4 fps. In contrast,\nMobileNet SSD achieved only 64.51% mAP, but at 53.4 fps.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 17:09:34 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Sheeny", "Marcel", ""], ["Wallace", "Andrew", ""], ["Emambakhsh", "Mehryar", ""], ["Wang", "Sen", ""], ["Connor", "Barry", ""]]}, {"id": "1804.02586", "submitter": "Yuyin Zhou", "authors": "Yuyin Zhou, Yan Wang, Peng Tang, Song Bai, Wei Shen, Elliot K.\n  Fishman, Alan L. Yuille", "title": "Semi-Supervised Multi-Organ Segmentation via Deep Multi-Planar\n  Co-Training", "comments": "accepted by WACV2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-organ segmentation of abdominal CT scans, most existing fully\nsupervised deep learning algorithms require lots of voxel-wise annotations,\nwhich are usually difficult, expensive, and slow to obtain. In comparison,\nmassive unlabeled 3D CT volumes are usually easily accessible. Current\nmainstream works to address the semi-supervised biomedical image segmentation\nproblem are mostly graph-based. By contrast, deep network based semi-supervised\nlearning methods have not drawn much attention in this field. In this work, we\npropose Deep Multi-Planar Co-Training (DMPCT), whose contributions can be\ndivided into two folds: 1) The deep model is learned in a co-training style\nwhich can mine consensus information from multiple planes like the sagittal,\ncoronal, and axial planes; 2) Multi-planar fusion is applied to generate more\nreliable pseudo-labels, which alleviates the errors occurring in the\npseudo-labels and thus can help to train better segmentation networks.\nExperiments are done on our newly collected large dataset with 100 unlabeled\ncases as well as 210 labeled cases where 16 anatomical structures are manually\nannotated by four radiologists and confirmed by a senior expert. The results\nsuggest that DMPCT significantly outperforms the fully supervised method by\nmore than 4% especially when only a small set of annotations is used.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 20:09:58 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 01:23:26 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 23:24:36 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Zhou", "Yuyin", ""], ["Wang", "Yan", ""], ["Tang", "Peng", ""], ["Bai", "Song", ""], ["Shen", "Wei", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1804.02591", "submitter": "Yunpeng Shi", "authors": "Yunpeng Shi and Gilad Lerman", "title": "Estimation of Camera Locations in Highly Corrupted Scenarios: All About\n  that Base, No Shape Trouble", "comments": "To appear in the CVPR 2018 proceedings", "journal-ref": "CVPR, 2018, pp. 2868-2876", "doi": "10.1109/CVPR.2018.00303", "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a strategy for improving camera location estimation in structure\nfrom motion. Our setting assumes highly corrupted pairwise directions (i.e.,\nnormalized relative location vectors), so there is a clear room for improving\ncurrent state-of-the-art solutions for this problem. Our strategy identifies\nseverely corrupted pairwise directions by using a geometric consistency\ncondition. It then selects a cleaner set of pairwise directions as a\npreprocessing step for common solvers. We theoretically guarantee the\nsuccessful performance of a basic version of our strategy under a synthetic\ncorruption model. Numerical results on artificial and real data demonstrate the\nsignificant improvement obtained by our strategy.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 20:48:58 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Shi", "Yunpeng", ""], ["Lerman", "Gilad", ""]]}, {"id": "1804.02595", "submitter": "Wei Shen", "authors": "Yan Wang, Yuyin Zhou, Peng Tang, Wei Shen, Elliot K. Fishman, Alan L.\n  Yuille", "title": "Training Multi-organ Segmentation Networks with Sample Selection by\n  Relaxed Upper Confident Bound", "comments": "Submitted to MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs), especially fully convolutional\nnetworks, have been widely applied to automatic medical image segmentation\nproblems, e.g., multi-organ segmentation. Existing CNN-based segmentation\nmethods mainly focus on looking for increasingly powerful network\narchitectures, but pay less attention to data sampling strategies for training\nnetworks more effectively. In this paper, we present a simple but effective\nsample selection method for training multi-organ segmentation networks. Sample\nselection exhibits an exploitation-exploration strategy, i.e., exploiting hard\nsamples and exploring less frequently visited samples. Based on the fact that\nvery hard samples might have annotation errors, we propose a new sample\nselection policy, named Relaxed Upper Confident Bound (RUCB). Compared with\nother sample selection policies, e.g., Upper Confident Bound (UCB), it exploits\na range of hard samples rather than being stuck with a small set of very hard\nones, which mitigates the influence of annotation errors during training. We\napply this new sample selection policy to training a multi-organ segmentation\nnetwork on a dataset containing 120 abdominal CT scans and show that it boosts\nsegmentation performance significantly.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 21:52:10 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Wang", "Yan", ""], ["Zhou", "Yuyin", ""], ["Tang", "Peng", ""], ["Shen", "Wei", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1804.02603", "submitter": "Jun Xu", "authors": "Jun Xu, Hui Li, Zhetong Liang, David Zhang, Lei Zhang", "title": "Real-world Noisy Image Denoising: A New Benchmark", "comments": "13 pages, 8 figures, 8 tables. arXiv admin note: text overlap with\n  arXiv:1707.01313 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of previous image denoising methods focus on additive white Gaussian\nnoise (AWGN). However,the real-world noisy image denoising problem with the\nadvancing of the computer vision techiniques. In order to promote the study on\nthis problem while implementing the concurrent real-world image denoising\ndatasets, we construct a new benchmark dataset which contains comprehensive\nreal-world noisy images of different natural scenes. These images are captured\nby different cameras under different camera settings. We evaluate the different\ndenoising methods on our new dataset as well as previous datasets. Extensive\nexperimental results demonstrate that the recently proposed methods designed\nspecifically for realistic noise removal based on sparse or low rank theories\nachieve better denoising performance and are more robust than other competing\nmethods, and the newly proposed dataset is more challenging. The constructed\ndataset of real photographs is publicly available at\n\\url{https://github.com/csjunxu/PolyUDataset} for researchers to investigate\nnew real-world image denoising methods. We will add more analysis on the noise\nstatistics in the real photographs of our new dataset in the next version of\nthis article.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 23:54:21 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Xu", "Jun", ""], ["Li", "Hui", ""], ["Liang", "Zhetong", ""], ["Zhang", "David", ""], ["Zhang", "Lei", ""]]}, {"id": "1804.02624", "submitter": "Daniel Lin", "authors": "Wen-Yan Lin, Siying Liu, Jian-Huang Lai, Yasuyuki Matsushita", "title": "Dimensionality's Blessing: Clustering Images by Underlying Distribution", "comments": "Accepted in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many high dimensional vector distances tend to a constant. This is typically\nconsidered a negative \"contrast-loss\" phenomenon that hinders clustering and\nother machine learning techniques. We reinterpret \"contrast-loss\" as a\nblessing. Re-deriving \"contrast-loss\" using the law of large numbers, we show\nit results in a distribution's instances concentrating on a thin \"hyper-shell\".\nThe hollow center means apparently chaotically overlapping distributions are\nactually intrinsically separable. We use this to develop\ndistribution-clustering, an elegant algorithm for grouping of data points by\ntheir (unknown) underlying distribution. Distribution-clustering, creates\nnotably clean clusters from raw unlabeled data, estimates the number of\nclusters for itself and is inherently robust to \"outliers\" which form their own\nclusters. This enables trawling for patterns in unorganized data and may be the\nkey to enabling machine intelligence.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 03:52:09 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Lin", "Wen-Yan", ""], ["Liu", "Siying", ""], ["Lai", "Jian-Huang", ""], ["Matsushita", "Yasuyuki", ""]]}, {"id": "1804.02638", "submitter": "Simon Korman", "authors": "Simon Korman, Mark Milam, Stefano Soatto", "title": "OATM: Occlusion Aware Template Matching by Consensus Set Maximization", "comments": "to appear at cvpr 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to template matching that is efficient, can\nhandle partial occlusions, and comes with provable performance guarantees. A\nkey component of the method is a reduction that transforms the problem of\nsearching a nearest neighbor among $N$ high-dimensional vectors, to searching\nneighbors among two sets of order $\\sqrt{N}$ vectors, which can be found\nefficiently using range search techniques. This allows for a quadratic\nimprovement in search complexity, and makes the method scalable in handling\nlarge search spaces. The second contribution is a hashing scheme based on\nconsensus set maximization, which allows us to handle occlusions. The resulting\nscheme can be seen as a randomized hypothesize-and-test algorithm, which is\nequipped with guarantees regarding the number of iterations required for\nobtaining an optimal solution with high probability. The predicted matching\nrates are validated empirically and the algorithm shows a significant\nimprovement over the state-of-the-art in both speed and robustness to\nocclusions.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 07:19:55 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Korman", "Simon", ""], ["Milam", "Mark", ""], ["Soatto", "Stefano", ""]]}, {"id": "1804.02675", "submitter": "Tomoyuki Suzuki", "authors": "Tomoyuki Suzuki, Hirokatsu Kataoka, Yoshimitsu Aoki and Yutaka Satoh", "title": "Anticipating Traffic Accidents with Adaptive Loss and Large-scale\n  Incident DB", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for traffic accident anticipation\nthrough (i) Adaptive Loss for Early Anticipation (AdaLEA) and (ii) a\nlarge-scale self-annotated incident database for anticipation. The proposed\nAdaLEA allows a model to gradually learn an earlier anticipation as training\nprogresses. The loss function adaptively assigns penalty weights depending on\nhow early the model can an- ticipate a traffic accident at each epoch.\nAdditionally, we construct a Near-miss Incident DataBase for anticipation. This\ndatabase contains an enormous number of traffic near- miss incident videos and\nannotations for detail evaluation of two tasks, risk anticipation and\nrisk-factor anticipation. In our experimental results, we found our proposal\nachieved the highest scores for risk anticipation (+6.6% better on mean average\nprecision (mAP) and 2.36 sec earlier than previous work on the average\ntime-to-collision (ATTC)) and risk-factor anticipation (+4.3% better on mAP and\n0.70 sec earlier than previous work on ATTC).\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 11:49:30 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Suzuki", "Tomoyuki", ""], ["Kataoka", "Hirokatsu", ""], ["Aoki", "Yoshimitsu", ""], ["Satoh", "Yutaka", ""]]}, {"id": "1804.02678", "submitter": "Lama Affara", "authors": "Lama Affara, Bernard Ghanem, Peter Wonka", "title": "Supervised Convolutional Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Sparse Coding (CSC) is a well-established image representation\nmodel especially suited for image restoration tasks. In this work, we extend\nthe applicability of this model by proposing a supervised approach to\nconvolutional sparse coding, which aims at learning discriminative dictionaries\ninstead of purely reconstructive ones. We incorporate a supervised\nregularization term into the traditional unsupervised CSC objective to\nencourage the final dictionary elements to be discriminative. Experimental\nresults show that using supervised convolutional learning results in two key\nadvantages. First, we learn more semantically relevant filters in the\ndictionary and second, we achieve improved image reconstruction on unseen data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 12:05:12 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Affara", "Lama", ""], ["Ghanem", "Bernard", ""], ["Wonka", "Peter", ""]]}, {"id": "1804.02684", "submitter": "Tae-Hyun Oh", "authors": "Tae-Hyun Oh, Ronnachai Jaroensri, Changil Kim, Mohamed Elgharib,\n  Fr\\'edo Durand, William T. Freeman, Wojciech Matusik", "title": "Learning-based Video Motion Magnification", "comments": "Accepted as ECCV 2018 Oral. The 1st and 2nd authors equally\n  contributed. Video result: https://youtu.be/GrMLeEcSNzY , Project page:\n  http://people.csail.mit.edu/tiam/deepmag/ Some bibliography information was\n  fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video motion magnification techniques allow us to see small motions\npreviously invisible to the naked eyes, such as those of vibrating airplane\nwings, or swaying buildings under the influence of the wind. Because the motion\nis small, the magnification results are prone to noise or excessive blurring.\nThe state of the art relies on hand-designed filters to extract representations\nthat may not be optimal. In this paper, we seek to learn the filters directly\nfrom examples using deep convolutional neural networks. To make training\ntractable, we carefully design a synthetic dataset that captures small motion\nwell, and use two-frame input for training. We show that the learned filters\nachieve high-quality results on real videos, with less ringing artifacts and\nbetter noise characteristics than previous methods. While our model is not\ntrained with temporal filters, we found that the temporal filters can be used\nwith our extracted representations up to a moderate magnification, enabling a\nfrequency-based motion selection. Finally, we analyze the learned filters and\nshow that they behave similarly to the derivative filters used in previous\nworks. Our code, trained model, and datasets will be available online.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 12:57:23 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 17:45:57 GMT"}, {"version": "v3", "created": "Wed, 1 Aug 2018 03:26:46 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Oh", "Tae-Hyun", ""], ["Jaroensri", "Ronnachai", ""], ["Kim", "Changil", ""], ["Elgharib", "Mohamed", ""], ["Durand", "Fr\u00e9do", ""], ["Freeman", "William T.", ""], ["Matusik", "Wojciech", ""]]}, {"id": "1804.02688", "submitter": "Xiaojie Guo", "authors": "Siyuan LI, Wenqi Ren, Jiawan Zhang, Jinke Yu, Xiaojie Guo", "title": "Fast Single Image Rain Removal via a Deep Decomposition-Composition\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain effect in images typically is annoying for many multimedia and computer\nvision tasks. For removing rain effect from a single image, deep leaning\ntechniques have been attracting considerable attentions. This paper designs a\nnovel multi-task leaning architecture in an end-to-end manner to reduce the\nmapping range from input to output and boost the performance. Concretely, a\ndecomposition net is built to split rain images into clean background and rain\nlayers. Different from previous architectures, our model consists of, besides a\ncomponent representing the desired clean image, an extra component for the rain\nlayer. During the training phase, we further employ a composition structure to\nreproduce the input by the separated clean image and rain information for\nimproving the quality of decomposition. Experimental results on both synthetic\nand real images are conducted to reveal the high-quality recovery by our\ndesign, and show its superiority over other state-of-the-art methods.\nFurthermore, our design is also applicable to other layer decomposition tasks\nlike dust removal. More importantly, our method only requires about 50ms,\nsignificantly faster than the competitors, to process a testing image in VGA\nresolution on a GTX 1080 GPU, making it attractive for practical use.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 13:14:09 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["LI", "Siyuan", ""], ["Ren", "Wenqi", ""], ["Zhang", "Jiawan", ""], ["Yu", "Jinke", ""], ["Guo", "Xiaojie", ""]]}, {"id": "1804.02690", "submitter": "Linjie Deng", "authors": "Linjie Deng, Yanxiang Gong, Yi Lin, Jingwen Shuai, Xiaoguang Tu,\n  Yuefei Zhang, Zheng Ma, Mei Xie", "title": "Detecting Multi-Oriented Text with Corner-based Region Proposals", "comments": null, "journal-ref": "Neurocomputing, Volume 334, 21 March 2019, Pages 134-142", "doi": "10.1016/j.neucom.2019.01.013", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous approaches for scene text detection usually rely on manually defined\nsliding windows. This work presents an intuitive two-stage region-based method\nto detect multi-oriented text without any prior knowledge regarding the textual\nshape. In the first stage, we estimate the possible locations of text instances\nby detecting and linking corners instead of shifting a set of default anchors.\nThe quadrilateral proposals are geometry adaptive, which allows our method to\ncope with various text aspect ratios and orientations. In the second stage, we\ndesign a new pooling layer named Dual-RoI Pooling which embeds data\naugmentation inside the region-wise subnetwork for more robust classification\nand regression over these proposals. Experimental results on public benchmarks\nconfirm that the proposed method is capable of achieving comparable performance\nwith state-of-the-art methods. The code is publicly available at\nhttps://github.com/xhzdeng/crpn\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 13:36:03 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 00:50:50 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Deng", "Linjie", ""], ["Gong", "Yanxiang", ""], ["Lin", "Yi", ""], ["Shuai", "Jingwen", ""], ["Tu", "Xiaoguang", ""], ["Zhang", "Yuefei", ""], ["Ma", "Zheng", ""], ["Xie", "Mei", ""]]}, {"id": "1804.02702", "submitter": "Ashwani Kumar", "authors": "Ashwani Kumar", "title": "Ordinal Pooling Networks: For Preserving Information over Shrinking\n  Feature Maps", "comments": "9 pages with 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of convolutional neural networks that lie at the heart of\ndeep learning, downsampling is often performed with a max-pooling operation\nthat only retains the element with maximum activation, while completely\ndiscarding the information contained in other elements in a pooling region. To\naddress this issue, a novel pooling scheme, Ordinal Pooling Network (OPN), is\nintroduced in this work. OPN rearranges all the elements of a pooling region in\na sequence and assigns different weights to these elements based upon their\norders in the sequence, where the weights are learned via the gradient-based\noptimisation. The results of our small-scale experiments on image\nclassification task demonstrate that this scheme leads to a consistent\nimprovement in the accuracy over max-pooling operation. This improvement is\nexpected to increase in deeper networks, where several layers of pooling become\nnecessary.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 15:00:46 GMT"}, {"version": "v2", "created": "Sun, 15 Apr 2018 18:02:14 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Kumar", "Ashwani", ""]]}, {"id": "1804.02721", "submitter": "Fariba Zohrizadeh", "authors": "Fariba Zohrizadeh, Mohsen Kheirandishfard, Farhad Kamangar", "title": "Image Segmentation using Sparse Subset Selection", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new image segmentation method based on the\nconcept of sparse subset selection. Starting with an over-segmentation, we\nadopt local spectral histogram features to encode the visual information of the\nsmall segments into high-dimensional vectors, called superpixel features. Then,\nthe superpixel features are fed into a novel convex model which efficiently\nleverages the features to group the superpixels into a proper number of\ncoherent regions. Our model automatically determines the optimal number of\ncoherent regions and superpixels assignment to shape final segments. To solve\nour model, we propose a numerical algorithm based on the alternating direction\nmethod of multipliers (ADMM), whose iterations consist of two highly\nparallelizable sub-problems. We show each sub-problem enjoys closed-form\nsolution which makes the ADMM iterations computationally very efficient.\nExtensive experiments on benchmark image segmentation datasets demonstrate that\nour proposed method in combination with an over-segmentation can provide high\nquality and competitive results compared to the existing state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 17:12:23 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Zohrizadeh", "Fariba", ""], ["Kheirandishfard", "Mohsen", ""], ["Kamangar", "Farhad", ""]]}, {"id": "1804.02740", "submitter": "Haiping Zhu", "authors": "Haiping Zhu, Qi Zhou, Junping Zhang and James Z. Wang", "title": "Facial Aging and Rejuvenation by Conditional Multi-Adversarial\n  Autoencoder with Ordinal Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial aging and facial rejuvenation analyze a given face photograph to\npredict a future look or estimate a past look of the person. To achieve this,\nit is critical to preserve human identity and the corresponding aging\nprogression and regression with high accuracy. However, existing methods cannot\nsimultaneously handle these two objectives well. We propose a novel generative\nadversarial network based approach, named the Conditional Multi-Adversarial\nAutoEncoder with Ordinal Regression (CMAAE-OR). It utilizes an age estimation\ntechnique to control the aging accuracy and takes a high-level feature\nrepresentation to preserve personalized identity. Specifically, the face is\nfirst mapped to a latent vector through a convolutional encoder. The latent\nvector is then projected onto the face manifold conditional on the age through\na deconvolutional generator. The latent vector preserves personalized face\nfeatures and the age controls facial aging and rejuvenation. A discriminator\nand an ordinal regression are imposed on the encoder and the generator in\ntandem, making the generated face images to be more photorealistic while\nsimultaneously exhibiting desirable aging effects. Besides, a high-level\nfeature representation is utilized to preserve personalized identity of the\ngenerated face. Experiments on two benchmark datasets demonstrate appealing\nperformance of the proposed method over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 18:55:38 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Zhu", "Haiping", ""], ["Zhou", "Qi", ""], ["Zhang", "Junping", ""], ["Wang", "James Z.", ""]]}, {"id": "1804.02745", "submitter": "Cagdas Ulas", "authors": "Cagdas Ulas, Giles Tetteh, Michael J. Thrippleton, Paul A. Armitage,\n  Stephen D. Makin, Joanna M. Wardlaw, Mike E. Davies, and Bjoern H. Menze", "title": "Direct Estimation of Pharmacokinetic Parameters from DCE-MRI using Deep\n  CNN with Forward Physical Model Loss", "comments": "Accepted at MICCAI 2018. 9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic contrast-enhanced (DCE) MRI is an evolving imaging technique that\nprovides a quantitative measure of pharmacokinetic (PK) parameters in body\ntissues, in which series of T1-weighted images are collected following the\nadministration of a paramagnetic contrast agent. Unfortunately, in many\napplications, conventional clinical DCE-MRI suffers from low spatiotemporal\nresolution and insufficient volume coverage. In this paper, we propose a novel\ndeep learning based approach to directly estimate the PK parameters from\nundersampled DCE-MRI data. Specifically, we design a custom loss function where\nwe incorporate a forward physical model that relates the PK parameters to\ncorrupted image-time series obtained due to subsampling in k-space. This allows\nthe network to directly exploit the knowledge of true contrast agent kinetics\nin the training phase, and hence provide more accurate restoration of PK\nparameters. Experiments on clinical brain DCE datasets demonstrate the efficacy\nof our approach in terms of fidelity of PK parameter reconstruction and\nsignificantly faster parameter inference compared to a model-based iterative\nreconstruction method.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 19:36:39 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 11:34:14 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Ulas", "Cagdas", ""], ["Tetteh", "Giles", ""], ["Thrippleton", "Michael J.", ""], ["Armitage", "Paul A.", ""], ["Makin", "Stephen D.", ""], ["Wardlaw", "Joanna M.", ""], ["Davies", "Mike E.", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "1804.02748", "submitter": "Dima Damen", "authors": "Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler,\n  Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby\n  Perrett, Will Price, Michael Wray", "title": "Scaling Egocentric Vision: The EPIC-KITCHENS Dataset", "comments": "European Conference on Computer Vision (ECCV) 2018 Dataset and\n  Project page: http://epic-kitchens.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-person vision is gaining interest as it offers a unique viewpoint on\npeople's interaction with objects, their attention, and even intention.\nHowever, progress in this challenging domain has been relatively slow due to\nthe lack of sufficiently large datasets. In this paper, we introduce\nEPIC-KITCHENS, a large-scale egocentric video benchmark recorded by 32\nparticipants in their native kitchen environments. Our videos depict\nnonscripted daily activities: we simply asked each participant to start\nrecording every time they entered their kitchen. Recording took place in 4\ncities (in North America and Europe) by participants belonging to 10 different\nnationalities, resulting in highly diverse cooking styles. Our dataset features\n55 hours of video consisting of 11.5M frames, which we densely labeled for a\ntotal of 39.6K action segments and 454.3K object bounding boxes. Our annotation\nis unique in that we had the participants narrate their own videos (after\nrecording), thus reflecting true intention, and we crowd-sourced ground-truths\nbased on these. We describe our object, action and anticipation challenges, and\nevaluate several baselines over two test splits, seen and unseen kitchens.\nDataset and Project page: http://epic-kitchens.github.io\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 20:07:13 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 09:05:07 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Damen", "Dima", ""], ["Doughty", "Hazel", ""], ["Farinella", "Giovanni Maria", ""], ["Fidler", "Sanja", ""], ["Furnari", "Antonino", ""], ["Kazakos", "Evangelos", ""], ["Moltisanti", "Davide", ""], ["Munro", "Jonathan", ""], ["Perrett", "Toby", ""], ["Price", "Will", ""], ["Wray", "Michael", ""]]}, {"id": "1804.02755", "submitter": "Cagdas Ulas", "authors": "Cagdas Ulas, Giles Tetteh, Stephan Kaczmarz, Christine Preibisch, and\n  Bjoern H. Menze", "title": "DeepASL: Kinetic Model Incorporated Loss for Denoising Arterial Spin\n  Labeled MRI via Deep Residual Learning", "comments": "Accepted at MICCAI 2018. 9 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arterial spin labeling (ASL) allows to quantify the cerebral blood flow (CBF)\nby magnetic labeling of the arterial blood water. ASL is increasingly used in\nclinical studies due to its noninvasiveness, repeatability and benefits in\nquantification. However, ASL suffers from an inherently low-signal-to-noise\nratio (SNR) requiring repeated measurements of control/spin-labeled (C/L) pairs\nto achieve a reasonable image quality, which in return increases motion\nsensitivity. This leads to clinically prolonged scanning times increasing the\nrisk of motion artifacts. Thus, there is an immense need of advanced imaging\nand processing techniques in ASL. In this paper, we propose a novel deep\nlearning based approach to improve the perfusion-weighted image quality\nobtained from a subset of all available pairwise C/L subtractions.\nSpecifically, we train a deep fully convolutional network (FCN) to learn a\nmapping from noisy perfusion-weighted image and its subtraction (residual) from\nthe clean image. Additionally, we incorporate the CBF estimation model in the\nloss function during training, which enables the network to produce high\nquality images while simultaneously enforcing the CBF estimates to be as close\nas reference CBF values. Extensive experiments on synthetic and clinical ASL\ndatasets demonstrate the effectiveness of our method in terms of improved ASL\nimage quality, accurate CBF parameter estimation and considerably small\ncomputation time during testing.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 20:27:44 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 11:34:02 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Ulas", "Cagdas", ""], ["Tetteh", "Giles", ""], ["Kaczmarz", "Stephan", ""], ["Preibisch", "Christine", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "1804.02767", "submitter": "Joseph Redmon", "authors": "Joseph Redmon, Ali Farhadi", "title": "YOLOv3: An Incremental Improvement", "comments": "Tech Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present some updates to YOLO! We made a bunch of little design changes to\nmake it better. We also trained this new network that's pretty swell. It's a\nlittle bigger than last time but more accurate. It's still fast though, don't\nworry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but\nthree times faster. When we look at the old .5 IOU mAP detection metric YOLOv3\nis quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5\nmAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always,\nall the code is online at https://pjreddie.com/yolo/\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 22:27:57 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Redmon", "Joseph", ""], ["Farhadi", "Ali", ""]]}, {"id": "1804.02771", "submitter": "Zhao Chen", "authors": "Zhao Chen, Vijay Badrinarayanan, Gilad Drozdov, Andrew Rabinovich", "title": "Estimating Depth from RGB and Sparse Sensing", "comments": "European Conference on Computer Vision (ECCV) 2018. Updated to\n  camera-ready version with additional experiments", "journal-ref": "In: European Conference on Computer Vision. pp. 176-192. Springer\n  (2018)", "doi": "10.1007/978-3-030-01225-0_11", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep model that can accurately produce dense depth maps given an\nRGB image with known depth at a very sparse set of pixels. The model works\nsimultaneously for both indoor/outdoor scenes and produces state-of-the-art\ndense depth maps at nearly real-time speeds on both the NYUv2 and KITTI\ndatasets. We surpass the state-of-the-art for monocular depth estimation even\nwith depth values for only 1 out of every ~10000 image pixels, and we\noutperform other sparse-to-dense depth methods at all sparsity levels. With\ndepth values for 1/256 of the image pixels, we achieve a mean absolute error of\nless than 1% of actual depth on indoor scenes, comparable to the performance of\nconsumer-grade depth sensor hardware. Our experiments demonstrate that it would\nindeed be possible to efficiently transform sparse depth measurements obtained\nusing e.g. lower-power depth sensors or SLAM systems into high-quality dense\ndepth maps.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 22:46:10 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 22:28:40 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Chen", "Zhao", ""], ["Badrinarayanan", "Vijay", ""], ["Drozdov", "Gilad", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1804.02792", "submitter": "Zeyu Chen", "authors": "Jiaxuan Zhuo, Zeyu Chen, Jianhuang Lai, Guangcong Wang", "title": "Occluded Person Re-identification", "comments": "6 pages, 7 figures, IEEE International Conference of Multimedia and\n  Expo 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id) suffers from a serious occlusion problem\nwhen applied to crowded public places. In this paper, we propose to retrieve a\nfull-body person image by using a person image with occlusions. This differs\nsignificantly from the conventional person re-id problem where it is assumed\nthat person images are detected without any occlusion. We thus call this new\nproblem the occluded person re-identitification. To address this new problem,\nwe propose a novel Attention Framework of Person Body (AFPB) based on deep\nlearning, consisting of 1) an Occlusion Simulator (OS) which automatically\ngenerates artificial occlusions for full-body person images, and 2) multi-task\nlosses that force the neural network not only to discriminate a person's\nidentity but also to determine whether a sample is from the occluded data\ndistribution or the full-body data distribution. Experiments on a new occluded\nperson re-id dataset and three existing benchmarks modified to include\nfull-body person images and occluded person images show the superiority of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 01:56:53 GMT"}, {"version": "v2", "created": "Sun, 15 Apr 2018 02:00:47 GMT"}, {"version": "v3", "created": "Fri, 20 Apr 2018 14:22:34 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Zhuo", "Jiaxuan", ""], ["Chen", "Zeyu", ""], ["Lai", "Jianhuang", ""], ["Wang", "Guangcong", ""]]}, {"id": "1804.02810", "submitter": "Mingxing Duan nudt", "authors": "Mingxing Duan, Kenli Li, Qi Tian", "title": "A Novel Multi-Task Tensor Correlation Neural Network for Facial\n  Attribute Prediction", "comments": "Submitted to ACM Multimedia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face multi-attribute prediction benefits substantially from multi-task\nlearning (MTL), which learns multiple face attributes simultaneously to achieve\nshared or mutually related representations of different attributes. The most\nwidely used MTL convolutional neural network is heuristically or empirically\ndesigned by sharing all of the convolutional layers and splitting at the fully\nconnected layers for task-specific losses. However, it is improper to view all\nlow and mid-level features for different attributes as being the same,\nespecially when these attributes are only loosely related. In this paper, we\npropose a novel multi-attribute tensor correlation neural network (MTCN) for\nface attribute prediction. The structure shares the information in low-level\nfeatures (e.g., the first two convolutional layers) but splits that in\nhigh-level features (e.g., from the third convolutional layer to the fully\nconnected layer). At the same time, during high-level feature extraction, each\nsubnetwork (e.g., Age-Net, Gender-Net, ..., and Smile-Net) excavates closely\nrelated features from other networks to enhance its features. Then, we project\nthe features of the C9 layers of the fine-tuned subnetworks into a highly\ncorrelated space by using a novel tensor correlation analysis algorithm\n(NTCCA). The final face attribute prediction is made based on the correlation\nmatrix. Experimental results on benchmarks with multiple face attributes\n(CelebA and LFWA) show that the proposed approach has superior performance\ncompared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 04:20:40 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Duan", "Mingxing", ""], ["Li", "Kenli", ""], ["Tian", "Qi", ""]]}, {"id": "1804.02815", "submitter": "Xintao Wang", "authors": "Xintao Wang, Ke Yu, Chao Dong, Chen Change Loy", "title": "Recovering Realistic Texture in Image Super-resolution by Deep Spatial\n  Feature Transform", "comments": "This work is accepted in CVPR 2018. Our project page is\n  http://mmlab.ie.cuhk.edu.hk/projects/SFTGAN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite that convolutional neural networks (CNN) have recently demonstrated\nhigh-quality reconstruction for single-image super-resolution (SR), recovering\nnatural and realistic texture remains a challenging problem. In this paper, we\nshow that it is possible to recover textures faithful to semantic classes. In\nparticular, we only need to modulate features of a few intermediate layers in a\nsingle network conditioned on semantic segmentation probability maps. This is\nmade possible through a novel Spatial Feature Transform (SFT) layer that\ngenerates affine transformation parameters for spatial-wise feature modulation.\nSFT layers can be trained end-to-end together with the SR network using the\nsame loss function. During testing, it accepts an input image of arbitrary size\nand generates a high-resolution image with just a single forward pass\nconditioned on the categorical priors. Our final results show that an SR\nnetwork equipped with SFT can generate more realistic and visually pleasing\ntextures in comparison to state-of-the-art SRGAN and EnhanceNet.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 04:57:06 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Wang", "Xintao", ""], ["Yu", "Ke", ""], ["Dong", "Chao", ""], ["Loy", "Chen Change", ""]]}, {"id": "1804.02816", "submitter": "Takumi Ichimura", "authors": "Shin Kamada, Takumi Ichimura", "title": "A Generation Method of Immunological Memory in Clonal Selection\n  Algorithm by using Restricted Boltzmann Machines", "comments": "6 pages, 10 figures, Proc. of 2015 IEEE International Conference on\n  Systems, Man, and Cybernetics(IEEE SMC 2015)", "journal-ref": null, "doi": "10.1109/SMC.2015.465", "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a high technique of image processing is required to extract the\nimage features in real time. In our research, the tourist subject data are\ncollected from the Mobile Phone based Participatory Sensing (MPPS) system. Each\nrecord consists of image files with GPS, geographic location name, user's\nnumerical evaluation, and comments written in natural language at sightseeing\nspots where a user really visits. In our previous research, the famous\nlandmarks in sightseeing spot can be detected by Clonal Selection Algorithm\nwith Immunological Memory Cell (CSAIM). However, some landmarks was not\ndetected correctly by the previous method because they didn't have enough\namount of information for the feature extraction. In order to improve the\nweakness, we propose the generation method of immunological memory by\nRestricted Boltzmann Machines. To verify the effectiveness of the method, some\nexperiments for classification of the subjective data are executed by using\nmachine learning tools for Deep Learning.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 05:14:26 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Kamada", "Shin", ""], ["Ichimura", "Takumi", ""]]}, {"id": "1804.02827", "submitter": "Yaodong He", "authors": "Yaodong He, Jianfeng Zhou and Shiu Yin Yuen", "title": "Composing photomosaic images using clustering based evolutionary\n  programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photomosaic images are a type of images consisting of various tiny images. A\ncomplete form can be seen clearly by viewing it from a long distance. Small\ntiny images which replace blocks of the original image can be seen clearly by\nviewing it from a short distance. In the past, many algorithms have been\nproposed trying to automatically compose photomosaic images. Most of these\nalgorithms are designed with greedy algorithms to match the blocks with the\ntiny images. To obtain a better visual sense and satisfy some commercial\nrequirements, a constraint that a tiny image should not be repeatedly used many\ntimes is usually added. With the constraint, the photomosaic problem becomes a\ncombinatorial optimization problem. Evolutionary algorithms imitating the\nprocess of natural selection are popular and powerful in combinatorial\noptimization problems. However, little work has been done on applying\nevolutionary algorithms to photomosaic problem. In this paper, we present an\nalgorithm called clustering based evolutionary programming to deal with the\nproblem. We give prior knowledge to the optimization algorithm which makes the\noptimization process converges faster. In our experiment, the proposed\nalgorithm is compared with the state of the art algorithms and software. The\nresults indicate that our algorithm performs the best.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 05:57:24 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["He", "Yaodong", ""], ["Zhou", "Jianfeng", ""], ["Yuen", "Shiu Yin", ""]]}, {"id": "1804.02836", "submitter": "Yuki Fujimura", "authors": "Yuki Fujimura and Masaaki Iiyama and Atsushi Hashimoto and Michihiko\n  Minoh", "title": "Photometric Stereo in Participating Media Considering Shape-Dependent\n  Forward Scatter", "comments": "9 pages, accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images captured in participating media such as murky water, fog, or smoke are\ndegraded by scattered light. Thus, the use of traditional three-dimensional\n(3D) reconstruction techniques in such environments is difficult. In this\npaper, we propose a photometric stereo method for participating media. The\nproposed method differs from previous studies with respect to modeling\nshape-dependent forward scatter. In the proposed model, forward scatter is\ndescribed as an analytical form using lookup tables and is represented by\nspatially-variant kernels. We also propose an approximation of a large-scale\ndense matrix as a sparse matrix, which enables the removal of forward scatter.\nExperiments with real and synthesized data demonstrate that the proposed method\nimproves 3D reconstruction in participating media.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 06:25:20 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 00:39:12 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Fujimura", "Yuki", ""], ["Iiyama", "Masaaki", ""], ["Hashimoto", "Atsushi", ""], ["Minoh", "Michihiko", ""]]}, {"id": "1804.02843", "submitter": "Atsushi Kanehira Mr.", "authors": "Atsushi Kanehira, Luc Van Gool, Yoshitaka Ushiku, and Tatsuya Harada", "title": "Viewpoint-aware Video Summarization", "comments": "to appear at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel variant of video summarization, namely building\na summary that depends on the particular aspect of a video the viewer focuses\non. We refer to this as $\\textit{viewpoint}$. To infer what the desired\n$\\textit{viewpoint}$ may be, we assume that several other videos are available,\nespecially groups of videos, e.g., as folders on a person's phone or laptop.\nThe semantic similarity between videos in a group vs. the dissimilarity between\ngroups is used to produce $\\textit{viewpoint}$-specific summaries. For\nconsidering similarity as well as avoiding redundancy, output summary should be\n(A) diverse, (B) representative of videos in the same group, and (C)\ndiscriminative against videos in the different groups. To satisfy these\nrequirements (A)-(C) simultaneously, we proposed a novel video summarization\nmethod from multiple groups of videos. Inspired by Fisher's discriminant\ncriteria, it selects summary by optimizing the combination of three terms (a)\ninner-summary, (b) inner-group, and (c) between-group variances defined on the\nfeature representation of summary, which can simply represent (A)-(C).\nMoreover, we developed a novel dataset to investigate how well the generated\nsummary reflects the underlying $\\textit{viewpoint}$. Quantitative and\nqualitative experiments conducted on the dataset demonstrate the effectiveness\nof proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 06:49:48 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 13:38:10 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Kanehira", "Atsushi", ""], ["Van Gool", "Luc", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1804.02864", "submitter": "Yun Liu", "authors": "Yun Liu, Ming-Ming Cheng, Deng-Ping Fan, Le Zhang, JiaWang Bian,\n  Dacheng Tao", "title": "Semantic Edge Detection with Diverse Deep Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic edge detection (SED), which aims at jointly extracting edges as well\nas their category information, has far-reaching applications in domains such as\nsemantic segmentation, object proposal generation, and object recognition. SED\nnaturally requires achieving two distinct supervision targets: locating fine\ndetailed edges and identifying high-level semantics. We shed light on how such\ndistracted supervision targets prevent state-of-the-art SED methods from\neffectively using deep supervision to improve results. In this paper, we\npropose a novel fully convolutional neural network using diverse deep\nsupervision (DDS) within a multi-task framework where lower layers aim at\ngenerating category-agnostic edges, while higher layers are responsible for the\ndetection of category-aware semantic edges. To overcome the distracted\nsupervision challenge, a novel information converter unit is introduced, whose\neffectiveness has been extensively evaluated in several popular benchmark\ndatasets, including SBD, Cityscapes, and PASCAL VOC2012. Source code will be\nreleased upon paper acceptance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 08:28:08 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2018 08:29:22 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 07:32:56 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Liu", "Yun", ""], ["Cheng", "Ming-Ming", ""], ["Fan", "Deng-Ping", ""], ["Zhang", "Le", ""], ["Bian", "JiaWang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1804.02872", "submitter": "Katrin Lasinger", "authors": "Katrin Lasinger, Christoph Vogel, Thomas Pock, Konrad Schindler", "title": "Variational 3D-PIV with Sparse Descriptors", "comments": "to be published in Measurement Science and Technology", "journal-ref": null, "doi": "10.1088/1361-6501/aab5a0", "report-no": null, "categories": "cs.CV physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Particle Imaging Velocimetry (3D-PIV) aim to recover the flow field in a\nvolume of fluid, which has been seeded with tracer particles and observed from\nmultiple camera viewpoints. The first step of 3D-PIV is to reconstruct the 3D\nlocations of the tracer particles from synchronous views of the volume. We\npropose a new method for iterative particle reconstruction (IPR), in which the\nlocations and intensities of all particles are inferred in one joint energy\nminimization. The energy function is designed to penalize deviations between\nthe reconstructed 3D particles and the image evidence, while at the same time\naiming for a sparse set of particles. We find that the new method, without any\npost-processing, achieves significantly cleaner particle volumes than a\nconventional, tomographic MART reconstruction, and can handle a wide range of\nparticle densities. The second step of 3D-PIV is to then recover the dense\nmotion field from two consecutive particle reconstructions. We propose a\nvariational model, which makes it possible to directly include physical\nproperties, such as incompressibility and viscosity, in the estimation of the\nmotion field. To further exploit the sparse nature of the input data, we\npropose a novel, compact descriptor of the local particle layout. Hence, we\navoid the memory-intensive storage of high-resolution intensity volumes. Our\nframework is generic and allows for a variety of different data costs\n(correlation measures) and regularizers. We quantitatively evaluate it with\nboth the sum of squared differences (SSD) and the normalized cross-correlation\n(NCC), respectively with both a hard and a soft version of the\nincompressibility constraint.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 09:09:25 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Lasinger", "Katrin", ""], ["Vogel", "Christoph", ""], ["Pock", "Thomas", ""], ["Schindler", "Konrad", ""]]}, {"id": "1804.02900", "submitter": "Federico Perazzi", "authors": "Yifan Wang, Federico Perazzi, Brian McWilliams, Alexander\n  Sorkine-Hornung, Olga Sorkine-Hornung, Christopher Schroers", "title": "A Fully Progressive Approach to Single-Image Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning approaches to single image super-resolution have\nachieved impressive results in terms of traditional error measures and\nperceptual quality. However, in each case it remains challenging to achieve\nhigh quality results for large upsampling factors. To this end, we propose a\nmethod (ProSR) that is progressive both in architecture and training: the\nnetwork upsamples an image in intermediate steps, while the learning process is\norganized from easy to hard, as is done in curriculum learning. To obtain more\nphotorealistic results, we design a generative adversarial network (GAN), named\nProGanSR, that follows the same progressive multi-scale design principle. This\nnot only allows to scale well to high upsampling factors (e.g., 8x) but\nconstitutes a principled multi-scale approach that increases the reconstruction\nquality for all upsampling factors simultaneously. In particular ProSR ranks\n2nd in terms of SSIM and 4th in terms of PSNR in the NTIRE2018 SISR challenge\n[34]. Compared to the top-ranking team, our model is marginally lower, but runs\n5 times faster.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 10:28:03 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 14:22:14 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Wang", "Yifan", ""], ["Perazzi", "Federico", ""], ["McWilliams", "Brian", ""], ["Sorkine-Hornung", "Alexander", ""], ["Sorkine-Hornung", "Olga", ""], ["Schroers", "Christopher", ""]]}, {"id": "1804.02913", "submitter": "Kuldeep Purohit", "authors": "Kuldeep Purohit, Anshul Shah, A. N. Rajagopalan", "title": "Bringing Alive Blurred Moments", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a solution for the goal of extracting a video from a single motion\nblurred image to sequentially reconstruct the clear views of a scene as beheld\nby the camera during the time of exposure. We first learn motion representation\nfrom sharp videos in an unsupervised manner through training of a convolutional\nrecurrent video autoencoder network that performs a surrogate task of video\nreconstruction. Once trained, it is employed for guided training of a motion\nencoder for blurred images. This network extracts embedded motion information\nfrom the blurred image to generate a sharp video in conjunction with the\ntrained recurrent video decoder. As an intermediate step, we also design an\nefficient architecture that enables real-time single image deblurring and\noutperforms competing methods across all factors: accuracy, speed, and\ncompactness. Experiments on real scenes and standard datasets demonstrate the\nsuperiority of our framework over the state-of-the-art and its ability to\ngenerate a plausible sequence of temporally consistent sharp frames.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 11:14:32 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2019 21:58:00 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Purohit", "Kuldeep", ""], ["Shah", "Anshul", ""], ["Rajagopalan", "A. N.", ""]]}, {"id": "1804.02941", "submitter": "Rohit Gajawada", "authors": "Ameya Prabhu, Vishal Batchu, Sri Aurobindo Munagala, Rohit Gajawada\n  and Anoop Namboodiri", "title": "Distribution-Aware Binarization of Neural Networks for Sketch\n  Recognition", "comments": "Accepted at WACV '18 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks are highly effective at a range of computational tasks.\nHowever, they tend to be computationally expensive, especially in\nvision-related problems, and also have large memory requirements. One of the\nmost effective methods to achieve significant improvements in\ncomputational/spatial efficiency is to binarize the weights and activations in\na network. However, naive binarization results in accuracy drops when applied\nto networks for most tasks. In this work, we present a highly generalized,\ndistribution-aware approach to binarizing deep networks that allows us to\nretain the advantages of a binarized network, while reducing accuracy drops. We\nalso develop efficient implementations for our proposed approach across\ndifferent architectures. We present a theoretical analysis of the technique to\nshow the effective representational power of the resulting layers, and explore\nthe forms of data they model best. Experiments on popular datasets show that\nour technique offers better accuracies than naive binarization, while retaining\nthe same benefits that binarization provides - with respect to run-time\ncompression, reduction of computational costs, and power consumption.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 12:31:07 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Prabhu", "Ameya", ""], ["Batchu", "Vishal", ""], ["Munagala", "Sri Aurobindo", ""], ["Gajawada", "Rohit", ""], ["Namboodiri", "Anoop", ""]]}, {"id": "1804.02943", "submitter": "Jian-Qing Zheng", "authors": "Jian-Qing Zheng, Xiao-Yun Zhou, Qing-Biao Li, Celia Riga and\n  Guang-Zhong Yang", "title": "Abdominal Aortic Aneurysm Segmentation with a Small Number of Training\n  Subjects", "comments": "2 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-operative Abdominal Aortic Aneurysm (AAA) 3D shape is critical for\ncustomized stent-graft design in Fenestrated Endovascular Aortic Repair\n(FEVAR). Traditional segmentation approaches implement expert-designed feature\nextractors while recent deep neural networks extract features automatically\nwith multiple non-linear modules. Usually, a large training dataset is\nessential for applying deep learning on AAA segmentation. In this paper, the\nAAA was segmented using U-net with a small number (two) of training subjects.\nFirstly, Computed Tomography Angiography (CTA) slices were augmented with gray\nvalue variation and translation to avoid the overfitting caused by the small\nnumber of training subjects. Then, U-net was trained to segment the AAA. Dice\nSimilarity Coefficients (DSCs) over 0.8 were achieved on the testing subjects.\nThe PLZ, DLZ and aortic branches are all reconstructed reasonably, which will\nfacilitate stent graft customization and help shape instantiation for\nintra-operative surgery navigation in FEVAR.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 12:37:45 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Zheng", "Jian-Qing", ""], ["Zhou", "Xiao-Yun", ""], ["Li", "Qing-Biao", ""], ["Riga", "Celia", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1804.02958", "submitter": "Michael Tschannen", "authors": "Eirikur Agustsson, Michael Tschannen, Fabian Mentzer, Radu Timofte,\n  Luc Van Gool", "title": "Generative Adversarial Networks for Extreme Learned Image Compression", "comments": "E. Agustsson, M. Tschannen, and F. Mentzer contributed equally to\n  this work. ICCV 2019 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learned image compression system based on GANs, operating at\nextremely low bitrates. Our proposed framework combines an encoder,\ndecoder/generator and a multi-scale discriminator, which we train jointly for a\ngenerative learned compression objective. The model synthesizes details it\ncannot afford to store, obtaining visually pleasing results at bitrates where\nprevious methods fail and show strong artifacts. Furthermore, if a semantic\nlabel map of the original image is available, our method can fully synthesize\nunimportant regions in the decoded image such as streets and trees from the\nlabel map, proportionally reducing the storage cost. A user study confirms that\nfor low bitrates, our approach is preferred to state-of-the-art methods, even\nwhen they use more than double the bits.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 13:13:29 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 17:13:59 GMT"}, {"version": "v3", "created": "Sun, 18 Aug 2019 13:02:02 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Agustsson", "Eirikur", ""], ["Tschannen", "Michael", ""], ["Mentzer", "Fabian", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1804.02967", "submitter": "Jose Dolz", "authors": "Jose Dolz and Karthik Gopinath and Jing Yuan and Herve Lombaert and\n  Christian Desrosiers and Ismail Ben Ayed", "title": "HyperDense-Net: A hyper-densely connected CNN for multi-modal image\n  segmentation", "comments": "Paper accepted at IEEE TMI in October 2018. Last version of this\n  paper updates the reference to the IEEE TMI paper which compares the\n  submissions to the iSEG 2017 MICCAI Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, dense connections have attracted substantial attention in computer\nvision because they facilitate gradient flow and implicit deep supervision\nduring training. Particularly, DenseNet, which connects each layer to every\nother layer in a feed-forward fashion, has shown impressive performances in\nnatural image classification tasks. We propose HyperDenseNet, a 3D fully\nconvolutional neural network that extends the definition of dense connectivity\nto multi-modal segmentation problems. Each imaging modality has a path, and\ndense connections occur not only between the pairs of layers within the same\npath, but also between those across different paths. This contrasts with the\nexisting multi-modal CNN approaches, in which modeling several modalities\nrelies entirely on a single joint layer (or level of abstraction) for fusion,\ntypically either at the input or at the output of the network. Therefore, the\nproposed network has total freedom to learn more complex combinations between\nthe modalities, within and in-between all the levels of abstraction, which\nincreases significantly the learning representation. We report extensive\nevaluations over two different and highly competitive multi-modal brain tissue\nsegmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing\non 6-month infant data and the latter on adult images. HyperDenseNet yielded\nsignificant improvements over many state-of-the-art segmentation networks,\nranking at the top on both benchmarks. We further provide a comprehensive\nexperimental analysis of features re-use, which confirms the importance of\nhyper-dense connections in multi-modal representation learning. Our code is\npublicly available at https://www.github.com/josedolz/HyperDenseNet.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 13:26:13 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 01:21:07 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Dolz", "Jose", ""], ["Gopinath", "Karthik", ""], ["Yuan", "Jing", ""], ["Lombaert", "Herve", ""], ["Desrosiers", "Christian", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "1804.02975", "submitter": "Deng-Ping Fan", "authors": "Deng-Ping Fan, ShengChuan Zhang, Yu-Huan Wu, Ming-Ming Cheng, Bo Ren,\n  Rongrong Ji and Paul L Rosin", "title": "Face Sketch Synthesis Style Similarity:A New Structure Co-occurrence\n  Texture Measure", "comments": "9pages, 15 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Existing face sketch synthesis (FSS) similarity measures are sensitive to\nslight image degradation (e.g., noise, blur). However, human perception of the\nsimilarity of two sketches will consider both structure and texture as\nessential factors and is not sensitive to slight (\"pixel-level\") mismatches.\nConsequently, the use of existing similarity measures can lead to better\nalgorithms receiving a lower score than worse algorithms. This unreliable\nevaluation has significantly hindered the development of the FSS field. To\nsolve this problem, we propose a novel and robust style similarity measure\ncalled Scoot-measure (Structure CO-Occurrence Texture Measure), which\nsimultaneously evaluates \"block-level\" spatial structure and co-occurrence\ntexture statistics. In addition, we further propose 4 new meta-measures and\ncreate 2 new datasets to perform a comprehensive evaluation of several\nwidely-used FSS measures on two large databases. Experimental results\ndemonstrate that our measure not only provides a reliable evaluation but also\nachieves significantly improved performance. Specifically, the study indicated\na higher degree (78.8%) of correlation between our measure and human judgment\nthan the best prior measure (58.6%). Our code will be made available.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 13:44:42 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Fan", "Deng-Ping", ""], ["Zhang", "ShengChuan", ""], ["Wu", "Yu-Huan", ""], ["Cheng", "Ming-Ming", ""], ["Ren", "Bo", ""], ["Ji", "Rongrong", ""], ["Rosin", "Paul L", ""]]}, {"id": "1804.03008", "submitter": "Gongning Luo", "authors": "Gongning Luo, Suyu Dong, Kuanquan Wang, Wangmeng Zuo, Shaodong Cao and\n  Henggui Zhang", "title": "Multi-views Fusion CNN for Left Ventricular Volumes Estimation on\n  Cardiac MR Images", "comments": "to appear on Transactions on Biomedical Engineering", "journal-ref": null, "doi": "10.1109/TBME.2017.2762762", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Left ventricular (LV) volumes estimation is a critical procedure for cardiac\ndisease diagnosis. The objective of this paper is to address direct LV volumes\nprediction task. Methods: In this paper, we propose a direct volumes prediction\nmethod based on the end-to-end deep convolutional neural networks (CNN). We\nstudy the end-to-end LV volumes prediction method in items of the data\npreprocessing, networks structure, and multi-views fusion strategy. The main\ncontributions of this paper are the following aspects. First, we propose a new\ndata preprocessing method on cardiac magnetic resonance (CMR). Second, we\npropose a new networks structure for end-to-end LV volumes estimation. Third,\nwe explore the representational capacity of different slices, and propose a\nfusion strategy to improve the prediction accuracy. Results: The evaluation\nresults show that the proposed method outperforms other state-of-the-art LV\nvolumes estimation methods on the open accessible benchmark datasets. The\nclinical indexes derived from the predicted volumes agree well with the ground\ntruth (EDV: R2=0.974, RMSE=9.6ml; ESV: R2=0.976, RMSE=7.1ml; EF: R2=0.828, RMSE\n=4.71%). Conclusion: Experimental results prove that the proposed method may be\nuseful for LV volumes prediction task. Significance: The proposed method not\nonly has application potential for cardiac diseases screening for large-scale\nCMR data, but also can be extended to other medical image research fields\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 14:14:02 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Luo", "Gongning", ""], ["Dong", "Suyu", ""], ["Wang", "Kuanquan", ""], ["Zuo", "Wangmeng", ""], ["Cao", "Shaodong", ""], ["Zhang", "Henggui", ""]]}, {"id": "1804.03022", "submitter": "Giovanni Saponaro", "authors": "Giovanni Saponaro, Pedro Vicente, Atabak Dehban, Lorenzo Jamone,\n  Alexandre Bernardino, Jos\\'e Santos-Victor", "title": "Learning at the Ends: From Hand to Tool Affordances in Humanoid Robots", "comments": "dataset available at htts://vislab.isr.tecnico.ulisboa.pt/, IEEE\n  International Conference on Development and Learning and on Epigenetic\n  Robotics (ICDL-EpiRob 2017)", "journal-ref": null, "doi": "10.1109/DEVLRN.2017.8329826", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the open challenges in designing robots that operate successfully in\nthe unpredictable human environment is how to make them able to predict what\nactions they can perform on objects, and what their effects will be, i.e., the\nability to perceive object affordances. Since modeling all the possible world\ninteractions is unfeasible, learning from experience is required, posing the\nchallenge of collecting a large amount of experiences (i.e., training data).\nTypically, a manipulative robot operates on external objects by using its own\nhands (or similar end-effectors), but in some cases the use of tools may be\ndesirable, nevertheless, it is reasonable to assume that while a robot can\ncollect many sensorimotor experiences using its own hands, this cannot happen\nfor all possible human-made tools.\n  Therefore, in this paper we investigate the developmental transition from\nhand to tool affordances: what sensorimotor skills that a robot has acquired\nwith its bare hands can be employed for tool use? By employing a visual and\nmotor imagination mechanism to represent different hand postures compactly, we\npropose a probabilistic model to learn hand affordances, and we show how this\nmodel can generalize to estimate the affordances of previously unseen tools,\nultimately supporting planning, decision-making and tool selection tasks in\nhumanoid robots. We present experimental results with the iCub humanoid robot,\nand we publicly release the collected sensorimotor data in the form of a hand\nposture affordances dataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 14:28:15 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Saponaro", "Giovanni", ""], ["Vicente", "Pedro", ""], ["Dehban", "Atabak", ""], ["Jamone", "Lorenzo", ""], ["Bernardino", "Alexandre", ""], ["Santos-Victor", "Jos\u00e9", ""]]}, {"id": "1804.03032", "submitter": "Wan-Lei Zhao", "authors": "Wan-Lei Zhao, Hui Wang, Chong-Wah Ngo", "title": "Approximate k-NN Graph Construction: a Generic Online Approach", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor search and k-nearest neighbor graph construction are two\nfundamental issues arise from many disciplines such as multimedia information\nretrieval, data-mining and machine learning. They become more and more imminent\ngiven the big data emerge in various fields in recent years. In this paper, a\nsimple but effective solution both for approximate k-nearest neighbor search\nand approximate k-nearest neighbor graph construction is presented. These two\nissues are addressed jointly in our solution. On the one hand, the approximate\nk-nearest neighbor graph construction is treated as a search task. Each sample\nalong with its k-nearest neighbors are joined into the k-nearest neighbor graph\nby performing the nearest neighbor search sequentially on the graph under\nconstruction. On the other hand, the built k-nearest neighbor graph is used to\nsupport k-nearest neighbor search. Since the graph is built online, the dynamic\nupdate on the graph, which is not possible from most of the existing solutions,\nis supported. This solution is feasible for various distance measures. Its\neffectiveness both as k-nearest neighbor construction and k-nearest neighbor\nsearch approaches is verified across different types of data in different\nscales, various dimensions and under different metrics.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 14:49:19 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 12:25:05 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 11:21:12 GMT"}, {"version": "v4", "created": "Wed, 21 Aug 2019 00:17:09 GMT"}, {"version": "v5", "created": "Thu, 17 Sep 2020 06:08:09 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Zhao", "Wan-Lei", ""], ["Wang", "Hui", ""], ["Ngo", "Chong-Wah", ""]]}, {"id": "1804.03037", "submitter": "Katrin Lasinger", "authors": "Katrin Lasinger, Christoph Vogel, Thomas Pock, Konrad Schindler", "title": "3D Fluid Flow Estimation with Integrated Particle Reconstruction", "comments": "To appear in International Journal of Computer Vision (IJCV)", "journal-ref": null, "doi": "10.1007/s11263-019-01261-6", "report-no": null, "categories": "cs.CV physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard approach to densely reconstruct the motion in a volume of fluid\nis to inject high-contrast tracer particles and record their motion with\nmultiple high-speed cameras. Almost all existing work processes the acquired\nmulti-view video in two separate steps, utilizing either a pure Eulerian or\npure Lagrangian approach. Eulerian methods perform a voxel-based reconstruction\nof particles per time step, followed by 3D motion estimation, with some form of\ndense matching between the precomputed voxel grids from different time steps.\nIn this sequential procedure, the first step cannot use temporal consistency\nconsiderations to support the reconstruction, while the second step has no\naccess to the original, high-resolution image data. Alternatively, Lagrangian\nmethods reconstruct an explicit, sparse set of particles and track the\nindividual particles over time. Physical constraints can only be incorporated\nin a post-processing step when interpolating the particle tracks to a dense\nmotion field. We show, for the first time, how to jointly reconstruct both the\nindividual tracer particles and a dense 3D fluid motion field from the image\ndata, using an integrated energy minimization. Our hybrid Lagrangian/Eulerian\nmodel reconstructs individual particles, and at the same time recovers a dense\n3D motion field in the entire domain. Making particles explicit greatly reduces\nthe memory consumption and allows one to use the high-res input images for\nmatching. Whereas the dense motion field makes it possible to include physical\na-priori constraints and account for the incompressibility and viscosity of the\nfluid. The method exhibits greatly (~70%) improved results over our recently\npublished baseline with two separate steps for 3D reconstruction and motion\nestimation. Our results with only two time steps are comparable to those of\nsota tracking-based methods that require much longer sequences.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 14:54:35 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 09:30:50 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 19:12:58 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Lasinger", "Katrin", ""], ["Vogel", "Christoph", ""], ["Pock", "Thomas", ""], ["Schindler", "Konrad", ""]]}, {"id": "1804.03068", "submitter": "Nicolas Dobigeon", "authors": "Vinicius Ferraris, Nicolas Dobigeon, Marie Chabert", "title": "Robust fusion algorithms for unsupervised change detection between\n  multi-band optical images - A comprehensive case study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised change detection techniques are generally constrained to two\nmulti-band optical images acquired at different times through sensors sharing\nthe same spatial and spectral resolution. This scenario is suitable for a\nstraight comparison of homologous pixels such as pixel-wise differencing.\nHowever, in some specific cases such as emergency situations, the only\navailable images may be those acquired through different kinds of sensors with\ndifferent resolutions. Recently some change detection techniques dealing with\nimages with different spatial and spectral resolutions, have been proposed.\nNevertheless, they are focused on a specific scenario where one image has a\nhigh spatial and low spectral resolution while the other has a low spatial and\nhigh spectral resolution. This paper addresses the problem of detecting changes\nbetween any two multi-band optical images disregarding their spatial and\nspectral resolution disparities. We propose a method that effectively uses the\navailable information by modeling the two observed images as spatially and\nspectrally degraded versions of two (unobserved) latent images characterized by\nthe same high spatial and high spectral resolutions. Covering the same scene,\nthe latent images are expected to be globally similar except for possible\nchanges in spatially sparse locations. Thus, the change detection task is\nenvisioned through a robust fusion task which enforces the differences between\nthe estimated latent images to be spatially sparse. We show that this robust\nfusion can be formulated as an inverse problem which is iteratively solved\nusing an alternate minimization strategy. The proposed framework is implemented\nfor an exhaustive list of applicative scenarios and applied to real multi-band\noptical images. A comparison with state-of-the-art change detection methods\nevidences the accuracy of the proposed robust fusion-based strategy.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 15:57:22 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Ferraris", "Vinicius", ""], ["Dobigeon", "Nicolas", ""], ["Chabert", "Marie", ""]]}, {"id": "1804.03080", "submitter": "Xiaolong Wang", "authors": "Xiaolong Wang, Rohit Girdhar, Abhinav Gupta", "title": "Binge Watching: Scaling Affordance Learning from Sitcoms", "comments": "CVPR 2017, project page:\n  http://www.cs.cmu.edu/~xiaolonw/affordance.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been a renewed interest in jointly modeling\nperception and action. At the core of this investigation is the idea of\nmodeling affordances(Affordances are opportunities of interaction in the scene.\nIn other words, it represents what actions can the object be used for).\nHowever, when it comes to predicting affordances, even the state of the art\napproaches still do not use any ConvNets. Why is that? Unlike semantic or 3D\ntasks, there still does not exist any large-scale dataset for affordances. In\nthis paper, we tackle the challenge of creating one of the biggest dataset for\nlearning affordances. We use seven sitcoms to extract a diverse set of scenes\nand how actors interact with different objects in the scenes. Our dataset\nconsists of more than 10K scenes and 28K ways humans can interact with these\n10K images. We also propose a two-step approach to predict affordances in a new\nscene. In the first step, given a location in the scene we classify which of\nthe 30 pose classes is the likely affordance pose. Given the pose class and the\nscene, we then use a Variational Autoencoder (VAE) to extract the scale and\ndeformation of the pose. The VAE allows us to sample the distribution of\npossible poses at test time. Finally, we show the importance of large-scale\ndata in learning a generalizable and robust model of affordances.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 16:14:05 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Wang", "Xiaolong", ""], ["Girdhar", "Rohit", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1804.03082", "submitter": "Hadi Kazemi", "authors": "Hadi Kazemi, Sobhan Soleymani, Ali Dabouei, Mehdi Iranmanesh, Nasser\n  M. Nasrabadi", "title": "Attribute-Centered Loss for Soft-Biometrics Guided Face Sketch-Photo\n  Recognition", "comments": "Accepted as a conference paper on CVPRW 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face sketches are able to capture the spatial topology of a face while\nlacking some facial attributes such as race, skin, or hair color. Existing\nsketch-photo recognition approaches have mostly ignored the importance of\nfacial attributes. In this paper, we propose a new loss function, called\nattribute-centered loss, to train a Deep Coupled Convolutional Neural Network\n(DCCNN) for the facial attribute guided sketch to photo matching. Specifically,\nan attribute-centered loss is proposed which learns several distinct centers,\nin a shared embedding space, for photos and sketches with different\ncombinations of attributes. The DCCNN simultaneously is trained to map photos\nand pairs of testified attributes and corresponding forensic sketches around\ntheir associated centers, while preserving the spatial topology information.\nImportantly, the centers learn to keep a relative distance from each other,\nrelated to their number of contradictory attributes. Extensive experiments are\nperformed on composite (E-PRIP) and semi-forensic (IIIT-D Semi-forensic)\ndatabases. The proposed method significantly outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 16:15:50 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Kazemi", "Hadi", ""], ["Soleymani", "Sobhan", ""], ["Dabouei", "Ali", ""], ["Iranmanesh", "Mehdi", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1804.03115", "submitter": "Jiri Fajtl", "authors": "Jiri Fajtl, Vasileios Argyriou, Dorothy Monekosso, Paolo Remagnino", "title": "AMNet: Memorability Estimation with Attention", "comments": "To appear at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the design and evaluation of an end-to-end\ntrainable, deep neural network with a visual attention mechanism for\nmemorability estimation in still images. We analyze the suitability of transfer\nlearning of deep models from image classification to the memorability task.\nFurther on we study the impact of the attention mechanism on the memorability\nestimation and evaluate our network on the SUN Memorability and the LaMem\ndatasets. Our network outperforms the existing state of the art models on both\ndatasets in terms of the Spearman's rank correlation as well as the mean\nsquared error, closely matching human consistency.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 17:28:00 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Fajtl", "Jiri", ""], ["Argyriou", "Vasileios", ""], ["Monekosso", "Dorothy", ""], ["Remagnino", "Paolo", ""]]}, {"id": "1804.03131", "submitter": "Yuhua Chen", "authors": "Yuhua Chen, Jordi Pont-Tuset, Alberto Montes, Luc Van Gool", "title": "Blazingly Fast Video Object Segmentation with Pixel-Wise Metric Learning", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of video object segmentation, given some user\nannotation which indicates the object of interest. The problem is formulated as\npixel-wise retrieval in a learned embedding space: we embed pixels of the same\nobject instance into the vicinity of each other, using a fully convolutional\nnetwork trained by a modified triplet loss as the embedding model. Then the\nannotated pixels are set as reference and the rest of the pixels are classified\nusing a nearest-neighbor approach. The proposed method supports different kinds\nof user input such as segmentation mask in the first frame (semi-supervised\nscenario), or a sparse set of clicked points (interactive scenario). In the\nsemi-supervised scenario, we achieve results competitive with the state of the\nart but at a fraction of computation cost (275 milliseconds per frame). In the\ninteractive scenario where the user is able to refine their input iteratively,\nthe proposed method provides instant response to each input, and reaches\ncomparable quality to competing methods with much less interaction.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 17:54:35 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Chen", "Yuhua", ""], ["Pont-Tuset", "Jordi", ""], ["Montes", "Alberto", ""], ["Van Gool", "Luc", ""]]}, {"id": "1804.03142", "submitter": "Alexander  Mathis", "authors": "Alexander Mathis, Pranav Mamidanna, Taiga Abe, Kevin M. Cury,\n  Venkatesh N. Murthy, Mackenzie W. Mathis and Matthias Bethge", "title": "Markerless tracking of user-defined features with deep learning", "comments": "Videos at http://www.mousemotorlab.org/deeplabcut", "journal-ref": "Nature Neuroscience, Technical Report, published: 20 August 2018", "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying behavior is crucial for many applications in neuroscience.\nVideography provides easy methods for the observation and recording of animal\nbehavior in diverse settings, yet extracting particular aspects of a behavior\nfor further analysis can be highly time consuming. In motor control studies,\nhumans or other animals are often marked with reflective markers to assist with\ncomputer-based tracking, yet markers are intrusive (especially for smaller\nanimals), and the number and location of the markers must be determined a\npriori. Here, we present a highly efficient method for markerless tracking\nbased on transfer learning with deep neural networks that achieves excellent\nresults with minimal training data. We demonstrate the versatility of this\nframework by tracking various body parts in a broad collection of experimental\nsettings: mice odor trail-tracking, egg-laying behavior in drosophila, and\nmouse hand articulation in a skilled forelimb task. For example, during the\nskilled reaching behavior, individual joints can be automatically tracked (and\na confidence score is reported). Remarkably, even when a small number of frames\nare labeled ($\\approx 200$), the algorithm achieves excellent tracking\nperformance on test frames that is comparable to human accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 17:10:39 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Mathis", "Alexander", ""], ["Mamidanna", "Pranav", ""], ["Abe", "Taiga", ""], ["Cury", "Kevin M.", ""], ["Murthy", "Venkatesh N.", ""], ["Mathis", "Mackenzie W.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1804.03160", "submitter": "Hang Zhao", "authors": "Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh\n  McDermott, Antonio Torralba", "title": "The Sound of Pixels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce PixelPlayer, a system that, by leveraging large amounts of\nunlabeled videos, learns to locate image regions which produce sounds and\nseparate the input sounds into a set of components that represents the sound\nfrom each pixel. Our approach capitalizes on the natural synchronization of the\nvisual and audio modalities to learn models that jointly parse sounds and\nimages, without requiring additional manual supervision. Experimental results\non a newly collected MUSIC dataset show that our proposed Mix-and-Separate\nframework outperforms several baselines on source separation. Qualitative\nresults suggest our model learns to ground sounds in vision, enabling\napplications such as independently adjusting the volume of sound sources.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 18:00:36 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 05:57:30 GMT"}, {"version": "v3", "created": "Mon, 30 Jul 2018 23:47:04 GMT"}, {"version": "v4", "created": "Sun, 14 Oct 2018 01:09:15 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Zhao", "Hang", ""], ["Gan", "Chuang", ""], ["Rouditchenko", "Andrew", ""], ["Vondrick", "Carl", ""], ["McDermott", "Josh", ""], ["Torralba", "Antonio", ""]]}, {"id": "1804.03166", "submitter": "Zhizhong Li", "authors": "Zhizhong Li, Derek Hoiem", "title": "Improving Confidence Estimates for Unfamiliar Examples", "comments": "Published in CVPR 2020 (oral). ERRATA: (1) a previous version (v3)\n  included erroneous results for $T$-scaling, where novel samples are\n  mistakenly included in the validation set for calibration. Please disregard\n  those results. (2) Previous versions (v4, v5) incorrectly stated that Adam\n  was used. In fact, we used SGD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intuitively, unfamiliarity should lead to lack of confidence. In reality,\ncurrent algorithms often make highly confident yet wrong predictions when faced\nwith relevant but unfamiliar examples. A classifier we trained to recognize\ngender is 12 times more likely to be wrong with a 99% confident prediction if\npresented with a subject from a different age group than those seen during\ntraining. In this paper, we compare and evaluate several methods to improve\nconfidence estimates for unfamiliar and familiar samples. We propose a testing\nmethodology of splitting unfamiliar and familiar samples by attribute (age,\nbreed, subcategory) or sampling (similar datasets collected by different people\nat different times). We evaluate methods including confidence calibration,\nensembles, distillation, and a Bayesian model and use several metrics to\nanalyze label, likelihood, and calibration error. While all methods reduce\nover-confident errors, the ensemble of calibrated models performs best overall,\nand T-scaling performs best among the approaches with fastest inference. Our\ncode is available at https://github.com/lizhitwo/ConfidenceEstimates .\n  $\\color{red}{\\text{Please see UPDATED ERRATA.}}$\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 18:08:14 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 15:41:03 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 17:59:22 GMT"}, {"version": "v4", "created": "Mon, 6 Jan 2020 18:58:24 GMT"}, {"version": "v5", "created": "Thu, 14 May 2020 17:57:18 GMT"}, {"version": "v6", "created": "Mon, 7 Sep 2020 18:42:19 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Li", "Zhizhong", ""], ["Hoiem", "Derek", ""]]}, {"id": "1804.03190", "submitter": "Hosein M. Golshan", "authors": "Hosein M. Golshan, Adam O. Hebb, Joshua Nedrud, Mohammad H. Mahoor", "title": "Studying the Effects of Deep Brain Stimulation and Medication on the\n  Dynamics of STN-LFP Signals for Human Behavior Analysis", "comments": "40th IEEE International Conference on Engineering in Medicine and\n  Biology (IEEE EMBC), Honolulu, Hawaii, July 17-21, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the results of our recent work on studying the effects of\ndeep brain stimulation (DBS) and medication on the dynamics of brain local\nfield potential (LFP) signals used for behavior analysis of patients with\nParkinson s disease (PD). DBS is a technique used to alleviate the severe\nsymptoms of PD when pharmacotherapy is not very effective. Behavior recognition\nfrom the LFP signals recorded from the subthalamic nucleus (STN) has\napplication in developing closed-loop DBS systems, where the stimulation pulse\nis adaptively generated according to subjects performing behavior. Most of the\nexisting studies on behavior recognition that use STN-LFPs are based on the DBS\nbeing off. This paper discovers how the performance and accuracy of automated\nbehavior recognition from the LFP signals are affected under different\nparadigms of stimulation on/off. We first study the notion of beta power\nsuppression in LFP signals under different scenarios (stimulation on/off and\nmedication on/off). Afterward, we explore the accuracy of support vector\nmachines in predicting human actions (button press and reach) using the\nspectrogram of STN-LFP signals. Our experiments on the recorded LFP signals of\nthree subjects confirm that the beta power is suppressed significantly when the\npatients take medication (p-value<0.002) or stimulation (p-value<0.0003). The\nresults also show that we can classify different behaviors with a reasonable\naccuracy of 85% even when the high-amplitude stimulation is applied.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 19:10:31 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Golshan", "Hosein M.", ""], ["Hebb", "Adam O.", ""], ["Nedrud", "Joshua", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "1804.03193", "submitter": "Pu Zhao", "authors": "Pu Zhao, Sijia Liu, Yanzhi Wang, Xue Lin", "title": "An ADMM-Based Universal Framework for Adversarial Attacks on Deep Neural\n  Networks", "comments": "9 pages, 3 figures, in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are known vulnerable to adversarial attacks. That\nis, adversarial examples, obtained by adding delicately crafted distortions\nonto original legal inputs, can mislead a DNN to classify them as any target\nlabels. In a successful adversarial attack, the targeted mis-classification\nshould be achieved with the minimal distortion added. In the literature, the\nadded distortions are usually measured by L0, L1, L2, and L infinity norms,\nnamely, L0, L1, L2, and L infinity attacks, respectively. However, there lacks\na versatile framework for all types of adversarial attacks.\n  This work for the first time unifies the methods of generating adversarial\nexamples by leveraging ADMM (Alternating Direction Method of Multipliers), an\noperator splitting optimization approach, such that L0, L1, L2, and L infinity\nattacks can be effectively implemented by this general framework with little\nmodifications. Comparing with the state-of-the-art attacks in each category,\nour ADMM-based attacks are so far the strongest, achieving both the 100% attack\nsuccess rate and the minimal distortion.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 19:23:01 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Zhao", "Pu", ""], ["Liu", "Sijia", ""], ["Wang", "Yanzhi", ""], ["Lin", "Xue", ""]]}, {"id": "1804.03224", "submitter": "Javad Fotouhi", "authors": "Javad Fotouhi, Mathias Unberath, Giacomo Taylor, Arash Ghaani\n  Farashahi, Bastian Bier, Russell H. Taylor, Greg M. Osgood, M.D., Mehran\n  Armand, and Nassir Navab", "title": "Exploiting Partial Structural Symmetry For Patient-Specific Image\n  Augmentation in Trauma Interventions", "comments": "JF, MU, and GT have contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unilateral pelvic fracture reductions, surgeons attempt to reconstruct the\nbone fragments such that bilateral symmetry in the bony anatomy is restored. We\npropose to exploit this \"structurally symmetric\" nature of the pelvic bone, and\nprovide intra-operative image augmentation to assist the surgeon in repairing\ndislocated fragments. The main challenge is to automatically estimate the\ndesired plane of symmetry within the patient's pre-operative CT. We propose to\nestimate this plane using a non-linear optimization strategy, by minimizing\nTukey's biweight robust estimator, relying on the partial symmetry of the\nanatomy. Moreover, a regularization term is designed to enforce the similarity\nof bone density histograms on both sides of this plane, relying on the\nbiological fact that, even if injured, the dislocated bone segments remain\nwithin the body. The experimental results demonstrate the performance of the\nproposed method in estimating this \"plane of partial symmetry\" using CT images\nof both healthy and injured anatomy. Examples of unilateral pelvic fractures\nare used to show how intra-operative X-ray images could be augmented with the\nforward-projections of the mirrored anatomy, acting as objective road-map for\nfracture reduction procedures.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 20:31:08 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Fotouhi", "Javad", ""], ["Unberath", "Mathias", ""], ["Taylor", "Giacomo", ""], ["Farashahi", "Arash Ghaani", ""], ["Bier", "Bastian", ""], ["Taylor", "Russell H.", ""], ["Osgood", "Greg M.", ""], ["D.", "M.", ""], ["Armand", "Mehran", ""], ["Navab", "Nassir", ""]]}, {"id": "1804.03230", "submitter": "Tien-Ju Yang", "authors": "Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark\n  Sandler, Vivienne Sze, Hartwig Adam", "title": "NetAdapt: Platform-Aware Neural Network Adaptation for Mobile\n  Applications", "comments": "Accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes an algorithm, called NetAdapt, that automatically adapts a\npre-trained deep neural network to a mobile platform given a resource budget.\nWhile many existing algorithms simplify networks based on the number of MACs or\nweights, optimizing those indirect metrics may not necessarily reduce the\ndirect metrics, such as latency and energy consumption. To solve this problem,\nNetAdapt incorporates direct metrics into its adaptation algorithm. These\ndirect metrics are evaluated using empirical measurements, so that detailed\nknowledge of the platform and toolchain is not required. NetAdapt automatically\nand progressively simplifies a pre-trained network until the resource budget is\nmet while maximizing the accuracy. Experiment results show that NetAdapt\nachieves better accuracy versus latency trade-offs on both mobile CPU and\nmobile GPU, compared with the state-of-the-art automated network simplification\nalgorithms. For image classification on the ImageNet dataset, NetAdapt achieves\nup to a 1.7$\\times$ speedup in measured inference latency with equal or higher\naccuracy on MobileNets (V1&V2).\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 20:45:26 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 19:20:16 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Yang", "Tien-Ju", ""], ["Howard", "Andrew", ""], ["Chen", "Bo", ""], ["Zhang", "Xiao", ""], ["Go", "Alec", ""], ["Sandler", "Mark", ""], ["Sze", "Vivienne", ""], ["Adam", "Hartwig", ""]]}, {"id": "1804.03247", "submitter": "Aj Piergiovanni", "authors": "AJ Piergiovanni and Michael S. Ryoo", "title": "Fine-grained Activity Recognition in Baseball Videos", "comments": "CVPR Workshop on Computer Vision in Sports", "journal-ref": "CVPR Workshop on Computer Vision in Sports 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a challenging new dataset, MLB-YouTube, designed\nfor fine-grained activity detection. The dataset contains two settings:\nsegmented video classification as well as activity detection in continuous\nvideos. We experimentally compare various recognition approaches capturing\ntemporal structure in activity videos, by classifying segmented videos and\nextending those approaches to continuous videos. We also compare models on the\nextremely difficult task of predicting pitch speed and pitch type from\nbroadcast baseball videos. We find that learning temporal structure is valuable\nfor fine-grained activity recognition.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 21:32:36 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1804.03270", "submitter": "Craig A. Glastonbury", "authors": "Michael Ferlaino, Craig A. Glastonbury, Carolina Motta-Mejia, Manu\n  Vatish, Ingrid Granne, Stephen Kennedy, Cecilia M. Lindgren, Christoffer\n  Nell{\\aa}ker", "title": "Towards Deep Cellular Phenotyping in Placental Histology", "comments": "Updated MRC funding material. Corrected typo that suggested\n  ensembling and Inception accuracy were the same (updated to reflect the fact\n  the ensemble model is 1% better than previously reported)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The placenta is a complex organ, playing multiple roles during fetal\ndevelopment. Very little is known about the association between placental\nmorphological abnormalities and fetal physiology. In this work, we present an\nopen sourced, computationally tractable deep learning pipeline to analyse\nplacenta histology at the level of the cell. By utilising two deep\nConvolutional Neural Network architectures and transfer learning, we can\nrobustly localise and classify placental cells within five classes with an\naccuracy of 89%. Furthermore, we learn deep embeddings encoding phenotypic\nknowledge that is capable of both stratifying five distinct cell populations\nand learn intraclass phenotypic variance. We envisage that the automation of\nthis pipeline to population scale studies of placenta histology has the\npotential to improve our understanding of basic cellular placental biology and\nits variations, particularly its role in predicting adverse birth outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 23:11:10 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 18:40:30 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Ferlaino", "Michael", ""], ["Glastonbury", "Craig A.", ""], ["Motta-Mejia", "Carolina", ""], ["Vatish", "Manu", ""], ["Granne", "Ingrid", ""], ["Kennedy", "Stephen", ""], ["Lindgren", "Cecilia M.", ""], ["Nell\u00e5ker", "Christoffer", ""]]}, {"id": "1804.03281", "submitter": "Jean-Baptiste Boin", "authors": "Jean-Baptiste Boin, Andre Araujo, Bernd Girod", "title": "Recurrent Neural Networks for Person Re-identification Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of person re-identification has recently received rising attention\ndue to the high performance achieved by new methods based on deep learning. In\nparticular, in the context of video-based re-identification, many\nstate-of-the-art works have explored the use of Recurrent Neural Networks\n(RNNs) to process input sequences. In this work, we revisit this tool by\nderiving an approximation which reveals the small effect of recurrent\nconnections, leading to a much simpler feed-forward architecture. Using the\nsame parameters as the recurrent version, our proposed feed-forward\narchitecture obtains very similar accuracy. More importantly, our model can be\ncombined with a new training process to significantly improve re-identification\nperformance. Our experiments demonstrate that the proposed models converge\nsubstantially faster than recurrent ones, with accuracy improvements by up to\n5% on two datasets. The performance achieved is better or on par with other\nRNN-based person re-identification techniques.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 00:14:37 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Boin", "Jean-Baptiste", ""], ["Araujo", "Andre", ""], ["Girod", "Bernd", ""]]}, {"id": "1804.03286", "submitter": "Nicholas Carlini", "authors": "Anish Athalye, Nicholas Carlini", "title": "On the Robustness of the CVPR 2018 White-Box Adversarial Example\n  Defenses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are known to be vulnerable to adversarial examples. In this\nnote, we evaluate the two white-box defenses that appeared at CVPR 2018 and\nfind they are ineffective: when applying existing techniques, we can reduce the\naccuracy of the defended models to 0%.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 04:54:29 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Athalye", "Anish", ""], ["Carlini", "Nicholas", ""]]}, {"id": "1804.03287", "submitter": "Jian Zhao", "authors": "Jian Zhao, Jianshu Li, Yu Cheng, Li Zhou, Terence Sim, Shuicheng Yan,\n  Jiashi Feng", "title": "Understanding Humans in Crowded Scenes: Deep Nested Adversarial Learning\n  and A New Benchmark for Multi-Human Parsing", "comments": "The first three authors are with equal contributions", "journal-ref": null, "doi": "10.13140/RG.2.2.23242.67523", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the noticeable progress in perceptual tasks like detection, instance\nsegmentation and human parsing, computers still perform unsatisfactorily on\nvisually understanding humans in crowded scenes, such as group behavior\nanalysis, person re-identification and autonomous driving, etc. To this end,\nmodels need to comprehensively perceive the semantic information and the\ndifferences between instances in a multi-human image, which is recently defined\nas the multi-human parsing task. In this paper, we present a new large-scale\ndatabase \"Multi-Human Parsing (MHP)\" for algorithm development and evaluation,\nand advances the state-of-the-art in understanding humans in crowded scenes.\nMHP contains 25,403 elaborately annotated images with 58 fine-grained semantic\ncategory labels, involving 2-26 persons per image and captured in real-world\nscenes from various viewpoints, poses, occlusion, interactions and background.\nWe further propose a novel deep Nested Adversarial Network (NAN) model for\nmulti-human parsing. NAN consists of three Generative Adversarial Network\n(GAN)-like sub-nets, respectively performing semantic saliency prediction,\ninstance-agnostic parsing and instance-aware clustering. These sub-nets form a\nnested structure and are carefully designed to learn jointly in an end-to-end\nway. NAN consistently outperforms existing state-of-the-art solutions on our\nMHP and several other datasets, and serves as a strong baseline to drive the\nfuture research for multi-human parsing.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 00:41:26 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 02:04:05 GMT"}, {"version": "v3", "created": "Fri, 6 Jul 2018 05:49:40 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Zhao", "Jian", ""], ["Li", "Jianshu", ""], ["Cheng", "Yu", ""], ["Zhou", "Li", ""], ["Sim", "Terence", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1804.03294", "submitter": "Tianyun Zhang", "authors": "Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Jian Tang, Wujie Wen, Makan\n  Fardad, Yanzhi Wang", "title": "A Systematic DNN Weight Pruning Framework using Alternating Direction\n  Method of Multipliers", "comments": null, "journal-ref": "ECCV 2018, pp 191-207", "doi": "10.1007/978-3-030-01237-3_12", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight pruning methods for deep neural networks (DNNs) have been investigated\nrecently, but prior work in this area is mainly heuristic, iterative pruning,\nthereby lacking guarantees on the weight reduction ratio and convergence time.\nTo mitigate these limitations, we present a systematic weight pruning framework\nof DNNs using the alternating direction method of multipliers (ADMM). We first\nformulate the weight pruning problem of DNNs as a nonconvex optimization\nproblem with combinatorial constraints specifying the sparsity requirements,\nand then adopt the ADMM framework for systematic weight pruning. By using ADMM,\nthe original nonconvex optimization problem is decomposed into two subproblems\nthat are solved iteratively. One of these subproblems can be solved using\nstochastic gradient descent, the other can be solved analytically. Besides, our\nmethod achieves a fast convergence rate.\n  The weight pruning results are very promising and consistently outperform the\nprior work. On the LeNet-5 model for the MNIST data set, we achieve 71.2 times\nweight reduction without accuracy loss. On the AlexNet model for the ImageNet\ndata set, we achieve 21 times weight reduction without accuracy loss. When we\nfocus on the convolutional layer pruning for computation reductions, we can\nreduce the total computation by five times compared with the prior work\n(achieving a total of 13.4 times weight reduction in convolutional layers). Our\nmodels and codes are released at https://github.com/KaiqiZhang/admm-pruning\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 01:14:51 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 02:19:16 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2018 16:52:02 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Zhang", "Tianyun", ""], ["Ye", "Shaokai", ""], ["Zhang", "Kaiqi", ""], ["Tang", "Jian", ""], ["Wen", "Wujie", ""], ["Fardad", "Makan", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1804.03312", "submitter": "Ke Yu", "authors": "Ke Yu, Chao Dong, Liang Lin, Chen Change Loy", "title": "Crafting a Toolchain for Image Restoration by Deep Reinforcement\n  Learning", "comments": "To appear at CVPR 2018 (Spotlight). Project page:\n  http://mmlab.ie.cuhk.edu.hk/projects/RL-Restore/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a novel approach for image restoration by reinforcement\nlearning. Unlike existing studies that mostly train a single large network for\na specialized task, we prepare a toolbox consisting of small-scale\nconvolutional networks of different complexities and specialized in different\ntasks. Our method, RL-Restore, then learns a policy to select appropriate tools\nfrom the toolbox to progressively restore the quality of a corrupted image. We\nformulate a step-wise reward function proportional to how well the image is\nrestored at each step to learn the action policy. We also devise a joint\nlearning scheme to train the agent and tools for better performance in handling\nuncertainty. In comparison to conventional human-designed networks, RL-Restore\nis capable of restoring images corrupted with complex and unknown distortions\nin a more parameter-efficient manner using the dynamically formed toolchain.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 02:30:40 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Yu", "Ke", ""], ["Dong", "Chao", ""], ["Lin", "Liang", ""], ["Loy", "Chen Change", ""]]}, {"id": "1804.03313", "submitter": "Liyao Gao Mr.", "authors": "Liyao Gao", "title": "Cortex Neural Network: learning with Neural Network groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Network has been successfully applied to many real-world problems,\nsuch as image recognition and machine translation. However, for the current\narchitecture of neural networks, it is hard to perform complex cognitive tasks,\nfor example, to process the image and audio inputs together. Cortex, as an\nimportant architecture in the brain, is important for animals to perform the\ncomplex cognitive task. We view the architecture of Cortex in the brain as a\nmissing part in the design of the current artificial neural network. In this\npaper, we purpose Cortex Neural Network (CrtxNN). The Cortex Neural Network is\nan upper architecture of neural networks which motivated from cerebral cortex\nin the brain to handle different tasks in the same learning system. It is able\nto identify different tasks and solve them with different methods. In our\nimplementation, the Cortex Neural Network is able to process different\ncognitive tasks and perform reflection to get a higher accuracy. We provide a\nseries of experiments to examine the capability of the cortex architecture on\ntraditional neural networks. Our experiments proved its ability on the Cortex\nNeural Network can reach accuracy by 98.32% on MNIST and 62% on CIFAR10 at the\nsame time, which can promisingly reduce the loss by 40%.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 02:33:47 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Gao", "Liyao", ""]]}, {"id": "1804.03343", "submitter": "Bo Zhao", "authors": "Bo Zhao, Bo Chang, Zequn Jie, Leonid Sigal", "title": "Modular Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for multi-domain image-to-image translation (or generation)\nattempt to directly map an input image (or a random vector) to an image in one\nof the output domains. However, most existing methods have limited scalability\nand robustness, since they require building independent models for each pair of\ndomains in question. This leads to two significant shortcomings: (1) the need\nto train exponential number of pairwise models, and (2) the inability to\nleverage data from other domains when training a particular pairwise mapping.\nInspired by recent work on module networks, this paper proposes ModularGAN for\nmulti-domain image generation and image-to-image translation. ModularGAN\nconsists of several reusable and composable modules that carry on different\nfunctions (e.g., encoding, decoding, transformations). These modules can be\ntrained simultaneously, leveraging data from all domains, and then combined to\nconstruct specific GAN networks at test time, according to the specific image\ntranslation task. This leads to ModularGAN's superior flexibility of generating\n(or translating to) an image in any desired domain. Experimental results\ndemonstrate that our model not only presents compelling perceptual results but\nalso outperforms state-of-the-art methods on multi-domain facial attribute\ntransfer.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 05:28:00 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Zhao", "Bo", ""], ["Chang", "Bo", ""], ["Jie", "Zequn", ""], ["Sigal", "Leonid", ""]]}, {"id": "1804.03348", "submitter": "Raghavendra Selvan", "authors": "Raghavendra Selvan, Max Welling, Jesper H. Pedersen, Jens Petersen,\n  Marleen de Bruijne", "title": "Mean Field Network based Graph Refinement with application to Airway\n  Tree Extraction", "comments": "10 pages. Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present tree extraction in 3D images as a graph refinement task, of\nobtaining a subgraph from an over-complete input graph. To this end, we\nformulate an approximate Bayesian inference framework on undirected graphs\nusing mean field approximation (MFA). Mean field networks are used for\ninference based on the interpretation that iterations of MFA can be seen as\nfeed-forward operations in a neural network. This allows us to learn the model\nparameters from training data using back-propagation algorithm. We demonstrate\nusefulness of the model to extract airway trees from 3D chest CT data. We first\nobtain probability images using a voxel classifier that distinguishes airways\nfrom background and use Bayesian smoothing to model individual airway branches.\nThis yields us joint Gaussian density estimates of position, orientation and\nscale as node features of the input graph. Performance of the method is\ncompared with two methods: the first uses probability images from a trained\nvoxel classifier with region growing, which is similar to one of the best\nperforming methods at EXACT'09 airway challenge, and the second method is based\non Bayesian smoothing on these probability images. Using centerline distance as\nerror measure the presented method shows significant improvement compared to\nthese two methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 05:52:22 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Selvan", "Raghavendra", ""], ["Welling", "Max", ""], ["Pedersen", "Jesper H.", ""], ["Petersen", "Jens", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1804.03360", "submitter": "Zhifei Zhang", "authors": "Zhifei Zhang, Zhaowen Wang, Zhe Lin, and Hairong Qi", "title": "Reference-Conditioned Super-Resolution by Neural Texture Transfer", "comments": "Project Page:\n  http://web.eecs.utk.edu/~zzhang61/project_page/SRNTT/SRNTT.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the recent advancement in deep learning, we have witnessed a great\nprogress in single image super-resolution. However, due to the significant\ninformation loss of the image downscaling process, it has become extremely\nchallenging to further advance the state-of-the-art, especially for large\nupscaling factors. This paper explores a new research direction in super\nresolution, called reference-conditioned super-resolution, in which a reference\nimage containing desired high-resolution texture details is provided besides\nthe low-resolution image. We focus on transferring the high-resolution texture\nfrom reference images to the super-resolution process without the constraint of\ncontent similarity between reference and target images, which is a key\ndifference from previous example-based methods. Inspired by recent work on\nimage stylization, we address the problem via neural texture transfer. We\ndesign an end-to-end trainable deep model which generates detail enriched\nresults by adaptively fusing the content from the low-resolution image with the\ntexture patterns from the reference image. We create a benchmark dataset for\nthe general research of reference-based super-resolution, which contains\nreference images paired with low-resolution inputs with varying degrees of\nsimilarity. Both objective and subjective evaluations demonstrate the great\npotential of using reference images as well as the superiority of our results\nover other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 06:30:44 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Zhang", "Zhifei", ""], ["Wang", "Zhaowen", ""], ["Lin", "Zhe", ""], ["Qi", "Hairong", ""]]}, {"id": "1804.03368", "submitter": "Dong Gong", "authors": "Dong Gong, Zhen Zhang, Qinfeng Shi, Anton van den Hengel, Chunhua\n  Shen, Yanning Zhang", "title": "Learning Deep Gradient Descent Optimization for Image Deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an integral component of blind image deblurring, non-blind deconvolution\nremoves image blur with a given blur kernel, which is essential but difficult\ndue to the ill-posed nature of the inverse problem. The predominant approach is\nbased on optimization subject to regularization functions that are either\nmanually designed, or learned from examples. Existing learning based methods\nhave shown superior restoration quality but are not practical enough due to\ntheir restricted and static model design. They solely focus on learning a prior\nand require to know the noise level for deconvolution. We address the gap\nbetween the optimization-based and learning-based approaches by learning a\nuniversal gradient descent optimizer. We propose a Recurrent Gradient Descent\nNetwork (RGDN) by systematically incorporating deep neural networks into a\nfully parameterized gradient descent scheme. A hyper-parameter-free update unit\nshared across steps is used to generate updates from the current estimates,\nbased on a convolutional neural network. By training on diverse examples, the\nRecurrent Gradient Descent Network learns an implicit image prior and a\nuniversal update rule through recursive supervision. The learned optimizer can\nbe repeatedly used to improve the quality of diverse degenerated observations.\nThe proposed method possesses strong interpretability and high generalization.\nExtensive experiments on synthetic benchmarks and challenging real-world images\ndemonstrate that the proposed deep optimization method is effective and robust\nto produce favorable results as well as practical for real-world image\ndeblurring applications.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 06:58:12 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 06:09:24 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Gong", "Dong", ""], ["Zhang", "Zhen", ""], ["Shi", "Qinfeng", ""], ["Hengel", "Anton van den", ""], ["Shen", "Chunhua", ""], ["Zhang", "Yanning", ""]]}, {"id": "1804.03390", "submitter": "Georg Poier", "authors": "Georg Poier, David Schinagl, Horst Bischof", "title": "Learning Pose Specific Representations by Predicting Different Views", "comments": "CVPR 2018 (Spotlight); Project Page at\n  https://poier.github.io/PreView/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The labeled data required to learn pose estimation for articulated objects is\ndifficult to provide in the desired quantity, realism, density, and accuracy.\nTo address this issue, we develop a method to learn representations, which are\nvery specific for articulated poses, without the need for labeled training\ndata. We exploit the observation that the object pose of a known object is\npredictive for the appearance in any known view. That is, given only the pose\nand shape parameters of a hand, the hand's appearance from any viewpoint can be\napproximated. To exploit this observation, we train a model that -- given input\nfrom one view -- estimates a latent representation, which is trained to be\npredictive for the appearance of the object when captured from another\nviewpoint. Thus, the only necessary supervision is the second view. The\ntraining process of this model reveals an implicit pose representation in the\nlatent space. Importantly, at test time the pose representation can be inferred\nusing only a single view. In qualitative and quantitative experiments we show\nthat the learned representations capture detailed pose information. Moreover,\nwhen training the proposed method jointly with labeled and unlabeled data, it\nconsistently surpasses the performance of its fully supervised counterpart,\nwhile reducing the amount of needed labeled samples by at least one order of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 08:22:23 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 15:02:46 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Poier", "Georg", ""], ["Schinagl", "David", ""], ["Bischof", "Horst", ""]]}, {"id": "1804.03393", "submitter": "Erik J Bekkers", "authors": "Erik J Bekkers, Maxime W Lafarge, Mitko Veta, Koen AJ Eppenhof, Josien\n  PW Pluim, Remco Duits", "title": "Roto-Translation Covariant Convolutional Networks for Medical Image\n  Analysis", "comments": "8 pages, 2 figures, 1 table, accepted at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for rotation and translation covariant deep learning\nusing $SE(2)$ group convolutions. The group product of the special Euclidean\nmotion group $SE(2)$ describes how a concatenation of two roto-translations\nresults in a net roto-translation. We encode this geometric structure into\nconvolutional neural networks (CNNs) via $SE(2)$ group convolutional layers,\nwhich fit into the standard 2D CNN framework, and which allow to generically\ndeal with rotated input samples without the need for data augmentation.\n  We introduce three layers: a lifting layer which lifts a 2D (vector valued)\nimage to an $SE(2)$-image, i.e., 3D (vector valued) data whose domain is\n$SE(2)$; a group convolution layer from and to an $SE(2)$-image; and a\nprojection layer from an $SE(2)$-image to a 2D image. The lifting and group\nconvolution layers are $SE(2)$ covariant (the output roto-translates with the\ninput). The final projection layer, a maximum intensity projection over\nrotations, makes the full CNN rotation invariant.\n  We show with three different problems in histopathology, retinal imaging, and\nelectron microscopy that with the proposed group CNNs, state-of-the-art\nperformance can be achieved, without the need for data augmentation by rotation\nand with increased performance compared to standard CNNs that do rely on\naugmentation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 08:23:44 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 15:07:57 GMT"}, {"version": "v3", "created": "Mon, 11 Jun 2018 17:53:31 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Bekkers", "Erik J", ""], ["Lafarge", "Maxime W", ""], ["Veta", "Mitko", ""], ["Eppenhof", "Koen AJ", ""], ["Pluim", "Josien PW", ""], ["Duits", "Remco", ""]]}, {"id": "1804.03415", "submitter": "Ka Chun Lam", "authors": "Thomas Y. Hou, De Huang, Ka Chun Lam, Ziyun Zhang", "title": "A Fast Hierarchically Preconditioned Eigensolver Based On\n  Multiresolution Matrix Decomposition", "comments": "46 pages, 11 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we propose a new iterative method to hierarchically compute a\nrelatively large number of leftmost eigenpairs of a sparse symmetric positive\nmatrix under the multiresolution operator compression framework. We exploit the\nwell-conditioned property of every decomposition components by integrating the\nmultiresolution framework into the Implicitly restarted Lanczos method. We\nachieve this combination by proposing an extension-refinement iterative scheme,\nin which the intrinsic idea is to decompose the target spectrum into several\nsegments such that the corresponding eigenproblem in each segment is\nwell-conditioned. Theoretical analysis and numerical illustration are also\nreported to illustrate the efficiency and effectiveness of this algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 09:30:37 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 15:06:57 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Hou", "Thomas Y.", ""], ["Huang", "De", ""], ["Lam", "Ka Chun", ""], ["Zhang", "Ziyun", ""]]}, {"id": "1804.03429", "submitter": "Chongxuan Li", "authors": "Chongxuan Li and Max Welling and Jun Zhu and Bo Zhang", "title": "Graphical Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Graphical Generative Adversarial Networks (Graphical-GAN) to model\nstructured data. Graphical-GAN conjoins the power of Bayesian networks on\ncompactly representing the dependency structures among random variables and\nthat of generative adversarial networks on learning expressive dependency\nfunctions. We introduce a structured recognition model to infer the posterior\ndistribution of latent variables given observations. We generalize the\nExpectation Propagation (EP) algorithm to learn the generative model and\nrecognition model jointly. Finally, we present two important instances of\nGraphical-GAN, i.e. Gaussian Mixture GAN (GMGAN) and State Space GAN (SSGAN),\nwhich can successfully learn the discrete and temporal structures on visual\ndatasets, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 10:12:38 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 08:20:54 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Li", "Chongxuan", ""], ["Welling", "Max", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "1804.03447", "submitter": "Ryota Natsume", "authors": "Ryota Natsume, Tatsuya Yatagawa, Shigeo Morishima", "title": "RSGAN: Face Swapping and Editing using Face and Hair Representation in\n  Latent Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an integrated system for automatically generating\nand editing face images through face swapping, attribute-based editing, and\nrandom face parts synthesis. The proposed system is based on a deep neural\nnetwork that variationally learns the face and hair regions with large-scale\nface image datasets. Different from conventional variational methods, the\nproposed network represents the latent spaces individually for faces and hairs.\nWe refer to the proposed network as region-separative generative adversarial\nnetwork (RSGAN). The proposed network independently handles face and hair\nappearances in the latent spaces, and then, face swapping is achieved by\nreplacing the latent-space representations of the faces, and reconstruct the\nentire face image with them. This approach in the latent space robustly\nperforms face swapping even for images which the previous methods result in\nfailure due to inappropriate fitting or the 3D morphable models. In addition,\nthe proposed system can further edit face-swapped images with the same network\nby manipulating visual attributes or by composing them with randomly generated\nface or hair parts.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 10:54:34 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 06:44:06 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Natsume", "Ryota", ""], ["Yatagawa", "Tatsuya", ""], ["Morishima", "Shigeo", ""]]}, {"id": "1804.03457", "submitter": "Jaroslaw Kwapien", "authors": "Rafa{\\l} Rak, Jaros{\\l}aw Kwapie\\'n, Pawe{\\l} O\\'swi\\k{e}cimka,\n  Pawe{\\l} Zi\\k{e}ba, Stanis{\\l}aw Dro\\.zd\\.z", "title": "Universal features of mountain ridge networks on Earth", "comments": "to appear in Journal of Complex Networks", "journal-ref": null, "doi": "10.1093/comnet/cnz017", "report-no": null, "categories": "physics.geo-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to the heavily studied surface drainage systems, the mountain ridge\nsystems have been a subject of less attention even on the empirical level,\ndespite the fact that their structure is richer. To reduce this deficiency, we\nanalyze different mountain ranges by means of a network approach and grasp some\nessential features of the ridge branching structure. We also employ a fractal\nanalysis as it is especially suitable for describing properties of rough\nobjects and surfaces. As our approach differs from typical analyses that are\ncarried out in geophysics, we believe that it can initialize a research\ndirection that will allow to shed more light on the processes that are\nresponsible for landscape formation and will contribute to the network theory\nby indicating a need for the construction of new models of the network growth\nas no existing model can properly describe the ridge formation. We also believe\nthat certain features of our study can offer help in the cartographic\ngeneralization. Specifically, we study structure of the ridge networks based on\nthe empirical elevation data collected by SRTM. We consider mountain ranges\nfrom different geological periods and geographical locations. For each mountain\nrange, we construct a simple topographic network representation (the ridge\njunctions are nodes) and a ridge representation (the ridges are nodes and the\njunctions are edges) and calculate the parameters characterizing their\ntopology. We observe that the topographic networks inherit the fractal\nstructure of the mountain ranges but do not show any other complex features. In\ncontrast, the ridge networks, while lacking the proper fractality, reveal the\npower-law degree distributions with the exponent $1.6\\le \\beta \\le 1.7$. By\ntaking into account the fact that the analyzed mountains differ in many\nproperties, these values seem to be universal for the earthly mountainous\nterrain.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 11:19:55 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 13:01:44 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 13:18:06 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Rak", "Rafa\u0142", ""], ["Kwapie\u0144", "Jaros\u0142aw", ""], ["O\u015bwi\u0119cimka", "Pawe\u0142", ""], ["Zi\u0119ba", "Pawe\u0142", ""], ["Dro\u017cd\u017c", "Stanis\u0142aw", ""]]}, {"id": "1804.03487", "submitter": "Yu Liu", "authors": "Yu Liu, Fangyin Wei, Jing Shao, Lu Sheng, Junjie Yan, Xiaogang Wang", "title": "Exploring Disentangled Feature Representation Beyond Face Identification", "comments": "Accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes learning disentangled but complementary face features\nwith minimal supervision by face identification. Specifically, we construct an\nidentity Distilling and Dispelling Autoencoder (D2AE) framework that\nadversarially learns the identity-distilled features for identity verification\nand the identity-dispelled features to fool the verification system. Thanks to\nthe design of two-stream cues, the learned disentangled features represent not\nonly the identity or attribute but the complete input image. Comprehensive\nevaluations further demonstrate that the proposed features not only maintain\nstate-of-the-art identity verification performance on LFW, but also acquire\ncompetitive discriminative power for face attribute recognition on CelebA and\nLFWA. Moreover, the proposed system is ready to semantically control the face\ngeneration/editing based on various identities and attributes in an\nunsupervised manner.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 12:59:53 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Liu", "Yu", ""], ["Wei", "Fangyin", ""], ["Shao", "Jing", ""], ["Sheng", "Lu", ""], ["Yan", "Junjie", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1804.03492", "submitter": "Mikaela Angelina Uy", "authors": "Mikaela Angelina Uy and Gim Hee Lee", "title": "PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place\n  Recognition", "comments": "CVPR 2018, 11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike its image based counterpart, point cloud based retrieval for place\nrecognition has remained as an unexplored and unsolved problem. This is largely\ndue to the difficulty in extracting local feature descriptors from a point\ncloud that can subsequently be encoded into a global descriptor for the\nretrieval task. In this paper, we propose the PointNetVLAD where we leverage on\nthe recent success of deep networks to solve point cloud based retrieval for\nplace recognition. Specifically, our PointNetVLAD is a combination/modification\nof the existing PointNet and NetVLAD, which allows end-to-end training and\ninference to extract the global descriptor from a given 3D point cloud.\nFurthermore, we propose the \"lazy triplet and quadruplet\" loss functions that\ncan achieve more discriminative and generalizable global descriptors to tackle\nthe retrieval task. We create benchmark datasets for point cloud based\nretrieval for place recognition, and the experimental results on these datasets\nshow the feasibility of our PointNetVLAD. Our code and the link for the\nbenchmark dataset downloads are available in our project website.\nhttp://github.com/mikacuy/pointnetvlad/\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 13:06:56 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 06:58:55 GMT"}, {"version": "v3", "created": "Wed, 16 May 2018 08:47:33 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Uy", "Mikaela Angelina", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1804.03531", "submitter": "Michael Miller", "authors": "Michael Snow, Jan Van lent", "title": "The Monge-Kantorovich Optimal Transport Distance for Image Comparison", "comments": "arXiv admin note: substantial text overlap with arXiv:1612.00181", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the Monge-Kantorovich formulation of the optimal\ntransport problem and the associated $L^2$ Wasserstein distance. We use the\n$L^2$ Wasserstein distance in the Nearest Neighbour (NN) machine learning\narchitecture to demonstrate the potential power of the optimal transport\ndistance for image comparison. We compare the Wasserstein distance to other\nestablished distances - including the partial differential equation (PDE)\nformulation of the optimal transport problem - and demonstrate that on the well\nknown MNIST optical character recognition dataset, it achieves excellent\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 10:27:33 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Snow", "Michael", ""], ["Van lent", "Jan", ""]]}, {"id": "1804.03547", "submitter": "Yujiang Wang", "authors": "Yujiang Wang, Jie Shen, Stavros Petridis, Maja Pantic", "title": "A real-time and unsupervised face Re-Identification system for\n  Human-Robot Interaction", "comments": "Accepted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of Human-Robot Interaction (HRI), face Re-Identification (face\nRe-ID) aims to verify if certain detected faces have already been observed by\nrobots. The ability of distinguishing between different users is crucial in\nsocial robots as it will enable the robot to tailor the interaction strategy\ntoward the users' individual preferences. So far face recognition research has\nachieved great success, however little attention has been paid to the realistic\napplications of Face Re-ID in social robots. In this paper, we present an\neffective and unsupervised face Re-ID system which simultaneously re-identifies\nmultiple faces for HRI. This Re-ID system employs Deep Convolutional Neural\nNetworks to extract features, and an online clustering algorithm to determine\nthe face's ID. Its performance is evaluated on two datasets: the TERESA video\ndataset collected by the TERESA robot, and the YouTube Face Dataset (YTF\nDataset). We demonstrate that the optimised combination of techniques achieves\nan overall 93.55% accuracy on TERESA dataset and an overall 90.41% accuracy on\nYTF dataset. We have implemented the proposed method into a software module in\nthe HCI^2 Framework for it to be further integrated into the TERESA robot, and\nhas achieved real-time performance at 10~26 Frames per second.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 14:07:45 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 15:20:31 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Wang", "Yujiang", ""], ["Shen", "Jie", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "1804.03550", "submitter": "Martin Garbade", "authors": "Martin Garbade, Yueh-Tung Chen, Johann Sawatzky, Juergen Gall", "title": "Two Stream 3D Semantic Scene Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the 3D geometry and the semantic meaning of surfaces, which are\noccluded, is a very challenging task. Recently, a first end-to-end learning\napproach has been proposed that completes a scene from a single depth image.\nThe approach voxelizes the scene and predicts for each voxel if it is occupied\nand, if it is occupied, the semantic class label. In this work, we propose a\ntwo stream approach that leverages depth information and semantic information,\nwhich is inferred from the RGB image, for this task. The approach constructs an\nincomplete 3D semantic tensor, which uses a compact three-channel encoding for\nthe inferred semantic information, and uses a 3D CNN to infer the complete 3D\nsemantic tensor. In our experimental evaluation, we show that the proposed two\nstream approach substantially outperforms the state-of-the-art for semantic\nscene completion.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 14:10:26 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 16:37:53 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 14:35:56 GMT"}, {"version": "v4", "created": "Wed, 15 May 2019 14:36:17 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Garbade", "Martin", ""], ["Chen", "Yueh-Tung", ""], ["Sawatzky", "Johann", ""], ["Gall", "Juergen", ""]]}, {"id": "1804.03558", "submitter": "Gaoussou Haidara", "authors": "Haidara Gaoussou and Peng Dewei", "title": "Evaluation of the visual odometry methods for semi-dense real-time", "comments": "14 pages, 6 figures", "journal-ref": "Advanced Computing: An International Journal (ACIJ), Vol.9, No.2,\n  March 2018", "doi": "10.5121/acij", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent decades have witnessed a significant increase in the use of visual\nodometry(VO) in the computer vision area. It has also been used in varieties of\nrobotic applications, for example on the Mars Exploration Rovers. This paper,\nfirstly, discusses two popular existing visual odometry approaches, namely\nLSD-SLAM and ORB-SLAM2 to improve the performance metrics of visual SLAM\nsystems using Umeyama Method. We carefully evaluate the methods referred to\nabove on three different well-known KITTI datasets, EuRoC MAV dataset, and TUM\nRGB-D dataset to obtain the best results and graphically compare the results to\nevaluation metrics from different visual odometry approaches. Secondly, we\npropose an approach running in real-time with a stereo camera, which combines\nan existing feature-based (indirect) method and an existing feature-less\n(direct) method matching with accurate semidense direct image alignment and\nreconstructing an accurate 3D environment directly on pixels that have image\ngradient. Keywords VO, performance metrics, Umeyama Method, feature-based\nmethod, feature-less method & semi-dense real-time.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 14:28:08 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 08:14:58 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Gaoussou", "Haidara", ""], ["Dewei", "Peng", ""]]}, {"id": "1804.03576", "submitter": "Daniel Harari", "authors": "Hadar Gorodissky, Daniel Harari and Shimon Ullman", "title": "Large Field and High Resolution: Detecting Needle in Haystack", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing use of convolutional neural networks (CNN) for a broad range of\nvisual tasks, including tasks involving fine details, raises the problem of\napplying such networks to a large field of view, since the amount of\ncomputations increases significantly with the number of pixels. To deal\neffectively with this difficulty, we develop and compare methods of using CNNs\nfor the task of small target localization in natural images, given a limited\n\"budget\" of samples to form an image. Inspired in part by human vision, we\ndevelop and compare variable sampling schemes, with peak resolution at the\ncenter and decreasing resolution with eccentricity, applied iteratively by\nre-centering the image at the previous predicted target location. The results\nindicate that variable resolution models significantly outperform constant\nresolution models. Surprisingly, variable resolution models and in particular\nmulti-channel models, outperform the optimal, \"budget-free\" full-resolution\nmodel, using only 5\\% of the samples.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 14:56:21 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Gorodissky", "Hadar", ""], ["Harari", "Daniel", ""], ["Ullman", "Shimon", ""]]}, {"id": "1804.03583", "submitter": "Xavier Roynard", "authors": "Xavier Roynard, Jean-Emmanuel Deschaud and Fran\\c{c}ois Goulette", "title": "Classification of Point Cloud Scenes with Multiscale Voxel Deep Network", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we describe a new convolutional neural network (CNN) to\nclassify 3D point clouds of urban or indoor scenes. Solutions are given to the\nproblems encountered working on scene point clouds, and a network is described\nthat allows for point classification using only the position of points in a\nmulti-scale neighborhood.\n  On the reduced-8 Semantic3D benchmark [Hackel et al., 2017], this network,\nranked second, beats the state of the art of point classification methods\n(those not using a regularization step).\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 15:14:11 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Roynard", "Xavier", ""], ["Deschaud", "Jean-Emmanuel", ""], ["Goulette", "Fran\u00e7ois", ""]]}, {"id": "1804.03584", "submitter": "Jos\\'e Ignacio Ronda", "authors": "Jos\\'e I. Ronda and Antonio Vald\\'es", "title": "Geometrical analysis of polynomial lens distortion models", "comments": "Accepted in the Journal of Mathematical Imaging and Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial functions are a usual choice to model the nonlinearity of lenses.\nTypically, these models are obtained through physical analysis of the lens\nsystem or on purely empirical grounds. The aim of this work is to facilitate an\nalternative approach to the selection or design of these models based on\nestablishing a priori the desired geometrical properties of the distortion\nfunctions. With this purpose we obtain all the possible isotropic linear models\nand also those that are formed by functions with symmetry with respect to some\naxis. In this way, the classical models (decentering, thin prism distortion)\nare found to be particular instances of the family of models found by geometric\nconsiderations. These results allow to find generalizations of the most usually\nemployed models while preserving the desired geometrical properties. Our\nresults also provide a better understanding of the geometric properties of the\nmodels employed in the most usual computer vision software libraries.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 15:16:05 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 10:18:19 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Ronda", "Jos\u00e9 I.", ""], ["Vald\u00e9s", "Antonio", ""]]}, {"id": "1804.03596", "submitter": "Xinghao Ding", "authors": "Liyan Sun, Zhiwen Fan, Yue Huang, Xinghao Ding and John Paisley", "title": "A Deep Information Sharing Network for Multi-contrast Compressed Sensing\n  MRI Reconstruction", "comments": "13 pages, 16 figures, 3 tables", "journal-ref": null, "doi": "10.1109/TIP.2019.2925288", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-contrast magnetic resonance imaging (MRI), compressed sensing theory\ncan accelerate imaging by sampling fewer measurements within each contrast. The\nconventional optimization-based models suffer several limitations: strict\nassumption of shared sparse support, time-consuming optimization and \"shallow\"\nmodels with difficulties in encoding the rich patterns hiding in massive MRI\ndata. In this paper, we propose the first deep learning model for\nmulti-contrast MRI reconstruction. We achieve information sharing through\nfeature sharing units, which significantly reduces the number of parameters.\nThe feature sharing unit is combined with a data fidelity unit to comprise an\ninference block. These inference blocks are cascaded with dense connections,\nwhich allows for information transmission across different depths of the\nnetwork efficiently. Our extensive experiments on various multi-contrast MRI\ndatasets show that proposed model outperforms both state-of-the-art\nsingle-contrast and multi-contrast MRI methods in accuracy and efficiency. We\nshow the improved reconstruction quality can bring great benefits for the later\nmedical image analysis stage. Furthermore, the robustness of the proposed model\nto the non-registration environment shows its potential in real MRI\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 15:43:48 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Sun", "Liyan", ""], ["Fan", "Zhiwen", ""], ["Huang", "Yue", ""], ["Ding", "Xinghao", ""], ["Paisley", "John", ""]]}, {"id": "1804.03608", "submitter": "Tanmay Gupta", "authors": "Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha\n  Kembhavi", "title": "Imagine This! Scripts to Compositions to Videos", "comments": "Supplementary material included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagining a scene described in natural language with realistic layout and\nappearance of entities is the ultimate test of spatial, visual, and semantic\nworld knowledge. Towards this goal, we present the Composition, Retrieval, and\nFusion Network (CRAFT), a model capable of learning this knowledge from\nvideo-caption data and applying it while generating videos from novel captions.\nCRAFT explicitly predicts a temporal-layout of mentioned entities (characters\nand objects), retrieves spatio-temporal entity segments from a video database\nand fuses them to generate scene videos. Our contributions include sequential\ntraining of components of CRAFT while jointly modeling layout and appearances,\nand losses that encourage learning compositional representations for retrieval.\nWe evaluate CRAFT on semantic fidelity to caption, composition consistency, and\nvisual quality. CRAFT outperforms direct pixel generation approaches and\ngeneralizes well to unseen captions and to unseen video databases with no text\nannotations. We demonstrate CRAFT on FLINTSTONES, a new richly annotated\nvideo-caption dataset with over 25000 videos. For a glimpse of videos generated\nby CRAFT, see https://youtu.be/688Vv86n0z8.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 15:59:45 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Gupta", "Tanmay", ""], ["Schwenk", "Dustin", ""], ["Farhadi", "Ali", ""], ["Hoiem", "Derek", ""], ["Kembhavi", "Aniruddha", ""]]}, {"id": "1804.03619", "submitter": "Ariel Ephrat", "authors": "Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson,\n  Avinatan Hassidim, William T. Freeman and Michael Rubinstein", "title": "Looking to Listen at the Cocktail Party: A Speaker-Independent\n  Audio-Visual Model for Speech Separation", "comments": "Accepted to SIGGRAPH 2018. Project webpage:\n  https://looking-to-listen.github.io", "journal-ref": "ACM Trans. Graph. 37(4): 112:1-112:11 (2018)", "doi": "10.1145/3197517.3201357", "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a joint audio-visual model for isolating a single speech signal\nfrom a mixture of sounds such as other speakers and background noise. Solving\nthis task using only audio as input is extremely challenging and does not\nprovide an association of the separated speech signals with speakers in the\nvideo. In this paper, we present a deep network-based model that incorporates\nboth visual and auditory signals to solve this task. The visual features are\nused to \"focus\" the audio on desired speakers in a scene and to improve the\nspeech separation quality. To train our joint audio-visual model, we introduce\nAVSpeech, a new dataset comprised of thousands of hours of video segments from\nthe Web. We demonstrate the applicability of our method to classic speech\nseparation tasks, as well as real-world scenarios involving heated interviews,\nnoisy bars, and screaming children, only requiring the user to specify the face\nof the person in the video whose speech they want to isolate. Our method shows\nclear advantage over state-of-the-art audio-only speech separation in cases of\nmixed speech. In addition, our model, which is speaker-independent (trained\nonce, applicable to any speaker), produces better results than recent\naudio-visual speech separation methods that are speaker-dependent (require\ntraining a separate model for each speaker of interest).\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 16:28:59 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 21:22:37 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Ephrat", "Ariel", ""], ["Mosseri", "Inbar", ""], ["Lang", "Oran", ""], ["Dekel", "Tali", ""], ["Wilson", "Kevin", ""], ["Hassidim", "Avinatan", ""], ["Freeman", "William T.", ""], ["Rubinstein", "Michael", ""]]}, {"id": "1804.03641", "submitter": "Andrew Owens", "authors": "Andrew Owens, Alexei A. Efros", "title": "Audio-Visual Scene Analysis with Self-Supervised Multisensory Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The thud of a bouncing ball, the onset of speech as lips open -- when visual\nand audio events occur together, it suggests that there might be a common,\nunderlying event that produced both signals. In this paper, we argue that the\nvisual and audio components of a video signal should be modeled jointly using a\nfused multisensory representation. We propose to learn such a representation in\na self-supervised way, by training a neural network to predict whether video\nframes and audio are temporally aligned. We use this learned representation for\nthree applications: (a) sound source localization, i.e. visualizing the source\nof sound in a video; (b) audio-visual action recognition; and (c) on/off-screen\naudio source separation, e.g. removing the off-screen translator's voice from a\nforeign official's speech. Code, models, and video results are available on our\nwebpage: http://andrewowens.com/multisensory\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 17:36:50 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 07:15:29 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Owens", "Andrew", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1804.03675", "submitter": "Baris Gecer", "authors": "Baris Gecer, Binod Bhattarai, Josef Kittler, and Tae-Kyun Kim", "title": "Semi-supervised Adversarial Learning to Generate Photorealistic Face\n  Images of New Identities from 3D Morphable Model", "comments": null, "journal-ref": "In Proceedings of the European conference on computer vision\n  (ECCV), 2018, pp. 217-234", "doi": "10.1007/978-3-030-01252-6_14", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel end-to-end semi-supervised adversarial framework to\ngenerate photorealistic face images of new identities with wide ranges of\nexpressions, poses, and illuminations conditioned by a 3D morphable model.\nPrevious adversarial style-transfer methods either supervise their networks\nwith large volume of paired data or use unpaired data with a highly\nunder-constrained two-way generative framework in an unsupervised fashion. We\nintroduce pairwise adversarial supervision to constrain two-way domain\nadaptation by a small number of paired real and synthetic images for training\nalong with the large volume of unpaired data. Extensive qualitative and\nquantitative experiments are performed to validate our idea. Generated face\nimages of new identities contain pose, lighting and expression diversity and\nqualitative results show that they are highly constraint by the synthetic input\nimage while adding photorealism and retaining identity information. We combine\nface images generated by the proposed method with the real data set to train\nface recognition algorithms. We evaluated the model on two challenging data\nsets: LFW and IJB-A. We observe that the generated images from our framework\nconsistently improves over the performance of deep face recognition network\ntrained with Oxford VGG Face dataset and achieves comparable results to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 18:18:30 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Gecer", "Baris", ""], ["Bhattarai", "Binod", ""], ["Kittler", "Josef", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1804.03683", "submitter": "Saman Sarraf", "authors": "Saman Sarraf", "title": "French Word Recognition through a Quick Survey on Recurrent Neural\n  Networks Using Long-Short Term Memory RNN-LSTM", "comments": null, "journal-ref": "American Scientific Research Journal for Engineering, Technology,\n  and Sciences (ASRJETS) (2018) Volume 39, No 1, pp 250-267", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical character recognition (OCR) is a fundamental problem in computer\nvision. Research studies have shown significant progress in classifying printed\ncharacters using deep learning-based methods and topologies. Among current\nalgorithms, recurrent neural networks with long-short term memory blocks called\nRNN-LSTM have provided the highest performance in terms of accuracy rate. Using\nthe top 5,000 French words collected from the internet including all signs and\naccents, RNN-LSTM models were trained and tested for several cases. Six fonts\nwere used to generate OCR samples and an additional dataset that included all\nsamples from these six fonts was prepared for training and testing purposes.\nThe trained RNN-LSTM models were tested and achieved the accuracy rates of\n99.98798% and 99.91889% for edit distance and sequence error, respectively. An\naccurate preprocessing followed by height normalization (standardization\nmethods in deep learning) enabled the RNN-LSTM model to be trained in the most\nefficient way. This machine learning work also revealed the robustness of\nRNN-LSTM topology to recognize printed characters.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 18:41:14 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Sarraf", "Saman", ""]]}, {"id": "1804.03700", "submitter": "Xin Yi", "authors": "Xin Yi, Ekta Walia, Paul Babyn", "title": "Unsupervised and semi-supervised learning with Categorical Generative\n  Adversarial Networks assisted by Wasserstein distance for dermoscopy image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Melanoma is a curable aggressive skin cancer if detected early. Typically,\nthe diagnosis involves initial screening with subsequent biopsy and\nhistopathological examination if necessary. Computer aided diagnosis offers an\nobjective score that is independent of clinical experience and the potential to\nlower the workload of a dermatologist. In the recent past, success of deep\nlearning algorithms in the field of general computer vision has motivated\nsuccessful application of supervised deep learning methods in computer aided\nmelanoma recognition. However, large quantities of labeled images are required\nto make further improvements on the supervised method. A good annotation\ngenerally requires clinical and histological confirmation, which requires\nsignificant effort. In an attempt to alleviate this constraint, we propose to\nuse categorical generative adversarial network to automatically learn the\nfeature representation of dermoscopy images in an unsupervised and\nsemi-supervised manner. Thorough experiments on ISIC 2016 skin lesion chal-\nlenge demonstrate that the proposed feature learning method has achieved an\naverage precision score of 0.424 with only 140 labeled images. Moreover, the\nproposed method is also capable of generating real-world like dermoscopy\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 19:53:53 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Yi", "Xin", ""], ["Walia", "Ekta", ""], ["Babyn", "Paul", ""]]}, {"id": "1804.03715", "submitter": "Nan Hu", "authors": "Nan Hu and Raif M. Rustamov and Leonidas Guibas", "title": "Graph Matching with Anchor Nodes: A Learning Approach", "comments": "final version for CVPR2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the weighted graph matching problem with partially\ndisclosed correspondences between a number of anchor nodes. Our construction\nexploits recently introduced node signatures based on graph Laplacians, namely\nthe Laplacian family signature (LFS) on the nodes, and the pairwise heat kernel\nmap on the edges. In this paper, without assuming an explicit form of\nparametric dependence nor a distance metric between node signatures, we\nformulate an optimization problem which incorporates the knowledge of anchor\nnodes. Solving this problem gives us an optimized proximity measure specific to\nthe graphs under consideration. Using this as a first order compatibility term,\nwe then set up an integer quadratic program (IQP) to solve for a near optimal\ngraph matching. Our experiments demonstrate the superior performance of our\napproach on randomly generated graphs and on two widely-used image sequences,\nwhen compared with other existing signature and adjacency matrix based graph\nmatching methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 20:49:50 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Hu", "Nan", ""], ["Rustamov", "Raif M.", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1804.03786", "submitter": "Luan Tran", "authors": "Luan Tran, Xiaoming Liu", "title": "Nonlinear 3D Face Morphable Model", "comments": "CVPR 2018 (Spotlight). Source code:\n  https://github.com/tranluan/Nonlinear_Face_3DMM . Project webpage:\n  http://cvlab.cse.msu.edu/project-nonlinear-3dmm.html . v3: minor revision,\n  adding source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a classic statistical model of 3D facial shape and texture, 3D Morphable\nModel (3DMM) is widely used in facial analysis, e.g., model fitting, image\nsynthesis. Conventional 3DMM is learned from a set of well-controlled 2D face\nimages with associated 3D face scans, and represented by two sets of PCA basis\nfunctions. Due to the type and amount of training data, as well as the linear\nbases, the representation power of 3DMM can be limited. To address these\nproblems, this paper proposes an innovative framework to learn a nonlinear 3DMM\nmodel from a large set of unconstrained face images, without collecting 3D face\nscans. Specifically, given a face image as input, a network encoder estimates\nthe projection, shape and texture parameters. Two decoders serve as the\nnonlinear 3DMM to map from the shape and texture parameters to the 3D shape and\ntexture, respectively. With the projection parameter, 3D shape, and texture, a\nnovel analytically-differentiable rendering layer is designed to reconstruct\nthe original input face. The entire network is end-to-end trainable with only\nweak supervision. We demonstrate the superior representation power of our\nnonlinear 3DMM over its linear counterpart, and its contribution to face\nalignment and 3D reconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 02:22:10 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 18:14:12 GMT"}, {"version": "v3", "created": "Sun, 26 Aug 2018 14:11:24 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Tran", "Luan", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1804.03787", "submitter": "Inchul Choi", "authors": "Inchul Choi, Arunava Banerjee", "title": "Multi-Scale Generalized Plane Match for Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite recent advances, estimating optical flow remains a challenging\nproblem in the presence of illumination change, large occlusions or fast\nmovement. In this paper, we propose a novel optical flow estimation framework\nwhich can provide accurate dense correspondence and occlusion localization\nthrough a multi-scale generalized plane matching approach. In our method, we\nregard the scene as a collection of planes at multiple scales, and for each\nsuch plane, compensate motion in consensus to improve match quality. We\nestimate the square patch plane distortion using a robust plane model detection\nmethod and iteratively apply a plane matching scheme within a multi-scale\nframework. During the flow estimation process, our enhanced plane matching\nmethod also clearly localizes the occluded regions. In experiments on\nMPI-Sintel datasets, our method robustly estimated optical flow from given\nnoisy correspondences, and also revealed the occluded regions accurately.\nCompared to other state-of-the-art optical flow methods, our method shows\naccurate occlusion localization, comparable optical flow quality, and better\nthin object detection.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 02:26:34 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Choi", "Inchul", ""], ["Banerjee", "Arunava", ""]]}, {"id": "1804.03789", "submitter": "Jatavallabhula Krishna Murthy", "authors": "Ganesh Iyer and J. Krishna Murthy and Gunshi Gupta and K. Madhava\n  Krishna and Liam Paull", "title": "Geometric Consistency for Self-Supervised End-to-End Visual Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the success of deep learning based approaches in tackling challenging\nproblems in computer vision, a wide range of deep architectures have recently\nbeen proposed for the task of visual odometry (VO) estimation. Most of these\nproposed solutions rely on supervision, which requires the acquisition of\nprecise ground-truth camera pose information, collected using expensive motion\ncapture systems or high-precision IMU/GPS sensor rigs. In this work, we propose\nan unsupervised paradigm for deep visual odometry learning. We show that using\na noisy teacher, which could be a standard VO pipeline, and by designing a loss\nterm that enforces geometric consistency of the trajectory, we can train\naccurate deep models for VO that do not require ground-truth labels. We\nleverage geometry as a self-supervisory signal and propose \"Composite\nTransformation Constraints (CTCs)\", that automatically generate supervisory\nsignals for training and enforce geometric consistency in the VO estimate. We\nalso present a method of characterizing the uncertainty in VO estimates thus\nobtained. To evaluate our VO pipeline, we present exhaustive ablation studies\nthat demonstrate the efficacy of end-to-end, self-supervised methodologies to\ntrain deep models for monocular VO. We show that leveraging concepts from\ngeometry and incorporating them into the training of a recurrent neural network\nresults in performance competitive to supervised deep VO methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 02:45:00 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Iyer", "Ganesh", ""], ["Murthy", "J. Krishna", ""], ["Gupta", "Gunshi", ""], ["Krishna", "K. Madhava", ""], ["Paull", "Liam", ""]]}, {"id": "1804.03803", "submitter": "Yu Wu", "authors": "Yu Wu, Linchao Zhu, Lu Jiang, Yi Yang", "title": "Decoupled Novel Object Captioner", "comments": "Accepted to ACM MM 2018", "journal-ref": null, "doi": "10.1145/3240508.3240640", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning is a challenging task where the machine automatically\ndescribes an image by sentences or phrases. It often requires a large number of\npaired image-sentence annotations for training. However, a pre-trained\ncaptioning model can hardly be applied to a new domain in which some novel\nobject categories exist, i.e., the objects and their description words are\nunseen during model training. To correctly caption the novel object, it\nrequires professional human workers to annotate the images by sentences with\nthe novel words. It is labor expensive and thus limits its usage in real-world\napplications.\n  In this paper, we introduce the zero-shot novel object captioning task where\nthe machine generates descriptions without extra sentences about the novel\nobject. To tackle the challenging problem, we propose a Decoupled Novel Object\nCaptioner (DNOC) framework that can fully decouple the language sequence model\nfrom the object descriptions. DNOC has two components. 1) A Sequence Model with\nthe Placeholder (SM-P) generates a sentence containing placeholders. The\nplaceholder represents an unseen novel object. Thus, the sequence model can be\ndecoupled from the novel object descriptions. 2) A key-value object memory\nbuilt upon the freely available detection model, contains the visual\ninformation and the corresponding word for each object. The SM-P will generate\na query to retrieve the words from the object memory. The placeholder will then\nbe filled with the correct word, resulting in a caption with novel object\ndescriptions. The experimental results on the held-out MSCOCO dataset\ndemonstrate the ability of DNOC in describing novel concepts in the zero-shot\nnovel object captioning task.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 04:21:22 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 08:36:55 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Wu", "Yu", ""], ["Zhu", "Linchao", ""], ["Jiang", "Lu", ""], ["Yang", "Yi", ""]]}, {"id": "1804.03809", "submitter": "Bolin Liu", "authors": "Bolin Liu, Xiao Shu, Xiaolin Wu", "title": "Demoir\\'eing of Camera-Captured Screen Images Using Deep Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking photos of optoelectronic displays is a direct and spontaneous way of\ntransferring data and keeping records, which is widely practiced. However, due\nto the analog signal interference between the pixel grids of the display screen\nand camera sensor array, objectionable moir\\'e (alias) patterns appear in\ncaptured screen images. As the moir\\'e patterns are structured and highly\nvariant, they are difficult to be completely removed without affecting the\nunderneath latent image. In this paper, we propose an approach of deep\nconvolutional neural network for demoir\\'eing screen photos. The proposed DCNN\nconsists of a coarse-scale network and a fine-scale network. In the\ncoarse-scale network, the input image is first downsampled and then processed\nby stacked residual blocks to remove the moir\\'e artifacts. After that, the\nfine-scale network upsamples the demoir\\'ed low-resolution image back to the\noriginal resolution. Extensive experimental results have demonstrated that the\nproposed technique can efficiently remove the moir\\'e patterns for camera\nacquired screen images; the new technique outperforms the existing ones.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 04:51:25 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Liu", "Bolin", ""], ["Shu", "Xiao", ""], ["Wu", "Xiaolin", ""]]}, {"id": "1804.03821", "submitter": "Zhenli Zhang", "authors": "Zhenli Zhang, Xiangyu Zhang, Chao Peng, Dazhi Cheng, Jian Sun", "title": "ExFuse: Enhancing Feature Fusion for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern semantic segmentation frameworks usually combine low-level and\nhigh-level features from pre-trained backbone convolutional models to boost\nperformance. In this paper, we first point out that a simple fusion of\nlow-level and high-level features could be less effective because of the gap in\nsemantic levels and spatial resolution. We find that introducing semantic\ninformation into low-level features and high-resolution details into high-level\nfeatures is more effective for the later fusion. Based on this observation, we\npropose a new framework, named ExFuse, to bridge the gap between low-level and\nhigh-level features thus significantly improve the segmentation quality by\n4.0\\% in total. Furthermore, we evaluate our approach on the challenging PASCAL\nVOC 2012 segmentation benchmark and achieve 87.9\\% mean IoU, which outperforms\nthe previous state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 05:54:31 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Zhang", "Zhenli", ""], ["Zhang", "Xiangyu", ""], ["Peng", "Chao", ""], ["Cheng", "Dazhi", ""], ["Sun", "Jian", ""]]}, {"id": "1804.03828", "submitter": "Takayasu Moriya", "authors": "Takayasu Moriya, Holger R. Roth, Shota Nakamura, Hirohisa Oda, Kai\n  Nagara, Masahiro Oda, Kensaku Mori", "title": "Unsupervised Pathology Image Segmentation Using Representation Learning\n  with Spherical K-means", "comments": "This paper was presented at SPIE Medical Imaging 2018, Houston, TX,\n  USA", "journal-ref": "Proc. SPIE 10581, Medical Imaging 2018: Digital Pathology, 1058111\n  (6 March 2018)", "doi": "10.1117/12.2292172", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for unsupervised segmentation of pathology\nimages. Staging of lung cancer is a major factor of prognosis. Measuring the\nmaximum dimensions of the invasive component in a pathology images is an\nessential task. Therefore, image segmentation methods for visualizing the\nextent of invasive and noninvasive components on pathology images could support\npathological examination. However, it is challenging for most of the recent\nsegmentation methods that rely on supervised learning to cope with unlabeled\npathology images. In this paper, we propose a unified approach to unsupervised\nrepresentation learning and clustering for pathology image segmentation. Our\nmethod consists of two phases. In the first phase, we learn feature\nrepresentations of training patches from a target image using the spherical\nk-means. The purpose of this phase is to obtain cluster centroids which could\nbe used as filters for feature extraction. In the second phase, we apply\nconventional k-means to the representations extracted by the centroids and then\nproject cluster labels to the target images. We evaluated our methods on\npathology images of lung cancer specimen. Our experiments showed that the\nproposed method outperforms traditional k-means segmentation and the\nmultithreshold Otsu method both quantitatively and qualitatively with an\nimproved normalized mutual information (NMI) score of 0.626 compared to 0.168\nand 0.167, respectively. Furthermore, we found that the centroids can be\napplied to the segmentation of other slices from the same sample.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 06:28:27 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Moriya", "Takayasu", ""], ["Roth", "Holger R.", ""], ["Nakamura", "Shota", ""], ["Oda", "Hirohisa", ""], ["Nagara", "Kai", ""], ["Oda", "Masahiro", ""], ["Mori", "Kensaku", ""]]}, {"id": "1804.03830", "submitter": "Takayasu Moriya", "authors": "Takayasu Moriya, Holger R. Roth, Shota Nakamura, Hirohisa Oda, Kai\n  Nagara, Masahiro Oda, Kensaku Mori", "title": "Unsupervised Segmentation of 3D Medical Images Based on Clustering and\n  Deep Representation Learning", "comments": "This paper was presented at SPIE Medical Imaging 2018, Houston, TX,\n  USA", "journal-ref": "Proc. SPIE 10578, Medical Imaging 2018: Biomedical Applications in\n  Molecular, Structural, and Functional Imaging, 1057820 (12 March 2018)", "doi": "10.1117/12.2293414", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel unsupervised segmentation method for 3D medical\nimages. Convolutional neural networks (CNNs) have brought significant advances\nin image segmentation. However, most of the recent methods rely on supervised\nlearning, which requires large amounts of manually annotated data. Thus, it is\nchallenging for these methods to cope with the growing amount of medical\nimages. This paper proposes a unified approach to unsupervised deep\nrepresentation learning and clustering for segmentation. Our proposed method\nconsists of two phases. In the first phase, we learn deep feature\nrepresentations of training patches from a target image using joint\nunsupervised learning (JULE) that alternately clusters representations\ngenerated by a CNN and updates the CNN parameters using cluster labels as\nsupervisory signals. We extend JULE to 3D medical images by utilizing 3D\nconvolutions throughout the CNN architecture. In the second phase, we apply\nk-means to the deep representations from the trained CNN and then project\ncluster labels to the target image in order to obtain the fully segmented\nimage. We evaluated our methods on three images of lung cancer specimens\nscanned with micro-computed tomography (micro-CT). The automatic segmentation\nof pathological regions in micro-CT could further contribute to the\npathological examination process. Hence, we aim to automatically divide each\nimage into the regions of invasive carcinoma, noninvasive carcinoma, and normal\ntissue. Our experiments show the potential abilities of unsupervised deep\nrepresentation learning for medical image segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 06:30:30 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Moriya", "Takayasu", ""], ["Roth", "Holger R.", ""], ["Nakamura", "Shota", ""], ["Oda", "Hirohisa", ""], ["Nagara", "Kai", ""], ["Oda", "Masahiro", ""], ["Mori", "Kensaku", ""]]}, {"id": "1804.03853", "submitter": "Qingguo Xiao", "authors": "Qingguo Xiao, Guangyao Li, Li Xie, Qiaochuan Chen", "title": "Real-world plant species identification based on deep convolutional\n  neural networks and visual attention", "comments": "published", "journal-ref": "Ecological Informatics, 2018, 48: 117-124", "doi": "10.1016/j.ecoinf.2018.09.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the issue of real-world identification to fulfill\nbetter species protection. We focus on plant species identification as it is a\nclassic and hot issue. In tradition plant species identification the samples\nare scanned specimen and the background is simple. However, real-world species\nrecognition is more challenging. We first systematically investigate what is\nrealistic species recognition and the difference from tradition plant species\nrecognition. To deal with the challenging task, an interdisciplinary\ncollaboration is presented based on the latest advances in computer science and\ntechnology. We propose a novel framework and an effective data augmentation\nmethod for deep learning in this paper. We first crop the image in terms of\nvisual attention before general recognition. Besides, we apply it as a data\naugmentation method. We call the novel data augmentation approach attention\ncropping (AC). Deep convolutional neural networks are trained to predict\nspecies from a large amount of data. Extensive experiments on traditional\ndataset and specific dataset for real-world recognition are conducted to\nevaluate the performance of our approach. Experiments first demonstrate that\nour approach achieves state-of-the-art results on different types of datasets.\nBesides, we also evaluate the performance of data augmentation method AC.\nResults show that AC provides superior performance. Compared with the precision\nof methods without AC, the results with AC achieve substantial improvement.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 07:51:59 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 05:19:05 GMT"}, {"version": "v3", "created": "Thu, 28 Feb 2019 11:38:01 GMT"}, {"version": "v4", "created": "Fri, 1 Mar 2019 02:08:45 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Xiao", "Qingguo", ""], ["Li", "Guangyao", ""], ["Xie", "Li", ""], ["Chen", "Qiaochuan", ""]]}, {"id": "1804.03864", "submitter": "Lei Qi", "authors": "Lei Qi, Jing Huo, Lei Wang, Yinghuan Shi, Yang Gao", "title": "MaskReID: A Mask Based Deep Ranking Neural Network for Person\n  Re-identification", "comments": "ICME2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person retrieval faces many challenges including cluttered background,\nappearance variations (e.g., illumination, pose, occlusion) among different\ncamera views and the similarity among different person's images. To address\nthese issues, we put forward a novel mask based deep ranking neural network\nwith a skipped fusing layer. Firstly, to alleviate the problem of cluttered\nbackground, masked images with only the foreground regions are incorporated as\ninput in the proposed neural network. Secondly, to reduce the impact of the\nappearance variations, the multi-layer fusion scheme is developed to obtain\nmore discriminative fine-grained information. Lastly, considering person\nretrieval is a special image retrieval task, we propose a novel ranking loss to\noptimize the whole network. The proposed ranking loss can further mitigate the\ninterference problem of similar negative samples when producing ranking\nresults. The extensive experiments validate the superiority of the proposed\nmethod compared with the state-of-the-art methods on many benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 08:17:14 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 02:16:24 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Qi", "Lei", ""], ["Huo", "Jing", ""], ["Wang", "Lei", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""]]}, {"id": "1804.03867", "submitter": "Rohit Gajawada", "authors": "Ameya Prabhu, Vishal Batchu, Rohit Gajawada, Sri Aurobindo Munagala,\n  Anoop Namboodiri", "title": "Hybrid Binary Networks: Optimizing for Accuracy, Efficiency and Memory", "comments": "Accepted in WACV'18 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Binarization is an extreme network compression approach that provides large\ncomputational speedups along with energy and memory savings, albeit at\nsignificant accuracy costs. We investigate the question of where to binarize\ninputs at layer-level granularity and show that selectively binarizing the\ninputs to specific layers in the network could lead to significant improvements\nin accuracy while preserving most of the advantages of binarization. We analyze\nthe binarization tradeoff using a metric that jointly models the input\nbinarization-error and computational cost and introduce an efficient algorithm\nto select layers whose inputs are to be binarized. Practical guidelines based\non insights obtained from applying the algorithm to a variety of models are\ndiscussed. Experiments on Imagenet dataset using AlexNet and ResNet-18 models\nshow 3-4% improvements in accuracy over fully binarized networks with minimal\nimpact on compression and computational speed. The improvements are even more\nsubstantial on sketch datasets like TU-Berlin, where we match state-of-the-art\naccuracy as well, getting over 8% increase in accuracies. We further show that\nour approach can be applied in tandem with other forms of compression that deal\nwith individual layers or overall model compression (e.g., SqueezeNets). Unlike\nprevious quantization approaches, we are able to binarize the weights in the\nlast layers of a network, which often have a large number of parameters,\nresulting in significant improvement in accuracy over fully binarized models.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 08:27:49 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Prabhu", "Ameya", ""], ["Batchu", "Vishal", ""], ["Gajawada", "Rohit", ""], ["Munagala", "Sri Aurobindo", ""], ["Namboodiri", "Anoop", ""]]}, {"id": "1804.03904", "submitter": "Nils Gessert", "authors": "Nils Gessert and Markus Heyder and Sarah Latus and Matthias Lutz and\n  Alexander Schlaefer", "title": "Plaque Classification in Coronary Arteries from IVOCT Images Using\n  Convolutional Neural Networks and Transfer Learning", "comments": "Submitted to CARS 2018, accepted for publication", "journal-ref": null, "doi": "10.1007/s11548-018-1766-y", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced atherosclerosis in the coronary arteries is one of the leading\ncauses of deaths worldwide while being preventable and treatable. In order to\nimage atherosclerotic lesions (plaque), intravascular optical coherence\ntomography (IVOCT) can be used. The technique provides high-resolution images\nof arterial walls which allows for early plaque detection by experts. Due to\nthe vast amount of IVOCT images acquired in clinical routines, automatic plaque\ndetection has been addressed. For example, attenuation profiles in single\nA-Scans of IVOCT images are examined to detect plaque. We address automatic\nplaque classification from entire IVOCT images, the cross-sectional view of the\nartery, using deep feature learning. In this way, we take context between\nA-Scans into account and we directly learn relevant features from the image\nsource without the need for handcrafting features.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 09:50:58 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Gessert", "Nils", ""], ["Heyder", "Markus", ""], ["Latus", "Sarah", ""], ["Lutz", "Matthias", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1804.03905", "submitter": "Hakan Karaoguz", "authors": "Hakan Karaoguz and Patric Jensfelt", "title": "Fusing Saliency Maps with Region Proposals for Unsupervised Object\n  Localization", "comments": "7 pages, 4 figures, 6 tables, submitted to IEEE IROS2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of unsupervised localization of objects\nin single images. Compared to previous state-of-the-art method our method is\nfully unsupervised in the sense that there is no prior instance level or\ncategory level information about the image. Furthermore, we treat each image\nindividually and do not rely on any neighboring image similarity. We employ\ndeep-learning based generation of saliency maps and region proposals to tackle\nthis problem. First salient regions in the image are determined using an\nencoder/decoder architecture. The resulting saliency map is matched with region\nproposals from a class agnostic region proposal network to roughly localize the\ncandidate object regions. These regions are further refined based on the\noverlap and similarity ratios. Our experimental evaluations on a benchmark\ndataset show that the method gets close to current state-of-the-art methods in\nterms of localization accuracy even though these make use of multiple frames.\nFurthermore, we created a more challenging and realistic dataset with multiple\nobject categories and varying viewpoint and illumination conditions for\nevaluating the method's performance in real world scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 09:51:08 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Karaoguz", "Hakan", ""], ["Jensfelt", "Patric", ""]]}, {"id": "1804.03928", "submitter": "Rohan Pattnaik", "authors": "Rajat Kumar Sinha, Ruchi Pandey, Rohan Pattnaik", "title": "Deep Learning For Computer Vision Tasks: A review", "comments": "Accepted in 2017 International Conference on Intelligent Computing\n  and Control (I2C2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently become one of the most popular sub-fields of\nmachine learning owing to its distributed data representation with multiple\nlevels of abstraction. A diverse range of deep learning algorithms are being\nemployed to solve conventional artificial intelligence problems. This paper\ngives an overview of some of the most widely used deep learning algorithms\napplied in the field of computer vision. It first inspects the various\napproaches of deep learning algorithms, followed by a description of their\napplications in image classification, object identification, image extraction\nand semantic segmentation in the presence of noise. The paper concludes with\nthe discussion of the future scope and challenges for construction and training\nof deep neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 11:13:35 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Sinha", "Rajat Kumar", ""], ["Pandey", "Ruchi", ""], ["Pattnaik", "Rohan", ""]]}, {"id": "1804.03933", "submitter": "Daniel Stumper", "authors": "Daniel Stumper, Fabian Gies, Stefan Hoermann, and Klaus Dietmayer", "title": "Offline Object Extraction from Dynamic Occupancy Grid Map Sequences", "comments": "8 Pages, 7 Figures, submitted to IEEE IV2018, waiting for acceptance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamic occupancy grid map (DOGMa) allows a fast, robust, and complete\nenvironment representation for automated vehicles. Dynamic objects in a DOGMa,\nhowever, are commonly represented as independent cells while modeled objects\nwith shape and pose are favorable. The evaluation of algorithms for object\nextraction or the training and validation of learning algorithms rely on\nlabeled ground truth data. Manually annotating objects in a DOGMa to obtain\nground truth data is a time consuming and expensive process. Additionally the\nquality of labeled data depend strongly on the variation of filtered input\ndata. The presented work introduces an automatic labeling process, where a full\nsequence is used to extract the best possible object pose and shape in terms of\ntemporal consistency. A two direction temporal search is executed to trace\nsingle objects over a sequence, where the best estimate of its extent and pose\nis refined in every time step. Furthermore, the presented algorithm only uses\nstatistical constraints of the cell clusters for the object extraction instead\nof fixed heuristic parameters. Experimental results show a well-performing\nautomatic labeling algorithm with real sensor data even at challenging\nscenarios.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 11:26:00 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Stumper", "Daniel", ""], ["Gies", "Fabian", ""], ["Hoermann", "Stefan", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1804.03939", "submitter": "Yong Man Ro", "authors": "Hak Gu Kim, Wissam J. Baddar, Heoun-taek Lim, Hyunwook Jeong, Yong Man\n  Ro", "title": "Measurement of exceptional motion in VR video contents for VR sickness\n  assessment using deep convolutional autoencoder", "comments": "In Proceedings of the 23rd ACM Symposium on Virtual Reality Software\n  and Technology (VRST 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new objective metric of exceptional motion in VR video\ncontents for VR sickness assessment. In VR environment, VR sickness can be\ncaused by several factors which are mismatched motion, field of view, motion\nparallax, viewing angle, etc. Similar to motion sickness, VR sickness can\ninduce a lot of physical symptoms such as general discomfort, headache, stomach\nawareness, nausea, vomiting, fatigue, and disorientation. To address the\nviewing safety issues in virtual environment, it is of great importance to\ndevelop an objective VR sickness assessment method that predicts and analyses\nthe degree of VR sickness induced by the VR content. The proposed method takes\ninto account motion information that is one of the most important factors in\ndetermining the overall degree of VR sickness. In this paper, we detect the\nexceptional motion that is likely to induce VR sickness. Spatio-temporal\nfeatures of the exceptional motion in the VR video content are encoded using a\nconvolutional autoencoder. For objectively assessing the VR sickness, the level\nof exceptional motion in VR video content is measured by using the\nconvolutional autoencoder as well. The effectiveness of the proposed method has\nbeen successfully evaluated by subjective assessment experiment using simulator\nsickness questionnaires (SSQ) in VR environment.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 11:41:47 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Kim", "Hak Gu", ""], ["Baddar", "Wissam J.", ""], ["Lim", "Heoun-taek", ""], ["Jeong", "Hyunwook", ""], ["Ro", "Yong Man", ""]]}, {"id": "1804.03943", "submitter": "Yong Man Ro", "authors": "Heoun-taek Lim, Hak Gu Kim, Yong Man Ro", "title": "VR IQA NET: Deep Virtual Reality Image Quality Assessment using\n  Adversarial Learning", "comments": "To appear at IEEE ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel virtual reality image quality assessment\n(VR IQA) with adversarial learning for omnidirectional images. To take into\naccount the characteristics of the omnidirectional image, we devise deep\nnetworks including novel quality score predictor and human perception guider.\nThe proposed quality score predictor automatically predicts the quality score\nof distorted image using the latent spatial and position feature. The proposed\nhuman perception guider criticizes the predicted quality score of the predictor\nwith the human perceptual score using adversarial learning. For evaluation, we\nconducted extensive subjective experiments with omnidirectional image dataset.\nExperimental results show that the proposed VR IQA metric outperforms the 2-D\nIQA and the state-of-the-arts VR IQA.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 11:45:56 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Lim", "Heoun-taek", ""], ["Kim", "Hak Gu", ""], ["Ro", "Yong Man", ""]]}, {"id": "1804.03955", "submitter": "Bernhard Stimpel", "authors": "Bernhard Stimpel, Christopher Syben, Tobias W\\\"urfl, Katharina\n  Breininger, Katrin Mentl, Jonathan M. Lommen, Arnd D\\\"orfler, Andreas Maier", "title": "Projection image-to-image translation in hybrid X-ray/MR imaging", "comments": "In proceedings of SPIE Medical Imaging 2019", "journal-ref": null, "doi": "10.1117/12.2512195", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential benefit of hybrid X-ray and MR imaging in the interventional\nenvironment is large due to the combination of fast imaging with high contrast\nvariety. However, a vast amount of existing image enhancement methods requires\nthe image information of both modalities to be present in the same domain. To\nunlock this potential, we present a solution to image-to-image translation from\nMR projections to corresponding X-ray projection images. The approach is based\non a state-of-the-art image generator network that is modified to fit the\nspecific application. Furthermore, we propose the inclusion of a gradient map\nin the loss function to allow the network to emphasize high-frequency details\nin image generation. Our approach is capable of creating X-ray projection\nimages with natural appearance. Additionally, our extensions show clear\nimprovement compared to the baseline method.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 12:23:03 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 14:40:43 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Stimpel", "Bernhard", ""], ["Syben", "Christopher", ""], ["W\u00fcrfl", "Tobias", ""], ["Breininger", "Katharina", ""], ["Mentl", "Katrin", ""], ["Lommen", "Jonathan M.", ""], ["D\u00f6rfler", "Arnd", ""], ["Maier", "Andreas", ""]]}, {"id": "1804.03959", "submitter": "Markus Oberweger", "authors": "Markus Oberweger and Mahdi Rad and Vincent Lepetit", "title": "Making Deep Heatmaps Robust to Partial Occlusions for 3D Object Pose\n  Estimation", "comments": null, "journal-ref": "Proc. of ECCV 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel method for robust and accurate 3D object pose estimation\nfrom a single color image under large occlusions. Following recent approaches,\nwe first predict the 2D projections of 3D points related to the target object\nand then compute the 3D pose from these correspondences using a geometric\nmethod. Unfortunately, as the results of our experiments show, predicting these\n2D projections using a regular CNN or a Convolutional Pose Machine is highly\nsensitive to partial occlusions, even when these methods are trained with\npartially occluded examples. Our solution is to predict heatmaps from multiple\nsmall patches independently and to accumulate the results to obtain accurate\nand robust predictions. Training subsequently becomes challenging because\npatches with similar appearances but different positions on the object\ncorrespond to different heatmaps. However, we provide a simple yet effective\nsolution to deal with such ambiguities. We show that our approach outperforms\nexisting methods on two challenging datasets: The Occluded LineMOD dataset and\nthe YCB-Video dataset, both exhibiting cluttered scenes with highly occluded\nobjects. Project website:\nhttps://www.tugraz.at/institute/icg/research/team-lepetit/research-projects/robust-object-pose-estimation/\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 12:39:19 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 15:19:13 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 06:38:54 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Oberweger", "Markus", ""], ["Rad", "Mahdi", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1804.03977", "submitter": "Silvia Biasotti", "authors": "Elia Moscoso Thompson and Silvia Biasotti", "title": "Edge-based LBP description of surfaces with colorimetric patterns", "comments": "Eurographics Workshop on 3D Object Retrieval 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we target the problem of the retrieval of colour patterns over\nsurfaces. We generalize to surface tessellations the well known Local Binary\nPattern (LBP) descriptor for images. The key concept of the LBP is to code the\nvariability of the colour values around each pixel. In the case of a surface\ntessellation we adopt rings around vertices that are obtained with a\nsphere-mesh intersection driven by the edges of the mesh; for this reason, we\nname our method edgeLBP. Experimental results are provided to show how this\ndescription performs well for pattern retrieval, also when patterns come from\ndegraded and corrupted archaeological fragments.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 13:34:31 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Thompson", "Elia Moscoso", ""], ["Biasotti", "Silvia", ""]]}, {"id": "1804.03999", "submitter": "Ozan Oktay Dr", "authors": "Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew Lee, Mattias\n  Heinrich, Kazunari Misawa, Kensaku Mori, Steven McDonagh, Nils Y Hammerla,\n  Bernhard Kainz, Ben Glocker, Daniel Rueckert", "title": "Attention U-Net: Learning Where to Look for the Pancreas", "comments": "Accepted to published in MIDL'18 (Revised Version) / OpenReview link:\n  https://openreview.net/forum?id=Skft7cijM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel attention gate (AG) model for medical imaging that\nautomatically learns to focus on target structures of varying shapes and sizes.\nModels trained with AGs implicitly learn to suppress irrelevant regions in an\ninput image while highlighting salient features useful for a specific task.\nThis enables us to eliminate the necessity of using explicit external\ntissue/organ localisation modules of cascaded convolutional neural networks\n(CNNs). AGs can be easily integrated into standard CNN architectures such as\nthe U-Net model with minimal computational overhead while increasing the model\nsensitivity and prediction accuracy. The proposed Attention U-Net architecture\nis evaluated on two large CT abdominal datasets for multi-class image\nsegmentation. Experimental results show that AGs consistently improve the\nprediction performance of U-Net across different datasets and training sizes\nwhile preserving computational efficiency. The code for the proposed\narchitecture is publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 14:13:03 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 09:44:19 GMT"}, {"version": "v3", "created": "Sun, 20 May 2018 23:33:30 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Oktay", "Ozan", ""], ["Schlemper", "Jo", ""], ["Folgoc", "Loic Le", ""], ["Lee", "Matthew", ""], ["Heinrich", "Mattias", ""], ["Misawa", "Kazunari", ""], ["Mori", "Kensaku", ""], ["McDonagh", "Steven", ""], ["Hammerla", "Nils Y", ""], ["Kainz", "Bernhard", ""], ["Glocker", "Ben", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1804.04020", "submitter": "Keiller Nogueira", "authors": "Keiller Nogueira, Mauro Dalla Mura, Jocelyn Chanussot, William R.\n  Schwartz, Jefersson A. dos Santos", "title": "Dynamic Multi-Context Segmentation of Remote Sensing Images based on\n  Convolutional Networks", "comments": "Accepted to Transactions on Geoscience & Remote Sensing (TGRS)", "journal-ref": null, "doi": "10.1109/TGRS.2019.2913861", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation requires methods capable of learning high-level\nfeatures while dealing with large volume of data. Towards such goal,\nConvolutional Networks can learn specific and adaptable features based on the\ndata. However, these networks are not capable of processing a whole remote\nsensing image, given its huge size. To overcome such limitation, the image is\nprocessed using fixed size patches. The definition of the input patch size is\nusually performed empirically (evaluating several sizes) or imposed (by network\nconstraint). Both strategies suffer from drawbacks and could not lead to the\nbest patch size. To alleviate this problem, several works exploited\nmulti-context information by combining networks or layers. This process\nincreases the number of parameters resulting in a more difficult model to\ntrain. In this work, we propose a novel technique to perform semantic\nsegmentation of remote sensing images that exploits a multi-context paradigm\nwithout increasing the number of parameters while defining, in training time,\nthe best patch size. The main idea is to train a dilated network with distinct\npatch sizes, allowing it to capture multi-context characteristics from\nheterogeneous contexts. While processing these varying patches, the network\nprovides a score for each patch size, helping in the definition of the best\nsize for the current scenario. A systematic evaluation of the proposed\nalgorithm is conducted using four high-resolution remote sensing datasets with\nvery distinct properties. Our results show that the proposed algorithm provides\nimprovements in pixelwise classification accuracy when compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 14:32:15 GMT"}, {"version": "v2", "created": "Sun, 24 Jun 2018 15:37:22 GMT"}, {"version": "v3", "created": "Tue, 23 Apr 2019 02:01:11 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Nogueira", "Keiller", ""], ["Mura", "Mauro Dalla", ""], ["Chanussot", "Jocelyn", ""], ["Schwartz", "William R.", ""], ["Santos", "Jefersson A. dos", ""]]}, {"id": "1804.04065", "submitter": "Givi Meishvili", "authors": "Meiguang Jin, Givi Meishvili, Paolo Favaro", "title": "Learning to Extract a Video Sequence from a Single Motion-Blurred Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to extract a video sequence from a single motion-blurred\nimage. Motion-blurred images are the result of an averaging process, where\ninstant frames are accumulated over time during the exposure of the sensor.\nUnfortunately, reversing this process is nontrivial. Firstly, averaging\ndestroys the temporal ordering of the frames. Secondly, the recovery of a\nsingle frame is a blind deconvolution task, which is highly ill-posed. We\npresent a deep learning scheme that gradually reconstructs a temporal ordering\nby sequentially extracting pairs of frames. Our main contribution is to\nintroduce loss functions invariant to the temporal order. This lets a neural\nnetwork choose during training what frame to output among the possible\ncombinations. We also address the ill-posedness of deblurring by designing a\nnetwork with a large receptive field and implemented via resampling to achieve\na higher computational efficiency. Our proposed method can successfully\nretrieve sharp image sequences from a single motion blurred image and can\ngeneralize well on synthetic and real datasets captured with different cameras.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 16:01:26 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Jin", "Meiguang", ""], ["Meishvili", "Givi", ""], ["Favaro", "Paolo", ""]]}, {"id": "1804.04071", "submitter": "James Kapaldo", "authors": "James Kapaldo, Xu Han, Domingo Mery", "title": "Seed-Point Detection of Clumped Convex Objects by Short-Range Attractive\n  Long-Range Repulsive Particle Clustering", "comments": "10 pages, 8 figures, with supplemental notes and videos. Submitted to\n  be published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locating the center of convex objects is important in both image processing\nand unsupervised machine learning/data clustering fields. The automated\nanalysis of biological images uses both of these fields for locating cell\nnuclei and for discovering new biological effects or cell phenotypes. In this\nwork, we develop a novel clustering method for locating the centers of\noverlapping convex objects by modeling particles that interact by a short-range\nattractive and long-range repulsive potential and are confined to a potential\nwell created from the data. We apply this method to locating the centers of\nclumped nuclei in cultured cells, where we show that it results in a\nsignificant improvement over existing methods (8.2% in F$_1$ score); and we\napply it to unsupervised learning on a difficult data set that has rare classes\nwithout local density maxima, and show it is able to well locate cluster\ncenters when other clustering techniques fail.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 16:18:16 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Kapaldo", "James", ""], ["Han", "Xu", ""], ["Mery", "Domingo", ""]]}, {"id": "1804.04076", "submitter": "Faraz Saeedan", "authors": "Faraz Saeedan, Nicolas Weber, Michael Goesele, Stefan Roth", "title": "Detail-Preserving Pooling in Deep Networks", "comments": "To appear at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most convolutional neural networks use some method for gradually downscaling\nthe size of the hidden layers. This is commonly referred to as pooling, and is\napplied to reduce the number of parameters, improve invariance to certain\ndistortions, and increase the receptive field size. Since pooling by nature is\na lossy process, it is crucial that each such layer maintains the portion of\nthe activations that is most important for the network's discriminability. Yet,\nsimple maximization or averaging over blocks, max or average pooling, or plain\ndownsampling in the form of strided convolutions are the standard. In this\npaper, we aim to leverage recent results on image downscaling for the purposes\nof deep learning. Inspired by the human visual system, which focuses on local\nspatial changes, we propose detail-preserving pooling (DPP), an adaptive\npooling method that magnifies spatial changes and preserves important\nstructural detail. Importantly, its parameters can be learned jointly with the\nrest of the network. We analyze some of its theoretical properties and show its\nempirical benefits on several datasets and networks, where DPP consistently\noutperforms previous pooling approaches.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 16:28:11 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Saeedan", "Faraz", ""], ["Weber", "Nicolas", ""], ["Goesele", "Michael", ""], ["Roth", "Stefan", ""]]}, {"id": "1804.04082", "submitter": "Yassir Saquil", "authors": "Yassir Saquil, Kwang In Kim, Peter Hall", "title": "Ranking CGANs: Subjective Control over Semantic Image Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the use of generative adversarial networks in\nthe task of image generation according to subjective measures of semantic\nattributes. Unlike the standard (CGAN) that generates images from discrete\ncategorical labels, our architecture handles both continuous and discrete\nscales. Given pairwise comparisons of images, our model, called RankCGAN,\nperforms two tasks: it learns to rank images using a subjective measure; and it\nlearns a generative model that can be controlled by that measure. RankCGAN\nassociates each subjective measure of interest to a distinct dimension of some\nlatent space. We perform experiments on UT-Zap50K, PubFig and OSR datasets and\ndemonstrate that the model is expressive and diverse enough to conduct\ntwo-attribute exploration and image editing.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 16:40:42 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 16:47:57 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 11:50:12 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Saquil", "Yassir", ""], ["Kim", "Kwang In", ""], ["Hall", "Peter", ""]]}, {"id": "1804.04112", "submitter": "Jo\\~ao Gante", "authors": "Jo\\~ao Gante, Gabriel Falc\\~ao, and Leonel Sousa", "title": "Beamformed Fingerprint Learning for Accurate Millimeter Wave Positioning", "comments": "5 pages, 7 figures. Submitted to VTC2018-Fall (Chicago)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With millimeter wave wireless communications, the resulting radiation\nreflects on most visible objects, creating rich multipath environments, namely\nin urban scenarios. The radiation captured by a listening device is thus shaped\nby the obstacles encountered, which carry latent information regarding their\nrelative positions. In this paper, a system to convert the received millimeter\nwave radiation into the device's position is proposed, making use of the\naforementioned hidden information. Using deep learning techniques and a\npre-established codebook of beamforming patterns transmitted by a base station,\nthe simulations show that average estimation errors below 10 meters are\nachievable in realistic outdoors scenarios that contain mostly\nnon-line-of-sight positions, paving the way for new positioning systems.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 17:36:30 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Gante", "Jo\u00e3o", ""], ["Falc\u00e3o", "Gabriel", ""], ["Sousa", "Leonel", ""]]}, {"id": "1804.04118", "submitter": "Eshed Ohn-Bar", "authors": "Eshed Ohn-Bar and Kris Kitani and Chieko Asakawa", "title": "Personalized Dynamics Models for Adaptive Assistive Navigation Systems", "comments": "Oral Presentation in 2nd Conference on Robot Learning (CoRL, 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an assistive system that guides visually impaired users through\nspeech and haptic feedback to their destination. Existing robotic and\nubiquitous navigation technologies (e.g., portable, ground, or wearable\nsystems) often operate in a generic, user-agnostic manner. However, to minimize\nconfusion and navigation errors, our real-world analysis reveals a crucial need\nto adapt the instructional guidance across different end-users with diverse\nmobility skills. To address this practical issue in scalable system design, we\npropose a novel model-based reinforcement learning framework for personalizing\nthe system-user interaction experience. When incrementally adapting the system\nto new users, we propose to use a weighted experts model for addressing\ndata-efficiency limitations in transfer learning with deep models. A real-world\ndataset of navigation by blind users is used to show that the proposed approach\nallows for (1) more accurate long-term human behavior prediction (up to 20\nseconds into the future) through improved reasoning over personal mobility\ncharacteristics, interaction with surrounding obstacles, and the current\nnavigation goal, and (2) quick adaptation at the onset of learning, when data\nis limited.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 17:55:00 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 12:20:33 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Ohn-Bar", "Eshed", ""], ["Kitani", "Kris", ""], ["Asakawa", "Chieko", ""]]}, {"id": "1804.04121", "submitter": "Joon Son Chung", "authors": "Triantafyllos Afouras, Joon Son Chung, Andrew Zisserman", "title": "The Conversation: Deep Audio-Visual Speech Enhancement", "comments": "To appear in Interspeech 2018. We provide supplementary material with\n  interactive demonstrations on\n  http://www.robots.ox.ac.uk/~vgg/demo/theconversation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to isolate individual speakers from multi-talker simultaneous\nspeech in videos. Existing works in this area have focussed on trying to\nseparate utterances from known speakers in controlled environments. In this\npaper, we propose a deep audio-visual speech enhancement network that is able\nto separate a speaker's voice given lip regions in the corresponding video, by\npredicting both the magnitude and the phase of the target signal. The method is\napplicable to speakers unheard and unseen during training, and for\nunconstrained environments. We demonstrate strong quantitative and qualitative\nresults, isolating extremely challenging real-world examples.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 17:57:28 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 17:51:32 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Afouras", "Triantafyllos", ""], ["Chung", "Joon Son", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1804.04128", "submitter": "Seungjoo Yoo", "authors": "Hyojin Bahng, Seungjoo Yoo, Wonwoong Cho, David K. Park, Ziming Wu,\n  Xiaojuan Ma, Jaegul Choo", "title": "Coloring with Words: Guiding Image Colorization Through Text-based\n  Palette Generation", "comments": "25 pages, 22 figures", "journal-ref": "ECCV 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a novel approach to generate multiple color palettes that\nreflect the semantics of input text and then colorize a given grayscale image\naccording to the generated color palette. In contrast to existing approaches,\nour model can understand rich text, whether it is a single word, a phrase, or a\nsentence, and generate multiple possible palettes from it. For this task, we\nintroduce our manually curated dataset called Palette-and-Text (PAT). Our\nproposed model called Text2Colors consists of two conditional generative\nadversarial networks: the text-to-palette generation networks and the\npalette-based colorization networks. The former captures the semantics of the\ntext input and produce relevant color palettes. The latter colorizes a\ngrayscale image using the generated color palette. Our evaluation results show\nthat people preferred our generated palettes over ground truth palettes and\nthat our model can effectively reflect the given palette when colorizing an\nimage.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 15:16:14 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 06:40:18 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Bahng", "Hyojin", ""], ["Yoo", "Seungjoo", ""], ["Cho", "Wonwoong", ""], ["Park", "David K.", ""], ["Wu", "Ziming", ""], ["Ma", "Xiaojuan", ""], ["Choo", "Jaegul", ""]]}, {"id": "1804.04192", "submitter": "Naifan Zhuang", "authors": "Naifan Zhuang, The Duc Kieu, Guo-Jun Qi, Kien A. Hua", "title": "Deep Differential Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the special gating schemes of Long Short-Term Memory (LSTM), LSTMs\nhave shown greater potential to process complex sequential information than the\ntraditional Recurrent Neural Network (RNN). The conventional LSTM, however,\nfails to take into consideration the impact of salient spatio-temporal dynamics\npresent in the sequential input data. This problem was first addressed by the\ndifferential Recurrent Neural Network (dRNN), which uses a differential gating\nscheme known as Derivative of States (DoS). DoS uses higher orders of internal\nstate derivatives to analyze the change in information gain caused by the\nsalient motions between the successive frames. The weighted combination of\nseveral orders of DoS is then used to modulate the gates in dRNN. While each\nindividual order of DoS is good at modeling a certain level of salient\nspatio-temporal sequences, the sum of all the orders of DoS could distort the\ndetected motion patterns. To address this problem, we propose to control the\nLSTM gates via individual orders of DoS and stack multiple levels of LSTM cells\nin an increasing order of state derivatives. The proposed model progressively\nbuilds up the ability of the LSTM gates to detect salient dynamical patterns in\ndeeper stacked layers modeling higher orders of DoS, and thus the proposed LSTM\nmodel is termed deep differential Recurrent Neural Network (d2RNN). The\neffectiveness of the proposed model is demonstrated on two publicly available\nhuman activity datasets: NUS-HGA and Violent-Flows. The proposed model\noutperforms both LSTM and non-LSTM based state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 20:02:25 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Zhuang", "Naifan", ""], ["Kieu", "The Duc", ""], ["Qi", "Guo-Jun", ""], ["Hua", "Kien A.", ""]]}, {"id": "1804.04206", "submitter": "Boheng Zhang", "authors": "Boheng Zhang, Shenglei Huang, Shaohan Hu", "title": "Multi-scale Neural Networks for Retinal Blood Vessels Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing supervised approaches didn't make use of the low-level features\nwhich are actually effective to this task. And another deficiency is that they\ndidn't consider the relation between pixels, which means effective features are\nnot extracted. In this paper, we proposed a novel convolutional neural network\nwhich make sufficient use of low-level features together with high-level\nfeatures and involves atrous convolution to get multi-scale features which\nshould be considered as effective features. Our model is tested on three\nstandard benchmarks - DRIVE, STARE, and CHASE databases. The results presents\nthat our model significantly outperforms existing approaches in terms of\naccuracy, sensitivity, specificity, the area under the ROC curve and the\nhighest prediction speed. Our work provides evidence of the power of wide and\ndeep neural networks in retinal blood vessels segmentation task which could be\napplied on other medical images tasks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 20:25:36 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Zhang", "Boheng", ""], ["Huang", "Shenglei", ""], ["Hu", "Shaohan", ""]]}, {"id": "1804.04213", "submitter": "Hao Zhu", "authors": "Hao Zhu, Hao Su, Peng Wang, Xun Cao, Ruigang Yang", "title": "View Extrapolation of Human Body from a Single Image", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to synthesize novel views of human body from a single image.\nThough recent deep learning based methods work well for rigid objects, they\noften fail on objects with large articulation, like human bodies. The core step\nof existing methods is to fit a map from the observable views to novel views by\nCNNs; however, the rich articulation modes of human body make it rather\nchallenging for CNNs to memorize and interpolate the data well. To address the\nproblem, we propose a novel deep learning based pipeline that explicitly\nestimates and leverages the geometry of the underlying human body. Our new\npipeline is a composition of a shape estimation network and an image generation\nnetwork, and at the interface a perspective transformation is applied to\ngenerate a forward flow for pixel value transportation. Our design is able to\nfactor out the space of data variation and makes learning at each step much\neasier. Empirically, we show that the performance for pose-varying objects can\nbe improved dramatically. Our method can also be applied on real data captured\nby 3D sensors, and the flow generated by our methods can be used for generating\nhigh quality results in higher resolution.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 20:41:19 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Zhu", "Hao", ""], ["Su", "Hao", ""], ["Wang", "Peng", ""], ["Cao", "Xun", ""], ["Yang", "Ruigang", ""]]}, {"id": "1804.04221", "submitter": "Emanuel A. Lazar", "authors": "Emanuel A. Lazar", "title": "VoroTop: Voronoi Cell Topology Visualization and Analysis Toolkit", "comments": "17 pages, 10 figures", "journal-ref": "Model. Simul. Mater. Sci. Eng 26:1 (2017)", "doi": "10.1088/1361-651X/aa9a01", "report-no": null, "categories": "cond-mat.mtrl-sci cs.CV physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new open-source software program called VoroTop,\nwhich uses Voronoi topology to analyze local structure in atomic systems.\nStrengths of this approach include its abilities to analyze high-temperature\nsystems and to characterize complex structure such as grain boundaries. This\napproach enables the automated analysis of systems and mechanisms previously\nnot possible.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 21:02:58 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Lazar", "Emanuel A.", ""]]}, {"id": "1804.04241", "submitter": "Rodney LaLonde Iii", "authors": "Rodney LaLonde and Ulas Bagci", "title": "Capsules for Object Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have shown remarkable results over the\nlast several years for a wide range of computer vision tasks. A new\narchitecture recently introduced by Sabour et al., referred to as a capsule\nnetworks with dynamic routing, has shown great initial results for digit\nrecognition and small image classification. The success of capsule networks\nlies in their ability to preserve more information about the input by replacing\nmax-pooling layers with convolutional strides and dynamic routing, allowing for\npreservation of part-whole relationships in the data. This preservation of the\ninput is demonstrated by reconstructing the input from the output capsule\nvectors. Our work expands the use of capsule networks to the task of object\nsegmentation for the first time in the literature. We extend the idea of\nconvolutional capsules with locally-connected routing and propose the concept\nof deconvolutional capsules. Further, we extend the masked reconstruction to\nreconstruct the positive input class. The proposed\nconvolutional-deconvolutional capsule network, called SegCaps, shows strong\nresults for the task of object segmentation with substantial decrease in\nparameter space. As an example application, we applied the proposed SegCaps to\nsegment pathological lungs from low dose CT scans and compared its accuracy and\nefficiency with other U-Net-based architectures. SegCaps is able to handle\nlarge image sizes (512 x 512) as opposed to baseline capsules (typically less\nthan 32 x 32). The proposed SegCaps reduced the number of parameters of U-Net\narchitecture by 95.4% while still providing a better segmentation accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 21:57:57 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["LaLonde", "Rodney", ""], ["Bagci", "Ulas", ""]]}, {"id": "1804.04259", "submitter": "Zhaoyang Lv", "authors": "Zhaoyang Lv, Kihwan Kim, Alejandro Troccoli, Deqing Sun, James M.\n  Rehg, Jan Kautz", "title": "Learning Rigidity in Dynamic Scenes with a Moving Camera for 3D Motion\n  Field Estimation", "comments": "This work is accepted at European Conference on Computer Vision 2018.\n  Project page (with the video):\n  http://research.nvidia.com/publication/2018-09_Learning-Rigidity-in The codes\n  will be released at https://github.com/NVlabs/learningrigidity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of 3D motion in a dynamic scene from a temporal pair of images is\na core task in many scene understanding problems. In real world applications, a\ndynamic scene is commonly captured by a moving camera (i.e., panning, tilting\nor hand-held), increasing the task complexity because the scene is observed\nfrom different view points. The main challenge is the disambiguation of the\ncamera motion from scene motion, which becomes more difficult as the amount of\nrigidity observed decreases, even with successful estimation of 2D image\ncorrespondences. Compared to other state-of-the-art 3D scene flow estimation\nmethods, in this paper we propose to \\emph{learn} the rigidity of a scene in a\nsupervised manner from a large collection of dynamic scene data, and directly\ninfer a rigidity mask from two sequential images with depths. With the learned\nnetwork, we show how we can effectively estimate camera motion and projected\nscene flow using computed 2D optical flow and the inferred rigidity mask. For\ntraining and testing the rigidity network, we also provide a new semi-synthetic\ndynamic scene dataset (synthetic foreground objects with a real background) and\nan evaluation split that accounts for the percentage of observed non-rigid\npixels. Through our evaluation we show the proposed framework outperforms\ncurrent state-of-the-art scene flow estimation methods in challenging dynamic\nscenes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 00:01:43 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 08:11:45 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Lv", "Zhaoyang", ""], ["Kim", "Kihwan", ""], ["Troccoli", "Alejandro", ""], ["Sun", "Deqing", ""], ["Rehg", "James M.", ""], ["Kautz", "Jan", ""]]}, {"id": "1804.04273", "submitter": "Yibing Song", "authors": "Yibing Song, Chao Ma, Xiaohe Wu, Lijun Gong, Linchao Bao, Wangmeng\n  Zuo, Chunhua Shen, Rynson Lau, Ming-Hsuan Yang", "title": "VITAL: VIsual Tracking via Adversarial Learning", "comments": "Spotlight in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tracking-by-detection framework consists of two stages, i.e., drawing\nsamples around the target object in the first stage and classifying each sample\nas the target object or as background in the second stage. The performance of\nexisting trackers using deep classification networks is limited by two aspects.\nFirst, the positive samples in each frame are highly spatially overlapped, and\nthey fail to capture rich appearance variations. Second, there exists extreme\nclass imbalance between positive and negative samples. This paper presents the\nVITAL algorithm to address these two problems via adversarial learning. To\naugment positive samples, we use a generative network to randomly generate\nmasks, which are applied to adaptively dropout input features to capture a\nvariety of appearance changes. With the use of adversarial learning, our\nnetwork identifies the mask that maintains the most robust features of the\ntarget objects over a long temporal span. In addition, to handle the issue of\nclass imbalance, we propose a high-order cost sensitive loss to decrease the\neffect of easy negative samples to facilitate training the classification\nnetwork. Extensive experiments on benchmark datasets demonstrate that the\nproposed tracker performs favorably against state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 01:44:12 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Song", "Yibing", ""], ["Ma", "Chao", ""], ["Wu", "Xiaohe", ""], ["Gong", "Lijun", ""], ["Bao", "Linchao", ""], ["Zuo", "Wangmeng", ""], ["Shen", "Chunhua", ""], ["Lau", "Rynson", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1804.04312", "submitter": "Cheng-Hao Deng", "authors": "Cheng-Hao Deng, Wan-Lei Zhao", "title": "Clustering via Boundary Erosion", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering analysis identifies samples as groups based on either their mutual\ncloseness or homogeneity. In order to detect clusters in arbitrary shapes, a\nnovel and generic solution based on boundary erosion is proposed. The clusters\nare assumed to be separated by relatively sparse regions. The samples are\neroded sequentially according to their dynamic boundary densities. The erosion\nstarts from low density regions, invading inwards, until all the samples are\neroded out. By this manner, boundaries between different clusters become more\nand more apparent. It therefore offers a natural and powerful way to separate\nthe clusters when the boundaries between them are hard to be drawn at once.\nWith the sequential order of being eroded, the sequential boundary levels are\nproduced, following which the clusters in arbitrary shapes are automatically\nreconstructed. As demonstrated across various clustering tasks, it is able to\noutperform most of the state-of-the-art algorithms and its performance is\nnearly perfect in some scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 04:39:04 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 03:23:59 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Deng", "Cheng-Hao", ""], ["Zhao", "Wan-Lei", ""]]}, {"id": "1804.04314", "submitter": "Bo Zhao", "authors": "Bo Zhao, Yanwei Fu, Rui Liang, Jiahong Wu, Yonggang Wang, Yizhou Wang", "title": "A Large-scale Attribute Dataset for Zero-shot Learning", "comments": null, "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR) Workshops, 2019, pp. 0-0", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Learning (ZSL) has attracted huge research attention over the past\nfew years; it aims to learn the new concepts that have never been seen before.\nIn classical ZSL algorithms, attributes are introduced as the intermediate\nsemantic representation to realize the knowledge transfer from seen classes to\nunseen classes. Previous ZSL algorithms are tested on several benchmark\ndatasets annotated with attributes. However, these datasets are defective in\nterms of the image distribution and attribute diversity. In addition, we argue\nthat the \"co-occurrence bias problem\" of existing datasets, which is caused by\nthe biased co-occurrence of objects, significantly hinders models from\ncorrectly learning the concept. To overcome these problems, we propose a\nLarge-scale Attribute Dataset (LAD). Our dataset has 78,017 images of 5\nsuper-classes, 230 classes. The image number of LAD is larger than the sum of\nthe four most popular attribute datasets. 359 attributes of visual, semantic\nand subjective properties are defined and annotated in instance-level. We\nanalyze our dataset by conducting both supervised learning and zero-shot\nlearning tasks. Seven state-of-the-art ZSL algorithms are tested on this new\ndataset. The experimental results reveal the challenge of implementing\nzero-shot learning on our dataset.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 04:58:34 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 04:13:44 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhao", "Bo", ""], ["Fu", "Yanwei", ""], ["Liang", "Rui", ""], ["Wu", "Jiahong", ""], ["Wang", "Yonggang", ""], ["Wang", "Yizhou", ""]]}, {"id": "1804.04318", "submitter": "Yale Song", "authors": "Yale Song and Mohammad Soleymani", "title": "Cross-Modal Retrieval with Implicit Concept Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional cross-modal retrieval assumes explicit association of concepts\nacross modalities, where there is no ambiguity in how the concepts are linked\nto each other, e.g., when we do the image search with a query \"dogs\", we expect\nto see dog images. In this paper, we consider a different setting for\ncross-modal retrieval where data from different modalities are implicitly\nlinked via concepts that must be inferred by high-level reasoning; we call this\nsetting implicit concept association. To foster future research in this\nsetting, we present a new dataset containing 47K pairs of animated GIFs and\nsentences crawled from the web, in which the GIFs depict physical or emotional\nreactions to the scenarios described in the text (called \"reaction GIFs\"). We\nreport on a user study showing that, despite the presence of implicit concept\nassociation, humans are able to identify video-sentence pairs with matching\nconcepts, suggesting the feasibility of our task. Furthermore, we propose a\nnovel visual-semantic embedding network based on multiple instance learning.\nUnlike traditional approaches, we compute multiple embeddings from each\nmodality, each representing different concepts, and measure their similarity by\nconsidering all possible combinations of visual-semantic embeddings in the\nframework of multiple instance learning. We evaluate our approach on two\nvideo-sentence datasets with explicit and implicit concept association and\nreport competitive results compared to existing approaches on cross-modal\nretrieval.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 05:10:33 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 16:30:57 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Song", "Yale", ""], ["Soleymani", "Mohammad", ""]]}, {"id": "1804.04326", "submitter": "Yuya Yoshikawa", "authors": "Yuya Yoshikawa, Jiaqing Lin, Akikazu Takeuchi", "title": "STAIR Actions: A Video Dataset of Everyday Home Actions", "comments": "STAIR Actions dataset can be downloaded from\n  http://actions.stair.center", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new large-scale video dataset for human action recognition, called STAIR\nActions is introduced. STAIR Actions contains 100 categories of action labels\nrepresenting fine-grained everyday home actions so that it can be applied to\nresearch in various home tasks such as nursing, caring, and security. In STAIR\nActions, each video has a single action label. Moreover, for each action\ncategory, there are around 1,000 videos that were obtained from YouTube or\nproduced by crowdsource workers. The duration of each video is mostly five to\nsix seconds. The total number of videos is 102,462. We explain how we\nconstructed STAIR Actions and show the characteristics of STAIR Actions\ncompared to existing datasets for human action recognition. Experiments with\nthree major models for action recognition show that STAIR Actions can train\nlarge models and achieve good performance. STAIR Actions can be downloaded from\nhttp://actions.stair.center\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 05:48:06 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 03:26:54 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 05:40:42 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Yoshikawa", "Yuya", ""], ["Lin", "Jiaqing", ""], ["Takeuchi", "Akikazu", ""]]}, {"id": "1804.04338", "submitter": "Christoph Baur", "authors": "Christoph Baur, Shadi Albarqouni, Nassir Navab", "title": "MelanoGANs: High Resolution Skin Lesion Synthesis with GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have been successfully used to\nsynthesize realistically looking images of faces, scenery and even medical\nimages. Unfortunately, they usually require large training datasets, which are\noften scarce in the medical field, and to the best of our knowledge GANs have\nbeen only applied for medical image synthesis at fairly low resolution.\nHowever, many state-of-the-art machine learning models operate on high\nresolution data as such data carries indispensable, valuable information. In\nthis work, we try to generate realistically looking high resolution images of\nskin lesions with GANs, using only a small training dataset of 2000 samples.\nThe nature of the data allows us to do a direct comparison between the image\nstatistics of the generated samples and the real dataset. We both\nquantitatively and qualitatively compare state-of-the-art GAN architectures\nsuch as DCGAN and LAPGAN against a modification of the latter for the task of\nimage generation at a resolution of 256x256px. Our investigation shows that we\ncan approximate the real data distribution with all of the models, but we\nnotice major differences when visually rating sample realism, diversity and\nartifacts. In a set of use-case experiments on skin lesion classification, we\nfurther show that we can successfully tackle the problem of heavy class\nimbalance with the help of synthesized high resolution melanoma samples.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 06:18:31 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Baur", "Christoph", ""], ["Albarqouni", "Shadi", ""], ["Navab", "Nassir", ""]]}, {"id": "1804.04339", "submitter": "Naveed Akhtar Dr.", "authors": "ShiJie Sun, Naveed Akhtar, HuanSheng Song, ChaoYang Zhang, JianXin Li,\n  Ajmal Mian", "title": "Benchmark data and method for real-time people counting in cluttered\n  scenes using depth sensors", "comments": "Submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based automatic counting of people has widespread applications in\nintelligent transportation systems, security, and logistics. However, there is\ncurrently no large-scale public dataset for benchmarking approaches on this\nproblem. This work fills this gap by introducing the first real-world RGB-D\nPeople Counting DataSet (PCDS) containing over 4,500 videos recorded at the\nentrance doors of buses in normal and cluttered conditions. It also proposes an\nefficient method for counting people in real-world cluttered scenes related to\npublic transportations using depth videos. The proposed method computes a point\ncloud from the depth video frame and re-projects it onto the ground plane to\nnormalize the depth information. The resulting depth image is analyzed for\nidentifying potential human heads. The human head proposals are meticulously\nrefined using a 3D human model. The proposals in each frame of the continuous\nvideo stream are tracked to trace their trajectories. The trajectories are\nagain refined to ascertain reliable counting. People are eventually counted by\naccumulating the head trajectories leaving the scene. To enable effective head\nand trajectory identification, we also propose two different compound features.\nA thorough evaluation on PCDS demonstrates that our technique is able to count\npeople in cluttered scenes with high accuracy at 45 fps on a 1.7 GHz processor,\nand hence it can be deployed for effective real-time people counting for\nintelligent transportation systems.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 06:22:31 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 07:45:37 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Sun", "ShiJie", ""], ["Akhtar", "Naveed", ""], ["Song", "HuanSheng", ""], ["Zhang", "ChaoYang", ""], ["Li", "JianXin", ""], ["Mian", "Ajmal", ""]]}, {"id": "1804.04340", "submitter": "Ankan Bansal", "authors": "Ankan Bansal and Karan Sikka and Gaurav Sharma and Rama Chellappa and\n  Ajay Divakaran", "title": "Zero-Shot Object Detection", "comments": "17 pages. ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and tackle the problem of zero-shot object detection (ZSD),\nwhich aims to detect object classes which are not observed during training. We\nwork with a challenging set of object classes, not restricting ourselves to\nsimilar and/or fine-grained categories as in prior works on zero-shot\nclassification. We present a principled approach by first adapting\nvisual-semantic embeddings for ZSD. We then discuss the problems associated\nwith selecting a background class and motivate two background-aware approaches\nfor learning robust detectors. One of these models uses a fixed background\nclass and the other is based on iterative latent assignments. We also outline\nthe challenge associated with using a limited number of training classes and\npropose a solution based on dense sampling of the semantic label space using\nauxiliary data with a large number of categories. We propose novel splits of\ntwo standard detection datasets - MSCOCO and VisualGenome, and present\nextensive empirical results in both the traditional and generalized zero-shot\nsettings to highlight the benefits of the proposed methods. We provide useful\ninsights into the algorithm and conclude by posing some open questions to\nencourage further research.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 06:23:11 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 06:07:37 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Bansal", "Ankan", ""], ["Sikka", "Karan", ""], ["Sharma", "Gaurav", ""], ["Chellappa", "Rama", ""], ["Divakaran", "Ajay", ""]]}, {"id": "1804.04341", "submitter": "Chengjia Wang", "authors": "Chengjia Wang, Tom MacGillivray, Gillian Macnaught, Guang Yang and\n  David Newby", "title": "A two-stage 3D Unet framework for multi-class segmentation on full\n  resolution image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have been intensively used for\nmulti-class segmentation of data from different modalities and achieved\nstate-of-the-art performances. However, a common problem when dealing with\nlarge, high resolution 3D data is that the volumes input into the deep CNNs has\nto be either cropped or downsampled due to limited memory capacity of computing\ndevices. These operations lead to loss of resolution and increment of class\nimbalance in the input data batches, which can downgrade the performances of\nsegmentation algorithms. Inspired by the architecture of image super-resolution\nCNN (SRCNN) and self-normalization network (SNN), we developed a two-stage\nmodified Unet framework that simultaneously learns to detect a ROI within the\nfull volume and to classify voxels without losing the original resolution.\nExperiments on a variety of multi-modal volumes demonstrated that, when trained\nwith a simply weighted dice coefficients and our customized learning procedure,\nthis framework shows better segmentation performances than state-of-the-art\nDeep CNNs with advanced similarity metrics.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 06:31:58 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Wang", "Chengjia", ""], ["MacGillivray", "Tom", ""], ["Macnaught", "Gillian", ""], ["Yang", "Guang", ""], ["Newby", "David", ""]]}, {"id": "1804.04360", "submitter": "Majd Zreik", "authors": "Majd Zreik, Robbert W. van Hamersvelt, Jelmer M. Wolterink, Tim\n  Leiner, Max A. Viergever, Ivana Isgum", "title": "A Recurrent CNN for Automatic Detection and Classification of Coronary\n  Artery Plaque and Stenosis in Coronary CT Angiography", "comments": "Published in IEEE Transactions on Medical Imaging, 2019", "journal-ref": null, "doi": "10.1109/TMI.2018.2883807", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various types of atherosclerotic plaque and varying grades of stenosis could\nlead to different management of patients with coronary artery disease.\nTherefore, it is crucial to detect and classify the type of coronary artery\nplaque, as well as to detect and determine the degree of coronary artery\nstenosis. This study includes retrospectively collected clinically obtained\ncoronary CT angiography (CCTA) scans of 163 patients. To perform automatic\nanalysis for coronary artery plaque and stenosis classification, a multi-task\nrecurrent convolutional neural network is applied on multi-planar reformatted\n(MPR) images of the coronary arteries. First, a 3D convolutional neural network\nis utilized to extract features along the coronary artery. Subsequently, the\nextracted features are aggregated by a recurrent neural network that performs\ntwo simultaneous multi-class classification tasks. In the first task, the\nnetwork detects and characterizes the type of the coronary artery plaque (no\nplaque, non-calcified, mixed, calcified). In the second task, the network\ndetects and determines the anatomical significance of the coronary artery\nstenosis (no stenosis, non-significant i.e. <50% luminal narrowing, significant\ni.e. >50% luminal narrowing). For detection and classification of coronary\nplaque, the method achieved an accuracy of 0.77. For detection and\nclassification of stenosis, the method achieved an accuracy of 0.80. The\nresults demonstrate that automatic detection and classification of coronary\nartery plaque and stenosis are feasible. This may enable automated triage of\npatients to those without coronary plaque and those with coronary plaque and\nstenosis in need for further cardiovascular workup.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 07:42:03 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 12:43:26 GMT"}, {"version": "v3", "created": "Mon, 3 Dec 2018 13:09:00 GMT"}, {"version": "v4", "created": "Mon, 10 Dec 2018 10:38:09 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Zreik", "Majd", ""], ["van Hamersvelt", "Robbert W.", ""], ["Wolterink", "Jelmer M.", ""], ["Leiner", "Tim", ""], ["Viergever", "Max A.", ""], ["Isgum", "Ivana", ""]]}, {"id": "1804.04366", "submitter": "Sahin Olut", "authors": "Sahin Olut, Yusuf Huseyin Sahin, Ugur Demir, Gozde Unal", "title": "Generative Adversarial Training for MRA Image Synthesis Using\n  Multi-Contrast MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Angiography (MRA) has become an essential MR contrast for\nimaging and evaluation of vascular anatomy and related diseases. MRA\nacquisitions are typically ordered for vascular interventions, whereas in\ntypical scenarios, MRA sequences can be absent in the patient scans. This\nmotivates the need for a technique that generates inexistent MRA from existing\nMR multi-contrast, which could be a valuable tool in retrospective subject\nevaluations and imaging studies. In this paper, we present a generative\nadversarial network (GAN) based technique to generate MRA from T1-weighted and\nT2-weighted MRI images, for the first time to our knowledge. To better model\nthe representation of vessels which the MRA inherently highlights, we design a\nloss term dedicated to a faithful reproduction of vascularities. To that end,\nwe incorporate steerable filter responses of the generated and reference images\ninside a Huber function loss term. Extending the well- established\ngenerator-discriminator architecture based on the recent PatchGAN model with\nthe addition of steerable filter loss, the proposed steerable GAN (sGAN) method\nis evaluated on the large public database IXI. Experimental results show that\nthe sGAN outperforms the baseline GAN method in terms of an overlap score with\nsimilar PSNR values, while it leads to improved visual perceptual quality.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 08:11:17 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Olut", "Sahin", ""], ["Sahin", "Yusuf Huseyin", ""], ["Demir", "Ugur", ""], ["Unal", "Gozde", ""]]}, {"id": "1804.04371", "submitter": "Yibing Song", "authors": "Xin Yang, Ke Xu, Yibing Song, Qiang Zhang, Xiaopeng Wei, Rynson Lau", "title": "Image Correction via Deep Reciprocating HDR Transformation", "comments": "in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image correction aims to adjust an input image into a visually pleasing one.\nExisting approaches are proposed mainly from the perspective of image pixel\nmanipulation. They are not effective to recover the details in the under/over\nexposed regions. In this paper, we revisit the image formation procedure and\nnotice that the missing details in these regions exist in the corresponding\nhigh dynamic range (HDR) data. These details are well perceived by the human\neyes but diminished in the low dynamic range (LDR) domain because of the tone\nmapping process. Therefore, we formulate the image correction task as an HDR\ntransformation process and propose a novel approach called Deep Reciprocating\nHDR Transformation (DRHT). Given an input LDR image, we first reconstruct the\nmissing details in the HDR domain. We then perform tone mapping on the\npredicted HDR data to generate the output LDR image with the recovered details.\nTo this end, we propose a united framework consisting of two CNNs for HDR\nreconstruction and tone mapping. They are integrated end-to-end for joint\ntraining and prediction. Experiments on the standard benchmarks demonstrate\nthat the proposed method performs favorably against state-of-the-art image\ncorrection methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 08:23:04 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Yang", "Xin", ""], ["Xu", "Ke", ""], ["Song", "Yibing", ""], ["Zhang", "Qiang", ""], ["Wei", "Xiaopeng", ""], ["Lau", "Rynson", ""]]}, {"id": "1804.04381", "submitter": "Jelmer Wolterink", "authors": "Jelmer M. Wolterink and Tim Leiner and Ivana Isgum", "title": "Blood Vessel Geometry Synthesis using Generative Adversarial Networks", "comments": "Submitted to the 1st Conference on Medical Imaging with Deep Learning\n  (MIDL2018), Amsterdam, The Netherlands\n  (https://openreview.net/forum?id=SJ4N7isiG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computationally synthesized blood vessels can be used for training and\nevaluation of medical image analysis applications. We propose a deep generative\nmodel to synthesize blood vessel geometries, with an application to coronary\narteries in cardiac CT angiography (CCTA).\n  In the proposed method, a Wasserstein generative adversarial network (GAN)\nconsisting of a generator and a discriminator network is trained. While the\ngenerator tries to synthesize realistic blood vessel geometries, the\ndiscriminator tries to distinguish synthesized geometries from those of real\nblood vessels. Both real and synthesized blood vessel geometries are\nparametrized as 1D signals based on the central vessel axis. The generator can\noptionally be provided with an attribute vector to synthesize vessels with\nparticular characteristics.\n  The GAN was optimized using a reference database with parametrizations of\n4,412 real coronary artery geometries extracted from CCTA scans. After\ntraining, plausible coronary artery geometries could be synthesized based on\nrandom vectors sampled from a latent space. A qualitative analysis showed\nstrong similarities between real and synthesized coronary arteries. A detailed\nanalysis of the latent space showed that the diversity present in coronary\nartery anatomy was accurately captured by the generator.\n  Results show that Wasserstein generative adversarial networks can be used to\nsynthesize blood vessel geometries.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 09:08:56 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Wolterink", "Jelmer M.", ""], ["Leiner", "Tim", ""], ["Isgum", "Ivana", ""]]}, {"id": "1804.04383", "submitter": "Nikolas Lessmann", "authors": "Nikolas Lessmann, Bram van Ginneken, Pim A. de Jong, Ivana I\\v{s}gum", "title": "Iterative fully convolutional neural networks for automatic vertebra\n  segmentation and identification", "comments": "Accepted for publication in Medical Image Analysis", "journal-ref": "Medical Image Analysis 53, pp. 142-155, 2019", "doi": "10.1016/j.media.2019.02.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise segmentation and anatomical identification of the vertebrae provides\nthe basis for automatic analysis of the spine, such as detection of vertebral\ncompression fractures or other abnormalities. Most dedicated spine CT and MR\nscans as well as scans of the chest, abdomen or neck cover only part of the\nspine. Segmentation and identification should therefore not rely on the\nvisibility of certain vertebrae or a certain number of vertebrae. We propose an\niterative instance segmentation approach that uses a fully convolutional neural\nnetwork to segment and label vertebrae one after the other, independently of\nthe number of visible vertebrae. This instance-by-instance segmentation is\nenabled by combining the network with a memory component that retains\ninformation about already segmented vertebrae. The network iteratively analyzes\nimage patches, using information from both image and memory to search for the\nnext vertebra. To efficiently traverse the image, we include the prior\nknowledge that the vertebrae are always located next to each other, which is\nused to follow the vertebral column. This method was evaluated with five\ndiverse datasets, including multiple modalities (CT and MR), various fields of\nview and coverages of different sections of the spine, and a particularly\nchallenging set of low-dose chest CT scans. The proposed iterative segmentation\nmethod compares favorably with state-of-the-art methods and is fast, flexible\nand generalizable.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 09:10:55 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 14:11:32 GMT"}, {"version": "v3", "created": "Mon, 11 Feb 2019 17:16:50 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Lessmann", "Nikolas", ""], ["van Ginneken", "Bram", ""], ["de Jong", "Pim A.", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1804.04391", "submitter": "Hyunjung Shim Dr.", "authors": "Duhyeon Bang and Hyunjung Shim", "title": "MGGAN: Solving Mode Collapse using Manifold Guided Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mode collapse is a critical problem in training generative adversarial\nnetworks. To alleviate mode collapse, several recent studies introduce new\nobjective functions, network architectures or alternative training schemes.\nHowever, their achievement is often the result of sacrificing the image\nquality. In this paper, we propose a new algorithm, namely a manifold guided\ngenerative adversarial network (MGGAN), which leverages a guidance network on\nexisting GAN architecture to induce generator learning all modes of data\ndistribution. Based on extensive evaluations, we show that our algorithm\nresolves mode collapse without losing image quality. In particular, we\ndemonstrate that our algorithm is easily extendable to various existing GANs.\nExperimental analysis justifies that the proposed algorithm is an effective and\nefficient tool for training GANs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 09:27:25 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Bang", "Duhyeon", ""], ["Shim", "Hyunjung", ""]]}, {"id": "1804.04395", "submitter": "Dimitri Block", "authors": "Sergej Grunau, Dimitri Block, Uwe Meier", "title": "Multi-Label Wireless Interference Identification with Convolutional\n  Neural Networks", "comments": "Submitted to the 16th International Conference on Industrial\n  Informatics (INDIN 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The steadily growing use of license-free frequency bands require reliable\ncoexistence management and therefore proper wireless interference\nidentification (WII). In this work, we propose a WII approach based upon a deep\nconvolutional neural network (CNN) which classifies multiple IEEE 802.15.1,\nIEEE 802.11 b/g and IEEE 802.15.4 interfering signals in the presence of a\nutilized signal. The generated multi-label dataset contains frequency- and\ntime-limited sensing snapshots with the bandwidth of 10 MHz and duration of\n12.8 $\\mu$s, respectively. Each snapshot combines one utilized signal with up\nto multiple interfering signals. The approach shows promising results for\nsame-technology interference with a classification accuracy of approximately\n100 % for IEEE 802.15.1 and IEEE 802.15.4 signals. For IEEE 802.11 b/g signals\nthe accuracy increases for cross-technology interference with at least 90 %.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 09:31:32 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Grunau", "Sergej", ""], ["Block", "Dimitri", ""], ["Meier", "Uwe", ""]]}, {"id": "1804.04397", "submitter": "Xiangbo Shu", "authors": "Jinhui Tang, Xiangbo Shu, Zechao Li, Yu-Gang Jiang, Qi Tian", "title": "Social Anchor-Unit Graph Regularized Tensor Completion for Large-Scale\n  Image Retagging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retagging aims to improve tag quality of social images by refining\ntheir original tags or assigning new high-quality tags. Recent approaches\nsimultaneously explore visual, user and tag information to improve the\nperformance of image retagging by constructing and exploring an image-tag-user\ngraph. However, such methods will become computationally infeasible with the\nrapidly increasing number of images, tags and users. It has been proven that\nAnchor Graph Regularization (AGR) can significantly accelerate large-scale\ngraph learning model by exploring only a small number of anchor points.\nInspired by this, we propose a novel Social anchor-Unit GrAph Regularized\nTensor Completion (SUGAR-TC) method to effectively refine the tags of social\nimages, which is insensitive to the scale of the applied data. First, we\nconstruct an anchor-unit graph across multiple domains (e.g., image and user\ndomains) rather than traditional anchor graph in a single domain. Second, a\ntensor completion based on SUGAR is implemented on the original image-tag-user\ntensor to refine the tags of the anchor images. Third, we efficiently assign\ntags to non-anchor images by leveraging the relationship between the non-anchor\nimages and the anchor units. Experimental results on a real-world social image\ndatabase well demonstrate the effectiveness of SUGAR-TC, outperforming several\nrelated methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 09:40:30 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 07:28:41 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Tang", "Jinhui", ""], ["Shu", "Xiangbo", ""], ["Li", "Zechao", ""], ["Jiang", "Yu-Gang", ""], ["Tian", "Qi", ""]]}, {"id": "1804.04412", "submitter": "Yuting Zhang", "authors": "Yuting Zhang, Yijie Guo, Yixin Jin, Yijun Luo, Zhiyuan He, Honglak Lee", "title": "Unsupervised Discovery of Object Landmarks as Structural Representations", "comments": "48 pages", "journal-ref": "CVPR 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks can model images with rich latent representations, but\nthey cannot naturally conceptualize structures of object categories in a\nhuman-perceptible way. This paper addresses the problem of learning object\nstructures in an image modeling process without supervision. We propose an\nautoencoding formulation to discover landmarks as explicit structural\nrepresentations. The encoding module outputs landmark coordinates, whose\nvalidity is ensured by constraints that reflect the necessary properties for\nlandmarks. The decoding module takes the landmarks as a part of the learnable\ninput representations in an end-to-end differentiable framework. Our discovered\nlandmarks are semantically meaningful and more predictive of manually annotated\nlandmarks than those discovered by previous methods. The coordinates of our\nlandmarks are also complementary features to pretrained deep-neural-network\nrepresentations in recognizing visual attributes. In addition, the proposed\nmethod naturally creates an unsupervised, perceptible interface to manipulate\nobject shapes and decode images with controllable structures. The project\nwebpage is at http://ytzhang.net/projects/lmdis-rep\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 10:25:41 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Zhang", "Yuting", ""], ["Guo", "Yijie", ""], ["Jin", "Yixin", ""], ["Luo", "Yijun", ""], ["He", "Zhiyuan", ""], ["Lee", "Honglak", ""]]}, {"id": "1804.04418", "submitter": "Hong Huy Nguyen", "authors": "Huy H. Nguyen, Ngoc-Dung T. Tieu, Hoang-Quoc Nguyen-Son, Junichi\n  Yamagishi, Isao Echizen", "title": "Transformation on Computer-Generated Facial Image to Avoid Detection by\n  Spoofing Detector", "comments": "Accepted to be Published in Proceedings of the IEEE International\n  Conference on Multimedia and Expo (ICME) 2018, San Diego, USA", "journal-ref": null, "doi": "10.1109/ICME.2018.8486579", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making computer-generated (CG) images more difficult to detect is an\ninteresting problem in computer graphics and security. While most approaches\nfocus on the image rendering phase, this paper presents a method based on\nincreasing the naturalness of CG facial images from the perspective of spoofing\ndetectors. The proposed method is implemented using a convolutional neural\nnetwork (CNN) comprising two autoencoders and a transformer and is trained\nusing a black-box discriminator without gradient information. Over 50% of the\ntransformed CG images were not detected by three state-of-the-art spoofing\ndetectors. This capability raises an alarm regarding the reliability of facial\nauthentication systems, which are becoming widely used in daily life.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 10:48:20 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Nguyen", "Huy H.", ""], ["Tieu", "Ngoc-Dung T.", ""], ["Nguyen-Son", "Hoang-Quoc", ""], ["Yamagishi", "Junichi", ""], ["Echizen", "Isao", ""]]}, {"id": "1804.04419", "submitter": "Julio Cezar Silveira Jacques Junior", "authors": "Julio C. S. Jacques Junior, Xavier Bar\\'o and Sergio Escalera", "title": "Exploiting feature representations through similarity learning,\n  post-ranking and ranking aggregation for person re-identification", "comments": "Preprint submitted to Image and Vision Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification has received special attention by the human analysis\ncommunity in the last few years. To address the challenges in this field, many\nresearchers have proposed different strategies, which basically exploit either\ncross-view invariant features or cross-view robust metrics. In this work, we\npropose to exploit a post-ranking approach and combine different feature\nrepresentations through ranking aggregation. Spatial information, which\npotentially benefits the person matching, is represented using a 2D body model,\nfrom which color and texture information are extracted and combined. We also\nconsider background/foreground information, automatically extracted via Deep\nDecompositional Network, and the usage of Convolutional Neural Network (CNN)\nfeatures. To describe the matching between images we use the polynomial feature\nmap, also taking into account local and global information. The Discriminant\nContext Information Analysis based post-ranking approach is used to improve\ninitial ranking lists. Finally, the Stuart ranking aggregation method is\nemployed to combine complementary ranking lists obtained from different feature\nrepresentations. Experimental results demonstrated that we improve the\nstate-of-the-art on VIPeR and PRID450s datasets, achieving 67.21% and 75.64% on\ntop-1 rank recognition rate, respectively, as well as obtaining competitive\nresults on CUHK01 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 10:49:37 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Junior", "Julio C. S. Jacques", ""], ["Bar\u00f3", "Xavier", ""], ["Escalera", "Sergio", ""]]}, {"id": "1804.04436", "submitter": "Raghavendra Selvan", "authors": "Raghavendra Selvan, Thomas Kipf, Max Welling, Jesper H. Pedersen, Jens\n  Petersen, Marleen de Bruijne", "title": "Extraction of Airways using Graph Neural Networks", "comments": "Extended Abstract submitted to MIDL, 2018. 3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present extraction of tree structures, such as airways, from image data as\na graph refinement task. To this end, we propose a graph auto-encoder model\nthat uses an encoder based on graph neural networks (GNNs) to learn embeddings\nfrom input node features and a decoder to predict connections between nodes.\nPerformance of the GNN model is compared with mean-field networks in their\nability to extract airways from 3D chest CT scans.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 11:36:57 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Selvan", "Raghavendra", ""], ["Kipf", "Thomas", ""], ["Welling", "Max", ""], ["Pedersen", "Jesper H.", ""], ["Petersen", "Jens", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1804.04438", "submitter": "Ari Morcos", "authors": "Avraham Ruderman, Neil C. Rabinowitz, Ari S. Morcos, Daniel Zoran", "title": "Pooling is neither necessary nor sufficient for appropriate deformation\n  stability in CNNs", "comments": "NIPS 2018 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of our core assumptions about how neural networks operate remain\nempirically untested. One common assumption is that convolutional neural\nnetworks need to be stable to small translations and deformations to solve\nimage recognition tasks. For many years, this stability was baked into CNN\narchitectures by incorporating interleaved pooling layers. Recently, however,\ninterleaved pooling has largely been abandoned. This raises a number of\nquestions: Are our intuitions about deformation stability right at all? Is it\nimportant? Is pooling necessary for deformation invariance? If not, how is\ndeformation invariance achieved in its absence? In this work, we rigorously\ntest these questions, and find that deformation stability in convolutional\nnetworks is more nuanced than it first appears: (1) Deformation invariance is\nnot a binary property, but rather that different tasks require different\ndegrees of deformation stability at different layers. (2) Deformation stability\nis not a fixed property of a network and is heavily adjusted over the course of\ntraining, largely through the smoothness of the convolutional filters. (3)\nInterleaved pooling layers are neither necessary nor sufficient for achieving\nthe optimal form of deformation stability for natural image classification. (4)\nPooling confers too much deformation stability for image classification at\ninitialization, and during training, networks have to learn to counteract this\ninductive bias. Together, these findings provide new insights into the role of\ninterleaved pooling and deformation invariance in CNNs, and demonstrate the\nimportance of rigorous empirical testing of even our most basic assumptions\nabout the working of neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 11:44:05 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 13:03:50 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Ruderman", "Avraham", ""], ["Rabinowitz", "Neil C.", ""], ["Morcos", "Ari S.", ""], ["Zoran", "Daniel", ""]]}, {"id": "1804.04450", "submitter": "Jongchan Park", "authors": "Jongchan Park, Joon-Young Lee, Donggeun Yoo, and In So Kweon", "title": "Distort-and-Recover: Color Enhancement using Deep Reinforcement Learning", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based color enhancement approaches typically learn to map from input\nimages to retouched images. Most of existing methods require expensive pairs of\ninput-retouched images or produce results in a non-interpretable way. In this\npaper, we present a deep reinforcement learning (DRL) based method for color\nenhancement to explicitly model the step-wise nature of human retouching\nprocess. We cast a color enhancement process as a Markov Decision Process where\nactions are defined as global color adjustment operations. Then we train our\nagent to learn the optimal global enhancement sequence of the actions. In\naddition, we present a 'distort-and-recover' training scheme which only\nrequires high-quality reference images for training instead of input and\nretouched image pairs. Given high-quality reference images, we distort the\nimages' color distribution and form distorted-reference image pairs for\ntraining. Through extensive experiments, we show that our method produces\ndecent enhancement results and our DRL approach is more suitable for the\n'distort-and-recover' training scheme than previous supervised approaches.\nSupplementary material and code are available at\nhttps://sites.google.com/view/distort-and-recover/\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 11:59:20 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 01:48:00 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Park", "Jongchan", ""], ["Lee", "Joon-Young", ""], ["Yoo", "Donggeun", ""], ["Kweon", "In So", ""]]}, {"id": "1804.04458", "submitter": "Daniel Worrall", "authors": "Daniel Worrall and Gabriel Brostow", "title": "CubeNet: Equivariance to 3D Rotation and Translation", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Convolutional Neural Networks are sensitive to transformations applied to\ntheir input. This is a problem because a voxelized version of a 3D object, and\nits rotated clone, will look unrelated to each other after passing through to\nthe last layer of a network. Instead, an idealized model would preserve a\nmeaningful representation of the voxelized object, while explaining the\npose-difference between the two inputs. An equivariant representation vector\nhas two components: the invariant identity part, and a discernable encoding of\nthe transformation. Models that can't explain pose-differences risk \"diluting\"\nthe representation, in pursuit of optimizing a classification or regression\nloss function.\n  We introduce a Group Convolutional Neural Network with linear equivariance to\ntranslations and right angle rotations in three dimensions. We call this\nnetwork CubeNet, reflecting its cube-like symmetry. By construction, this\nnetwork helps preserve a 3D shape's global and local signature, as it is\ntransformed through successive layers. We apply this network to a variety of 3D\ninference problems, achieving state-of-the-art on the ModelNet10 classification\nchallenge, and comparable performance on the ISBI 2012 Connectome Segmentation\nBenchmark. To the best of our knowledge, this is the first 3D rotation\nequivariant CNN for voxel representations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 12:14:18 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Worrall", "Daniel", ""], ["Brostow", "Gabriel", ""]]}, {"id": "1804.04483", "submitter": "Shiguang Wang", "authors": "Shiguang Wang, Jian Cheng, Haijun Liu, Ming Tang", "title": "PCN: Part and Context Information for Pedestrian Detection with CNNs", "comments": "Accepted by British Machine Vision Conference(BMVC) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection has achieved great improvements in recent years, while\ncomplex occlusion handling is still one of the most important problems. To take\nadvantage of the body parts and context information for pedestrian detection,\nwe propose the part and context network (PCN) in this work. PCN specially\nutilizes two branches which detect the pedestrians through body parts semantic\nand context information, respectively. In the Part Branch, the semantic\ninformation of body parts can communicate with each other via recurrent neural\nnetworks. In the Context Branch, we adopt a local competition mechanism for\nadaptive context scale selection. By combining the outputs of all branches, we\ndevelop a strong complementary pedestrian detector with a lower miss rate and\nbetter localization accuracy, especially for occlusion pedestrian.\nComprehensive evaluations on two challenging pedestrian detection datasets\n(i.e. Caltech and INRIA) well demonstrated the effectiveness of the proposed\nPCN.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 12:59:59 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Wang", "Shiguang", ""], ["Cheng", "Jian", ""], ["Liu", "Haijun", ""], ["Tang", "Ming", ""]]}, {"id": "1804.04488", "submitter": "Christoph Baur", "authors": "Christoph Baur, Benedikt Wiestler, Shadi Albarqouni, Nassir Navab", "title": "Deep Autoencoding Models for Unsupervised Anomaly Segmentation in Brain\n  MR Images", "comments": null, "journal-ref": "BrainLesion: Glioma, Multiple Sclerosis, Stroke and Traumatic\n  Brain Injuries. BrainLes 2018", "doi": "10.1007/978-3-030-11723-8_16", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliably modeling normality and differentiating abnormal appearances from\nnormal cases is a very appealing approach for detecting pathologies in medical\nimages. A plethora of such unsupervised anomaly detection approaches has been\nmade in the medical domain, based on statistical methods, content-based\nretrieval, clustering and recently also deep learning. Previous approaches\ntowards deep unsupervised anomaly detection model patches of normal anatomy\nwith variants of Autoencoders or GANs, and detect anomalies either as outliers\nin the learned feature space or from large reconstruction errors. In contrast\nto these patch-based approaches, we show that deep spatial autoencoding models\ncan be efficiently used to capture normal anatomical variability of entire 2D\nbrain MR images. A variety of experiments on real MR data containing MS lesions\ncorroborates our hypothesis that we can detect and even delineate anomalies in\nbrain MR images by simply comparing input images to their reconstruction.\nResults show that constraints on the latent space and adversarial training can\nfurther improve the segmentation performance over standard deep representation\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 13:13:29 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Baur", "Christoph", ""], ["Wiestler", "Benedikt", ""], ["Albarqouni", "Shadi", ""], ["Navab", "Nassir", ""]]}, {"id": "1804.04512", "submitter": "Baptiste Wicht", "authors": "Baptiste Wicht and Jean Hennebert and Andreas Fischer", "title": "DLL: A Blazing Fast Deep Neural Network Library", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning Library (DLL) is a new library for machine learning with deep\nneural networks that focuses on speed. It supports feed-forward neural networks\nsuch as fully-connected Artificial Neural Networks (ANNs) and Convolutional\nNeural Networks (CNNs). It also has very comprehensive support for Restricted\nBoltzmann Machines (RBMs) and Convolutional RBMs. Our main motivation for this\nwork was to propose and evaluate novel software engineering strategies with\npotential to accelerate runtime for training and inference. Such strategies are\nmostly independent of the underlying deep learning algorithms. On three\ndifferent datasets and for four different neural network models, we compared\nDLL to five popular deep learning frameworks. Experimentally, it is shown that\nthe proposed framework is systematically and significantly faster on CPU and\nGPU. In terms of classification performance, similar accuracies as the other\nframeworks are reported.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 13:56:07 GMT"}], "update_date": "2018-04-15", "authors_parsed": [["Wicht", "Baptiste", ""], ["Hennebert", "Jean", ""], ["Fischer", "Andreas", ""]]}, {"id": "1804.04522", "submitter": "Dongwei Ren", "authors": "Dongwei Ren, Wangmeng Zuo, David Zhang, Lei Zhang and Ming-Hsuan Yang", "title": "Simultaneous Fidelity and Regularization Learning for Image Restoration", "comments": "The supplementary file is at\n  https://csdwren.github.io/papers/sfarl_supp.pdf, and the source code is at\n  https://github.com/csdwren/sfarl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing non-blind restoration methods are based on the assumption that\na precise degradation model is known. As the degradation process can only be\npartially known or inaccurately modeled, images may not be well restored. Rain\nstreak removal and image deconvolution with inaccurate blur kernels are two\nrepresentative examples of such tasks. For rain streak removal, although an\ninput image can be decomposed into a scene layer and a rain streak layer, there\nexists no explicit formulation for modeling rain streaks and the composition\nwith scene layer. For blind deconvolution, as estimation error of blur kernel\nis usually introduced, the subsequent non-blind deconvolution process does not\nrestore the latent image well. In this paper, we propose a principled algorithm\nwithin the maximum a posterior framework to tackle image restoration with a\npartially known or inaccurate degradation model. Specifically, the residual\ncaused by a partially known or inaccurate degradation model is spatially\ndependent and complexly distributed. With a training set of degraded and\nground-truth image pairs, we parameterize and learn the fidelity term for a\ndegradation model in a task-driven manner. Furthermore, the regularization term\ncan also be learned along with the fidelity term, thereby forming a\nsimultaneous fidelity and regularization learning model. Extensive experimental\nresults demonstrate the effectiveness of the proposed model for image\ndeconvolution with inaccurate blur kernels, deconvolution with multiple\ndegradations and rain streak removal.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 14:04:13 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 09:00:22 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 11:54:36 GMT"}, {"version": "v4", "created": "Wed, 10 Jul 2019 05:16:21 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Ren", "Dongwei", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "David", ""], ["Zhang", "Lei", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1804.04527", "submitter": "Silvio Giancola", "authors": "Silvio Giancola, Mohieddine Amine, Tarek Dghaily and Bernard Ghanem", "title": "SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos", "comments": "CVPR Workshop on Computer Vision in Sports 2018", "journal-ref": null, "doi": "10.1109/CVPRW.2018.00223", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce SoccerNet, a benchmark for action spotting in\nsoccer videos. The dataset is composed of 500 complete soccer games from six\nmain European leagues, covering three seasons from 2014 to 2017 and a total\nduration of 764 hours. A total of 6,637 temporal annotations are automatically\nparsed from online match reports at a one minute resolution for three main\nclasses of events (Goal, Yellow/Red Card, and Substitution). As such, the\ndataset is easily scalable. These annotations are manually refined to a one\nsecond resolution by anchoring them at a single timestamp following\nwell-defined soccer rules. With an average of one event every 6.9 minutes, this\ndataset focuses on the problem of localizing very sparse events within long\nvideos. We define the task of spotting as finding the anchors of soccer events\nin a video. Making use of recent developments in the realm of generic action\nrecognition and detection in video, we provide strong baselines for detecting\nsoccer events. We show that our best model for classifying temporal segments of\nlength one minute reaches a mean Average Precision (mAP) of 67.8%. For the\nspotting task, our baseline reaches an Average-mAP of 49.7% for tolerances\n$\\delta$ ranging from 5 to 60 seconds. Our dataset and models are available at\nhttps://silviogiancola.github.io/SoccerNet.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 14:19:50 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 13:05:03 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Giancola", "Silvio", ""], ["Amine", "Mohieddine", ""], ["Dghaily", "Tarek", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1804.04539", "submitter": "Jennifer Tang Dr", "authors": "Jarrel Seah, Jennifer Tang, Andy Kitchen and Jonathan Seah", "title": "Generative Visual Rationales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability and small labelled datasets are key issues in the practical\napplication of deep learning, particularly in areas such as medicine. In this\npaper, we present a semi-supervised technique that addresses both these issues\nby leveraging large unlabelled datasets to encode and decode images into a\ndense latent representation. Using chest radiography as an example, we apply\nthis encoder to other labelled datasets and apply simple models to the latent\nvectors to learn algorithms to identify heart failure.\n  For each prediction, we generate visual rationales by optimizing a latent\nrepresentation to minimize the prediction of disease while constrained by a\nsimilarity measure in image space. Decoding the resultant latent representation\nproduces an image without apparent disease. The difference between the original\ndecoding and the altered image forms an interpretable visual rationale for the\nalgorithm's prediction on that image. We also apply our method to the MNIST\ndataset and compare the generated rationales to other techniques described in\nthe literature.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 10:00:24 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Seah", "Jarrel", ""], ["Tang", "Jennifer", ""], ["Kitchen", "Andy", ""], ["Seah", "Jonathan", ""]]}, {"id": "1804.04540", "submitter": "John Mashford PhD", "authors": "John Mashford, Brad Lane, Vic Ciesielski, Felix Lipkin", "title": "A Neural Markovian Multiresolution Image Labeling Algorithm", "comments": "replacement provides formal evaluation of MCV algorithm in published\n  form", "journal-ref": "Intelligent Computing. SAI 2020. Advances in Intelligent Systems\n  and Computing, vol 1229. Springer, Cham", "doi": "10.1007/978-3-030-52246-9_27", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the results of formally evaluating the MCV (Markov\nconcurrent vision) image labeling algorithm which is a (semi-) hierarchical\nalgorithm commencing with a partition made up of single pixel regions and\nmerging regions or subsets of regions using a Markov random field (MRF) image\nmodel. It is an example of a general approach to computer vision called\nconcurrent vision in which the operations of image segmentation and image\nclassification are carried out concurrently. While many image labeling\nalgorithms output a single partition, or segmentation, the MCV algorithm\noutputs a sequence of partitions and this more elaborate structure may provide\ninformation that is valuable for higher level vision systems. With certain\ntypes of MRF the component of the system for image evaluation can be\nimplemented as a hardwired feed forward neural network. While being applicable\nto images (i.e. 2D signals), the algorithm is equally applicable to 1D signals\n(e.g. speech) or 3D signals (e.g. video sequences) (though its performance in\nsuch domains remains to be tested). The algorithm is assessed using subjective\nand objective criteria with very good results.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 06:32:35 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 07:53:03 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Mashford", "John", ""], ["Lane", "Brad", ""], ["Ciesielski", "Vic", ""], ["Lipkin", "Felix", ""]]}, {"id": "1804.04543", "submitter": "Aaron Lee", "authors": "Joanne C. Wen, Cecilia S. Lee, Pearse A. Keane, Sa Xiao, Yue Wu, Ariel\n  Rokem, Philip P. Chen, Aaron Y. Lee", "title": "Forecasting Future Humphrey Visual Fields Using Deep Learning", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0214875", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To determine if deep learning networks could be trained to forecast\na future 24-2 Humphrey Visual Field (HVF).\n  Participants: All patients who obtained a HVF 24-2 at the University of\nWashington.\n  Methods: All datapoints from consecutive 24-2 HVFs from 1998 to 2018 were\nextracted from a University of Washington database. Ten-fold cross validation\nwith a held out test set was used to develop the three main phases of model\ndevelopment: model architecture selection, dataset combination selection, and\ntime-interval model training with transfer learning, to train a deep learning\nartificial neural network capable of generating a point-wise visual field\nprediction.\n  Results: More than 1.7 million perimetry points were extracted to the\nhundredth decibel from 32,443 24-2 HVFs. The best performing model with 20\nmillion trainable parameters, CascadeNet-5, was selected. The overall MAE for\nthe test set was 2.47 dB (95% CI: 2.45 dB to 2.48 dB). The 100 fully trained\nmodels were able to successfully predict progressive field loss in glaucomatous\neyes up to 5.5 years in the future with a correlation of 0.92 between the MD of\npredicted and actual future HVF (p < 2.2 x 10 -16 ) and an average difference\nof 0.41 dB.\n  Conclusions: Using unfiltered real-world datasets, deep learning networks\nshow an impressive ability to not only learn spatio-temporal HVF changes but\nalso to generate predictions for future HVFs up to 5.5 years, given only a\nsingle HVF.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 21:05:22 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Wen", "Joanne C.", ""], ["Lee", "Cecilia S.", ""], ["Keane", "Pearse A.", ""], ["Xiao", "Sa", ""], ["Wu", "Yue", ""], ["Rokem", "Ariel", ""], ["Chen", "Philip P.", ""], ["Lee", "Aaron Y.", ""]]}, {"id": "1804.04549", "submitter": "James Kapaldo", "authors": "James Kapaldo", "title": "Seed-Point Based Geometric Partitioning of Nuclei Clumps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When applying automatic analysis of fluorescence or histopathological images\nof cells, it is necessary to partition, or de-clump, partially overlapping cell\nnuclei. In this work, I describe a method of partitioning partially overlapping\ncell nuclei using a seed-point based geometric partitioning. The geometric\npartitioning creates two different types of cuts, cuts between two boundary\nvertices and cuts between one boundary vertex and a new vertex introduced to\nthe boundary interior. The cuts are then ranked according to a scoring metric,\nand the highest scoring cuts are used. This method was tested on a set of 2420\nclumps of nuclei and was found to produced better results than current popular\nanalysis software.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 14:46:24 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Kapaldo", "James", ""]]}, {"id": "1804.04555", "submitter": "Cong Ma", "authors": "Cong Ma, Changshui Yang, Fan Yang, Yueqing Zhuang, Ziwei Zhang, Huizhu\n  Jia, Xiaodong Xie", "title": "Trajectory Factory: Tracklet Cleaving and Re-connection by Deep Siamese\n  Bi-GRU for Multiple Object Tracking", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Object Tracking (MOT) is a challenging task in the complex scene such\nas surveillance and autonomous driving. In this paper, we propose a novel\ntracklet processing method to cleave and re-connect tracklets on crowd or\nlong-term occlusion by Siamese Bi-Gated Recurrent Unit (GRU). The tracklet\ngeneration utilizes object features extracted by CNN and RNN to create the\nhigh-confidence tracklet candidates in sparse scenario. Due to mis-tracking in\nthe generation process, the tracklets from different objects are split into\nseveral sub-tracklets by a bidirectional GRU. After that, a Siamese GRU based\ntracklet re-connection method is applied to link the sub-tracklets which belong\nto the same object to form a whole trajectory. In addition, we extract the\ntracklet images from existing MOT datasets and propose a novel dataset to train\nour networks. The proposed dataset contains more than 95160 pedestrian images.\nIt has 793 different persons in it. On average, there are 120 images for each\nperson with positions and sizes. Experimental results demonstrate the\nadvantages of our model over the state-of-the-art methods on MOT16.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 15:05:55 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Ma", "Cong", ""], ["Yang", "Changshui", ""], ["Yang", "Fan", ""], ["Zhuang", "Yueqing", ""], ["Zhang", "Ziwei", ""], ["Jia", "Huizhu", ""], ["Xie", "Xiaodong", ""]]}, {"id": "1804.04563", "submitter": "Pierre-Antoine Ganaye", "authors": "Pierre-Antoine Ganaye, Micha\\\"el Sdika, Hugues Benoit-Cattin", "title": "Towards integrating spatial localization in convolutional neural\n  networks for brain image segmentation", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is an established while rapidly evolving field in\nmedical imaging. In this paper we focus on the segmentation of brain Magnetic\nResonance Images (MRI) into cerebral structures using convolutional neural\nnetworks (CNN). CNNs achieve good performance by finding effective high\ndimensional image features describing the patch content only. In this work, we\npropose different ways to introduce spatial constraints into the network to\nfurther reduce prediction inconsistencies.\n  A patch based CNN architecture was trained, making use of multiple scales to\ngather contextual information. Spatial constraints were introduced within the\nCNN through a distance to landmarks feature or through the integration of a\nprobability atlas. We demonstrate experimentally that using spatial information\nhelps to reduce segmentation inconsistencies.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 15:20:48 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Ganaye", "Pierre-Antoine", ""], ["Sdika", "Micha\u00ebl", ""], ["Benoit-Cattin", "Hugues", ""]]}, {"id": "1804.04591", "submitter": "Alvaro Ulloa Cerna", "authors": "Alvaro Ulloa, Sergey Plis, Vince Calhoun", "title": "Improving Classification Rate of Schizophrenia Using a Multimodal\n  Multi-Layer Perceptron Model with Structural and Functional MR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide variety of brain imaging technologies allows us to exploit\ninformation inherent to different data modalities. The richness of multimodal\ndatasets may increase predictive power and reveal latent variables that\notherwise would have not been found. However, the analysis of multimodal data\nis often conducted by assuming linear interactions which impact the accuracy of\nthe results. We propose the use of a multimodal multi-layer perceptron model to\nenhance the predictive power of structural and functional magnetic resonance\nimaging (sMRI and fMRI) combined.\n  We also use a synthetic data generator to pre-train each modality input\nlayers, alleviating the effects of the small sample size that is often the case\nfor brain imaging modalities. The proposed model improved the average and\nuncertainty of the area under the ROC curve to 0.850+-0.051 compared to the\nbest results on individual modalities (0.741+-0.075 for sMRI, and 0.833+-0.050\nfor fMRI).\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 14:46:10 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Ulloa", "Alvaro", ""], ["Plis", "Sergey", ""], ["Calhoun", "Vince", ""]]}, {"id": "1804.04593", "submitter": "Tamar Rott Shaham", "authors": "Tamar Rott Shaham and Tomer Michaeli", "title": "Deformation Aware Image Compression", "comments": "Conference on Computer Vision and Pattern Recognition (CVPR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy compression algorithms aim to compactly encode images in a way which\nenables to restore them with minimal error. We show that a key limitation of\nexisting algorithms is that they rely on error measures that are extremely\nsensitive to geometric deformations (e.g. SSD, SSIM). These force the encoder\nto invest many bits in describing the exact geometry of every fine detail in\nthe image, which is obviously wasteful, because the human visual system is\nindifferent to small local translations. Motivated by this observation, we\npropose a deformation-insensitive error measure that can be easily incorporated\ninto any existing compression scheme. As we show, optimal compression under our\ncriterion involves slightly deforming the input image such that it becomes more\n\"compressible\". Surprisingly, while these small deformations are barely\nnoticeable, they enable the CODEC to preserve details that are otherwise\ncompletely lost. Our technique uses the CODEC as a \"black box\", thus allowing\nsimple integration with arbitrary compression methods. Extensive experiments,\nincluding user studies, confirm that our approach significantly improves the\nvisual quality of many CODECs. These include JPEG, JPEG2000, WebP, BPG, and a\nrecent deep-net method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 16:04:40 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Shaham", "Tamar Rott", ""], ["Michaeli", "Tomer", ""]]}, {"id": "1804.04595", "submitter": "Maximilian Baust", "authors": "Matthias Kohl, Christoph Walz, Florian Ludwig, Stefan Braunewell,\n  Maximilian Baust", "title": "Assessment of Breast Cancer Histology using Densely Connected\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the most frequently diagnosed cancer and leading cause of\ncancer-related death among females worldwide. In this article, we investigate\nthe applicability of densely connected convolutional neural networks to the\nproblems of histology image classification and whole slide image segmentation\nin the area of computer-aided diagnoses for breast cancer. To this end, we\nstudy various approaches for transfer learning and apply them to the data set\nfrom the 2018 grand challenge on breast cancer histology images (BACH).\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 19:40:02 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Kohl", "Matthias", ""], ["Walz", "Christoph", ""], ["Ludwig", "Florian", ""], ["Braunewell", "Stefan", ""], ["Baust", "Maximilian", ""]]}, {"id": "1804.04600", "submitter": "Shota Horiguchi", "authors": "Shota Horiguchi, Sosuke Amano, Makoto Ogawa, Kiyoharu Aizawa", "title": "Personalized Classifier for Food Image Recognition", "comments": "Accepted to IEEE Transaction on Multimedia.\n  http://ieeexplore.ieee.org/document/8316919/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, food image recognition tasks are evaluated against fixed datasets.\nHowever, in real-world conditions, there are cases in which the number of\nsamples in each class continues to increase and samples from novel classes\nappear. In particular, dynamic datasets in which each individual user creates\nsamples and continues the updating process often have content that varies\nconsiderably between different users, and the number of samples per person is\nvery limited. A single classifier common to all users cannot handle such\ndynamic data. Bridging the gap between the laboratory environment and the real\nworld has not yet been accomplished on a large scale. Personalizing a\nclassifier incrementally for each user is a promising way to do this. In this\npaper, we address the personalization problem, which involves adapting to the\nuser's domain incrementally using a very limited number of samples. We propose\na simple yet effective personalization framework which is a combination of the\nnearest class mean classifier and the 1-nearest neighbor classifier based on\ndeep features. To conduct realistic experiments, we made use of a new dataset\nof daily food images collected by a food-logging application. Experimental\nresults show that our proposed method significantly outperforms existing\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 14:08:13 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Horiguchi", "Shota", ""], ["Amano", "Sosuke", ""], ["Ogawa", "Makoto", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1804.04601", "submitter": "Xiaogang Cheng", "authors": "Xiaogang Cheng, Guoqing Liu, Anders Hedman, Kun Wang, Haibo Li", "title": "Expressway visibility estimation based on image entropy and piecewise\n  stationary time series analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based methods for visibility estimation can play a critical role in\nreducing traffic accidents caused by fog and haze. To overcome the\ndisadvantages of current visibility estimation methods, we present a novel\ndata-driven approach based on Gaussian image entropy and piecewise stationary\ntime series analysis (SPEV). This is the first time that Gaussian image entropy\nis used for estimating atmospheric visibility. To lessen the impact of\nlandscape and sunshine illuminance on visibility estimation, we used region of\ninterest (ROI) analysis and took into account relative ratios of image entropy,\nto improve estimation accuracy. We assume fog and haze cause blurred images and\nthat fog and haze can be considered as a piecewise stationary signal. We used\npiecewise stationary time series analysis to construct the piecewise causal\nrelationship between image entropy and visibility. To obtain a real-world\nvisibility measure during fog and haze, a subjective assessment was established\nthrough a study with 36 subjects who performed visibility observations.\nFinally, a total of two million videos were used for training the SPEV model\nand validate its effectiveness. The videos were collected from the constantly\nfoggy and hazy Tongqi expressway in Jiangsu, China. The contrast model of\nvisibility estimation was used for algorithm performance comparison, and the\nvalidation results of the SPEV model were encouraging as 99.14% of the relative\nerrors were less than 10%.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 21:46:05 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Cheng", "Xiaogang", ""], ["Liu", "Guoqing", ""], ["Hedman", "Anders", ""], ["Wang", "Kun", ""], ["Li", "Haibo", ""]]}, {"id": "1804.04602", "submitter": "Ahmad Hassanat", "authors": "A.S. Tarawneh, D. Chetverikov, A.B. Hassanat", "title": "Pilot Comparative Study of Different Deep Features for Palmprint\n  Identification in Low-Quality Images", "comments": "5 pages, 5 figures, Ninth Hungarian Conference on Computer Graphics\n  and Geometry, Budapest, 2018", "journal-ref": "Ninth Hungarian Conference on Computer Graphics and Geometry,\n  Budapest, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) are widespread, efficient tools of\nvisual recognition. In this paper, we present a comparative study of three\npopular pre-trained CNN models: AlexNet, VGG-16 and VGG-19. We address the\nproblem of palmprint identification in low-quality imagery and apply Support\nVector Machines (SVMs) with all of the compared models. For the comparison, we\nuse the MOHI palmprint image database whose images are characterized by low\ncontrast, shadows, and varying illumination, scale, translation and rotation.\nAnother, high-quality database called COEP is also considered to study the\nrecognition gap between high-quality and low-quality imagery. Our experiments\nshow that the deeper pre-trained CNN models, e.g., VGG-16 and VGG-19, tend to\nextract highly distinguishable features that recognize low-quality palmprints\nmore efficiently than the less deep networks such as AlexNet. Furthermore, our\nexperiments on the two databases using various models demonstrate that the\nfeatures extracted from lower-level fully connected layers provide higher\nrecognition rates than higher-layer features. Our results indicate that\ndifferent pre-trained models can be efficiently used in touchless\nidentification systems with low-quality palmprint images.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 01:13:33 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Tarawneh", "A. S.", ""], ["Chetverikov", "D.", ""], ["Hassanat", "A. B.", ""]]}, {"id": "1804.04603", "submitter": "Zhenxin Wang", "authors": "Zhenxin Wang, Sayan Sarcar, Jingxin Liu, Yilin Zheng, Xiangshi Ren", "title": "Outline Objects using Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image segmentation needs both local boundary position information and global\nobject context information. The performance of the recent state-of-the-art\nmethod, fully convolutional networks, reaches a bottleneck due to the neural\nnetwork limit after balancing between the two types of information\nsimultaneously in an end-to-end training style. To overcome this problem, we\ndivide the semantic image segmentation into temporal subtasks. First, we find a\npossible pixel position of some object boundary; then trace the boundary at\nsteps within a limited length until the whole object is outlined. We present\nthe first deep reinforcement learning approach to semantic image segmentation,\ncalled DeepOutline, which outperforms other algorithms in Coco detection\nleaderboard in the middle and large size person category in Coco val2017\ndataset. Meanwhile, it provides an insight into a divide and conquer way by\nreinforcement learning on computer vision problems.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 03:25:31 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 07:10:44 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Wang", "Zhenxin", ""], ["Sarcar", "Sayan", ""], ["Liu", "Jingxin", ""], ["Zheng", "Yilin", ""], ["Ren", "Xiangshi", ""]]}, {"id": "1804.04604", "submitter": "Daniel Harari", "authors": "Daniel Harari, Joshua B. Tenenbaum and Shimon Ullman", "title": "Discovery and usage of joint attention in images", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint visual attention is characterized by two or more individuals looking at\na common target at the same time. The ability to identify joint attention in\nscenes, the people involved, and their common target, is fundamental to the\nunderstanding of social interactions, including others' intentions and goals.\nIn this work we deal with the extraction of joint attention events, and the use\nof such events for image descriptions. The work makes two novel contributions.\nFirst, our extraction algorithm is the first which identifies joint visual\nattention in single static images. It computes 3D gaze direction, identifies\nthe gaze target by combining gaze direction with a 3D depth map computed for\nthe image, and identifies the common gaze target. Second, we use a human study\nto demonstrate the sensitivity of humans to joint attention, suggesting that\nthe detection of such a configuration in an image can be useful for\nunderstanding the image, including the goals of the agents and their joint\nactivity, and therefore can contribute to image captioning and related tasks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 07:04:19 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Harari", "Daniel", ""], ["Tenenbaum", "Joshua B.", ""], ["Ullman", "Shimon", ""]]}, {"id": "1804.04606", "submitter": "Hao Yu", "authors": "Hao Yu, Zhaoning Zhang, Zheng Qin, Hao Wu, Dongsheng Li, Jun Zhao,\n  Xicheng Lu", "title": "Loss Rank Mining: A General Hard Example Mining Method for Real-time\n  Detectors", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern object detectors usually suffer from low accuracy issues, as\nforegrounds always drown in tons of backgrounds and become hard examples during\ntraining. Compared with those proposal-based ones, real-time detectors are in\nfar more serious trouble since they renounce the use of region-proposing stage\nwhich is used to filter a majority of backgrounds for achieving real-time\nrates. Though foregrounds as hard examples are in urgent need of being mined\nfrom tons of backgrounds, a considerable number of state-of-the-art real-time\ndetectors, like YOLO series, have yet to profit from existing hard example\nmining methods, as using these methods need detectors fit series of\nprerequisites. In this paper, we propose a general hard example mining method\nnamed Loss Rank Mining (LRM) to fill the gap. LRM is a general method for\nreal-time detectors, as it utilizes the final feature map which exists in all\nreal-time detectors to mine hard examples. By using LRM, some elements\nrepresenting easy examples in final feature map are filtered and detectors are\nforced to concentrate on hard examples during training. Extensive experiments\nvalidate the effectiveness of our method. With our method, the improvements of\nYOLOv2 detector on auto-driving related dataset KITTI and more general dataset\nPASCAL VOC are over 5% and 2% mAP, respectively. In addition, LRM is the first\nhard example mining strategy which could fit YOLOv2 perfectly and make it\nbetter applied in series of real scenarios where both real-time rates and\naccurate detection are strongly demanded.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 07:43:16 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Yu", "Hao", ""], ["Zhang", "Zhaoning", ""], ["Qin", "Zheng", ""], ["Wu", "Hao", ""], ["Li", "Dongsheng", ""], ["Zhao", "Jun", ""], ["Lu", "Xicheng", ""]]}, {"id": "1804.04610", "submitter": "Jiajun Wu", "authors": "Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai\n  Zhang, Tianfan Xue, Joshua B. Tenenbaum, William T. Freeman", "title": "Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling", "comments": "CVPR 2018. The first two authors contributed equally to this work.\n  Project page: http://pix3d.csail.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study 3D shape modeling from a single image and make contributions to it\nin three aspects. First, we present Pix3D, a large-scale benchmark of diverse\nimage-shape pairs with pixel-level 2D-3D alignment. Pix3D has wide applications\nin shape-related tasks including reconstruction, retrieval, viewpoint\nestimation, etc. Building such a large-scale dataset, however, is highly\nchallenging; existing datasets either contain only synthetic data, or lack\nprecise alignment between 2D images and 3D shapes, or only have a small number\nof images. Second, we calibrate the evaluation criteria for 3D shape\nreconstruction through behavioral studies, and use them to objectively and\nsystematically benchmark cutting-edge reconstruction algorithms on Pix3D.\nThird, we design a novel model that simultaneously performs 3D reconstruction\nand pose estimation; our multi-task learning approach achieves state-of-the-art\nperformance on both tasks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 16:30:39 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Sun", "Xingyuan", ""], ["Wu", "Jiajun", ""], ["Zhang", "Xiuming", ""], ["Zhang", "Zhoutong", ""], ["Zhang", "Chengkai", ""], ["Xue", "Tianfan", ""], ["Tenenbaum", "Joshua B.", ""], ["Freeman", "William T.", ""]]}, {"id": "1804.04647", "submitter": "Yigit Baran Can", "authors": "Yigit Baran Can, Radu Timofte", "title": "An efficient CNN for spectral reconstruction from RGB images", "comments": "Submitted to ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the example-based single image spectral reconstruction from RGB\nimages task, aka, spectral super-resolution was approached by means of deep\nlearning by Galliani et al. The proposed very deep convolutional neural network\n(CNN) achieved superior performance on recent large benchmarks. However,\nAeschbacher et al showed that comparable performance can be achieved by shallow\nlearning method based on A+, a method introduced for image super-resolution by\nTimofte et al. In this paper, we propose a moderately deep CNN model and\nsubstantially improve the reported performance on three spectral reconstruction\nstandard benchmarks: ICVL, CAVE, and NUS.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 17:48:05 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Can", "Yigit Baran", ""], ["Timofte", "Radu", ""]]}, {"id": "1804.04687", "submitter": "Hongyu Xu", "authors": "Hongyu Xu, Jingjing Zheng, Azadeh Alavi and Rama Chellappa", "title": "Cross-Domain Visual Recognition via Domain Adaptive Dictionary Learning", "comments": "Submitted to IEEE TIP Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world visual recognition problems, the assumption that the training\ndata (source domain) and test data (target domain) are sampled from the same\ndistribution is often violated. This is known as the domain adaptation problem.\nIn this work, we propose a novel domain-adaptive dictionary learning framework\nfor cross-domain visual recognition. Our method generates a set of intermediate\ndomains. These intermediate domains form a smooth path and bridge the gap\nbetween the source and target domains. Specifically, we not only learn a common\ndictionary to encode the domain-shared features, but also learn a set of\ndomain-specific dictionaries to model the domain shift. The separation of the\ncommon and domain-specific dictionaries enables us to learn more compact and\nreconstructive dictionaries for domain adaptation. These dictionaries are\nlearned by alternating between domain-adaptive sparse coding and dictionary\nupdating steps. Meanwhile, our approach gradually recovers the feature\nrepresentations of both source and target data along the domain path. By\naligning all the recovered domain data, we derive the final domain-adaptive\nfeatures for cross-domain visual recognition. Extensive experiments on three\npublic datasets demonstrates that our approach outperforms most\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 18:48:17 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 03:05:33 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Xu", "Hongyu", ""], ["Zheng", "Jingjing", ""], ["Alavi", "Azadeh", ""], ["Chellappa", "Rama", ""]]}, {"id": "1804.04694", "submitter": "Patrick Esser", "authors": "Patrick Esser, Ekaterina Sutter, Bj\\\"orn Ommer", "title": "A Variational U-Net for Conditional Appearance and Shape Generation", "comments": "CVPR 2018 (Spotlight). Project Page at\n  https://compvis.github.io/vunet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have demonstrated great performance in image\nsynthesis. However, results deteriorate in case of spatial deformations, since\nthey generate images of objects directly, rather than modeling the intricate\ninterplay of their inherent shape and appearance. We present a conditional\nU-Net for shape-guided image generation, conditioned on the output of a\nvariational autoencoder for appearance. The approach is trained end-to-end on\nimages, without requiring samples of the same object with varying pose or\nappearance. Experiments show that the model enables conditional image\ngeneration and transfer. Therefore, either shape or appearance can be retained\nfrom a query image, while freely altering the other. Moreover, appearance can\nbe sampled due to its stochastic latent representation, while preserving shape.\nIn quantitative and qualitative experiments on COCO, DeepFashion, shoes,\nMarket-1501 and handbags, the approach demonstrates significant improvements\nover the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 19:05:57 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Esser", "Patrick", ""], ["Sutter", "Ekaterina", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "1804.04732", "submitter": "Xun Huang", "authors": "Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz", "title": "Multimodal Unsupervised Image-to-Image Translation", "comments": "Accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation is an important and challenging\nproblem in computer vision. Given an image in the source domain, the goal is to\nlearn the conditional distribution of corresponding images in the target\ndomain, without seeing any pairs of corresponding images. While this\nconditional distribution is inherently multimodal, existing approaches make an\noverly simplified assumption, modeling it as a deterministic one-to-one\nmapping. As a result, they fail to generate diverse outputs from a given source\ndomain image. To address this limitation, we propose a Multimodal Unsupervised\nImage-to-image Translation (MUNIT) framework. We assume that the image\nrepresentation can be decomposed into a content code that is domain-invariant,\nand a style code that captures domain-specific properties. To translate an\nimage to another domain, we recombine its content code with a random style code\nsampled from the style space of the target domain. We analyze the proposed\nframework and establish several theoretical results. Extensive experiments with\ncomparisons to the state-of-the-art approaches further demonstrates the\nadvantage of the proposed framework. Moreover, our framework allows users to\ncontrol the style of translation outputs by providing an example style image.\nCode and pretrained models are available at https://github.com/nvlabs/MUNIT\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 21:17:54 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 18:44:12 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Huang", "Xun", ""], ["Liu", "Ming-Yu", ""], ["Belongie", "Serge", ""], ["Kautz", "Jan", ""]]}, {"id": "1804.04779", "submitter": "Qianru Sun", "authors": "Qianru Sun, Ayush Tewari, Weipeng Xu, Mario Fritz, Christian Theobalt,\n  Bernt Schiele", "title": "A Hybrid Model for Identity Obfuscation by Face Replacement", "comments": "ECCV'18, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As more and more personal photos are shared and tagged in social media,\navoiding privacy risks such as unintended recognition becomes increasingly\nchallenging. We propose a new hybrid approach to obfuscate identities in photos\nby head replacement. Our approach combines state of the art parametric face\nsynthesis with latest advances in Generative Adversarial Networks (GAN) for\ndata-driven image synthesis. On the one hand, the parametric part of our method\ngives us control over the facial parameters and allows for explicit\nmanipulation of the identity. On the other hand, the data-driven aspects allow\nfor adding fine details and overall realism as well as seamless blending into\nthe scene context. In our experiments, we show highly realistic output of our\nsystem that improves over the previous state of the art in obfuscation rate\nwhile preserving a higher similarity to the original image content.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 03:55:32 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 10:31:03 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Sun", "Qianru", ""], ["Tewari", "Ayush", ""], ["Xu", "Weipeng", ""], ["Fritz", "Mario", ""], ["Theobalt", "Christian", ""], ["Schiele", "Bernt", ""]]}, {"id": "1804.04784", "submitter": "Dacheng Tao", "authors": "Xiaoqing Yin, Xinchao Wang, Jun Yu, Maojun Zhang, Pascal Fua, Dacheng\n  Tao", "title": "FishEyeRecNet: A Multi-Context Collaborative Deep Network for Fisheye\n  Image Rectification", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images captured by fisheye lenses violate the pinhole camera assumption and\nsuffer from distortions. Rectification of fisheye images is therefore a crucial\npreprocessing step for many computer vision applications. In this paper, we\npropose an end-to-end multi-context collaborative deep network for removing\ndistortions from single fisheye images. In contrast to conventional approaches,\nwhich focus on extracting hand-crafted features from input images, our method\nlearns high-level semantics and low-level appearance features simultaneously to\nestimate the distortion parameters. To facilitate training, we construct a\nsynthesized dataset that covers various scenes and distortion parameter\nsettings. Experiments on both synthesized and real-world datasets show that the\nproposed model significantly outperforms current state of the art methods. Our\ncode and synthesized dataset will be made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 04:18:47 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Yin", "Xiaoqing", ""], ["Wang", "Xinchao", ""], ["Yu", "Jun", ""], ["Zhang", "Maojun", ""], ["Fua", "Pascal", ""], ["Tao", "Dacheng", ""]]}, {"id": "1804.04785", "submitter": "Dacheng Tao", "authors": "Xiaoqing Yin, Xiyang Dai, Xinchao Wang, Maojun Zhang, Dacheng Tao,\n  Larry Davis", "title": "Deep Motion Boundary Detection", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion boundary detection is a crucial yet challenging problem. Prior methods\nfocus on analyzing the gradients and distributions of optical flow fields, or\nuse hand-crafted features for motion boundary learning. In this paper, we\npropose the first dedicated end-to-end deep learning approach for motion\nboundary detection, which we term as MoBoNet. We introduce a refinement network\nstructure which takes source input images, initial forward and backward optical\nflows as well as corresponding warping errors as inputs and produces\nhigh-resolution motion boundaries. Furthermore, we show that the obtained\nmotion boundaries, through a fusion sub-network we design, can in turn guide\nthe optical flows for removing the artifacts. The proposed MoBoNet is generic\nand works with any optical flows. Our motion boundary detection and the refined\noptical flow estimation achieve results superior to the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 04:19:06 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Yin", "Xiaoqing", ""], ["Dai", "Xiyang", ""], ["Wang", "Xinchao", ""], ["Zhang", "Maojun", ""], ["Tao", "Dacheng", ""], ["Davis", "Larry", ""]]}, {"id": "1804.04786", "submitter": "Yang Song", "authors": "Yang Song, Jingwen Zhu, Dawei Li, Xiaolong Wang, Hairong Qi", "title": "Talking Face Generation by Conditional Recurrent Adversarial Network", "comments": "Project\n  Page:http://web.eecs.utk.edu/~ysong18/projects/talkingface/talkingface.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given an arbitrary face image and an arbitrary speech clip, the proposed work\nattempts to generating the talking face video with accurate lip synchronization\nwhile maintaining smooth transition of both lip and facial movement over the\nentire video clip. Existing works either do not consider temporal dependency on\nface images across different video frames thus easily yielding\nnoticeable/abrupt facial and lip movement or are only limited to the generation\nof talking face video for a specific person thus lacking generalization\ncapacity. We propose a novel conditional video generation network where the\naudio input is treated as a condition for the recurrent adversarial network\nsuch that temporal dependency is incorporated to realize smooth transition for\nthe lip and facial movement. In addition, we deploy a multi-task adversarial\ntraining scheme in the context of video generation to improve both\nphoto-realism and the accuracy for lip synchronization. Finally, based on the\nphoneme distribution information extracted from the audio clip, we develop a\nsample selection method that effectively reduces the size of the training\ndataset without sacrificing the quality of the generated video. Extensive\nexperiments on both controlled and uncontrolled datasets demonstrate the\nsuperiority of the proposed approach in terms of visual quality, lip sync\naccuracy, and smooth transition of lip and facial movement, as compared to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 04:19:52 GMT"}, {"version": "v2", "created": "Sat, 5 May 2018 21:29:05 GMT"}, {"version": "v3", "created": "Thu, 25 Jul 2019 20:40:22 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Song", "Yang", ""], ["Zhu", "Jingwen", ""], ["Li", "Dawei", ""], ["Wang", "Xiaolong", ""], ["Qi", "Hairong", ""]]}, {"id": "1804.04803", "submitter": "Yingbin Zheng", "authors": "Haonan Qiu, Yingbin Zheng, Hao Ye, Yao Lu, Feng Wang, Liang He", "title": "Precise Temporal Action Localization by Evolving Temporal Proposals", "comments": null, "journal-ref": "ACM International Conference on Multimedia Retrieval (ICMR), 2018,\n  pp. 388-396", "doi": "10.1145/3206025.3206029", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locating actions in long untrimmed videos has been a challenging problem in\nvideo content analysis. The performances of existing action localization\napproaches remain unsatisfactory in precisely determining the beginning and the\nend of an action. Imitating the human perception procedure with observations\nand refinements, we propose a novel three-phase action localization framework.\nOur framework is embedded with an Actionness Network to generate initial\nproposals through frame-wise similarity grouping, and then a Refinement Network\nto conduct boundary adjustment on these proposals. Finally, the refined\nproposals are sent to a Localization Network for further fine-grained location\nregression. The whole process can be deemed as multi-stage refinement using a\nnovel non-local pyramid feature under various temporal granularities. We\nevaluate our framework on THUMOS14 benchmark and obtain a significant\nimprovement over the state-of-the-arts approaches. Specifically, the\nperformance gain is remarkable under precise localization with high IoU\nthresholds. Our proposed framework achieves mAP@IoU=0.5 of 34.2%.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 07:10:36 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Qiu", "Haonan", ""], ["Zheng", "Yingbin", ""], ["Ye", "Hao", ""], ["Lu", "Yao", ""], ["Wang", "Feng", ""], ["He", "Liang", ""]]}, {"id": "1804.04804", "submitter": "Umar Riaz Muhammad", "authors": "Umar Riaz Muhammad, Yongxin Yang, Yi-Zhe Song, Tao Xiang and Timothy\n  M. Hospedales", "title": "Learning Deep Sketch Abstraction", "comments": "This paper is accepted at CVPR 2018 as poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human free-hand sketches have been studied in various contexts including\nsketch recognition, synthesis and fine-grained sketch-based image retrieval\n(FG-SBIR). A fundamental challenge for sketch analysis is to deal with\ndrastically different human drawing styles, particularly in terms of\nabstraction level. In this work, we propose the first stroke-level sketch\nabstraction model based on the insight of sketch abstraction as a process of\ntrading off between the recognizability of a sketch and the number of strokes\nused to draw it. Concretely, we train a model for abstract sketch generation\nthrough reinforcement learning of a stroke removal policy that learns to\npredict which strokes can be safely removed without affecting recognizability.\nWe show that our abstraction model can be used for various sketch analysis\ntasks including: (1) modeling stroke saliency and understanding the decision of\nsketch recognition models, (2) synthesizing sketches of variable abstraction\nfor a given category, or reference object instance in a photo, and (3) training\na FG-SBIR model with photos only, bypassing the expensive photo-sketch pair\ncollection step.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 07:12:02 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Muhammad", "Umar Riaz", ""], ["Yang", "Yongxin", ""], ["Song", "Yi-Zhe", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1804.04810", "submitter": "Jungbeom Lee", "authors": "Jungbeom Lee, Jangho Lee, Sungmin Lee, Sungroh Yoon", "title": "Mutual Suppression Network for Video Prediction using Disentangled\n  Features", "comments": "BMVC 2019 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video prediction has been considered a difficult problem because the video\ncontains not only high-dimensional spatial information but also complex\ntemporal information. Video prediction can be performed by finding features in\nrecent frames, and using them to generate approximations to upcoming frames. We\napproach this problem by disentangling spatial and temporal features in videos.\nWe introduce a mutual suppression network (MSnet) which are trained in an\nadversarial manner and then produces spatial features which are free of motion\ninformation, and motion features with no spatial information. MSnet then uses\nmotion-guided connection within an encoder-decoder-based architecture to\ntransform spatial features from a previous frame to the time of an upcoming\nframe. We show how MSnet can be used for video prediction using disentangled\nrepresentations. We also carry out experiments to assess the effectiveness of\nour method to disentangle features. MSnet obtains better results than other\nrecent video prediction methods even though it has simpler encoders.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 07:35:22 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 04:55:49 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Lee", "Jungbeom", ""], ["Lee", "Jangho", ""], ["Lee", "Sungmin", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1804.04817", "submitter": "Ryoichi Ishikawa", "authors": "Ryoichi Ishikawa, Takeshi Oishi, Katsushi Ikeuchi", "title": "Offline and Online calibration of Mobile Robot and SLAM Device for\n  Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot navigation technology is required to accomplish difficult tasks in\nvarious environments. In navigation, it is necessary to know the information of\nthe external environments and the state of the robot under the environment. On\nthe other hand, various studies have been done on SLAM technology, which is\nalso used for navigation, but also applied to devices for Mixed Reality and the\nlike.\n  In this paper, we propose a robot-device calibration method for navigation\nwith a device using SLAM technology on a robot. The calibration is performed by\nusing the position and orientation information given by the robot and the\ndevice. In the calibration, the most efficient way of movement is clarified\naccording to the restriction of the robot movement. Furthermore, we also show a\nmethod to dynamically correct the position and orientation of the robot so that\nthe information of the external environment and the shape information of the\nrobot maintain consistency in order to reduce the dynamic error occurring\nduring navigation.\n  Our method can be easily used for various kinds of robots and localization\nwith sufficient precision for navigation is possible with offline calibration\nand online position correction. In the experiments, we confirm the parameters\nobtained by two types of offline calibration according to the degree of freedom\nof robot movement and validate the effectiveness of online correction method by\nplotting localized position error during robot's intense movement. Finally, we\nshow the demonstration of navigation using SLAM device.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 07:48:44 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Ishikawa", "Ryoichi", ""], ["Oishi", "Takeshi", ""], ["Ikeuchi", "Katsushi", ""]]}, {"id": "1804.04820", "submitter": "Hannes Ovr\\'en", "authors": "Hannes Ovr\\'en and Per-Erik Forss\\'en", "title": "Spline Error Weighting for Robust Visual-Inertial Fusion", "comments": "To appear in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive and test a probability-based weighting that can\nbalance residuals of different types in spline fitting. In contrast to previous\nformulations, the proposed spline error weighting scheme also incorporates a\nprediction of the approximation error of the spline fit. We demonstrate the\neffectiveness of the prediction in a synthetic experiment, and apply it to\nvisual-inertial fusion on rolling shutter cameras. This results in a method\nthat can estimate 3D structure with metric scale on generic first-person\nvideos. We also propose a quality measure for spline fitting, that can be used\nto automatically select the knot spacing. Experiments verify that the obtained\ntrajectory quality corresponds well with the requested quality. Finally, by\nlinearly scaling the weights, we show that the proposed spline error weighting\nminimizes the estimation errors on real sequences, in terms of scale and\nend-point errors.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 07:54:10 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Ovr\u00e9n", "Hannes", ""], ["Forss\u00e9n", "Per-Erik", ""]]}, {"id": "1804.04829", "submitter": "Xiaoming Li", "authors": "Xiaoming Li, Ming Liu, Yuting Ye, Wangmeng Zuo, Liang Lin, Ruigang\n  Yang", "title": "Learning Warped Guidance for Blind Face Restoration", "comments": "25 pages, 14 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of blind face restoration from an\nunconstrained blurry, noisy, low-resolution, or compressed image (i.e.,\ndegraded observation). For better recovery of fine facial details, we modify\nthe problem setting by taking both the degraded observation and a high-quality\nguided image of the same identity as input to our guided face restoration\nnetwork (GFRNet). However, the degraded observation and guided image generally\nare different in pose, illumination and expression, thereby making plain CNNs\n(e.g., U-Net) fail to recover fine and identity-aware facial details. To tackle\nthis issue, our GFRNet model includes both a warping subnetwork (WarpNet) and a\nreconstruction subnetwork (RecNet). The WarpNet is introduced to predict flow\nfield for warping the guided image to correct pose and expression (i.e., warped\nguidance), while the RecNet takes the degraded observation and warped guidance\nas input to produce the restoration result. Due to that the ground-truth flow\nfield is unavailable, landmark loss together with total variation\nregularization are incorporated to guide the learning of WarpNet. Furthermore,\nto make the model applicable to blind restoration, our GFRNet is trained on the\nsynthetic data with versatile settings on blur kernel, noise level,\ndownsampling scale factor, and JPEG quality factor. Experiments show that our\nGFRNet not only performs favorably against the state-of-the-art image and face\nrestoration methods, but also generates visually photo-realistic results on\nreal degraded facial images.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 08:22:09 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 05:24:47 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Li", "Xiaoming", ""], ["Liu", "Ming", ""], ["Ye", "Yuting", ""], ["Zuo", "Wangmeng", ""], ["Lin", "Liang", ""], ["Yang", "Ruigang", ""]]}, {"id": "1804.04875", "submitter": "G\\\"ul Varol", "authors": "G\\\"ul Varol, Duygu Ceylan, Bryan Russell, Jimei Yang, Ersin Yumer,\n  Ivan Laptev, Cordelia Schmid", "title": "BodyNet: Volumetric Inference of 3D Human Body Shapes", "comments": "Appears in: European Conference on Computer Vision 2018 (ECCV 2018).\n  27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human shape estimation is an important task for video editing, animation and\nfashion industry. Predicting 3D human body shape from natural images, however,\nis highly challenging due to factors such as variation in human bodies,\nclothing and viewpoint. Prior methods addressing this problem typically attempt\nto fit parametric body models with certain priors on pose and shape. In this\nwork we argue for an alternative representation and propose BodyNet, a neural\nnetwork for direct inference of volumetric body shape from a single image.\nBodyNet is an end-to-end trainable network that benefits from (i) a volumetric\n3D loss, (ii) a multi-view re-projection loss, and (iii) intermediate\nsupervision of 2D pose, 2D body part segmentation, and 3D pose. Each of them\nresults in performance improvement as demonstrated by our experiments. To\nevaluate the method, we fit the SMPL model to our network output and show\nstate-of-the-art results on the SURREAL and Unite the People datasets,\noutperforming recent approaches. Besides achieving state-of-the-art\nperformance, our method also enables volumetric body-part segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 10:21:40 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 22:49:49 GMT"}, {"version": "v3", "created": "Sat, 18 Aug 2018 11:14:43 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Varol", "G\u00fcl", ""], ["Ceylan", "Duygu", ""], ["Russell", "Bryan", ""], ["Yang", "Jimei", ""], ["Yumer", "Ersin", ""], ["Laptev", "Ivan", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1804.04876", "submitter": "Raghav Chalapathy", "authors": "Raghavendra Chalapathy (The University of Sydney and Capital Markets\n  CRC), Edward Toth (School of Information Technologies, The University of\n  Sydney), and Sanjay Chawla (Qatar Computing Research Institute, HBKU)", "title": "Group Anomaly Detection using Deep Generative Models", "comments": "Submitted Under review to The European Conference on Machine Learning\n  and Principles and Practice of Knowledge Discovery in Databases ECML-2018\n  Conference Dublin, Ireland during the 10-14 September 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike conventional anomaly detection research that focuses on point\nanomalies, our goal is to detect anomalous collections of individual data\npoints. In particular, we perform group anomaly detection (GAD) with an\nemphasis on irregular group distributions (e.g. irregular mixtures of image\npixels). GAD is an important task in detecting unusual and anomalous phenomena\nin real-world applications such as high energy particle physics, social media,\nand medical imaging. In this paper, we take a generative approach by proposing\ndeep generative models: Adversarial autoencoder (AAE) and variational\nautoencoder (VAE) for group anomaly detection. Both AAE and VAE detect group\nanomalies using point-wise input data where group memberships are known a\npriori. We conduct extensive experiments to evaluate our models on real-world\ndatasets. The empirical results demonstrate that our approach is effective and\nrobust in detecting group anomalies.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 10:33:03 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Chalapathy", "Raghavendra", "", "The University of Sydney and Capital Markets\n  CRC"], ["Toth", "Edward", "", "School of Information Technologies, The University of\n  Sydney"], ["Chawla", "Sanjay", "", "Qatar Computing Research Institute, HBKU"]]}, {"id": "1804.04882", "submitter": "Roberto J. L\\'opez-Sastre", "authors": "Carolina Redondo-Cabrera, Marcos Baptista-R\\'ios and Roberto J.\n  L\\'opez-Sastre", "title": "Learning to Exploit the Prior Network Knowledge for Weakly-Supervised\n  Semantic Segmentation", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, 2019", "doi": "10.1109/TIP.2019.2901393", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a Convolutional Neural Network (CNN) for semantic segmentation\ntypically requires to collect a large amount of accurate pixel-level\nannotations, a hard and expensive task. In contrast, simple image tags are\neasier to gather. With this paper we introduce a novel weakly-supervised\nsemantic segmentation model able to learn from image labels, and just image\nlabels. Our model uses the prior knowledge of a network trained for image\nrecognition, employing these image annotations as an attention mechanism to\nidentify semantic regions in the images. We then present a methodology that\nbuilds accurate class-specific segmentation masks from these regions, where\nneither external objectness nor saliency algorithms are required. We describe\nhow to incorporate this mask generation strategy into a fully end-to-end\ntrainable process where the network jointly learns to classify and segment\nimages. Our experiments on PASCAL VOC 2012 dataset show that exploiting these\ngenerated class-specific masks in conjunction with our novel end-to-end\nlearning process outperforms several recent weakly-supervised semantic\nsegmentation methods that use image tags only, and even some models that\nleverage additional supervision or training data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 10:52:07 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 14:26:29 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Redondo-Cabrera", "Carolina", ""], ["Baptista-R\u00edos", "Marcos", ""], ["L\u00f3pez-Sastre", "Roberto J.", ""]]}, {"id": "1804.04922", "submitter": "Damien Mariyanayagam", "authors": "Mariyanayagam Damien, and Gurdjos Pierre, and Chambon Sylvie, and\n  Brunet Florent, and Charvillat Vincent", "title": "Pose estimation of a single circle using default intrinsic calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Circular markers are planar markers which offer great performances for\ndetection and pose estimation. For an uncalibrated camera with an unknown focal\nlength, at least the images of at least two coplanar circles are generally\nrequired to recover their poses. Unfortunately, detecting more than one ellipse\nin the image must be tricky and time-consuming, especially regarding concentric\ncircles. On the other hand, when the camera is calibrated, one circle suffices\nbut the solution is twofold and can hardly be disambiguated. Our contribution\nis to put beyond this limit by dealing with the uncalibrated case of a camera\nseeing one circle and discussing how to remove the ambiguity. We propose a new\nproblem formulation that enables to show how to detect geometric configurations\nin which the ambiguity can be removed. Furthermore, we introduce the notion of\ndefault camera intrinsics and show, using intensive empirical works, the\nsurprising observation that very approximate calibration can lead to accurate\ncircle pose estimation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 12:52:42 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Damien", "Mariyanayagam", ""], ["Pierre", "Gurdjos", ""], ["Sylvie", "Chambon", ""], ["Florent", "Brunet", ""], ["Vincent", "Charvillat", ""]]}, {"id": "1804.04963", "submitter": "Julia Noothout", "authors": "Julia M. H. Noothout, Bob D. de Vos, Jelmer M. Wolterink, Tim Leiner,\n  Ivana I\\v{s}gum", "title": "CNN-based Landmark Detection in Cardiac CTA Scans", "comments": "This work was submitted to MIDL 2018 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and accurate anatomical landmark detection can benefit many medical\nimage analysis methods. Here, we propose a method to automatically detect\nanatomical landmarks in medical images. Automatic landmark detection is\nperformed with a patch-based fully convolutional neural network (FCNN) that\ncombines regression and classification. For any given image patch, regression\nis used to predict the 3D displacement vector from the image patch to the\nlandmark. Simultaneously, classification is used to identify patches that\ncontain the landmark. Under the assumption that patches close to a landmark can\ndetermine the landmark location more precisely than patches farther from it,\nonly those patches that contain the landmark according to classification are\nused to determine the landmark location. The landmark location is obtained by\ncalculating the average landmark location using the computed 3D displacement\nvectors. The method is evaluated using detection of six clinically relevant\nlandmarks in coronary CT angiography (CCTA) scans: the right and left ostium,\nthe bifurcation of the left main coronary artery (LM) into the left anterior\ndescending and the left circumflex artery, and the origin of the right,\nnon-coronary, and left aortic valve commissure. The proposed method achieved an\naverage Euclidean distance error of 2.19 mm and 2.88 mm for the right and left\nostium respectively, 3.78 mm for the bifurcation of the LM, and 1.82 mm, 2.10\nmm and 1.89 mm for the origin of the right, non-coronary, and left aortic valve\ncommissure respectively, demonstrating accurate performance. The proposed\ncombination of regression and classification can be used to accurately detect\nlandmarks in CCTA scans.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 14:32:42 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Noothout", "Julia M. H.", ""], ["de Vos", "Bob D.", ""], ["Wolterink", "Jelmer M.", ""], ["Leiner", "Tim", ""], ["I\u0161gum", "Ivana", ""]]}, {"id": "1804.04970", "submitter": "Wenxue Cui", "authors": "Wenxue Cui, Heyao Xu, Xinwei Gao, Shengping Zhang, Feng Jiang, Debin\n  Zhao", "title": "An efficient deep convolutional laplacian pyramid architecture for CS\n  reconstruction at low sampling ratios", "comments": "5 pages. Accepted by ICASSP2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The compressed sensing (CS) has been successfully applied to image\ncompression in the past few years as most image signals are sparse in a certain\ndomain. Several CS reconstruction models have been proposed and obtained\nsuperior performance. However, these methods suffer from blocking artifacts or\nringing effects at low sampling ratios in most cases. To address this problem,\nwe propose a deep convolutional Laplacian Pyramid Compressed Sensing Network\n(LapCSNet) for CS, which consists of a sampling sub-network and a\nreconstruction sub-network. In the sampling sub-network, we utilize a\nconvolutional layer to mimic the sampling operator. In contrast to the fixed\nsampling matrices used in traditional CS methods, the filters used in our\nconvolutional layer are jointly optimized with the reconstruction sub-network.\nIn the reconstruction sub-network, two branches are designed to reconstruct\nmulti-scale residual images and muti-scale target images progressively using a\nLaplacian pyramid architecture. The proposed LapCSNet not only integrates\nmulti-scale information to achieve better performance but also reduces\ncomputational cost dramatically. Experimental results on benchmark datasets\ndemonstrate that the proposed method is capable of reconstructing more details\nand sharper edges against the state-of-the-arts methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 14:42:20 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Cui", "Wenxue", ""], ["Xu", "Heyao", ""], ["Gao", "Xinwei", ""], ["Zhang", "Shengping", ""], ["Jiang", "Feng", ""], ["Zhao", "Debin", ""]]}, {"id": "1804.04988", "submitter": "Oeslle Lucena", "authors": "Oeslle Lucena, Roberto Souza, Leticia Rittner, Richard Frayne, Roberto\n  Lotufo", "title": "Convolutional Neural Networks for Skull-stripping in Brain MR Imaging\n  using Consensus-based Silver standard Masks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) for medical imaging are constrained by\nthe number of annotated data required in the training stage. Usually, manual\nannotation is considered to be the \"gold standard\". However, medical imaging\ndatasets that include expert manual segmentation are scarce as this step is\ntime-consuming, and therefore expensive. Moreover, single-rater manual\nannotation is most often used in data-driven approaches making the network\noptimal with respect to only that single expert. In this work, we propose a CNN\nfor brain extraction in magnetic resonance (MR) imaging, that is fully trained\nwith what we refer to as silver standard masks. Our method consists of 1)\ndeveloping a dataset with \"silver standard\" masks as input, and implementing\nboth 2) a tri-planar method using parallel 2D U-Net-based CNNs (referred to as\nCONSNet) and 3) an auto-context implementation of CONSNet. The term CONSNet\nrefers to our integrated approach, i.e., training with silver standard masks\nand using a 2D U-Net-based architecture. Our results showed that we\noutperformed (i.e., larger Dice coefficients) the current state-of-the-art SS\nmethods. Our use of silver standard masks reduced the cost of manual\nannotation, decreased inter-intra-rater variability, and avoided CNN\nsegmentation super-specialization towards one specific manual annotation\nguideline that can occur when gold standard masks are used. Moreover, the usage\nof silver standard masks greatly enlarges the volume of input annotated data\nbecause we can relatively easily generate labels for unlabeled data. In\naddition, our method has the advantage that, once trained, it takes only a few\nseconds to process a typical brain image volume using modern hardware, such as\na high-end graphics processing unit. In contrast, many of the other competitive\nmethods have processing times in the order of minutes.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 15:18:06 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Lucena", "Oeslle", ""], ["Souza", "Roberto", ""], ["Rittner", "Leticia", ""], ["Frayne", "Richard", ""], ["Lotufo", "Roberto", ""]]}, {"id": "1804.05018", "submitter": "Sandro Pezzelle", "authors": "Sandro Pezzelle and Ionut-Teodor Sorodoc and Raffaella Bernardi", "title": "Comparatives, Quantifiers, Proportions: A Multi-Task Model for the\n  Learning of Quantities from Vision", "comments": "12 pages (references included). To appear in the Proceedings of\n  NAACL-HLT 2018", "journal-ref": "Proceedings of NAACL-HLT 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work investigates whether different quantification mechanisms\n(set comparison, vague quantification, and proportional estimation) can be\njointly learned from visual scenes by a multi-task computational model. The\nmotivation is that, in humans, these processes underlie the same cognitive,\nnon-symbolic ability, which allows an automatic estimation and comparison of\nset magnitudes. We show that when information about lower-complexity tasks is\navailable, the higher-level proportional task becomes more accurate than when\nperformed in isolation. Moreover, the multi-task model is able to generalize to\nunseen combinations of target/non-target objects. Consistently with behavioral\nevidence showing the interference of absolute number in the proportional task,\nthe multi-task model no longer works when asked to provide the number of target\nobjects in the scene.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 16:36:52 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Pezzelle", "Sandro", ""], ["Sorodoc", "Ionut-Teodor", ""], ["Bernardi", "Raffaella", ""]]}, {"id": "1804.05042", "submitter": "Ying Qu", "authors": "Ying Qu, Hairong Qi and Chiman Kwan", "title": "Unsupervised Sparse Dirichlet-Net for Hyperspectral Image\n  Super-Resolution", "comments": "Accepted by The IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2018, Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many computer vision applications, obtaining images of high resolution in\nboth the spatial and spectral domains are equally important. However, due to\nhardware limitations, one can only expect to acquire images of high resolution\nin either the spatial or spectral domains. This paper focuses on hyperspectral\nimage super-resolution (HSI-SR), where a hyperspectral image (HSI) with low\nspatial resolution (LR) but high spectral resolution is fused with a\nmultispectral image (MSI) with high spatial resolution (HR) but low spectral\nresolution to obtain HR HSI. Existing deep learning-based solutions are all\nsupervised that would need a large training set and the availability of HR HSI,\nwhich is unrealistic. Here, we make the first attempt to solving the HSI-SR\nproblem using an unsupervised encoder-decoder architecture that carries the\nfollowing uniquenesses. First, it is composed of two encoder-decoder networks,\ncoupled through a shared decoder, in order to preserve the rich spectral\ninformation from the HSI network. Second, the network encourages the\nrepresentations from both modalities to follow a sparse Dirichlet distribution\nwhich naturally incorporates the two physical constraints of HSI and MSI.\nThird, the angular difference between representations are minimized in order to\nreduce the spectral distortion. We refer to the proposed architecture as\nunsupervised Sparse Dirichlet-Net, or uSDN. Extensive experimental results\ndemonstrate the superior performance of uSDN as compared to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 17:17:05 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 07:55:42 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 12:53:39 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Qu", "Ying", ""], ["Qi", "Hairong", ""], ["Kwan", "Chiman", ""]]}, {"id": "1804.05091", "submitter": "Cosmin Ancuti", "authors": "Codruta O. Ancuti, Cosmin Ancuti, Radu Timofte and Christophe De\n  Vleeschouwer", "title": "I-HAZE: a dehazing benchmark with real hazy and haze-free indoor images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image dehazing has become an important computational imaging topic in the\nrecent years. However, due to the lack of ground truth images, the comparison\nof dehazing methods is not straightforward, nor objective. To overcome this\nissue we introduce a new dataset -named I-HAZE- that contains 35 image pairs of\nhazy and corresponding haze-free (ground-truth) indoor images. Different from\nmost of the existing dehazing databases, hazy images have been generated using\nreal haze produced by a professional haze machine. For easy color calibration\nand improved assessment of dehazing algorithms, each scene include a MacBeth\ncolor checker. Moreover, since the images are captured in a controlled\nenvironment, both haze-free and hazy images are captured under the same\nillumination conditions. This represents an important advantage of the I-HAZE\ndataset that allows us to objectively compare the existing image dehazing\ntechniques using traditional image quality metrics such as PSNR and SSIM.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 19:01:39 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Ancuti", "Codruta O.", ""], ["Ancuti", "Cosmin", ""], ["Timofte", "Radu", ""], ["De Vleeschouwer", "Christophe", ""]]}, {"id": "1804.05101", "submitter": "Cosmin Ancuti", "authors": "Codruta O. Ancuti, Cosmin Ancuti, Radu Timofte and Christophe De\n  Vleeschouwer", "title": "O-HAZE: a dehazing benchmark with real hazy and haze-free outdoor images", "comments": "arXiv admin note: text overlap with arXiv:1804.05091", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haze removal or dehazing is a challenging ill-posed problem that has drawn a\nsignificant attention in the last few years. Despite this growing interest, the\nscientific community is still lacking a reference dataset to evaluate\nobjectively and quantitatively the performance of proposed dehazing methods.\nThe few datasets that are currently considered, both for assessment and\ntraining of learning-based dehazing techniques, exclusively rely on synthetic\nhazy images. To address this limitation, we introduce the first outdoor scenes\ndatabase (named O-HAZE) composed of pairs of real hazy and corresponding\nhaze-free images. In practice, hazy images have been captured in presence of\nreal haze, generated by professional haze machines, and OHAZE contains 45\ndifferent outdoor scenes depicting the same visual content recorded in\nhaze-free and hazy conditions, under the same illumination parameters. To\nillustrate its usefulness, O-HAZE is used to compare a representative set of\nstate-of-the-art dehazing techniques, using traditional image quality metrics\nsuch as PSNR, SSIM and CIEDE2000. This reveals the limitations of current\ntechniques, and questions some of their underlying assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 19:58:17 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Ancuti", "Codruta O.", ""], ["Ancuti", "Cosmin", ""], ["Timofte", "Radu", ""], ["De Vleeschouwer", "Christophe", ""]]}, {"id": "1804.05113", "submitter": "Huijuan Xu", "authors": "Huijuan Xu, Kun He, Bryan A. Plummer, Leonid Sigal, Stan Sclaroff,\n  Kate Saenko", "title": "Multilevel Language and Vision Integration for Text-to-Clip Retrieval", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of text-based activity retrieval in video. Given a\nsentence describing an activity, our task is to retrieve matching clips from an\nuntrimmed video. To capture the inherent structures present in both text and\nvideo, we introduce a multilevel model that integrates vision and language\nfeatures earlier and more tightly than prior work. First, we inject text\nfeatures early on when generating clip proposals, to help eliminate unlikely\nclips and thus speed up processing and boost performance. Second, to learn a\nfine-grained similarity metric for retrieval, we use visual features to\nmodulate the processing of query sentences at the word level in a recurrent\nneural network. A multi-task loss is also employed by adding query\nre-generation as an auxiliary task. Our approach significantly outperforms\nprior work on two challenging benchmarks: Charades-STA and ActivityNet\nCaptions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 20:46:37 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 00:17:35 GMT"}, {"version": "v3", "created": "Tue, 25 Dec 2018 08:29:56 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Xu", "Huijuan", ""], ["He", "Kun", ""], ["Plummer", "Bryan A.", ""], ["Sigal", "Leonid", ""], ["Sclaroff", "Stan", ""], ["Saenko", "Kate", ""]]}, {"id": "1804.05132", "submitter": "Di Feng", "authors": "Di Feng, Lars Rosenbaum, Klaus Dietmayer", "title": "Towards Safe Autonomous Driving: Capture Uncertainty in the Deep Neural\n  Network For Lidar 3D Vehicle Detection", "comments": "Accepted to present in the 21st IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To assure that an autonomous car is driving safely on public roads, its\nobject detection module should not only work correctly, but show its prediction\nconfidence as well. Previous object detectors driven by deep learning do not\nexplicitly model uncertainties in the neural network. We tackle with this\nproblem by presenting practical methods to capture uncertainties in a 3D\nvehicle detector for Lidar point clouds. The proposed probabilistic detector\nrepresents reliable epistemic uncertainty and aleatoric uncertainty in\nclassification and localization tasks. Experimental results show that the\nepistemic uncertainty is related to the detection accuracy, whereas the\naleatoric uncertainty is influenced by vehicle distance and occlusion. The\nresults also show that we can improve the detection performance by 1%-5% by\nmodeling the aleatoric uncertainty.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 22:13:30 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 08:10:46 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Feng", "Di", ""], ["Rosenbaum", "Lars", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1804.05142", "submitter": "Pingping Zhang Mr", "authors": "Pingping Zhang, Huchuan Lu and Chunhua Shen", "title": "HyperFusion-Net: Densely Reflective Fusion for Salient Object Detection", "comments": "Submmited to ECCV 2018, 16 pages, including 6 figures and 4 tables.\n  arXiv admin note: text overlap with arXiv:1802.06527", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Salient object detection (SOD), which aims to find the most important region\nof interest and segment the relevant object/item in that area, is an important\nyet challenging vision task. This problem is inspired by the fact that human\nseems to perceive main scene elements with high priorities. Thus, accurate\ndetection of salient objects in complex scenes is critical for human-computer\ninteraction. In this paper, we present a novel feature learning framework for\nSOD, in which we cast the SOD as a pixel-wise classification problem. The\nproposed framework utilizes a densely hierarchical feature fusion network,\nnamed HyperFusion-Net, automatically predicts the most important area and\nsegments the associated objects in an end-to-end manner. Specifically, inspired\nby the human perception system and image reflection separation, we first\ndecompose input images into reflective image pairs by content-preserving\ntransforms. Then, the complementary information of reflective image pairs is\njointly extracted by an interweaved convolutional neural network (ICNN) and\nhierarchically combined with a hyper-dense fusion mechanism. Based on the fused\nmulti-scale features, our method finally achieves a promising way of predicting\nSOD. As shown in our extensive experiments, the proposed method consistently\noutperforms other state-of-the-art methods on seven public datasets with a\nlarge margin.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 01:06:04 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Zhang", "Pingping", ""], ["Lu", "Huchuan", ""], ["Shen", "Chunhua", ""]]}, {"id": "1804.05164", "submitter": "Yecheng Lyu", "authors": "Yecheng Lyu and Xinming Huang", "title": "Road Segmentation Using CNN with GRU", "comments": "submitted to IV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents an accurate and fast algorithm for road segmentation\nusing convolutional neural network (CNN) and gated recurrent units (GRU). For\nautonomous vehicles, road segmentation is a fundamental task that can provide\nthe drivable area for path planning. The existing deep neural network based\nsegmentation algorithms usually take a very deep encoder-decoder structure to\nfuse pixels, which requires heavy computations, large memory and long\nprocessing time. Hereby, a CNN-GRU network model is proposed and trained to\nperform road segmentation using data captured by the front camera of a vehicle.\nGRU network obtains a long spatial sequence with lower computational\ncomplexity, comparing to traditional encoder-decoder architecture. The proposed\nroad detector is evaluated on the KITTI road benchmark and achieves high\naccuracy for road segmentation at real-time processing speed.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 04:27:47 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Lyu", "Yecheng", ""], ["Huang", "Xinming", ""]]}, {"id": "1804.05178", "submitter": "Ryoichi Ishikawa", "authors": "Ryoichi Ishikawa, Takeshi Oishi, Katsushi Ikeuchi", "title": "LiDAR and Camera Calibration using Motion Estimated by Sensor Fusion\n  Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method of targetless and automatic Camera-LiDAR\ncalibration. Our approach is an extension of hand-eye calibration framework to\n2D-3D calibration. By using the sensor fusion odometry method, the scaled\ncamera motions are calculated with high accuracy. In addition to this, we\nclarify the suitable motion for this calibration method.\n  The proposed method only requires the three-dimensional point cloud and the\ncamera image and does not need other information such as reflectance of LiDAR\nand to give initial extrinsic parameter. In the experiments, we demonstrate our\nmethod using several sensor configurations in indoor and outdoor scenes to\nverify the effectiveness. The accuracy of our method achieves more than other\ncomparable state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 06:53:07 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Ishikawa", "Ryoichi", ""], ["Oishi", "Takeshi", ""], ["Ikeuchi", "Katsushi", ""]]}, {"id": "1804.05181", "submitter": "Saeid Asgari Taghanaki", "authors": "Saeid Asgari Taghanaki, Aicha Bentaieb, Anmol Sharma, S. Kevin Zhou,\n  Yefeng Zheng, Bogdan Georgescu, Puneet Sharma, Sasa Grbic, Zhoubing Xu, Dorin\n  Comaniciu, Ghassan Hamarneh", "title": "Select, Attend, and Transfer: Light, Learnable Skip Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skip connections in deep networks have improved both segmentation and\nclassification performance by facilitating the training of deeper network\narchitectures, and reducing the risks for vanishing gradients. They equip\nencoder-decoder-like networks with richer feature representations, but at the\ncost of higher memory usage, computation, and possibly resulting in\ntransferring non-discriminative feature maps. In this paper, we focus on\nimproving skip connections used in segmentation networks (e.g., U-Net, V-Net,\nand The One Hundred Layers Tiramisu (DensNet) architectures). We propose light,\nlearnable skip connections which learn to first select the most discriminative\nchannels and then attend to the most discriminative regions of the selected\nfeature maps. The output of the proposed skip connections is a unique feature\nmap which not only reduces the memory usage and network parameters to a high\nextent, but also improves segmentation accuracy. We evaluate the proposed\nmethod on three different 2D and volumetric datasets and demonstrate that the\nproposed light, learnable skip connections can outperform the traditional heavy\nskip connections in terms of segmentation accuracy, memory usage, and number of\nnetwork parameters.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 07:30:15 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 00:11:41 GMT"}, {"version": "v3", "created": "Thu, 3 May 2018 00:15:10 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Taghanaki", "Saeid Asgari", ""], ["Bentaieb", "Aicha", ""], ["Sharma", "Anmol", ""], ["Zhou", "S. Kevin", ""], ["Zheng", "Yefeng", ""], ["Georgescu", "Bogdan", ""], ["Sharma", "Puneet", ""], ["Grbic", "Sasa", ""], ["Xu", "Zhoubing", ""], ["Comaniciu", "Dorin", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1804.05195", "submitter": "Lin Shao", "authors": "Lin Shao, Parth Shah, Vikranth Dwaracherla, Jeannette Bohg", "title": "Motion-based Object Segmentation based on Dense RGB-D Scene Flow", "comments": "Accepted to IEEE Robotics and Automation Letters and selected by\n  IROS'18 Program Committee for presentation at the Conference", "journal-ref": null, "doi": "10.1109/LRA.2018.2856525", "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two consecutive RGB-D images, we propose a model that estimates a dense\n3D motion field, also known as scene flow. We take advantage of the fact that\nin robot manipulation scenarios, scenes often consist of a set of rigidly\nmoving objects. Our model jointly estimates (i) the segmentation of the scene\ninto an unknown but finite number of objects, (ii) the motion trajectories of\nthese objects and (iii) the object scene flow. We employ an hourglass, deep\nneural network architecture. In the encoding stage, the RGB and depth images\nundergo spatial compression and correlation. In the decoding stage, the model\noutputs three images containing a per-pixel estimate of the corresponding\nobject center as well as object translation and rotation. This forms the basis\nfor inferring the object segmentation and final object scene flow. To evaluate\nour model, we generated a new and challenging, large-scale, synthetic dataset\nthat is specifically targeted at robotic manipulation: It contains a large\nnumber of scenes with a very diverse set of simultaneously moving 3D objects\nand is recorded with a simulated, static RGB-D camera. In quantitative\nexperiments, we show that we outperform state-of-the-art scene flow and\nmotion-segmentation methods on this data set. In qualitative experiments, we\nshow how our learned model transfers to challenging real-world scenes, visually\ngenerating better results than existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 09:33:40 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 08:49:35 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Shao", "Lin", ""], ["Shah", "Parth", ""], ["Dwaracherla", "Vikranth", ""], ["Bohg", "Jeannette", ""]]}, {"id": "1804.05197", "submitter": "Yu Liu", "authors": "Guanglu Song, Yu Liu, Ming Jiang, Yujie Wang, Junjie Yan, Biao Leng", "title": "Beyond Trade-off: Accelerate FCN-based Face Detector with Higher\n  Accuracy", "comments": "Accepted by CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional neural network (FCN) has been dominating the game of face\ndetection task for a few years with its congenital capability of\nsliding-window-searching with shared kernels, which boiled down all the\nredundant calculation, and most recent state-of-the-art methods such as\nFaster-RCNN, SSD, YOLO and FPN use FCN as their backbone. So here comes one\nquestion: Can we find a universal strategy to further accelerate FCN with\nhigher accuracy, so could accelerate all the recent FCN-based methods? To\nanalyze this, we decompose the face searching space into two orthogonal\ndirections, `scale' and `spatial'. Only a few coordinates in the space expanded\nby the two base vectors indicate foreground. So if FCN could ignore most of the\nother points, the searching space and false alarm should be significantly\nboiled down. Based on this philosophy, a novel method named scale estimation\nand spatial attention proposal ($S^2AP$) is proposed to pay attention to some\nspecific scales and valid locations in the image pyramid. Furthermore, we adopt\na masked-convolution operation based on the attention result to accelerate FCN\ncalculation. Experiments show that FCN-based method RPN can be accelerated by\nabout $4\\times$ with the help of $S^2AP$ and masked-FCN and at the same time it\ncan also achieve the state-of-the-art on FDDB, AFW and MALF face detection\nbenchmarks as well.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 09:38:16 GMT"}, {"version": "v2", "created": "Sat, 2 Jun 2018 10:56:10 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Song", "Guanglu", ""], ["Liu", "Yu", ""], ["Jiang", "Ming", ""], ["Wang", "Yujie", ""], ["Yan", "Junjie", ""], ["Leng", "Biao", ""]]}, {"id": "1804.05260", "submitter": "Danushka Bollegala", "authors": "Danushka Bollegala, Vincent Atanasov, Takanori Maehara, Ken-ichi\n  Kawarabayashi", "title": "ClassiNet -- Predicting Missing Features for Short-Text Classification", "comments": "Accepted to ACM TKDD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental problem in short-text classification is \\emph{feature\nsparseness} -- the lack of feature overlap between a trained model and a test\ninstance to be classified. We propose \\emph{ClassiNet} -- a network of\nclassifiers trained for predicting missing features in a given instance, to\novercome the feature sparseness problem. Using a set of unlabeled training\ninstances, we first learn binary classifiers as feature predictors for\npredicting whether a particular feature occurs in a given instance. Next, each\nfeature predictor is represented as a vertex $v_i$ in the ClassiNet where a\none-to-one correspondence exists between feature predictors and vertices. The\nweight of the directed edge $e_{ij}$ connecting a vertex $v_i$ to a vertex\n$v_j$ represents the conditional probability that given $v_i$ exists in an\ninstance, $v_j$ also exists in the same instance. We show that ClassiNets\ngeneralize word co-occurrence graphs by considering implicit co-occurrences\nbetween features. We extract numerous features from the trained ClassiNet to\novercome feature sparseness. In particular, for a given instance $\\vec{x}$, we\nfind similar features from ClassiNet that did not appear in $\\vec{x}$, and\nappend those features in the representation of $\\vec{x}$. Moreover, we propose\na method based on graph propagation to find features that are indirectly\nrelated to a given short-text. We evaluate ClassiNets on several benchmark\ndatasets for short-text classification. Our experimental results show that by\nusing ClassiNet, we can statistically significantly improve the accuracy in\nshort-text classification tasks, without having to use any external resources\nsuch as thesauri for finding related features.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 18:24:06 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Bollegala", "Danushka", ""], ["Atanasov", "Vincent", ""], ["Maehara", "Takanori", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1804.05261", "submitter": "Garoe Dorta", "authors": "Garoe Dorta, Luca Benedetti, Dmitry Kit, Yong-Liang Yang", "title": "Physics-driven Fire Modeling from Multi-view Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fire effects are widely used in various computer graphics applications such\nas visual effects and video games. Modeling the shape and appearance of fire\nphenomenon is challenging as the underlying effects are driven by complex laws\nof physics. State-of-the-art fire modeling techniques rely on sophisticated\nphysical simulations which require intensive parameter tuning, or use\nsimplifications which produce physically invalid results. In this paper, we\npresent a novel method of reconstructing physically valid fire models from\nmulti-view stereo images. Our method, for the first time, provides plausible\nestimation of physical properties (e.g., temperature, density) of a fire volume\nusing RGB cameras. This allows for a number of novel phenomena such as global\nfire illumination effects. The effectiveness and usefulness of our method are\ntested by generating fire models from a variety of input data, and applying the\nreconstructed fire models for realistic illumination of virtual scenes.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 18:28:51 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Dorta", "Garoe", ""], ["Benedetti", "Luca", ""], ["Kit", "Dmitry", ""], ["Yang", "Yong-Liang", ""]]}, {"id": "1804.05273", "submitter": "Felix M. Riese", "authors": "Felix M. Riese, Sina Keller", "title": "Fusion of hyperspectral and ground penetrating radar to estimate soil\n  moisture", "comments": "This work has been accepted to the IEEE WHISPERS 2018 conference. (C)\n  2018 IEEE", "journal-ref": null, "doi": "10.1109/WHISPERS.2018.8747076", "report-no": null, "categories": "cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we investigate the potential of hyperspectral data\ncombined with either simulated ground penetrating radar (GPR) or simulated\n(sensor-like) soil-moisture data to estimate soil moisture. We propose two\nsimulation approaches to extend a given multi-sensor dataset which contains\nsparse GPR data. In the first approach, simulated GPR data is generated either\nby an interpolation along the time axis or by a machine learning model. The\nsecond approach includes the simulation of soil-moisture along the GPR profile.\nThe soil-moisture estimation is improved significantly by the fusion of\nhyperspectral and GPR data. In contrast, the combination of simulated,\nsensor-like soil-moisture values and hyperspectral data achieves the worst\nregression performance. In conclusion, the estimation of soil moisture with\nhyperspectral and GPR data engages further investigations.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 20:51:54 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 12:06:06 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 13:22:06 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Riese", "Felix M.", ""], ["Keller", "Sina", ""]]}, {"id": "1804.05275", "submitter": "Yang Fu", "authors": "Yang Fu, Yunchao Wei, Yuqian Zhou, Honghui Shi, Gao Huang, Xinchao\n  Wang, Zhiqiang Yao, Thomas Huang", "title": "Horizontal Pyramid Matching for Person Re-identification", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable recent progress, person re-identification (Re-ID)\napproaches are still suffering from the failure cases where the discriminative\nbody parts are missing. To mitigate such cases, we propose a simple yet\neffective Horizontal Pyramid Matching (HPM) approach to fully exploit various\npartial information of a given person, so that correct person candidates can be\nstill identified even even some key parts are missing. Within the HPM, we make\nthe following contributions to produce a more robust feature representation for\nthe Re-ID task: 1) we learn to classify using partial feature representations\nat different horizontal pyramid scales, which successfully enhance the\ndiscriminative capabilities of various person parts; 2) we exploit average and\nmax pooling strategies to account for person-specific discriminative\ninformation in a global-local manner. To validate the effectiveness of the\nproposed HPM, extensive experiments are conducted on three popular benchmarks,\nincluding Market-1501, DukeMTMC-ReID and CUHK03. In particular, we achieve mAP\nscores of 83.1%, 74.5% and 59.7% on these benchmarks, which are the new\nstate-of-the-arts. Our code is available on Github\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 20:53:40 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 19:13:23 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2018 15:51:07 GMT"}, {"version": "v4", "created": "Sat, 10 Nov 2018 03:58:35 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Fu", "Yang", ""], ["Wei", "Yunchao", ""], ["Zhou", "Yuqian", ""], ["Shi", "Honghui", ""], ["Huang", "Gao", ""], ["Wang", "Xinchao", ""], ["Yao", "Zhiqiang", ""], ["Huang", "Thomas", ""]]}, {"id": "1804.05286", "submitter": "Zhi-Qi Cheng", "authors": "Zhi-Qi Cheng, Hao Zhang, Xiao Wu, Chong-Wah Ngo", "title": "On the Selection of Anchors and Targets for Video Hyperlinking", "comments": "ACM International Conference on Multimedia Retrieval (ICMR), 2017.\n  (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A problem not well understood in video hyperlinking is what qualifies a\nfragment as an anchor or target. Ideally, anchors provide good starting points\nfor navigation, and targets supplement anchors with additional details while\nnot distracting users with irrelevant, false and redundant information. The\nproblem is not trivial for intertwining relationship between data\ncharacteristics and user expectation. Imagine that in a large dataset, there\nare clusters of fragments spreading over the feature space. The nature of each\ncluster can be described by its size (implying popularity) and structure\n(implying complexity). A principle way of hyperlinking can be carried out by\npicking centers of clusters as anchors and from there reach out to targets\nwithin or outside of clusters with consideration of neighborhood complexity.\nThe question is which fragments should be selected either as anchors or\ntargets, in one way to reflect the rich content of a dataset, and meanwhile to\nminimize the risk of frustrating user experience. This paper provides some\ninsights to this question from the perspective of hubness and local intrinsic\ndimensionality, which are two statistical properties in assessing the\npopularity and complexity of data space. Based these properties, two novel\nalgorithms are proposed for low-risk automatic selection of anchors and\ntargets.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 22:53:39 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Cheng", "Zhi-Qi", ""], ["Zhang", "Hao", ""], ["Wu", "Xiao", ""], ["Ngo", "Chong-Wah", ""]]}, {"id": "1804.05287", "submitter": "Zhi-Qi Cheng", "authors": "Zhi-Qi Cheng, Xiao Wu, Yang Liu, Xian-Sheng Hua", "title": "Video2Shop: Exact Matching Clothes in Videos to Online Shopping Images", "comments": "IEEE International Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, both online retail and video hosting service are\nexponentially growing. In this paper, we explore a new cross-domain task,\nVideo2Shop, targeting for matching clothes appeared in videos to the exact same\nitems in online shops. A novel deep neural network, called AsymNet, is proposed\nto explore this problem. For the image side, well-established methods are used\nto detect and extract features for clothing patches with arbitrary sizes. For\nthe video side, deep visual features are extracted from detected object regions\nin each frame, and further fed into a Long Short-Term Memory (LSTM) framework\nfor sequence modeling, which captures the temporal dynamics in videos. To\nconduct exact matching between videos and online shopping images, LSTM hidden\nstates, representing the video, and image features, which represent static\nobject images, are jointly modeled under the similarity network with\nreconfigurable deep tree structure. Moreover, an approximate training method is\nproposed to achieve the efficiency when training. Extensive experiments\nconducted on a large cross-domain dataset have demonstrated the effectiveness\nand efficiency of the proposed AsymNet, which outperforms the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 22:59:44 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 16:50:02 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Cheng", "Zhi-Qi", ""], ["Wu", "Xiao", ""], ["Liu", "Yang", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "1804.05298", "submitter": "Zitian Chen", "authors": "Zitian Chen, Yanwei Fu, Yinda Zhang, Yu-Gang Jiang, Xiangyang Xue, and\n  Leonid Sigal", "title": "Multi-level Semantic Feature Augmentation for One-shot Learning", "comments": "The paper has been ACCEPTED for publication as a REGULAR paper in the\n  IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2910052", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to quickly recognize and learn new visual concepts from limited\nsamples enables humans to swiftly adapt to new environments. This ability is\nenabled by semantic associations of novel concepts with those that have already\nbeen learned and stored in memory. Computers can start to ascertain similar\nabilities by utilizing a semantic concept space. A concept space is a\nhigh-dimensional semantic space in which similar abstract concepts appear close\nand dissimilar ones far apart. In this paper, we propose a novel approach to\none-shot learning that builds on this idea. Our approach learns to map a novel\nsample instance to a concept, relates that concept to the existing ones in the\nconcept space and generates new instances, by interpolating among the concepts,\nto help learning. Instead of synthesizing new image instance, we propose to\ndirectly synthesize instance features by leveraging semantics using a novel\nauto-encoder network we call dual TriNet. The encoder part of the TriNet learns\nto map multi-layer visual features of deep CNNs, that is, multi-level concepts,\nto a semantic vector. In semantic space, we search for related concepts, which\nare then projected back into the image feature spaces by the decoder portion of\nthe TriNet. Two strategies in the semantic space are explored. Notably, this\nseemingly simple strategy results in complex augmented feature distributions in\nthe image feature space, leading to substantially better performance.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 02:44:54 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 05:46:20 GMT"}, {"version": "v3", "created": "Tue, 2 Oct 2018 15:01:57 GMT"}, {"version": "v4", "created": "Fri, 15 Mar 2019 09:37:14 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Chen", "Zitian", ""], ["Fu", "Yanwei", ""], ["Zhang", "Yinda", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""], ["Sigal", "Leonid", ""]]}, {"id": "1804.05312", "submitter": "Kun He", "authors": "Kun He, Yan Lu, Stan Sclaroff", "title": "Local Descriptors Optimized for Average Precision", "comments": "13 pages, 8 figures. IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extraction of local feature descriptors is a vital stage in the solution\npipelines for numerous computer vision tasks. Learning-based approaches improve\nperformance in certain tasks, but still cannot replace handcrafted features in\ngeneral. In this paper, we improve the learning of local feature descriptors by\noptimizing the performance of descriptor matching, which is a common stage that\nfollows descriptor extraction in local feature based pipelines, and can be\nformulated as nearest neighbor retrieval. Specifically, we directly optimize a\nranking-based retrieval performance metric, Average Precision, using deep\nneural networks. This general-purpose solution can also be viewed as a listwise\nlearning to rank approach, which is advantageous compared to recent local\nranking approaches. On standard benchmarks, descriptors learned with our\nformulation achieve state-of-the-art results in patch verification, patch\nretrieval, and image matching.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 07:08:24 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 19:59:23 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["He", "Kun", ""], ["Lu", "Yan", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1804.05338", "submitter": "Jo Schlemper", "authors": "Jo Schlemper, Ozan Oktay, Liang Chen, Jacqueline Matthew, Caroline\n  Knight, Bernhard Kainz, Ben Glocker, Daniel Rueckert", "title": "Attention-Gated Networks for Improving Ultrasound Scan Plane Detection", "comments": "Submitted to MIDL2018 (OpenReview:\n  https://openreview.net/forum?id=BJtn7-3sM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we apply an attention-gated network to real-time automated scan\nplane detection for fetal ultrasound screening. Scan plane detection in fetal\nultrasound is a challenging problem due the poor image quality resulting in low\ninterpretability for both clinicians and automated algorithms. To solve this,\nwe propose incorporating self-gated soft-attention mechanisms. A soft-attention\nmechanism generates a gating signal that is end-to-end trainable, which allows\nthe network to contextualise local information useful for prediction. The\nproposed attention mechanism is generic and it can be easily incorporated into\nany existing classification architectures, while only requiring a few\nadditional parameters. We show that, when the base network has a high capacity,\nthe incorporated attention mechanism can provide efficient object localisation\nwhile improving the overall performance. When the base network has a low\ncapacity, the method greatly outperforms the baseline approach and\nsignificantly reduces false positives. Lastly, the generated attention maps\nallow us to understand the model's reasoning process, which can also be used\nfor weakly supervised object localisation.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 11:15:28 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Schlemper", "Jo", ""], ["Oktay", "Ozan", ""], ["Chen", "Liang", ""], ["Matthew", "Jacqueline", ""], ["Knight", "Caroline", ""], ["Kainz", "Bernhard", ""], ["Glocker", "Ben", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1804.05340", "submitter": "Wenqi Liu", "authors": "Wenqi Liu and Kun Zeng", "title": "SparseNet: A Sparse DenseNet for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have made remarkable progresses on various computer\nvision tasks. Recent works have shown that depth, width and shortcut\nconnections of networks are all vital to their performances. In this paper, we\nintroduce a method to sparsify DenseNet which can reduce connections of a\nL-layer DenseNet from O(L^2) to O(L), and thus we can simultaneously increase\ndepth, width and connections of neural networks in a more parameter-efficient\nand computation-efficient way. Moreover, an attention module is introduced to\nfurther boost our network's performance. We denote our network as SparseNet. We\nevaluate SparseNet on datasets of CIFAR(including CIFAR10 and CIFAR100) and\nSVHN. Experiments show that SparseNet can obtain improvements over the\nstate-of-the-art on CIFAR10 and SVHN. Furthermore, while achieving comparable\nperformances as DenseNet on these datasets, SparseNet is x2.6 smaller and x3.7\nfaster than the original DenseNet.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 11:29:30 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Liu", "Wenqi", ""], ["Zeng", "Kun", ""]]}, {"id": "1804.05370", "submitter": "Jonghye Woo", "authors": "Jonghye Woo, Jerry L. Prince, Maureen Stone, Fangxu Xing, Arnold\n  Gomez, Jordan R. Green, Christopher J. Hartnick, Thomas J. Brady, Timothy G.\n  Reese, Van J. Wedeen, Georges El Fakhri", "title": "A Sparse Non-negative Matrix Factorization Framework for Identifying\n  Functional Units of Tongue Behavior from MRI", "comments": "Accepted at IEEE TMI (https://ieeexplore.ieee.org/document/8467354)", "journal-ref": null, "doi": "10.1109/TMI.2018.2870939", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Muscle coordination patterns of lingual behaviors are synergies generated by\ndeforming local muscle groups in a variety of ways. Functional units are\nfunctional muscle groups of local structural elements within the tongue that\ncompress, expand, and move in a cohesive and consistent manner. Identifying the\nfunctional units using tagged-Magnetic Resonance Imaging (MRI) sheds light on\nthe mechanisms of normal and pathological muscle coordination patterns,\nyielding improvement in surgical planning, treatment, or rehabilitation\nprocedures. Here, to mine this information, we propose a matrix factorization\nand probabilistic graphical model framework to produce building blocks and\ntheir associated weighting map using motion quantities extracted from\ntagged-MRI. Our tagged-MRI imaging and accurate voxel-level tracking provide\npreviously unavailable internal tongue motion patterns, thus revealing the\ninner workings of the tongue during speech or other lingual behaviors. We then\nemploy spectral clustering on the weighting map to identify the cohesive\nregions defined by the tongue motion that may involve multiple or undocumented\nregions. To evaluate our method, we perform a series of experiments. We first\nuse two-dimensional images and synthetic data to demonstrate the accuracy of\nour method. We then use three-dimensional synthetic and \\textit{in vivo} tongue\nmotion data using protrusion and simple speech tasks to identify\nsubject-specific and data-driven functional units of the tongue in localized\nregions.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 15:32:11 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 02:32:20 GMT"}, {"version": "v3", "created": "Sat, 29 Sep 2018 17:13:08 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Woo", "Jonghye", ""], ["Prince", "Jerry L.", ""], ["Stone", "Maureen", ""], ["Xing", "Fangxu", ""], ["Gomez", "Arnold", ""], ["Green", "Jordan R.", ""], ["Hartnick", "Christopher J.", ""], ["Brady", "Thomas J.", ""], ["Reese", "Timothy G.", ""], ["Wedeen", "Van J.", ""], ["Fakhri", "Georges El", ""]]}, {"id": "1804.05406", "submitter": "Chongsheng Cheng", "authors": "Chongsheng Cheng and Zhigang Shen", "title": "Detecting Concrete Abnormality Using Time-series Thermal Imaging and\n  Supervised Learning", "comments": "Accepted as Poster for 98th Annual Meeting of Transportation Research\n  Board (TRB)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nondestructive detecting defects (NDD) in concrete structures have been\nexplored for decades. Although limited successes were reported, major\nlimitations still exist. The major limitations are the high noises to signal\nratio created from the environmental factors, such as cloud, shadow, water,\nsurface texture etc. and the decision making still relies on the engineering\njudgment of interpretation of image content. Time-series approach, such as\nprinciple component thermography approach has been experimented with some\nimproved results. Recent progress in image processing using machine learning\napproach made it possible for detecting defects thermal features in more\nquantitative ways. In this paper, we provide a procedure to represent the\nthermal feature in the time domain by principal component analysis and regress\nthe prediction of detection by two schemes of supervised learning models. Three\nindependent experiments were conducted in a similar laboratory setup but varied\nin conditions to illustrate the performance and generalization of models.\nResults showed the effectiveness for the detection purpose with appropriate\ntuning for parameters. Future studies will focus on implementing more\nsophisticated structured models to handle more realistic cases under natural\nconditions.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 18:57:11 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 17:02:02 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Cheng", "Chongsheng", ""], ["Shen", "Zhigang", ""]]}, {"id": "1804.05422", "submitter": "Georges Younes Mr.", "authors": "Georges Younes, Daniel Asmar and John Zelek", "title": "FDMO: Feature Assisted Direct Monocular Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Odometry (VO) can be categorized as being either direct or feature\nbased. When the system is calibrated photometrically, and images are captured\nat high rates, direct methods have shown to outperform feature-based ones in\nterms of accuracy and processing time; they are also more robust to failure in\nfeature-deprived environments. On the downside, Direct methods rely on\nheuristic motion models to seed the estimation of camera motion between frames;\nin the event that these models are violated (e.g., erratic motion), Direct\nmethods easily fail. This paper proposes a novel system entitled FDMO (Feature\nassisted Direct Monocular Odometry), which complements the advantages of both\ndirect and featured based techniques. FDMO bootstraps indirect feature tracking\nupon the sub-pixel accurate localized direct keyframes only when failure modes\n(e.g., large baselines) of direct tracking occur. Control returns back to\ndirect odometry when these conditions are no longer violated. Efficiencies are\nintroduced to help FDMO perform in real time. FDMO shows significant drift\n(alignment, rotation & scale) reduction when compared to DSO & ORB SLAM when\nevaluated using the TumMono and EuroC datasets.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 20:24:23 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Younes", "Georges", ""], ["Asmar", "Daniel", ""], ["Zelek", "John", ""]]}, {"id": "1804.05427", "submitter": "Kuldeep Kumar", "authors": "Kuldeep Kumar, Kaleem Siddiqi, and Christian Desrosiers", "title": "White matter fiber analysis using kernel dictionary learning and\n  sparsity priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Diffusion magnetic resonance imaging, a non-invasive tool to infer white\nmatter fiber connections, produces a large number of streamlines containing a\nwealth of information on structural connectivity. The size of these\ntractography outputs makes further analyses complex, creating a need for\nmethods to group streamlines into meaningful bundles. In this work, we address\nthis by proposing a set of kernel dictionary learning and sparsity priors based\nmethods. Proposed frameworks include L-0 norm, group sparsity, as well as\nmanifold regularization prior. The proposed methods allow streamlines to be\nassigned to more than one bundle, making it more robust to overlapping bundles\nand inter-subject variations. We evaluate the performance of our method on a\nlabeled set and data from Human Connectome Project. Results highlight the\nability of our method to group streamlines into plausible bundles and\nillustrate the impact of sparsity priors on the performance of the proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 20:48:04 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Kumar", "Kuldeep", ""], ["Siddiqi", "Kaleem", ""], ["Desrosiers", "Christian", ""]]}, {"id": "1804.05448", "submitter": "Xin Wang", "authors": "Xin Wang, Yuan-Fang Wang, William Yang Wang", "title": "Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal\n  Attentions for Video Captioning", "comments": "NAACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge for video captioning is to combine audio and visual cues.\nExisting multi-modal fusion methods have shown encouraging results in video\nunderstanding. However, the temporal structures of multiple modalities at\ndifferent granularities are rarely explored, and how to selectively fuse the\nmulti-modal representations at different levels of details remains uncharted.\nIn this paper, we propose a novel hierarchically aligned cross-modal attention\n(HACA) framework to learn and selectively fuse both global and local temporal\ndynamics of different modalities. Furthermore, for the first time, we validate\nthe superior performance of the deep audio features on the video captioning\ntask. Finally, our HACA model significantly outperforms the previous best\nsystems and achieves new state-of-the-art results on the widely used MSR-VTT\ndataset.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 23:04:57 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Wang", "Xin", ""], ["Wang", "Yuan-Fang", ""], ["Wang", "William Yang", ""]]}, {"id": "1804.05459", "submitter": "Kamal Sehairi", "authors": "Kamal Sehairi, Chouireb Fatima and Jean Meunier", "title": "Comparative study of motion detection methods for video surveillance\n  systems", "comments": "69 pages, 18 figures, journal paper", "journal-ref": "J. Electron. Imaging 26(2), 023025 (2017)", "doi": "10.1117/1.JEI.26.2.023025", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The objective of this study is to compare several change detection methods\nfor a mono static camera and identify the best method for different complex\nenvironments and backgrounds in indoor and outdoor scenes. To this end, we used\nthe CDnet video dataset as a benchmark that consists of many challenging\nproblems, ranging from basic simple scenes to complex scenes affected by bad\nweather and dynamic backgrounds. Twelve change detection methods, ranging from\nsimple temporal differencing to more sophisticated methods, were tested and\nseveral performance metrics were used to precisely evaluate the results.\nBecause most of the considered methods have not previously been evaluated on\nthis recent large scale dataset, this work compares these methods to fill a\nlack in the literature, and thus this evaluation joins as complementary\ncompared with the previous comparative evaluations. Our experimental results\nshow that there is no perfect method for all challenging cases, each method\nperforms well in certain cases and fails in others. However, this study enables\nthe user to identify the most suitable method for his or her needs.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 00:51:30 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Sehairi", "Kamal", ""], ["Fatima", "Chouireb", ""], ["Meunier", "Jean", ""]]}, {"id": "1804.05469", "submitter": "Kai Xu", "authors": "Chengjie Niu, Jun Li and Kai Xu", "title": "Im2Struct: Recovering 3D Shape Structure from a Single RGB Image", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to recover 3D shape structures from single RGB images, where\nstructure refers to shape parts represented by cuboids and part relations\nencompassing connectivity and symmetry. Given a single 2D image with an object\ndepicted, our goal is automatically recover a cuboid structure of the object\nparts as well as their mutual relations. We develop a convolutional-recursive\nauto-encoder comprised of structure parsing of a 2D image followed by structure\nrecovering of a cuboid hierarchy. The encoder is achieved by a multi-scale\nconvolutional network trained with the task of shape contour estimation,\nthereby learning to discern object structures in various forms and scales. The\ndecoder fuses the features of the structure parsing network and the original\nimage, and recursively decodes a hierarchy of cuboids. Since the decoder\nnetwork is learned to recover part relations including connectivity and\nsymmetry explicitly, the plausibility and generality of part structure recovery\ncan be ensured. The two networks are jointly trained using the training data of\ncontour-mask and cuboid structure pairs. Such pairs are generated by rendering\nstock 3D CAD models coming with part segmentation. Our method achieves\nunprecedentedly faithful and detailed recovery of diverse 3D part structures\nfrom single-view 2D images. We demonstrate two applications of our method\nincluding structure-guided completion of 3D volumes reconstructed from\nsingle-view images and structure-aware interactive editing of 2D images.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 01:32:30 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Niu", "Chengjie", ""], ["Li", "Jun", ""], ["Xu", "Kai", ""]]}, {"id": "1804.05470", "submitter": "Anant Gupta", "authors": "Laura Graesser and Anant Gupta", "title": "Composable Unpaired Image to Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been remarkable recent work in unpaired image-to-image translation.\nHowever, they're restricted to translation on single pairs of distributions,\nwith some exceptions. In this study, we extend one of these works to a scalable\nmultidistribution translation mechanism. Our translation models not only\nconverts from one distribution to another but can be stacked to create\ncomposite translation functions. We show that this composite property makes it\npossible to generate images with characteristics not seen in the training set.\nWe also propose a decoupled training mechanism to train multiple distributions\nseparately, which we show, generates better samples than isolated joint\ntraining. Further, we do a qualitative and quantitative analysis to assess the\nplausibility of the samples. The code is made available at\nhttps://github.com/lgraesser/im2im2im.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 01:38:11 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Graesser", "Laura", ""], ["Gupta", "Anant", ""]]}, {"id": "1804.05472", "submitter": "Kai Chen", "authors": "Kai Chen, Jiaqi Wang, Shuo Yang, Xingcheng Zhang, Yuanjun Xiong, Chen\n  Change Loy, Dahua Lin", "title": "Optimizing Video Object Detection via a Scale-Time Lattice", "comments": "Accepted to CVPR 2018. Project page:\n  http://mmlab.ie.cuhk.edu.hk/projects/ST-Lattice/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performance object detection relies on expensive convolutional networks\nto compute features, often leading to significant challenges in applications,\ne.g. those that require detecting objects from video streams in real time. The\nkey to this problem is to trade accuracy for efficiency in an effective way,\ni.e. reducing the computing cost while maintaining competitive performance. To\nseek a good balance, previous efforts usually focus on optimizing the model\narchitectures. This paper explores an alternative approach, that is, to\nreallocate the computation over a scale-time space. The basic idea is to\nperform expensive detection sparsely and propagate the results across both\nscales and time with substantially cheaper networks, by exploiting the strong\ncorrelations among them. Specifically, we present a unified framework that\nintegrates detection, temporal propagation, and across-scale refinement on a\nScale-Time Lattice. On this framework, one can explore various strategies to\nbalance performance and cost. Taking advantage of this flexibility, we further\ndevelop an adaptive scheme with the detector invoked on demand and thus obtain\nimproved tradeoff. On ImageNet VID dataset, the proposed method can achieve a\ncompetitive mAP 79.6% at 20 fps, or 79.0% at 62 fps as a performance/speed\ntradeoff.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 01:52:01 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Chen", "Kai", ""], ["Wang", "Jiaqi", ""], ["Yang", "Shuo", ""], ["Zhang", "Xingcheng", ""], ["Xiong", "Yuanjun", ""], ["Loy", "Chen Change", ""], ["Lin", "Dahua", ""]]}, {"id": "1804.05482", "submitter": "Ignacio Ramirez", "authors": "Ignacio Ramirez", "title": "Binary Matrix Factorization via Dictionary Learning", "comments": "submitted for review to IEEE JSTSP on April 15th, 2018", "journal-ref": null, "doi": "10.1109/JSTSP.2018.2875674", "report-no": null, "categories": "stat.ML cs.CV cs.IR cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Matrix factorization is a key tool in data analysis; its applications include\nrecommender systems, correlation analysis, signal processing, among others.\nBinary matrices are a particular case which has received significant attention\nfor over thirty years, especially within the field of data mining. Dictionary\nlearning refers to a family of methods for learning overcomplete basis (also\ncalled frames) in order to efficiently encode samples of a given type; this\narea, now also about twenty years old, was mostly developed within the signal\nprocessing field. In this work we propose two binary matrix factorization\nmethods based on a binary adaptation of the dictionary learning paradigm to\nbinary matrices. The proposed algorithms focus on speed and scalability; they\nwork with binary factors combined with bit-wise operations and a few auxiliary\ninteger ones. Furthermore, the methods are readily applicable to online binary\nmatrix factorization. Another important issue in matrix factorization is the\nchoice of rank for the factors; we address this model selection problem with an\nefficient method based on the Minimum Description Length principle. Our\npreliminary results show that the proposed methods are effective at producing\ninterpretable factorizations of various data types of different nature.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 02:36:24 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 01:13:05 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Ramirez", "Ignacio", ""]]}, {"id": "1804.05483", "submitter": "Shuming Jiao", "authors": "Shuming Jiao, Changyuan Zhou, Yishi Shi, Wenbin Zou, Xia Li", "title": "Review on Optical Image Hiding and Watermarking Techniques", "comments": null, "journal-ref": null, "doi": "10.1016/j.optlastec.2018.08.011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information security is a critical issue in modern society and image\nwatermarking can effectively prevent unauthorized information access. Optical\nimage watermarking techniques generally have advantages of parallel high-speed\nprocessing and multi-dimensional capabilities compared with digital approaches.\nThis paper provides a comprehensive review on the research works related to\noptical image hiding and watermarking techniques conducted in the past decade.\nThe past research works are focused on two major aspects, various optical\nsystems for image hiding and the methods for embedding optical system output\ninto a host image. A summary of the state-of-the-art works is made from these\ntwo perspectives.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 02:43:38 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Jiao", "Shuming", ""], ["Zhou", "Changyuan", ""], ["Shi", "Yishi", ""], ["Zou", "Wenbin", ""], ["Li", "Xia", ""]]}, {"id": "1804.05493", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Lina Yao, Chaoran Huang, Sen Wang, Mingkui Tan, Guodong\n  Long, Can Wang", "title": "Multi-modality Sensor Data Classification with Selective Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal wearable sensor data classification plays an important role in\nubiquitous computing and has a wide range of applications in scenarios from\nhealthcare to entertainment. However, most existing work in this field employs\ndomain-specific approaches and is thus ineffective in complex sit- uations\nwhere multi-modality sensor data are col- lected. Moreover, the wearable sensor\ndata are less informative than the conventional data such as texts or images.\nIn this paper, to improve the adapt- ability of such classification methods\nacross differ- ent application domains, we turn this classification task into a\ngame and apply a deep reinforcement learning scheme to deal with complex\nsituations dynamically. Additionally, we introduce a selective attention\nmechanism into the reinforcement learn- ing scheme to focus on the crucial\ndimensions of the data. This mechanism helps to capture extra information from\nthe signal and thus it is able to significantly improve the discriminative\npower of the classifier. We carry out several experiments on three wearable\nsensor datasets and demonstrate the competitive performance of the proposed\napproach compared to several state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 03:40:41 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 14:12:52 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Zhang", "Xiang", ""], ["Yao", "Lina", ""], ["Huang", "Chaoran", ""], ["Wang", "Sen", ""], ["Tan", "Mingkui", ""], ["Long", "Guodong", ""], ["Wang", "Can", ""]]}, {"id": "1804.05515", "submitter": "Hongyu Xu", "authors": "Hongyu Xu, Zhangyang Wang, Haichuan Yang, Ding Liu and Ji Liu", "title": "Learning Simple Thresholded Features with Sparse Support Recovery", "comments": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The thresholded feature has recently emerged as an extremely efficient, yet\nrough empirical approximation, of the time-consuming sparse coding inference\nprocess. Such an approximation has not yet been rigorously examined, and\nstandard dictionaries often lead to non-optimal performance when used for\ncomputing thresholded features. In this paper, we first present two theoretical\nrecovery guarantees for the thresholded feature to exactly recover the nonzero\nsupport of the sparse code. Motivated by them, we then formulate the Dictionary\nLearning for Thresholded Features (DLTF) model, which learns an optimized\ndictionary for applying the thresholded feature. In particular, for the $(k,\n2)$ norm involved, a novel proximal operator with log-linear time complexity\n$O(m\\log m)$ is derived. We evaluate the performance of DLTF on a vast range of\nsynthetic and real-data tasks, where DLTF demonstrates remarkable efficiency,\neffectiveness and robustness in all experiments. In addition, we briefly\ndiscuss the potential link between DLTF and deep learning building blocks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 06:20:55 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 22:10:17 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Xu", "Hongyu", ""], ["Wang", "Zhangyang", ""], ["Yang", "Haichuan", ""], ["Liu", "Ding", ""], ["Liu", "Ji", ""]]}, {"id": "1804.05526", "submitter": "Sourav Garg", "authors": "Sourav Garg, Niko Suenderhauf and Michael Milford", "title": "LoST? Appearance-Invariant Place Recognition for Opposite Viewpoints\n  using Visual Semantics", "comments": "Accepted for Robotics: Science and Systems (RSS) 2018. Source code\n  now available at https://github.com/oravus/lostX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human visual scene understanding is so remarkable that we are able to\nrecognize a revisited place when entering it from the opposite direction it was\nfirst visited, even in the presence of extreme variations in appearance. This\ncapability is especially apparent during driving: a human driver can recognize\nwhere they are when travelling in the reverse direction along a route for the\nfirst time, without having to turn back and look. The difficulty of this\nproblem exceeds any addressed in past appearance- and viewpoint-invariant\nvisual place recognition (VPR) research, in part because large parts of the\nscene are not commonly observable from opposite directions. Consequently, as\nshown in this paper, the precision-recall performance of current\nstate-of-the-art viewpoint- and appearance-invariant VPR techniques is orders\nof magnitude below what would be usable in a closed-loop system. Current\nengineered solutions predominantly rely on panoramic camera or LIDAR sensing\nsetups; an eminently suitable engineering solution but one that is clearly very\ndifferent to how humans navigate, which also has implications for how naturally\nhumans could interact and communicate with the navigation system. In this paper\nwe develop a suite of novel semantic- and appearance-based techniques to enable\nfor the first time high performance place recognition in this challenging\nscenario. We first propose a novel Local Semantic Tensor (LoST) descriptor of\nimages using the convolutional feature maps from a state-of-the-art dense\nsemantic segmentation network. Then, to verify the spatial semantic arrangement\nof the top matching candidates, we develop a novel approach for mining\nsemantically-salient keypoint correspondences.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 07:26:52 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 02:48:41 GMT"}, {"version": "v3", "created": "Sat, 26 May 2018 10:34:17 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Garg", "Sourav", ""], ["Suenderhauf", "Niko", ""], ["Milford", "Michael", ""]]}, {"id": "1804.05535", "submitter": "Peng Gao", "authors": "Peng Gao, Ruyue Yuan, Zhicong Lin, Linsheng Zhang, Yan Zhang", "title": "A Novel Low-cost FPGA-based Real-time Object Tracking System", "comments": "Accepted by ASICON 2017", "journal-ref": null, "doi": "10.1109/ASICON.2017.8252560", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current visual object tracking system, the CPU or GPU-based visual object\ntracking systems have high computational cost and consume a prohibitive amount\nof power. Therefore, in this paper, to reduce the computational burden of the\nCamshift algorithm, we propose a novel visual object tracking algorithm by\nexploiting the properties of the binary classifier and Kalman predictor.\nMoreover, we present a low-cost FPGA-based real-time object tracking hardware\narchitecture. Extensive evaluations on OTB benchmark demonstrate that the\nproposed system has extremely compelling real-time, stability and robustness.\nThe evaluation results show that the accuracy of our algorithm is about 48%,\nand the average speed is about 309 frames per second.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 08:04:34 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 13:55:04 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Gao", "Peng", ""], ["Yuan", "Ruyue", ""], ["Lin", "Zhicong", ""], ["Zhang", "Linsheng", ""], ["Zhang", "Yan", ""]]}, {"id": "1804.05541", "submitter": "Peng Gao", "authors": "Yan Zhang, Peng Gao, Xiao-Qing Li", "title": "A Novel Parallel Ray-Casting Algorithm", "comments": "Accepted by ICCWAMTIP 2016", "journal-ref": null, "doi": "10.1109/ICCWAMTIP.2016.8079804", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ray-Casting algorithm is an important method for fast real-time surface\ndisplay from 3D medical images. Based on the Ray-Casting algorithm, a novel\nparallel Ray-Casting algorithm is proposed in this paper. A novel operation is\nintroduced and defined as a star operation, and star operations can be computed\nin parallel in the proposed algorithm compared with the serial chain of star\noperations in the Ray-Casting algorithm. The computation complexity of the\nproposed algorithm is reduced from $O(n)$ to $O(\\log^n_2)$.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 08:15:38 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 13:40:00 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Zhang", "Yan", ""], ["Gao", "Peng", ""], ["Li", "Xiao-Qing", ""]]}, {"id": "1804.05546", "submitter": "Ronny Hug", "authors": "Ronny Hug, Stefan Becker, Wolfgang H\\\"ubner and Michael Arens", "title": "Particle-based pedestrian path prediction using LSTM-MDL models", "comments": "Accepted at ITSC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks are able to learn complex long-term relationships\nfrom sequential data and output a pdf over the state space. Therefore,\nrecurrent models are a natural choice to address path prediction tasks, where a\ntrained model is used to generate future expectations from past observations.\nWhen applied to security applications, like predicting the path of pedestrians\nfor risk assessment, a point-wise greedy (ML) evaluation of the output pdf is\nnot feasible, since the environment often allows multiple choices. Therefore, a\nrobust risk assessment has to take all options into account, even if they are\noverall not very likely.\n  Towards this end, a combination of particle filter sampling strategies and a\nLSTM-MDL model is proposed to address a multi-modal path prediction task. The\ncapabilities and viability of the proposed approach are evaluated on several\nsynthetic test conditions, yielding the counter-intuitive result that the\nsimplest approach performs best. Further, the feasibility of the proposed\napproach is illustrated on several real world scenes.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 08:27:24 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 21:02:10 GMT"}, {"version": "v3", "created": "Wed, 29 Aug 2018 07:02:13 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Hug", "Ronny", ""], ["Becker", "Stefan", ""], ["H\u00fcbner", "Wolfgang", ""], ["Arens", "Michael", ""]]}, {"id": "1804.05624", "submitter": "Ziang Cheng", "authors": "Ziang Cheng, Shaodi You, Viorela Ila, Hongdong Li", "title": "Semantic Single-Image Dehazing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-image haze-removal is challenging due to limited information contained\nin one single image. Previous solutions largely rely on handcrafted priors to\ncompensate for this deficiency. Recent convolutional neural network (CNN)\nmodels have been used to learn haze-related priors but they ultimately work as\nadvanced image filters. In this paper we propose a novel semantic ap- proach\ntowards single image haze removal. Unlike existing methods, we infer color\npriors based on extracted semantic features. We argue that semantic context can\nbe exploited to give informative cues for (a) learning color prior on clean\nimage and (b) estimating ambient illumination. This design allowed our model to\nrecover clean images from challenging cases with strong ambiguity, e.g.\nsaturated illumination color and sky regions in image. In experiments, we\nvalidate our ap- proach upon synthetic and real hazy images, where our method\nshowed superior performance over state-of-the-art approaches, suggesting\nsemantic information facilitates the haze removal task.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 11:59:31 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 03:39:52 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Cheng", "Ziang", ""], ["You", "Shaodi", ""], ["Ila", "Viorela", ""], ["Li", "Hongdong", ""]]}, {"id": "1804.05625", "submitter": "Lukas von Stumberg", "authors": "Lukas von Stumberg, Vladyslav Usenko, Daniel Cremers", "title": "Direct Sparse Visual-Inertial Odometry using Dynamic Marginalization", "comments": null, "journal-ref": null, "doi": "10.1109/ICRA.2018.8462905", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present VI-DSO, a novel approach for visual-inertial odometry, which\njointly estimates camera poses and sparse scene geometry by minimizing\nphotometric and IMU measurement errors in a combined energy functional. The\nvisual part of the system performs a bundle-adjustment like optimization on a\nsparse set of points, but unlike key-point based systems it directly minimizes\na photometric error. This makes it possible for the system to track not only\ncorners, but any pixels with large enough intensity gradients. IMU information\nis accumulated between several frames using measurement preintegration, and is\ninserted into the optimization as an additional constraint between keyframes.\nWe explicitly include scale and gravity direction into our model and jointly\noptimize them together with other variables such as poses. As the scale is\noften not immediately observable using IMU data this allows us to initialize\nour visual-inertial system with an arbitrary scale instead of having to delay\nthe initialization until everything is observable. We perform partial\nmarginalization of old variables so that updates can be computed in a\nreasonable time. In order to keep the system consistent we propose a novel\nstrategy which we call \"dynamic marginalization\". This technique allows us to\nuse partial marginalization even in cases where the initial scale estimate is\nfar from the optimum. We evaluate our method on the challenging EuRoC dataset,\nshowing that VI-DSO outperforms the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 12:00:56 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["von Stumberg", "Lukas", ""], ["Usenko", "Vladyslav", ""], ["Cremers", "Daniel", ""]]}, {"id": "1804.05651", "submitter": "Thomas Mensink", "authors": "Ysbrand Galama and Thomas Mensink", "title": "IterGANs: Iterative GANs to Learn and Control 3D Object Transformation", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2019.102803", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in learning visual representations which allow for 3D\nmanipulations of visual objects based on a single 2D image. We cast this into\nan image-to-image transformation task, and propose Iterative Generative\nAdversarial Networks (IterGANs) which iteratively transform an input image into\nan output image. Our models learn a visual representation that can be used for\nobjects seen in training, but also for never seen objects. Since object\nmanipulation requires a full understanding of the geometry and appearance of\nthe object, our IterGANs learn an implicit 3D model and a full appearance model\nof the object, which are both inferred from a single (test) image. Two\nadvantages of IterGANs are that the intermediate generated images can be used\nfor an additional supervision signal, even in an unsupervised fashion, and that\nthe number of iterations can be used as a control signal to steer the\ntransformation. Experiments on rotated objects and scenes show how IterGANs\nhelp with the generation process.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 13:08:58 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 07:03:52 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Galama", "Ysbrand", ""], ["Mensink", "Thomas", ""]]}, {"id": "1804.05653", "submitter": "Ruben Villegas", "authors": "Ruben Villegas, Jimei Yang, Duygu Ceylan, Honglak Lee", "title": "Neural Kinematic Networks for Unsupervised Motion Retargetting", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a recurrent neural network architecture with a Forward Kinematics\nlayer and cycle consistency based adversarial training objective for\nunsupervised motion retargetting. Our network captures the high-level\nproperties of an input motion by the forward kinematics layer, and adapts them\nto a target character with different skeleton bone lengths (e.g., shorter,\nlonger arms etc.). Collecting paired motion training sequences from different\ncharacters is expensive. Instead, our network utilizes cycle consistency to\nlearn to solve the Inverse Kinematics problem in an unsupervised manner. Our\nmethod works online, i.e., it adapts the motion sequence on-the-fly as new\nframes are received. In our experiments, we use the Mixamo animation data to\ntest our method for a variety of motions and characters and achieve\nstate-of-the-art results. We also demonstrate motion retargetting from\nmonocular human videos to 3D characters using an off-the-shelf 3D pose\nestimator.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 13:15:50 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Villegas", "Ruben", ""], ["Yang", "Jimei", ""], ["Ceylan", "Duygu", ""], ["Lee", "Honglak", ""]]}, {"id": "1804.05661", "submitter": "Thameur Dhieb", "authors": "Thameur Dhieb, Sourour Njah, Houcine Boubaker, Wael Ouarda, Mounir Ben\n  Ayed, and Adel M. Alimi", "title": "An Extended Beta-Elliptic Model and Fuzzy Elementary Perceptual Codes\n  for Online Multilingual Writer Identification using Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actually, the ability to identify the documents authors provides more chances\nfor using these documents for various purposes. In this paper, we present a new\neffective biometric writer identification system from online handwriting. The\nsystem consists of the preprocessing and the segmentation of online handwriting\ninto a sequence of Beta strokes in a first step. Then, from each stroke, we\nextract a set of static and dynamic features from new proposed model that we\ncalled Extended Beta-Elliptic model and from the Fuzzy Elementary Perceptual\nCodes. Next, all the segments which are composed of N consecutive strokes are\ncategorized into groups and subgroups according to their position and their\ngeometric characteristics. Finally, Deep Neural Network is used as classifier.\nExperimental results reveal that the proposed system achieves interesting\nresults as compared to those of the existing writer identification systems on\nLatin and Arabic scripts.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 13:27:11 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 16:10:40 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 19:14:28 GMT"}, {"version": "v4", "created": "Sat, 10 Nov 2018 11:28:36 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Dhieb", "Thameur", ""], ["Njah", "Sourour", ""], ["Boubaker", "Houcine", ""], ["Ouarda", "Wael", ""], ["Ayed", "Mounir Ben", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1804.05669", "submitter": "Takumi Ichimura", "authors": "Takumi Ichimura, Shin Kamada", "title": "A Clonal Selection Algorithm with Levenshtein Distance based Image\n  Similarity in Multidimensional Subjective Tourist Information and Discovery\n  of Cryptic Spots by Interactive GHSOM", "comments": "6 pages, 9 figures, Proc. of 2013 IEEE International Conference on\n  Systems, Man, and Cybernetics (IEEE SMC 2013). arXiv admin note: substantial\n  text overlap with arXiv:1804.03993, arXiv:1804.02628, arXiv:1804.02816", "journal-ref": null, "doi": "10.1109/SMC.2013.357", "report-no": null, "categories": "cs.IR cs.CV cs.SI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile Phone based Participatory Sensing (MPPS) system involves a community\nof users sending personal information and participating in autonomous sensing\nthrough their mobile phones. Sensed data can also be obtained from external\nsensing devices that can communicate wirelessly to the phone. Our developed\ntourist subjective data collection system with Android smartphone can determine\nthe filtering rules to provide the important information of sightseeing spot.\nThe rules are automatically generated by Interactive Growing Hierarchical SOM.\nHowever, the filtering rules related to photograph were not generated, because\nthe extraction of the specified characteristics from images cannot be realized.\nWe propose the effective method of the Levenshtein distance to deduce the\nspatial proximity of image viewpoints and thus determine the specified pattern\nin which images should be processed. To verify the proposed method, some\nexperiments to classify the subjective data with images are executed by\nInteractive GHSOM and Clonal Selection Algorithm with Immunological Memory\nCells in this paper.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 08:18:16 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Ichimura", "Takumi", ""], ["Kamada", "Shin", ""]]}, {"id": "1804.05712", "submitter": "Hans Pinckaers", "authors": "Hans Pinckaers, Geert Litjens", "title": "Training convolutional neural networks with megapixel images", "comments": "Submitted to MIDL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To train deep convolutional neural networks, the input data and the\nintermediate activations need to be kept in memory to calculate the gradient\ndescent step. Given the limited memory available in the current generation\naccelerator cards, this limits the maximum dimensions of the input data. We\ndemonstrate a method to train convolutional neural networks holding only parts\nof the image in memory while giving equivalent results. We quantitatively\ncompare this new way of training convolutional neural networks with\nconventional training. In addition, as a proof of concept, we train a\nconvolutional neural network with 64 megapixel images, which requires 97% less\nmemory than the conventional approach.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 14:52:22 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Pinckaers", "Hans", ""], ["Litjens", "Geert", ""]]}, {"id": "1804.05764", "submitter": "Samuel Remedios", "authors": "Samuel Remedios, Dzung L. Pham, John A. Butman, Snehashis Roy", "title": "Classifying magnetic resonance image modalities with convolutional\n  neural networks", "comments": "Github: https://github.com/sremedios/phinet", "journal-ref": null, "doi": "10.1117/12.2293943", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance (MR) imaging allows the acquisition of images with\ndifferent contrast properties depending on the acquisition protocol and the\nmagnetic properties of tissues. Many MR brain image processing techniques, such\nas tissue segmentation, require multiple MR contrasts as inputs, and each\ncontrast is treated differently. Thus it is advantageous to automate the\nidentification of image contrasts for various purposes, such as facilitating\nimage processing pipelines, and managing and maintaining large databases via\ncontent-based image retrieval (CBIR). Most automated CBIR techniques focus on a\ntwo-step process: extracting features from data and classifying the image based\non these features. We present a novel 3D deep convolutional neural network\n(CNN)-based method for MR image contrast classification. The proposed CNN\nautomatically identifies the MR contrast of an input brain image volume.\nSpecifically, we explored three classification problems: (1) identify\nT1-weighted (T1-w), T2-weighted (T2-w), and fluid-attenuated inversion recovery\n(FLAIR) contrasts, (2) identify pre vs post-contrast T1, (3) identify pre vs\npost-contrast FLAIR. A total of 3418 image volumes acquired from multiple sites\nand multiple scanners were used. To evaluate each task, the proposed model was\ntrained on 2137 images and tested on the remaining 1281 images. Results showed\nthat image volumes were correctly classified with 97.57% accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 16:08:39 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 01:47:16 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Remedios", "Samuel", ""], ["Pham", "Dzung L.", ""], ["Butman", "John A.", ""], ["Roy", "Snehashis", ""]]}, {"id": "1804.05788", "submitter": "Samarth Tripathi", "authors": "Samarth Tripathi, Sarthak Tripathi and Homayoon Beigi", "title": "Multi-Modal Emotion recognition on IEMOCAP Dataset using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition has become an important field of research in Human\nComputer Interactions as we improve upon the techniques for modelling the\nvarious aspects of behaviour. With the advancement of technology our\nunderstanding of emotions are advancing, there is a growing need for automatic\nemotion recognition systems. One of the directions the research is heading is\nthe use of Neural Networks which are adept at estimating complex functions that\ndepend on a large number and diverse source of input data. In this paper we\nattempt to exploit this effectiveness of Neural networks to enable us to\nperform multimodal Emotion recognition on IEMOCAP dataset using data from\nSpeech, Text, and Motion capture data from face expressions, rotation and hand\nmovements. Prior research has concentrated on Emotion detection from Speech on\nthe IEMOCAP dataset, but our approach is the first that uses the multiple modes\nof data offered by IEMOCAP for a more robust and accurate emotion detection.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 16:58:37 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 08:46:57 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 20:10:26 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Tripathi", "Samarth", ""], ["Tripathi", "Sarthak", ""], ["Beigi", "Homayoon", ""]]}, {"id": "1804.05790", "submitter": "Zhengqin Li", "authors": "Zhengqin Li, Kalyan Sunkavalli, Manmohan Chandraker", "title": "Materials for Masses: SVBRDF Acquisition with a Single Mobile Phone\n  Image", "comments": "submitted to European Conference on Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a material acquisition approach to recover the spatially-varying\nBRDF and normal map of a near-planar surface from a single image captured by a\nhandheld mobile phone camera. Our method images the surface under arbitrary\nenvironment lighting with the flash turned on, thereby avoiding shadows while\nsimultaneously capturing high-frequency specular highlights. We train a CNN to\nregress an SVBRDF and surface normals from this image. Our network is trained\nusing a large-scale SVBRDF dataset and designed to incorporate physical\ninsights for material estimation, including an in-network rendering layer to\nmodel appearance and a material classifier to provide additional supervision\nduring training. We refine the results from the network using a dense CRF\nmodule whose terms are designed specifically for our task. The framework is\ntrained end-to-end and produces high quality results for a variety of\nmaterials. We provide extensive ablation studies to evaluate our network on\nboth synthetic and real data, while demonstrating significant improvements in\ncomparisons with prior works.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 16:59:38 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Li", "Zhengqin", ""], ["Sunkavalli", "Kalyan", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1804.05805", "submitter": "Wenjie Ruan", "authors": "Wenjie Ruan, Min Wu, Youcheng Sun, Xiaowei Huang, Daniel Kroening,\n  Marta Kwiatkowska", "title": "Global Robustness Evaluation of Deep Neural Networks with Provable\n  Guarantees for the $L_0$ Norm", "comments": "42 Pages, Github: https://github.com/TrustAI/L0-TRE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deployment of deep neural networks (DNNs) in safety- or security-critical\nsystems requires provable guarantees on their correct behaviour. A common\nrequirement is robustness to adversarial perturbations in a neighbourhood\naround an input. In this paper we focus on the $L_0$ norm and aim to compute,\nfor a trained DNN and an input, the maximal radius of a safe norm ball around\nthe input within which there are no adversarial examples. Then we define global\nrobustness as an expectation of the maximal safe radius over a test data set.\nWe first show that the problem is NP-hard, and then propose an approximate\napproach to iteratively compute lower and upper bounds on the network's\nrobustness. The approach is \\emph{anytime}, i.e., it returns intermediate\nbounds and robustness estimates that are gradually, but strictly, improved as\nthe computation proceeds; \\emph{tensor-based}, i.e., the computation is\nconducted over a set of inputs simultaneously, instead of one by one, to enable\nefficient GPU computation; and has \\emph{provable guarantees}, i.e., both the\nbounds and the robustness estimates can converge to their optimal values.\nFinally, we demonstrate the utility of the proposed approach in practice to\ncompute tight bounds by applying and adapting the anytime algorithm to a set of\nchallenging problems, including global robustness evaluation, competitive $L_0$\nattacks, test case generation for DNNs, and local robustness evaluation on\nlarge-scale ImageNet DNNs. We release the code of all case studies via GitHub.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 17:24:51 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 16:57:22 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Ruan", "Wenjie", ""], ["Wu", "Min", ""], ["Sun", "Youcheng", ""], ["Huang", "Xiaowei", ""], ["Kroening", "Daniel", ""], ["Kwiatkowska", "Marta", ""]]}, {"id": "1804.05810", "submitter": "Shang-Tse Chen", "authors": "Shang-Tse Chen, Cory Cornelius, Jason Martin, Duen Horng Chau", "title": "ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object\n  Detector", "comments": null, "journal-ref": "Joint European Conference on Machine Learning and Knowledge\n  Discovery in Databases, pp. 52-68, 2018", "doi": "10.1007/978-3-030-10925-7_4", "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the ability to directly manipulate image pixels in the digital input\nspace, an adversary can easily generate imperceptible perturbations to fool a\nDeep Neural Network (DNN) image classifier, as demonstrated in prior work. In\nthis work, we propose ShapeShifter, an attack that tackles the more challenging\nproblem of crafting physical adversarial perturbations to fool image-based\nobject detectors like Faster R-CNN. Attacking an object detector is more\ndifficult than attacking an image classifier, as it needs to mislead the\nclassification results in multiple bounding boxes with different scales.\nExtending the digital attack to the physical world adds another layer of\ndifficulty, because it requires the perturbation to be robust enough to survive\nreal-world distortions due to different viewing distances and angles, lighting\nconditions, and camera limitations. We show that the Expectation over\nTransformation technique, which was originally proposed to enhance the\nrobustness of adversarial perturbations in image classification, can be\nsuccessfully adapted to the object detection setting. ShapeShifter can generate\nadversarially perturbed stop signs that are consistently mis-detected by Faster\nR-CNN as other objects, posing a potential threat to autonomous vehicles and\nother safety-critical computer vision systems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 17:29:43 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 02:22:39 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 03:41:44 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Chen", "Shang-Tse", ""], ["Cornelius", "Cory", ""], ["Martin", "Jason", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1804.05827", "submitter": "Zuxuan Wu", "authors": "Zuxuan Wu, Xintong Han, Yen-Liang Lin, Mustafa Gkhan Uzunbas, Tom\n  Goldstein, Ser Nam Lim, Larry S. Davis", "title": "DCAN: Dual Channel-wise Alignment Networks for Unsupervised Scene\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Harvesting dense pixel-level annotations to train deep neural networks for\nsemantic segmentation is extremely expensive and unwieldy at scale. While\nlearning from synthetic data where labels are readily available sounds\npromising, performance degrades significantly when testing on novel realistic\ndata due to domain discrepancies. We present Dual Channel-wise Alignment\nNetworks (DCAN), a simple yet effective approach to reduce domain shift at both\npixel-level and feature-level. Exploring statistics in each channel of CNN\nfeature maps, our framework performs channel-wise feature alignment, which\npreserves spatial structures and semantic information, in both an image\ngenerator and a segmentation network. In particular, given an image from the\nsource domain and unlabeled samples from the target domain, the generator\nsynthesizes new images on-the-fly to resemble samples from the target domain in\nappearance and the segmentation network further refines high-level features\nbefore predicting semantic maps, both of which leverage feature statistics of\nsampled images from the target domain. Unlike much recent and concurrent work\nrelying on adversarial training, our framework is lightweight and easy to\ntrain. Extensive experiments on adapting models trained on synthetic\nsegmentation benchmarks to real urban scenes demonstrate the effectiveness of\nthe proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 17:54:08 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Wu", "Zuxuan", ""], ["Han", "Xintong", ""], ["Lin", "Yen-Liang", ""], ["Uzunbas", "Mustafa Gkhan", ""], ["Goldstein", "Tom", ""], ["Lim", "Ser Nam", ""], ["Davis", "Larry S.", ""]]}, {"id": "1804.05830", "submitter": "Jifeng Dai", "authors": "Xizhou Zhu, Jifeng Dai, Xingchi Zhu, Yichen Wei, Lu Yuan", "title": "Towards High Performance Video Object Detection for Mobiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of video object detection on Desktop GPUs, its\narchitecture is still far too heavy for mobiles. It is also unclear whether the\nkey principles of sparse feature propagation and multi-frame feature\naggregation apply at very limited computational resources. In this paper, we\npresent a light weight network architecture for video object detection on\nmobiles. Light weight image object detector is applied on sparse key frames. A\nvery small network, Light Flow, is designed for establishing correspondence\nacross frames. A flow-guided GRU module is designed to effectively aggregate\nfeatures on key frames. For non-key frames, sparse feature propagation is\nperformed. The whole network can be trained end-to-end. The proposed system\nachieves 60.2% mAP score at speed of 25.6 fps on mobiles (e.g., HuaWei Mate 8).\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 17:59:36 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Zhu", "Xizhou", ""], ["Dai", "Jifeng", ""], ["Zhu", "Xingchi", ""], ["Wei", "Yichen", ""], ["Yuan", "Lu", ""]]}, {"id": "1804.05870", "submitter": "Rohit Pandey", "authors": "Rohit Pandey, Pavel Pidlypenskyi, Shuoran Yang, Christine Kaeser-Chen", "title": "Egocentric 6-DoF Tracking of Small Handheld Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Virtual and augmented reality technologies have seen significant growth in\nthe past few years. A key component of such systems is the ability to track the\npose of head mounted displays and controllers in 3D space. We tackle the\nproblem of efficient 6-DoF tracking of a handheld controller from egocentric\ncamera perspectives. We collected the HMD Controller dataset which consist of\nover 540,000 stereo image pairs labelled with the full 6-DoF pose of the\nhandheld controller. Our proposed SSD-AF-Stereo3D model achieves a mean average\nerror of 33.5 millimeters in 3D keypoint prediction and is used in conjunction\nwith an IMU sensor on the controller to enable 6-DoF tracking. We also present\nresults on approaches for model based full 6-DoF tracking. All our models\noperate under the strict constraints of real time mobile CPU inference.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 18:08:51 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Pandey", "Rohit", ""], ["Pidlypenskyi", "Pavel", ""], ["Yang", "Shuoran", ""], ["Kaeser-Chen", "Christine", ""]]}, {"id": "1804.05879", "submitter": "Eric Hofesmann", "authors": "Eric Hofesmann, Madan Ravi Ganesh, Jason J. Corso", "title": "M-PACT: An Open Source Platform for Repeatable Activity Classification\n  Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many hurdles that prevent the replication of existing work which\nhinders the development of new activity classification models. These hurdles\ninclude switching between multiple deep learning libraries and the development\nof boilerplate experimental pipelines. We present M-PACT to overcome existing\nissues by removing the need to develop boilerplate code which allows users to\nquickly prototype action classification models while leveraging existing\nstate-of-the-art (SOTA) models available in the platform. M-PACT is the first\nto offer four SOTA activity classification models, I3D, C3D, ResNet50+LSTM, and\nTSN, under a single platform with reproducible competitive results. This\nplatform allows for the generation of models and results over activity\nrecognition datasets through the use of modular code, various preprocessing and\nneural network layers, and seamless data flow. In this paper, we present the\nsystem architecture, detail the functions of various modules, and describe the\nbasic tools to develop a new model in M-PACT.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 18:20:14 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 18:38:13 GMT"}, {"version": "v3", "created": "Fri, 5 Oct 2018 18:14:21 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Hofesmann", "Eric", ""], ["Ganesh", "Madan Ravi", ""], ["Corso", "Jason J.", ""]]}, {"id": "1804.05902", "submitter": "Yiwen Huang", "authors": "Yiwen Huang, Ming Qin", "title": "Densely Connected High Order Residual Network for Single Frame Image\n  Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNN) have been widely adopted for\nresearch on super resolution recently, however previous work focused mainly on\nstacking as many layers as possible in their model, in this paper, we present a\nnew perspective regarding to image restoration problems that we can construct\nthe neural network model reflecting the physical significance of the image\nrestoration process, that is, embedding the a priori knowledge of image\nrestoration directly into the structure of our neural network model, we\nemployed a symmetric non-linear colorspace, the sigmoidal transfer, to replace\ntraditional transfers such as, sRGB, Rec.709, which are asymmetric non-linear\ncolorspaces, we also propose a \"reuse plus patch\" method to deal with super\nresolution of different scaling factors, our proposed methods and model show\ngenerally superior performance over previous work even though our model was\nonly roughly trained and could still be underfitting the training set.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 19:25:33 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Huang", "Yiwen", ""], ["Qin", "Ming", ""]]}, {"id": "1804.05905", "submitter": "Wenbo Dong", "authors": "Wenbo Dong, Volkan Isler", "title": "Tree Morphology for Phenotyping from Semantics-Based Mapping in Orchard\n  Environments", "comments": "10 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring tree morphology for phenotyping is an essential but labor-intensive\nactivity in horticulture. Researchers often rely on manual measurements which\nmay not be accurate for example when measuring tree volume. Recent approaches\non automating the measurement process rely on LIDAR measurements coupled with\nhigh-accuracy GPS. Usually each side of a row is reconstructed independently\nand then merged using GPS information. Such approaches have two disadvantages:\n(1) they rely on specialized and expensive equipment, and (2) since the\nreconstruction process does not simultaneously use information from both sides,\nside reconstructions may not be accurate. We also show that standard loop\nclosure methods do not necessarily align tree trunks well. In this paper, we\npresent a novel vision system that employs only an RGB-D camera to estimate\nmorphological parameters. A semantics-based mapping algorithm merges the\ntwo-sides 3D models of tree rows, where integrated semantic information is\nobtained and refined by robust fitting algorithms. We focus on measuring tree\nheight, canopy volume and trunk diameter from the optimized 3D model.\nExperiments conducted in real orchards quantitatively demonstrate the accuracy\nof our method.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 19:33:37 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Dong", "Wenbo", ""], ["Isler", "Volkan", ""]]}, {"id": "1804.05944", "submitter": "Noel Codella", "authors": "Noel C. F. Codella, Daren Anderson, Tyler Philips, Anthony Porto,\n  Kevin Massey, Jane Snowdon, Rogerio Feris, John Smith", "title": "Segmentation of both Diseased and Healthy Skin from Clinical Photographs\n  in a Primary Care Setting", "comments": "Accepted to IEEE EMBC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents the first segmentation study of both diseased and healthy\nskin in standard camera photographs from a clinical environment. Challenges\narise from varied lighting conditions, skin types, backgrounds, and\npathological states. For study, 400 clinical photographs (with skin\nsegmentation masks) representing various pathological states of skin are\nretrospectively collected from a primary care network. 100 images are used for\ntraining and fine-tuning, and 300 are used for evaluation. This distribution\nbetween training and test partitions is chosen to reflect the difficulty in\namassing large quantities of labeled data in this domain. A deep learning\napproach is used, and 3 public segmentation datasets of healthy skin are\ncollected to study the potential benefits of pre-training. Two variants of\nU-Net are evaluated: U-Net and Dense Residual U-Net. We find that Dense\nResidual U-Nets have a 7.8% improvement in Jaccard, compared to classical U-Net\narchitectures (0.55 vs. 0.51 Jaccard), for direct transfer, where fine-tuning\ndata is not utilized. However, U-Net outperforms Dense Residual U-Net for both\ndirect training (0.83 vs. 0.80) and fine-tuning (0.89 vs. 0.88). The stark\nperformance improvement with fine-tuning compared to direct transfer and direct\ntraining emphasizes both the need for adequate representative data of diseased\nskin, and the utility of other publicly available data sources for this task.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 21:08:04 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 01:38:05 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Codella", "Noel C. F.", ""], ["Anderson", "Daren", ""], ["Philips", "Tyler", ""], ["Porto", "Anthony", ""], ["Massey", "Kevin", ""], ["Snowdon", "Jane", ""], ["Feris", "Rogerio", ""], ["Smith", "John", ""]]}, {"id": "1804.05984", "submitter": "Shuai Li", "authors": "Shuai Li, Dinei Florencio, Wanqing Li, Yaqin Zhao, Chris Cook", "title": "A Fusion Framework for Camouflaged Moving Foreground Detection in the\n  Wavelet Domain", "comments": "13 pages, accepted by IEEE TIP", "journal-ref": null, "doi": "10.1109/TIP.2018.2828329", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting camouflaged moving foreground objects has been known to be\ndifficult due to the similarity between the foreground objects and the\nbackground. Conventional methods cannot distinguish the foreground from\nbackground due to the small differences between them and thus suffer from\nunder-detection of the camouflaged foreground objects. In this paper, we\npresent a fusion framework to address this problem in the wavelet domain. We\nfirst show that the small differences in the image domain can be highlighted in\ncertain wavelet bands. Then the likelihood of each wavelet coefficient being\nforeground is estimated by formulating foreground and background models for\neach wavelet band. The proposed framework effectively aggregates the\nlikelihoods from different wavelet bands based on the characteristics of the\nwavelet transform. Experimental results demonstrated that the proposed method\nsignificantly outperformed existing methods in detecting camouflaged foreground\nobjects. Specifically, the average F-measure for the proposed algorithm was\n0.87, compared to 0.71 to 0.8 for the other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 23:49:56 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Li", "Shuai", ""], ["Florencio", "Dinei", ""], ["Li", "Wanqing", ""], ["Zhao", "Yaqin", ""], ["Cook", "Chris", ""]]}, {"id": "1804.06008", "submitter": "Miaomiao Liu", "authors": "Miaomiao Liu, Xuming He, Mathieu Salzmann", "title": "Geometry-aware Deep Network for Single-Image Novel View Synthesis", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of novel view synthesis from a single image.\nIn particular, we target real-world scenes with rich geometric structure, a\nchallenging task due to the large appearance variations of such scenes and the\nlack of simple 3D models to represent them. Modern, learning-based approaches\nmostly focus on appearance to synthesize novel views and thus tend to generate\npredictions that are inconsistent with the underlying scene structure. By\ncontrast, in this paper, we propose to exploit the 3D geometry of the scene to\nsynthesize a novel view. Specifically, we approximate a real-world scene by a\nfixed number of planes, and learn to predict a set of homographies and their\ncorresponding region masks to transform the input image into a novel view. To\nthis end, we develop a new region-aware geometric transform network that\nperforms these multiple tasks in a common framework. Our results on the outdoor\nKITTI and the indoor ScanNet datasets demonstrate the effectiveness of our\nnetwork in generating high quality synthetic views that respect the scene\ngeometry, thus outperforming the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 01:31:25 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Liu", "Miaomiao", ""], ["He", "Xuming", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1804.06023", "submitter": "Tao Yu", "authors": "Tao Yu, Zerong Zheng, Kaiwen Guo, Jianhui Zhao, Qionghai Dai, Hao Li,\n  Gerard Pons-Moll, Yebin Liu", "title": "DoubleFusion: Real-time Capture of Human Performances with Inner Body\n  Shapes from a Single Depth Sensor", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DoubleFusion, a new real-time system that combines volumetric\ndynamic reconstruction with data-driven template fitting to simultaneously\nreconstruct detailed geometry, non-rigid motion and the inner human body shape\nfrom a single depth camera. One of the key contributions of this method is a\ndouble layer representation consisting of a complete parametric body shape\ninside, and a gradually fused outer surface layer. A pre-defined node graph on\nthe body surface parameterizes the non-rigid deformations near the body, and a\nfree-form dynamically changing graph parameterizes the outer surface layer far\nfrom the body, which allows more general reconstruction. We further propose a\njoint motion tracking method based on the double layer representation to enable\nrobust and fast motion tracking performance. Moreover, the inner body shape is\noptimized online and forced to fit inside the outer surface layer. Overall, our\nmethod enables increasingly denoised, detailed and complete surface\nreconstructions, fast motion tracking performance and plausible inner body\nshape reconstruction in real-time. In particular, experiments show improved\nfast motion tracking and loop closure performance on more challenging\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 03:09:01 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Yu", "Tao", ""], ["Zheng", "Zerong", ""], ["Guo", "Kaiwen", ""], ["Zhao", "Jianhui", ""], ["Dai", "Qionghai", ""], ["Li", "Hao", ""], ["Pons-Moll", "Gerard", ""], ["Liu", "Yebin", ""]]}, {"id": "1804.06026", "submitter": "Varun Manjunatha", "authors": "Varun Manjunatha and Mohit Iyyer and Jordan Boyd-Graber and Larry\n  Davis", "title": "Learning to Color from Language", "comments": "6 pages", "journal-ref": "North American Chapter of the Association for Computational\n  Linguistics (NAACL), 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic colorization is the process of adding color to greyscale images. We\ncondition this process on language, allowing end users to manipulate a\ncolorized image by feeding in different captions. We present two different\narchitectures for language-conditioned colorization, both of which produce more\naccurate and plausible colorizations than a language-agnostic version. Through\nthis language-based framework, we can dramatically alter colorizations by\nmanipulating descriptive color words in captions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 03:22:00 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Manjunatha", "Varun", ""], ["Iyyer", "Mohit", ""], ["Boyd-Graber", "Jordan", ""], ["Davis", "Larry", ""]]}, {"id": "1804.06032", "submitter": "Daeyun Shin", "authors": "Daeyun Shin, Charless C. Fowlkes, Derek Hoiem", "title": "Pixels, voxels, and views: A study of shape representations for single\n  view 3D object shape prediction", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to compare surface-based and volumetric 3D object\nshape representations, as well as viewer-centered and object-centered reference\nframes for single-view 3D shape prediction. We propose a new algorithm for\npredicting depth maps from multiple viewpoints, with a single depth or RGB\nimage as input. By modifying the network and the way models are evaluated, we\ncan directly compare the merits of voxels vs. surfaces and viewer-centered vs.\nobject-centered for familiar vs. unfamiliar objects, as predicted from RGB or\ndepth images. Among our findings, we show that surface-based methods outperform\nvoxel representations for objects from novel classes and produce higher\nresolution outputs. We also find that using viewer-centered coordinates is\nadvantageous for novel objects, while object-centered representations are\nbetter for more familiar objects. Interestingly, the coordinate frame\nsignificantly affects the shape representation learned, with object-centered\nplacing more importance on implicitly recognizing the object category and\nviewer-centered producing shape representations with less dependence on\ncategory recognition.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 03:32:00 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 03:12:21 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Shin", "Daeyun", ""], ["Fowlkes", "Charless C.", ""], ["Hoiem", "Derek", ""]]}, {"id": "1804.06039", "submitter": "Xuepeng Shi Mr", "authors": "Xuepeng Shi, Shiguang Shan, Meina Kan, Shuzhe Wu, Xilin Chen", "title": "Real-Time Rotation-Invariant Face Detection with Progressive Calibration\n  Networks", "comments": "Accepted by The IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2018). Code: \\url{https://github.com/Jack-CV/PCN}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation-invariant face detection, i.e. detecting faces with arbitrary\nrotation-in-plane (RIP) angles, is widely required in unconstrained\napplications but still remains as a challenging task, due to the large\nvariations of face appearances. Most existing methods compromise with speed or\naccuracy to handle the large RIP variations. To address this problem more\nefficiently, we propose Progressive Calibration Networks (PCN) to perform\nrotation-invariant face detection in a coarse-to-fine manner. PCN consists of\nthree stages, each of which not only distinguishes the faces from non-faces,\nbut also calibrates the RIP orientation of each face candidate to upright\nprogressively. By dividing the calibration process into several progressive\nsteps and only predicting coarse orientations in early stages, PCN can achieve\nprecise and fast calibration. By performing binary classification of face vs.\nnon-face with gradually decreasing RIP ranges, PCN can accurately detect faces\nwith full $360^{\\circ}$ RIP angles. Such designs lead to a real-time\nrotation-invariant face detector. The experiments on multi-oriented FDDB and a\nchallenging subset of WIDER FACE containing rotated faces in the wild show that\nour PCN achieves quite promising performance.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 04:27:14 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Shi", "Xuepeng", ""], ["Shan", "Shiguang", ""], ["Kan", "Meina", ""], ["Wu", "Shuzhe", ""], ["Chen", "Xilin", ""]]}, {"id": "1804.06042", "submitter": "Li Si-Yao", "authors": "Li Si-Yao, Dongwei Ren, Furong Zhao, Zijian Hu, Junfeng Li, Qian Yin", "title": "Iterative Residual Image Deconvolution", "comments": "rejected by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image deblurring, a.k.a. image deconvolution, recovers a clear image from\npixel superposition caused by blur degradation. Few deep convolutional neural\nnetworks (CNN) succeed in addressing this task. In this paper, we first\ndemonstrate that the minimum-mean-square-error (MMSE) solution to image\ndeblurring can be interestingly unfolded into a series of residual components.\nBased on this analysis, we propose a novel iterative residual deconvolution\n(IRD) algorithm. Further, IRD motivates us to take one step forward to design\nan explicable and effective CNN architecture for image deconvolution.\nSpecifically, a sequence of residual CNN units are deployed, whose intermediate\noutputs are then concatenated and integrated, resulting in concatenated\nresidual convolutional network (CRCNet). The experimental results demonstrate\nthat proposed CRCNet not only achieves better quantitative metrics but also\nrecovers more visually plausible texture details compared with state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 05:07:28 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 16:36:01 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Si-Yao", "Li", ""], ["Ren", "Dongwei", ""], ["Zhao", "Furong", ""], ["Hu", "Zijian", ""], ["Li", "Junfeng", ""], ["Yin", "Qian", ""]]}, {"id": "1804.06055", "submitter": "Di Xie", "authors": "Chao Li and Qiaoyong Zhong and Di Xie and Shiliang Pu", "title": "Co-occurrence Feature Learning from Skeleton Data for Action Recognition\n  and Detection with Hierarchical Aggregation", "comments": "IJCAI18 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton-based human action recognition has recently drawn increasing\nattentions with the availability of large-scale skeleton datasets. The most\ncrucial factors for this task lie in two aspects: the intra-frame\nrepresentation for joint co-occurrences and the inter-frame representation for\nskeletons' temporal evolutions. In this paper we propose an end-to-end\nconvolutional co-occurrence feature learning framework. The co-occurrence\nfeatures are learned with a hierarchical methodology, in which different levels\nof contextual information are aggregated gradually. Firstly point-level\ninformation of each joint is encoded independently. Then they are assembled\ninto semantic representation in both spatial and temporal domains.\nSpecifically, we introduce a global spatial aggregation scheme, which is able\nto learn superior joint co-occurrence features over local aggregation. Besides,\nraw skeleton coordinates as well as their temporal difference are integrated\nwith a two-stream paradigm. Experiments show that our approach consistently\noutperforms other state-of-the-arts on action recognition and detection\nbenchmarks like NTU RGB+D, SBU Kinect Interaction and PKU-MMD.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 06:00:13 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Li", "Chao", ""], ["Zhong", "Qiaoyong", ""], ["Xie", "Di", ""], ["Pu", "Shiliang", ""]]}, {"id": "1804.06061", "submitter": "Hanjiang Lai", "authors": "Jikai Chen, Hanjiang Lai, Libing Geng, Yan Pan", "title": "Improving Deep Binary Embedding Networks by Order-aware Reweighting of\n  Triplets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on triplet-based deep binary embedding networks for\nimage retrieval task. The triplet loss has been shown to be most effective for\nthe ranking problem. However, most of the previous works treat the triplets\nequally or select the hard triplets based on the loss. Such strategies do not\nconsider the order relations, which is important for retrieval task. To this\nend, we propose an order-aware reweighting method to effectively train the\ntriplet-based deep networks, which up-weights the important triplets and\ndown-weights the uninformative triplets. First, we present the order-aware\nweighting factors to indicate the importance of the triplets, which depend on\nthe rank order of binary codes. Then, we reshape the triplet loss to the\nsquared triplet loss such that the loss function will put more weights on the\nimportant triplets. Extensive evaluations on four benchmark datasets show that\nthe proposed method achieves significant performance compared with the\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 06:29:40 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Chen", "Jikai", ""], ["Lai", "Hanjiang", ""], ["Geng", "Libing", ""], ["Pan", "Yan", ""]]}, {"id": "1804.06078", "submitter": "Haodi Hou", "authors": "Haodi Hou, Jing Huo, Yang Gao", "title": "Cross-Domain Adversarial Auto-Encoder", "comments": "Under review as a conference paper of KDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the Cross-Domain Adversarial Auto-Encoder (CDAAE)\nto address the problem of cross-domain image inference, generation and\ntransformation. We make the assumption that images from different domains share\nthe same latent code space for content, while having separate latent code space\nfor style. The proposed framework can map cross-domain data to a latent code\nvector consisting of a content part and a style part. The latent code vector is\nmatched with a prior distribution so that we can generate meaningful samples\nfrom any part of the prior space. Consequently, given a sample of one domain,\nour framework can generate various samples of the other domain with the same\ncontent of the input. This makes the proposed framework different from the\ncurrent work of cross-domain transformation. Besides, the proposed framework\ncan be trained with both labeled and unlabeled data, which makes it also\nsuitable for domain adaptation. Experimental results on data sets SVHN, MNIST\nand CASIA show the proposed framework achieved visually appealing performance\nfor image generation task. Besides, we also demonstrate the proposed method\nachieved superior results for domain adaptation. Code of our experiments is\navailable in https://github.com/luckycallor/CDAAE.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 07:12:58 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Hou", "Haodi", ""], ["Huo", "Jing", ""], ["Gao", "Yang", ""]]}, {"id": "1804.06094", "submitter": "David Rawlinson", "authors": "David Rawlinson, Abdelrahman Ahmed, Gideon Kowadlo", "title": "Sparse Unsupervised Capsules Generalize Better", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that unsupervised training of latent capsule layers using only the\nreconstruction loss, without masking to select the correct output class, causes\na loss of equivariances and other desirable capsule qualities. This implies\nthat supervised capsules networks can't be very deep. Unsupervised sparsening\nof latent capsule layer activity both restores these qualities and appears to\ngeneralize better than supervised masking, while potentially enabling deeper\ncapsules networks. We train a sparse, unsupervised capsules network of similar\ngeometry to Sabour et al (2017) on MNIST, and then test classification accuracy\non affNIST using an SVM layer. Accuracy is improved from benchmark 79% to 90%.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 08:03:42 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Rawlinson", "David", ""], ["Ahmed", "Abdelrahman", ""], ["Kowadlo", "Gideon", ""]]}, {"id": "1804.06112", "submitter": "Xiaowei Zhou", "authors": "Xiaowei Zhou, Sikang Liu, Georgios Pavlakos, Vijay Kumar, Kostas\n  Daniilidis", "title": "Human Motion Capture Using a Drone", "comments": "In International Conference on Robotics and Automation (ICRA) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current motion capture (MoCap) systems generally require markers and multiple\ncalibrated cameras, which can be used only in constrained environments. In this\nwork we introduce a drone-based system for 3D human MoCap. The system only\nneeds an autonomously flying drone with an on-board RGB camera and is usable in\nvarious indoor and outdoor environments. A reconstruction algorithm is\ndeveloped to recover full-body motion from the video recorded by the drone. We\nargue that, besides the capability of tracking a moving subject, a flying drone\nalso provides fast varying viewpoints, which is beneficial for motion\nreconstruction. We evaluate the accuracy of the proposed system using our new\nDroCap dataset and also demonstrate its applicability for MoCap in the wild\nusing a consumer drone.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 08:57:40 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Zhou", "Xiaowei", ""], ["Liu", "Sikang", ""], ["Pavlakos", "Georgios", ""], ["Kumar", "Vijay", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1804.06114", "submitter": "Cong Chen", "authors": "Cong Chen, Kim Batselier, Ching-Yun Ko and Ngai Wong", "title": "A Support Tensor Train Machine", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been growing interest in extending traditional vector-based machine\nlearning techniques to their tensor forms. An example is the support tensor\nmachine (STM) that utilizes a rank-one tensor to capture the data structure,\nthereby alleviating the overfitting and curse of dimensionality problems in the\nconventional support vector machine (SVM). However, the expressive power of a\nrank-one tensor is restrictive for many real-world data. To overcome this\nlimitation, we introduce a support tensor train machine (STTM) by replacing the\nrank-one tensor in an STM with a tensor train. Experiments validate and confirm\nthe superiority of an STTM over the SVM and STM.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 08:59:13 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Chen", "Cong", ""], ["Batselier", "Kim", ""], ["Ko", "Ching-Yun", ""], ["Wong", "Ngai", ""]]}, {"id": "1804.06120", "submitter": "David Schubert", "authors": "David Schubert, Thore Goll, Nikolaus Demmel, Vladyslav Usenko, J\\\"org\n  St\\\"uckler and Daniel Cremers", "title": "The TUM VI Benchmark for Evaluating Visual-Inertial Odometry", "comments": "Updates compared to previous version include additional evaluations\n  and DOI", "journal-ref": null, "doi": "10.1109/IROS.2018.8593419", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual odometry and SLAM methods have a large variety of applications in\ndomains such as augmented reality or robotics. Complementing vision sensors\nwith inertial measurements tremendously improves tracking accuracy and\nrobustness, and thus has spawned large interest in the development of\nvisual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI\nbenchmark, a novel dataset with a diverse set of sequences in different scenes\nfor evaluating VI odometry. It provides camera images with 1024x1024 resolution\nat 20 Hz, high dynamic range and photometric calibration. An IMU measures\naccelerations and angular velocities on 3 axes at 200 Hz, while the cameras and\nIMU sensors are time-synchronized in hardware. For trajectory evaluation, we\nalso provide accurate pose ground truth from a motion capture system at high\nfrequency (120 Hz) at the start and end of the sequences which we accurately\naligned with the camera and IMU measurements. The full dataset with raw and\ncalibrated data is publicly available. We also evaluate state-of-the-art VI\nodometry approaches on our dataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 09:11:23 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 16:50:09 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 13:42:21 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Schubert", "David", ""], ["Goll", "Thore", ""], ["Demmel", "Nikolaus", ""], ["Usenko", "Vladyslav", ""], ["St\u00fcckler", "J\u00f6rg", ""], ["Cremers", "Daniel", ""]]}, {"id": "1804.06124", "submitter": "Snehasis Mukherjee", "authors": "Ashish Verma, Kranthi Koukuntla, Rohit Varma, Snehasis Mukherjee", "title": "Automatic Assessment of Artistic Quality of Photos", "comments": "Submitted to a journal 7 months back, waiting for their response", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a technique to assess the aesthetic quality of\nphotographs. The goal of the study is to predict whether a given photograph is\ncaptured by professional photographers, or by common people, based on a\nmeasurement of artistic quality of the photograph. We propose a\nMulti-Layer-Perceptron based system to analyze some low, mid and high level\nimage features and find their effectiveness to measure artistic quality of the\nimage and produce a measurement of the artistic quality of the image on a scale\nof 10. We validate the proposed system on a large dataset, containing images\ndownloaded from the internet. The dataset contains some images captured by\nprofessional photographers and the rest of the images captured by common\npeople. The proposed measurement of artistic quality of images provides higher\nvalue of photo quality for the images captured by professional photographers,\ncompared to the values provided for the other images.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 09:35:16 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Verma", "Ashish", ""], ["Koukuntla", "Kranthi", ""], ["Varma", "Rohit", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "1804.06128", "submitter": "Ching-Yun Ko", "authors": "Ching-Yun Ko, Kim Batselier, Wenjian Yu, Ngai Wong", "title": "Fast and Accurate Tensor Completion with Total Variation Regularized\n  Tensor Trains", "comments": "13 pages. Source code and supplemental materials are available via:\n  https://github.com/IRENEKO/TTC Updates 11/13: included more comparisons and\n  experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new tensor completion method based on tensor trains. The\nto-be-completed tensor is modeled as a low-rank tensor train, where we use the\nknown tensor entries and their coordinates to update the tensor train. A novel\ntensor train initialization procedure is proposed specifically for image and\nvideo completion, which is demonstrated to ensure fast convergence of the\ncompletion algorithm. The tensor train framework is also shown to easily\naccommodate Total Variation and Tikhonov regularization due to their low-rank\ntensor train representations. Image and video inpainting experiments verify the\nsuperiority of the proposed scheme in terms of both speed and scalability,\nwhere a speedup of up to 155X is observed compared to state-of-the-art tensor\ncompletion methods at a similar accuracy. Moreover, we demonstrate the proposed\nscheme is especially advantageous over existing algorithms when only tiny\nportions (say, 1%) of the to-be-completed images/videos are known.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 09:37:11 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 05:26:47 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 07:59:54 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Ko", "Ching-Yun", ""], ["Batselier", "Kim", ""], ["Yu", "Wenjian", ""], ["Wong", "Ngai", ""]]}, {"id": "1804.06202", "submitter": "Jingdong Wang", "authors": "Guotian Xie, Jingdong Wang, Ting Zhang, Jianhuang Lai, Richang Hong,\n  and Guo-Jun Qi", "title": "IGCV$2$: Interleaved Structured Sparse Convolutional Neural Networks", "comments": "Accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of designing efficient convolutional\nneural network architectures with the interest in eliminating the redundancy in\nconvolution kernels. In addition to structured sparse kernels, low-rank kernels\nand the product of low-rank kernels, the product of structured sparse kernels,\nwhich is a framework for interpreting the recently-developed interleaved group\nconvolutions (IGC) and its variants (e.g., Xception), has been attracting\nincreasing interests.\n  Motivated by the observation that the convolutions contained in a group\nconvolution in IGC can be further decomposed in the same manner, we present a\nmodularized building block, {IGCV$2$:} interleaved structured sparse\nconvolutions. It generalizes interleaved group convolutions, which is composed\nof two structured sparse kernels, to the product of more structured sparse\nkernels, further eliminating the redundancy. We present the complementary\ncondition and the balance condition to guide the design of structured sparse\nkernels, obtaining a balance among three aspects: model size, computation\ncomplexity and classification accuracy. Experimental results demonstrate the\nadvantage on the balance among these three aspects compared to interleaved\ngroup convolutions and Xception, and competitive performance compared to other\nstate-of-the-art architecture design methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 12:36:36 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Xie", "Guotian", ""], ["Wang", "Jingdong", ""], ["Zhang", "Ting", ""], ["Lai", "Jianhuang", ""], ["Hong", "Richang", ""], ["Qi", "Guo-Jun", ""]]}, {"id": "1804.06208", "submitter": "Bin Xiao", "authors": "Bin Xiao, Haiping Wu and Yichen Wei", "title": "Simple Baselines for Human Pose Estimation and Tracking", "comments": "Accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant progress on pose estimation and increasing\ninterests on pose tracking in recent years. At the same time, the overall\nalgorithm and system complexity increases as well, making the algorithm\nanalysis and comparison more difficult. This work provides simple and effective\nbaseline methods. They are helpful for inspiring and evaluating new ideas for\nthe field. State-of-the-art results are achieved on challenging benchmarks. The\ncode will be available at https://github.com/leoxiaobin/pose.pytorch.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 12:55:23 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 06:54:53 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Xiao", "Bin", ""], ["Wu", "Haiping", ""], ["Wei", "Yichen", ""]]}, {"id": "1804.06215", "submitter": "Zeming Li", "authors": "Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, Jian Sun", "title": "DetNet: A Backbone network for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent CNN based object detectors, no matter one-stage methods like YOLO,\nSSD, and RetinaNe or two-stage detectors like Faster R-CNN, R-FCN and FPN are\nusually trying to directly finetune from ImageNet pre-trained models designed\nfor image classification. There has been little work discussing on the backbone\nfeature extractor specifically designed for the object detection. More\nimportantly, there are several differences between the tasks of image\nclassification and object detection. 1. Recent object detectors like FPN and\nRetinaNet usually involve extra stages against the task of image classification\nto handle the objects with various scales. 2. Object detection not only needs\nto recognize the category of the object instances but also spatially locate the\nposition. Large downsampling factor brings large valid receptive field, which\nis good for image classification but compromises the object location ability.\nDue to the gap between the image classification and object detection, we\npropose DetNet in this paper, which is a novel backbone network specifically\ndesigned for object detection. Moreover, DetNet includes the extra stages\nagainst traditional backbone network for image classification, while maintains\nhigh spatial resolution in deeper layers. Without any bells and whistles,\nstate-of-the-art results have been obtained for both object detection and\ninstance segmentation on the MSCOCO benchmark based on our DetNet~(4.8G FLOPs)\nbackbone. The code will be released for the reproduction.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 13:09:13 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 06:36:36 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Li", "Zeming", ""], ["Peng", "Chao", ""], ["Yu", "Gang", ""], ["Zhang", "Xiangyu", ""], ["Deng", "Yangdong", ""], ["Sun", "Jian", ""]]}, {"id": "1804.06236", "submitter": "Isaak Kavasidis", "authors": "I. Kavasidis, S. Palazzo, C. Spampinato, C. Pino, D. Giordano, D.\n  Giuffrida, P. Messina", "title": "A Saliency-based Convolutional Neural Network for Table and Chart\n  Detection in Digitized Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) have recently been applied\nsuccessfully to a variety of vision and multimedia tasks, thus driving\ndevelopment of novel solutions in several application domains. Document\nanalysis is a particularly promising area for DCNNs: indeed, the number of\navailable digital documents has reached unprecedented levels, and humans are no\nlonger able to discover and retrieve all the information contained in these\ndocuments without the help of automation. Under this scenario, DCNNs offers a\nviable solution to automate the information extraction process from digital\ndocuments. Within the realm of information extraction from documents, detection\nof tables and charts is particularly needed as they contain a visual summary of\nthe most valuable information contained in a document. For a complete\nautomation of visual information extraction process from tables and charts, it\nis necessary to develop techniques that localize them and identify precisely\ntheir boundaries. In this paper we aim at solving the table/chart detection\ntask through an approach that combines deep convolutional neural networks,\ngraphical models and saliency concepts. In particular, we propose a\nsaliency-based fully-convolutional neural network performing multi-scale\nreasoning on visual cues followed by a fully-connected conditional random field\n(CRF) for localizing tables and charts in digital/digitized documents.\nPerformance analysis carried out on an extended version of ICDAR 2013 (with\nannotated charts as well as tables) shows that our approach yields promising\nresults, outperforming existing models.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 13:39:29 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Kavasidis", "I.", ""], ["Palazzo", "S.", ""], ["Spampinato", "C.", ""], ["Pino", "C.", ""], ["Giordano", "D.", ""], ["Giuffrida", "D.", ""], ["Messina", "P.", ""]]}, {"id": "1804.06242", "submitter": "Chen-Wei Xie", "authors": "Chen-Wei Xie and Hong-Yu Zhou and Jianxin Wu", "title": "Vortex Pooling: Improving Context Representation in Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a fundamental task in computer vision, which can be\nconsidered as a per-pixel classification problem. Recently, although fully\nconvolutional neural network (FCN) based approaches have made remarkable\nprogress in such task, aggregating local and contextual information in\nconvolutional feature maps is still a challenging problem. In this paper, we\nargue that, when predicting the category of a given pixel, the regions close to\nthe target are more important than those far from it. To tackle this problem,\nwe then propose an effective yet efficient approach named Vortex Pooling to\neffectively utilize contextual information. Empirical studies are also provided\nto validate the effectiveness of the proposed method. To be specific, our\napproach outperforms the previous state-of-the-art model named DeepLab v3 by\n1.5% on the PASCAL VOC 2012 val set and 0.6% on the test set by replacing the\nAtrous Spatial Pyramid Pooling (ASPP) module in DeepLab v3 with the proposed\nVortex Pooling. Moreover, our model (10.13FPS) shares similar computation cost\nwith DeepLab v3 (10.37 FPS).\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 13:44:51 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 14:09:33 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Xie", "Chen-Wei", ""], ["Zhou", "Hong-Yu", ""], ["Wu", "Jianxin", ""]]}, {"id": "1804.06248", "submitter": "Lan Wang", "authors": "Lan Wang, Chenqiang Gao, Luyu Yang, Yue Zhao, Wangmeng Zuo, and Deyu\n  Meng", "title": "PM-GANs: Discriminative Representation Learning for Action Recognition\n  Using Partial-modalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data of different modalities generally convey complimentary but heterogeneous\ninformation, and a more discriminative representation is often preferred by\ncombining multiple data modalities like the RGB and infrared features. However\nin reality, obtaining both data channels is challenging due to many\nlimitations. For example, the RGB surveillance cameras are often restricted\nfrom private spaces, which is in conflict with the need of abnormal activity\ndetection for personal security. As a result, using partial data channels to\nbuild a full representation of multi-modalities is clearly desired. In this\npaper, we propose a novel Partial-modal Generative Adversarial Networks\n(PM-GANs) that learns a full-modal representation using data from only partial\nmodalities. The full representation is achieved by a generated representation\nin place of the missing data channel. Extensive experiments are conducted to\nverify the performance of our proposed method on action recognition, compared\nwith four state-of-the-art methods. Meanwhile, a new Infrared-Visible Dataset\nfor action recognition is introduced, and will be the first publicly available\naction dataset that contains paired infrared and visible spectrum.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 13:48:18 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Wang", "Lan", ""], ["Gao", "Chenqiang", ""], ["Yang", "Luyu", ""], ["Zhao", "Yue", ""], ["Zuo", "Wangmeng", ""], ["Meng", "Deyu", ""]]}, {"id": "1804.06252", "submitter": "Aritra Dutta", "authors": "Aritra Dutta, Xin Li, Peter Richtarik", "title": "Weighted Low-Rank Approximation of Matrices and Background Modeling", "comments": "arXiv admin note: text overlap with arXiv:1707.00281", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We primarily study a special a weighted low-rank approximation of matrices\nand then apply it to solve the background modeling problem. We propose two\nalgorithms for this purpose: one operates in the batch mode on the entire data\nand the other one operates in the batch-incremental mode on the data and\nnaturally captures more background variations and computationally more\neffective. Moreover, we propose a robust technique that learns the background\nframe indices from the data and does not require any training frames. We\ndemonstrate through extensive experiments that by inserting a simple weight in\nthe Frobenius norm, it can be made robust to the outliers similar to the\n$\\ell_1$ norm. Our methods match or outperform several state-of-the-art online\nand batch background modeling methods in virtually all quantitative and\nqualitative measures.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 21:43:08 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Dutta", "Aritra", ""], ["Li", "Xin", ""], ["Richtarik", "Peter", ""]]}, {"id": "1804.06253", "submitter": "Bo Jiang", "authors": "Bo Jiang, Doudou Lin, Bin Luo, Jin Tang", "title": "Temporal Coherent and Graph Optimized Manifold Ranking for Visual\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, weighted patch representation has been widely studied for\nalleviating the impact of background information included in bounding box to\nimprove visual tracking results. However, existing weighted patch\nrepresentation models generally exploit spatial structure information among\npatches in each frame separately which ignore (1) unary featureof each patch\nand (2) temporal correlation among patches in different frames. To address this\nproblem, we propose a novel unified temporal coherence and graph optimized\nranking model for weighted patch representation in visual tracking problem.\nThere are three main contributions of this paper. First, we propose to employ a\nflexible graph ranking for patch weight computation which exploits both\nstructure information among patches and unary feature of each patch\nsimultaneously. Second, we propose a new more discriminative ranking model by\nfurther considering the temporal correlation among patches in different frames.\nThird, a neighborhood preserved, low-rank graph is learned and incorporated to\nbuild a unified optimized ranking model. Experiments on two benchmark datasets\nshow the benefits of our model.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 13:50:24 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Jiang", "Bo", ""], ["Lin", "Doudou", ""], ["Luo", "Bin", ""], ["Tang", "Jin", ""]]}, {"id": "1804.06254", "submitter": "Partha Roy Dr.", "authors": "Partha Pratim Roy, Akash Mohta, Bidyut B. Chaudhuri", "title": "Synthetic data generation for Indic handwritten text recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to generate synthetic dataset for\nhandwritten word recognition systems. It is difficult to recognize handwritten\nscripts for which sufficient training data is not readily available or it may\nbe expensive to collect such data. Hence, it becomes hard to train recognition\nsystems owing to lack of proper dataset. To overcome such problems, synthetic\ndata could be used to create or expand the existing training dataset to improve\nrecognition performance. Any available digital data from online newspaper and\nsuch sources can be used to generate synthetic data. In this paper, we propose\nto add distortion/deformation to digital data in such a way that the underlying\npattern is preserved, so that the image so produced bears a close similarity to\nactual handwritten samples. The images thus produced can be used independently\nto train the system or be combined with natural handwritten data to augment the\noriginal dataset and improve the recognition system. We experimented using\nsynthetic data to improve the recognition accuracy of isolated characters and\nwords. The framework is tested on 2 Indic scripts - Devanagari (Hindi) and\nBengali (Bangla), for numeral, character and word recognition. We have obtained\nencouraging results from the experiment. Finally, the experiment with Latin\ntext verifies the utility of the approach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 13:52:59 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Roy", "Partha Pratim", ""], ["Mohta", "Akash", ""], ["Chaudhuri", "Bidyut B.", ""]]}, {"id": "1804.06275", "submitter": "Kshiteesh Hegde", "authors": "Kshiteesh Hegde, Malik Magdon-Ismail, Ram Ramanathan, Bishal Thapa", "title": "Network Signatures from Image Representation of Adjacency Matrices:\n  Deep/Transfer Learning for Subgraph Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel subgraph image representation for classification of\nnetwork fragments with the targets being their parent networks. The graph image\nrepresentation is based on 2D image embeddings of adjacency matrices. We use\nthis image representation in two modes. First, as the input to a machine\nlearning algorithm. Second, as the input to a pure transfer learner. Our\nconclusions from several datasets are that (a) deep learning using our\nstructured image features performs the best compared to benchmark graph kernel\nand classical features based methods; and, (b) pure transfer learning works\neffectively with minimum interference from the user and is robust against small\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 14:14:26 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Hegde", "Kshiteesh", ""], ["Magdon-Ismail", "Malik", ""], ["Ramanathan", "Ram", ""], ["Thapa", "Bishal", ""]]}, {"id": "1804.06278", "submitter": "Chen Liu", "authors": "Chen Liu, Jimei Yang, Duygu Ceylan, Ersin Yumer, Yasutaka Furukawa", "title": "PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a deep neural network (DNN) for piece-wise planar\ndepthmap reconstruction from a single RGB image. While DNNs have brought\nremarkable progress to single-image depth prediction, piece-wise planar\ndepthmap reconstruction requires a structured geometry representation, and has\nbeen a difficult task to master even for DNNs. The proposed end-to-end DNN\nlearns to directly infer a set of plane parameters and corresponding plane\nsegmentation masks from a single RGB image. We have generated more than 50,000\npiece-wise planar depthmaps for training and testing from ScanNet, a\nlarge-scale RGBD video database. Our qualitative and quantitative evaluations\ndemonstrate that the proposed approach outperforms baseline methods in terms of\nboth plane segmentation and depth estimation accuracy. To the best of our\nknowledge, this paper presents the first end-to-end neural architecture for\npiece-wise planar reconstruction from a single RGB image. Code and data are\navailable at https://github.com/art-programmer/PlaneNet.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 14:18:33 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Liu", "Chen", ""], ["Yang", "Jimei", ""], ["Ceylan", "Duygu", ""], ["Yumer", "Ersin", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "1804.06291", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Farhad Pourkamali-Anaraki and James Folberth and Stephen Becker", "title": "Efficient Solvers for Sparse Subspace Clustering", "comments": "This paper is accepted for publication in Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse subspace clustering (SSC) clusters $n$ points that lie near a union of\nlow-dimensional subspaces. The SSC model expresses each point as a linear or\naffine combination of the other points, using either $\\ell_1$ or $\\ell_0$\nregularization. Using $\\ell_1$ regularization results in a convex problem but\nrequires $O(n^2)$ storage, and is typically solved by the alternating direction\nmethod of multipliers which takes $O(n^3)$ flops. The $\\ell_0$ model is\nnon-convex but only needs memory linear in $n$, and is solved via orthogonal\nmatching pursuit and cannot handle the case of affine subspaces. This paper\nshows that a proximal gradient framework can solve SSC, covering both $\\ell_1$\nand $\\ell_0$ models, and both linear and affine constraints. For both $\\ell_1$\nand $\\ell_0$, algorithms to compute the proximity operator in the presence of\naffine constraints have not been presented in the SSC literature, so we derive\nan exact and efficient algorithm that solves the $\\ell_1$ case with just\n$O(n^2)$ flops. In the $\\ell_0$ case, our algorithm retains the low-memory\noverhead, and is the first algorithm to solve the SSC-$\\ell_0$ model with\naffine constraints. Experiments show our algorithms do not rely on sensitive\nregularization parameters, and they are less sensitive to sparsity\nmisspecification and high noise.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 14:41:52 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 03:19:41 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Pourkamali-Anaraki", "Farhad", ""], ["Folberth", "James", ""], ["Becker", "Stephen", ""]]}, {"id": "1804.06300", "submitter": "Yunbo Wang", "authors": "Yunbo Wang, Zhifeng Gao, Mingsheng Long, Jianmin Wang, Philip S. Yu", "title": "PredRNN++: Towards A Resolution of the Deep-in-Time Dilemma in\n  Spatiotemporal Predictive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PredRNN++, an improved recurrent network for video predictive\nlearning. In pursuit of a greater spatiotemporal modeling capability, our\napproach increases the transition depth between adjacent states by leveraging a\nnovel recurrent unit, which is named Causal LSTM for re-organizing the spatial\nand temporal memories in a cascaded mechanism. However, there is still a\ndilemma in video predictive learning: increasingly deep-in-time models have\nbeen designed for capturing complex variations, while introducing more\ndifficulties in the gradient back-propagation. To alleviate this undesirable\neffect, we propose a Gradient Highway architecture, which provides alternative\nshorter routes for gradient flows from outputs back to long-range inputs. This\narchitecture works seamlessly with causal LSTMs, enabling PredRNN++ to capture\nshort-term and long-term dependencies adaptively. We assess our model on both\nsynthetic and real video datasets, showing its ability to ease the vanishing\ngradient problem and yield state-of-the-art prediction results even in a\ndifficult objects occlusion scenario.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 14:52:19 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 02:17:17 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Wang", "Yunbo", ""], ["Gao", "Zhifeng", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""], ["Yu", "Philip S.", ""]]}, {"id": "1804.06304", "submitter": "Mahsa Lotfollahi", "authors": "Mahsa Lotfollahi, Sebastian Berisha, Leila Saadatifard, Laura Montier,\n  Jokubas Ziburkus, David Mayerich", "title": "Three-Dimensional GPU-Accelerated Active Contours for Automated\n  Localization of Cells in Large Images", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0215843", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell segmentation in microscopy is a challenging problem, since cells are\noften asymmetric and densely packed. This becomes particularly challenging for\nextremely large images, since manual intervention and processing time can make\nsegmentation intractable. In this paper, we present an efficient and highly\nparallel formulation for symmetric three-dimensional (3D) contour evolution\nthat extends previous work on fast two-dimensional active contours. We provide\na formulation for optimization on 3D images, as well as a strategy for\naccelerating computation on consumer graphics hardware. The proposed software\ntakes advantage of Monte-Carlo sampling schemes in order to speed up\nconvergence and reduce thread divergence. Experimental results show that this\nmethod provides superior performance for large 2D and 3D cell segmentation\ntasks when compared to existing methods on large 3D brain images.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 15:01:11 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Lotfollahi", "Mahsa", ""], ["Berisha", "Sebastian", ""], ["Saadatifard", "Leila", ""], ["Montier", "Laura", ""], ["Ziburkus", "Jokubas", ""], ["Mayerich", "David", ""]]}, {"id": "1804.06324", "submitter": "Shiv Ram Dubey", "authors": "Vamshi Krishna Repala, Shiv Ram Dubey", "title": "Dual CNN Models for Unsupervised Monocular Depth Estimation", "comments": "Accepted in 8th Pattern Recognition and Machine Intelligence\n  Conference (PReMI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unsupervised depth estimation is the recent trend by utilizing the\nbinocular stereo images to get rid of depth map ground truth. In unsupervised\ndepth computation, the disparity images are generated by training the CNN with\nan image reconstruction loss. In this paper, a dual CNN based model is\npresented for unsupervised depth estimation with 6 losses (DNM6) with\nindividual CNN for each view to generate the corresponding disparity map. The\nproposed dual CNN model is also extended with 12 losses (DNM12) by utilizing\nthe cross disparities. The presented DNM6 and DNM12 models are experimented\nover KITTI driving and Cityscapes urban database and compared with the recent\nstate-of-the-art result of unsupervised depth estimation. The code is available\nat:\nhttps://github.com/ishmav16/Dual-CNN-Models-for-Unsupervised-Monocular-Depth-Estimation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 07:04:36 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 11:08:56 GMT"}, {"version": "v3", "created": "Sun, 22 Jul 2018 02:00:04 GMT"}, {"version": "v4", "created": "Wed, 6 Nov 2019 09:10:51 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Repala", "Vamshi Krishna", ""], ["Dubey", "Shiv Ram", ""]]}, {"id": "1804.06332", "submitter": "Jiaolong Xu", "authors": "Jiaolong Xu, Peng Wang, Heng Yang, Antonio M. L\\'opez", "title": "Training a Binary Weight Object Detector by Knowledge Transfer for\n  Autonomous Driving", "comments": "Accepted by ICRA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Autonomous driving has harsh requirements of small model size and energy\nefficiency, in order to enable the embedded system to achieve real-time\non-board object detection. Recent deep convolutional neural network based\nobject detectors have achieved state-of-the-art accuracy. However, such models\nare trained with numerous parameters and their high computational costs and\nlarge storage prohibit the deployment to memory and computation resource\nlimited systems. Low-precision neural networks are popular techniques for\nreducing the computation requirements and memory footprint. Among them, binary\nweight neural network (BWN) is the extreme case which quantizes the float-point\ninto just $1$ bit. BWNs are difficult to train and suffer from accuracy\ndeprecation due to the extreme low-bit representation. To address this problem,\nwe propose a knowledge transfer (KT) method to aid the training of BWN using a\nfull-precision teacher network. We built DarkNet- and MobileNet-based binary\nweight YOLO-v2 detectors and conduct experiments on KITTI benchmark for car,\npedestrian and cyclist detection. The experimental results show that the\nproposed method maintains high detection accuracy while reducing the model size\nof DarkNet-YOLO from 257 MB to 8.8 MB and MobileNet-YOLO from 193 MB to 7.9 MB.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 15:53:44 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 17:03:05 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Xu", "Jiaolong", ""], ["Wang", "Peng", ""], ["Yang", "Heng", ""], ["L\u00f3pez", "Antonio M.", ""]]}, {"id": "1804.06353", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina, Marleen de Bruijne, Josien P. W. Pluim", "title": "Not-so-supervised: a survey of semi-supervised, multi-instance, and\n  transfer learning in medical image analysis", "comments": "Submitted to Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) algorithms have made a tremendous impact in the field\nof medical imaging. While medical imaging datasets have been growing in size, a\nchallenge for supervised ML algorithms that is frequently mentioned is the lack\nof annotated data. As a result, various methods which can learn with less/other\ntypes of supervision, have been proposed. We review semi-supervised, multiple\ninstance, and transfer learning in medical imaging, both in diagnosis/detection\nor segmentation tasks. We also discuss connections between these learning\nscenarios, and opportunities for future research.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 16:25:31 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 07:34:46 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Cheplygina", "Veronika", ""], ["de Bruijne", "Marleen", ""], ["Pluim", "Josien P. W.", ""]]}, {"id": "1804.06364", "submitter": "Rodrigo de Bem", "authors": "Rodrigo de Bem, Arnab Ghosh, Thalaiyasingam Ajanthan, Ondrej Miksik,\n  Adnane Boukhayma, N. Siddharth and Philip Torr", "title": "DGPose: Deep Generative Models for Human Body Analysis", "comments": "IJCV 2020 special issue on 'Generating Realistic Visual Data of Human\n  Behavior' preprint. Keywords: deep generative models, semi-supervised\n  learning, human pose estimation, variational autoencoders, generative\n  adversarial networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative modelling for human body analysis is an emerging problem with\nmany interesting applications. However, the latent space learned by such\napproaches is typically not interpretable, resulting in less flexibility. In\nthis work, we present deep generative models for human body analysis in which\nthe body pose and the visual appearance are disentangled. Such a\ndisentanglement allows independent manipulation of pose and appearance, and\nhence enables applications such as pose-transfer without specific training for\nsuch a task. Our proposed models, the Conditional-DGPose and the Semi-DGPose,\nhave different characteristics. In the first, body pose labels are taken as\nconditioners, from a fully-supervised training set. In the second, our\nstructured semi-supervised approach allows for pose estimation to be performed\nby the model itself and relaxes the need for labelled data. Therefore, the\nSemi-DGPose aims for the joint understanding and generation of people in\nimages. It is not only capable of mapping images to interpretable latent\nrepresentations but also able to map these representations back to the image\nspace. We compare our models with relevant baselines, the ClothNet-Body and the\nPose Guided Person Generation networks, demonstrating their merits on the\nHuman3.6M, ChictopiaPlus and DeepFashion benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 16:43:35 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 19:48:00 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["de Bem", "Rodrigo", ""], ["Ghosh", "Arnab", ""], ["Ajanthan", "Thalaiyasingam", ""], ["Miksik", "Ondrej", ""], ["Boukhayma", "Adnane", ""], ["Siddharth", "N.", ""], ["Torr", "Philip", ""]]}, {"id": "1804.06375", "submitter": "Yongbin Sun", "authors": "Yongbin Sun, Ziwei Liu, Yue Wang, Sanjay E. Sarma", "title": "Im2Avatar: Colorful 3D Reconstruction from a Single Image", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing works on single-image 3D reconstruction mainly focus on shape\nrecovery. In this work, we study a new problem, that is, simultaneously\nrecovering 3D shape and surface color from a single image, namely \"colorful 3D\nreconstruction\". This problem is both challenging and intriguing because the\nability to infer textured 3D model from a single image is at the core of visual\nunderstanding. Here, we propose an end-to-end trainable framework, Colorful\nVoxel Network (CVN), to tackle this problem. Conditioned on a single 2D input,\nCVN learns to decompose shape and surface color information of a 3D object into\na 3D shape branch and a surface color branch, respectively. Specifically, for\nthe shape recovery, we generate a shape volume with the state of its voxels\nindicating occupancy. For the surface color recovery, we combine the strength\nof appearance hallucination and geometric projection by concurrently learning a\nregressed color volume and a 2D-to-3D flow volume, which are then fused into a\nblended color volume. The final textured 3D model is obtained by sampling color\nfrom the blended color volume at the positions of occupied voxels in the shape\nvolume. To handle the severe sparse volume representations, a novel loss\nfunction, Mean Squared False Cross-Entropy Loss (MSFCEL), is designed.\nExtensive experiments demonstrate that our approach achieves significant\nimprovement over baselines, and shows great generalization across diverse\nobject categories and arbitrary viewpoints.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 17:02:20 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Sun", "Yongbin", ""], ["Liu", "Ziwei", ""], ["Wang", "Yue", ""], ["Sarma", "Sanjay E.", ""]]}, {"id": "1804.06423", "submitter": "Omid Hosseini Jafari", "authors": "Weihao Li, Omid Hosseini Jafari, Carsten Rother", "title": "Deep Object Co-Segmentation", "comments": "Accepted at ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a deep object co-segmentation (DOCS) approach for\nsegmenting common objects of the same class within a pair of images. This means\nthat the method learns to ignore common, or uncommon, background stuff and\nfocuses on objects. If multiple object classes are presented in the image pair,\nthey are jointly extracted as foreground. To address this task, we propose a\nCNN-based Siamese encoder-decoder architecture. The encoder extracts high-level\nsemantic features of the foreground objects, a mutual correlation layer detects\nthe common objects, and finally, the decoder generates the output foreground\nmasks for each image. To train our model, we compile a large object\nco-segmentation dataset consisting of image pairs from the PASCAL VOC dataset\nwith common objects masks. We evaluate our approach on commonly used datasets\nfor co-segmentation tasks and observe that our approach consistently\noutperforms competing methods, for both seen and unseen object classes.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 18:24:51 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 17:05:49 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Li", "Weihao", ""], ["Jafari", "Omid Hosseini", ""], ["Rother", "Carsten", ""]]}, {"id": "1804.06438", "submitter": "Karthik Muthuraman", "authors": "Karthik Muthuraman, Pranav Joshi, Suraj Kiran Raman", "title": "Vision Based Dynamic Offside Line Marker for Soccer Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Offside detection in soccer has emerged as one of the most important\ndecisions with an average of 50 offside decisions every game. False detections\nand rash calls adversely affect game conditions and in many cases drastically\nchange the outcome of the game. The human eye has finite precision and can only\ndiscern a limited amount of detail in a given instance. Current offside\ndecisions are made manually by sideline referees and tend to remain\ncontroversial in many games. This calls for automated offside detection\ntechniques in order to assist accurate refereeing. In this work, we have\nexplicitly used computer vision and image processing techniques like Hough\ntransform, color similarity (quantization), graph connected components, and\nvanishing point ideas to identify the probable offside regions.\n  Keywords: Hough transform, connected components, KLT tracking, color\nsimilarity.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 19:00:01 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Muthuraman", "Karthik", ""], ["Joshi", "Pranav", ""], ["Raman", "Suraj Kiran", ""]]}, {"id": "1804.06498", "submitter": "Mahdi Abavisani", "authors": "Mahdi Abavisani, Vishal M. Patel", "title": "Deep Multimodal Subspace Clustering Networks", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol. 12, no.\n  6, pp. 1601-1614, Dec. 2018", "doi": "10.1109/JSTSP.2018.2875385", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present convolutional neural network (CNN) based approaches for\nunsupervised multimodal subspace clustering. The proposed framework consists of\nthree main stages - multimodal encoder, self-expressive layer, and multimodal\ndecoder. The encoder takes multimodal data as input and fuses them to a latent\nspace representation. The self-expressive layer is responsible for enforcing\nthe self-expressiveness property and acquiring an affinity matrix corresponding\nto the data points. The decoder reconstructs the original input data. The\nnetwork uses the distance between the decoder's reconstruction and the original\ninput in its training. We investigate early, late and intermediate fusion\ntechniques and propose three different encoders corresponding to them for\nspatial fusion. The self-expressive layers and multimodal decoders are\nessentially the same for different spatial fusion-based approaches. In addition\nto various spatial fusion-based methods, an affinity fusion-based network is\nalso proposed in which the self-expressive layer corresponding to different\nmodalities is enforced to be the same. Extensive experiments on three datasets\nshow that the proposed methods significantly outperform the state-of-the-art\nmultimodal subspace clustering methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 23:04:33 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 21:53:14 GMT"}, {"version": "v3", "created": "Fri, 4 Jan 2019 20:19:14 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Abavisani", "Mahdi", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1804.06504", "submitter": "Juan-Manuel Perez-Rua", "authors": "Juan-Manuel Perez-Rua and Tomas Crivelli and Patrick Bouthemy and\n  Patrick Perez", "title": "Learning how to be robust: Deep polynomial regression", "comments": "18 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial regression is a recurrent problem with a large number of\napplications. In computer vision it often appears in motion analysis. Whatever\nthe application, standard methods for regression of polynomial models tend to\ndeliver biased results when the input data is heavily contaminated by outliers.\nMoreover, the problem is even harder when outliers have strong structure.\nDeparting from problem-tailored heuristics for robust estimation of parametric\nmodels, we explore deep convolutional neural networks. Our work aims to find a\ngeneric approach for training deep regression models without the explicit need\nof supervised annotation. We bypass the need for a tailored loss function on\nthe regression parameters by attaching to our model a differentiable hard-wired\ndecoder corresponding to the polynomial operation at hand. We demonstrate the\nvalue of our findings by comparing with standard robust regression methods.\nFurthermore, we demonstrate how to use such models for a real computer vision\nproblem, i.e., video stabilization. The qualitative and quantitative\nexperiments show that neural networks are able to learn robustness for general\npolynomial regression, with results that well overpass scores of traditional\nrobust estimation methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 23:36:12 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 10:21:03 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Perez-Rua", "Juan-Manuel", ""], ["Crivelli", "Tomas", ""], ["Bouthemy", "Patrick", ""], ["Perez", "Patrick", ""]]}, {"id": "1804.06505", "submitter": "Xiaofeng Xu", "authors": "Xiaofeng Xu, Ivor W. Tsang and Chuancai Liu", "title": "Complementary Attributes: A New Clue to Zero-Shot Learning", "comments": "Accepted by IEEE TRANSACTIONS ON CYBERNETICS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to recognize unseen objects using disjoint seen\nobjects via sharing attributes. The generalization performance of ZSL is\ngoverned by the attributes, which transfer semantic information from seen\nclasses to unseen classes. To take full advantage of the knowledge transferred\nby attributes, in this paper, we introduce the notion of complementary\nattributes (CA), as a supplement to the original attributes, to enhance the\nsemantic representation ability. Theoretical analyses demonstrate that\ncomplementary attributes can improve the PAC-style generalization bound of\noriginal ZSL model. Since the proposed CA focuses on enhancing the semantic\nrepresentation, CA can be easily applied to any existing attribute-based ZSL\nmethods, including the label-embedding strategy based ZSL (LEZSL) and the\nprobability-prediction strategy based ZSL (PPZSL). In PPZSL, there is a strong\nassumption that all the attributes are independent of each other, which is\narguably unrealistic in practice. To solve this problem, a novel rank\naggregation framework is proposed to circumvent the assumption. Extensive\nexperiments on five ZSL benchmark datasets and the large-scale ImageNet dataset\ndemonstrate that the proposed complementary attributes and rank aggregation can\nsignificantly and robustly improve existing ZSL methods and achieve the\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 23:48:21 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 07:56:03 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Xu", "Xiaofeng", ""], ["Tsang", "Ivor W.", ""], ["Liu", "Chuancai", ""]]}, {"id": "1804.06510", "submitter": "Xiu Li", "authors": "Xiu Li, Hongdong Li, Hanbyul Joo, Yebin Liu, Yaser Sheikh", "title": "Structure from Recurrent Motion: From Rigidity to Recurrency", "comments": "To appear in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for Non-Rigid Structure-from-Motion (NRSfM)\nfrom a long monocular video sequence observing a non-rigid object performing\nrecurrent and possibly repetitive dynamic action. Departing from the\ntraditional idea of using linear low-order or lowrank shape model for the task\nof NRSfM, our method exploits the property of shape recurrency (i.e., many\ndeforming shapes tend to repeat themselves in time). We show that recurrency is\nin fact a generalized rigidity. Based on this, we reduce NRSfM problems to\nrigid ones provided that certain recurrency condition is satisfied. Given such\na reduction, standard rigid-SfM techniques are directly applicable (without any\nchange) to the reconstruction of non-rigid dynamic shapes. To implement this\nidea as a practical approach, this paper develops efficient algorithms for\nautomatic recurrency detection, as well as camera view clustering via a\nrigidity-check. Experiments on both simulated sequences and real data\ndemonstrate the effectiveness of the method. Since this paper offers a novel\nperspective on rethinking structure-from-motion, we hope it will inspire other\nnew problems in the field.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 00:17:23 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Li", "Xiu", ""], ["Li", "Hongdong", ""], ["Joo", "Hanbyul", ""], ["Liu", "Yebin", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1804.06516", "submitter": "Stan Birchfield", "authors": "Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun\n  Jampani, Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, Stan Birchfield", "title": "Training Deep Networks with Synthetic Data: Bridging the Reality Gap by\n  Domain Randomization", "comments": "CVPR 2018 Workshop on Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for training deep neural networks for object detection\nusing synthetic images. To handle the variability in real-world data, the\nsystem relies upon the technique of domain randomization, in which the\nparameters of the simulator$-$such as lighting, pose, object textures,\netc.$-$are randomized in non-realistic ways to force the neural network to\nlearn the essential features of the object of interest. We explore the\nimportance of these parameters, showing that it is possible to produce a\nnetwork with compelling performance using only non-artistically-generated\nsynthetic data. With additional fine-tuning on real data, the network yields\nbetter performance than using real data alone. This result opens up the\npossibility of using inexpensive synthetic data for training neural networks\nwhile avoiding the need to collect large amounts of hand-annotated real-world\ndata or to generate high-fidelity synthetic worlds$-$both of which remain\nbottlenecks for many applications. The approach is evaluated on bounding box\ndetection of cars on the KITTI dataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 00:48:40 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 03:09:32 GMT"}, {"version": "v3", "created": "Mon, 23 Apr 2018 18:03:02 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Tremblay", "Jonathan", ""], ["Prakash", "Aayush", ""], ["Acuna", "David", ""], ["Brophy", "Mark", ""], ["Jampani", "Varun", ""], ["Anil", "Cem", ""], ["To", "Thang", ""], ["Cameracci", "Eric", ""], ["Boochoon", "Shaad", ""], ["Birchfield", "Stan", ""]]}, {"id": "1804.06534", "submitter": "Stan Birchfield", "authors": "Jonathan Tremblay, Thang To, Stan Birchfield", "title": "Falling Things: A Synthetic Dataset for 3D Object Detection and Pose\n  Estimation", "comments": "CVPR 2018 Workshop on Real World Challenges and New Benchmarks for\n  Deep Learning in Robotic Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new dataset, called Falling Things (FAT), for advancing the\nstate-of-the-art in object detection and 3D pose estimation in the context of\nrobotics. By synthetically combining object models and backgrounds of complex\ncomposition and high graphical quality, we are able to generate photorealistic\nimages with accurate 3D pose annotations for all objects in all images. Our\ndataset contains 60k annotated photos of 21 household objects taken from the\nYCB dataset. For each image, we provide the 3D poses, per-pixel class\nsegmentation, and 2D/3D bounding box coordinates for all objects. To facilitate\ntesting different input modalities, we provide mono and stereo RGB images,\nalong with registered dense depth images. We describe in detail the generation\nprocess and statistical analysis of the data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 03:03:42 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 23:14:49 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Tremblay", "Jonathan", ""], ["To", "Thang", ""], ["Birchfield", "Stan", ""]]}, {"id": "1804.06559", "submitter": "Ye Yuan", "authors": "Jianfeng Wang, Ye Yuan, Boxun Li, Gang Yu, Sun Jian", "title": "SFace: An Efficient Network for Face Detection in Large Scale Variations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection serves as a fundamental research topic for many applications\nlike face recognition. Impressive progress has been made especially with the\nrecent development of convolutional neural networks. However, the issue of\nlarge scale variations, which widely exists in high resolution images/videos,\nhas not been well addressed in the literature. In this paper, we present a\nnovel algorithm called SFace, which efficiently integrates the anchor-based\nmethod and anchor-free method to address the scale issues. A new dataset called\n4K-Face is also introduced to evaluate the performance of face detection with\nextreme large scale variations. The SFace architecture shows promising results\non the new 4K-Face benchmarks. In addition, our method can run at 50 frames per\nsecond (fps) with an accuracy of 80% AP on the standard WIDER FACE dataset,\nwhich outperforms the state-of-art algorithms by almost one order of magnitude\nin speed while achieves comparative performance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 05:25:57 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 09:40:22 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Wang", "Jianfeng", ""], ["Yuan", "Ye", ""], ["Li", "Boxun", ""], ["Yu", "Gang", ""], ["Jian", "Sun", ""]]}, {"id": "1804.06579", "submitter": "Fenggen Yu", "authors": "Fenggen Yu, Yan Zhang, Kai Xu, Ali Mahdavi-Amiri, Hao Zhang", "title": "Semi-Supervised Co-Analysis of 3D Shape Styles from Projected Lines", "comments": "17 pages, 25 figures", "journal-ref": "ACM Transactions on Graphics 37(2). February 2018", "doi": "10.1145/3182158", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semi-supervised co-analysis method for learning 3D shape styles\nfrom projected feature lines, achieving style patch localization with only weak\nsupervision. Given a collection of 3D shapes spanning multiple object\ncategories and styles, we perform style co-analysis over projected feature\nlines of each 3D shape and then backproject the learned style features onto the\n3D shapes. Our core analysis pipeline starts with mid-level patch sampling and\npre-selection of candidate style patches. Projective features are then encoded\nvia patch convolution. Multi-view feature integration and style clustering are\ncarried out under the framework of partially shared latent factor (PSLF)\nlearning, a multi-view feature learning scheme. PSLF achieves effective\nmulti-view feature fusion by distilling and exploiting consistent and\ncomplementary feature information from multiple views, while also selecting\nstyle patches from the candidates. Our style analysis approach supports both\nunsupervised and semi-supervised analysis. For the latter, our method accepts\nboth user-specified shape labels and style-ranked triplets as clustering\nconstraints.We demonstrate results from 3D shape style analysis and patch\nlocalization as well as improvements over state-of-the-art methods. We also\npresent several applications enabled by our style analysis.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 07:23:18 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Yu", "Fenggen", ""], ["Zhang", "Yan", ""], ["Xu", "Kai", ""], ["Mahdavi-Amiri", "Ali", ""], ["Zhang", "Hao", ""]]}, {"id": "1804.06604", "submitter": "Ana Garc\\'ia del Molino", "authors": "Ana Garc\\'ia del Molino and Michael Gygli", "title": "PHD-GIFs: Personalized Highlight Detection for Automatic GIF Creation", "comments": "Accepted for publication at the 2018 ACM Multimedia Conference (MM\n  '18)", "journal-ref": null, "doi": "10.1145/3240508.3240599", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highlight detection models are typically trained to identify cues that make\nvisual content appealing or interesting for the general public, with the\nobjective of reducing a video to such moments. However, the \"interestingness\"\nof a video segment or image is subjective. Thus, such highlight models provide\nresults of limited relevance for the individual user. On the other hand,\ntraining one model per user is inefficient and requires large amounts of\npersonal information which is typically not available. To overcome these\nlimitations, we present a global ranking model which conditions on each\nparticular user's interests. Rather than training one model per user, our model\nis personalized via its inputs, which allows it to effectively adapt its\npredictions, given only a few user-specific examples. To train this model, we\ncreate a large-scale dataset of users and the GIFs they created, giving us an\naccurate indication of their interests. Our experiments show that using the\nuser history substantially improves the prediction accuracy. On our test set of\n850 videos, our model improves the recall by 8% with respect to generic\nhighlight detectors. Furthermore, our method proves more precise than the\nuser-agnostic baselines even with just one person-specific example.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 08:44:11 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 09:10:34 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["del Molino", "Ana Garc\u00eda", ""], ["Gygli", "Michael", ""]]}, {"id": "1804.06633", "submitter": "Trung-Hieu Tran", "authors": "Trung-Hieu Tran, Zhe Wang, Sven Simon", "title": "Variational Disparity Estimation Framework for Plenoptic Image", "comments": null, "journal-ref": null, "doi": "10.1109/ICME.2017.8019377", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a computational framework for accurately estimating the\ndisparity map of plenoptic images. The proposed framework is based on the\nvariational principle and provides intrinsic sub-pixel precision. The\nlight-field motion tensor introduced in the framework allows us to combine\nadvanced robust data terms as well as provides explicit treatments for\ndifferent color channels. A warping strategy is embedded in our framework for\ntackling the large displacement problem. We also show that by applying a simple\nregularization term and a guided median filtering, the accuracy of displacement\nfield at occluded area could be greatly enhanced. We demonstrate the excellent\nperformance of the proposed framework by intensive comparisons with the Lytro\nsoftware and contemporary approaches on both synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 10:26:52 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Tran", "Trung-Hieu", ""], ["Wang", "Zhe", ""], ["Simon", "Sven", ""]]}, {"id": "1804.06642", "submitter": "Hajar Sadeghi Sokeh", "authors": "Hajar Sadeghi Sokeh, Vasileios Argyriou, Dorothy Monekosso, Paolo\n  Remagnino", "title": "Superframes, A Temporal Video Segmentation", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of video segmentation is to turn video data into a set of concrete\nmotion clusters that can be easily interpreted as building blocks of the video.\nThere are some works on similar topics like detecting scene cuts in a video,\nbut there is few specific research on clustering video data into the desired\nnumber of compact segments. It would be more intuitive, and more efficient, to\nwork with perceptually meaningful entity obtained from a low-level grouping\nprocess which we call it superframe. This paper presents a new simple and\nefficient technique to detect superframes of similar content patterns in\nvideos. We calculate the similarity of content-motion to obtain the strength of\nchange between consecutive frames. With the help of existing optical flow\ntechnique using deep models, the proposed method is able to perform more\naccurate motion estimation efficiently. We also propose two criteria for\nmeasuring and comparing the performance of different algorithms on various\ndatabases. Experimental results on the videos from benchmark databases have\ndemonstrated the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 10:39:01 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 15:42:10 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Sokeh", "Hajar Sadeghi", ""], ["Argyriou", "Vasileios", ""], ["Monekosso", "Dorothy", ""], ["Remagnino", "Paolo", ""]]}, {"id": "1804.06655", "submitter": "Mei Wang", "authors": "Mei Wang, Weihong Deng", "title": "Deep Face Recognition: A Survey", "comments": "Neurocomputing", "journal-ref": "Neurocomputing, 2021, 429:215-244", "doi": "10.1016/j.neucom.2020.10.081", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning applies multiple processing layers to learn representations of\ndata with multiple levels of feature extraction. This emerging technique has\nreshaped the research landscape of face recognition (FR) since 2014, launched\nby the breakthroughs of DeepFace and DeepID. Since then, deep learning\ntechnique, characterized by the hierarchical architecture to stitch together\npixels into invariant face representation, has dramatically improved the\nstate-of-the-art performance and fostered successful real-world applications.\nIn this survey, we provide a comprehensive review of the recent developments on\ndeep FR, covering broad topics on algorithm designs, databases, protocols, and\napplication scenes. First, we summarize different network architectures and\nloss functions proposed in the rapid evolution of the deep FR methods. Second,\nthe related face processing methods are categorized into two classes:\n\"one-to-many augmentation\" and \"many-to-one normalization\". Then, we summarize\nand compare the commonly used databases for both model training and evaluation.\nThird, we review miscellaneous scenes in deep FR, such as cross-factor,\nheterogenous, multiple-media and industrial scenes. Finally, the technical\nchallenges and several promising directions are highlighted.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 11:20:32 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 07:01:19 GMT"}, {"version": "v3", "created": "Mon, 4 Jun 2018 11:28:32 GMT"}, {"version": "v4", "created": "Sat, 14 Jul 2018 06:25:00 GMT"}, {"version": "v5", "created": "Tue, 24 Jul 2018 00:56:21 GMT"}, {"version": "v6", "created": "Sat, 15 Sep 2018 09:44:43 GMT"}, {"version": "v7", "created": "Fri, 28 Sep 2018 03:02:03 GMT"}, {"version": "v8", "created": "Tue, 12 Feb 2019 13:26:40 GMT"}, {"version": "v9", "created": "Sat, 1 Aug 2020 13:12:07 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Mei", ""], ["Deng", "Weihong", ""]]}, {"id": "1804.06670", "submitter": "Yuexiang Li", "authors": "Xinpeng Xie, Yuexiang Li and Linlin Shen", "title": "Active Learning for Breast Cancer Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is the second most common malignancy among women and has become\na major public health problem in current society. Traditional breast cancer\nidentification requires experienced pathologists to carefully read the breast\nslice, which is laborious and suffers from inter-observer variations.\nConsequently, an automatic classification framework for breast cancer\nidentification is worthwhile to develop. Recent years witnessed the development\nof deep learning technique. Increasing number of medical applications start to\nuse deep learning to improve diagnosis accuracy. In this paper, we proposed a\nnovel training strategy, namely reversed active learning (RAL), to train\nnetwork to automatically classify breast cancer images. Our RAL is applied to\nthe training set of a simple convolutional neural network (CNN) to remove\nmislabeled images. We evaluate the CNN trained with RAL on publicly available\nICIAR 2018 Breast Cancer Dataset (IBCD). The experimental results show that our\nRAL increases the slice-based accuracy of CNN from 93.75% to 96.25%.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 12:08:25 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Xie", "Xinpeng", ""], ["Li", "Yuexiang", ""], ["Shen", "Linlin", ""]]}, {"id": "1804.06679", "submitter": "Bernhard C. Geiger", "authors": "Rana Ali Amjad and Kairen Liu and Bernhard C. Geiger", "title": "Understanding Neural Networks and Individual Neuron Importance via\n  Information-Ordered Cumulative Ablation", "comments": "12 pages; accepted for publication in IEEE Transactions on Neural\n  Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the use of three information-theoretic\nquantities -- entropy, mutual information with the class variable, and a class\nselectivity measure based on Kullback-Leibler divergence -- to understand and\nstudy the behavior of already trained fully-connected feed-forward neural\nnetworks. We analyze the connection between these information-theoretic\nquantities and classification performance on the test set by cumulatively\nablating neurons in networks trained on MNIST, FashionMNIST, and CIFAR-10. Our\nresults parallel those recently published by Morcos et al., indicating that\nclass selectivity is not a good indicator for classification performance.\nHowever, looking at individual layers separately, both mutual information and\nclass selectivity are positively correlated with classification performance, at\nleast for networks with ReLU activation functions. We provide explanations for\nthis phenomenon and conclude that it is ill-advised to compare the proposed\ninformation-theoretic quantities across layers. Furthermore, we show that\ncumulative ablation of neurons with ascending or descending\ninformation-theoretic quantities can be used to formulate hypotheses regarding\nthe joint behavior of multiple neurons, such as redundancy and synergy, with\ncomparably low computational cost. We also draw connections to the information\nbottleneck theory for neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 12:29:24 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 11:35:26 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 06:52:01 GMT"}, {"version": "v4", "created": "Wed, 9 Jun 2021 15:28:37 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Amjad", "Rana Ali", ""], ["Liu", "Kairen", ""], ["Geiger", "Bernhard C.", ""]]}, {"id": "1804.06680", "submitter": "Santhosh Kelathodi", "authors": "Santhosh Kelathodi Kumaran, Debi Prosad Dogra and Partha Pratim Roy", "title": "Temporal Unknown Incremental Clustering (TUIC) Model for Analysis of\n  Traffic Surveillance Videos", "comments": null, "journal-ref": null, "doi": "10.1109/TITS.2018.2834958", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimized scene representation is an important characteristic of a framework\nfor detecting abnormalities on live videos. One of the challenges for detecting\nabnormalities in live videos is real-time detection of objects in a\nnon-parametric way. Another challenge is to efficiently represent the state of\nobjects temporally across frames. In this paper, a Gibbs sampling based\nheuristic model referred to as Temporal Unknown Incremental Clustering (TUIC)\nhas been proposed to cluster pixels with motion. Pixel motion is first detected\nusing optical flow and a Bayesian algorithm has been applied to associate\npixels belonging to similar cluster in subsequent frames. The algorithm is fast\nand produces accurate results in $\\Theta(kn)$ time, where $k$ is the number of\nclusters and $n$ the number of pixels. Our experimental validation with\npublicly available datasets reveals that the proposed framework has good\npotential to open-up new opportunities for real-time traffic analysis.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 12:29:42 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Kumaran", "Santhosh Kelathodi", ""], ["Dogra", "Debi Prosad", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "1804.06702", "submitter": "Mat\\'ias  Di Martino", "authors": "J. Matias Di Martino, Qiang Qiu, Trishul Nagenalli and Guillermo\n  Sapiro", "title": "Liveness Detection Using Implicit 3D Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoofing attacks are a threat to modern face recognition systems. In this\nwork we present a simple yet effective liveness detection approach to enhance\n2D face recognition methods and make them robust against spoofing attacks. We\nshow that the risk to spoofing attacks can be re- duced through the use of an\nadditional source of light, for example a flash. From a pair of input images\ntaken under different illumination, we define discriminative features that\nimplicitly contain facial three-dimensional in- formation. Furthermore, we show\nthat when multiple sources of light are considered, we are able to validate\nwhich one has been activated. This makes possible the design of a highly secure\nactive-light authentication framework. Finally, further investigating the use\nof 3D features without 3D reconstruction, we introduce an approximated\ndisparity-based implicit 3D feature obtained from an uncalibrated stereo-pair\nof cameras. Valida- tion experiments show that the proposed methods produce\nstate-of-the-art results in challenging scenarios with nearly no feature\nextraction latency.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 13:12:35 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 13:16:10 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Di Martino", "J. Matias", ""], ["Qiu", "Qiang", ""], ["Nagenalli", "Trishul", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1804.06786", "submitter": "Jack Hessel", "authors": "Jack Hessel, David Mimno, Lillian Lee", "title": "Quantifying the visual concreteness of words and topics in multimodal\n  datasets", "comments": "NAACL HLT 2018, 14 pages, 6 figures, data available at\n  http://www.cs.cornell.edu/~jhessel/concreteness/concreteness.html", "journal-ref": "2018 North American Chapter of the Association for Computational\n  Linguistics: Human Language Technologies (NAACL HLT)", "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal machine learning algorithms aim to learn visual-textual\ncorrespondences. Previous work suggests that concepts with concrete visual\nmanifestations may be easier to learn than concepts with abstract ones. We give\nan algorithm for automatically computing the visual concreteness of words and\ntopics within multimodal datasets. We apply the approach in four settings,\nranging from image captions to images/text scraped from historical books. In\naddition to enabling explorations of concepts in multimodal datasets, our\nconcreteness scores predict the capacity of machine learning algorithms to\nlearn textual/visual relationships. We find that 1) concrete concepts are\nindeed easier to learn; 2) the large number of algorithms we consider have\nsimilar failure cases; 3) the precise positive relationship between\nconcreteness and performance varies between datasets. We conclude with\nrecommendations for using concreteness scores to facilitate future multimodal\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 15:23:04 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 19:15:45 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Hessel", "Jack", ""], ["Mimno", "David", ""], ["Lee", "Lillian", ""]]}, {"id": "1804.06812", "submitter": "Tae Joon Jun", "authors": "Tae Joon Jun, Hoang Minh Nguyen, Daeyoun Kang, Dohyeun Kim, Daeyoung\n  Kim, Young-Hak Kim", "title": "ECG arrhythmia classification using a 2-D convolutional neural network", "comments": "Submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an effective electrocardiogram (ECG) arrhythmia\nclassification method using a deep two-dimensional convolutional neural network\n(CNN) which recently shows outstanding performance in the field of pattern\nrecognition. Every ECG beat was transformed into a two-dimensional grayscale\nimage as an input data for the CNN classifier. Optimization of the proposed CNN\nclassifier includes various deep learning techniques such as batch\nnormalization, data augmentation, Xavier initialization, and dropout. In\naddition, we compared our proposed classifier with two well-known CNN models;\nAlexNet and VGGNet. ECG recordings from the MIT-BIH arrhythmia database were\nused for the evaluation of the classifier. As a result, our classifier achieved\n99.05% average accuracy with 97.85% average sensitivity. To precisely validate\nour CNN classifier, 10-fold cross-validation was performed at the evaluation\nwhich involves every ECG recording as a test data. Our experimental results\nhave successfully validated that the proposed CNN classifier with the\ntransformed ECG images can achieve excellent classification accuracy without\nany manual pre-processing of the ECG signals such as noise filtering, feature\nextraction, and feature reduction.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 16:54:57 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Jun", "Tae Joon", ""], ["Nguyen", "Hoang Minh", ""], ["Kang", "Daeyoun", ""], ["Kim", "Dohyeun", ""], ["Kim", "Daeyoung", ""], ["Kim", "Young-Hak", ""]]}, {"id": "1804.06817", "submitter": "Tae Joon Jun", "authors": "Tae Joon Jun, Soo-Jin Kang, June-Goo Lee, Jihoon Kweon, Wonjun Na,\n  Daeyoun Kang, Dohyeun Kim, Daeyoung Kim, Young-Hak Kim", "title": "Automated detection of vulnerable plaque in intravascular ultrasound\n  images", "comments": "Submitted to journal", "journal-ref": null, "doi": "10.1007/s11517-018-1925-x", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acute Coronary Syndrome (ACS) is a syndrome caused by a decrease in blood\nflow in the coronary arteries. The ACS is usually related to coronary\nthrombosis and is primarily caused by plaque rupture followed by plaque erosion\nand calcified nodule. Thin-cap fibroatheroma (TCFA) is known to be the most\nsimilar lesion morphologically to a plaque rupture. In this paper, we propose\nmethods to classify TCFA using various machine learning classifiers including\nFeed-forward Neural Network (FNN), K-Nearest Neighbor (KNN), Random Forest (RF)\nand Convolutional Neural Network (CNN) to figure out a classifier that shows\noptimal TCFA classification accuracy. In addition, we suggest pixel range based\nfeature extraction method to extract the ratio of pixels in the different\nregion of interests to reflect the physician's TCFA discrimination criteria. A\ntotal of 12,325 IVUS images were labeled with corresponding OCT images to train\nand evaluate the classifiers. We achieved 0.884, 0.890, 0.878 and 0.933 Area\nUnder the ROC Curve (AUC) in the order of using FNN, KNN, RF and CNN\nclassifier. As a result, the CNN classifier performed best and the top 10\nfeatures of the feature-based classifiers (FNN, KNN, RF) were found to be\nsimilar to the physician's TCFA diagnostic criteria.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 17:09:24 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Jun", "Tae Joon", ""], ["Kang", "Soo-Jin", ""], ["Lee", "June-Goo", ""], ["Kweon", "Jihoon", ""], ["Na", "Wonjun", ""], ["Kang", "Daeyoun", ""], ["Kim", "Dohyeun", ""], ["Kim", "Daeyoung", ""], ["Kim", "Young-Hak", ""]]}, {"id": "1804.06821", "submitter": "Tae Joon Jun", "authors": "Tae Joon Jun, Dohyeun Kim, Daeyoung Kim", "title": "Automated diagnosis of pneumothorax using an ensemble of convolutional\n  neural networks with multi-sized chest radiography images", "comments": "Submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pneumothorax is a relatively common disease, but in some cases, it may be\ndifficult to find with chest radiography. In this paper, we propose a novel\nmethod of detecting pneumothorax in chest radiography. We propose an ensemble\nmodel of identical convolutional neural networks (CNN) with three different\nsizes of radiography images. Conventional methods may not properly characterize\nlost features while resizing large size images into 256 x 256 or 224 x 224\nsizes. Our model is evaluated with ChestX-ray dataset which contains over\n100,000 chest radiography images. As a result of the experiment, the proposed\nmodel showed AUC 0.911, which is the state of the art result in pneumothorax\ndetection. Our method is expected to be effective when applying CNN to large\nsize medical images.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 17:14:54 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Jun", "Tae Joon", ""], ["Kim", "Dohyeun", ""], ["Kim", "Daeyoung", ""]]}, {"id": "1804.06833", "submitter": "Goutam Bhat", "authors": "Goutam Bhat, Joakim Johnander, Martin Danelljan, Fahad Shahbaz Khan,\n  Michael Felsberg", "title": "Unveiling the Power of Deep Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of generic object tracking numerous attempts have been made to\nexploit deep features. Despite all expectations, deep trackers are yet to reach\nan outstanding level of performance compared to methods solely based on\nhandcrafted features. In this paper, we investigate this key issue and propose\nan approach to unlock the true potential of deep features for tracking. We\nsystematically study the characteristics of both deep and shallow features, and\ntheir relation to tracking accuracy and robustness. We identify the limited\ndata and low spatial resolution as the main challenges, and propose strategies\nto counter these issues when integrating deep features for tracking.\nFurthermore, we propose a novel adaptive fusion approach that leverages the\ncomplementary properties of deep and shallow features to improve both\nrobustness and accuracy. Extensive experiments are performed on four\nchallenging datasets. On VOT2017, our approach significantly outperforms the\ntop performing tracker from the challenge with a relative gain of 17% in EAO.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 17:40:44 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Bhat", "Goutam", ""], ["Johnander", "Joakim", ""], ["Danelljan", "Martin", ""], ["Khan", "Fahad Shahbaz", ""], ["Felsberg", "Michael", ""]]}, {"id": "1804.06870", "submitter": "Hao Tan", "authors": "Hao Tan, Mohit Bansal", "title": "Object Ordering with Bidirectional Matchings for Visual Reasoning", "comments": "NAACL 2018 (8 pages; added pointer-ordering examples)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual reasoning with compositional natural language instructions, e.g.,\nbased on the newly-released Cornell Natural Language Visual Reasoning (NLVR)\ndataset, is a challenging task, where the model needs to have the ability to\ncreate an accurate mapping between the diverse phrases and the several objects\nplaced in complex arrangements in the image. Further, this mapping needs to be\nprocessed to answer the question in the statement given the ordering and\nrelationship of the objects across three similar images. In this paper, we\npropose a novel end-to-end neural model for the NLVR task, where we first use\njoint bidirectional attention to build a two-way conditioning between the\nvisual information and the language phrases. Next, we use an RL-based pointer\nnetwork to sort and process the varying number of unordered objects (so as to\nmatch the order of the statement phrases) in each of the three images and then\npool over the three decisions. Our model achieves strong improvements (of 4-6%\nabsolute) over the state-of-the-art on both the structured representation and\nraw image versions of the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 18:39:17 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 16:56:32 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "1804.06882", "submitter": "Robert J. Wang", "authors": "Robert J. Wang, Xiang Li, Charles X. Ling", "title": "Pelee: A Real-Time Object Detection System on Mobile Devices", "comments": "Accepted to NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing need of running Convolutional Neural Network (CNN) models on\nmobile devices with limited computing power and memory resource encourages\nstudies on efficient model design. A number of efficient architectures have\nbeen proposed in recent years, for example, MobileNet, ShuffleNet, and\nMobileNetV2. However, all these models are heavily dependent on depthwise\nseparable convolution which lacks efficient implementation in most deep\nlearning frameworks. In this study, we propose an efficient architecture named\nPeleeNet, which is built with conventional convolution instead. On ImageNet\nILSVRC 2012 dataset, our proposed PeleeNet achieves a higher accuracy and over\n1.8 times faster speed than MobileNet and MobileNetV2 on NVIDIA TX2. Meanwhile,\nPeleeNet is only 66% of the model size of MobileNet. We then propose a\nreal-time object detection system by combining PeleeNet with Single Shot\nMultiBox Detector (SSD) method and optimizing the architecture for fast speed.\nOur proposed detection system2, named Pelee, achieves 76.4% mAP (mean average\nprecision) on PASCAL VOC2007 and 22.4 mAP on MS COCO dataset at the speed of\n23.6 FPS on iPhone 8 and 125 FPS on NVIDIA TX2. The result on COCO outperforms\nYOLOv2 in consideration of a higher precision, 13.6 times lower computational\ncost and 11.3 times smaller model size.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 19:27:27 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 00:25:30 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2019 05:46:59 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Wang", "Robert J.", ""], ["Li", "Xiang", ""], ["Ling", "Charles X.", ""]]}, {"id": "1804.06913", "submitter": "Nhan Tran", "authors": "Javier Duarte, Song Han, Philip Harris, Sergo Jindariani, Edward\n  Kreinar, Benjamin Kreis, Jennifer Ngadiuba, Maurizio Pierini, Ryan Rivera,\n  Nhan Tran, Zhenbin Wu", "title": "Fast inference of deep neural networks in FPGAs for particle physics", "comments": "22 pages, 17 figures, 2 tables, JINST revision", "journal-ref": "JINST 13 P07027 (2018)", "doi": "10.1088/1748-0221/13/07/P07027", "report-no": "FERMILAB-PUB-18-089-E", "categories": "physics.ins-det cs.CV hep-ex stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results at the Large Hadron Collider (LHC) have pointed to enhanced\nphysics capabilities through the improvement of the real-time event processing\ntechniques. Machine learning methods are ubiquitous and have proven to be very\npowerful in LHC physics, and particle physics as a whole. However, exploration\nof the use of such techniques in low-latency, low-power FPGA hardware has only\njust begun. FPGA-based trigger and data acquisition (DAQ) systems have\nextremely low, sub-microsecond latency requirements that are unique to particle\nphysics. We present a case study for neural network inference in FPGAs focusing\non a classifier for jet substructure which would enable, among many other\nphysics scenarios, searches for new dark sector particles and novel\nmeasurements of the Higgs boson. While we focus on a specific example, the\nlessons are far-reaching. We develop a package based on High-Level Synthesis\n(HLS) called hls4ml to build machine learning models in FPGAs. The use of HLS\nincreases accessibility across a broad user community and allows for a drastic\ndecrease in firmware development time. We map out FPGA resource usage and\nlatency versus neural network hyperparameters to identify the problems in\nparticle physics that would benefit from performing neural network inference\nwith FPGAs. For our example jet substructure model, we fit well within the\navailable resources of modern FPGAs with a latency on the scale of 100 ns.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 18:00:02 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 16:46:35 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 21:43:10 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Duarte", "Javier", ""], ["Han", "Song", ""], ["Harris", "Philip", ""], ["Jindariani", "Sergo", ""], ["Kreinar", "Edward", ""], ["Kreis", "Benjamin", ""], ["Ngadiuba", "Jennifer", ""], ["Pierini", "Maurizio", ""], ["Rivera", "Ryan", ""], ["Tran", "Nhan", ""], ["Wu", "Zhenbin", ""]]}, {"id": "1804.06919", "submitter": "Chao-Yuan Wu", "authors": "Chao-Yuan Wu, Nayan Singhal, Philipp Kr\\\"ahenb\\\"uhl", "title": "Video Compression through Image Interpolation", "comments": "Project page: https://chaoyuaw.github.io/vcii/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ever increasing amount of our digital communication, media consumption,\nand content creation revolves around videos. We share, watch, and archive many\naspects of our lives through them, all of which are powered by strong video\ncompression. Traditional video compression is laboriously hand designed and\nhand optimized. This paper presents an alternative in an end-to-end deep\nlearning codec. Our codec builds on one simple idea: Video compression is\nrepeated image interpolation. It thus benefits from recent advances in deep\nimage interpolation and generation. Our deep video codec outperforms today's\nprevailing codecs, such as H.261, MPEG-4 Part 2, and performs on par with\nH.264.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 21:18:01 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Wu", "Chao-Yuan", ""], ["Singhal", "Nayan", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""]]}, {"id": "1804.06955", "submitter": "Yoshihide Sawada PhD", "authors": "Yoshihide Sawada", "title": "Disentangling Controllable and Uncontrollable Factors of Variation by\n  Interacting with the World", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to disentangle controllable and uncontrollable factors\nof variation by interacting with the world. Disentanglement leads to good\nrepresentations and is important when applying deep neural networks (DNNs) in\nfields where explanations are required. This study attempts to improve an\nexisting reinforcement learning (RL) approach to disentangle controllable and\nuncontrollable factors of variation, because the method lacks a mechanism to\nrepresent uncontrollable obstacles. To address this problem, we train two DNNs\nsimultaneously: one that represents the controllable object and another that\nrepresents uncontrollable obstacles. For stable training, we applied a\npretraining approach using a model robust against uncontrollable obstacles.\nSimulation experiments demonstrate that the proposed model can disentangle\nindependently controllable and uncontrollable factors without annotated data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 00:53:34 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 00:10:17 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Sawada", "Yoshihide", ""]]}, {"id": "1804.06958", "submitter": "Saeed Amirgholipour Kasmani", "authors": "Saeed Amirgholipour Kasmani, Xiangjian He, Wenjing Jia, Dadong Wang,\n  Michelle Zeibots", "title": "A-CCNN: adaptive ccnn for density estimation and crowd counting", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting, for estimating the number of people in a crowd using\nvision-based computer techniques, has attracted much interest in the research\ncommunity. Although many attempts have been reported, real-world problems, such\nas huge variation in subjects' sizes in images and serious occlusion among\npeople, make it still a challenging problem. In this paper, we propose an\nAdaptive Counting Convolutional Neural Network (A-CCNN) and consider the scale\nvariation of objects in a frame adaptively so as to improve the accuracy of\ncounting. Our method takes advantages of contextual information to provide more\naccurate and adaptive density maps and crowd counting in a scene. Extensively\nexperimental evaluation is conducted using different benchmark datasets for\nobject-counting and shows that the proposed approach is effective and\noutperforms state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 01:01:16 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 03:57:26 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Kasmani", "Saeed Amirgholipour", ""], ["He", "Xiangjian", ""], ["Jia", "Wenjing", ""], ["Wang", "Dadong", ""], ["Zeibots", "Michelle", ""]]}, {"id": "1804.06962", "submitter": "Xiaolin Zhang", "authors": "Xiaolin Zhang, Yunchao Wei, Jiashi Feng, Yi Yang, Thomas Huang", "title": "Adversarial Complementary Learning for Weakly Supervised Object\n  Localization", "comments": "CVPR 2018 Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose Adversarial Complementary Learning (ACoL) to\nautomatically localize integral objects of semantic interest with weak\nsupervision. We first mathematically prove that class localization maps can be\nobtained by directly selecting the class-specific feature maps of the last\nconvolutional layer, which paves a simple way to identify object regions. We\nthen present a simple network architecture including two parallel-classifiers\nfor object localization. Specifically, we leverage one classification branch to\ndynamically localize some discriminative object regions during the forward\npass. Although it is usually responsive to sparse parts of the target objects,\nthis classifier can drive the counterpart classifier to discover new and\ncomplementary object regions by erasing its discovered regions from the feature\nmaps. With such an adversarial learning, the two parallel-classifiers are\nforced to leverage complementary object regions for classification and can\nfinally generate integral object localization together. The merits of ACoL are\nmainly two-fold: 1) it can be trained in an end-to-end manner; 2) dynamically\nerasing enables the counterpart classifier to discover complementary object\nregions more effectively. We demonstrate the superiority of our ACoL approach\nin a variety of experiments. In particular, the Top-1 localization error rate\non the ILSVRC dataset is 45.14%, which is the new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 01:17:40 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Zhang", "Xiaolin", ""], ["Wei", "Yunchao", ""], ["Feng", "Jiashi", ""], ["Yang", "Yi", ""], ["Huang", "Thomas", ""]]}, {"id": "1804.06964", "submitter": "Siyu Huang", "authors": "Siyu Huang, Xi Li, Zhi-Qi Cheng, Zhongfei Zhang, Alexander Hauptmann", "title": "GNAS: A Greedy Neural Architecture Search Method for Multi-Attribute\n  Learning", "comments": "ACM MM 2018 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key problem in deep multi-attribute learning is to effectively discover the\ninter-attribute correlation structures. Typically, the conventional deep\nmulti-attribute learning approaches follow the pipeline of manually designing\nthe network architectures based on task-specific expertise prior knowledge and\ncareful network tunings, leading to the inflexibility for various complicated\nscenarios in practice. Motivated by addressing this problem, we propose an\nefficient greedy neural architecture search approach (GNAS) to automatically\ndiscover the optimal tree-like deep architecture for multi-attribute learning.\nIn a greedy manner, GNAS divides the optimization of global architecture into\nthe optimizations of individual connections step by step. By iteratively\nupdating the local architectures, the global tree-like architecture gets\nconverged where the bottom layers are shared across relevant attributes and the\nbranches in top layers more encode attribute-specific features. Experiments on\nthree benchmark multi-attribute datasets show the effectiveness and compactness\nof neural architectures derived by GNAS, and also demonstrate the efficiency of\nGNAS in searching neural architectures.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 01:29:00 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 21:45:17 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Huang", "Siyu", ""], ["Li", "Xi", ""], ["Cheng", "Zhi-Qi", ""], ["Zhang", "Zhongfei", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1804.06992", "submitter": "Hui Li", "authors": "Hui Li, Xiao-Jun Wu, Josef Kittler", "title": "Infrared and Visible Image Fusion using a Deep Learning Framework", "comments": "6 pages, 6 figures, 2 tables, ICPR 2018(accepted)", "journal-ref": null, "doi": "10.1109/ICPR.2018.8546006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning has become a very active research tool which\nis used in many image processing fields. In this paper, we propose an effective\nimage fusion method using a deep learning framework to generate a single image\nwhich contains all the features from infrared and visible images. First, the\nsource images are decomposed into base parts and detail content. Then the base\nparts are fused by weighted-averaging. For the detail content, we use a deep\nlearning network to extract multi-layer features. Using these features, we use\nl_1-norm and weighted-average strategy to generate several candidates of the\nfused detail content. Once we get these candidates, the max selection strategy\nis used to get final fused detail content. Finally, the fused image will be\nreconstructed by combining the fused base part and detail content. The\nexperimental results demonstrate that our proposed method achieves\nstate-of-the-art performance in both objective assessment and visual quality.\nThe Code of our fusion method is available at\nhttps://github.com/hli1221/imagefusion_deeplearning\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 04:30:08 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 11:34:40 GMT"}, {"version": "v3", "created": "Sat, 19 May 2018 08:45:41 GMT"}, {"version": "v4", "created": "Tue, 18 Dec 2018 08:36:54 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Li", "Hui", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1804.07006", "submitter": "Peng Gao", "authors": "Peng Gao, Yipeng Ma, Ke Song, Chao Li, Fei Wang, Liyi Xiao", "title": "Large Margin Structured Convolution Operator for Thermal Infrared Object\n  Tracking", "comments": "Accepted as contributed paper in ICPR'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with visible object tracking, thermal infrared (TIR) object tracking\ncan track an arbitrary target in total darkness since it cannot be influenced\nby illumination variations. However, there are many unwanted attributes that\nconstrain the potentials of TIR tracking, such as the absence of visual color\npatterns and low resolutions. Recently, structured output support vector\nmachine (SOSVM) and discriminative correlation filter (DCF) have been\nsuccessfully applied to visible object tracking, respectively. Motivated by\nthese, in this paper, we propose a large margin structured convolution operator\n(LMSCO) to achieve efficient TIR object tracking. To improve the tracking\nperformance, we employ the spatial regularization and implicit interpolation to\nobtain continuous deep feature maps, including deep appearance features and\ndeep motion features, of the TIR targets. Finally, a collaborative optimization\nstrategy is exploited to significantly update the operators. Our approach not\nonly inherits the advantage of the strong discriminative capability of SOSVM\nbut also achieves accurate and robust tracking with higher-dimensional features\nand more dense samples. To the best of our knowledge, we are the first to\nincorporate the advantages of DCF and SOSVM for TIR object tracking.\nComprehensive evaluations on two thermal infrared tracking benchmarks, i.e.\nVOT-TIR2015 and VOT-TIR2016, clearly demonstrate that our LMSCO tracker\nachieves impressive results and outperforms most state-of-the-art trackers in\nterms of accuracy and robustness with sufficient frame rate.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 06:12:02 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 08:21:20 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Gao", "Peng", ""], ["Ma", "Yipeng", ""], ["Song", "Ke", ""], ["Li", "Chao", ""], ["Wang", "Fei", ""], ["Xiao", "Liyi", ""]]}, {"id": "1804.07008", "submitter": "Subrahmanyam Murala", "authors": "Murari Mandal, Prafulla Saxena, Santosh Kumar Vipparthi and\n  Subrahmanyam Murala", "title": "CANDID: Robust Change Dynamics and Deterministic Update Policy for\n  Dynamic Background Subtraction", "comments": "Accepted in ICPR-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background subtraction in video provides the preliminary information which is\nessential for many computer vision applications. In this paper, we propose a\nsequence of approaches named CANDID to handle the change detection problem in\nchallenging video scenarios. The CANDID adaptively initializes the pixel-level\ndistance threshold and update rate. These parameters are updated by computing\nthe change dynamics at a location. Further, the background model is maintained\nby formulating a deterministic update policy. The performance of the proposed\nmethod is evaluated over various challenging scenarios such as dynamic\nbackground and extreme weather conditions. The qualitative and quantitative\nmeasures of the proposed method outperform the existing state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 06:17:19 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Mandal", "Murari", ""], ["Saxena", "Prafulla", ""], ["Vipparthi", "Santosh Kumar", ""], ["Murala", "Subrahmanyam", ""]]}, {"id": "1804.07014", "submitter": "Yitian Yuan", "authors": "Yitian Yuan, Tao Mei, Wenwu Zhu", "title": "To Find Where You Talk: Temporal Sentence Localization in Video with\n  Attention Based Location Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an untrimmed video and a sentence description, temporal sentence\nlocalization aims to automatically determine the start and end points of the\ndescribed sentence within the video. The problem is challenging as it needs the\nunderstanding of both video and sentence. Existing research predominantly\nemploys a costly \"scan and localize\" framework, neglecting the global video\ncontext and the specific details within sentences which play as critical issues\nfor this problem. In this paper, we propose a novel Attention Based Location\nRegression (ABLR) approach to solve the temporal sentence localization from a\nglobal perspective. Specifically, to preserve the context information, ABLR\nfirst encodes both video and sentence via Bidirectional LSTM networks. Then, a\nmulti-modal co-attention mechanism is introduced to generate not only video\nattention which reflects the global video structure, but also sentence\nattention which highlights the crucial details for temporal localization.\nFinally, a novel attention based location regression network is designed to\npredict the temporal coordinates of sentence query from the previous attention.\nABLR is jointly trained in an end-to-end manner. Comprehensive experiments on\nActivityNet Captions and TACoS datasets demonstrate both the effectiveness and\nthe efficiency of the proposed ABLR approach.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 06:48:40 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 03:53:47 GMT"}, {"version": "v3", "created": "Sat, 1 Sep 2018 01:35:13 GMT"}, {"version": "v4", "created": "Sat, 3 Nov 2018 06:48:06 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Yuan", "Yitian", ""], ["Mei", "Tao", ""], ["Zhu", "Wenwu", ""]]}, {"id": "1804.07027", "submitter": "Junqiao Zhao", "authors": "Yan Wu, Tao Yang, Junqiao Zhao, Linting Guan, Wei Jiang", "title": "VH-HFCN based Parking Slot and Lane Markings Segmentation on Panoramic\n  Surround View", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic parking is being massively developed by car manufacturers and\nproviders. Until now, there are two problems with the automatic parking. First,\nthere is no openly-available segmentation labels of parking slot on panoramic\nsurround view (PSV) dataset. Second, how to detect parking slot and road\nstructure robustly. Therefore, in this paper, we build up a public PSV dataset.\nAt the same time, we proposed a highly fused convolutional network (HFCN) based\nsegmentation method for parking slot and lane markings based on the PSV\ndataset. A surround-view image is made of four calibrated images captured from\nfour fisheye cameras. We collect and label more than 4,200 surround view images\nfor this task, which contain various illuminated scenes of different types of\nparking slots. A VH-HFCN network is proposed, which adopts an HFCN as the base,\nwith an extra efficient VH-stage for better segmenting various markings. The\nVH-stage consists of two independent linear convolution paths with vertical and\nhorizontal convolution kernels respectively. This modification enables the\nnetwork to robustly and precisely extract linear features. We evaluated our\nmodel on the PSV dataset and the results showed outstanding performance in\nground markings segmentation. Based on the segmented markings, parking slots\nand lanes are acquired by skeletonization, hough line transform and line\narrangement.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 07:56:54 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 02:20:04 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Wu", "Yan", ""], ["Yang", "Tao", ""], ["Zhao", "Junqiao", ""], ["Guan", "Linting", ""], ["Jiang", "Wei", ""]]}, {"id": "1804.07046", "submitter": "Abhijit Guha Roy", "authors": "Abhijit Guha Roy, Sailesh Conjeti, Nassir Navab and Christian\n  Wachinger", "title": "Inherent Brain Segmentation Quality Control from Fully ConvNet Monte\n  Carlo Sampling", "comments": "Accepted at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce inherent measures for effective quality control of brain\nsegmentation based on a Bayesian fully convolutional neural network, using\nmodel uncertainty. Monte Carlo samples from the posterior distribution are\nefficiently generated using dropout at test time. Based on these samples, we\nintroduce next to a voxel-wise uncertainty map also three metrics for\nstructure-wise uncertainty. We then incorporate these structure-wise\nuncertainty in group analyses as a measure of confidence in the observation.\nOur results show that the metrics are highly correlated to segmentation\naccuracy and therefore present an inherent measure of segmentation quality.\nFurthermore, group analysis with uncertainty results in effect sizes closer to\nthat of manual annotations. The introduced uncertainty metrics can not only be\nvery useful in translation to clinical practice but also provide automated\nquality control and group analyses in processing large data repositories.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 09:16:35 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 15:40:39 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Roy", "Abhijit Guha", ""], ["Conjeti", "Sailesh", ""], ["Navab", "Nassir", ""], ["Wachinger", "Christian", ""]]}, {"id": "1804.07056", "submitter": "Alan Lukezic", "authors": "Alan Luke\\v{z}i\\v{c}, Luka \\v{C}ehovin Zajc, Tom\\'a\\v{s} Voj\\'i\\v{r},\n  Ji\\v{r}\\'i Matas, Matej Kristan", "title": "Now you see me: evaluating performance in long-term visual tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new long-term tracking performance evaluation methodology and\npresent a new challenging dataset of carefully selected sequences with many\ntarget disappearances. We perform an extensive evaluation of six long-term and\nnine short-term state-of-the-art trackers, using new performance measures,\nsuitable for evaluating long-term tracking - tracking precision, recall and\nF-score. The evaluation shows that a good model update strategy and the\ncapability of image-wide re-detection are critical for long-term tracking\nperformance. We integrated the methodology in the VOT toolkit to automate\nexperimental analysis and benchmarking and to facilitate the development of\nlong-term trackers.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 09:41:58 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Luke\u017ei\u010d", "Alan", ""], ["Zajc", "Luka \u010cehovin", ""], ["Voj\u00ed\u0159", "Tom\u00e1\u0161", ""], ["Matas", "Ji\u0159\u00ed", ""], ["Kristan", "Matej", ""]]}, {"id": "1804.07062", "submitter": "Jiawei Su <", "authors": "Jiawei Su, Danilo Vasconcellos Vargas, Kouichi Sakurai", "title": "Attacking Convolutional Neural Network using Differential Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The output of Convolutional Neural Networks (CNN) has been shown to be\ndiscontinuous which can make the CNN image classifier vulnerable to small\nwell-tuned artificial perturbations. That is, images modified by adding such\nperturbations(i.e. adversarial perturbations) that make little difference to\nhuman eyes, can completely alter the CNN classification results. In this paper,\nwe propose a practical attack using differential evolution(DE) for generating\neffective adversarial perturbations. We comprehensively evaluate the\neffectiveness of different types of DEs for conducting the attack on different\nnetwork structures. The proposed method is a black-box attack which only\nrequires the miracle feedback of the target CNN systems. The results show that\nunder strict constraints which simultaneously control the number of pixels\nchanged and overall perturbation strength, attacking can achieve 72.29%, 78.24%\nand 61.28% non-targeted attack success rates, with 88.68%, 99.85% and 73.07%\nconfidence on average, on three common types of CNNs. The attack only requires\nmodifying 5 pixels with 20.44, 14.76 and 22.98 pixel values distortion. Thus,\nthe result shows that the current DNNs are also vulnerable to such simpler\nblack-box attacks even under very limited attack conditions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 10:05:52 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Su", "Jiawei", ""], ["Vargas", "Danilo Vasconcellos", ""], ["Sakurai", "Kouichi", ""]]}, {"id": "1804.07091", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Erik Rodner, Yanira Guanche Garcia, Joachim Denzler", "title": "Detecting Regions of Maximal Divergence for Spatio-Temporal Anomaly\n  Detection", "comments": "Accepted by TPAMI. Examples and code:\n  https://cvjena.github.io/libmaxdiv/", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  vol. 41, no. 5, pp. 1088-1101, 1 May 2019", "doi": "10.1109/TPAMI.2018.2823766", "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of anomalies in space- and time-varying measurements is\nan important tool in several fields, e.g., fraud detection, climate analysis,\nor healthcare monitoring. We present an algorithm for detecting anomalous\nregions in multivariate spatio-temporal time-series, which allows for spotting\nthe interesting parts in large amounts of data, including video and text data.\nIn opposition to existing techniques for detecting isolated anomalous data\npoints, we propose the \"Maximally Divergent Intervals\" (MDI) framework for\nunsupervised detection of coherent spatial regions and time intervals\ncharacterized by a high Kullback-Leibler divergence compared with all other\ndata given. In this regard, we define an unbiased Kullback-Leibler divergence\nthat allows for ranking regions of different size and show how to enable the\nalgorithm to run on large-scale data sets in reasonable time using an interval\nproposal technique. Experiments on both synthetic and real data from various\ndomains, such as climate analysis, video surveillance, and text forensics,\ndemonstrate that our method is widely applicable and a valuable tool for\nfinding interesting events in different types of data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 11:23:07 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 07:23:39 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Rodner", "Erik", ""], ["Garcia", "Yanira Guanche", ""], ["Denzler", "Joachim", ""]]}, {"id": "1804.07094", "submitter": "Yumin Suh", "authors": "Yumin Suh, Jingdong Wang, Siyu Tang, Tao Mei, Kyoung Mu Lee", "title": "Part-Aligned Bilinear Representations for Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel network that learns a part-aligned representation for\nperson re-identification. It handles the body part misalignment problem, that\nis, body parts are misaligned across human detections due to pose/viewpoint\nchange and unreliable detection. Our model consists of a two-stream network\n(one stream for appearance map extraction and the other one for body part map\nextraction) and a bilinear-pooling layer that generates and spatially pools a\npart-aligned map. Each local feature of the part-aligned map is obtained by a\nbilinear mapping of the corresponding local appearance and body part\ndescriptors. Our new representation leads to a robust image matching\nsimilarity, which is equivalent to an aggregation of the local similarities of\nthe corresponding body parts combined with the weighted appearance similarity.\nThis part-aligned representation reduces the part misalignment problem\nsignificantly. Our approach is also advantageous over other pose-guided\nrepresentations (e.g., extracting representations over the bounding box of each\nbody part) by learning part descriptors optimal for person re-identification.\nFor training the network, our approach does not require any part annotation on\nthe person re-identification dataset. Instead, we simply initialize the part\nsub-stream using a pre-trained sub-network of an existing pose estimation\nnetwork, and train the whole network to minimize the re-identification loss. We\nvalidate the effectiveness of our approach by demonstrating its superiority\nover the state-of-the-art methods on the standard benchmark datasets, including\nMarket-1501, CUHK03, CUHK01 and DukeMTMC, and standard video dataset MARS.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 11:35:19 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Suh", "Yumin", ""], ["Wang", "Jingdong", ""], ["Tang", "Siyu", ""], ["Mei", "Tao", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1804.07098", "submitter": "Wouter Bulten", "authors": "Wouter Bulten, Geert Litjens", "title": "Unsupervised Prostate Cancer Detection on H&E using Convolutional\n  Adversarial Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised method using self-clustering convolutional\nadversarial autoencoders to classify prostate tissue as tumor or non-tumor\nwithout any labeled training data. The clustering method is integrated into the\ntraining of the autoencoder and requires only little post-processing. Our\nnetwork trains on hematoxylin and eosin (H&E) input patches and we tested two\ndifferent reconstruction targets, H&E and immunohistochemistry (IHC). We show\nthat antibody-driven feature learning using IHC helps the network to learn\nrelevant features for the clustering task. Our network achieves a F1 score of\n0.62 using only a small set of validation labels to assign classes to clusters.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 11:40:23 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Bulten", "Wouter", ""], ["Litjens", "Geert", ""]]}, {"id": "1804.07116", "submitter": "Li Qingbiao", "authors": "Qing-Biao Li, Xiao-Yun Zhou, Jianyu Lin, Jian-Qing Zheng, Neil T.\n  Clancy, Daniel S. Elson", "title": "Estimation of Tissue Oxygen Saturation from RGB Images based on\n  Pixel-level Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intra-operative measurement of tissue oxygen saturation (StO2) has been\nwidely explored by pulse oximetry or hyperspectral imaging (HSI) to assess the\nfunction and viability of tissue. In this paper we propose a pixel- level\nimage-to-image translation approach based on conditional Generative Adversarial\nNetworks (cGAN) to estimate tissue oxygen saturation (StO2) directly from RGB\nimages. The real-time performance and non-reliance on additional hardware,\nenable a seamless integration of the proposed method into surgical and\ndiagnostic workflows with standard endoscope systems. For validation, RGB\nimages and StO2 ground truth were simulated and estimated from HSI images\ncollected by a liquid crystal tuneable filter (LCTF) endoscope for three tissue\ntypes (porcine bowel, lamb uterus and rabbit uterus). The result show that the\nproposed method can achieve visually identical images with comparable accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 12:41:21 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Li", "Qing-Biao", ""], ["Zhou", "Xiao-Yun", ""], ["Lin", "Jianyu", ""], ["Zheng", "Jian-Qing", ""], ["Clancy", "Neil T.", ""], ["Elson", "Daniel S.", ""]]}, {"id": "1804.07155", "submitter": "Ludmila Kuncheva", "authors": "Ludmila I. Kuncheva, \\'Alvar Arnaiz-Gonz\\'alez, Jos\\'e-Francisco\n  D\\'iez-Pastor, and Iain A. D. Gunn", "title": "Instance Selection Improves Geometric Mean Accuracy: A Study on\n  Imbalanced Data Classification", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural way of handling imbalanced data is to attempt to equalise the class\nfrequencies and train the classifier of choice on balanced data. For two-class\nimbalanced problems, the classification success is typically measured by the\ngeometric mean (GM) of the true positive and true negative rates. Here we prove\nthat GM can be improved upon by instance selection, and give the theoretical\nconditions for such an improvement. We demonstrate that GM is non-monotonic\nwith respect to the number of retained instances, which discourages systematic\ninstance selection. We also show that balancing the distribution frequencies is\ninferior to a direct maximisation of GM. To verify our theoretical findings, we\ncarried out an experimental study of 12 instance selection methods for\nimbalanced data, using 66 standard benchmark data sets. The results reveal\npossible room for new instance selection methods for imbalanced data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 13:32:50 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Kuncheva", "Ludmila I.", ""], ["Arnaiz-Gonz\u00e1lez", "\u00c1lvar", ""], ["D\u00edez-Pastor", "Jos\u00e9-Francisco", ""], ["Gunn", "Iain A. D.", ""]]}, {"id": "1804.07172", "submitter": "Julian Krebs", "authors": "Julian Krebs, Tommaso Mansi, Boris Mailh\\'e, Nicholas Ayache and\n  Herv\\'e Delingette", "title": "Unsupervised Probabilistic Deformation Modeling for Robust Diffeomorphic\n  Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deformable registration algorithm based on unsupervised learning\nof a low-dimensional probabilistic parameterization of deformations. We model\nregistration in a probabilistic and generative fashion, by applying a\nconditional variational autoencoder (CVAE) network. This model enables to also\ngenerate normal or pathological deformations of any new image based on the\nprobabilistic latent space. Most recent learning-based registration algorithms\nuse supervised labels or deformation models, that miss important properties\nsuch as diffeomorphism and sufficiently regular deformation fields. In this\nwork, we constrain transformations to be diffeomorphic by using a\ndifferentiable exponentiation layer with a symmetric loss function. We\nevaluated our method on 330 cardiac MR sequences and demonstrate robust\nintra-subject registration results comparable to two state-of-the-art methods\nbut with more regular deformation fields compared to a recent learning-based\nalgorithm. Our method reached a mean DICE score of 78.3% and a mean Hausdorff\ndistance of 7.9mm. In two preliminary experiments, we illustrate the model's\nabilities to transport pathological deformations to healthy subjects and to\ncluster five diseases in the unsupervised deformation encoding space with a\nclassification performance of 70%.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 13:53:04 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 13:03:25 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Krebs", "Julian", ""], ["Mansi", "Tommaso", ""], ["Mailh\u00e9", "Boris", ""], ["Ayache", "Nicholas", ""], ["Delingette", "Herv\u00e9", ""]]}, {"id": "1804.07177", "submitter": "Stefan Kahl", "authors": "Stefan Kahl, Thomas Wilhelm-Stein, Holger Klinck, Danny Kowerko,\n  Maximilian Eibl", "title": "Recognizing Birds from Sound - The 2018 BirdCLEF Baseline System", "comments": "The repository and a continuative tutorial can be found here:\n  https://github.com/kahst/BirdCLEF-Baseline", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable identification of bird species in recorded audio files would be a\ntransformative tool for researchers, conservation biologists, and birders. In\nrecent years, artificial neural networks have greatly improved the detection\nquality of machine learning systems for bird species recognition. We present a\nbaseline system using convolutional neural networks. We publish our code base\nas reference for participants in the 2018 LifeCLEF bird identification task and\ndiscuss our experiments and potential improvements.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 14:01:01 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Kahl", "Stefan", ""], ["Wilhelm-Stein", "Thomas", ""], ["Klinck", "Holger", ""], ["Kowerko", "Danny", ""], ["Eibl", "Maximilian", ""]]}, {"id": "1804.07187", "submitter": "Okan K\\\"op\\\"ukl\\\"u", "authors": "Okan K\\\"op\\\"ukl\\\"u, Neslihan K\\\"ose, Gerhard Rigoll", "title": "Motion Fused Frames: Data Level Fusion Strategy for Hand Gesture\n  Recognition", "comments": "Accepted to CVPR 2018 as workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring spatio-temporal states of an action is the most crucial step for\naction classification. In this paper, we propose a data level fusion strategy,\nMotion Fused Frames (MFFs), designed to fuse motion information into static\nimages as better representatives of spatio-temporal states of an action. MFFs\ncan be used as input to any deep learning architecture with very little\nmodification on the network. We evaluate MFFs on hand gesture recognition tasks\nusing three video datasets - Jester, ChaLearn LAP IsoGD and NVIDIA Dynamic Hand\nGesture Datasets - which require capturing long-term temporal relations of hand\nmovements. Our approach obtains very competitive performance on Jester and\nChaLearn benchmarks with the classification accuracies of 96.28% and 57.4%,\nrespectively, while achieving state-of-the-art performance with 84.7% accuracy\non NVIDIA benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 14:20:50 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 08:12:39 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["K\u00f6se", "Neslihan", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1804.07237", "submitter": "Jiamiao Xu", "authors": "Jiamiao Xu, Shujian Yu, Xinge You, Mengjun Leng, Xiao-Yuan Jing and C.\n  L. Philip Chen", "title": "Multi-view Hybrid Embedding: A Divide-and-Conquer Approach", "comments": "This paper has been accepted by IEEE Transactions on Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel cross-view classification algorithm where the gallery and\nprobe data come from different views. A popular approach to tackle this problem\nis the multi-view subspace learning (MvSL) that aims to learn a latent subspace\nshared by multi-view data. Despite promising results obtained on some\napplications, the performance of existing methods deteriorates dramatically\nwhen the multi-view data is sampled from nonlinear manifolds or suffers from\nheavy outliers. To circumvent this drawback, motivated by the\nDivide-and-Conquer strategy, we propose Multi-view Hybrid Embedding (MvHE), a\nunique method of dividing the problem of cross-view classification into three\nsubproblems and building one model for each subproblem. Specifically, the first\nmodel is designed to remove view discrepancy, whereas the second and third\nmodels attempt to discover the intrinsic nonlinear structure and to increase\ndiscriminability in intra-view and inter-view samples respectively. The kernel\nextension is conducted to further boost the representation power of MvHE.\nExtensive experiments are conducted on four benchmark datasets. Our methods\ndemonstrate overwhelming advantages against the state-of-the-art MvSL based\ncross-view classification approaches in terms of classification accuracy and\nrobustness.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 15:38:15 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 10:46:35 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Xu", "Jiamiao", ""], ["Yu", "Shujian", ""], ["You", "Xinge", ""], ["Leng", "Mengjun", ""], ["Jing", "Xiao-Yuan", ""], ["Chen", "C. L. Philip", ""]]}, {"id": "1804.07345", "submitter": "Sanjeel Parekh", "authors": "Sanjeel Parekh, Slim Essid, Alexey Ozerov, Ngoc Q. K. Duong, Patrick\n  P\\'erez, Ga\\\"el Richard", "title": "Weakly Supervised Representation Learning for Unsynchronized\n  Audio-Visual Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio-visual representation learning is an important task from the\nperspective of designing machines with the ability to understand complex\nevents. To this end, we propose a novel multimodal framework that instantiates\nmultiple instance learning. We show that the learnt representations are useful\nfor classifying events and localizing their characteristic audio-visual\nelements. The system is trained using only video-level event labels without any\ntiming information. An important feature of our method is its capacity to learn\nfrom unsynchronized audio-visual events. We achieve state-of-the-art results on\na large-scale dataset of weakly-labeled audio event videos. Visualizations of\nlocalized visual regions and audio segments substantiate our system's efficacy,\nespecially when dealing with noisy situations where modality-specific cues\nappear asynchronously.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 19:33:11 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 16:34:42 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Parekh", "Sanjeel", ""], ["Essid", "Slim", ""], ["Ozerov", "Alexey", ""], ["Duong", "Ngoc Q. K.", ""], ["P\u00e9rez", "Patrick", ""], ["Richard", "Ga\u00ebl", ""]]}, {"id": "1804.07351", "submitter": "Seong Jae Hwang", "authors": "Seong Jae Hwang, Ronak Mehta, Hyunwoo J. Kim, Vikas Singh", "title": "Sampling-free Uncertainty Estimation in Gated Recurrent Units with\n  Exponential Families", "comments": "Version 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been a concerted effort to derive mechanisms in vision and\nmachine learning systems to offer uncertainty estimates of the predictions they\nmake. Clearly, there are enormous benefits to a system that is not only\naccurate but also has a sense for when it is not sure. Existing proposals\ncenter around Bayesian interpretations of modern deep architectures -- these\nare effective but can often be computationally demanding. We show how classical\nideas in the literature on exponential families on probabilistic networks\nprovide an excellent starting point to derive uncertainty estimates in Gated\nRecurrent Units (GRU). Our proposal directly quantifies uncertainty\ndeterministically, without the need for costly sampling-based estimation. We\ndemonstrate how our model can be used to quantitatively and qualitatively\nmeasure uncertainty in unsupervised image sequence prediction. To our\nknowledge, this is the first result describing sampling-free uncertainty\nestimation for powerful sequential models such as GRUs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 19:40:47 GMT"}, {"version": "v2", "created": "Sun, 2 Sep 2018 20:00:32 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Hwang", "Seong Jae", ""], ["Mehta", "Ronak", ""], ["Kim", "Hyunwoo J.", ""], ["Singh", "Vikas", ""]]}, {"id": "1804.07353", "submitter": "Yuqian Zhou", "authors": "Yuqian Zhou, Kuangxiao Gu, Thomas Huang", "title": "Unsupervised Representation Adversarial Learning Network: from\n  Reconstruction to Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good representation for arbitrarily complicated data should have the\ncapability of semantic generation, clustering and reconstruction. Previous\nresearch has already achieved impressive performance on either one. This paper\naims at learning a disentangled representation effective for all of them in an\nunsupervised way. To achieve all the three tasks together, we learn the forward\nand inverse mapping between data and representation on the basis of a symmetric\nadversarial process. In theory, we minimize the upper bound of the two\nconditional entropy loss between the latent variables and the observations\ntogether to achieve the cycle consistency. The newly proposed RepGAN is tested\non MNIST, fashionMNIST, CelebA, and SVHN datasets to perform unsupervised\nclassification, generation and reconstruction tasks. The result demonstrates\nthat RepGAN is able to learn a useful and competitive representation. To the\nauthor's knowledge, our work is the first one to achieve both a high\nunsupervised classification accuracy and low reconstruction error on MNIST.\nCodes are available at https://github.com/yzhouas/RepGAN-tensorflow.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 19:42:22 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 16:44:38 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Zhou", "Yuqian", ""], ["Gu", "Kuangxiao", ""], ["Huang", "Thomas", ""]]}, {"id": "1804.07362", "submitter": "Yuqian Zhou", "authors": "Yuqian Zhou, Ding Liu, Thomas Huang", "title": "Survey of Face Detection on Low-quality Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection is a well-explored problem. Many challenges on face detectors\nlike extreme pose, illumination, low resolution and small scales are studied in\nthe previous work. However, previous proposed models are mostly trained and\ntested on good-quality images which are not always the case for practical\napplications like surveillance systems. In this paper, we first review the\ncurrent state-of-the-art face detectors and their performance on benchmark\ndataset FDDB, and compare the design protocols of the algorithms. Secondly, we\ninvestigate their performance degradation while testing on low-quality images\nwith different levels of blur, noise, and contrast. Our results demonstrate\nthat both hand-crafted and deep-learning based face detectors are not robust\nenough for low-quality images. It inspires researchers to produce more robust\ndesign for face detection in the wild.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 20:13:36 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Zhou", "Yuqian", ""], ["Liu", "Ding", ""], ["Huang", "Thomas", ""]]}, {"id": "1804.07399", "submitter": "Shubham Dash", "authors": "Akash Ganesan, Divyansh Pal, Karthik Muthuraman, Shubham Dash", "title": "Video based Contextual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The primary aim of this project is to build a contextual Question-Answering\nmodel for videos. The current methodologies provide a robust model for image\nbased Question-Answering, but we are aim to generalize this approach to be\nvideos. We propose a graphical representation of video which is able to handle\nseveral types of queries across the whole video. For example, if a frame has an\nimage of a man and a cat sitting, it should be able to handle queries like,\nwhere is the cat sitting with respect to the man? or ,what is the man holding\nin his hand?. It should be able to answer queries relating to temporal\nrelationships also.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 23:06:00 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Ganesan", "Akash", ""], ["Pal", "Divyansh", ""], ["Muthuraman", "Karthik", ""], ["Dash", "Shubham", ""]]}, {"id": "1804.07427", "submitter": "Sergey Alexandrov", "authors": "Sergey V. Alexandrov, Johann Prankl, Michael Zillich, Markus Vincze", "title": "High Dynamic Range SLAM with Map-Aware Exposure Time Control", "comments": "3DV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research in dense online 3D mapping is mostly focused on the geometrical\naccuracy and spatial extent of the reconstructions. Their color appearance is\noften neglected, leading to inconsistent colors and noticeable artifacts. We\nrectify this by extending a state-of-the-art SLAM system to accumulate colors\nin HDR space. We replace the simplistic pixel intensity averaging scheme with\nHDR color fusion rules tailored to the incremental nature of SLAM and a noise\nmodel suitable for off-the-shelf RGB-D cameras. Our main contribution is a\nmap-aware exposure time controller. It makes decisions based on the global\nstate of the map and predicted camera motion, attempting to maximize the\ninformation gain of each observation. We report a set of experiments\ndemonstrating the improved texture quality and advantages of using the custom\ncontroller that is tightly integrated in the mapping loop.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 02:26:09 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Alexandrov", "Sergey V.", ""], ["Prankl", "Johann", ""], ["Zillich", "Michael", ""], ["Vincze", "Markus", ""]]}, {"id": "1804.07436", "submitter": "Arvind Balachandrasekaran", "authors": "Arvind Balachandrasekaran, Merry Mani and Mathews Jacob", "title": "Calibration-free B0 correction of EPI data using structured low rank\n  matrix recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a structured low rank algorithm for the calibration-free\ncompensation of field inhomogeneity artifacts in Echo Planar Imaging (EPI) MRI\ndata. We acquire the data using two EPI readouts that differ in echo-time (TE).\nUsing time segmentation, we reformulate the field inhomogeneity compensation\nproblem as the recovery of an image time series from highly undersampled\nFourier measurements. The temporal profile at each pixel is modeled as a single\nexponential, which is exploited to fill in the missing entries. We show that\nthe exponential behavior at each pixel, along with the spatial smoothness of\nthe exponential parameters, can be exploited to derive a 3D annihilation\nrelation in the Fourier domain. This relation translates to a low rank property\non a structured multi-fold Toeplitz matrix, whose entries correspond to the\nmeasured k-space samples. We introduce a fast two-step algorithm for the\ncompletion of the Toeplitz matrix from the available samples. In the first\nstep, we estimate the null space vectors of the Toeplitz matrix using only its\nfully sampled rows. The null space is then used to estimate the signal\nsubspace, which facilitates the efficient recovery of the time series of\nimages. We finally demonstrate the proposed approach on spherical MR phantom\ndata and human data and show that the artifacts are significantly reduced. The\nproposed approach could potentially be used to compensate for time varying\nfield map variations in dynamic applications such as functional MRI.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 03:13:27 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Balachandrasekaran", "Arvind", ""], ["Mani", "Merry", ""], ["Jacob", "Mathews", ""]]}, {"id": "1804.07437", "submitter": "Longyin Wen", "authors": "Pengfei Zhu, Longyin Wen, Xiao Bian, Haibin Ling, Qinghua Hu", "title": "Vision Meets Drones: A Challenge", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a large-scale visual object detection and tracking\nbenchmark, named VisDrone2018, aiming at advancing visual understanding tasks\non the drone platform. The images and video sequences in the benchmark were\ncaptured over various urban/suburban areas of 14 different cities across China\nfrom north to south. Specifically, VisDrone2018 consists of 263 video clips and\n10,209 images (no overlap with video clips) with rich annotations, including\nobject bounding boxes, object categories, occlusion, truncation ratios, etc.\nWith intensive amount of effort, our benchmark has more than 2.5 million\nannotated instances in 179,264 images/video frames. Being the largest such\ndataset ever published, the benchmark enables extensive evaluation and\ninvestigation of visual analysis algorithms on the drone platform. In\nparticular, we design four popular tasks with the benchmark, including object\ndetection in images, object detection in videos, single object tracking, and\nmulti-object tracking. All these tasks are extremely challenging in the\nproposed dataset due to factors such as occlusion, large scale and pose\nvariation, and fast motion. We hope the benchmark largely boost the research\nand development in visual analysis on drone platforms.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 03:19:21 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 02:49:46 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Zhu", "Pengfei", ""], ["Wen", "Longyin", ""], ["Bian", "Xiao", ""], ["Ling", "Haibin", ""], ["Hu", "Qinghua", ""]]}, {"id": "1804.07453", "submitter": "Cuiling Lan", "authors": "Pengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun Zeng, Jianru Xue,\n  Nanning Zheng", "title": "View Adaptive Neural Networks for High Performance Skeleton-based Human\n  Action Recognition", "comments": "Accepted in Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton-based human action recognition has recently attracted increasing\nattention thanks to the accessibility and the popularity of 3D skeleton data.\nOne of the key challenges in skeleton-based action recognition lies in the\nlarge view variations when capturing data. In order to alleviate the effects of\nview variations, this paper introduces a novel view adaptation scheme, which\nautomatically determines the virtual observation viewpoints in a learning based\ndata driven manner. We design two view adaptive neural networks, i.e., VA-RNN\nbased on RNN, and VA-CNN based on CNN. For each network, a novel view\nadaptation module learns and determines the most suitable observation\nviewpoints, and transforms the skeletons to those viewpoints for the end-to-end\nrecognition with a main classification network. Ablation studies find that the\nproposed view adaptive models are capable of transforming the skeletons of\nvarious viewpoints to much more consistent virtual viewpoints which largely\neliminates the viewpoint influence. In addition, we design a two-stream scheme\n(referred to as VA-fusion) that fuses the scores of the two networks to provide\nthe fused prediction. Extensive experimental evaluations on five challenging\nbenchmarks demonstrate that the effectiveness of the proposed view-adaptive\nnetworks and superior performance over state-of-the-art approaches. The source\ncode is available at\nhttps://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 05:37:47 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 03:55:23 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 08:08:52 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhang", "Pengfei", ""], ["Lan", "Cuiling", ""], ["Xing", "Junliang", ""], ["Zeng", "Wenjun", ""], ["Xue", "Jianru", ""], ["Zheng", "Nanning", ""]]}, {"id": "1804.07455", "submitter": "Doyeon Kim", "authors": "Donggyu Joo, Doyeon Kim, and Junmo Kim", "title": "Generating a Fusion Image: One's Identity and Another's Shape", "comments": "To appear in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating a novel image by manipulating two input images is an interesting\nresearch problem in the study of generative adversarial networks (GANs). We\npropose a new GAN-based network that generates a fusion image with the identity\nof input image x and the shape of input image y. Our network can simultaneously\ntrain on more than two image datasets in an unsupervised manner. We define an\nidentity loss LI to catch the identity of image x and a shape loss LS to get\nthe shape of y. In addition, we propose a novel training method called\nMin-Patch training to focus the generator on crucial parts of an image, rather\nthan its entirety. We show qualitative results on the VGG Youtube Pose dataset,\nEye dataset (MPIIGaze and UnityEyes), and the Photo-Sketch-Cartoon dataset.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 06:00:31 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Joo", "Donggyu", ""], ["Kim", "Doyeon", ""], ["Kim", "Junmo", ""]]}, {"id": "1804.07459", "submitter": "Peng Gao", "authors": "Peng Gao, Yipeng Ma, Chao Li, Ke Song, Fei Wang, Liyi Xiao", "title": "A Complementary Tracking Model with Multiple Features", "comments": "Accepted by IVPAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative Correlation Filters based tracking algorithms exploiting\nconventional handcrafted features have achieved impressive results both in\nterms of accuracy and robustness. Template handcrafted features have shown\nexcellent performance, but they perform poorly when the appearance of target\nchanges rapidly such as fast motions and fast deformations. In contrast,\nstatistical handcrafted features are insensitive to fast states changes, but\nthey yield inferior performance in the scenarios of illumination variations and\nbackground clutters. In this work, to achieve an efficient tracking\nperformance, we propose a novel visual tracking algorithm, named MFCMT, based\non a complementary ensemble model with multiple features, including Histogram\nof Oriented Gradients (HOGs), Color Names (CNs) and Color Histograms (CHs).\nAdditionally, to improve tracking results and prevent targets drift, we\nintroduce an effective fusion method by exploiting relative entropy to coalesce\nall basic response maps and get an optimal response. Furthermore, we suggest a\nsimple but efficient update strategy to boost tracking performance.\nComprehensive evaluations are conducted on two tracking benchmarks demonstrate\nand the experimental results demonstrate that our method is competitive with\nnumerous state-of-the-art trackers. Our tracker achieves impressive performance\nwith faster speed on these benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 06:23:37 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 08:32:38 GMT"}, {"version": "v3", "created": "Thu, 19 Jul 2018 08:47:44 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Gao", "Peng", ""], ["Ma", "Yipeng", ""], ["Li", "Chao", ""], ["Song", "Ke", ""], ["Wang", "Fei", ""], ["Xiao", "Liyi", ""]]}, {"id": "1804.07470", "submitter": "Shaohui Sun Dr.", "authors": "Shaohui Sun, Ramesh Sarukkai, Jack Kwok, Vinay Shet", "title": "Accurate Deep Direct Geo-Localization from Ground Imagery and\n  Phone-Grade GPS", "comments": "To appear in CVPR 2018 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most critical topics in autonomous driving or ride-sharing\ntechnology is to accurately localize vehicles in the world frame. In addition\nto common multi-view camera systems, it usually also relies on industrial grade\nsensors, such as LiDAR, differential GPS, high precision IMU, and etc. In this\npaper, we develop an approach to provide an effective solution to this problem.\nWe propose a method to train a geo-spatial deep neural network (CNN+LSTM) to\npredict accurate geo-locations (latitude and longitude) using only ordinary\nground imagery and low accuracy phone-grade GPS. We evaluate our approach on\nthe open dataset released during ACM Multimedia 2017 Grand Challenge. Having\nground truth locations for training, we are able to reach nearly lane-level\naccuracy. We also evaluate the proposed method on our own collected images in\nSan Francisco downtown area often described as \"downtown canyon\" where consumer\nGPS signals are extremely inaccurate. The results show the model can predict\nquality locations that suffice in real business applications, such as\nride-sharing, only using phone-grade GPS. Unlike classic visual localization or\nrecent PoseNet-like methods that may work well in indoor environments or\nsmall-scale outdoor environments, we avoid using a map or an SFM\n(structure-from-motion) model at all. More importantly, the proposed method can\nbe scaled up without concerns over the potential failure of 3D reconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 07:01:59 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Sun", "Shaohui", ""], ["Sarukkai", "Ramesh", ""], ["Kwok", "Jack", ""], ["Shet", "Vinay", ""]]}, {"id": "1804.07492", "submitter": "Jing Chen", "authors": "Jing Chen, Nan Li and Tianli Liao", "title": "Graph-based Hypothesis Generation for Parallax-tolerant Image Stitching", "comments": "3 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seam-driven approach has been proven fairly effective for\nparallax-tolerant image stitching, whose strategy is to search for an invisible\nseam from finite representative hypotheses of local alignment. In this paper,\nwe propose a graph-based hypothesis generation and a seam-guided local\nalignment for improving the effectiveness and the efficiency of the seam-driven\napproach. The experiment demonstrates the significant reduction of number of\nhypotheses and the improved quality of naturalness of final stitching results,\ncomparing to the state-of-the-art method SEAGULL.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 08:45:08 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Chen", "Jing", ""], ["Li", "Nan", ""], ["Liao", "Tianli", ""]]}, {"id": "1804.07493", "submitter": "Xinghao Ding", "authors": "Zhiwen Fan, Huafeng Wu, Xueyang Fu, Yue Hunag, Xinghao Ding", "title": "Residual-Guide Feature Fusion Network for Single Image Deraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image rain streaks removal is extremely important since rainy images\nadversely affect many computer vision systems. Deep learning based methods have\nfound great success in image deraining tasks. In this paper, we propose a novel\nresidual-guide feature fusion network, called ResGuideNet, for single image\nderaining that progressively predicts highquality reconstruction. Specifically,\nwe propose a cascaded network and adopt residuals generated from shallower\nblocks to guide deeper blocks. By using this strategy, we can obtain a coarse\nto fine estimation of negative residual as the blocks go deeper. The outputs of\ndifferent blocks are merged into the final reconstruction. We adopt recursive\nconvolution to build each block and apply supervision to all intermediate\nresults, which enable our model to achieve promising performance on synthetic\nand real-world data while using fewer parameters than previous required.\nResGuideNet is detachable to meet different rainy conditions. For images with\nlight rain streaks and limited computational resource at test time, we can\nobtain a decent performance even with several building blocks. Experiments\nvalidate that ResGuideNet can benefit other low- and high-level vision tasks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 08:46:40 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Fan", "Zhiwen", ""], ["Wu", "Huafeng", ""], ["Fu", "Xueyang", ""], ["Hunag", "Yue", ""], ["Ding", "Xinghao", ""]]}, {"id": "1804.07514", "submitter": "Zicheng Liao", "authors": "Zicheng Liao, Kevin Karsch, Hongyi Zhang, David Forsyth", "title": "An Approximate Shading Model with Detail Decomposition for Object\n  Relighting", "comments": null, "journal-ref": null, "doi": "10.1007/s11263-018-1090-6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an object relighting system that allows an artist to select an\nobject from an image and insert it into a target scene. Through simple\ninteractions, the system can adjust illumination on the inserted object so that\nit appears naturally in the scene. To support image-based relighting, we build\nobject model from the image, and propose a \\emph{perceptually-inspired}\napproximate shading model for the relighting. It decomposes the shading field\ninto (a) a rough shape term that can be reshaded, (b) a parametric shading\ndetail that encodes missing features from the first term, and (c) a geometric\ndetail term that captures fine-scale material properties. With this\ndecomposition, the shading model combines 3D rendering and image-based\ncomposition and allows more flexible compositing than image-based methods.\nQuantitative evaluation and a set of user studies suggest our method is a\npromising alternative to existing methods of object insertion.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 09:29:39 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Liao", "Zicheng", ""], ["Karsch", "Kevin", ""], ["Zhang", "Hongyi", ""], ["Forsyth", "David", ""]]}, {"id": "1804.07573", "submitter": "Sheng Chen", "authors": "Sheng Chen, Yang Liu, Xiang Gao, Zhen Han", "title": "MobileFaceNets: Efficient CNNs for Accurate Real-Time Face Verification\n  on Mobile Devices", "comments": "Accepted as a conference paper at CCBR 2018. Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a class of extremely efficient CNN models, MobileFaceNets, which\nuse less than 1 million parameters and are specifically tailored for\nhigh-accuracy real-time face verification on mobile and embedded devices. We\nfirst make a simple analysis on the weakness of common mobile networks for face\nverification. The weakness has been well overcome by our specifically designed\nMobileFaceNets. Under the same experimental conditions, our MobileFaceNets\nachieve significantly superior accuracy as well as more than 2 times actual\nspeedup over MobileNetV2. After trained by ArcFace loss on the refined\nMS-Celeb-1M, our single MobileFaceNet of 4.0MB size achieves 99.55% accuracy on\nLFW and 92.59% TAR@FAR1e-6 on MegaFace, which is even comparable to\nstate-of-the-art big CNN models of hundreds MB size. The fastest one of\nMobileFaceNets has an actual inference time of 18 milliseconds on a mobile\nphone. For face verification, MobileFaceNets achieve significantly improved\nefficiency over previous state-of-the-art mobile CNNs.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 12:18:57 GMT"}, {"version": "v2", "created": "Sun, 29 Apr 2018 13:20:46 GMT"}, {"version": "v3", "created": "Mon, 28 May 2018 02:38:01 GMT"}, {"version": "v4", "created": "Fri, 15 Jun 2018 02:50:58 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Chen", "Sheng", ""], ["Liu", "Yang", ""], ["Gao", "Xiang", ""], ["Han", "Zhen", ""]]}, {"id": "1804.07612", "submitter": "Dominic Masters", "authors": "Dominic Masters and Carlo Luschi", "title": "Revisiting Small Batch Training for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural network training is typically based on mini-batch\nstochastic gradient optimization. While the use of large mini-batches increases\nthe available computational parallelism, small batch training has been shown to\nprovide improved generalization performance and allows a significantly smaller\nmemory footprint, which might also be exploited to improve machine throughput.\n  In this paper, we review common assumptions on learning rate scaling and\ntraining duration, as a basis for an experimental comparison of test\nperformance for different mini-batch sizes. We adopt a learning rate that\ncorresponds to a constant average weight update per gradient calculation (i.e.,\nper unit cost of computation), and point out that this results in a variance of\nthe weight updates that increases linearly with the mini-batch size $m$.\n  The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet\ndatasets show that increasing the mini-batch size progressively reduces the\nrange of learning rates that provide stable convergence and acceptable test\nperformance. On the other hand, small mini-batch sizes provide more up-to-date\ngradient calculations, which yields more stable and reliable training. The best\nperformance has been consistently obtained for mini-batch sizes between $m = 2$\nand $m = 32$, which contrasts with recent work advocating the use of mini-batch\nsizes in the thousands.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 13:44:12 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Masters", "Dominic", ""], ["Luschi", "Carlo", ""]]}, {"id": "1804.07629", "submitter": "Soumyabrata Dev", "authors": "Shilpa Manandhar, Soumyabrata Dev, Yee Hui Lee and Yu Song Meng", "title": "Analyzing Solar Irradiance Variation From GPS and Cameras", "comments": "Published in IEEE AP-S Symposium on Antennas and Propagation and\n  USNC-URSI Radio Science Meeting, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The total amount of solar irradiance falling on the earth's surface is an\nimportant area of study amongst the photo-voltaic (PV) engineers and remote\nsensing analysts. The received solar irradiance impacts the total amount of\ngenerated solar energy. However, this generation is often hindered by the high\ndegree of solar irradiance variability. In this paper, we study the main\nfactors behind such variability with the assistance of Global Positioning\nSystem (GPS) and ground-based, high-resolution sky cameras. This analysis will\nalso be helpful for understanding cloud phenomenon and other events in the\nearth's atmosphere.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 09:04:07 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Manandhar", "Shilpa", ""], ["Dev", "Soumyabrata", ""], ["Lee", "Yee Hui", ""], ["Meng", "Yu Song", ""]]}, {"id": "1804.07645", "submitter": "Decebal Constantin Mocanu", "authors": "Decebal Constantin Mocanu and Elena Mocanu", "title": "One-Shot Learning using Mixture of Variational Autoencoders: a\n  Generalization Learning approach", "comments": null, "journal-ref": "17th International Conference on Autonomous Agents and Multiagent\n  Systems (AAMAS 2018)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, even if it is very successful nowadays, traditionally needs\nvery large amounts of labeled data to perform excellent on the classification\ntask. In an attempt to solve this problem, the one-shot learning paradigm,\nwhich makes use of just one labeled sample per class and prior knowledge,\nbecomes increasingly important. In this paper, we propose a new one-shot\nlearning method, dubbed MoVAE (Mixture of Variational AutoEncoders), to perform\nclassification. Complementary to prior studies, MoVAE represents a shift of\nparadigm in comparison with the usual one-shot learning methods, as it does not\nuse any prior knowledge. Instead, it starts from zero knowledge and one labeled\nsample per class. Afterward, by using unlabeled data and the generalization\nlearning concept (in a way, more as humans do), it is capable to gradually\nimprove by itself its performance. Even more, if there are no unlabeled data\navailable MoVAE can still perform well in one-shot learning classification. We\ndemonstrate empirically the efficiency of our proposed approach on three\ndatasets, i.e. the handwritten digits (MNIST), fashion products\n(Fashion-MNIST), and handwritten characters (Omniglot), showing that MoVAE\noutperforms state-of-the-art one-shot learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 22:00:20 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Mocanu", "Decebal Constantin", ""], ["Mocanu", "Elena", ""]]}, {"id": "1804.07661", "submitter": "Ruud JG van Sloun", "authors": "Ruud J.G. van Sloun, Oren Solomon, Matthew Bruce, Zin Z. Khaing,\n  Hessel Wijkstra, Yonina C. Eldar, Massimo Mischi", "title": "Super-resolution Ultrasound Localization Microscopy through Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound localization microscopy has enabled super-resolution vascular\nimaging through precise localization of individual ultrasound contrast agents\n(microbubbles) across numerous imaging frames. However, analysis of\nhigh-density regions with significant overlaps among the microbubble point\nspread responses yields high localization errors, constraining the technique to\nlow-concentration conditions. As such, long acquisition times are required to\nsufficiently cover the vascular bed. In this work, we present a fast and\nprecise method for obtaining super-resolution vascular images from high-density\ncontrast-enhanced ultrasound imaging data. This method, which we term Deep\nUltrasound Localization Microscopy (Deep-ULM), exploits modern deep learning\nstrategies and employs a convolutional neural network to perform localization\nmicroscopy in dense scenarios. This end-to-end fully convolutional neural\nnetwork architecture is trained effectively using on-line synthesized data,\nenabling robust inference in-vivo under a wide variety of imaging conditions.\nWe show that deep learning attains super-resolution with challenging\ncontrast-agent densities, both in-silico as well as in-vivo. Deep-ULM is\nsuitable for real-time applications, resolving about 70 high-resolution patches\n(128x128 pixels) per second on a standard PC. Exploiting GPU computation, this\nnumber increases to 1250 patches per second.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 15:12:05 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 15:13:22 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["van Sloun", "Ruud J. G.", ""], ["Solomon", "Oren", ""], ["Bruce", "Matthew", ""], ["Khaing", "Zin Z.", ""], ["Wijkstra", "Hessel", ""], ["Eldar", "Yonina C.", ""], ["Mischi", "Massimo", ""]]}, {"id": "1804.07667", "submitter": "Yu-Wei Chao", "authors": "Yu-Wei Chao and Sudheendra Vijayanarasimhan and Bryan Seybold and\n  David A. Ross and Jia Deng and Rahul Sukthankar", "title": "Rethinking the Faster R-CNN Architecture for Temporal Action\n  Localization", "comments": "Accepted in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose TAL-Net, an improved approach to temporal action localization in\nvideo that is inspired by the Faster R-CNN object detection framework. TAL-Net\naddresses three key shortcomings of existing approaches: (1) we improve\nreceptive field alignment using a multi-scale architecture that can accommodate\nextreme variation in action durations; (2) we better exploit the temporal\ncontext of actions for both proposal generation and action classification by\nappropriately extending receptive fields; and (3) we explicitly consider\nmulti-stream feature fusion and demonstrate that fusing motion late is\nimportant. We achieve state-of-the-art performance for both action proposal and\nlocalization on THUMOS'14 detection benchmark and competitive performance on\nActivityNet challenge.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 15:22:07 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Chao", "Yu-Wei", ""], ["Vijayanarasimhan", "Sudheendra", ""], ["Seybold", "Bryan", ""], ["Ross", "David A.", ""], ["Deng", "Jia", ""], ["Sukthankar", "Rahul", ""]]}, {"id": "1804.07723", "submitter": "Guilin Liu", "authors": "Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao,\n  Bryan Catanzaro", "title": "Image Inpainting for Irregular Holes Using Partial Convolutions", "comments": "Update: camera-ready; L1 loss is size-averaged; code of partial conv\n  layer: https://github.com/NVIDIA/partialconv. Published at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep learning based image inpainting methods use a standard\nconvolutional network over the corrupted image, using convolutional filter\nresponses conditioned on both valid pixels as well as the substitute values in\nthe masked holes (typically the mean value). This often leads to artifacts such\nas color discrepancy and blurriness. Post-processing is usually used to reduce\nsuch artifacts, but are expensive and may fail. We propose the use of partial\nconvolutions, where the convolution is masked and renormalized to be\nconditioned on only valid pixels. We further include a mechanism to\nautomatically generate an updated mask for the next layer as part of the\nforward pass. Our model outperforms other methods for irregular masks. We show\nqualitative and quantitative comparisons with other methods to validate our\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 17:00:14 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 22:22:34 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Liu", "Guilin", ""], ["Reda", "Fitsum A.", ""], ["Shih", "Kevin J.", ""], ["Wang", "Ting-Chun", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1804.07729", "submitter": "Tandri Gauksson", "authors": "Rima Alaifari, Giovanni S. Alberti and Tandri Gauksson", "title": "ADef: an Iterative Algorithm to Construct Adversarial Deformations", "comments": "ICLR 2019 conference paper. 25 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks have proven to be a powerful tool for many\nrecognition and classification tasks, their stability properties are still not\nwell understood. In the past, image classifiers have been shown to be\nvulnerable to so-called adversarial attacks, which are created by additively\nperturbing the correctly classified image. In this paper, we propose the ADef\nalgorithm to construct a different kind of adversarial attack created by\niteratively applying small deformations to the image, found through a gradient\ndescent step. We demonstrate our results on MNIST with convolutional neural\nnetworks and on ImageNet with Inception-v3 and ResNet-101.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 17:11:06 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 17:25:10 GMT"}, {"version": "v3", "created": "Fri, 11 Jan 2019 15:42:59 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Alaifari", "Rima", ""], ["Alberti", "Giovanni S.", ""], ["Gauksson", "Tandri", ""]]}, {"id": "1804.07739", "submitter": "Guha Balakrishnan", "authors": "Guha Balakrishnan, Amy Zhao, Adrian V. Dalca, Fredo Durand, John\n  Guttag", "title": "Synthesizing Images of Humans in Unseen Poses", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the computational problem of novel human pose synthesis. Given an\nimage of a person and a desired pose, we produce a depiction of that person in\nthat pose, retaining the appearance of both the person and background. We\npresent a modular generative neural network that synthesizes unseen poses using\ntraining pairs of images and poses taken from human action videos. Our network\nseparates a scene into different body part and background layers, moves body\nparts to new locations and refines their appearances, and composites the new\nforeground with a hole-filled background. These subtasks, implemented with\nseparate modules, are trained jointly using only a single target image as a\nsupervised label. We use an adversarial discriminator to force our network to\nsynthesize realistic details conditioned on pose. We demonstrate image\nsynthesis results on three action classes: golf, yoga/workouts and tennis, and\nshow that our method produces accurate results within action classes as well as\nacross action classes. Given a sequence of desired poses, we also produce\ncoherent videos of actions.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 17:34:44 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Balakrishnan", "Guha", ""], ["Zhao", "Amy", ""], ["Dalca", "Adrian V.", ""], ["Durand", "Fredo", ""], ["Guttag", "John", ""]]}, {"id": "1804.07821", "submitter": "Diptodip Deb", "authors": "Diptodip Deb, Jonathan Ventura", "title": "An Aggregated Multicolumn Dilated Convolution Network for\n  Perspective-Free Counting", "comments": "CVPR 2018 Workshop On Visual Understanding of Humans in Crowd Scene", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of dilated filters to construct an aggregation module in a\nmulticolumn convolutional neural network for perspective-free counting.\nCounting is a common problem in computer vision (e.g. traffic on the street or\npedestrians in a crowd). Modern approaches to the counting problem involve the\nproduction of a density map via regression whose integral is equal to the\nnumber of objects in the image. However, objects in the image can occur at\ndifferent scales (e.g. due to perspective effects) which can make it difficult\nfor a learning agent to learn the proper density map. While the use of multiple\ncolumns to extract multiscale information from images has been shown before,\nour approach aggregates the multiscale information gathered by the multicolumn\nconvolutional neural network to improve performance. Our experiments show that\nour proposed network outperforms the state-of-the-art on many benchmark\ndatasets, and also that using our aggregation module in combination with a\nhigher number of columns is beneficial for multiscale counting.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 20:49:10 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Deb", "Diptodip", ""], ["Ventura", "Jonathan", ""]]}, {"id": "1804.07834", "submitter": "Akshay Rangesh", "authors": "Akshay Rangesh and Mohan M. Trivedi", "title": "HandyNet: A One-stop Solution to Detect, Segment, Localize & Analyze\n  Driver Hands", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tasks related to human hands have long been part of the computer vision\ncommunity. Hands being the primary actuators for humans, convey a lot about\nactivities and intents, in addition to being an alternative form of\ncommunication/interaction with other humans and machines. In this study, we\nfocus on training a single feedforward convolutional neural network (CNN)\ncapable of executing many hand related tasks that may be of use in autonomous\nand semi-autonomous vehicles of the future. The resulting network, which we\nrefer to as HandyNet, is capable of detecting, segmenting and localizing (in\n3D) driver hands inside a vehicle cabin. The network is additionally trained to\nidentify handheld objects that the driver may be interacting with. To meet the\ndata requirements to train such a network, we propose a method for cheap\nannotation based on chroma-keying, thereby bypassing weeks of human effort\nrequired to label such data. This process can generate thousands of labeled\ntraining samples in an efficient manner, and may be replicated in new\nenvironments with relative ease.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 21:38:32 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 20:21:15 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Rangesh", "Akshay", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1804.07836", "submitter": "Michael Kampffmeyer", "authors": "Michael Kampffmeyer, Nanqing Dong, Xiaodan Liang, Yujia Zhang and Eric\n  P. Xing", "title": "ConnNet: A Long-Range Relation-Aware Pixel-Connectivity Network for\n  Salient Segmentation", "comments": null, "journal-ref": "IEEE Transactions on Image Processing (Volume: 28 , Issue: 5 , May\n  2019)", "doi": "10.1109/TIP.2018.2886997", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient segmentation aims to segment out attention-grabbing regions, a\ncritical yet challenging task and the foundation of many high-level computer\nvision applications. It requires semantic-aware grouping of pixels into salient\nregions and benefits from the utilization of global multi-scale contexts to\nachieve good local reasoning. Previous works often address it as two-class\nsegmentation problems utilizing complicated multi-step procedures including\nrefinement networks and complex graphical models. We argue that semantic\nsalient segmentation can instead be effectively resolved by reformulating it as\na simple yet intuitive pixel-pair based connectivity prediction task. Following\nthe intuition that salient objects can be naturally grouped via semantic-aware\nconnectivity between neighboring pixels, we propose a pure Connectivity Net\n(ConnNet). ConnNet predicts connectivity probabilities of each pixel with its\nneighboring pixels by leveraging multi-level cascade contexts embedded in the\nimage and long-range pixel relations. We investigate our approach on two tasks,\nnamely salient object segmentation and salient instance-level segmentation, and\nillustrate that consistent improvements can be obtained by modeling these tasks\nas connectivity instead of binary segmentation tasks for a variety of network\narchitectures. We achieve state-of-the-art performance, outperforming or being\ncomparable to existing approaches while reducing inference time due to our less\ncomplex approach.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 21:40:57 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 11:35:27 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Kampffmeyer", "Michael", ""], ["Dong", "Nanqing", ""], ["Liang", "Xiaodan", ""], ["Zhang", "Yujia", ""], ["Xing", "Eric P.", ""]]}, {"id": "1804.07839", "submitter": "Jonathan Rubin", "authors": "Jonathan Rubin, Deepan Sanghavi, Claire Zhao, Kathy Lee, Ashequl\n  Qadir, Minnan Xu-Wilson", "title": "Large Scale Automated Reading of Frontal and Lateral Chest X-Rays using\n  Dual Convolutional Neural Networks", "comments": "First draft, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MIMIC-CXR dataset is (to date) the largest released chest x-ray dataset\nconsisting of 473,064 chest x-rays and 206,574 radiology reports collected from\n63,478 patients. We present the results of training and evaluating a collection\nof deep convolutional neural networks on this dataset to recognize multiple\ncommon thorax diseases. To the best of our knowledge, this is the first work\nthat trains CNNs for this task on such a large collection of chest x-ray\nimages, which is over four times the size of the largest previously released\nchest x-ray corpus (ChestX-Ray14). We describe and evaluate individual CNN\nmodels trained on frontal and lateral CXR view types. In addition, we present a\nnovel DualNet architecture that emulates routine clinical practice by\nsimultaneously processing both frontal and lateral CXR images obtained from a\nradiological exam. Our DualNet architecture shows improved performance in\nrecognizing findings in CXR images when compared to applying separate baseline\nfrontal and lateral classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 21:48:35 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 21:13:59 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Rubin", "Jonathan", ""], ["Sanghavi", "Deepan", ""], ["Zhao", "Claire", ""], ["Lee", "Kathy", ""], ["Qadir", "Ashequl", ""], ["Xu-Wilson", "Minnan", ""]]}, {"id": "1804.07851", "submitter": "Ida H\\\"aggstr\\\"om", "authors": "Ida H\\\"aggstr\\\"om, C. Ross Schmidtlein, Gabriele Campanella, Thomas J.\n  Fuchs", "title": "DeepPET: A deep encoder-decoder network for directly solving the PET\n  reconstruction inverse problem", "comments": null, "journal-ref": "Medical Image Analysis 54, pp. 253-262, 2019", "doi": "10.1016/j.media.2019.03.013", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positron emission tomography (PET) is a cornerstone of modern radiology. The\nability to detect cancer and metastases in whole body scans fundamentally\nchanged cancer diagnosis and treatment. One of the main bottlenecks in the\nclinical application is the time it takes to reconstruct the anatomical image\nfrom the deluge of data in PET imaging. State-of-the art methods based on\nexpectation maximization can take hours for a single patient and depend on\nmanual fine-tuning. This results not only in financial burden for hospitals but\nmore importantly leads to less efficient patient handling, evaluation, and\nultimately diagnosis and treatment for patients. To overcome this problem we\npresent a novel PET image reconstruction technique based on a deep\nconvolutional encoder-decoder network, that takes PET sinogram data as input\nand directly outputs full PET images. Using realistic simulated data, we\ndemonstrate that our network is able to reconstruct images >100 times faster,\nand with comparable image quality (in terms of root mean squared error)\nrelative to conventional iterative reconstruction techniques.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 22:44:00 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 19:03:06 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["H\u00e4ggstr\u00f6m", "Ida", ""], ["Schmidtlein", "C. Ross", ""], ["Campanella", "Gabriele", ""], ["Fuchs", "Thomas J.", ""]]}, {"id": "1804.07873", "submitter": "Henry M. Clever", "authors": "Henry M. Clever, Ariel Kapusta, Daehyung Park, Zackory Erickson, Yash\n  Chitalia, Charles C. Kemp", "title": "3D Human Pose Estimation on a Configurable Bed from a Pressure Image", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots have the potential to assist people in bed, such as in healthcare\nsettings, yet bedding materials like sheets and blankets can make observation\nof the human body difficult for robots. A pressure-sensing mat on a bed can\nprovide pressure images that are relatively insensitive to bedding materials.\nHowever, prior work on estimating human pose from pressure images has been\nrestricted to 2D pose estimates and flat beds. In this work, we present two\nconvolutional neural networks to estimate the 3D joint positions of a person in\na configurable bed from a single pressure image. The first network directly\noutputs 3D joint positions, while the second outputs a kinematic model that\nincludes estimated joint angles and limb lengths. We evaluated our networks on\ndata from 17 human participants with two bed configurations: supine and seated.\nOur networks achieved a mean joint position error of 77 mm when tested with\ndata from people outside the training set, outperforming several baselines. We\nalso present a simple mechanical model that provides insight into ambiguity\nassociated with limbs raised off of the pressure mat, and demonstrate that\nMonte Carlo dropout can be used to estimate pose confidence in these\nsituations. Finally, we provide a demonstration in which a mobile manipulator\nuses our network's estimated kinematic model to reach a location on a person's\nbody in spite of the person being seated in a bed and covered by a blanket.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 01:15:44 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 22:30:32 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Clever", "Henry M.", ""], ["Kapusta", "Ariel", ""], ["Park", "Daehyung", ""], ["Erickson", "Zackory", ""], ["Chitalia", "Yash", ""], ["Kemp", "Charles C.", ""]]}, {"id": "1804.07909", "submitter": "Mihai Fieraru", "authors": "Mihai Fieraru, Anna Khoreva, Leonid Pishchulin, Bernt Schiele", "title": "Learning to Refine Human Pose Estimation", "comments": "To appear in CVPRW (2018). Workshop: Visual Understanding of Humans\n  in Crowd Scene and the 2nd Look Into Person Challenge (VUHCS-LIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person pose estimation in images and videos is an important yet\nchallenging task with many applications. Despite the large improvements in\nhuman pose estimation enabled by the development of convolutional neural\nnetworks, there still exist a lot of difficult cases where even the\nstate-of-the-art models fail to correctly localize all body joints. This\nmotivates the need for an additional refinement step that addresses these\nchallenging cases and can be easily applied on top of any existing method. In\nthis work, we introduce a pose refinement network (PoseRefiner) which takes as\ninput both the image and a given pose estimate and learns to directly predict a\nrefined pose by jointly reasoning about the input-output space. In order for\nthe network to learn to refine incorrect body joint predictions, we employ a\nnovel data augmentation scheme for training, where we model \"hard\" human pose\ncases. We evaluate our approach on four popular large-scale pose estimation\nbenchmarks such as MPII Single- and Multi-Person Pose Estimation, PoseTrack\nPose Estimation, and PoseTrack Pose Tracking, and report systematic improvement\nover the state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 07:46:13 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Fieraru", "Mihai", ""], ["Khoreva", "Anna", ""], ["Pishchulin", "Leonid", ""], ["Schiele", "Bernt", ""]]}, {"id": "1804.07926", "submitter": "Jihua Zhu", "authors": "Jihua Zhu, Siyu Xu, Zutao Jiang, Shanmin Pang, Jun Wang, Zhongyu Li", "title": "Multi-view registration of unordered range scans by fast correspondence\n  propagation of multi-scale descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a global approach for the multi-view registration of\nunordered range scans. As the basis of multi-view registration, pair-wise\nregistration is very pivotal. Therefore, we first select a good descriptor and\naccelerate its correspondence propagation for the pair-wise registration. Then,\nwe design an effective rule to judge the reliability of pair-wise registration\nresults. Subsequently, we propose a model augmentation method, which can\nutilize reliable results of pair-wise registration to augment the model shape.\nFinally, multi-view registration can be accomplished by operating the pair-wise\nregistration and judgment, and model augmentation alternately. Experimental\nresults on public available data sets show, that this approach can\nautomatically achieve the multi-view registration of unordered range scans with\ngood accuracy and effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 09:36:33 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Zhu", "Jihua", ""], ["Xu", "Siyu", ""], ["Jiang", "Zutao", ""], ["Pang", "Shanmin", ""], ["Wang", "Jun", ""], ["Li", "Zhongyu", ""]]}, {"id": "1804.08010", "submitter": "Qibin Zheng", "authors": "Qibin Zheng, Xingchun Diao, Jianjun Cao, Xiaolei Zhou, Yi Liu, and\n  Hongmei Li", "title": "Multi-Modal Coreference Resolution with the Correlation between Space\n  Structures", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal data is becoming more common in big data background. Finding the\nsemantically similar objects from different modality is one of the heart\nproblems of multi-modal learning. Most of the current methods try to learn the\ninter-modal correlation with extrinsic supervised information, while intrinsic\nstructural information of each modality is neglected. The performance of these\nmethods heavily depends on the richness of training samples. However, obtaining\nthe multi-modal training samples is still a labor and cost intensive work. In\nthis paper, we bring a extrinsic correlation between the space structures of\neach modalities in coreference resolution. With this correlation, a\nsemi-supervised learning model for multi-modal coreference resolution is\nproposed. We firstly extract high-level features of images and text, then\ncompute the distances of each object from some reference points to build the\nspace structure of each modality. With a shared reference point set, the space\nstructures of each modality are correlated. We employ the correlation to build\na commonly shared space that the semantic distance between multi-modal objects\ncan be computed directly. The experiments on two multi-modal datasets show that\nour model performs better than the existing methods with insufficient training\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 19:15:19 GMT"}, {"version": "v2", "created": "Sat, 1 Sep 2018 12:33:13 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Zheng", "Qibin", ""], ["Diao", "Xingchun", ""], ["Cao", "Jianjun", ""], ["Zhou", "Xiaolei", ""], ["Liu", "Yi", ""], ["Li", "Hongmei", ""]]}, {"id": "1804.08018", "submitter": "Oliver Groth", "authors": "Oliver Groth, Fabian B. Fuchs, Ingmar Posner and Andrea Vedaldi", "title": "ShapeStacks: Learning Vision-Based Physical Intuition for Generalised\n  Object Stacking", "comments": "revised version to appear at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical intuition is pivotal for intelligent agents to perform complex\ntasks. In this paper we investigate the passive acquisition of an intuitive\nunderstanding of physical principles as well as the active utilisation of this\nintuition in the context of generalised object stacking. To this end, we\nprovide: a simulation-based dataset featuring 20,000 stack configurations\ncomposed of a variety of elementary geometric primitives richly annotated\nregarding semantics and structural stability. We train visual classifiers for\nbinary stability prediction on the ShapeStacks data and scrutinise their\nlearned physical intuition. Due to the richness of the training data our\napproach also generalises favourably to real-world scenarios achieving\nstate-of-the-art stability prediction on a publicly available benchmark of\nblock towers. We then leverage the physical intuition learned by our model to\nactively construct stable stacks and observe the emergence of an intuitive\nnotion of stackability - an inherent object affordance - induced by the active\nstacking task. Our approach performs well even in challenging conditions where\nit considerably exceeds the stack height observed during training or in cases\nwhere initially unstable structures must be stabilised via counterbalancing.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 19:52:10 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 14:00:34 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Groth", "Oliver", ""], ["Fuchs", "Fabian B.", ""], ["Posner", "Ingmar", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1804.08020", "submitter": "S. Alireza Golestaneh", "authors": "S. Alireza Golestaneh and Lina Karam", "title": "Synthesized Texture Quality Assessment via Multi-scale Spatial and\n  Statistical Texture Attributes of Image and Gradient Magnitude Coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptual quality assessment for synthesized textures is a challenging task.\nIn this paper, we propose a training-free reduced-reference (RR) objective\nquality assessment method that quantifies the perceived quality of synthesized\ntextures. The proposed reduced-reference synthesized texture quality assessment\nmetric is based on measuring the spatial and statistical attributes of the\ntexture image using both image- and gradient-based wavelet coefficients at\nmultiple scales. Performance evaluations on two synthesized texture databases\ndemonstrate that our proposed RR synthesized texture quality metric\nsignificantly outperforms both full-reference and RR state-of-the-art quality\nmetrics in predicting the perceived visual quality of the synthesized textures.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 20:18:45 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 20:54:38 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Golestaneh", "S. Alireza", ""], ["Karam", "Lina", ""]]}, {"id": "1804.08024", "submitter": "Alexey Shvets", "authors": "Alexey Shvets, Vladimir Iglovikov, Alexander Rakhlin and Alexandr A.\n  Kalinin", "title": "Angiodysplasia Detection and Localization Using Deep Convolutional\n  Neural Networks", "comments": "12 pages, 6 figures", "journal-ref": "2018 17th IEEE International Conference on Machine Learning and\n  Applications (ICMLA)", "doi": "10.1109/ICMLA.2018.00098", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection and localization for angiodysplasia lesions is an\nimportant problem in early stage diagnostics of gastrointestinal bleeding and\nanemia. Gold-standard for angiodysplasia detection and localization is\nperformed using wireless capsule endoscopy. This pill-like device is able to\nproduce thousand of high enough resolution images during one passage through\ngastrointestinal tract. In this paper we present our winning solution for\nMICCAI 2017 Endoscopic Vision SubChallenge: Angiodysplasia Detection and\nLocalization its further improvements over the state-of-the-art results using\nseveral novel deep neural network architectures. It address the binary\nsegmentation problem, where every pixel in an image is labeled as an\nangiodysplasia lesions or background. Then, we analyze connected component of\neach predicted mask. Based on the analysis we developed a classifier that\npredict angiodysplasia lesions (binary variable) and a detector for their\nlocalization (center of a component). In this setting, our approach outperforms\nother methods in every task subcategory for angiodysplasia detection and\nlocalization thereby providing state-of-the-art results for these problems. The\nsource code for our solution is made publicly available at\nhttps://github.com/ternaus/angiodysplasia-segmentatio\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 20:38:32 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Shvets", "Alexey", ""], ["Iglovikov", "Vladimir", ""], ["Rakhlin", "Alexander", ""], ["Kalinin", "Alexandr A.", ""]]}, {"id": "1804.08039", "submitter": "Wen Wei", "authors": "Wen Wei and Emilie Poirion and Benedetta Bodini and Stanley Durrleman\n  and Nicholas Ayache and Bruno Stankoff and Olivier Colliot", "title": "Learning Myelin Content in Multiple Sclerosis from Multimodal MRI\n  through Adversarial Training", "comments": "Accepted by MICCAI2018", "journal-ref": null, "doi": "10.1007/978-3-030-00931-1_59", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple sclerosis (MS) is a demyelinating disease of the central nervous\nsystem (CNS). A reliable measure of the tissue myelin content is therefore\nessential for the understanding of the physiopathology of MS, tracking\nprogression and assessing treatment efficacy. Positron emission tomography\n(PET) with $[^{11} \\mbox{C}] \\mbox{PIB}$ has been proposed as a promising\nbiomarker for measuring myelin content changes in-vivo in MS. However, PET\nimaging is expensive and invasive due to the injection of a radioactive tracer.\nOn the contrary, magnetic resonance imaging (MRI) is a non-invasive, widely\navailable technique, but existing MRI sequences do not provide, to date, a\nreliable, specific, or direct marker of either demyelination or remyelination.\nIn this work, we therefore propose Sketcher-Refiner Generative Adversarial\nNetworks (GANs) with specifically designed adversarial loss functions to\npredict the PET-derived myelin content map from a combination of MRI\nmodalities. The prediction problem is solved by a sketch-refinement process in\nwhich the sketcher generates the preliminary anatomical and physiological\ninformation and the refiner refines and generates images reflecting the tissue\nmyelin content in the human brain. We evaluated the ability of our method to\npredict myelin content at both global and voxel-wise levels. The evaluation\nresults show that the demyelination in lesion regions and myelin content in\nnormal-appearing white matter (NAWM) can be well predicted by our method. The\nmethod has the potential to become a useful tool for clinical management of\npatients with MS.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 22:23:51 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 09:49:59 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Wei", "Wen", ""], ["Poirion", "Emilie", ""], ["Bodini", "Benedetta", ""], ["Durrleman", "Stanley", ""], ["Ayache", "Nicholas", ""], ["Stankoff", "Bruno", ""], ["Colliot", "Olivier", ""]]}, {"id": "1804.08042", "submitter": "Najeeb Khan", "authors": "Najeeb Khan, Jawad Shah and Ian Stavness", "title": "Bridgeout: stochastic bridge regularization for deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in training deep neural networks is overfitting, i.e.\ninferior performance on unseen test examples compared to performance on\ntraining examples. To reduce overfitting, stochastic regularization methods\nhave shown superior performance compared to deterministic weight penalties on a\nnumber of image recognition tasks. Stochastic methods such as Dropout and\nShakeout, in expectation, are equivalent to imposing a ridge and elastic-net\npenalty on the model parameters, respectively. However, the choice of the norm\nof weight penalty is problem dependent and is not restricted to $\\{L_1,L_2\\}$.\nTherefore, in this paper we propose the Bridgeout stochastic regularization\ntechnique and prove that it is equivalent to an $L_q$ penalty on the weights,\nwhere the norm $q$ can be learned as a hyperparameter from data. Experimental\nresults show that Bridgeout results in sparse model weights, improved gradients\nand superior classification performance compared to Dropout and Shakeout on\nsynthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 23:27:24 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Khan", "Najeeb", ""], ["Shah", "Jawad", ""], ["Stavness", "Ian", ""]]}, {"id": "1804.08046", "submitter": "Julio Cezar Silveira Jacques Junior", "authors": "Julio C. S. Jacques Junior, Ya\\u{g}mur G\\\"u\\c{c}l\\\"ut\\\"urk, Marc\n  P\\'erez, Umut G\\\"u\\c{c}l\\\"u, Carlos Andujar, Xavier Bar\\'o, Hugo Jair\n  Escalante, Isabelle Guyon, Marcel A. J. van Gerven, Rob van Lier and Sergio\n  Escalera", "title": "First Impressions: A Survey on Vision-Based Apparent Personality Trait\n  Analysis", "comments": "Accepted on IEEE Transactions on Affective Computing (TAC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personality analysis has been widely studied in psychology, neuropsychology,\nand signal processing fields, among others. From the past few years, it also\nbecame an attractive research area in visual computing. From the computational\npoint of view, by far speech and text have been the most considered cues of\ninformation for analyzing personality. However, recently there has been an\nincreasing interest from the computer vision community in analyzing personality\nfrom visual data. Recent computer vision approaches are able to accurately\nanalyze human faces, body postures and behaviors, and use these information to\ninfer apparent personality traits. Because of the overwhelming research\ninterest in this topic, and of the potential impact that this sort of methods\ncould have in society, we present in this paper an up-to-date review of\nexisting vision-based approaches for apparent personality trait recognition. We\ndescribe seminal and cutting edge works on the subject, discussing and\ncomparing their distinctive features and limitations. Future venues of research\nin the field are identified and discussed. Furthermore, aspects on the\nsubjectivity in data labeling/evaluation, as well as current datasets and\nchallenges organized to push the research on the field are reviewed.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 23:53:18 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 09:57:49 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2019 08:48:31 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Junior", "Julio C. S. Jacques", ""], ["G\u00fc\u00e7l\u00fct\u00fcrk", "Ya\u011fmur", ""], ["P\u00e9rez", "Marc", ""], ["G\u00fc\u00e7l\u00fc", "Umut", ""], ["Andujar", "Carlos", ""], ["Bar\u00f3", "Xavier", ""], ["Escalante", "Hugo Jair", ""], ["Guyon", "Isabelle", ""], ["van Gerven", "Marcel A. J.", ""], ["van Lier", "Rob", ""], ["Escalera", "Sergio", ""]]}, {"id": "1804.08071", "submitter": "Weiyang Liu", "authors": "Weiyang Liu, Zhen Liu, Zhiding Yu, Bo Dai, Rongmei Lin, Yisen Wang,\n  James M. Rehg, Le Song", "title": "Decoupled Networks", "comments": "CVPR 2018 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inner product-based convolution has been a central component of convolutional\nneural networks (CNNs) and the key to learning visual representations. Inspired\nby the observation that CNN-learned features are naturally decoupled with the\nnorm of features corresponding to the intra-class variation and the angle\ncorresponding to the semantic difference, we propose a generic decoupled\nlearning framework which models the intra-class variation and semantic\ndifference independently. Specifically, we first reparametrize the inner\nproduct to a decoupled form and then generalize it to the decoupled convolution\noperator which serves as the building block of our decoupled networks. We\npresent several effective instances of the decoupled convolution operator. Each\ndecoupled operator is well motivated and has an intuitive geometric\ninterpretation. Based on these decoupled operators, we further propose to\ndirectly learn the operator from data. Extensive experiments show that such\ndecoupled reparameterization renders significant performance gain with easier\nconvergence and stronger robustness.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 05:26:08 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Liu", "Weiyang", ""], ["Liu", "Zhen", ""], ["Yu", "Zhiding", ""], ["Dai", "Bo", ""], ["Lin", "Rongmei", ""], ["Wang", "Yisen", ""], ["Rehg", "James M.", ""], ["Song", "Le", ""]]}, {"id": "1804.08087", "submitter": "Fusheng Hao", "authors": "Fusheng Hao, Jun Cheng, Lei Wang, Xinchao Wang, Jianzhong Cao, Xiping\n  Hu, Dapeng Tao", "title": "Anchor-based Nearest Class Mean Loss for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative features are critical for machine learning applications. Most\nexisting deep learning approaches, however, rely on convolutional neural\nnetworks (CNNs) for learning features, whose discriminant power is not\nexplicitly enforced. In this paper, we propose a novel approach to train deep\nCNNs by imposing the intra-class compactness and the inter-class separability,\nso as to enhance the learned features' discriminant power. To this end, we\nintroduce anchors, which are predefined vectors regarded as the centers for\neach class and fixed during training. Discriminative features are obtained by\nconstraining the deep CNNs to map training samples to the corresponding anchors\nas close as possible. We propose two principles to select the anchors, and\nmeasure the proximity of two points using the Euclidean and cosine distance\nmetric functions, which results in two novel loss functions. These loss\nfunctions require no sample pairs or triplets and can be efficiently optimized\nby batch stochastic gradient descent. We test the proposed method on three\nbenchmark image classification datasets and demonstrate its promising results.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 08:49:59 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Hao", "Fusheng", ""], ["Cheng", "Jun", ""], ["Wang", "Lei", ""], ["Wang", "Xinchao", ""], ["Cao", "Jianzhong", ""], ["Hu", "Xiping", ""], ["Tao", "Dapeng", ""]]}, {"id": "1804.08122", "submitter": "Tarang Chugh", "authors": "Debayan Deb, Tarang Chugh, Joshua Engelsma, Kai Cao, Neeta Nain, Jake\n  Kendall, and Anil K. Jain", "title": "Matching Fingerphotos to Slap Fingerprint Images", "comments": "9 pages, 16 figures, 5 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of comparing fingerphotos, fingerprint images from a\ncommodity smartphone camera, with the corresponding legacy slap contact-based\nfingerprint images. Development of robust versions of these technologies would\nenable the use of the billions of standard Android phones as biometric readers\nthrough a simple software download, dramatically lowering the cost and\ncomplexity of deployment relative to using a separate fingerprint reader. Two\nfingerphoto apps running on Android phones and an optical slap reader were\nutilized for fingerprint collection of 309 subjects who primarily work as\nconstruction workers, farmers, and domestic helpers. Experimental results show\nthat a True Accept Rate (TAR) of 95.79 at a False Accept Rate (FAR) of 0.1% can\nbe achieved in matching fingerphotos to slaps (two thumbs and two index\nfingers) using a COTS fingerprint matcher. By comparison, a baseline TAR of\n98.55% at 0.1% FAR is achieved when matching fingerprint images from two\ndifferent contact-based optical readers. We also report the usability of the\ntwo smartphone apps, in terms of failure to acquire rate and fingerprint\nacquisition time. Our results show that fingerphotos are promising to\nauthenticate individuals (against a national ID database) for banking, welfare\ndistribution, and healthcare applications in developing countries.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 15:21:42 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Deb", "Debayan", ""], ["Chugh", "Tarang", ""], ["Engelsma", "Joshua", ""], ["Cao", "Kai", ""], ["Nain", "Neeta", ""], ["Kendall", "Jake", ""], ["Jain", "Anil K.", ""]]}, {"id": "1804.08145", "submitter": "Shan E Ahmed Raza", "authors": "Shan E Ahmed Raza, Linda Cheung, Muhammad Shaban, Simon Graham, David\n  Epstein, Stella Pelengaris, Michael Khan, Nasir M. Rajpoot", "title": "Micro-Net: A unified model for segmentation of various objects in\n  microscopy images", "comments": null, "journal-ref": "Medical Image Analysis. 52 (2019) 160-173", "doi": "10.1016/j.media.2018.12.003", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object segmentation and structure localization are important steps in\nautomated image analysis pipelines for microscopy images. We present a\nconvolution neural network (CNN) based deep learning architecture for\nsegmentation of objects in microscopy images. The proposed network can be used\nto segment cells, nuclei and glands in fluorescence microscopy and histology\nimages after slight tuning of input parameters. The network trains at multiple\nresolutions of the input image, connects the intermediate layers for better\nlocalization and context and generates the output using multi-resolution\ndeconvolution filters. The extra convolutional layers which bypass the\nmax-pooling operation allow the network to train for variable input intensities\nand object size and make it robust to noisy data. We compare our results on\npublicly available data sets and show that the proposed network outperforms\nrecent deep learning algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 17:40:32 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 20:26:05 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Raza", "Shan E Ahmed", ""], ["Cheung", "Linda", ""], ["Shaban", "Muhammad", ""], ["Graham", "Simon", ""], ["Epstein", "David", ""], ["Pelengaris", "Stella", ""], ["Khan", "Michael", ""], ["Rajpoot", "Nasir M.", ""]]}, {"id": "1804.08170", "submitter": "Bahram Lavi", "authors": "Mehdi Fatan Serj, Bahram Lavi, Gabriela Hoff, and Domenec Puig Valls", "title": "A Deep Convolutional Neural Network for Lung Cancer Diagnostic", "comments": "10 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we examine the strength of deep learning technique for\ndiagnosing lung cancer on medical image analysis problem. Convolutional neural\nnetworks (CNNs) models become popular among the pattern recognition and\ncomputer vision research area because of their promising outcome on generating\nhigh-level image representations. We propose a new deep learning architecture\nfor learning high-level image representation to achieve high classification\naccuracy with low variance in medical image binary classification tasks. We aim\nto learn discriminant compact features at beginning of our deep convolutional\nneural network. We evaluate our model on Kaggle Data Science Bowl 2017 (KDSB17)\ndata set, and compare it with some related works proposed in the Kaggle\ncompetition.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 21:00:28 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Serj", "Mehdi Fatan", ""], ["Lavi", "Bahram", ""], ["Hoff", "Gabriela", ""], ["Valls", "Domenec Puig", ""]]}, {"id": "1804.08181", "submitter": "George Seif", "authors": "George Seif, Dimitrios Androutsos", "title": "Large Receptive Field Networks for High-Scale Image Super-Resolution", "comments": "Accepted as a conference paper at CVPR2018 in the NTIRE Workshop\n  http://www.vision.ee.ethz.ch/en/ntire18/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have been the backbone of recent rapid progress\nin Single-Image Super-Resolution. However, existing networks are very deep with\nmany network parameters, thus having a large memory footprint and being\nchallenging to train. We propose Large Receptive Field Networks which strive to\ndirectly expand the receptive field of Super-Resolution networks without\nincreasing depth or parameter count. In particular, we use two different\nmethods to expand the network receptive field: 1-D separable kernels and atrous\nconvolutions. We conduct considerable experiments to study the performance of\nvarious arrangement schemes of the 1-D separable kernels and atrous convolution\nin terms of accuracy (PSNR / SSIM), parameter count, and speed, while focusing\non the more challenging high upscaling factors. Extensive benchmark evaluations\ndemonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 21:50:51 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Seif", "George", ""], ["Androutsos", "Dimitrios", ""]]}, {"id": "1804.08197", "submitter": "Stanislav Pidhorskyi", "authors": "Stanislav Pidhorskyi, Michael Morehead, Quinn Jones, George Spirou,\n  Gianfranco Doretto", "title": "syGlass: Interactive Exploration of Multidimensional Images Using\n  Virtual Reality Head-mounted Displays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quest for deeper understanding of biological systems has driven the\nacquisition of increasingly larger multidimensional image datasets. Inspecting\nand manipulating data of this complexity is very challenging in traditional\nvisualization systems. We developed syGlass, a software package capable of\nvisualizing large scale volumetric data with inexpensive virtual reality\nhead-mounted display technology. This allows leveraging stereoscopic vision to\nsignificantly improve perception of complex 3D structures, and provides\nimmersive interaction with data directly in 3D. We accomplished this by\ndeveloping highly optimized data flow and volume rendering pipelines, tested on\ndatasets up to 16TB in size, as well as tools available in a virtual reality\nGUI to support advanced data exploration, annotation, and cataloguing.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 00:04:54 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 03:58:42 GMT"}, {"version": "v3", "created": "Tue, 21 Aug 2018 04:00:45 GMT"}, {"version": "v4", "created": "Wed, 22 Aug 2018 01:48:32 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Pidhorskyi", "Stanislav", ""], ["Morehead", "Michael", ""], ["Jones", "Quinn", ""], ["Spirou", "George", ""], ["Doretto", "Gianfranco", ""]]}, {"id": "1804.08208", "submitter": "Peng Gao", "authors": "Peng Gao, Yipeng Ma, Ke Song, Chao Li, Fei Wang, Liyi Xiao, Yan Zhang", "title": "High Performance Visual Tracking with Circular and Structural Operators", "comments": "Accepted to Knowledge-Based SYSTEMS", "journal-ref": null, "doi": "10.1016/j.knosys.2018.08.008", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel circular and structural operator tracker (CSOT) is\nproposed for high performance visual tracking, it not only possesses the\npowerful discriminative capability of SOSVM but also efficiently inherits the\nsuperior computational efficiency of DCF. Based on the proposed circular and\nstructural operators, a set of primal confidence score maps can be obtained by\ncircular correlating feature maps with their corresponding structural\ncorrelation filters. Furthermore, an implicit interpolation is applied to\nconvert the multi-resolution feature maps to the continuous domain and make all\nprimal confidence score maps have the same spatial resolution. Then, we exploit\nan efficient ensemble post-processor based on relative entropy, which can\ncoalesce primal confidence score maps and create an optimal confidence score\nmap for more accurate localization. The target is localized on the peak of the\noptimal confidence score map. Besides, we introduce a collaborative\noptimization strategy to update circular and structural operators by\niteratively training structural correlation filters, which significantly\nreduces computational complexity and improves robustness. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance in mean AUC\nscores of 71.5% and 69.4% on the OTB-2013 and OTB-2015 benchmarks respectively,\nand obtains a third-best expected average overlap (EAO) score of 29.8% on the\nVOT-2017 benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 01:08:43 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 08:34:03 GMT"}, {"version": "v3", "created": "Sat, 13 Oct 2018 07:06:14 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Gao", "Peng", ""], ["Ma", "Yipeng", ""], ["Song", "Ke", ""], ["Li", "Chao", ""], ["Wang", "Fei", ""], ["Xiao", "Liyi", ""], ["Zhang", "Yan", ""]]}, {"id": "1804.08220", "submitter": "Yong Wang", "authors": "Ding Lu, Yong Wang, Robert Laganiere, Xinbin Luo, Shan Fu", "title": "Multi-scale prediction for robust hand detection and classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multi-scale Fully Convolutional Networks\n(MSP-RFCN) to robustly detect and classify human hands under various\nchallenging conditions. In our approach, the input image is passed through the\nproposed network to generate score maps, based on multi-scale predictions. The\nnetwork has been specifically designed to deal with small objects. It uses an\narchitecture based on region proposals generated at multiple scales. Our method\nis evaluated on challenging hand datasets, namely the Vision for Intelligent\nVehicles and Applications (VIVA) Challenge and the Oxford hand dataset. It is\ncompared against recent hand detection algorithms. The experimental results\ndemonstrate that our proposed method achieves state-of-the-art detection for\nhands of various sizes.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 02:02:14 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Lu", "Ding", ""], ["Wang", "Yong", ""], ["Laganiere", "Robert", ""], ["Luo", "Xinbin", ""], ["Fu", "Shan", ""]]}, {"id": "1804.08243", "submitter": "Zixu Zhao", "authors": "Fouad Amer, Zixu Zhao, Siwei Tang, Wilfredo Torres", "title": "Constructing Locally Dense Point Clouds Using OpenSfM and ORB-SLAM2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at finding a method to register two different point clouds\nconstructed by ORB-SLAM2 and OpenSfM. To do this, we post some tags with unique\ntextures in the scene and take videos and photos of that area. Then we take\nshort videos of only the tags to extract their features. By matching the ORB\nfeature of the tags with their corresponding features in the scene, it is then\npossible to localize the position of these tags both in point clouds\nconstructed by ORB-SLAM2 and OpenSfM. Thus, the best transformation matrix\nbetween two point clouds can be calculated, and the two point clouds can be\naligned.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 05:07:51 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Amer", "Fouad", ""], ["Zhao", "Zixu", ""], ["Tang", "Siwei", ""], ["Torres", "Wilfredo", ""]]}, {"id": "1804.08254", "submitter": "Chunyu Xie", "authors": "Chunyu Xie and Ce Li and Baochang Zhang and Chen Chen and Jungong Han\n  and Changqing Zou and Jianzhuang Liu", "title": "Memory Attention Networks for Skeleton-based Action Recognition", "comments": "Accepted by IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton-based action recognition task is entangled with complex\nspatio-temporal variations of skeleton joints, and remains challenging for\nRecurrent Neural Networks (RNNs). In this work, we propose a\ntemporal-then-spatial recalibration scheme to alleviate such complex\nvariations, resulting in an end-to-end Memory Attention Networks (MANs) which\nconsist of a Temporal Attention Recalibration Module (TARM) and a\nSpatio-Temporal Convolution Module (STCM). Specifically, the TARM is deployed\nin a residual learning module that employs a novel attention learning network\nto recalibrate the temporal attention of frames in a skeleton sequence. The\nSTCM treats the attention calibrated skeleton joint sequences as images and\nleverages the Convolution Neural Networks (CNNs) to further model the spatial\nand temporal information of skeleton data. These two modules (TARM and STCM)\nseamlessly form a single network architecture that can be trained in an\nend-to-end fashion. MANs significantly boost the performance of skeleton-based\naction recognition and achieve the best results on four challenging benchmark\ndatasets: NTU RGB+D, HDM05, SYSU-3D and UT-Kinect.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 06:32:27 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 08:20:42 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Xie", "Chunyu", ""], ["Li", "Ce", ""], ["Zhang", "Baochang", ""], ["Chen", "Chen", ""], ["Han", "Jungong", ""], ["Zou", "Changqing", ""], ["Liu", "Jianzhuang", ""]]}, {"id": "1804.08256", "submitter": "Jiagao Hu", "authors": "Jiagao Hu, Zhengxing Sun, Yunhan Sun, Jinlong Shi", "title": "Progressive refinement: a method of coarse-to-fine image parsing using\n  stacked network", "comments": "Accepted for presentation in an ORAL session at ICME 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To parse images into fine-grained semantic parts, the complex fine-grained\nelements will put it in trouble when using off-the-shelf semantic segmentation\nnetworks. In this paper, for image parsing task, we propose to parse images\nfrom coarse to fine with progressively refined semantic classes. It is achieved\nby stacking the segmentation layers in a segmentation network several times.\nThe former segmentation module parses images at a coarser-grained level, and\nthe result will be feed to the following one to provide effective contextual\nclues for the finer-grained parsing. To recover the details of small\nstructures, we add skip connections from shallow layers of the network to\nfine-grained parsing modules. As for the network training, we merge classes in\ngroundtruth to get coarse-to-fine label maps, and train the stacked network\nwith these hierarchical supervision end-to-end. Our coarse-to-fine stacked\nframework can be injected into many advanced neural networks to improve the\nparsing results. Extensive evaluations on several public datasets including\nface parsing and human parsing well demonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 06:33:53 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Hu", "Jiagao", ""], ["Sun", "Zhengxing", ""], ["Sun", "Yunhan", ""], ["Shi", "Jinlong", ""]]}, {"id": "1804.08264", "submitter": "Ting Yao", "authors": "Yingwei Pan and Zhaofan Qiu and Ting Yao and Houqiang Li and Tao Mei", "title": "To Create What You Tell: Generating Videos from Captions", "comments": "ACM MM 2017 Brave New Idea", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are creating multimedia contents everyday and everywhere. While automatic\ncontent generation has played a fundamental challenge to multimedia community\nfor decades, recent advances of deep learning have made this problem feasible.\nFor example, the Generative Adversarial Networks (GANs) is a rewarding approach\nto synthesize images. Nevertheless, it is not trivial when capitalizing on GANs\nto generate videos. The difficulty originates from the intrinsic structure\nwhere a video is a sequence of visually coherent and semantically dependent\nframes. This motivates us to explore semantic and temporal coherence in\ndesigning GANs to generate videos. In this paper, we present a novel Temporal\nGANs conditioning on Captions, namely TGANs-C, in which the input to the\ngenerator network is a concatenation of a latent noise vector and caption\nembedding, and then is transformed into a frame sequence with 3D\nspatio-temporal convolutions. Unlike the naive discriminator which only judges\npairs as fake or real, our discriminator additionally notes whether the video\nmatches the correct caption. In particular, the discriminator network consists\nof three discriminators: video discriminator classifying realistic videos from\ngenerated ones and optimizes video-caption matching, frame discriminator\ndiscriminating between real and fake frames and aligning frames with the\nconditioning caption, and motion discriminator emphasizing the philosophy that\nthe adjacent frames in the generated videos should be smoothly connected as in\nreal ones. We qualitatively demonstrate the capability of our TGANs-C to\ngenerate plausible videos conditioning on the given captions on two synthetic\ndatasets (SBMG and TBMG) and one real-world dataset (MSVD). Moreover,\nquantitative experiments on MSVD are performed to validate our proposal via\nGenerative Adversarial Metric and human study.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 07:07:20 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Pan", "Yingwei", ""], ["Qiu", "Zhaofan", ""], ["Yao", "Ting", ""], ["Li", "Houqiang", ""], ["Mei", "Tao", ""]]}, {"id": "1804.08274", "submitter": "Ting Yao", "authors": "Yehao Li and Ting Yao and Yingwei Pan and Hongyang Chao and Tao Mei", "title": "Jointly Localizing and Describing Events for Dense Video Captioning", "comments": "CVPR 2018 Spotlight, Rank 1 in ActivityNet Captions Challenge 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically describing a video with natural language is regarded as a\nfundamental challenge in computer vision. The problem nevertheless is not\ntrivial especially when a video contains multiple events to be worthy of\nmention, which often happens in real videos. A valid question is how to\ntemporally localize and then describe events, which is known as \"dense video\ncaptioning.\" In this paper, we present a novel framework for dense video\ncaptioning that unifies the localization of temporal event proposals and\nsentence generation of each proposal, by jointly training them in an end-to-end\nmanner. To combine these two worlds, we integrate a new design, namely\ndescriptiveness regression, into a single shot detection structure to infer the\ndescriptive complexity of each detected proposal via sentence generation. This\nin turn adjusts the temporal locations of each event proposal. Our model\ndiffers from existing dense video captioning methods since we propose a joint\nand global optimization of detection and captioning, and the framework uniquely\ncapitalizes on an attribute-augmented video captioning architecture. Extensive\nexperiments are conducted on ActivityNet Captions dataset and our framework\nshows clear improvements when compared to the state-of-the-art techniques. More\nremarkably, we obtain a new record: METEOR of 12.96% on ActivityNet Captions\nofficial test set.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 08:18:35 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Li", "Yehao", ""], ["Yao", "Ting", ""], ["Pan", "Yingwei", ""], ["Chao", "Hongyang", ""], ["Mei", "Tao", ""]]}, {"id": "1804.08275", "submitter": "Ting Yao", "authors": "Zhaofan Qiu and Yingwei Pan and Ting Yao and Tao Mei", "title": "Deep Semantic Hashing with Generative Adversarial Networks", "comments": "SIGIR 2017 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has been a widely-adopted technique for nearest neighbor search in\nlarge-scale image retrieval tasks. Recent research has shown that leveraging\nsupervised information can lead to high quality hashing. However, the cost of\nannotating data is often an obstacle when applying supervised hashing to a new\ndomain. Moreover, the results can suffer from the robustness problem as the\ndata at training and test stage could come from similar but different\ndistributions. This paper studies the exploration of generating synthetic data\nthrough semi-supervised generative adversarial networks (GANs), which leverages\nlargely unlabeled and limited labeled training data to produce highly\ncompelling data with intrinsic invariance and global coherence, for better\nunderstanding statistical structures of natural data. We demonstrate that the\nabove two limitations can be well mitigated by applying the synthetic data for\nhashing. Specifically, a novel deep semantic hashing with GANs (DSH-GANs) is\npresented, which mainly consists of four components: a deep convolution neural\nnetworks (CNN) for learning image representations, an adversary stream to\ndistinguish synthetic images from real ones, a hash stream for encoding image\nrepresentations to hash codes and a classification stream. The whole\narchitecture is trained end-to-end by jointly optimizing three losses, i.e.,\nadversarial loss to correct label of synthetic or real for each sample, triplet\nranking loss to preserve the relative similarity ordering in the input\nreal-synthetic triplets and classification loss to classify each sample\naccurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image\nbenchmarks validate the capability of exploiting synthetic images for hashing.\nOur framework also achieves superior results when compared to state-of-the-art\ndeep hash models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 08:19:55 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Qiu", "Zhaofan", ""], ["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Mei", "Tao", ""]]}, {"id": "1804.08281", "submitter": "Ting Yao", "authors": "Qi Cai and Yingwei Pan and Ting Yao and Chenggang Yan and Tao Mei", "title": "Memory Matching Networks for One-Shot Image Recognition", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the new ideas of augmenting Convolutional Neural\nNetworks (CNNs) with Memory and learning to learn the network parameters for\nthe unlabelled images on the fly in one-shot learning. Specifically, we present\nMemory Matching Networks (MM-Net) --- a novel deep architecture that explores\nthe training procedure, following the philosophy that training and test\nconditions must match. Technically, MM-Net writes the features of a set of\nlabelled images (support set) into memory and reads from memory when performing\ninference to holistically leverage the knowledge in the set. Meanwhile, a\nContextual Learner employs the memory slots in a sequential manner to predict\nthe parameters of CNNs for unlabelled images. The whole architecture is trained\nby once showing only a few examples per class and switching the learning from\nminibatch to minibatch, which is tailored for one-shot learning when presented\nwith a few examples of new categories at test time. Unlike the conventional\none-shot learning approaches, our MM-Net could output one unified model\nirrespective of the number of shots and categories. Extensive experiments are\nconducted on two public datasets, i.e., Omniglot and \\emph{mini}ImageNet, and\nsuperior results are reported when compared to state-of-the-art approaches.\nMore remarkably, our MM-Net improves one-shot accuracy on Omniglot from 98.95%\nto 99.28% and from 49.21% to 53.37% on \\emph{mini}ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 08:31:26 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Cai", "Qi", ""], ["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Yan", "Chenggang", ""], ["Mei", "Tao", ""]]}, {"id": "1804.08286", "submitter": "Ting Yao", "authors": "Yiheng Zhang and Zhaofan Qiu and Ting Yao and Dong Liu and Tao Mei", "title": "Fully Convolutional Adaptation Networks for Semantic Segmentation", "comments": "CVPR 2018, Rank 1 in Segmentation Track of Visual Domain Adaptation\n  Challenge 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances in deep neural networks have convincingly demonstrated\nhigh capability in learning vision models on large datasets. Nevertheless,\ncollecting expert labeled datasets especially with pixel-level annotations is\nan extremely expensive process. An appealing alternative is to render synthetic\ndata (e.g., computer games) and generate ground truth automatically. However,\nsimply applying the models learnt on synthetic images may lead to high\ngeneralization error on real images due to domain shift. In this paper, we\nfacilitate this issue from the perspectives of both visual appearance-level and\nrepresentation-level domain adaptation. The former adapts source-domain images\nto appear as if drawn from the \"style\" in the target domain and the latter\nattempts to learn domain-invariant representations. Specifically, we present\nFully Convolutional Adaptation Networks (FCAN), a novel deep architecture for\nsemantic segmentation which combines Appearance Adaptation Networks (AAN) and\nRepresentation Adaptation Networks (RAN). AAN learns a transformation from one\ndomain to the other in the pixel space and RAN is optimized in an adversarial\nlearning manner to maximally fool the domain discriminator with the learnt\nsource and target representations. Extensive experiments are conducted on the\ntransfer from GTA5 (game videos) to Cityscapes (urban street scenes) on\nsemantic segmentation and our proposal achieves superior results when comparing\nto state-of-the-art unsupervised adaptation techniques. More remarkably, we\nobtain a new record: mIoU of 47.5% on BDDS (drive-cam videos) in an\nunsupervised setting.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 08:44:52 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Zhang", "Yiheng", ""], ["Qiu", "Zhaofan", ""], ["Yao", "Ting", ""], ["Liu", "Dong", ""], ["Mei", "Tao", ""]]}, {"id": "1804.08292", "submitter": "Patrick Follmann", "authors": "Patrick Follmann, Tobias B\\\"ottger, Philipp H\\\"artinger, Rebecca\n  K\\\"onig, Markus Ulrich", "title": "MVTec D2S: Densely Segmented Supermarket Dataset", "comments": "accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the Densely Segmented Supermarket (D2S) dataset, a novel\nbenchmark for instance-aware semantic segmentation in an industrial domain. It\ncontains 21,000 high-resolution images with pixel-wise labels of all object\ninstances. The objects comprise groceries and everyday products from 60\ncategories. The benchmark is designed such that it resembles the real-world\nsetting of an automatic checkout, inventory, or warehouse system. The training\nimages only contain objects of a single class on a homogeneous background,\nwhile the validation and test sets are much more complex and diverse. To\nfurther benchmark the robustness of instance segmentation methods, the scenes\nare acquired with different lightings, rotations, and backgrounds. We ensure\nthat there are no ambiguities in the labels and that every instance is labeled\ncomprehensively. The annotations are pixel-precise and allow using crops of\nsingle instances for articial data augmentation. The dataset covers several\nchallenges highly relevant in the field, such as a limited amount of training\ndata and a high diversity in the test and validation sets. The evaluation of\nstate-of-the-art object detection and instance segmentation methods on D2S\nreveals significant room for improvement.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 09:01:26 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 15:50:26 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Follmann", "Patrick", ""], ["B\u00f6ttger", "Tobias", ""], ["H\u00e4rtinger", "Philipp", ""], ["K\u00f6nig", "Rebecca", ""], ["Ulrich", "Markus", ""]]}, {"id": "1804.08302", "submitter": "Boitumelo Ruf", "authors": "Boitumelo Ruf, Laurenz Thiel, Martin Weinmann", "title": "Deep cross-domain building extraction for selective depth estimation\n  from oblique aerial imagery", "comments": "Accepted in the ISPRS Annals of the Photogrammetry, Remote Sensing\n  and Spatial Information Science", "journal-ref": "ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., IV-1,\n  125-132, 2018", "doi": "10.5194/isprs-annals-IV-1-125-2018", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the technological advancements of aerial imagery and accurate 3d\nreconstruction of urban environments, more and more attention has been paid to\nthe automated analyses of urban areas. In our work, we examine two important\naspects that allow live analysis of building structures in city models given\noblique aerial imagery, namely automatic building extraction with convolutional\nneural networks (CNNs) and selective real-time depth estimation from aerial\nimagery. We use transfer learning to train the Faster R-CNN method for\nreal-time deep object detection, by combining a large ground-based dataset for\nurban scene understanding with a smaller number of images from an aerial\ndataset. We achieve an average precision (AP) of about 80% for the task of\nbuilding extraction on a selected evaluation dataset. Our evaluation focuses on\nboth dataset-specific learning and transfer learning. Furthermore, we present\nan algorithm that allows for multi-view depth estimation from aerial imagery in\nreal-time. We adopt the semi-global matching (SGM) optimization strategy to\npreserve sharp edges at object boundaries. In combination with the Faster\nR-CNN, it allows a selective reconstruction of buildings, identified with\nregions of interest (RoIs), from oblique aerial imagery.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 09:22:55 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 07:49:31 GMT"}, {"version": "v3", "created": "Sat, 21 Sep 2019 20:24:52 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ruf", "Boitumelo", ""], ["Thiel", "Laurenz", ""], ["Weinmann", "Martin", ""]]}, {"id": "1804.08328", "submitter": "Amir Zamir", "authors": "Amir Zamir, Alexander Sax, William Shen, Leonidas Guibas, Jitendra\n  Malik, Silvio Savarese", "title": "Taskonomy: Disentangling Task Transfer Learning", "comments": "CVPR 2018 (Oral). See project website and live demos at\n  http://taskonomy.vision/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do visual tasks have a relationship, or are they unrelated? For instance,\ncould having surface normals simplify estimating the depth of an image?\nIntuition answers these questions positively, implying existence of a structure\namong visual tasks. Knowing this structure has notable values; it is the\nconcept underlying transfer learning and provides a principled way for\nidentifying redundancies across tasks, e.g., to seamlessly reuse supervision\namong related tasks or solve many tasks in one system without piling up the\ncomplexity.\n  We proposes a fully computational approach for modeling the structure of\nspace of visual tasks. This is done via finding (first and higher-order)\ntransfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D,\nand semantic tasks in a latent space. The product is a computational taxonomic\nmap for task transfer learning. We study the consequences of this structure,\ne.g. nontrivial emerged relationships, and exploit them to reduce the demand\nfor labeled data. For example, we show that the total number of labeled\ndatapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3\n(compared to training independently) while keeping the performance nearly the\nsame. We provide a set of tools for computing and probing this taxonomical\nstructure including a solver that users can employ to devise efficient\nsupervision policies for their use cases.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 10:46:28 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Zamir", "Amir", ""], ["Sax", "Alexander", ""], ["Shen", "William", ""], ["Guibas", "Leonidas", ""], ["Malik", "Jitendra", ""], ["Savarese", "Silvio", ""]]}, {"id": "1804.08348", "submitter": "Shan Li", "authors": "Shan Li and Weihong Deng", "title": "Deep Facial Expression Recognition: A Survey", "comments": null, "journal-ref": "IEEE Transactions on Affective Computing 2020", "doi": "10.1109/TAFFC.2020.2981446", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the transition of facial expression recognition (FER) from\nlaboratory-controlled to challenging in-the-wild conditions and the recent\nsuccess of deep learning techniques in various fields, deep neural networks\nhave increasingly been leveraged to learn discriminative representations for\nautomatic FER. Recent deep FER systems generally focus on two important issues:\noverfitting caused by a lack of sufficient training data and\nexpression-unrelated variations, such as illumination, head pose and identity\nbias. In this paper, we provide a comprehensive survey on deep FER, including\ndatasets and algorithms that provide insights into these intrinsic problems.\nFirst, we describe the standard pipeline of a deep FER system with the related\nbackground knowledge and suggestions of applicable implementations for each\nstage. We then introduce the available datasets that are widely used in the\nliterature and provide accepted data selection and evaluation principles for\nthese datasets. For the state of the art in deep FER, we review existing novel\ndeep neural networks and related training strategies that are designed for FER\nbased on both static images and dynamic image sequences, and discuss their\nadvantages and limitations. Competitive performances on widely used benchmarks\nare also summarized in this section. We then extend our survey to additional\nrelated issues and application scenarios. Finally, we review the remaining\nchallenges and corresponding opportunities in this field as well as future\ndirections for the design of robust deep FER systems.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 11:40:09 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 02:26:49 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Li", "Shan", ""], ["Deng", "Weihong", ""]]}, {"id": "1804.08355", "submitter": "Hui Li", "authors": "Hui Li, Xiao-Jun Wu", "title": "Multi-focus Image Fusion using dictionary learning and Low-Rank\n  Representation", "comments": "12 pages, 5 figures, 2 tables. The 9th International Conference on\n  Image and Graphics (ICIG 2017, Oral)", "journal-ref": null, "doi": "10.1007/978-3-319-71607-7_59", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the representation learning, the low-rank representation (LRR) is one\nof the hot research topics in many fields, especially in image processing and\npattern recognition. Although LRR can capture the global structure, the ability\nof local structure preservation is limited because LRR lacks dictionary\nlearning. In this paper, we propose a novel multi-focus image fusion method\nbased on dictionary learning and LRR to get a better performance in both global\nand local structure. Firstly, the source images are divided into several\npatches by sliding window technique. Then, the patches are classified according\nto the Histogram of Oriented Gradient (HOG) features. And the sub-dictionaries\nof each class are learned by K-singular value decomposition (K-SVD) algorithm.\nSecondly, a global dictionary is constructed by combining these\nsub-dictionaries. Then, we use the global dictionary in LRR to obtain the LRR\ncoefficients vector for each patch. Finally, the l_1-norm and choose-max fuse\nstrategy for each coefficients vector is adopted to reconstruct fused image\nfrom the fused LRR coefficients and the global dictionary. Experimental results\ndemonstrate that the proposed method can obtain state-of-the-art performance in\nboth qualitative and quantitative evaluations compared with serval classical\nmethods and novel methods.The Code of our fusion method is available at\nhttps://github.com/hli1221/imagefusion_dllrr\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 11:57:44 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 08:32:20 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Li", "Hui", ""], ["Wu", "Xiao-Jun", ""]]}, {"id": "1804.08361", "submitter": "Hui Li", "authors": "Hui Li, Xiao-Jun Wu", "title": "DenseFuse: A Fusion Approach to Infrared and Visible Images", "comments": "10 pages, 11 figures, 2 tables. Accepted by IEEE Transactions on\n  Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2018.2887342", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel deep learning architecture for infrared and\nvisible images fusion problem. In contrast to conventional convolutional\nnetworks, our encoding network is combined by convolutional layers, fusion\nlayer and dense block in which the output of each layer is connected to every\nother layer. We attempt to use this architecture to get more useful features\nfrom source images in encoding process. And two fusion layers(fusion\nstrategies) are designed to fuse these features. Finally, the fused image is\nreconstructed by decoder. Compared with existing fusion methods, the proposed\nfusion method achieves state-of-the-art performance in objective and subjective\nassessment. Code and pre-trained models are available at\nhttps://github.com/hli1221/imagefusion_densefuse\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 12:20:11 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 11:36:31 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2018 08:00:38 GMT"}, {"version": "v4", "created": "Mon, 6 Aug 2018 13:48:40 GMT"}, {"version": "v5", "created": "Sat, 18 Aug 2018 06:20:39 GMT"}, {"version": "v6", "created": "Wed, 5 Sep 2018 02:12:17 GMT"}, {"version": "v7", "created": "Mon, 17 Dec 2018 07:45:44 GMT"}, {"version": "v8", "created": "Tue, 18 Dec 2018 08:24:29 GMT"}, {"version": "v9", "created": "Sun, 20 Jan 2019 05:46:09 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Li", "Hui", ""], ["Wu", "Xiao-Jun", ""]]}, {"id": "1804.08366", "submitter": "Noha Radwan", "authors": "Noha Radwan, Abhinav Valada, Wolfram Burgard", "title": "VLocNet++: Deep Multitask Learning for Semantic Visual Localization and\n  Odometry", "comments": "Demo and dataset available at http://deeploc.cs.uni-freiburg.de", "journal-ref": "IEEE Robotics and Automation Letters (RA-L), 3(4):4407-4414, 2018", "doi": "10.1109/LRA.2018.2869640", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic understanding and localization are fundamental enablers of robot\nautonomy that have for the most part been tackled as disjoint problems. While\ndeep learning has enabled recent breakthroughs across a wide spectrum of scene\nunderstanding tasks, its applicability to state estimation tasks has been\nlimited due to the direct formulation that renders it incapable of encoding\nscene-specific constrains. In this work, we propose the VLocNet++ architecture\nthat employs a multitask learning approach to exploit the inter-task\nrelationship between learning semantics, regressing 6-DoF global pose and\nodometry, for the mutual benefit of each of these tasks. Our network overcomes\nthe aforementioned limitation by simultaneously embedding geometric and\nsemantic knowledge of the world into the pose regression network. We propose a\nnovel adaptive weighted fusion layer to aggregate motion-specific temporal\ninformation and to fuse semantic features into the localization stream based on\nregion activations. Furthermore, we propose a self-supervised warping technique\nthat uses the relative motion to warp intermediate network representations in\nthe segmentation stream for learning consistent semantics. Finally, we\nintroduce a first-of-a-kind urban outdoor localization dataset with pixel-level\nsemantic labels and multiple loops for training deep networks. Extensive\nexperiments on the challenging Microsoft 7-Scenes benchmark and our DeepLoc\ndataset demonstrate that our approach exceeds the state-of-the-art\noutperforming local feature-based methods while simultaneously performing\nmultiple tasks and exhibiting substantial robustness in challenging scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 12:30:16 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 11:10:43 GMT"}, {"version": "v3", "created": "Wed, 2 May 2018 09:47:14 GMT"}, {"version": "v4", "created": "Mon, 30 Jul 2018 10:35:48 GMT"}, {"version": "v5", "created": "Wed, 26 Sep 2018 20:06:13 GMT"}, {"version": "v6", "created": "Thu, 11 Oct 2018 15:36:09 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Radwan", "Noha", ""], ["Valada", "Abhinav", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1804.08376", "submitter": "Tomas Iesmantas", "authors": "Tomas Iesmantas and Robertas Alzbutas", "title": "Convolutional capsule network for classification of breast cancer\n  histology images", "comments": "Submitted to ICIAR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatization of the diagnosis of any kind of disease is of great importance\nand it's gaining speed as more and more deep learning solutions are applied to\ndifferent problems. One of such computer aided systems could be a decision\nsupport too able to accurately differentiate between different types of breast\ncancer histological images - normal tissue or carcinoma. In this paper authors\npresent a deep learning solution, based on convolutional capsule network for\nclassification of four types of images of breast tissue biopsy when hematoxylin\nand eusin staining is applied. The cross-validation accuracy was achieved to be\n0.87 with equaly high sensitivity.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 12:48:49 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Iesmantas", "Tomas", ""], ["Alzbutas", "Robertas", ""]]}, {"id": "1804.08378", "submitter": "Nicolas Weber", "authors": "Nicolas Weber, Florian Schmidt, Mathias Niepert, Felipe Huici", "title": "BrainSlug: Transparent Acceleration of Deep Learning Through Depth-First\n  Parallelism", "comments": "Technical Report, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CV cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network frameworks such as PyTorch and TensorFlow are the workhorses\nof numerous machine learning applications ranging from object recognition to\nmachine translation. While these frameworks are versatile and straightforward\nto use, the training of and inference in deep neural networks is resource\n(energy, compute, and memory) intensive. In contrast to recent works focusing\non algorithmic enhancements, we introduce BrainSlug, a framework that\ntransparently accelerates neural network workloads by changing the default\nlayer-by-layer processing to a depth-first approach, reducing the amount of\ndata required by the computations and thus improving the performance of the\navailable hardware caches. BrainSlug achieves performance improvements of up to\n41.1% on CPUs and 35.7% on GPUs. These optimizations come at zero cost to the\nuser as they do not require hardware changes and only need tiny adjustments to\nthe software.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 12:49:04 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Weber", "Nicolas", ""], ["Schmidt", "Florian", ""], ["Niepert", "Mathias", ""], ["Huici", "Felipe", ""]]}, {"id": "1804.08381", "submitter": "Yong Man Ro", "authors": "Sangmin Lee, Hak Gu Kim, Yong Man Ro", "title": "STAN: Spatio-Temporal Adversarial Networks for Abnormal Event Detection", "comments": "ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel abnormal event detection method with\nspatio-temporal adversarial networks (STAN). We devise a spatio-temporal\ngenerator which synthesizes an inter-frame by considering spatio-temporal\ncharacteristics with bidirectional ConvLSTM. A proposed spatio-temporal\ndiscriminator determines whether an input sequence is real-normal or not with\n3D convolutional layers. These two networks are trained in an adversarial way\nto effectively encode spatio-temporal features of normal patterns. After the\nlearning, the generator and the discriminator can be independently used as\ndetectors, and deviations from the learned normal patterns are detected as\nabnormalities. Experimental results show that the proposed method achieved\ncompetitive performance compared to the state-of-the-art methods. Further, for\nthe interpretation, we visualize the location of abnormal events detected by\nthe proposed networks using a generator loss and discriminator gradients.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 12:50:26 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Lee", "Sangmin", ""], ["Kim", "Hak Gu", ""], ["Ro", "Yong Man", ""]]}, {"id": "1804.08414", "submitter": "Seyoun Park", "authors": "Yan Wang, Yuyin Zhou, Wei Shen, Seyoun Park, Elliot K. Fishman, Alan\n  L. Yuille", "title": "Abdominal multi-organ segmentation with organ-attention networks and\n  statistical fusion", "comments": "21 pages, 11 figures", "journal-ref": "Medical Image Analysis, 2019", "doi": "10.1016/j.media.2019.04.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and robust segmentation of abdominal organs on CT is essential for\nmany clinical applications such as computer-aided diagnosis and computer-aided\nsurgery. But this task is challenging due to the weak boundaries of organs, the\ncomplexity of the background, and the variable sizes of different organs. To\naddress these challenges, we introduce a novel framework for multi-organ\nsegmentation by using organ-attention networks with reverse connections\n(OAN-RCs) which are applied to 2D views, of the 3D CT volume, and output\nestimates which are combined by statistical fusion exploiting structural\nsimilarity. OAN is a two-stage deep convolutional network, where deep network\nfeatures from the first stage are combined with the original image, in a second\nstage, to reduce the complex background and enhance the discriminative\ninformation for the target organs. RCs are added to the first stage to give the\nlower layers semantic information thereby enabling them to adapt to the sizes\nof different organs. Our networks are trained on 2D views enabling us to use\nholistic information and allowing efficient computation. To compensate for the\nlimited cross-sectional information of the original 3D volumetric CT,\nmulti-sectional images are reconstructed from the three different 2D view\ndirections. Then we combine the segmentation results from the different views\nusing statistical fusion, with a novel term relating the structural similarity\nof the 2D views to the original 3D structure. To train the network and evaluate\nresults, 13 structures were manually annotated by four human raters and\nconfirmed by a senior expert on 236 normal cases. We tested our algorithm and\ncomputed Dice-Sorensen similarity coefficients and surface distances for\nevaluating our estimates of the 13 structures. Our experiments show that the\nproposed approach outperforms 2D- and 3D-patch based state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 13:38:34 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Wang", "Yan", ""], ["Zhou", "Yuyin", ""], ["Shen", "Wei", ""], ["Park", "Seyoun", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1804.08424", "submitter": "Jens Grubert", "authors": "Fabian G\\\"ottl and Philipp Gagel and Jens Grubert", "title": "Efficient Pose Tracking from Natural Features in Standard Web Browsers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer Vision-based natural feature tracking is at the core of modern\nAugmented Reality applications. Still, Web-based Augmented Reality typically\nrelies on location-based sensing (using GPS and orientation sensors) or\nmarker-based approaches to solve the pose estimation problem.\n  We present an implementation and evaluation of an efficient natural feature\ntracking pipeline for standard Web browsers using HTML5 and WebAssembly. Our\nsystem can track image targets at real-time frame rates tablet PCs (up to 60\nHz) and smartphones (up to 25 Hz).\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 13:46:01 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["G\u00f6ttl", "Fabian", ""], ["Gagel", "Philipp", ""], ["Grubert", "Jens", ""]]}, {"id": "1804.08450", "submitter": "Dawei Yang", "authors": "Lei Huang, Dawei Yang, Bo Lang, Jia Deng", "title": "Decorrelated Batch Normalization", "comments": "Accepted to CVPR 2018. Code available at\n  https://github.com/umich-vl/DecorrelatedBN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) is capable of accelerating the training of deep\nmodels by centering and scaling activations within mini-batches. In this work,\nwe propose Decorrelated Batch Normalization (DBN), which not just centers and\nscales activations but whitens them. We explore multiple whitening techniques,\nand find that PCA whitening causes a problem we call stochastic axis swapping,\nwhich is detrimental to learning. We show that ZCA whitening does not suffer\nfrom this problem, permitting successful learning. DBN retains the desirable\nqualities of BN and further improves BN's optimization efficiency and\ngeneralization ability. We design comprehensive experiments to show that DBN\ncan improve the performance of BN on multilayer perceptrons and convolutional\nneural networks. Furthermore, we consistently improve the accuracy of residual\nnetworks on CIFAR-10, CIFAR-100, and ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 14:06:50 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Huang", "Lei", ""], ["Yang", "Dawei", ""], ["Lang", "Bo", ""], ["Deng", "Jia", ""]]}, {"id": "1804.08454", "submitter": "Akilesh Badrinaaraayanan", "authors": "Akilesh B, Abhishek Sinha, Mausoom Sarkar, Balaji Krishnamurthy", "title": "Attention Based Natural Language Grounding by Navigating Virtual\n  Environment", "comments": "Accepted at WACV 2019. Also at NeurIPS 2017 workshop on\n  Visually-Grounded Interaction and Language (ViGIL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on the problem of grounding language by training an\nagent to follow a set of natural language instructions and navigate to a target\nobject in an environment. The agent receives visual information through raw\npixels and a natural language instruction telling what task needs to be\nachieved and is trained in an end-to-end way. We develop an attention mechanism\nfor multi-modal fusion of visual and textual modalities that allows the agent\nto learn to complete the task and achieve language grounding. Our experimental\nresults show that our attention mechanism outperforms the existing multi-modal\nfusion mechanisms proposed for both 2D and 3D environments in order to solve\nthe above-mentioned task in terms of both speed and success rate. We show that\nthe learnt textual representations are semantically meaningful as they follow\nvector arithmetic in the embedding space. The effectiveness of our attention\napproach over the contemporary fusion mechanisms is also highlighted from the\ntextual embeddings learnt by the different approaches. We also show that our\nmodel generalizes effectively to unseen scenarios and exhibit zero-shot\ngeneralization capabilities both in 2D and 3D environments. The code for our 2D\nenvironment as well as the models that we developed for both 2D and 3D are\navailable at https://github.com/rl-lang-grounding/rl-lang-ground.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 14:11:17 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 19:00:54 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["B", "Akilesh", ""], ["Sinha", "Abhishek", ""], ["Sarkar", "Mausoom", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "1804.08468", "submitter": "Xutong Ren", "authors": "Xutong Ren, Mading Li, Wen-Huang Cheng and Jiaying Liu", "title": "Joint Enhancement and Denoising Method via Sequential Decomposition", "comments": "Accepted by ISCAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many low-light enhancement methods ignore intensive noise in original images.\nAs a result, they often simultaneously enhance the noise as well. Furthermore,\nextra denoising procedures adopted by most methods ruin the details. In this\npaper, we introduce a joint low-light enhancement and denoising strategy, aimed\nat obtaining well-enhanced low-light images while getting rid of the inherent\nnoise issue simultaneously. The proposed method performs Retinex model based\ndecomposition in a successive sequence, which sequentially estimates a\npiece-wise smoothed illumination and a noise-suppressed reflectance. After\ngetting the illumination and reflectance map, we adjust the illumination layer\nand generate our enhancement result. In this noise-suppressed sequential\ndecomposition process we enforce the spatial smoothness on each component and\nskillfully make use of weight matrices to suppress the noise and improve the\ncontrast. Results of extensive experiments demonstrate the effectiveness and\npracticability of our method. It performs well for a wide variety of images,\nand achieves better or comparable quality compared with the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 14:31:21 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 09:23:10 GMT"}, {"version": "v3", "created": "Sat, 28 Apr 2018 13:55:01 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Ren", "Xutong", ""], ["Li", "Mading", ""], ["Cheng", "Wen-Huang", ""], ["Liu", "Jiaying", ""]]}, {"id": "1804.08473", "submitter": "Bei Liu", "authors": "Bei Liu, Jianlong Fu, Makoto P. Kato, Masatoshi Yoshikawa", "title": "Beyond Narrative Description: Generating Poetry from Images by\n  Multi-Adversarial Training", "comments": null, "journal-ref": null, "doi": "10.1145/3240508.3240587", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic generation of natural language from images has attracted extensive\nattention. In this paper, we take one step further to investigate generation of\npoetic language (with multiple lines) to an image for automatic poetry\ncreation. This task involves multiple challenges, including discovering poetic\nclues from the image (e.g., hope from green), and generating poems to satisfy\nboth relevance to the image and poeticness in language level. To solve the\nabove challenges, we formulate the task of poem generation into two correlated\nsub-tasks by multi-adversarial training via policy gradient, through which the\ncross-modal relevance and poetic language style can be ensured. To extract\npoetic clues from images, we propose to learn a deep coupled visual-poetic\nembedding, in which the poetic representation from objects, sentiments and\nscenes in an image can be jointly learned. Two discriminative networks are\nfurther introduced to guide the poem generation, including a multi-modal\ndiscriminator and a poem-style discriminator. To facilitate the research, we\nhave released two poem datasets by human annotators with two distinct\nproperties: 1) the first human annotated image-to-poem pair dataset (with 8,292\npairs in total), and 2) to-date the largest public English poem corpus dataset\n(with 92,265 different poems in total). Extensive experiments are conducted\nwith 8K images, among which 1.5K image are randomly picked for evaluation. Both\nobjective and subjective evaluations show the superior performances against the\nstate-of-the-art methods for poem generation from images. Turing test carried\nout with over 500 human subjects, among which 30 evaluators are poetry experts,\ndemonstrates the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 14:35:59 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 06:45:53 GMT"}, {"version": "v3", "created": "Fri, 24 Aug 2018 07:35:26 GMT"}, {"version": "v4", "created": "Wed, 10 Oct 2018 03:23:38 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Liu", "Bei", ""], ["Fu", "Jianlong", ""], ["Kato", "Makoto P.", ""], ["Yoshikawa", "Masatoshi", ""]]}, {"id": "1804.08497", "submitter": "Rana Hanocka", "authors": "Rana Hanocka, Noa Fish, Zhenhua Wang, Raja Giryes, Shachar Fleishman\n  and Daniel Cohen-Or", "title": "ALIGNet: Partial-Shape Agnostic Alignment via Unsupervised Learning", "comments": "To be presented at SIGGRAPH Asia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of aligning a pair of shapes is a fundamental operation in\ncomputer graphics. Traditional approaches rely heavily on matching\ncorresponding points or features to guide the alignment, a paradigm that\nfalters when significant shape portions are missing. These techniques generally\ndo not incorporate prior knowledge about expected shape characteristics, which\ncan help compensate for any misleading cues left by inaccuracies exhibited in\nthe input shapes. We present an approach based on a deep neural network,\nleveraging shape datasets to learn a shape-aware prior for source-to-target\nalignment that is robust to shape incompleteness. In the absence of ground\ntruth alignments for supervision, we train a network on the task of shape\nalignment using incomplete shapes generated from full shapes for\nself-supervision. Our network, called ALIGNet, is trained to warp complete\nsource shapes to incomplete targets, as if the target shapes were complete,\nthus essentially rendering the alignment partial-shape agnostic. We aim for the\nnetwork to develop specialized expertise over the common characteristics of the\nshapes in each dataset, thereby achieving a higher-level understanding of the\nexpected shape space to which a local approach would be oblivious. We constrain\nALIGNet through an anisotropic total variation identity regularization to\npromote piecewise smooth deformation fields, facilitating both partial-shape\nagnosticism and post-deformation applications. We demonstrate that ALIGNet\nlearns to align geometrically distinct shapes, and is able to infer plausible\nmappings even when the target shape is significantly incomplete. We show that\nour network learns the common expected characteristics of shape collections,\nwithout over-fitting or memorization, enabling it to produce plausible\ndeformations on unseen data during test time.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 15:17:26 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 22:26:39 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Hanocka", "Rana", ""], ["Fish", "Noa", ""], ["Wang", "Zhenhua", ""], ["Giryes", "Raja", ""], ["Fleishman", "Shachar", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1804.08506", "submitter": "Maryam Babaee", "authors": "Maryam Babaee, Linwei Li, Gerhard Rigoll", "title": "Person Identification from Partial Gait Cycle Using Fully Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait as a biometric property for person identification plays a key role in\nvideo surveillance and security applications. In gait recognition, normally,\ngait feature such as Gait Energy Image (GEI) is extracted from one full gait\ncycle. However in many circumstances, such a full gait cycle might not be\navailable due to occlusion. Thus, the GEI is not complete giving rise to a\ndegrading in gait-based person identification rate. In this paper, we address\nthis issue by proposing a novel method to identify individuals from gait\nfeature when a few (or even single) frame(s) is available. To do so, we propose\na deep learning-based approach to transform incomplete GEI to the corresponding\ncomplete GEI obtained from a full gait cycle. More precisely, this\ntransformation is done gradually by training several auto encoders\nindependently and then combining these as a uniform model. Experimental results\non two public gait datasets, namely OULP and Casia-B demonstrate the validity\nof the proposed method in dealing with very incomplete gait cycles.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 15:27:20 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Babaee", "Maryam", ""], ["Li", "Linwei", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1804.08528", "submitter": "Asifullah Khan", "authors": "Asifullah Khan, Anabia Sohail and Amna Ali", "title": "A New Channel Boosted Convolutional Neural Network using Transfer\n  Learning", "comments": "24 Pages, 5 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel architectural enhancement of Channel Boosting in a deep\nconvolutional neural network (CNN). This idea of Channel Boosting exploits both\nthe channel dimension of CNN (learning from multiple input channels) and\nTransfer learning (TL). TL is utilized at two different stages; channel\ngeneration and channel exploitation. In the proposed methodology, a deep CNN is\nboosted by various channels available through TL from already trained Deep\nNeural Networks, in addition to its original channel. The deep architecture of\nCNN then exploits the original and boosted channels down the stream for\nlearning discriminative patterns. Churn prediction in telecom is a challenging\ntask due to the high dimensionality and imbalanced nature of the data.\nTherefore, churn prediction data is used to evaluate the performance of the\nproposed Channel Boosted CNN (CB CNN). In the first phase, informative\ndiscriminative features are being extracted using a stacked autoencoder, and\nthen in the second phase, these features are combined with the original\nfeatures to form Channel Boosted images. Finally, the knowledge gained by a\npretrained CNN is exploited by employing TL. The results are promising and show\nthe ability of the Channel Boosting concept in learning complex classification\nproblems by discerning even minute differences in churners and nonchurners. The\nproposed work validates the concept observed from the evolution of recent CNN\narchitectures that the innovative restructuring of a CNN architecture may\nincrease the networks representative capacity.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 16:02:35 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 09:34:43 GMT"}, {"version": "v3", "created": "Sun, 20 May 2018 06:39:24 GMT"}, {"version": "v4", "created": "Fri, 18 Jan 2019 22:03:31 GMT"}, {"version": "v5", "created": "Sat, 4 Jul 2020 19:56:44 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Khan", "Asifullah", ""], ["Sohail", "Anabia", ""], ["Ali", "Amna", ""]]}, {"id": "1804.08529", "submitter": "Vishaal Munusamy Kabilan", "authors": "Vishaal Munusamy Kabilan, Brandon Morris, Anh Nguyen", "title": "VectorDefense: Vectorization as a Defense to Adversarial Examples", "comments": "17 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training deep neural networks on images represented as grids of pixels has\nbrought to light an interesting phenomenon known as adversarial examples.\nInspired by how humans reconstruct abstract concepts, we attempt to codify the\ninput bitmap image into a set of compact, interpretable elements to avoid being\nfooled by the adversarial structures. We take the first step in this direction\nby experimenting with image vectorization as an input transformation step to\nmap the adversarial examples back into the natural manifold of MNIST\nhandwritten digits. We compare our method vs. state-of-the-art input\ntransformations and further discuss the trade-offs between a hand-designed and\na learned transformation defense.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 16:04:55 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Kabilan", "Vishaal Munusamy", ""], ["Morris", "Brandon", ""], ["Nguyen", "Anh", ""]]}, {"id": "1804.08572", "submitter": "Shalini De Mello", "authors": "Rajeev Ranjan, Shalini De Mello, Jan Kautz", "title": "Light-weight Head Pose Invariant Gaze Tracking", "comments": "9 pages, IEEE Conference on Computer Vision and Pattern Recognition\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconstrained remote gaze tracking using off-the-shelf cameras is a\nchallenging problem. Recently, promising algorithms for appearance-based gaze\nestimation using convolutional neural networks (CNN) have been proposed.\nImproving their robustness to various confounding factors including variable\nhead pose, subject identity, illumination and image quality remain open\nproblems. In this work, we study the effect of variable head pose on machine\nlearning regressors trained to estimate gaze direction. We propose a novel\nbranched CNN architecture that improves the robustness of gaze classifiers to\nvariable head pose, without increasing computational cost. We also present\nvarious procedures to effectively train our gaze network including transfer\nlearning from the more closely related task of object viewpoint estimation and\nfrom a large high-fidelity synthetic gaze dataset, which enable our ten times\nfaster gaze network to achieve competitive accuracy to its current\nstate-of-the-art direct competitor.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 17:12:11 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Ranjan", "Rajeev", ""], ["De Mello", "Shalini", ""], ["Kautz", "Jan", ""]]}, {"id": "1804.08588", "submitter": "Dafang He", "authors": "Dafang He, Yeqing Li, Alexander Gorban, Derrall Heath, Julian Ibarz,\n  Qian Yu, Daniel Kifer, C. Lee Giles", "title": "Large Scale Scene Text Verification with Guided Attention", "comments": "18 pages, ACCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks are related to determining if a particular text string exists in\nan image. In this work, we propose a new framework that learns this task in an\nend-to-end way. The framework takes an image and a text string as input and\nthen outputs the probability of the text string being present in the image.\nThis is the first end-to-end framework that learns such relationships between\ntext and images in scene text area. The framework does not require explicit\nscene text detection or recognition and thus no bounding box annotations are\nneeded for it. It is also the first work in scene text area that tackles suh a\nweakly labeled problem. Based on this framework, we developed a model called\nGuided Attention. Our designed model achieves much better results than several\nstate-of-the-art scene text reading based solutions for a challenging Street\nView Business Matching task. The task tries to find correct business names for\nstorefront images and the dataset we collected for it is substantially larger,\nand more challenging than existing scene text dataset. This new real-world task\nprovides a new perspective for studying scene text related problems. We also\ndemonstrate the uniqueness of our task via a comparison between our problem and\na typical Visual Question Answering problem.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 17:30:49 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 01:01:52 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["He", "Dafang", ""], ["Li", "Yeqing", ""], ["Gorban", "Alexander", ""], ["Heath", "Derrall", ""], ["Ibarz", "Julian", ""], ["Yu", "Qian", ""], ["Kifer", "Daniel", ""], ["Giles", "C. Lee", ""]]}, {"id": "1804.08598", "submitter": "Anish Athalye", "authors": "Andrew Ilyas, Logan Engstrom, Anish Athalye, Jessy Lin", "title": "Black-box Adversarial Attacks with Limited Queries and Information", "comments": "ICML 2018. This supercedes the previous paper \"Query-efficient\n  Black-box adversarial examples.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current neural network-based classifiers are susceptible to adversarial\nexamples even in the black-box setting, where the attacker only has query\naccess to the model. In practice, the threat model for real-world systems is\noften more restrictive than the typical black-box model where the adversary can\nobserve the full output of the network on arbitrarily many chosen inputs. We\ndefine three realistic threat models that more accurately characterize many\nreal-world classifiers: the query-limited setting, the partial-information\nsetting, and the label-only setting. We develop new attacks that fool\nclassifiers under these more restrictive threat models, where previous methods\nwould be impractical or ineffective. We demonstrate that our methods are\neffective against an ImageNet classifier under our proposed threat models. We\nalso demonstrate a targeted black-box attack against a commercial classifier,\novercoming the challenges of limited query access, partial information, and\nother practical issues to break the Google Cloud Vision API.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 17:46:34 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 18:17:09 GMT"}, {"version": "v3", "created": "Wed, 11 Jul 2018 13:51:00 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Ilyas", "Andrew", ""], ["Engstrom", "Logan", ""], ["Athalye", "Anish", ""], ["Lin", "Jessy", ""]]}, {"id": "1804.08606", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian\n  Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, Trevor\n  Darrell", "title": "Zero-Shot Visual Imitation", "comments": "Oral presentation at ICLR 2018. Website at\n  https://pathak22.github.io/zeroshot-imitation/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current dominant paradigm for imitation learning relies on strong\nsupervision of expert actions to learn both 'what' and 'how' to imitate. We\npursue an alternative paradigm wherein an agent first explores the world\nwithout any expert supervision and then distills its experience into a\ngoal-conditioned skill policy with a novel forward consistency loss. In our\nframework, the role of the expert is only to communicate the goals (i.e., what\nto imitate) during inference. The learned policy is then employed to mimic the\nexpert (i.e., how to imitate) after seeing just a sequence of images\ndemonstrating the desired task. Our method is 'zero-shot' in the sense that the\nagent never has access to expert actions during training or for the task\ndemonstration at inference. We evaluate our zero-shot imitator in two\nreal-world settings: complex rope manipulation with a Baxter robot and\nnavigation in previously unseen office environments with a TurtleBot. Through\nfurther experiments in VizDoom simulation, we provide evidence that better\nmechanisms for exploration lead to learning a more capable policy which in turn\nimproves end task performance. Videos, models, and more details are available\nat https://pathak22.github.io/zeroshot-imitation/\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 17:58:26 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Pathak", "Deepak", ""], ["Mahmoudieh", "Parsa", ""], ["Luo", "Guanghao", ""], ["Agrawal", "Pulkit", ""], ["Chen", "Dian", ""], ["Shentu", "Yide", ""], ["Shelhamer", "Evan", ""], ["Malik", "Jitendra", ""], ["Efros", "Alexei A.", ""], ["Darrell", "Trevor", ""]]}, {"id": "1804.08651", "submitter": "Peyman  Milanfar", "authors": "Peyman Milanfar", "title": "Rendition: Reclaiming what a black box takes away", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The premise of our work is deceptively familiar: A black box $f(\\cdot)$ has\naltered an image $\\mathbf{x} \\rightarrow f(\\mathbf{x})$. Recover the image\n$\\mathbf{x}$. This black box might be any number of simple or complicated\nthings: a linear or non-linear filter, some app on your phone, etc. The latter\nis a good canonical example for the problem we address: Given only \"the app\"\nand an image produced by the app, find the image that was fed to the app. You\ncan run the given image (or any other image) through the app as many times as\nyou like, but you can not look inside the (code for the) app to see how it\nworks. At first blush, the problem sounds a lot like a standard inverse\nproblem, but it is not in the following sense: While we have access to the\nblack box $f(\\cdot)$ and can run any image through it and observe the output,\nwe do not know how the block box alters the image. Therefore we have no\nexplicit form or model of $f(\\cdot)$. Nor are we necessarily interested in the\ninternal workings of the black box. We are simply happy to reverse its effect\non a particular image, to whatever extent possible. This is what we call the\n\"rendition\" (rather than restoration) problem, as it does not fit the mold of\nan inverse problem (blind or otherwise). We describe general conditions under\nwhich rendition is possible, and provide a remarkably simple algorithm that\nworks for both contractive and expansive black box operators. The principal and\nnovel take-away message from our work is this surprising fact: One simple\nalgorithm can reliably undo a wide class of (not too violent) image\ndistortions.\n  A higher quality pdf of this paper is available at http://www.milanfar.org\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 18:23:00 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Milanfar", "Peyman", ""]]}, {"id": "1804.08659", "submitter": "Joshua Engelsma", "authors": "Joshua J. Engelsma, Kai Cao, and Anil K. Jain", "title": "Fingerprint Match in Box", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We open source fingerprint Match in Box, a complete end-to-end fingerprint\nrecognition system embedded within a 4 inch cube. Match in Box stands in\ncontrast to a typical bulky and expensive proprietary fingerprint recognition\nsystem which requires sending a fingerprint image to an external host for\nprocessing and subsequent spoof detection and matching. In particular, Match in\nBox is a first of a kind, portable, low-cost, and easy-to-assemble fingerprint\nreader with an enrollment database embedded within the reader's memory and open\nsource fingerprint spoof detector, feature extractor, and matcher all running\non the reader's internal vision processing unit (VPU). An onboard touch screen\nand rechargeable battery pack make this device extremely portable and ideal for\napplying both fingerprint authentication (1:1 comparison) and fingerprint\nidentification (1:N search) to applications (vaccination tracking, food and\nbenefit distribution programs, human trafficking prevention) in rural\ncommunities, especially in developing countries. We also show that Match in Box\nis suited for capturing neonate fingerprints due to its high resolution (1900\nppi) cameras.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 18:38:39 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Engelsma", "Joshua J.", ""], ["Cao", "Kai", ""], ["Jain", "Anil K.", ""]]}, {"id": "1804.08704", "submitter": "Kamelia Aryafar", "authors": "Murium Iqbal, Adair Kovac, Kamelia Aryafar", "title": "Discovering Style Trends through Deep Visually Aware Latent Item\n  Embeddings", "comments": "CVPR Workshops Accepted Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore Latent Dirichlet Allocation (LDA) and Polylingual\nLatent Dirichlet Allocation (PolyLDA), as a means to discover trending styles\nin Overstock from deep visual semantic features transferred from a pretrained\nconvolutional neural network and text-based item attributes. To utilize deep\nvisual semantic features in conjunction with LDA, we develop a method for\ncreating a bag of words representation of unrolled image vectors. By viewing\nthe channels within the convolutional layers of a Resnet-50 as being\nrepresentative of a word, we can index these activations to create visual\ndocuments. We then train LDA over these documents to discover the latent style\nin the images. We also incorporate text-based data with PolyLDA, where each\nrepresentation is viewed as an independent language attempting to describe the\nsame style. The resulting topics are shown to be excellent indicators of visual\nstyle across our platform.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 20:07:28 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Iqbal", "Murium", ""], ["Kovac", "Adair", ""], ["Aryafar", "Kamelia", ""]]}, {"id": "1804.08757", "submitter": "Witold Oleszkiewicz", "authors": "Witold Oleszkiewicz, Peter Kairouz, Karol Piczak, Ram Rajagopal,\n  Tomasz Trzcinski", "title": "Siamese Generative Adversarial Privatizer for Biometric Data", "comments": "Paper accepted to ACCV 2018 (Asian Conference on Computer Vision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art machine learning algorithms can be fooled by carefully\ncrafted adversarial examples. As such, adversarial examples present a concrete\nproblem in AI safety. In this work we turn the tables and ask the following\nquestion: can we harness the power of adversarial examples to prevent malicious\nadversaries from learning identifying information from data while allowing\nnon-malicious entities to benefit from the utility of the same data? For\ninstance, can we use adversarial examples to anonymize biometric dataset of\nfaces while retaining usefulness of this data for other purposes, such as\nemotion recognition? To address this question, we propose a simple yet\neffective method, called Siamese Generative Adversarial Privatizer (SGAP), that\nexploits the properties of a Siamese neural network to find discriminative\nfeatures that convey identifying information. When coupled with a generative\nmodel, our approach is able to correctly locate and disguise identifying\ninformation, while minimally reducing the utility of the privatized dataset.\nExtensive evaluation on a biometric dataset of fingerprints and cartoon faces\nconfirms usefulness of our simple yet effective method.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 21:57:28 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 18:36:20 GMT"}, {"version": "v3", "created": "Mon, 8 Oct 2018 07:23:51 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Oleszkiewicz", "Witold", ""], ["Kairouz", "Peter", ""], ["Piczak", "Karol", ""], ["Rajagopal", "Ram", ""], ["Trzcinski", "Tomasz", ""]]}, {"id": "1804.08758", "submitter": "Sifei Liu", "authors": "Sifei Liu, Guangyu Zhong, Shalini De Mello, Jinwei Gu, Varun Jampani,\n  Ming-Hsuan Yang, Jan Kautz", "title": "Switchable Temporal Propagation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos contain highly redundant information between frames. Such redundancy\nhas been extensively studied in video compression and encoding, but is less\nexplored for more advanced video processing. In this paper, we propose a\nlearnable unified framework for propagating a variety of visual properties of\nvideo images, including but not limited to color, high dynamic range (HDR), and\nsegmentation information, where the properties are available for only a few\nkey-frames. Our approach is based on a temporal propagation network (TPN),\nwhich models the transition-related affinity between a pair of frames in a\npurely data-driven manner. We theoretically prove two essential factors for\nTPN: (a) by regularizing the global transformation matrix as orthogonal, the\n\"style energy\" of the property can be well preserved during propagation; (b)\nsuch regularization can be achieved by the proposed switchable TPN with\nbi-directional training on pairs of frames. We apply the switchable TPN to\nthree tasks: colorizing a gray-scale video based on a few color key-frames,\ngenerating an HDR video from a low dynamic range (LDR) video and a few HDR\nframes, and propagating a segmentation mask from the first frame in videos.\nExperimental results show that our approach is significantly more accurate and\nefficient than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 22:03:41 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 17:55:41 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Liu", "Sifei", ""], ["Zhong", "Guangyu", ""], ["De Mello", "Shalini", ""], ["Gu", "Jinwei", ""], ["Jampani", "Varun", ""], ["Yang", "Ming-Hsuan", ""], ["Kautz", "Jan", ""]]}, {"id": "1804.08790", "submitter": "Debayan Deb", "authors": "Debayan Deb, Susan Wiper, Alexandra Russo, Sixue Gong, Yichun Shi,\n  Cori Tymoszek, Anil Jain", "title": "Face Recognition: Primates in the Wild", "comments": "8 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method of primate face recognition, and evaluate this method\non several endangered primates, including golden monkeys, lemurs, and\nchimpanzees. The three datasets contain a total of 11,637 images of 280\nindividual primates from 14 species. Primate face recognition performance is\nevaluated using two existing state-of-the-art open-source systems, (i) FaceNet\nand (ii) SphereFace, (iii) a lemur face recognition system from literature, and\n(iv) our new convolutional neural network (CNN) architecture called PrimNet.\nThree recognition scenarios are considered: verification (1:1 comparison), and\nboth open-set and closed-set identification (1:N search). We demonstrate that\nPrimNet outperforms all of the other systems in all three scenarios for all\nprimate species tested. Finally, we implement an Android application of this\nrecognition system to assist primate researchers and conservationists in the\nwild for individual recognition of primates.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 00:19:32 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Deb", "Debayan", ""], ["Wiper", "Susan", ""], ["Russo", "Alexandra", ""], ["Gong", "Sixue", ""], ["Shi", "Yichun", ""], ["Tymoszek", "Cori", ""], ["Jain", "Anil", ""]]}, {"id": "1804.08831", "submitter": "Koushik Nagasubramanian", "authors": "Koushik Nagasubramanian (1), Sarah Jones (1), Asheesh K. Singh (1),\n  Arti Singh (1), Baskar Ganapathysubramanian (1), Soumik Sarkar (1) ((1) Iowa\n  State University)", "title": "Explaining hyperspectral imaging based plant disease identification: 3D\n  CNN and saliency maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our overarching goal is to develop an accurate and explainable model for\nplant disease identification using hyperspectral data. Charcoal rot is a soil\nborne fungal disease that affects the yield of soybean crops worldwide.\nHyperspectral images were captured at 240 different wavelengths in the range of\n383 - 1032 nm. We developed a 3D Convolutional Neural Network model for soybean\ncharcoal rot disease identification. Our model has classification accuracy of\n95.73\\% and an infected class F1 score of 0.87. We infer the trained model\nusing saliency map and visualize the most sensitive pixel locations that enable\nclassification. The sensitivity of individual wavelengths for classification\nwas also determined using the saliency map visualization. We identify the most\nsensitive wavelength as 733 nm using the saliency map visualization. Since the\nmost sensitive wavelength is in the Near Infrared Region(700 - 1000 nm) of the\nelectromagnetic spectrum, which is also the commonly used spectrum region for\ndetermining the vegetation health of the plant, we were more confident in the\npredictions using our model.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 03:39:36 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Nagasubramanian", "Koushik", ""], ["Jones", "Sarah", ""], ["Singh", "Asheesh K.", ""], ["Singh", "Arti", ""], ["Ganapathysubramanian", "Baskar", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1804.08835", "submitter": "Zixu Zhao", "authors": "Zixu Zhao", "title": "Matlab Implementation of Machine Vision Algorithm on Ballast Degradation\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  America has a massive railway system. As of 2006, U.S. freight railroads have\n140,490 route- miles of standard gauge, but maintaining such a huge system and\neliminating any dangers, like reduced track stability and poor drainage, caused\nby railway ballast degradation require huge amount of labor. The traditional\nway to quantify the degradation of ballast is to use an index called Fouling\nIndex (FI) through ballast sampling and sieve analysis. However, determining\nthe FI values in lab is very time-consuming and laborious, but with the help of\nrecent development in the field of computer vision, a novel method for a\npotential machine-vison based ballast inspection system can be employed that\ncan hopefully replace the traditional mechanical method. The new machine-vision\napproach analyses the images of the in-service ballasts, and then utilizes\nimage segmentation algorithm to get ballast segments. By comparing the segment\nresults and their corresponding FI values, this novel method produces a\nmachine-vision-based index that has the best-fit relation with FI. The\nimplementation details of how this algorithm works are discussed in this\nreport.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 04:16:46 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Zhao", "Zixu", ""]]}, {"id": "1804.08859", "submitter": "Joshua Owoyemi", "authors": "Joshua Owoyemi, Koichi Hashimoto", "title": "Spatiotemporal Learning of Dynamic Gestures from 3D Point Cloud Data", "comments": "Accepted to ICRA2018, 6 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate an end-to-end spatiotemporal gesture learning\napproach for 3D point cloud data using a new gestures dataset of point clouds\nacquired from a 3D sensor. Nine classes of gestures were learned from gestures\nsample data. We mapped point cloud data into dense occupancy grids, then time\nsteps of the occupancy grids are used as inputs into a 3D convolutional neural\nnetwork which learns the spatiotemporal features in the data without explicit\nmodeling of gesture dynamics. We also introduced a 3D region of interest\njittering approach for point cloud data augmentation. This resulted in an\nincreased classification accuracy of up to 10% when the augmented data is added\nto the original training data. The developed model is able to classify gestures\nfrom the dataset with 84.44% accuracy. We propose that point cloud data will be\na more viable data type for scene understanding and motion recognition, as 3D\nsensors become ubiquitous in years to come.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 06:48:56 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Owoyemi", "Joshua", ""], ["Hashimoto", "Koichi", ""]]}, {"id": "1804.08864", "submitter": "Patrick Follmann", "authors": "Patrick Follmann, Rebecca K\\\"onig, Philipp H\\\"artinger, Michael\n  Klostermann", "title": "Learning to See the Invisible: End-to-End Trainable Amodal Instance\n  Segmentation", "comments": "14 pages, plus appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Semantic amodal segmentation is a recently proposed extension to\ninstance-aware segmentation that includes the prediction of the invisible\nregion of each object instance. We present the first all-in-one end-to-end\ntrainable model for semantic amodal segmentation that predicts the amodal\ninstance masks as well as their visible and invisible part in a single forward\npass. In a detailed analysis, we provide experiments to show which architecture\nchoices are beneficial for an all-in-one amodal segmentation model. On the COCO\namodal dataset, our model outperforms the current baseline for amodal\nsegmentation by a large margin. To further evaluate our model, we provide two\nnew datasets with ground truth for semantic amodal segmentation, D2S amodal and\nCOCOA cls. For both datasets, our model provides a strong baseline performance.\nUsing special data augmentation techniques, we show that amodal segmentation on\nD2S amodal is possible with reasonable performance, even without providing\namodal training data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 06:55:44 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Follmann", "Patrick", ""], ["K\u00f6nig", "Rebecca", ""], ["H\u00e4rtinger", "Philipp", ""], ["Klostermann", "Michael", ""]]}, {"id": "1804.08866", "submitter": "Wangmeng Xiang", "authors": "Wangmeng Xiang, Jianqiang Huang, Xianbiao Qi, Xiansheng Hua, Lei Zhang", "title": "Homocentric Hypersphere Feature Embedding for Person Re-identification", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Person ReID) is a challenging task due to the large\nvariations in camera viewpoint, lighting, resolution, and human pose. Recently,\nwith the advancement of deep learning technologies, the performance of Person\nReID has been improved swiftly. Feature extraction and feature matching are two\ncrucial components in the training and deployment stages of Person ReID.\nHowever, many existing Person ReID methods have measure inconsistency between\nthe training stage and the deployment stage, and they couple magnitude and\norientation information of feature vectors in feature representation.\nMeanwhile, traditional triplet loss methods focus on samples within a\nmini-batch and lack knowledge of global feature distribution. To address these\nissues, we propose a novel homocentric hypersphere embedding scheme to decouple\nmagnitude and orientation information for both feature and weight vectors, and\nreformulate classification loss and triplet loss to their angular versions and\ncombine them into an angular discriminative loss. We evaluate our proposed\nmethod extensively on the widely used Person ReID benchmarks, including\nMarket1501, CUHK03 and DukeMTMC-ReID. Our method demonstrates leading\nperformance on all datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 07:09:58 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 01:49:49 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Xiang", "Wangmeng", ""], ["Huang", "Jianqiang", ""], ["Qi", "Xianbiao", ""], ["Hua", "Xiansheng", ""], ["Zhang", "Lei", ""]]}, {"id": "1804.08872", "submitter": "Marcus Nolte", "authors": "Marcus Nolte and Nikita Kister and Markus Maurer", "title": "Assessment of Deep Convolutional Neural Networks for Road Surface\n  Classification", "comments": null, "journal-ref": null, "doi": "10.1109/ITSC.2018.8569396", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When parameterizing vehicle control algorithms for stability or trajectory\ncontrol, the road-tire friction coefficient is an essential model parameter\nwhen it comes to control performance. One major impact on the friction\ncoefficient is the condition of the road surface. A camera-based,\nforward-looking classification of the road-surface helps enabling an early\nparametrization of vehicle control algorithms. In this paper, we train and\ncompare two different Deep Convolutional Neural Network models, regarding their\napplication for road friction estimation and describe the challenges for\ntraining the classifier in terms of available training data and the\nconstruction of suitable datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 07:20:45 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 10:32:41 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Nolte", "Marcus", ""], ["Kister", "Nikita", ""], ["Maurer", "Markus", ""]]}, {"id": "1804.08882", "submitter": "Ruoqi Sun", "authors": "Ruoqi Sun, Chen Huang, Jianping Shi, Lizhuang Ma", "title": "Mask-aware Photorealistic Face Attribute Manipulation", "comments": "7 pages, 4 figures. Computational Visual Media(2021)", "journal-ref": "Computer Visual Media(2021)", "doi": "10.1007/s41095-021-0219-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of face attribute manipulation has found increasing applications,\nbut still remains challenging with the requirement of editing the attributes of\na face image while preserving its unique details. In this paper, we choose to\ncombine the Variational AutoEncoder (VAE) and Generative Adversarial Network\n(GAN) for photorealistic image generation. We propose an effective method to\nmodify a modest amount of pixels in the feature maps of an encoder, changing\nthe attribute strength continuously without hindering global information. Our\ntraining objectives of VAE and GAN are reinforced by the supervision of face\nrecognition loss and cycle consistency loss for faithful preservation of face\ndetails. Moreover, we generate facial masks to enforce background consistency,\nwhich allows our training to focus on manipulating the foreground face rather\nthan background. Experimental results demonstrate our method, called\nMask-Adversarial AutoEncoder (M-AAE), can generate high-quality images with\nchanging attributes and outperforms prior methods in detail preservation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 08:03:11 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Sun", "Ruoqi", ""], ["Huang", "Chen", ""], ["Shi", "Jianping", ""], ["Ma", "Lizhuang", ""]]}, {"id": "1804.08890", "submitter": "Adina Ciomaga", "authors": "Bui Kevin, Fauman Jacob, Kes David, Torres Mandiola Leticia, Ciomaga\n  Adina, Salazar Ricardo, Bertozzi L. Andrea, Gilles Jerome, Guttentag I.\n  Andrew, Weiss S. Paul", "title": "Segmentation of Scanning Tunneling Microscopy Images Using Variational\n  Methods and Empirical Wavelets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the fields of nanoscience and nanotechnology, it is important to be able\nto functionalize surfaces chemically for a wide variety of applications.\nScanning tunneling microscopes (STMs) are important instruments in this area\nused to measure the surface structure and chemistry with better than molecular\nresolution. Self-assembly is frequently used to create monolayers that redefine\nthe surface chemistry in just a single-molecule-thick layer. Indeed, STM images\nreveal rich information about the structure of self-assembled monolayers since\nthey convey chemical and physical properties of the studied material.\n  In order to assist in and to enhance the analysis of STM and other images, we\npropose and demonstrate an image-processing framework that produces two image\nsegmentations: one is based on intensities (apparent heights in STM images) and\nthe other is based on textural patterns. The proposed framework begins with a\ncartoon+texture decomposition, which separates an image into its cartoon and\ntexture components. Afterward, the cartoon image is segmented by a modified\nmultiphase version of the local Chan-Vese model, while the texture image is\nsegmented by a combination of 2D empirical wavelet transform and a clustering\nalgorithm. Overall, our proposed framework contains several new features,\nspecifically in presenting a new application of cartoon+texture decomposition\nand of the empirical wavelet transforms and in developing a specialized\nframework to segment STM images and other data. To demonstrate the potential of\nour approach, we apply it to actual STM images of cyanide monolayers on\nAu\\{111\\} and present their corresponding segmentation results.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 08:16:59 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Kevin", "Bui", ""], ["Jacob", "Fauman", ""], ["David", "Kes", ""], ["Leticia", "Torres Mandiola", ""], ["Adina", "Ciomaga", ""], ["Ricardo", "Salazar", ""], ["Andrea", "Bertozzi L.", ""], ["Jerome", "Gilles", ""], ["Andrew", "Guttentag I.", ""], ["Paul", "Weiss S.", ""]]}, {"id": "1804.08912", "submitter": "Markus Ylim\\\"aki", "authors": "Markus Ylim\\\"aki, Juho Kannala, Janne Heikkil\\\"a", "title": "Accurate 3-D Reconstruction with RGB-D Cameras using Depth Map Fusion\n  and Pose Refinement", "comments": "Accepted to ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth map fusion is an essential part in both stereo and RGB-D based 3-D\nreconstruction pipelines. Whether produced with a passive stereo reconstruction\nor using an active depth sensor, such as Microsoft Kinect, the depth maps have\nnoise and may have poor initial registration. In this paper, we introduce a\nmethod which is capable of handling outliers, and especially, even significant\nregistration errors. The proposed method first fuses a sequence of depth maps\ninto a single non-redundant point cloud so that the redundant points are merged\ntogether by giving more weight to more certain measurements. Then, the original\ndepth maps are re-registered to the fused point cloud to refine the original\ncamera extrinsic parameters. The fusion is then performed again with the\nrefined extrinsic parameters. This procedure is repeated until the result is\nsatisfying or no significant changes happen between iterations. The method is\nrobust to outliers and erroneous depth measurements as well as even significant\ndepth map registration errors due to inaccurate initial camera poses.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 09:12:04 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Ylim\u00e4ki", "Markus", ""], ["Kannala", "Juho", ""], ["Heikkil\u00e4", "Janne", ""]]}, {"id": "1804.08944", "submitter": "Moritz Einfalt", "authors": "Rainer Lienhart, Moritz Einfalt, Dan Zecha", "title": "Mining Automatically Estimated Poses from Video Recordings of Top\n  Athletes", "comments": "Under review for the International Journal of Computer Science in\n  Sport", "journal-ref": null, "doi": "10.2478/ijcss-2018-0005", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose detection systems based on state-of-the-art DNNs are on the go to\nbe extended, adapted and re-trained to fit the application domain of specific\nsports. Therefore, plenty of noisy pose data will soon be available from videos\nrecorded at a regular and frequent basis. This work is among the first to\ndevelop mining algorithms that can mine the expected abundance of noisy and\nannotation-free pose data from video recordings in individual sports. Using\nswimming as an example of a sport with dominant cyclic motion, we show how to\ndetermine unsupervised time-continuous cycle speeds and temporally striking\nposes as well as measure unsupervised cycle stability over time. Additionally,\nwe use long jump as an example of a sport with a rigid phase-based motion to\npresent a technique to automatically partition the temporally estimated pose\nsequences into their respective phases. This enables the extraction of\nperformance relevant, pose-based metrics currently used by national\nprofessional sports associations. Experimental results prove the effectiveness\nof our mining algorithms, which can also be applied to other cycle-based or\nphase-based types of sport.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 10:30:12 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 12:43:27 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Lienhart", "Rainer", ""], ["Einfalt", "Moritz", ""], ["Zecha", "Dan", ""]]}, {"id": "1804.08965", "submitter": "Chong Sun", "authors": "Chong Sun, Dong Wang, Huchuan Lu, Ming-Hsuan Yang", "title": "Correlation Tracking via Joint Discrimination and Reliability Learning", "comments": "To appear in CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For visual tracking, an ideal filter learned by the correlation filter (CF)\nmethod should take both discrimination and reliability information. However,\nexisting attempts usually focus on the former one while pay less attention to\nreliability learning. This may make the learned filter be dominated by the\nunexpected salient regions on the feature map, thereby resulting in model\ndegradation. To address this issue, we propose a novel CF-based optimization\nproblem to jointly model the discrimination and reliability information. First,\nwe treat the filter as the element-wise product of a base filter and a\nreliability term. The base filter is aimed to learn the discrimination\ninformation between the target and backgrounds, and the reliability term\nencourages the final filter to focus on more reliable regions. Second, we\nintroduce a local response consistency regular term to emphasize equal\ncontributions of different regions and avoid the tracker being dominated by\nunreliable regions. The proposed optimization problem can be solved using the\nalternating direction method and speeded up in the Fourier domain. We conduct\nextensive experiments on the OTB-2013, OTB-2015 and VOT-2016 datasets to\nevaluate the proposed tracker. Experimental results show that our tracker\nperforms favorably against other state-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 11:38:22 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Sun", "Chong", ""], ["Wang", "Dong", ""], ["Lu", "Huchuan", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1804.08972", "submitter": "Tiziano Portenier", "authors": "Tiziano Portenier, Qiyang Hu, Attila Szab\\'o, Siavash Arjomand\n  Bigdeli, Paolo Favaro, Matthias Zwicker", "title": "FaceShop: Deep Sketch-based Face Image Editing", "comments": "13 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel system for sketch-based face image editing, enabling users\nto edit images intuitively by sketching a few strokes on a region of interest.\nOur interface features tools to express a desired image manipulation by\nproviding both geometry and color constraints as user-drawn strokes. As an\nalternative to the direct user input, our proposed system naturally supports a\ncopy-paste mode, which allows users to edit a given image region by using parts\nof another exemplar image without the need of hand-drawn sketching at all. The\nproposed interface runs in real-time and facilitates an interactive and\niterative workflow to quickly express the intended edits. Our system is based\non a novel sketch domain and a convolutional neural network trained end-to-end\nto automatically learn to render image regions corresponding to the input\nstrokes. To achieve high quality and semantically consistent results we train\nour neural network on two simultaneous tasks, namely image completion and image\ntranslation. To the best of our knowledge, we are the first to combine these\ntwo tasks in a unified framework for interactive image editing. Our results\nshow that the proposed sketch domain, network architecture, and training\nprocedure generalize well to real user input and enable high quality synthesis\nresults without additional post-processing.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 12:03:45 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 13:28:54 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Portenier", "Tiziano", ""], ["Hu", "Qiyang", ""], ["Szab\u00f3", "Attila", ""], ["Bigdeli", "Siavash Arjomand", ""], ["Favaro", "Paolo", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1804.08992", "submitter": "Hui Li", "authors": "Hui Li, Xiao-Jun Wu", "title": "Infrared and visible image fusion using Latent Low-Rank Representation", "comments": "6 pages, 8 figures, 1 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared and visible image fusion is an important problem in the field of\nimage fusion which has been applied widely in many fields. To better preserve\nthe useful information from source images, in this paper, we propose a novel\nimage fusion method based on latent low-rank representation(LatLRR) which is\nsimple and effective. Firstly, the source images are decomposed into low-rank\nparts(global structure) and saliency parts(local structure) by LatLRR. Then,\nthe lowrank parts are fused by weighted-average strategy to preserve more\ncontour information. Then, the saliency parts are simply fused by sum strategy\nwhich is a efficient operation in this fusion framework. Finally, the fused\nimage is obtained by combining the fused low-rank part and the fused saliency\npart. Compared with other fusion methods experimentally, the proposed method\nhas better fusion performance than stateof-the-art fusion methods in both\nsubjective and objective evaluation. The Code of our fusion method is available\nat https://github.com/hli1221/imagefusion Infrared visible latlrr\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 12:44:02 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 12:32:19 GMT"}, {"version": "v3", "created": "Tue, 18 Dec 2018 08:17:12 GMT"}, {"version": "v4", "created": "Fri, 9 Aug 2019 05:28:07 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Li", "Hui", ""], ["Wu", "Xiao-Jun", ""]]}, {"id": "1804.09003", "submitter": "Zhuoyao Zhong", "authors": "Zhuoyao Zhong, Lei Sun and Qiang Huo", "title": "An Anchor-Free Region Proposal Network for Faster R-CNN based Text\n  Detection Approaches", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The anchor mechanism of Faster R-CNN and SSD framework is considered not\neffective enough to scene text detection, which can be attributed to its IoU\nbased matching criterion between anchors and ground-truth boxes. In order to\nbetter enclose scene text instances of various shapes, it requires to design\nanchors of various scales, aspect ratios and even orientations manually, which\nmakes anchor-based methods sophisticated and inefficient. In this paper, we\npropose a novel anchor-free region proposal network (AF-RPN) to replace the\noriginal anchor-based RPN in the Faster R-CNN framework to address the above\nproblem. Compared with a vanilla RPN and FPN-RPN, AF-RPN can get rid of\ncomplicated anchor design and achieve higher recall rate on large-scale\nCOCO-Text dataset. Owing to the high-quality text proposals, our Faster R-CNN\nbased two-stage text detection approach achieves state-of-the-art results on\nICDAR-2017 MLT, ICDAR-2015 and ICDAR-2013 text detection benchmarks when using\nsingle-scale and single-model (ResNet50) testing only.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 13:08:32 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Zhong", "Zhuoyao", ""], ["Sun", "Lei", ""], ["Huo", "Qiang", ""]]}, {"id": "1804.09004", "submitter": "Roman Seidel", "authors": "Andr\\'e Apitzsch and Roman Seidel and Gangolf Hirtz", "title": "Cubes3D: Neural Network based Optical Flow in Omnidirectional Image\n  Scenes", "comments": "ICPRAI 2018", "journal-ref": "International Journal on Pattern Recognition and Artificial\n  Intelligence, Montreal, 2018, 164-169", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow estimation with convolutional neural networks (CNNs) has\nrecently solved various tasks of computer vision successfully. In this paper we\nadapt a state-of-the-art approach for optical flow estimation to\nomnidirectional images. We investigate CNN architectures to determine high\nmotion variations caused by the geometry of fish-eye images. Further we\ndetermine the qualitative influence of texture on the non-rigid object to the\nmotion vectors. For evaluation of the results we create ground truth motion\nfields synthetically. The ground truth contains cubes with static background.\nWe test variations of pre-trained FlowNet 2.0 architectures by indicating\ncommon error metrics. We generate competitive results for the motion of the\nforeground with inhomogeneous texture on the moving object.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 13:08:39 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 07:01:30 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Apitzsch", "Andr\u00e9", ""], ["Seidel", "Roman", ""], ["Hirtz", "Gangolf", ""]]}, {"id": "1804.09046", "submitter": "Felix M. Riese", "authors": "Sina Keller and Felix M. Riese and Johanna St\\\"otzer and Philipp M.\n  Maier and Stefan Hinz", "title": "Developing a machine learning framework for estimating soil moisture\n  with VNIR hyperspectral data", "comments": "Accepted at ISPRS TC I Midterm Symposium Karlsruhe (October 2018)", "journal-ref": "ISPRS Ann. Photogramm. Remote Sens. Spatial Inf. Sci., IV-1,\n  101-108, 2018", "doi": "10.5194/isprs-annals-IV-1-101-2018", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the potential of estimating the soil-moisture\ncontent based on VNIR hyperspectral data combined with LWIR data. Measurements\nfrom a multi-sensor field campaign represent the benchmark dataset which\ncontains measured hyperspectral, LWIR, and soil-moisture data conducted on\ngrassland site. We introduce a regression framework with three steps consisting\nof feature selection, preprocessing, and well-chosen regression models. The\nlatter are mainly supervised machine learning models. An exception are the\nself-organizing maps which combine unsupervised and supervised learning. We\nanalyze the impact of the distinct preprocessing methods on the regression\nresults. Of all regression models, the extremely randomized trees model without\npreprocessing provides the best estimation performance. Our results reveal the\npotential of the respective regression framework combined with the VNIR\nhyperspectral data to estimate soil moisture measured under real-world\nconditions. In conclusion, the results of this paper provide a basis for\nfurther improvements in different research directions.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 13:52:35 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 12:15:49 GMT"}, {"version": "v3", "created": "Wed, 11 Jul 2018 11:14:41 GMT"}, {"version": "v4", "created": "Thu, 12 Jul 2018 10:40:59 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Keller", "Sina", ""], ["Riese", "Felix M.", ""], ["St\u00f6tzer", "Johanna", ""], ["Maier", "Philipp M.", ""], ["Hinz", "Stefan", ""]]}, {"id": "1804.09066", "submitter": "Mohammadreza Zolfaghari", "authors": "Mohammadreza Zolfaghari, Kamaljeet Singh, Thomas Brox", "title": "ECO: Efficient Convolutional Network for Online Video Understanding", "comments": "Submitted to ECCV 2018. 17 pages, 7 figures, Supplementary Material,\n  https://github.com/mzolfaghari/ECO-efficient-video-understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state of the art in video understanding suffers from two problems: (1)\nThe major part of reasoning is performed locally in the video, therefore, it\nmisses important relationships within actions that span several seconds. (2)\nWhile there are local methods with fast per-frame processing, the processing of\nthe whole video is not efficient and hampers fast video retrieval or online\nclassification of long-term activities. In this paper, we introduce a network\narchitecture that takes long-term content into account and enables fast\nper-video processing at the same time. The architecture is based on merging\nlong-term content already in the network rather than in a post-hoc fusion.\nTogether with a sampling strategy, which exploits that neighboring frames are\nlargely redundant, this yields high-quality action classification and video\ncaptioning at up to 230 videos per second, where each video can consist of a\nfew hundred frames. The approach achieves competitive performance across all\ndatasets while being 10x to 80x faster than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 14:30:56 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 09:46:08 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Zolfaghari", "Mohammadreza", ""], ["Singh", "Kamaljeet", ""], ["Brox", "Thomas", ""]]}, {"id": "1804.09102", "submitter": "Matthew Sinclair Dr", "authors": "Matthew Sinclair and Christian F. Baumgartner and Jacqueline Matthew\n  and Wenjia Bai and Juan Cerrolaza Martinez and Yuanwei Li and Sandra Smith\n  and Caroline L. Knight and Bernhard Kainz and Jo Hajnal and Andrew P. King\n  and Daniel Rueckert", "title": "Human-level Performance On Automatic Head Biometrics In Fetal Ultrasound\n  Using Fully Convolutional Neural Networks", "comments": "EMBC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurement of head biometrics from fetal ultrasonography images is of key\nimportance in monitoring the healthy development of fetuses. However, the\naccurate measurement of relevant anatomical structures is subject to large\ninter-observer variability in the clinic. To address this issue, an automated\nmethod utilizing Fully Convolutional Networks (FCN) is proposed to determine\nmeasurements of fetal head circumference (HC) and biparietal diameter (BPD). An\nFCN was trained on approximately 2000 2D ultrasound images of the head with\nannotations provided by 45 different sonographers during routine screening\nexaminations to perform semantic segmentation of the head. An ellipse is fitted\nto the resulting segmentation contours to mimic the annotation typically\nproduced by a sonographer. The model's performance was compared with\ninter-observer variability, where two experts manually annotated 100 test\nimages. Mean absolute model-expert error was slightly better than\ninter-observer error for HC (1.99mm vs 2.16mm), and comparable for BPD (0.61mm\nvs 0.59mm), as well as Dice coefficient (0.980 vs 0.980). Our results\ndemonstrate that the model performs at a level similar to a human expert, and\nlearns to produce accurate predictions from a large dataset annotated by many\nsonographers. Additionally, measurements are generated in near real-time at\n15fps on a GPU, which could speed up clinical workflow for both skilled and\ntrainee sonographers.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 15:40:59 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Sinclair", "Matthew", ""], ["Baumgartner", "Christian F.", ""], ["Matthew", "Jacqueline", ""], ["Bai", "Wenjia", ""], ["Martinez", "Juan Cerrolaza", ""], ["Li", "Yuanwei", ""], ["Smith", "Sandra", ""], ["Knight", "Caroline L.", ""], ["Kainz", "Bernhard", ""], ["Hajnal", "Jo", ""], ["King", "Andrew P.", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1804.09111", "submitter": "Mehdi Hosseinzadeh", "authors": "Mehdi Hosseinzadeh, Yasir Latif, Trung Pham, Niko Suenderhauf, Ian\n  Reid", "title": "Structure Aware SLAM using Quadrics and Planes", "comments": "Accepted to ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous Localization And Mapping (SLAM) is a fundamental problem in\nmobile robotics. While point-based SLAM methods provide accurate camera\nlocalization, the generated maps lack semantic information. On the other hand,\nstate of the art object detection methods provide rich information about\nentities present in the scene from a single image. This work marries the two\nand proposes a method for representing generic objects as quadrics which allows\nobject detections to be seamlessly integrated in a SLAM framework. For scene\ncoverage, additional dominant planar structures are modeled as infinite planes.\nExperiments show that the proposed points-planes-quadrics representation can\neasily incorporate Manhattan and object affordance constraints, greatly\nimproving camera localization and leading to semantically meaningful maps. The\nperformance of our SLAM system is demonstrated in https://youtu.be/dR-rB9keF8M .\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 15:58:57 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 16:09:18 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2018 07:07:20 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Hosseinzadeh", "Mehdi", ""], ["Latif", "Yasir", ""], ["Pham", "Trung", ""], ["Suenderhauf", "Niko", ""], ["Reid", "Ian", ""]]}, {"id": "1804.09113", "submitter": "Benjamin Planche", "authors": "Sergey Zakharov, Benjamin Planche, Ziyan Wu, Andreas Hutter, Harald\n  Kosch, Slobodan Ilic", "title": "Keep it Unreal: Bridging the Realism Gap for 2.5D Recognition with\n  Geometry Priors Only", "comments": "10 pages + supplemetary material + references. The first two authors\n  contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing availability of large databases of 3D CAD models,\ndepth-based recognition methods can be trained on an uncountable number of\nsynthetically rendered images. However, discrepancies with the real data\nacquired from various depth sensors still noticeably impede progress. Previous\nworks adopted unsupervised approaches to generate more realistic depth data,\nbut they all require real scans for training, even if unlabeled. This still\nrepresents a strong requirement, especially when considering\nreal-life/industrial settings where real training images are hard or impossible\nto acquire, but texture-less 3D models are available. We thus propose a novel\napproach leveraging only CAD models to bridge the realism gap. Purely trained\non synthetic data, playing against an extensive augmentation pipeline in an\nunsupervised manner, our generative adversarial network learns to effectively\nsegment depth images and recover the clean synthetic-looking depth information\neven from partial occlusions. As our solution is not only fully decoupled from\nthe real domains but also from the task-specific analytics, the pre-processed\nscans can be handed to any kind and number of recognition methods also trained\non synthetic data. Through various experiments, we demonstrate how this\nsimplifies their training and consistently enhances their performance, with\nresults on par with the same methods trained on real data, and better than\nusual approaches doing the reverse mapping.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 16:02:59 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 16:08:07 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Zakharov", "Sergey", ""], ["Planche", "Benjamin", ""], ["Wu", "Ziyan", ""], ["Hutter", "Andreas", ""], ["Kosch", "Harald", ""], ["Ilic", "Slobodan", ""]]}, {"id": "1804.09160", "submitter": "Xin Wang", "authors": "Xin Wang, Wenhu Chen, Yuan-Fang Wang, William Yang Wang", "title": "No Metrics Are Perfect: Adversarial Reward Learning for Visual\n  Storytelling", "comments": "ACL 2018. 15 pages, 10 figures, 4 tables, with supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though impressive results have been achieved in visual captioning, the task\nof generating abstract stories from photo streams is still a little-tapped\nproblem. Different from captions, stories have more expressive language styles\nand contain many imaginary concepts that do not appear in the images. Thus it\nposes challenges to behavioral cloning algorithms. Furthermore, due to the\nlimitations of automatic metrics on evaluating story quality, reinforcement\nlearning methods with hand-crafted rewards also face difficulties in gaining an\noverall performance boost. Therefore, we propose an Adversarial REward Learning\n(AREL) framework to learn an implicit reward function from human\ndemonstrations, and then optimize policy search with the learned reward\nfunction. Though automatic eval- uation indicates slight performance boost over\nstate-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation\nshows that our approach achieves significant improvement in generating more\nhuman-like stories than SOTA systems.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 17:41:24 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 00:15:14 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Wang", "Xin", ""], ["Chen", "Wenhu", ""], ["Wang", "Yuan-Fang", ""], ["Wang", "William Yang", ""]]}, {"id": "1804.09194", "submitter": "Martin R\\\"unz", "authors": "Martin R\\\"unz, Maud Buffier, Lourdes Agapito", "title": "MaskFusion: Real-Time Recognition, Tracking and Reconstruction of\n  Multiple Moving Objects", "comments": "Presented at IEEE International Symposium on Mixed and Augmented\n  Reality (ISMAR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MaskFusion, a real-time, object-aware, semantic and dynamic RGB-D\nSLAM system that goes beyond traditional systems which output a purely\ngeometric map of a static scene. MaskFusion recognizes, segments and assigns\nsemantic class labels to different objects in the scene, while tracking and\nreconstructing them even when they move independently from the camera.\n  As an RGB-D camera scans a cluttered scene, image-based instance-level\nsemantic segmentation creates semantic object masks that enable real-time\nobject recognition and the creation of an object-level representation for the\nworld map. Unlike previous recognition-based SLAM systems, MaskFusion does not\nrequire known models of the objects it can recognize, and can deal with\nmultiple independent motions. MaskFusion takes full advantage of using\ninstance-level semantic segmentation to enable semantic labels to be fused into\nan object-aware map, unlike recent semantics enabled SLAM systems that perform\nvoxel-level semantic segmentation. We show augmented-reality applications that\ndemonstrate the unique features of the map output by MaskFusion:\ninstance-aware, semantic and dynamic.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 18:15:15 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 17:47:27 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["R\u00fcnz", "Martin", ""], ["Buffier", "Maud", ""], ["Agapito", "Lourdes", ""]]}, {"id": "1804.09205", "submitter": "Naveen Ashish", "authors": "Naveen Ashish and Mi-Youn Brusniak", "title": "Automated Mouse Organ Segmentation: A Deep Learning Based Solution", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of animal cross section images, such as cross sections of\nlaboratory mice, is critical in assessing the effect of experimental drugs such\nas the biodistribution of candidate compounds in preclinical drug development\nstage. Tissue distribution of radiolabeled candidate therapeutic compounds can\nbe quantified using techniques like Quantitative Whole-Body Autoradiography\n(QWBA).QWBA relies, among other aspects, on the accurate segmentation or\nidentification of key organs of interest in the animal cross section image such\nas the brain, spine, heart, liver and others. We present a deep learning based\norgan segmentation solution to this problem, using which we can achieve\nautomated organ segmentation with high precision (dice coefficient in the\n0.83-0.95 range depending on organ) for the key organs of interest.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 18:38:01 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 19:52:50 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Ashish", "Naveen", ""], ["Brusniak", "Mi-Youn", ""]]}, {"id": "1804.09235", "submitter": "Farzaneh Mahdisoltani", "authors": "Farzaneh Mahdisoltani, Guillaume Berger, Waseem Gharbieh, David Fleet,\n  Roland Memisevic", "title": "On the effectiveness of task granularity for transfer learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a DNN for video classification and captioning, trained\nend-to-end, with shared features, to solve tasks at different levels of\ngranularity, exploring the link between granularity in a source task and the\nquality of learned features for transfer learning. For solving the new task\ndomain in transfer learning, we freeze the trained encoder and fine-tune a\nneural net on the target domain. We train on the Something-Something dataset\nwith over 220, 000 videos, and multiple levels of target granularity, including\n50 action groups, 174 fine-grained action categories and captions.\nClassification and captioning with Something-Something are challenging because\nof the subtle differences between actions, applied to thousands of different\nobject classes, and the diversity of captions penned by crowd actors. Our model\nperforms better than existing classification baselines for SomethingSomething,\nwith impressive fine-grained results. And it yields a strong baseline on the\nnew Something-Something captioning task. Experiments reveal that training with\nmore fine-grained tasks tends to produce better features for transfer learning.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 20:06:55 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 03:59:19 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Mahdisoltani", "Farzaneh", ""], ["Berger", "Guillaume", ""], ["Gharbieh", "Waseem", ""], ["Fleet", "David", ""], ["Memisevic", "Roland", ""]]}, {"id": "1804.09236", "submitter": "Germain Haessig", "authors": "Germain Haessig, Ryad Benosman", "title": "A Sparse Coding Multi-Scale Precise-Timing Machine Learning Algorithm\n  for Neuromorphic Event-Based Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an unsupervised compact architecture that can extract\nfeatures and classify the contents of dynamic scenes from the temporal output\nof a neuromorphic asynchronous event-based camera. Event-based cameras are\nclock-less sensors where each pixel asynchronously reports intensity changes\nencoded in time at the microsecond precision. While this technology is gaining\nmore attention, there is still a lack of methodology and understanding of their\ntemporal properties. This paper introduces an unsupervised time-oriented\nevent-based machine learning algorithm building on the concept of hierarchy of\ntemporal descriptors called time surfaces. In this work we show that the use of\nsparse coding allows for a very compact yet efficient time-based machine\nlearning that lowers both the computational cost and memory need. We show that\nwe can represent visual scene temporal dynamics with a finite set of elementary\ntime surfaces while providing similar recognition rates as an uncompressed\nversion by storing the most representative time surfaces using clustering\ntechniques. Experiments will illustrate the main optimizations and trade-offs\nto consider when implementing the method for online continuous vs. offline\nlearning. We report results on the same previously published 36 class character\nrecognition task and a 4 class canonical dynamic card pip task, achieving 100%\naccuracy on each.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 20:10:18 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Haessig", "Germain", ""], ["Benosman", "Ryad", ""]]}, {"id": "1804.09279", "submitter": "Andre G Hochuli", "authors": "Andre G Hochuli, Luiz E S Oliveira, Alceu S Britto Jr, Robert Sabourin", "title": "Segmentation-Free Approaches for Handwritten Numeral String Recognition", "comments": "Paper accepted for publication on IJCNN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents segmentation-free strategies for the recognition of\nhandwritten numeral strings of unknown length. A synthetic dataset of touching\nnumeral strings of sizes 2-, 3- and 4-digits was created to train end-to-end\nsolutions based on Convolutional Neural Networks. A robust experimental\nprotocol is used to show that the proposed segmentation-free methods may reach\nthe state-of-the-art performance without suffering the heavy burden of\nover-segmentation based methods. In addition, they confirmed the importance of\nintroducing contextual information in the design of end-to-end solutions, such\nas the proposed length classifier when recognizing numeral strings.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 22:15:11 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 12:38:33 GMT"}, {"version": "v3", "created": "Sat, 28 Apr 2018 03:06:52 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Hochuli", "Andre G", ""], ["Oliveira", "Luiz E S", ""], ["Britto", "Alceu S", "Jr"], ["Sabourin", "Robert", ""]]}, {"id": "1804.09322", "submitter": "Hung Luu Viet", "authors": "Viet Hung Luu, Nguyen Hoang Hoa Luong, Quang Hung Bui, and Thi Nhat\n  Thanh Nguyen", "title": "Robust Anomaly-Based Ship Proposals Detection from Pan-sharpened\n  High-Resolution Satellite Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-screening of ship proposals is now employed by top ship detectors to\navoid exhaustive search across image. In very high resolution (VHR) optical\nimage, ships appeared as a cluster of abnormal bright pixels in open sea\nclutter (noise-like background). Anomaly-based detector utilizing Panchromatic\n(PAN) data has been widely used in many researches to detect ships, however,\nstill facing two main drawbacks: 1) detection rate tend to be low particularly\nwhen a ship is low contrast and 2) these models require a high manual\nconfiguration to select a threshold value best separate ships from sea surface\nbackground. This paper aims at further investigation of anomaly-based model to\nsolve those issues. First, pan-sharpened Multi Spectral (MS) data is\nincorporated together with PAN to enhance ship discrimination. Second, we\npropose an improved anomaly-based model combining both global intensity anomaly\nand local texture anomaly map. Regarding noise appeared due to the present of\nsea clutter and because of pan-sharpen process, texture abnormality suppression\nterm based on quantization theory is introduced. Experimental results on\nVNREDSat-1 VHR optical satellite images suggest that the pan-sharpened\nnear-infrared (P-NIR) band can improve discrimination of ships from surrounding\nwaters. Compared to state-of-the-art anomaly-based detectors, our proposed\nanomaly-based model on the combination of PAN and P-NIR data cannot only\nachieved highest ship detection's recall rate (91.14% and 45.9% on\nhigh-contrast and low-contrast dataset respectively) but also robust to\ndifferent automatic threshold selection techniques.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 02:19:50 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Luu", "Viet Hung", ""], ["Luong", "Nguyen Hoang Hoa", ""], ["Bui", "Quang Hung", ""], ["Nguyen", "Thi Nhat Thanh", ""]]}, {"id": "1804.09323", "submitter": "Shihan Cai", "authors": "Bo Du, Shihan Cai, Chen Wu, Liangpei Zhang, and Dacheng Tao", "title": "Object Tracking in Satellite Videos Based on a Multi-Frame Optical Flow\n  Tracker", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object tracking is a hot topic in computer vision. Thanks to the booming of\nthe very high resolution (VHR) remote sensing techniques, it is now possible to\ntrack targets of interests in satellite videos. However, since the targets in\nthe satellite videos are usually too small compared with the entire image, and\ntoo similar with the background, most state-of-the-art algorithms failed to\ntrack the target in satellite videos with a satisfactory accuracy. Due to the\nfact that optical flow shows the great potential to detect even the slight\nmovement of the targets, we proposed a multi-frame optical flow tracker (MOFT)\nfor object tracking in satellite videos. The Lucas-Kanade optical flow method\nwas fused with the HSV color system and integral image to track the targets in\nthe satellite videos, while multi-frame difference method was utilized in the\noptical flow tracker for a better interpretation. The experiments with three\nVHR remote sensing satellite video datasets indicate that compared with\nstate-of-the-art object tracking algorithms, the proposed method can track the\ntarget more accurately.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 02:27:30 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Du", "Bo", ""], ["Cai", "Shihan", ""], ["Wu", "Chen", ""], ["Zhang", "Liangpei", ""], ["Tao", "Dacheng", ""]]}, {"id": "1804.09325", "submitter": "Hui Li", "authors": "Hui Li, Xiao-Jun Wu, Tariq Durrani", "title": "Multi-focus Noisy Image Fusion using Low-Rank Representation", "comments": "11 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-focus noisy image fusion represents an important task in the field of\nimage fusion which generates a single, clear and focused image from all source\nimages. In this paper, we propose a novel multi-focus noisy image fusion method\nbased on low-rank representation (LRR) which is a powerful tool in\nrepresentation learning. A multi-scale transform framework is adopted in which\nsource images are decomposed into low frequency and high frequency\ncoefficients, respectively. For low frequency coefficients, the fused low\nfrequency coefficients are determined by a spatial frequency strategy, while\nthe high frequency coefficients are fused by the LRR-based fusion strategy.\nFinally, the fused image is reconstructed by inverse multi-scale transforms\nwith fused coefficients. Experimental results demonstrate that the proposed\nalgorithm offers state-of-the-art performance even when the source images\ncontain noise. The Code of our fusion method is available at\nhttps://github.com/hli1221/imagefusion_noisy_lrr\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 02:36:35 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 09:10:40 GMT"}, {"version": "v3", "created": "Sun, 7 Oct 2018 11:40:21 GMT"}, {"version": "v4", "created": "Tue, 9 Oct 2018 14:23:13 GMT"}, {"version": "v5", "created": "Tue, 18 Dec 2018 08:01:12 GMT"}, {"version": "v6", "created": "Wed, 6 Nov 2019 01:20:58 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Li", "Hui", ""], ["Wu", "Xiao-Jun", ""], ["Durrani", "Tariq", ""]]}, {"id": "1804.09337", "submitter": "Changqian Yu", "authors": "Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, Nong Sang", "title": "Learning a Discriminative Feature Network for Semantic Segmentation", "comments": "Accepted to CVPR 2018. 10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing methods of semantic segmentation still suffer from two aspects\nof challenges: intra-class inconsistency and inter-class indistinction. To\ntackle these two problems, we propose a Discriminative Feature Network (DFN),\nwhich contains two sub-networks: Smooth Network and Border Network.\nSpecifically, to handle the intra-class inconsistency problem, we specially\ndesign a Smooth Network with Channel Attention Block and global average pooling\nto select the more discriminative features. Furthermore, we propose a Border\nNetwork to make the bilateral features of boundary distinguishable with deep\nsemantic boundary supervision. Based on our proposed DFN, we achieve\nstate-of-the-art performance 86.2% mean IOU on PASCAL VOC 2012 and 80.3% mean\nIOU on Cityscapes dataset.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 03:49:30 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Yu", "Changqian", ""], ["Wang", "Jingbo", ""], ["Peng", "Chao", ""], ["Gao", "Changxin", ""], ["Yu", "Gang", ""], ["Sang", "Nong", ""]]}, {"id": "1804.09347", "submitter": "Yu-Jhe Li", "authors": "Yu-Jhe Li, Fu-En Yang, Yen-Cheng Liu, Yu-Ying Yeh, Xiaofei Du,\n  Yu-Chiang Frank Wang", "title": "Adaptation and Re-Identification Network: An Unsupervised Deep Transfer\n  Learning Approach to Person Re-Identification", "comments": "7 pages, 3 figures. CVPR 2018 workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) aims at recognizing the same person from\nimages taken across different cameras. To address this task, one typically\nrequires a large amount labeled data for training an effective Re-ID model,\nwhich might not be practical for real-world applications. To alleviate this\nlimitation, we choose to exploit a sufficient amount of pre-existing labeled\ndata from a different (auxiliary) dataset. By jointly considering such an\nauxiliary dataset and the dataset of interest (but without label information),\nour proposed adaptation and re-identification network (ARN) performs\nunsupervised domain adaptation, which leverages information across datasets and\nderives domain-invariant features for Re-ID purposes. In our experiments, we\nverify that our network performs favorably against state-of-the-art\nunsupervised Re-ID approaches, and even outperforms a number of baseline Re-ID\nmethods which require fully supervised data for training.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 04:54:34 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Li", "Yu-Jhe", ""], ["Yang", "Fu-En", ""], ["Liu", "Yen-Cheng", ""], ["Yeh", "Yu-Ying", ""], ["Du", "Xiaofei", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "1804.09364", "submitter": "Matthias M\\\"uller", "authors": "Matthias M\\\"uller, Alexey Dosovitskiy, Bernard Ghanem, Vladlen Koltun", "title": "Driving Policy Transfer via Modularity and Abstraction", "comments": "Accepted at Conference on Robotic Learning (CoRL'18)\n  http://proceedings.mlr.press/v87/mueller18a.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end approaches to autonomous driving have high sample complexity and\nare difficult to scale to realistic urban driving. Simulation can help\nend-to-end driving systems by providing a cheap, safe, and diverse training\nenvironment. Yet training driving policies in simulation brings up the problem\nof transferring such policies to the real world. We present an approach to\ntransferring driving policies from simulation to reality via modularity and\nabstraction. Our approach is inspired by classic driving systems and aims to\ncombine the benefits of modular architectures and end-to-end deep learning\napproaches. The key idea is to encapsulate the driving policy such that it is\nnot directly exposed to raw perceptual input or low-level vehicle dynamics. We\nevaluate the presented approach in simulated urban environments and in the real\nworld. In particular, we transfer a driving policy trained in simulation to a\n1/5-scale robotic truck that is deployed in a variety of conditions, with no\nfinetuning, on two continents. The supplementary video can be viewed at\nhttps://youtu.be/BrMDJqI6H5U\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 06:20:12 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 00:06:52 GMT"}, {"version": "v3", "created": "Thu, 13 Dec 2018 15:42:35 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["M\u00fcller", "Matthias", ""], ["Dosovitskiy", "Alexey", ""], ["Ghanem", "Bernard", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1804.09396", "submitter": "Jong Chul Ye", "authors": "Hyun-Seo Ahn, Sung-Hong Park and Jong Chul Ye", "title": "Quantitative Susceptibility Map Reconstruction Using Annihilating\n  Filter-based Low-Rank Hankel Matrix Approach", "comments": "accepted for Magnetic Resonance in Medicine", "journal-ref": null, "doi": "10.1002/mrm.27976", "report-no": "doi: 10.1002/mrm.27976", "categories": "cs.CV eess.SP physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative susceptibility mapping (QSM) inevitably suffers from streaking\nartifacts caused by zeros on the conical surface of the dipole kernel in\nk-space. This work proposes a novel and accurate QSM reconstruction method\nbased on a direct k-space interpolation approach, avoiding problems of over\nsmoothing and streaking artifacts. Inspired by the recent theory of\nannihilating filter-based low-rank Hankel matrix approach (ALOHA), QSM\nreconstruction problem is formulated as deconvolution problem under low-rank\nHankel matrix constraint in the k-space. To reduce the computational complexity\nand the memory requirement, the problem is formulated as successive\nreconstruction of 2-D planes along three independent axes of the 3-D phase\nimage in Fourier domain. Extensive experiments were performed to verify and\ncompare the proposed method with existing QSM reconstruction methods. The\nproposed ALOHA-QSM effectively reduced streaking artifacts and accurately\nestimated susceptibility values in deep gray matter structures, compared to the\nexisting QSM methods. Our suggested ALOHA-QSM algorithm successfully solves the\nthree-dimensional QSM dipole inversion problem without additional anatomical\ninformation or prior assumption and provides good image quality and\nquantitative accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 07:22:48 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 06:59:29 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Ahn", "Hyun-Seo", ""], ["Park", "Sung-Hong", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1804.09398", "submitter": "Zhe Wang", "authors": "Zhe Wang, Hongsheng Li, Wanli Ouyang, Xiaogang Wang", "title": "Learnable Histogram: Statistical Context Features for Deep Neural\n  Networks", "comments": "refined some typos, ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical features, such as histogram, Bag-of-Words (BoW) and Fisher\nVector, were commonly used with hand-crafted features in conventional\nclassification methods, but attract less attention since the popularity of deep\nlearning methods. In this paper, we propose a learnable histogram layer, which\nlearns histogram features within deep neural networks in end-to-end training.\nSuch a layer is able to back-propagate (BP) errors, learn optimal bin centers\nand bin widths, and be jointly optimized with other layers in deep networks\nduring training. Two vision problems, semantic segmentation and object\ndetection, are explored by integrating the learnable histogram layer into deep\nnetworks, which show that the proposed layer could be well generalized to\ndifferent applications. In-depth investigations are conducted to provide\ninsights on the newly introduced layer.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 07:30:47 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 03:06:20 GMT"}, {"version": "v3", "created": "Mon, 15 Oct 2018 03:00:53 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Wang", "Zhe", ""], ["Li", "Hongsheng", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1804.09400", "submitter": "Qiao Zheng", "authors": "Qiao Zheng, Herv\\'e Delingette, Nicolas Duchateau, Nicholas Ayache", "title": "3D Consistent & Robust Segmentation of Cardiac Images by Deep Learning\n  with Spatial Propagation", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method based on deep learning to perform cardiac segmentation on\nshort axis MRI image stacks iteratively from the top slice (around the base) to\nthe bottom slice (around the apex). At each iteration, a novel variant of U-net\nis applied to propagate the segmentation of a slice to the adjacent slice below\nit. In other words, the prediction of a segmentation of a slice is dependent\nupon the already existing segmentation of an adjacent slice. 3D-consistency is\nhence explicitly enforced. The method is trained on a large database of 3078\ncases from UK Biobank. It is then tested on 756 different cases from UK Biobank\nand three other state-of-the-art cohorts (ACDC with 100 cases, Sunnybrook with\n30 cases, RVSC with 16 cases). Results comparable or even better than the\nstate-of-the-art in terms of distance measures are achieved. They also\nemphasize the assets of our method, namely enhanced spatial consistency\n(currently neither considered nor achieved by the state-of-the-art), and the\ngeneralization ability to unseen cases even from other databases.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 07:39:36 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Zheng", "Qiao", ""], ["Delingette", "Herv\u00e9", ""], ["Duchateau", "Nicolas", ""], ["Ayache", "Nicholas", ""]]}, {"id": "1804.09404", "submitter": "Fumio Okura", "authors": "Takahiro Isokane, Fumio Okura, Ayaka Ide, Yasuyuki Matsushita, Yasushi\n  Yagi", "title": "Probabilistic Plant Modeling via Multi-View Image-to-Image Translation", "comments": "To appear in CVPR2018. The first two authors contributed equally.\n  Project website:\n  http://www.am.sanken.osaka-u.ac.jp/~okura/project/cvpr2018_plant.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for inferring three-dimensional (3D) plant\nbranch structures that are hidden under leaves from multi-view observations.\nUnlike previous geometric approaches that heavily rely on the visibility of the\nbranches or use parametric branching models, our method makes statistical\ninferences of branch structures in a probabilistic framework. By inferring the\nprobability of branch existence using a Bayesian extension of image-to-image\ntranslation applied to each of multi-view images, our method generates a\nprobabilistic plant 3D model, which represents the 3D branching pattern that\ncannot be directly observed. Experiments demonstrate the usefulness of the\nproposed approach in generating convincing branch structures in comparison to\nprior approaches.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 07:44:52 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Isokane", "Takahiro", ""], ["Okura", "Fumio", ""], ["Ide", "Ayaka", ""], ["Matsushita", "Yasuyuki", ""], ["Yagi", "Yasushi", ""]]}, {"id": "1804.09412", "submitter": "Bo Wang", "authors": "Bo Wang, Youjiang Xu, Yahong Han, Richang Hong", "title": "Movie Question Answering: Remembering the Textual Cues for Layered\n  Visual Contents", "comments": "Accepted by AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Movies provide us with a mass of visual content as well as attracting\nstories. Existing methods have illustrated that understanding movie stories\nthrough only visual content is still a hard problem. In this paper, for\nanswering questions about movies, we put forward a Layered Memory Network (LMN)\nthat represents frame-level and clip-level movie content by the Static Word\nMemory module and the Dynamic Subtitle Memory module, respectively.\nParticularly, we firstly extract words and sentences from the training movie\nsubtitles. Then the hierarchically formed movie representations, which are\nlearned from LMN, not only encode the correspondence between words and visual\ncontent inside frames, but also encode the temporal alignment between sentences\nand frames inside movie clips. We also extend our LMN model into three variant\nframeworks to illustrate the good extendable capabilities. We conduct extensive\nexperiments on the MovieQA dataset. With only visual content as inputs, LMN\nwith frame-level representation obtains a large performance improvement. When\nincorporating subtitles into LMN to form the clip-level representation, we\nachieve the state-of-the-art performance on the online evaluation task of\n'Video+Subtitles'. The good performance successfully demonstrates that the\nproposed framework of LMN is effective and the hierarchically formed movie\nrepresentations have good potential for the applications of movie question\nanswering.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 08:10:35 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Wang", "Bo", ""], ["Xu", "Youjiang", ""], ["Han", "Yahong", ""], ["Hong", "Richang", ""]]}, {"id": "1804.09428", "submitter": "Mohammadhassan Izadyyazdanabadi", "authors": "Mohammadhassan Izadyyazdanabadi, Evgenii Belykh, Claudio Cavallo,\n  Xiaochun Zhao, Sirin Gandhi, Leandro Borba Moreira, Jennifer Eschbacher,\n  Peter Nakaji, Mark C. Preul, and Yezhou Yang", "title": "Weakly-Supervised Learning-Based Feature Localization in Confocal Laser\n  Endomicroscopy Glioma Images", "comments": "Accepted in MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confocal Laser Endomicroscope (CLE) is a novel handheld fluorescence imaging\ndevice that has shown promise for rapid intraoperative diagnosis of brain tumor\ntissue. Currently CLE is capable of image display only and lacks an automatic\nsystem to aid the surgeon in analyzing the images. The goal of this project was\nto develop a computer-aided diagnostic approach for CLE imaging of human glioma\nwith feature localization function. Despite the tremendous progress in object\ndetection and image segmentation methods in recent years, most of such methods\nrequire large annotated datasets for training. However, manual annotation of\nthousands of histopathological images by physicians is costly and time\nconsuming. To overcome this problem, we propose a Weakly-Supervised Learning\n(WSL)-based model for feature localization that trains on image-level\nannotations, and then localizes incidences of a class-of-interest in the test\nimage. We developed a novel convolutional neural network for diagnostic\nfeatures localization from CLE images by employing a novel multiscale\nactivation map that is laterally inhibited and collaterally integrated. To\nvalidate our method, we compared proposed model's output to the manual\nannotation performed by four neurosurgeons on test images. Proposed model\nachieved 88% mean accuracy and 86% mean intersection over union on intermediate\nfeatures and 87% mean accuracy and 88% mean intersection over union on\nrestrictive fine features, while outperforming other state of the art methods\ntested. This system can improve accuracy and efficiency in characterization of\nCLE images of glioma tissue during surgery, augment intraoperative\ndecision-making process regarding tumor margin and affect resection rates.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 08:48:56 GMT"}, {"version": "v2", "created": "Sat, 18 Aug 2018 06:18:39 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Izadyyazdanabadi", "Mohammadhassan", ""], ["Belykh", "Evgenii", ""], ["Cavallo", "Claudio", ""], ["Zhao", "Xiaochun", ""], ["Gandhi", "Sirin", ""], ["Moreira", "Leandro Borba", ""], ["Eschbacher", "Jennifer", ""], ["Nakaji", "Peter", ""], ["Preul", "Mark C.", ""], ["Yang", "Yezhou", ""]]}, {"id": "1804.09458", "submitter": "Spyros Gidaris", "authors": "Spyros Gidaris and Nikos Komodakis", "title": "Dynamic Few-Shot Visual Learning without Forgetting", "comments": "Accepted at CVPR 2018. Code and models will be published on:\n  https://github.com/gidariss/FewShotWithoutForgetting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual system has the remarkably ability to be able to effortlessly\nlearn novel concepts from only a few examples. Mimicking the same behavior on\nmachine learning vision systems is an interesting and very challenging research\nproblem with many practical advantages on real world vision applications. In\nthis context, the goal of our work is to devise a few-shot visual learning\nsystem that during test time it will be able to efficiently learn novel\ncategories from only a few training data while at the same time it will not\nforget the initial categories on which it was trained (here called base\ncategories). To achieve that goal we propose (a) to extend an object\nrecognition system with an attention based few-shot classification weight\ngenerator, and (b) to redesign the classifier of a ConvNet model as the cosine\nsimilarity function between feature representations and classification weight\nvectors. The latter, apart from unifying the recognition of both novel and base\ncategories, it also leads to feature representations that generalize better on\n\"unseen\" categories. We extensively evaluate our approach on Mini-ImageNet\nwhere we manage to improve the prior state-of-the-art on few-shot recognition\n(i.e., we achieve 56.20% and 73.00% on the 1-shot and 5-shot settings\nrespectively) while at the same time we do not sacrifice any accuracy on the\nbase categories, which is a characteristic that most prior approaches lack.\nFinally, we apply our approach on the recently introduced few-shot benchmark of\nBharath and Girshick [4] where we also achieve state-of-the-art results. The\ncode and models of our paper will be published on:\nhttps://github.com/gidariss/FewShotWithoutForgetting\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 09:57:10 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Gidaris", "Spyros", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1804.09460", "submitter": "Pedro Miraldo", "authors": "Pedro Miraldo, Francisco Eiras, and Srikumar Ramalingam", "title": "Analytical Modeling of Vanishing Points and Curves in Catadioptric\n  Cameras", "comments": null, "journal-ref": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vanishing points and vanishing lines are classical geometrical concepts in\nperspective cameras that have a lineage dating back to 3 centuries. A vanishing\npoint is a point on the image plane where parallel lines in 3D space appear to\nconverge, whereas a vanishing line passes through 2 or more vanishing points.\nWhile such concepts are simple and intuitive in perspective cameras, their\ncounterparts in catadioptric cameras (obtained using mirrors and lenses) are\nmore involved. For example, lines in the 3D space map to higher degree curves\nin catadioptric cameras. The projection of a set of 3D parallel lines converges\non a single point in perspective images, whereas they converge to more than one\npoint in catadioptric cameras. To the best of our knowledge, we are not aware\nof any systematic development of analytical models for vanishing points and\nvanishing curves in different types of catadioptric cameras. In this paper, we\nderive parametric equations for vanishing points and vanishing curves using the\ncalibration parameters, mirror shape coefficients, and direction vectors of\nparallel lines in 3D space. We show compelling experimental results on\nvanishing point estimation and absolute pose estimation for a wide range of\ncatadioptric cameras in both simulations and real experiments.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 09:59:34 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Miraldo", "Pedro", ""], ["Eiras", "Francisco", ""], ["Ramalingam", "Srikumar", ""]]}, {"id": "1804.09466", "submitter": "Xiaopeng Zhang", "authors": "Xiaopeng Zhang, Jiashi Feng, Hongkai Xiong, Qi Tian", "title": "Zigzag Learning for Weakly Supervised Object Detection", "comments": "accepted by CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses weakly supervised object detection with only image-level\nsupervision at training stage. Previous approaches train detection models with\nentire images all at once, making the models prone to being trapped in\nsub-optimums due to the introduced false positive examples. Unlike them, we\npropose a zigzag learning strategy to simultaneously discover reliable object\ninstances and prevent the model from overfitting initial seeds. Towards this\ngoal, we first develop a criterion named mean Energy Accumulation Scores (mEAS)\nto automatically measure and rank localization difficulty of an image\ncontaining the target object, and accordingly learn the detector progressively\nby feeding examples with increasing difficulty. In this way, the model can be\nwell prepared by training on easy examples for learning from more difficult\nones and thus gain a stronger detection ability more efficiently. Furthermore,\nwe introduce a novel masking regularization strategy over the high level\nconvolutional feature maps to avoid overfitting initial samples. These two\nmodules formulate a zigzag learning process, where progressive learning\nendeavors to discover reliable object instances, and masking regularization\nincreases the difficulty of finding object instances properly. We achieve 47.6%\nmAP on PASCAL VOC 2007, surpassing the state-of-the-arts by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 10:26:57 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Zhang", "Xiaopeng", ""], ["Feng", "Jiashi", ""], ["Xiong", "Hongkai", ""], ["Tian", "Qi", ""]]}, {"id": "1804.09493", "submitter": "Jiayuan Li Dr", "authors": "Jiayuan Li, Qingwu Hu, Mingyao Ai", "title": "RIFT: Multi-modal Image Matching Based on Radiation-invariant Feature\n  Transform", "comments": "14 pages,17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional feature matching methods such as scale-invariant feature\ntransform (SIFT) usually use image intensity or gradient information to detect\nand describe feature points; however, both intensity and gradient are sensitive\nto nonlinear radiation distortions (NRD). To solve the problem, this paper\nproposes a novel feature matching algorithm that is robust to large NRD. The\nproposed method is called radiation-invariant feature transform (RIFT). There\nare three main contributions in RIFT: first, RIFT uses phase congruency (PC)\ninstead of image intensity for feature point detection. RIFT considers both the\nnumber and repeatability of feature points, and detects both corner points and\nedge points on the PC map. Second, RIFT originally proposes a maximum index map\n(MIM) for feature description. MIM is constructed from the log-Gabor\nconvolution sequence and is much more robust to NRD than traditional gradient\nmap. Thus, RIFT not only largely improves the stability of feature detection,\nbut also overcomes the limitation of gradient information for feature\ndescription. Third, RIFT analyzes the inherent influence of rotations on the\nvalues of MIM, and realizes rotation invariance. We use six different types of\nmulti-model image datasets to evaluate RIFT, including optical-optical,\ninfrared-optical, synthetic aperture radar (SAR)-optical, depth-optical,\nmap-optical, and day-night datasets. Experimental results show that RIFT is\nmuch more superior to SIFT and SAR-SIFT. To the best of our knowledge, RIFT is\nthe first feature matching algorithm that can achieve good performance on all\nthe above-mentioned types of multi-model images. The source code of RIFT and\nmulti-modal remote sensing image datasets are made public .\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 11:58:27 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Li", "Jiayuan", ""], ["Hu", "Qingwu", ""], ["Ai", "Mingyao", ""]]}, {"id": "1804.09534", "submitter": "Umar Iqbal", "authors": "Umar Iqbal, Pavlo Molchanov, Thomas Breuel, Juergen Gall, Jan Kautz", "title": "Hand Pose Estimation via Latent 2.5D Heatmap Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the 3D pose of a hand is an essential part of human-computer\ninteraction. Estimating 3D pose using depth or multi-view sensors has become\neasier with recent advances in computer vision, however, regressing pose from a\nsingle RGB image is much less straightforward. The main difficulty arises from\nthe fact that 3D pose requires some form of depth estimates, which are\nambiguous given only an RGB image. In this paper we propose a new method for 3D\nhand pose estimation from a monocular image through a novel 2.5D pose\nrepresentation. Our new representation estimates pose up to a scaling factor,\nwhich can be estimated additionally if a prior of the hand size is given. We\nimplicitly learn depth maps and heatmap distributions with a novel CNN\narchitecture. Our system achieves the state-of-the-art estimation of 2D and 3D\nhand pose on several challenging datasets in presence of severe occlusions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 13:16:26 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Iqbal", "Umar", ""], ["Molchanov", "Pavlo", ""], ["Breuel", "Thomas", ""], ["Gall", "Juergen", ""], ["Kautz", "Jan", ""]]}, {"id": "1804.09535", "submitter": "Zhengxue Cheng", "authors": "Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto", "title": "Deep Convolutional AutoEncoder-based Lossy Image Compression", "comments": "accepted by Picture Coding Symposium 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image compression has been investigated as a fundamental research topic for\nmany decades. Recently, deep learning has achieved great success in many\ncomputer vision tasks, and is gradually being used in image compression. In\nthis paper, we present a lossy image compression architecture, which utilizes\nthe advantages of convolutional autoencoder (CAE) to achieve a high coding\nefficiency. First, we design a novel CAE architecture to replace the\nconventional transforms and train this CAE using a rate-distortion loss\nfunction. Second, to generate a more energy-compact representation, we utilize\nthe principal components analysis (PCA) to rotate the feature maps produced by\nthe CAE, and then apply the quantization and entropy coder to generate the\ncodes. Experimental results demonstrate that our method outperforms traditional\nimage coding algorithms, by achieving a 13.7% BD-rate decrement on the Kodak\ndatabase images compared to JPEG2000. Besides, our method maintains a moderate\ncomplexity similar to JPEG2000.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 13:19:30 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Cheng", "Zhengxue", ""], ["Sun", "Heming", ""], ["Takeuchi", "Masaru", ""], ["Katto", "Jiro", ""]]}, {"id": "1804.09548", "submitter": "Jane Hung", "authors": "Jane Hung, Deepali Ravel, Stefanie C.P. Lopes, Gabriel Rangel,\n  Odailton Amaral Nery, Benoit Malleret, Francois Nosten, Marcus V. G. Lacerda,\n  Marcelo U. Ferreira, Laurent R\\'enia, Manoj T. Duraisingh, Fabio T. M. Costa,\n  Matthias Marti, Anne E. Carpenter", "title": "Applying Faster R-CNN for Object Detection on Malaria Images", "comments": "CVPR 2017: computer vision for microscopy image analysis (CVMI)\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based models have had great success in object detection, but\nthe state of the art models have not yet been widely applied to biological\nimage data. We apply for the first time an object detection model previously\nused on natural images to identify cells and recognize their stages in\nbrightfield microscopy images of malaria-infected blood. Many micro-organisms\nlike malaria parasites are still studied by expert manual inspection and hand\ncounting. This type of object detection task is challenging due to factors like\nvariations in cell shape, density, and color, and uncertainty of some cell\nclasses. In addition, annotated data useful for training is scarce, and the\nclass distribution is inherently highly imbalanced due to the dominance of\nuninfected red blood cells. We use Faster Region-based Convolutional Neural\nNetwork (Faster R-CNN), one of the top performing object detection models in\nrecent years, pre-trained on ImageNet but fine tuned with our data, and compare\nit to a baseline, which is based on a traditional approach consisting of cell\nsegmentation, extraction of several single-cell features, and classification\nusing random forests. To conduct our initial study, we collect and label a\ndataset of 1300 fields of view consisting of around 100,000 individual cells.\nWe demonstrate that Faster R-CNN outperforms our baseline and put the results\nin context of human performance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 13:30:39 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 16:14:46 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Hung", "Jane", ""], ["Ravel", "Deepali", ""], ["Lopes", "Stefanie C. P.", ""], ["Rangel", "Gabriel", ""], ["Nery", "Odailton Amaral", ""], ["Malleret", "Benoit", ""], ["Nosten", "Francois", ""], ["Lacerda", "Marcus V. G.", ""], ["Ferreira", "Marcelo U.", ""], ["R\u00e9nia", "Laurent", ""], ["Duraisingh", "Manoj T.", ""], ["Costa", "Fabio T. M.", ""], ["Marti", "Matthias", ""], ["Carpenter", "Anne E.", ""]]}, {"id": "1804.09555", "submitter": "Jie Chen", "authors": "Jie Chen, Cheen-Hau Tan, Junhui Hou, Lap-Pui Chau, and He Li", "title": "Robust Video Content Alignment and Compensation for Clear Vision Through\n  the Rain", "comments": "arXiv admin note: text overlap with arXiv:1803.10433", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outdoor vision-based systems suffer from atmospheric turbulences, and rain is\none of the worst factors for vision degradation. Current rain removal methods\nshow limitations either for complex dynamic scenes, or under torrential rain\nwith opaque occlusions. We propose a novel derain framework which applies\nsuperpixel (SP) segmentation to decompose the scene into depth consistent\nunits. Alignment of scene contents are done at the SP level, which proves to be\nrobust against rain occlusion interferences and fast camera motion. Two\nalignment output tensors, i.e., optimal temporal match tensor and sorted\nspatial-temporal match tensor, provide informative clues for the location of\nrain streaks and the occluded background contents. Different classical and\nnovel methods such as Robust Principle Component Analysis and Convolutional\nNeural Networks are applied and compared for their respective advantages in\nefficiently exploiting the rich spatial-temporal features provided by the two\ntensors. Extensive evaluations show that advantage of up to 5dB is achieved on\nthe scene restoration PSNR over state-of-the-art methods, and the advantage is\nespecially obvious with highly complex and dynamic scenes. Visual evaluations\nshow that the proposed framework is not only able to suppress heavy and opaque\noccluding rain streaks, but also large semi-transparent regional fluctuations\nand distortions.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 14:50:34 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Chen", "Jie", ""], ["Tan", "Cheen-Hau", ""], ["Hou", "Junhui", ""], ["Chau", "Lap-Pui", ""], ["Li", "He", ""]]}, {"id": "1804.09557", "submitter": "Renaud Dub\\'e", "authors": "Renaud Dub\\'e, Andrei Cramariuc, Daniel Dugas, Juan Nieto, Roland\n  Siegwart, and Cesar Cadena", "title": "SegMap: 3D Segment Mapping using Data-Driven Descriptors", "comments": null, "journal-ref": null, "doi": "10.15607/RSS.2018.XIV.003", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When performing localization and mapping, working at the level of structure\ncan be advantageous in terms of robustness to environmental changes and\ndifferences in illumination. This paper presents SegMap: a map representation\nsolution to the localization and mapping problem based on the extraction of\nsegments in 3D point clouds. In addition to facilitating the computationally\nintensive task of processing 3D point clouds, working at the level of segments\naddresses the data compression requirements of real-time single- and\nmulti-robot systems. While current methods extract descriptors for the single\ntask of localization, SegMap leverages a data-driven descriptor in order to\nextract meaningful features that can also be used for reconstructing a dense 3D\nmap of the environment and for extracting semantic information. This is\nparticularly interesting for navigation tasks and for providing visual feedback\nto end-users such as robot operators, for example in search and rescue\nscenarios. These capabilities are demonstrated in multiple urban driving and\nsearch and rescue experiments. Our method leads to an increase of area under\nthe ROC curve of 28.3% over current state of the art using eigenvalue based\nfeatures. We also obtain very similar reconstruction capabilities to a model\nspecifically trained for this task. The SegMap implementation will be made\navailable open-source along with easy to run demonstrations at\nwww.github.com/ethz-asl/segmap. A video demonstration is available at\nhttps://youtu.be/CMk4w4eRobg.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 13:41:55 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 11:03:34 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Dub\u00e9", "Renaud", ""], ["Cramariuc", "Andrei", ""], ["Dugas", "Daniel", ""], ["Nieto", "Juan", ""], ["Siegwart", "Roland", ""], ["Cadena", "Cesar", ""]]}, {"id": "1804.09578", "submitter": "Guanyu Cai", "authors": "Guanyu Cai, Yuqin Wang, Mengchu Zhou, Lianghua He", "title": "Unsupervised Domain Adaptation with Adversarial Residual Transform\n  Networks", "comments": "accepted by IEEE Transactions on Neural Networks and Learning Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2019.2935384", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is widely used in learning problems lacking labels. Recent\nstudies show that deep adversarial domain adaptation models can make markable\nimprovements in performance, which include symmetric and asymmetric\narchitectures. However, the former has poor generalization ability whereas the\nlatter is very hard to train. In this paper, we propose a novel adversarial\ndomain adaptation method named Adversarial Residual Transform Networks (ARTNs)\nto improve the generalization ability, which directly transforms the source\nfeatures into the space of target features. In this model, residual connections\nare used to share features and adversarial loss is reconstructed, thus making\nthe model more generalized and easier to train. Moreover, a special\nregularization term is added to the loss function to alleviate a vanishing\ngradient problem, which enables its training process stable. A series of\nexperiments based on Amazon review dataset, digits datasets and Office-31 image\ndatasets are conducted to show that the proposed ARTN can be comparable with\nthe methods of the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 14:02:25 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 11:29:01 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Cai", "Guanyu", ""], ["Wang", "Yuqin", ""], ["Zhou", "Mengchu", ""], ["He", "Lianghua", ""]]}, {"id": "1804.09626", "submitter": "Gunnar Sigurdsson", "authors": "Gunnar A. Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi,\n  Karteek Alahari", "title": "Charades-Ego: A Large-Scale Dataset of Paired Third and First Person\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Actor and Observer we introduced a dataset linking the first and\nthird-person video understanding domains, the Charades-Ego Dataset. In this\npaper we describe the egocentric aspect of the dataset and present annotations\nfor Charades-Ego with 68,536 activity instances in 68.8 hours of first and\nthird-person video, making it one of the largest and most diverse egocentric\ndatasets available. Charades-Ego furthermore shares activity classes, scripts,\nand methodology with the Charades dataset, that consist of additional 82.3\nhours of third-person video with 66,500 activity instances. Charades-Ego has\ntemporal annotations and textual descriptions, making it suitable for\negocentric video classification, localization, captioning, and new tasks\nutilizing the cross-modal nature of the data.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 15:30:27 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 16:57:00 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Sigurdsson", "Gunnar A.", ""], ["Gupta", "Abhinav", ""], ["Schmid", "Cordelia", ""], ["Farhadi", "Ali", ""], ["Alahari", "Karteek", ""]]}, {"id": "1804.09627", "submitter": "Gunnar Sigurdsson", "authors": "Gunnar A. Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi,\n  Karteek Alahari", "title": "Actor and Observer: Joint Modeling of First and Third-Person Videos", "comments": "CVPR 2018 spotlight presentation", "journal-ref": "CVPR 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several theories in cognitive neuroscience suggest that when people interact\nwith the world, or simulate interactions, they do so from a first-person\negocentric perspective, and seamlessly transfer knowledge between third-person\n(observer) and first-person (actor). Despite this, learning such models for\nhuman action recognition has not been achievable due to the lack of data. This\npaper takes a step in this direction, with the introduction of Charades-Ego, a\nlarge-scale dataset of paired first-person and third-person videos, involving\n112 people, with 4000 paired videos. This enables learning the link between the\ntwo, actor and observer perspectives. Thereby, we address one of the biggest\nbottlenecks facing egocentric vision research, providing a link from\nfirst-person to the abundant third-person data on the web. We use this data to\nlearn a joint representation of first and third-person videos, with only weak\nsupervision, and show its effectiveness for transferring knowledge from the\nthird-person to the first-person domain.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 15:30:34 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Sigurdsson", "Gunnar A.", ""], ["Gupta", "Abhinav", ""], ["Schmid", "Cordelia", ""], ["Farhadi", "Ali", ""], ["Alahari", "Karteek", ""]]}, {"id": "1804.09650", "submitter": "Dinh-Luan Nguyen", "authors": "Dinh-Luan Nguyen, Kai Cao and Anil K. Jain", "title": "Automatic Latent Fingerprint Segmentation", "comments": "Accepted (Oral) in BTAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple but effective method for automatic latent fingerprint\nsegmentation, called SegFinNet. SegFinNet takes a latent image as an input and\noutputs a binary mask highlighting the friction ridge pattern. Our algorithm\ncombines fully convolutional neural network and detection-based approaches to\nprocess the entire input latent image in one shot instead of using latent\npatches. Experimental results on three different latent databases (i.e. NIST\nSD27, WVU, and an operational forensic database) show that SegFinNet\noutperforms both human markup for latents and the state-of-the-art latent\nsegmentation algorithms. We further show that this improved cropping boosts the\nhit rate of a latent fingerprint matcher.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 16:09:02 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 02:20:28 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Nguyen", "Dinh-Luan", ""], ["Cao", "Kai", ""], ["Jain", "Anil K.", ""]]}, {"id": "1804.09669", "submitter": "Skand Vishwanath Peri", "authors": "Skand Vishwanath Peri, Abhinav Dhall", "title": "DisguiseNet : A Contrastive Approach for Disguised Face Verification in\n  the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our approach for the Disguised Faces in the Wild (DFW)\n2018 challenge. The task here is to verify the identity of a person among\ndisguised and impostors images. Given the importance of the task of face\nverification it is essential to compare methods across a common platform. Our\napproach is based on VGG-face architecture paired with Contrastive loss based\non cosine distance metric. For augmenting the data set, we source more data\nfrom the internet. The experiments show the effectiveness of the approach on\nthe DFW data. We show that adding extra data to the DFW dataset with noisy\nlabels also helps in increasing the generalization performance of the network.\nThe proposed network achieves 27.13% absolute increase in accuracy over the DFW\nbaseline.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 16:32:07 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 04:24:21 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Peri", "Skand Vishwanath", ""], ["Dhall", "Abhinav", ""]]}, {"id": "1804.09690", "submitter": "Tewodros Habtegebrial", "authors": "Tewodros Habtegebrial, Kiran Varanasi, Christian Bailer, Didier\n  Stricker", "title": "Fast View Synthesis with Deep Stereo Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Novel view synthesis is an important problem in computer vision and graphics.\nOver the years a large number of solutions have been put forward to solve the\nproblem. However, the large-baseline novel view synthesis problem is far from\nbeing \"solved\". Recent works have attempted to use Convolutional Neural\nNetworks (CNNs) to solve view synthesis tasks. Due to the difficulty of\nlearning scene geometry and interpreting camera motion, CNNs are often unable\nto generate realistic novel views. In this paper, we present a novel view\nsynthesis approach based on stereo-vision and CNNs that decomposes the problem\ninto two sub-tasks: view dependent geometry estimation and texture inpainting.\nBoth tasks are structured prediction problems that could be effectively learned\nwith CNNs. Experiments on the KITTI Odometry dataset show that our approach is\nmore accurate and significantly faster than the current state-of-the-art. The\ncode and supplementary material will be publicly available. Results could be\nfound here https://youtu.be/5pzS9jc-5t0\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 17:35:50 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 11:54:58 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Habtegebrial", "Tewodros", ""], ["Varanasi", "Kiran", ""], ["Bailer", "Christian", ""], ["Stricker", "Didier", ""]]}, {"id": "1804.09691", "submitter": "Zhiyi Cheng", "authors": "Zhiyi Cheng and Xiatian Zhu and Shaogang Gong", "title": "Surveillance Face Recognition Challenge", "comments": "The QMUL-SurvFace challenge is publicly available at\n  https://qmul-survface.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition (FR) is one of the most extensively investigated problems in\ncomputer vision. Significant progress in FR has been made due to the recent\nintroduction of the larger scale FR challenges, particularly with constrained\nsocial media web images, e.g. high-resolution photos of celebrity faces taken\nby professional photo-journalists. However, the more challenging FR in\nunconstrained and low-resolution surveillance images remains largely\nunder-studied. To facilitate more studies on developing FR models that are\neffective and robust for low-resolution surveillance facial images, we\nintroduce a new Surveillance Face Recognition Challenge, which we call the\nQMUL-SurvFace benchmark. This new benchmark is the largest and more importantly\nthe only true surveillance FR benchmark to our best knowledge, where\nlow-resolution images are not synthesised by artificial down-sampling of native\nhigh-resolution images. This challenge contains 463,507 face images of 15,573\ndistinct identities captured in real-world uncooperative surveillance scenes\nover wide space and time. As a consequence, it presents an extremely\nchallenging FR benchmark. We benchmark the FR performance on this challenge\nusing five representative deep learning face recognition models, in comparison\nto existing benchmarks. We show that the current state of the arts are still\nfar from being satisfactory to tackle the under-investigated surveillance FR\nproblem in practical forensic scenarios. Face recognition is generally more\ndifficult in an open-set setting which is typical for surveillance scenarios,\nowing to a large number of non-target people (distractors) appearing open\nspaced scenes. This is evidently so that on the new Surveillance FR Challenge,\nthe top-performing CentreFace deep learning FR model on the MegaFace benchmark\ncan now only achieve 13.2% success rate (at Rank-20) at a 10% false alarm rate.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 17:36:06 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 16:49:22 GMT"}, {"version": "v3", "created": "Sat, 2 Jun 2018 14:54:50 GMT"}, {"version": "v4", "created": "Tue, 5 Jun 2018 15:22:18 GMT"}, {"version": "v5", "created": "Wed, 6 Jun 2018 14:32:40 GMT"}, {"version": "v6", "created": "Wed, 29 Aug 2018 15:51:31 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Cheng", "Zhiyi", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1804.09699", "submitter": "Huan Zhang", "authors": "Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh,\n  Duane Boning, Inderjit S. Dhillon, Luca Daniel", "title": "Towards Fast Computation of Certified Robustness for ReLU Networks", "comments": "Tsui-Wei Weng and Huan Zhang contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying the robustness property of a general Rectified Linear Unit (ReLU)\nnetwork is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer\nCAV17]. Although finding the exact minimum adversarial distortion is hard,\ngiving a certified lower bound of the minimum distortion is possible. Current\navailable methods of computing such a bound are either time-consuming or\ndelivering low quality bounds that are too loose to be useful. In this paper,\nwe exploit the special structure of ReLU networks and provide two\ncomputationally efficient algorithms Fast-Lin and Fast-Lip that are able to\ncertify non-trivial lower bounds of minimum distortions, by bounding the ReLU\nunits with appropriate linear functions Fast-Lin, or by bounding the local\nLipschitz constant Fast-Lip. Experiments show that (1) our proposed methods\ndeliver bounds close to (the gap is 2-3X) exact minimum distortion found by\nReluplex in small MNIST networks while our algorithms are more than 10,000\ntimes faster; (2) our methods deliver similar quality of bounds (the gap is\nwithin 35% and usually around 10%; sometimes our bounds are even better) for\nlarger networks compared to the methods based on solving linear programming\nproblems but our algorithms are 33-14,000 times faster; (3) our method is\ncapable of solving large MNIST and CIFAR networks up to 7 layers with more than\n10,000 neurons within tens of seconds on a single CPU core.\n  In addition, we show that, in fact, there is no polynomial time algorithm\nthat can approximately find the minimum $\\ell_1$ adversarial distortion of a\nReLU network with a $0.99\\ln n$ approximation ratio unless\n$\\mathsf{NP}$=$\\mathsf{P}$, where $n$ is the number of neurons in the network.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 17:47:56 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 20:50:00 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 21:48:37 GMT"}, {"version": "v4", "created": "Tue, 2 Oct 2018 08:25:08 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Weng", "Tsui-Wei", ""], ["Zhang", "Huan", ""], ["Chen", "Hongge", ""], ["Song", "Zhao", ""], ["Hsieh", "Cho-Jui", ""], ["Boning", "Duane", ""], ["Dhillon", "Inderjit S.", ""], ["Daniel", "Luca", ""]]}, {"id": "1804.09803", "submitter": "Zhi Zhang", "authors": "Zhi Zhang, Guanghan Ning, Yigang Cen, Yang Li, Zhiqun Zhao, Hao Sun,\n  Zhihai He", "title": "Progressive Neural Networks for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inference structures and computational complexity of existing deep neural\nnetworks, once trained, are fixed and remain the same for all test images.\nHowever, in practice, it is highly desirable to establish a progressive\nstructure for deep neural networks which is able to adapt its inference process\nand complexity for images with different visual recognition complexity. In this\nwork, we develop a multi-stage progressive structure with integrated confidence\nanalysis and decision policy learning for deep neural networks. This new\nframework consists of a set of network units to be activated in a sequential\nmanner with progressively increased complexity and visual recognition power.\nOur extensive experimental results on the CIFAR-10 and ImageNet datasets\ndemonstrate that the proposed progressive deep neural network is able to obtain\nmore than 10 fold complexity scalability while achieving the state-of-the-art\nperformance using a single network model satisfying different\ncomplexity-accuracy requirements.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 21:22:27 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Zhang", "Zhi", ""], ["Ning", "Guanghan", ""], ["Cen", "Yigang", ""], ["Li", "Yang", ""], ["Zhao", "Zhiqun", ""], ["Sun", "Hao", ""], ["He", "Zhihai", ""]]}, {"id": "1804.09858", "submitter": "Zhou Honggang", "authors": "Honggang Zhou and Yunchun Li and Hailong Yang and Wei Li and Jie Jia", "title": "Generative Model for Heterogeneous Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models (GMs) such as Generative Adversary Network (GAN) and\nVariational Auto-Encoder (VAE) have thrived these years and achieved high\nquality results in generating new samples. Especially in Computer Vision, GMs\nhave been used in image inpainting, denoising and completion, which can be\ntreated as the inference from observed pixels to corrupted pixels. However,\nimages are hierarchically structured which are quite different from many\nreal-world inference scenarios with non-hierarchical features. These inference\nscenarios contain heterogeneous stochastic variables and irregular mutual\ndependences. Traditionally they are modeled by Bayesian Network (BN). However,\nthe learning and inference of BN model are NP-hard thus the number of\nstochastic variables in BN is highly constrained. In this paper, we adapt\ntypical GMs to enable heterogeneous learning and inference in polynomial\ntime.We also propose an extended autoregressive (EAR) model and an EAR with\nadversary loss (EARA) model and give theoretical results on their\neffectiveness. Experiments on several BN datasets show that our proposed EAR\nmodel achieves the best performance in most cases compared to other GMs. Except\nfor black box analysis, we've also done a serial of experiments on Markov\nborder inference of GMs for white box analysis and give theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 02:28:34 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Zhou", "Honggang", ""], ["Li", "Yunchun", ""], ["Yang", "Hailong", ""], ["Li", "Wei", ""], ["Jia", "Jie", ""]]}, {"id": "1804.09859", "submitter": "Takashi Shinozaki", "authors": "Takashi Shinozaki", "title": "Competitive Learning Enriches Learning Representation and Accelerates\n  the Fine-tuning of CNNs", "comments": "Appeared at NIPS 2017 Workshop: Deep Learning: Bridging Theory and\n  Practice", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose the integration of competitive learning into\nconvolutional neural networks (CNNs) to improve the representation learning and\nefficiency of fine-tuning. Conventional CNNs use back propagation learning, and\nit enables powerful representation learning by a discrimination task. However,\nit requires huge amount of labeled data, and acquisition of labeled data is\nmuch harder than that of unlabeled data. Thus, efficient use of unlabeled data\nis getting crucial for DNNs. To address the problem, we introduce unsupervised\ncompetitive learning into the convolutional layer, and utilize unlabeled data\nfor effective representation learning. The results of validation experiments\nusing a toy model demonstrated that strong representation learning effectively\nextracted bases of images into convolutional filters using unlabeled data, and\naccelerated the speed of the fine-tuning of subsequent supervised back\npropagation learning. The leverage was more apparent when the number of filters\nwas sufficiently large, and, in such a case, the error rate steeply decreased\nin the initial phase of fine-tuning. Thus, the proposed method enlarged the\nnumber of filters in CNNs, and enabled a more detailed and generalized\nrepresentation. It could provide a possibility of not only deep but broad\nneural networks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 02:28:48 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Shinozaki", "Takashi", ""]]}, {"id": "1804.09862", "submitter": "Hyeong-Ju Kang", "authors": "Hyeong-Ju Kang", "title": "Accelerator-Aware Pruning for Convolutional Neural Networks", "comments": "11 pages, 9 figures", "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology,\n  vol. 30, no. 7, pp. 2093-2103, Jul. 2020", "doi": "10.1109/TCSVT.2019.2911674", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have shown tremendous performance capabilities\nin computer vision tasks, but their excessive amounts of weight storage and\narithmetic operations prevent them from being adopted in embedded environments.\nOne of the solutions involves pruning, where certain unimportant weights are\nforced to have a value of zero. Many pruning schemes have been proposed, but\nthese have mainly focused on the number of pruned weights. Previous pruning\nschemes scarcely considered ASIC or FPGA accelerator architectures. When these\npruned networks are run on accelerators, the lack of consideration of the\narchitecture causes some inefficiency problems, including internal buffer\nmisalignments and load imbalances. This paper proposes a new pruning scheme\nthat reflects accelerator architectures. In the proposed scheme, pruning is\nperformed so that the same number of weights remain for each weight group\ncorresponding to activations fetched simultaneously. In this way, the pruning\nscheme resolves the inefficiency problems, doubling the accelerator\nperformance. Even with this constraint, the proposed pruning scheme reached a\npruning ratio similar to that of previous unconstrained pruning schemes, not\nonly on AlexNet and VGG16 but also on state-of-the-art very deep networks such\nas ResNet. Furthermore, the proposed scheme demonstrated a comparable pruning\nratio on compact networks such as MobileNet and on slimmed networks that were\nalready pruned in a channel-wise manner. In addition to improving the\nefficiency of previous sparse accelerators, it will be also shown that the\nproposed pruning scheme can be used to reduce the logic complexity of sparse\naccelerators.The pruned models are publicly available at\nhttps://github.com/HyeongjuKang/accelerator-aware-pruning.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 02:35:04 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 12:49:00 GMT"}, {"version": "v3", "created": "Sat, 5 Sep 2020 07:22:14 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Kang", "Hyeong-Ju", ""]]}, {"id": "1804.09873", "submitter": "Mohammadhassan Izadyyazdanabadi", "authors": "Mohammadhassan Izadyyazdanabadi, Evgenii Belykh, Michael Mooney,\n  Jennifer Eschbacher, Peter Nakaji, Yezhou Yang, and Mark C. Preul", "title": "Prospects for Theranostics in Neurosurgical Imaging: Empowering Confocal\n  Laser Endomicroscopy Diagnostics via Deep Learning", "comments": "See the final version published in Frontiers in Oncology here:\n  https://www.frontiersin.org/articles/10.3389/fonc.2018.00240/full", "journal-ref": null, "doi": "10.3389/fonc.2018.00240", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confocal laser endomicroscopy (CLE) is an advanced optical fluorescence\nimaging technology that has the potential to increase intraoperative precision,\nextend resection, and tailor surgery for malignant invasive brain tumors\nbecause of its subcellular dimension resolution. Despite its promising\ndiagnostic potential, interpreting the gray tone fluorescence images can be\ndifficult for untrained users. In this review, we provide a detailed\ndescription of bioinformatical analysis methodology of CLE images that begins\nto assist the neurosurgeon and pathologist to rapidly connect on-the-fly\nintraoperative imaging, pathology, and surgical observation into a\nconclusionary system within the concept of theranostics. We present an overview\nand discuss deep learning models for automatic detection of the diagnostic CLE\nimages and discuss various training regimes and ensemble modeling effect on the\npower of deep learning predictive models. Two major approaches reviewed in this\npaper include the models that can automatically classify CLE images into\ndiagnostic/nondiagnostic, glioma/nonglioma, tumor/injury/normal categories and\nmodels that can localize histological features on the CLE images using weakly\nsupervised methods. We also briefly review advances in the deep learning\napproaches used for CLE image analysis in other organs. Significant advances in\nspeed and precision of automated diagnostic frame selection would augment the\ndiagnostic potential of CLE, improve operative workflow and integration into\nbrain tumor surgery. Such technology and bioinformatics analytics lend\nthemselves to improved precision, personalization, and theranostics in brain\ntumor treatment.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 03:33:37 GMT"}, {"version": "v2", "created": "Sat, 18 Aug 2018 06:42:49 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Izadyyazdanabadi", "Mohammadhassan", ""], ["Belykh", "Evgenii", ""], ["Mooney", "Michael", ""], ["Eschbacher", "Jennifer", ""], ["Nakaji", "Peter", ""], ["Yang", "Yezhou", ""], ["Preul", "Mark C.", ""]]}, {"id": "1804.09915", "submitter": "Florian Piewak", "authors": "Florian Piewak, Peter Pinggera, Manuel Sch\\\"afer, David Peter, Beate\n  Schwarz, Nick Schneider, David Pfeiffer, Markus Enzweiler, and Marius\n  Z\\\"ollner", "title": "Boosting LiDAR-based Semantic Labeling by Cross-Modal Training Data\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile robots and autonomous vehicles rely on multi-modal sensor setups to\nperceive and understand their surroundings. Aside from cameras, LiDAR sensors\nrepresent a central component of state-of-the-art perception systems. In\naddition to accurate spatial perception, a comprehensive semantic understanding\nof the environment is essential for efficient and safe operation. In this paper\nwe present a novel deep neural network architecture called LiLaNet for\npoint-wise, multi-class semantic labeling of semi-dense LiDAR data. The network\nutilizes virtual image projections of the 3D point clouds for efficient\ninference. Further, we propose an automated process for large-scale cross-modal\ntraining data generation called Autolabeling, in order to boost semantic\nlabeling performance while keeping the manual annotation effort low. The\neffectiveness of the proposed network architecture as well as the automated\ndata generation process is demonstrated on a manually annotated ground truth\ndataset. LiLaNet is shown to significantly outperform current state-of-the-art\nCNN architectures for LiDAR data. Applying our automatically generated\nlarge-scale training data yields a boost of up to 14 percentage points compared\nto networks trained on manually annotated data only.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 07:18:30 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Piewak", "Florian", ""], ["Pinggera", "Peter", ""], ["Sch\u00e4fer", "Manuel", ""], ["Peter", "David", ""], ["Schwarz", "Beate", ""], ["Schneider", "Nick", ""], ["Pfeiffer", "David", ""], ["Enzweiler", "Markus", ""], ["Z\u00f6llner", "Marius", ""]]}, {"id": "1804.09949", "submitter": "Adam Bielski", "authors": "Adam Bielski, Tomasz Trzcinski", "title": "Pay Attention to Virality: understanding popularity of social media\n  videos with the attention mechanism", "comments": "Conference on Computer Vision and Pattern Recognition Workshops\n  (CVPRW), CVPR 2018 workshop on Visual Understanding of Subjective Attributes\n  of Data (V-USAD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting popularity of social media videos before they are published is a\nchallenging task, mainly due to the complexity of content distribution network\nas well as the number of factors that play part in this process. As solving\nthis task provides tremendous help for media content creators, many successful\nmethods were proposed to solve this problem with machine learning. In this\nwork, we change the viewpoint and postulate that it is not only the predicted\npopularity that matters, but also, maybe even more importantly, understanding\nof how individual parts influence the final popularity score. To that end, we\npropose to combine the Grad-CAM visualization method with a soft attention\nmechanism. Our preliminary results show that this approach allows for more\nintuitive interpretation of the content impact on video popularity, while\nachieving competitive results in terms of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 09:06:06 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Bielski", "Adam", ""], ["Trzcinski", "Tomasz", ""]]}, {"id": "1804.09963", "submitter": "Hyomin Choi", "authors": "Hyomin Choi and Ivan V. Bajic", "title": "Near-Lossless Deep Feature Compression for Collaborative Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative intelligence is a new paradigm for efficient deployment of deep\nneural networks across the mobile-cloud infrastructure. By dividing the network\nbetween the mobile and the cloud, it is possible to distribute the\ncomputational workload such that the overall energy and/or latency of the\nsystem is minimized. However, this necessitates sending deep feature data from\nthe mobile to the cloud in order to perform inference. In this work, we examine\nthe differences between the deep feature data and natural image data, and\npropose a simple and effective near-lossless deep feature compressor. The\nproposed method achieves up to 5% bit rate reduction compared to HEVC-Intra and\neven more against other popular image codecs. Finally, we suggest an approach\nfor reconstructing the input image from compressed deep features in the cloud,\nthat could serve to supplement the inference performed by the deep model.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 09:52:42 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 21:42:37 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Choi", "Hyomin", ""], ["Bajic", "Ivan V.", ""]]}, {"id": "1804.09979", "submitter": "Pongsate Tangseng", "authors": "Pongsate Tangseng, Kota Yamaguchi, Takayuki Okatani", "title": "Recommending Outfits from Personal Closet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider grading a fashion outfit for recommendation, where we assume that\nusers have a closet of items and we aim at producing a score for an arbitrary\ncombination of items in the closet. The challenge in outfit grading is that the\ninput to the system is a bag of item pictures that are unordered and vary in\nsize. We build a deep neural network-based system that can take variable-length\nitems and predict a score. We collect a large number of outfits from a popular\nfashion sharing website, Polyvore, and evaluate the performance of our grading\nsystem. We compare our model with a random-choice baseline, both on the\ntraditional classification evaluation and on people's judgment using a\ncrowdsourcing platform. With over 84% in classification accuracy and 91%\nmatching ratio to human annotators, our model can reliably grade the quality of\nan outfit. We also build an outfit recommender on top of our grader to\ndemonstrate the practical application of our model for a personal closet\nassistant.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 10:30:06 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Tangseng", "Pongsate", ""], ["Yamaguchi", "Kota", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1804.09996", "submitter": "Matthijs Douze", "authors": "Matthijs Douze and Alexandre Sablayrolles and Herv\\'e J\\'egou", "title": "Link and code: Fast indexing with graphs and compact regression codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search approaches based on graph walks have recently attained\noutstanding speed-accuracy trade-offs, taking aside the memory requirements. In\nthis paper, we revisit these approaches by considering, additionally, the\nmemory constraint required to index billions of images on a single server. This\nleads us to propose a method based both on graph traversal and compact\nrepresentations. We encode the indexed vectors using quantization and exploit\nthe graph structure to refine the similarity estimation.\n  In essence, our method takes the best of these two worlds: the search\nstrategy is based on nested graphs, thereby providing high precision with a\nrelatively small set of comparisons. At the same time it offers a significant\nmemory compression. As a result, our approach outperforms the state of the art\non operating points considering 64-128 bytes per vector, as demonstrated by our\nresults on two billion-scale public benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 11:24:42 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 10:01:51 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Douze", "Matthijs", ""], ["Sablayrolles", "Alexandre", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1804.10002", "submitter": "Nils Gessert", "authors": "Nils Gessert, Jens Beringhoff, Christoph Otte, Alexander Schlaefer", "title": "Force Estimation from OCT Volumes using 3D CNNs", "comments": "Published in the International Journal of Computer Assisted Radiology\n  and Surgery", "journal-ref": null, "doi": "10.1007/s11548-018-1777-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\textit{Purpose} Estimating the interaction forces of instruments and tissue\nis of interest, particularly to provide haptic feedback during robot assisted\nminimally invasive interventions. Different approaches based on external and\nintegrated force sensors have been proposed. These are hampered by friction,\nsensor size, and sterilizability. We investigate a novel approach to estimate\nthe force vector directly from optical coherence tomography image volumes.\n  \\textit{Methods} We introduce a novel Siamese 3D CNN architecture. The\nnetwork takes an undeformed reference volume and a deformed sample volume as an\ninput and outputs the three components of the force vector. We employ a deep\nresidual architecture with bottlenecks for increased efficiency. We compare the\nSiamese approach to methods using difference volumes and two-dimensional\nprojections. Data was generated using a robotic setup to obtain ground truth\nforce vectors for silicon tissue phantoms as well as porcine tissue.\n  \\textit{Results} Our method achieves a mean average error of 7.7 +- 4.3 mN\nwhen estimating the force vector. Our novel Siamese 3D CNN architecture\noutperforms single-path methods that achieve a mean average error of 11.59 +-\n6.7 mN. Moreover, the use of volume data leads to significantly higher\nperformance compared to processing only surface information which achieves a\nmean average error of 24.38 +- 22.0 mN. Based on the tissue dataset, our\nmethods shows good generalization in between different subjects.\n  \\textit{Conclusions} We propose a novel image-based force estimation method\nusing optical coherence tomography. We illustrate that capturing the\ndeformation of subsurface structures substantially improves force estimation.\nOur approach can provide accurate force estimates in surgical setups when using\nintraoperative optical coherence tomography.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 11:46:39 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Gessert", "Nils", ""], ["Beringhoff", "Jens", ""], ["Otte", "Christoph", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1804.10019", "submitter": "Khaled Khairy", "authors": "Khaled Khairy, Gennady Denisov and Stephan Saalfeld", "title": "Joint Deformable Registration of Large EM Image Volumes: A Matrix Solver\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large electron microscopy image datasets for connectomics are typically\ncomposed of thousands to millions of partially overlapping two-dimensional\nimages (tiles), which must be registered into a coherent volume prior to\nfurther analysis. A common registration strategy is to find matching features\nbetween neighboring and overlapping image pairs, followed by a numerical\nestimation of optimal image deformation using a so-called solver program.\n  Existing solvers are inadequate for large data volumes, and inefficient for\nsmall-scale image registration.\n  In this work, an efficient and accurate matrix-based solver method is\npresented. A linear system is constructed that combines minimization of\nfeature-pair square distances with explicit constraints in a regularization\nterm. In absence of reliable priors for regularization, we show how to\nconstruct a rigid-model approximation to use as prior. The linear system is\nsolved using available computer programs, whose performance on typical\nregistration tasks we briefly compare, and to which future scale-up is\ndelegated. Our method is applied to the joint alignment of 2.67 million images,\nwith more than 200 million point-pairs and has been used for successfully\naligning the first full adult fruit fly brain.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 12:35:58 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Khairy", "Khaled", ""], ["Denisov", "Gennady", ""], ["Saalfeld", "Stephan", ""]]}, {"id": "1804.10021", "submitter": "Xiang Yan", "authors": "Xiang Yan, Syed Zulqarnain Gilani, Hanlin Qin, Mingtao Feng, Liang\n  Zhang, Ajmal Mian", "title": "Deep Keyframe Detection in Human Action Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting representative frames in videos based on human actions is quite\nchallenging because of the combined factors of human pose in action and the\nbackground. This paper addresses this problem and formulates the key frame\ndetection as one of finding the video frames that optimally maximally\ncontribute to differentiating the underlying action category from all other\ncategories. To this end, we introduce a deep two-stream ConvNet for key frame\ndetection in videos that learns to directly predict the location of key frames.\nOur key idea is to automatically generate labeled data for the CNN learning\nusing a supervised linear discriminant method. While the training data is\ngenerated taking many different human action videos into account, the trained\nCNN can predict the importance of frames from a single video. We specify a new\nConvNet framework, consisting of a summarizer and discriminator. The summarizer\nis a two-stream ConvNet aimed at, first, capturing the appearance and motion\nfeatures of video frames, and then encoding the obtained appearance and motion\nfeatures for video representation. The discriminator is a fitting function\naimed at distinguishing between the key frames and others in the video. We\nconduct experiments on a challenging human action dataset UCF101 and show that\nour method can detect key frames with high accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 12:41:05 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Yan", "Xiang", ""], ["Gilani", "Syed Zulqarnain", ""], ["Qin", "Hanlin", ""], ["Feng", "Mingtao", ""], ["Zhang", "Liang", ""], ["Mian", "Ajmal", ""]]}, {"id": "1804.10069", "submitter": "Yuxin Peng", "authors": "Chenrui Zhang and Yuxin Peng", "title": "Better and Faster: Knowledge Transfer from Multiple Self-supervised\n  Learning Tasks via Graph Distillation for Video Classification", "comments": "7 pages, accepted by International Joint Conference on Artificial\n  Intelligence (IJCAI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video representation learning is a vital problem for classification task.\nRecently, a promising unsupervised paradigm termed self-supervised learning has\nemerged, which explores inherent supervisory signals implied in massive data\nfor feature learning via solving auxiliary tasks. However, existing methods in\nthis regard suffer from two limitations when extended to video classification.\nFirst, they focus only on a single task, whereas ignoring complementarity among\ndifferent task-specific features and thus resulting in suboptimal video\nrepresentation. Second, high computational and memory cost hinders their\napplication in real-world scenarios. In this paper, we propose a graph-based\ndistillation framework to address these problems: (1) We propose logits graph\nand representation graph to transfer knowledge from multiple self-supervised\ntasks, where the former distills classifier-level knowledge by solving a\nmulti-distribution joint matching problem, and the latter distills internal\nfeature knowledge from pairwise ensembled representations with tackling the\nchallenge of heterogeneity among different features; (2) The proposal that\nadopts a teacher-student framework can reduce the redundancy of knowledge\nlearnt from teachers dramatically, leading to a lighter student model that\nsolves classification task more efficiently. Experimental results on 3 video\ndatasets validate that our proposal not only helps learn better video\nrepresentation but also compress model for faster inference.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 13:57:17 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Zhang", "Chenrui", ""], ["Peng", "Yuxin", ""]]}, {"id": "1804.10073", "submitter": "Yuxin Peng", "authors": "Chenrui Zhang and Yuxin Peng", "title": "Visual Data Synthesis via GAN for Zero-Shot Video Classification", "comments": "7 pages, accepted by International Joint Conference on Artificial\n  Intelligence (IJCAI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Learning (ZSL) in video classification is a promising research\ndirection, which aims to tackle the challenge from explosive growth of video\ncategories. Most existing methods exploit seen-to-unseen correlation via\nlearning a projection between visual and semantic spaces. However, such\nprojection-based paradigms cannot fully utilize the discriminative information\nimplied in data distribution, and commonly suffer from the information\ndegradation issue caused by \"heterogeneity gap\". In this paper, we propose a\nvisual data synthesis framework via GAN to address these problems.\nSpecifically, both semantic knowledge and visual distribution are leveraged to\nsynthesize video feature of unseen categories, and ZSL can be turned into\ntypical supervised problem with the synthetic features. First, we propose\nmulti-level semantic inference to boost video feature synthesis, which captures\nthe discriminative information implied in joint visual-semantic distribution\nvia feature-level and label-level semantic inference. Second, we propose\nMatching-aware Mutual Information Correlation to overcome information\ndegradation issue, which captures seen-to-unseen correlation in matched and\nmismatched visual-semantic pairs by mutual information, providing the zero-shot\nsynthesis procedure with robust guidance signals. Experimental results on four\nvideo datasets demonstrate that our approach can improve the zero-shot video\nclassification performance significantly.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 14:10:41 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Zhang", "Chenrui", ""], ["Peng", "Yuxin", ""]]}, {"id": "1804.10094", "submitter": "Slawomir Bak", "authors": "Slawomir Bak, Peter Carr, Jean-Francois Lalonde", "title": "Domain Adaptation through Synthesis for Unsupervised Person\n  Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drastic variations in illumination across surveillance cameras make the\nperson re-identification problem extremely challenging. Current large scale\nre-identification datasets have a significant number of training subjects, but\nlack diversity in lighting conditions. As a result, a trained model requires\nfine-tuning to become effective under an unseen illumination condition. To\nalleviate this problem, we introduce a new synthetic dataset that contains\nhundreds of illumination conditions. Specifically, we use 100 virtual humans\nilluminated with multiple HDR environment maps which accurately model realistic\nindoor and outdoor lighting. To achieve better accuracy in unseen illumination\nconditions we propose a novel domain adaptation technique that takes advantage\nof our synthetic data and performs fine-tuning in a completely unsupervised\nway. Our approach yields significantly higher accuracy than semi-supervised and\nunsupervised state-of-the-art methods, and is very competitive with supervised\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 14:45:24 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Bak", "Slawomir", ""], ["Carr", "Peter", ""], ["Lalonde", "Jean-Francois", ""]]}, {"id": "1804.10113", "submitter": "Matthias Zeppelzauer", "authors": "David Koch, Miroslav Despotovic, Muntaha Sakeena, Mario D\\\"oller,\n  Matthias Zeppelzauer", "title": "Visual Estimation of Building Condition with Patch-level ConvNets", "comments": "To appear in: Workshop on Multimedia for Real Estate Tech, ICMR 2018,\n  Yokohama, Japan", "journal-ref": null, "doi": "10.1145/3210499.3210526", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The condition of a building is an important factor for real estate valuation.\nCurrently, the estimation of condition is determined by real estate appraisers\nwhich makes it subjective to a certain degree. We propose a novel vision-based\napproach for the assessment of the building condition from exterior views of\nthe building. To this end, we develop a multi-scale patch-based pattern\nextraction approach and combine it with convolutional neural networks to\nestimate building condition from visual clues. Our evaluation shows that\nvisually estimated building condition can serve as a proxy for condition\nestimates by appraisers.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 15:33:57 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Koch", "David", ""], ["Despotovic", "Miroslav", ""], ["Sakeena", "Muntaha", ""], ["D\u00f6ller", "Mario", ""], ["Zeppelzauer", "Matthias", ""]]}, {"id": "1804.10123", "submitter": "Sam Leroux", "authors": "Sam Leroux, Pavlo Molchanov, Pieter Simoens, Bart Dhoedt, Thomas\n  Breuel, Jan Kautz", "title": "IamNN: Iterative and Adaptive Mobile Neural Network for Efficient Image\n  Classification", "comments": "ICLR 2018 Workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep residual networks (ResNets) made a recent breakthrough in deep learning.\nThe core idea of ResNets is to have shortcut connections between layers that\nallow the network to be much deeper while still being easy to optimize avoiding\nvanishing gradients. These shortcut connections have interesting side-effects\nthat make ResNets behave differently from other typical network architectures.\nIn this work we use these properties to design a network based on a ResNet but\nwith parameter sharing and with adaptive computation time. The resulting\nnetwork is much smaller than the original network and can adapt the\ncomputational cost to the complexity of the input image.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 15:57:00 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Leroux", "Sam", ""], ["Molchanov", "Pavlo", ""], ["Simoens", "Pieter", ""], ["Dhoedt", "Bart", ""], ["Breuel", "Thomas", ""], ["Kautz", "Jan", ""]]}, {"id": "1804.10134", "submitter": "Lucas Beyer", "authors": "Stefan Breuers, Lucas Beyer, Umer Rafi, Bastian Leibe", "title": "Detection-Tracking for Efficient Person Analysis: The DetTA Pipeline", "comments": "Code available at: https://github.com/sbreuers/detta", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade many robots were deployed in the wild, and people\ndetection and tracking is an important component of such deployments. On top of\nthat, one often needs to run modules which analyze persons and extract higher\nlevel attributes such as age and gender, or dynamic information like gaze and\npose. The latter ones are especially necessary for building a reactive, social\nrobot-person interaction.\n  In this paper, we combine those components in a fully modular\ndetection-tracking-analysis pipeline, called DetTA. We investigate the benefits\nof such an integration on the example of head and skeleton pose, by using the\nconsistent track ID for a temporal filtering of the analysis modules'\nobservations, showing a slight improvement in a challenging real-world\nscenario. We also study the potential of a so-called \"free-flight\" mode, where\nthe analysis of a person attribute only relies on the filter's predictions for\ncertain frames. Here, our study shows that this boosts the runtime\ndramatically, while the prediction quality remains stable. This insight is\nespecially important for reducing power consumption and sharing precious\n(GPU-)memory when running many analysis components on a mobile platform,\nespecially so in the era of expensive deep learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 16:04:30 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 10:33:47 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Breuers", "Stefan", ""], ["Beyer", "Lucas", ""], ["Rafi", "Umer", ""], ["Leibe", "Bastian", ""]]}, {"id": "1804.10160", "submitter": "Cairong Zhang", "authors": "Yi Wei, Guijin Wang, Cairong Zhang, Hengkai Guo, Xinghao Chen,\n  Huazhong Yang", "title": "Two-Stream Binocular Network: Accurate Near Field Finger Detection Based\n  On Binocular Images", "comments": "Published in: Visual Communications and Image Processing (VCIP), 2017\n  IEEE. Original IEEE publication available on\n  https://ieeexplore.ieee.org/abstract/document/8305146/. Dataset available on\n  https://sites.google.com/view/thuhand17", "journal-ref": "Visual Communications and Image Processing (VCIP), 2017 IEEE\n  (2017) 1-4", "doi": "10.1109/VCIP.2017.8305146", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingertip detection plays an important role in human computer interaction.\nPrevious works transform binocular images into depth images. Then depth-based\nhand pose estimation methods are used to predict 3D positions of fingertips.\nDifferent from previous works, we propose a new framework, named Two-Stream\nBinocular Network (TSBnet) to detect fingertips from binocular images directly.\nTSBnet first shares convolutional layers for low level features of right and\nleft images. Then it extracts high level features in two-stream convolutional\nnetworks separately. Further, we add a new layer: binocular distance\nmeasurement layer to improve performance of our model. To verify our scheme, we\nbuild a binocular hand image dataset, containing about 117k pairs of images in\ntraining set and 10k pairs of images in test set. Our methods achieve an\naverage error of 10.9mm on our test set, outperforming previous work by 5.9mm\n(relatively 35.1%).\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 16:36:36 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Wei", "Yi", ""], ["Wang", "Guijin", ""], ["Zhang", "Cairong", ""], ["Guo", "Hengkai", ""], ["Chen", "Xinghao", ""], ["Yang", "Huazhong", ""]]}, {"id": "1804.10163", "submitter": "Evgeny Burnaev", "authors": "Alexander Bernstein, Evgeny Burnaev, Ekaterina Kondratyeva, Svetlana\n  Sushchinskaya, Maxim Sharaev, Alexander Andreev, Alexey Artemov, Renat\n  Akzhigitov", "title": "Machine Learning pipeline for discovering neuroimaging-based biomarkers\n  in neurology and psychiatry", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of diagnostic pattern recognition/classification from\nneuroimaging data. We propose a common data analysis pipeline for\nneuroimaging-based diagnostic classification problems using various ML\nalgorithms and processing toolboxes for brain imaging. We illustrate the\npipeline application by discovering new biomarkers for diagnostics of epilepsy\nand depression based on clinical and MRI/fMRI data for patients and healthy\nvolunteers.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 16:42:38 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""], ["Kondratyeva", "Ekaterina", ""], ["Sushchinskaya", "Svetlana", ""], ["Sharaev", "Maxim", ""], ["Andreev", "Alexander", ""], ["Artemov", "Alexey", ""], ["Akzhigitov", "Renat", ""]]}, {"id": "1804.10167", "submitter": "Evgeny Burnaev", "authors": "Maxim Sharaev, Alexander Andreev, Alexey Artemov, Alexander Bernstein,\n  Evgeny Burnaev, Ekaterina Kondratyeva, Svetlana Sushchinskaya, Renat\n  Akzhigitov", "title": "fMRI: preprocessing, classification and pattern recognition", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning continues to gain momentum in the neuroscience community,\nwe witness the emergence of novel applications such as diagnostics,\ncharacterization, and treatment outcome prediction for psychiatric and\nneurological disorders, for instance, epilepsy and depression. Systematic\nresearch into these mental disorders increasingly involves drawing clinical\nconclusions on the basis of data-driven approaches; to this end, structural and\nfunctional neuroimaging serve as key source modalities. Identification of\ninformative neuroimaging markers requires establishing a comprehensive\npreparation pipeline for data which may be severely corrupted by artifactual\nsignal fluctuations. In this work, we review a large body of literature to\nprovide ample evidence for the advantages of pattern recognition approaches in\nclinical applications, overview advanced graph-based pattern recognition\napproaches, and propose a noise-aware neuroimaging data processing pipeline. To\ndemonstrate the effectiveness of our approach, we provide results from a pilot\nstudy, which show a significant improvement in classification accuracy,\nindicating a promising research direction.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 16:48:52 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Sharaev", "Maxim", ""], ["Andreev", "Alexander", ""], ["Artemov", "Alexey", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""], ["Kondratyeva", "Ekaterina", ""], ["Sushchinskaya", "Svetlana", ""], ["Akzhigitov", "Renat", ""]]}, {"id": "1804.10172", "submitter": "Andrew Gritsevskiy", "authors": "Andrew Gritsevskiy and Maksym Korablyov", "title": "Capsule networks for low-data transfer learning", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a capsule network-based architecture for generalizing learning to\nnew data with few examples. Using both generative and non-generative capsule\nnetworks with intermediate routing, we are able to generalize to new\ninformation over 25 times faster than a similar convolutional neural network.\nWe train the networks on the multiMNIST dataset lacking one digit. After the\nnetworks reach their maximum accuracy, we inject 1-100 examples of the missing\ndigit into the training set, and measure the number of batches needed to return\nto a comparable level of accuracy. We then discuss the improvement in low-data\ntransfer learning that capsule networks bring, and propose future directions\nfor capsule research.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 17:01:12 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Gritsevskiy", "Andrew", ""], ["Korablyov", "Maksym", ""]]}, {"id": "1804.10201", "submitter": "Anis Davoudi", "authors": "Anis Davoudi, Kumar Rohit Malhotra, Benjamin Shickel, Scott Siegel,\n  Seth Williams, Matthew Ruppert, Emel Bihorac, Tezcan Ozrazgat-Baslanti,\n  Patrick J. Tighe, Azra Bihorac, Parisa Rashidi", "title": "The Intelligent ICU Pilot Study: Using Artificial Intelligence\n  Technology for Autonomous Patient Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, many critical care indices are repetitively assessed and recorded\nby overburdened nurses, e.g. physical function or facial pain expressions of\nnonverbal patients. In addition, many essential information on patients and\ntheir environment are not captured at all, or are captured in a non-granular\nmanner, e.g. sleep disturbance factors such as bright light, loud background\nnoise, or excessive visitations. In this pilot study, we examined the\nfeasibility of using pervasive sensing technology and artificial intelligence\nfor autonomous and granular monitoring of critically ill patients and their\nenvironment in the Intensive Care Unit (ICU). As an exemplar prevalent\ncondition, we also characterized delirious and non-delirious patients and their\nenvironment. We used wearable sensors, light and sound sensors, and a\nhigh-resolution camera to collected data on patients and their environment. We\nanalyzed collected data using deep learning and statistical analysis. Our\nsystem performed face detection, face recognition, facial action unit\ndetection, head pose detection, facial expression recognition, posture\nrecognition, actigraphy analysis, sound pressure and light level detection, and\nvisitation frequency detection. We were able to detect patient's face (Mean\naverage precision (mAP)=0.94), recognize patient's face (mAP=0.80), and their\npostures (F1=0.94). We also found that all facial expressions, 11 activity\nfeatures, visitation frequency during the day, visitation frequency during the\nnight, light levels, and sound pressure levels during the night were\nsignificantly different between delirious and non-delirious patients\n(p-value<0.05). In summary, we showed that granular and autonomous monitoring\nof critically ill patients and their environment is feasible and can be used\nfor characterizing critical care conditions and related environment factors.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 21:24:46 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 18:25:32 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Davoudi", "Anis", ""], ["Malhotra", "Kumar Rohit", ""], ["Shickel", "Benjamin", ""], ["Siegel", "Scott", ""], ["Williams", "Seth", ""], ["Ruppert", "Matthew", ""], ["Bihorac", "Emel", ""], ["Ozrazgat-Baslanti", "Tezcan", ""], ["Tighe", "Patrick J.", ""], ["Bihorac", "Azra", ""], ["Rashidi", "Parisa", ""]]}, {"id": "1804.10272", "submitter": "Quanshi Zhang", "authors": "Quanshi Zhang, Yu Yang, Qian Yu, Ying Nian Wu", "title": "Network Transplanting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on a new task, i.e., transplanting a\ncategory-and-task-specific neural network to a generic, modular network without\nstrong supervision. We design an functionally interpretable structure for the\ngeneric network. Like building LEGO blocks, we teach the generic network a new\ncategory by directly transplanting the module corresponding to the category\nfrom a pre-trained network with a few or even without sample annotations. Our\nmethod incrementally adds new categories to the generic network but does not\naffect representations of existing categories. In this way, our method breaks\nthe typical bottleneck of learning a net for massive tasks and categories, i.e.\nthe requirement of collecting samples for all tasks and categories at the same\ntime before the learning begins. Thus, we use a new distillation algorithm,\nnamely back-distillation, to overcome specific challenges of network\ntransplanting. Our method without training samples even outperformed the\nbaseline with 100 training samples.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 20:25:36 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 05:18:29 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Zhang", "Quanshi", ""], ["Yang", "Yu", ""], ["Yu", "Qian", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1804.10275", "submitter": "He Zhang", "authors": "Hajime Nada, Vishwanath A. Sindagi, He Zhang, Vishal M. Patel", "title": "Pushing the Limits of Unconstrained Face Detection: a Challenge Dataset\n  and Baseline Results", "comments": "Accepted in BTAS'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection has witnessed immense progress in the last few years, with new\nmilestones being surpassed every year. While many challenges such as large\nvariations in scale, pose, appearance are successfully addressed, there still\nexist several issues which are not specifically captured by existing methods or\ndatasets. In this work, we identify the next set of challenges that requires\nattention from the research community and collect a new dataset of face images\nthat involve these issues such as weather-based degradations, motion blur,\nfocus blur and several others. We demonstrate that there is a considerable gap\nin the performance of state-of-the-art detectors and real-world requirements.\nHence, in an attempt to fuel further research in unconstrained face detection,\nwe present a new annotated Unconstrained Face Detection Dataset (UFDD) with\nseveral challenges and benchmark recent methods. Additionally, we provide an\nin-depth analysis of the results and failure cases of these methods. The\ndataset as well as baseline results will be made publicly available in due\ntime. The UFDD dataset as well as baseline results are available at:\nwww.ufdd.info/\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 20:44:06 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 00:46:57 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 15:13:27 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Nada", "Hajime", ""], ["Sindagi", "Vishwanath A.", ""], ["Zhang", "He", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1804.10323", "submitter": "Salman Khan Dr.", "authors": "Salman H. Khan, Munawar Hayat, Nick Barnes", "title": "Adversarial Training of Variational Auto-encoders for High Fidelity\n  Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational auto-encoders (VAEs) provide an attractive solution to image\ngeneration problem. However, they tend to produce blurred and over-smoothed\nimages due to their dependence on pixel-wise reconstruction loss. This paper\nintroduces a new approach to alleviate this problem in the VAE based generative\nmodels. Our model simultaneously learns to match the data, reconstruction loss\nand the latent distributions of real and fake images to improve the quality of\ngenerated samples. To compute the loss distributions, we introduce an\nauto-encoder based discriminator model which allows an adversarial learning\nprocedure. The discriminator in our model also provides perceptual guidance to\nthe VAE by matching the learned similarity metric of the real and fake samples\nin the latent space. To stabilize the overall training process, our model uses\nan error feedback approach to maintain the equilibrium between competing\nnetworks in the model. Our experiments show that the generated samples from our\nproposed model exhibit a diverse set of attributes and facial expressions and\nscale up to high-resolution images very well.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 02:19:35 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Khan", "Salman H.", ""], ["Hayat", "Munawar", ""], ["Barnes", "Nick", ""]]}, {"id": "1804.10337", "submitter": "Kai Cao", "authors": "Kai Cao and Anil K. Jain", "title": "Latent Fingerprint Recognition: Role of Texture Template", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a texture template approach, consisting of a set of virtual\nminutiae, to improve the overall latent fingerprint recognition accuracy. To\ncompensate for the lack of sufficient number of minutiae in poor quality latent\nprints, we generate a set of virtual minutiae. However, due to a large number\nof these regularly placed virtual minutiae, texture based template matching has\na large computational requirement compared to matching true minutiae templates.\nTo improve both the accuracy and efficiency of the texture template matching,\nwe investigate: i) both original and enhanced fingerprint patches for training\nconvolutional neural networks (ConvNets) to improve the distinctiveness of\ndescriptors associated with each virtual minutiae, ii) smaller patches around\nvirtual minutiae and a fast ConvNet architecture to speed up descriptor\nextraction, iii) reduce the descriptor length, iv) a modified hierarchical\ngraph matching strategy to improve the matching speed, and v) extraction of\nmultiple texture templates to boost the performance. Experiments on NIST SD27\nlatent database show that the above strategies can improve the matching speed\nfrom 11 ms (24 threads) per comparison (between a latent and a reference print)\nto only 7.7 ms (single thread) per comparison while improving the rank-1\naccuracy by 8.9% against 10K gallery.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 04:26:03 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Cao", "Kai", ""], ["Jain", "Anil K.", ""]]}, {"id": "1804.10343", "submitter": "Sohil Shah", "authors": "Sohil Shah, Pallabi Ghosh, Larry S Davis and Tom Goldstein", "title": "Stacked U-Nets: A No-Frills Approach to Natural Image Segmentation", "comments": "The code is available at https://github.com/shahsohil/sunets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many imaging tasks require global information about all pixels in an image.\nConventional bottom-up classification networks globalize information by\ndecreasing resolution; features are pooled and downsampled into a single\noutput. But for semantic segmentation and object detection tasks, a network\nmust provide higher-resolution pixel-level outputs. To globalize information\nwhile preserving resolution, many researchers propose the inclusion of\nsophisticated auxiliary blocks, but these come at the cost of a considerable\nincrease in network size and computational cost. This paper proposes stacked\nu-nets (SUNets), which iteratively combine features from different resolution\nscales while maintaining resolution. SUNets leverage the information\nglobalization power of u-nets in a deeper network architectures that is capable\nof handling the complexity of natural images. SUNets perform extremely well on\nsemantic segmentation tasks using a small number of parameters.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 05:03:32 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Shah", "Sohil", ""], ["Ghosh", "Pallabi", ""], ["Davis", "Larry S", ""], ["Goldstein", "Tom", ""]]}, {"id": "1804.10361", "submitter": "Jie Chang", "authors": "Yujun Gu, Jie Chang, Ya Zhang, Yanfeng Wang", "title": "An Element Sensitive Saliency Model with Position Prior Learning for Web\n  Pages", "comments": "Submitted to ICIAI-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding human visual attention is important for multimedia\napplications. Many studies have attempted to learn from eye-tracking data and\nbuild computational saliency prediction models. However, limited efforts have\nbeen devoted to saliency prediction for Web pages, which are characterized by\nmore diverse content elements and spatial layouts. In this paper, we propose a\nnovel end-to-end deep generative saliency model for Web pages. To capture\nposition biases introduced by page layouts, a Position Prior Learning\nsub-network is proposed, which models position biases as multivariate Gaussian\ndistribution using variational auto-encoder. To model different elements of a\nWeb page, a Multi Discriminative Region Detection (MDRD) branch and a Text\nRegion Detection(TRD) branch are introduced, which target to extract\ndiscriminative localizations and \"prominent\" text regions likely to correspond\nto human attention, respectively. We validate the proposed model with FiWI, a\npublic Web-page dataset, and shows that the proposed model outperforms the\nstate-of-art models for Web-page saliency prediction.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 07:04:11 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 09:31:22 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Gu", "Yujun", ""], ["Chang", "Jie", ""], ["Zhang", "Ya", ""], ["Wang", "Yanfeng", ""]]}, {"id": "1804.10366", "submitter": "Quanming Yao", "authors": "Yaqing Wang and Quanming Yao and James T. Kwok and Lionel M. Ni", "title": "Online Convolutional Sparse Coding with Sample-Dependent Dictionary", "comments": "Accepted by ICML-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional sparse coding (CSC) has been popularly used for the learning of\nshift-invariant dictionaries in image and signal processing. However, existing\nmethods have limited scalability. In this paper, instead of convolving with a\ndictionary shared by all samples, we propose the use of a sample-dependent\ndictionary in which filters are obtained as linear combinations of a small set\nof base filters learned from the data. This added flexibility allows a large\nnumber of sample-dependent patterns to be captured, while the resultant model\ncan still be efficiently learned by online learning. Extensive experimental\nresults show that the proposed method outperforms existing CSC algorithms with\nsignificantly reduced time and space requirements.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 07:21:16 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 08:24:16 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Wang", "Yaqing", ""], ["Yao", "Quanming", ""], ["Kwok", "James T.", ""], ["Ni", "Lionel M.", ""]]}, {"id": "1804.10371", "submitter": "Sofia Ares Oliveira", "authors": "Sofia Ares Oliveira, Benoit Seguin, and Frederic Kaplan (Digital\n  Humanities Laboratory, EPFL, Switzerland)", "title": "dhSegment: A generic deep-learning approach for document segmentation", "comments": null, "journal-ref": null, "doi": "10.1109/ICFHR-2018.2018.00011", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In recent years there have been multiple successful attempts tackling\ndocument processing problems separately by designing task specific hand-tuned\nstrategies. We argue that the diversity of historical document processing tasks\nprohibits to solve them one at a time and shows a need for designing generic\napproaches in order to handle the variability of historical series. In this\npaper, we address multiple tasks simultaneously such as page extraction,\nbaseline extraction, layout analysis or multiple typologies of illustrations\nand photograph extraction. We propose an open-source implementation of a\nCNN-based pixel-wise predictor coupled with task dependent post-processing\nblocks. We show that a single CNN-architecture can be used across tasks with\ncompetitive results. Moreover most of the task-specific post-precessing steps\ncan be decomposed in a small number of simple and standard reusable operations,\nadding to the flexibility of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 07:53:53 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 07:43:04 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Oliveira", "Sofia Ares", "", "Digital\n  Humanities Laboratory, EPFL, Switzerland"], ["Seguin", "Benoit", "", "Digital\n  Humanities Laboratory, EPFL, Switzerland"], ["Kaplan", "Frederic", "", "Digital\n  Humanities Laboratory, EPFL, Switzerland"]]}, {"id": "1804.10390", "submitter": "Masanori Onishi", "authors": "Masanori Onishi, Takeshi Ise", "title": "Automatic classification of trees using a UAV onboard camera and deep\n  learning", "comments": "9 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic classification of trees using remotely sensed data has been a dream\nof many scientists and land use managers. Recently, Unmanned aerial vehicles\n(UAV) has been expected to be an easy-to-use, cost-effective tool for remote\nsensing of forests, and deep learning has attracted attention for its ability\nconcerning machine vision. In this study, using a commercially available UAV\nand a publicly available package for deep learning, we constructed a machine\nvision system for the automatic classification of trees. In our method, we\nsegmented a UAV photography image of forest into individual tree crowns and\ncarried out object-based deep learning. As a result, the system was able to\nclassify 7 tree types at 89.0% accuracy. This performance is notable because we\nonly used basic RGB images from a standard UAV. In contrast, most of previous\nstudies used expensive hardware such as multispectral imagers to improve the\nperformance. This result means that our method has the potential to classify\nindividual trees in a cost-effective manner. This can be a usable tool for many\nforest researchers and managements.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 08:38:22 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Onishi", "Masanori", ""], ["Ise", "Takeshi", ""]]}, {"id": "1804.10427", "submitter": "Kuniaki Saito Saito Kuniaki", "authors": "Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku and Tatsuya Harada", "title": "Open Set Domain Adaptation by Backpropagation", "comments": "Accepted by ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous algorithms have been proposed for transferring knowledge from a\nlabel-rich domain (source) to a label-scarce domain (target). Almost all of\nthem are proposed for a closed-set scenario, where the source and the target\ndomain completely share the class of their samples. We call the shared class\nthe \\doublequote{known class.} However, in practice, when samples in target\ndomain are not labeled, we cannot know whether the domains share the class. A\ntarget domain can contain samples of classes that are not shared by the source\ndomain. We call such classes the \\doublequote{unknown class} and algorithms\nthat work well in the open set situation are very practical. However, most\nexisting distribution matching methods for domain adaptation do not work well\nin this setting because unknown target samples should not be aligned with the\nsource.\n  In this paper, we propose a method for an open set domain adaptation scenario\nwhich utilizes adversarial training. A classifier is trained to make a boundary\nbetween the source and the target samples whereas a generator is trained to\nmake target samples far from the boundary. Thus, we assign two options to the\nfeature generator: aligning them with source known samples or rejecting them as\nunknown target samples. This approach allows extracting features that separate\nunknown target samples from known target samples. Our method was extensively\nevaluated in domain adaptation setting and outperformed other methods with a\nlarge margin in most settings.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 10:16:15 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 16:19:51 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Saito", "Kuniaki", ""], ["Yamamoto", "Shohei", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1804.10428", "submitter": "Songwen Pei", "authors": "Songwen Pei, Fuwu Tang, Yanfei Ji, Jing Fan, Zhong Ning", "title": "Localized Traffic Sign Detection with Multi-scale Deconvolution Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving is becoming a future practical lifestyle greatly driven by\ndeep learning. Specifically, an effective traffic sign detection by deep\nlearning plays a critical role for it. However, different countries have\ndifferent sets of traffic signs, making localized traffic sign recognition\nmodel training a tedious and daunting task. To address the issues of taking\namount of time to compute complicate algorithm and low ratio of detecting\nblurred and sub-pixel images of localized traffic signs, we propose Multi-Scale\nDeconvolution Networks (MDN), which flexibly combines multi-scale convolutional\nneural network with deconvolution sub-network, leading to efficient and\nreliable localized traffic sign recognition model training. It is demonstrated\nthat the proposed MDN is effective compared with classical algorithms on the\nbenchmarks of the localized traffic sign, such as Chinese Traffic Sign Dataset\n(CTSD), and the German Traffic Sign Benchmarks (GTSRB).\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 10:20:44 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 10:21:49 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Pei", "Songwen", ""], ["Tang", "Fuwu", ""], ["Ji", "Yanfei", ""], ["Fan", "Jing", ""], ["Ning", "Zhong", ""]]}, {"id": "1804.10432", "submitter": "Martin Storath", "authors": "Martin Storath, Andreas Weinmann", "title": "Variational Regularization of Inverse Problems for Manifold-Valued Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the variational regularization of manifold-valued\ndata in the inverse problems setting. In particular, we consider TV and TGV\nregularization for manifold-valued data with indirect measurement operators. We\nprovide results on the well-posedness and present algorithms for a numerical\nrealization of these models in the manifold setup. Further, we provide\nexperimental results for synthetic and real data to show the potential of the\nproposed schemes for applications.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 10:40:51 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Storath", "Martin", ""], ["Weinmann", "Andreas", ""]]}, {"id": "1804.10448", "submitter": "Adam Scholefield", "authors": "Adam Scholefield, Alireza Ghasemi, Martin Vetterli", "title": "Bound and Conquer: Improving Triangulation by Enforcing Consistency", "comments": "8 pages, 4 figures, Submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the accuracy of triangulation in multi-camera systems with respect\nto the number of cameras. We show that, under certain conditions, the optimal\nachievable reconstruction error decays quadratically as more cameras are added\nto the system. Furthermore, we analyse the error decay-rate of major\nstate-of-the-art algorithms with respect to the number of cameras. To this end,\nwe introduce the notion of consistency for triangulation, and show that\nconsistent reconstruction algorithms achieve the optimal quadratic decay, which\nis asymptotically faster than some other methods. Finally, we present\nsimulations results supporting our findings. Our simulations have been\nimplemented in MATLAB and the resulting code is available in the supplementary\nmaterial.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 11:40:23 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Scholefield", "Adam", ""], ["Ghasemi", "Alireza", ""], ["Vetterli", "Martin", ""]]}, {"id": "1804.10462", "submitter": "Abdolrahim Kadkhodamohammadi", "authors": "Abdolrahim Kadkhodamohammadi, Nicolas Padoy", "title": "A generalizable approach for multi-view 3D human pose regression", "comments": "The supplementary video is available at https://youtu.be/Cx_kTRzqqzA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the significant improvement in the performance of monocular pose\nestimation approaches and their ability to generalize to unseen environments,\nmulti-view (MV) approaches are often lagging behind in terms of accuracy and\nare specific to certain datasets. This is mainly due to the fact that (1)\ncontrary to real world single-view (SV) datasets, MV datasets are often\ncaptured in controlled environments to collect precise 3D annotations, which do\nnot cover all real world challenges, and (2) the model parameters are learned\nfor specific camera setups. To alleviate these problems, we propose a two-stage\napproach to detect and estimate 3D human poses, which separates SV pose\ndetection from MV 3D pose estimation. This separation enables us to utilize\neach dataset for the right task, i.e. SV datasets for constructing robust pose\ndetection models and MV datasets for constructing precise MV 3D regression\nmodels. In addition, our 3D regression approach only requires 3D pose data and\nits projections to the views for building the model, hence removing the need\nfor collecting annotated data from the test setup. Our approach can therefore\nbe easily generalized to a new environment by simply projecting 3D poses into\n2D during training according to the camera setup used at test time. As 2D poses\nare collected at test time using a SV pose detector, which might generate\ninaccurate detections, we model its characteristics and incorporate this\ninformation during training. We demonstrate that incorporating the detector's\ncharacteristics is important to build a robust 3D regression model and that the\nresulting regression model generalizes well to new MV environments. Our\nevaluation results show that our approach achieves competitive results on the\nHuman3.6M dataset and significantly improves results on a MV clinical dataset\nthat is the first MV dataset generated from live surgery recordings.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 12:14:40 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 15:15:51 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Kadkhodamohammadi", "Abdolrahim", ""], ["Padoy", "Nicolas", ""]]}, {"id": "1804.10469", "submitter": "Ananya Harsh Jha", "authors": "Ananya Harsh Jha, Saket Anand, Maneesh Singh, V. S. R. Veeravasarapu", "title": "Disentangling Factors of Variation with Cycle-Consistent Variational\n  Auto-Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models that learn disentangled representations for different\nfactors of variation in an image can be very useful for targeted data\naugmentation. By sampling from the disentangled latent subspace of interest, we\ncan efficiently generate new data necessary for a particular task. Learning\ndisentangled representations is a challenging problem, especially when certain\nfactors of variation are difficult to label. In this paper, we introduce a\nnovel architecture that disentangles the latent space into two complementary\nsubspaces by using only weak supervision in form of pairwise similarity labels.\nInspired by the recent success of cycle-consistent adversarial architectures,\nwe use cycle-consistency in a variational auto-encoder framework. Our\nnon-adversarial approach is in contrast with the recent works that combine\nadversarial training with auto-encoders to disentangle representations. We show\ncompelling results of disentangled latent subspaces on three datasets and\ncompare with recent works that leverage adversarial training.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 12:37:35 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Jha", "Ananya Harsh", ""], ["Anand", "Saket", ""], ["Singh", "Maneesh", ""], ["Veeravasarapu", "V. S. R.", ""]]}, {"id": "1804.10481", "submitter": "Yinghuan Shi", "authors": "Jinquan Sun, Yinghuan Shi, Yang Gao, Lei Wang, Luping Zhou, Wanqi\n  Yang, Dinggang Shen", "title": "Interactive Medical Image Segmentation via Point-Based Interaction and\n  Sequential Patch Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to low tissue contrast, irregular object appearance, and unpredictable\nlocation variation, segmenting the objects from different medical imaging\nmodalities (e.g., CT, MR) is considered as an important yet challenging task.\nIn this paper, we present a novel method for interactive medical image\nsegmentation with the following merits. (1) Our design is fundamentally\ndifferent from previous pure patch-based and image-based segmentation methods.\nWe observe that during delineation, the physician repeatedly check the\ninside-outside intensity changing to determine the boundary, which indicates\nthat comparison in an inside-outside manner is extremely important. Thus, we\ninnovatively model our segmentation task as learning the representation of the\nbi-directional sequential patches, starting from (or ending in) the given\ncentral point of the object. This can be realized by our proposed ConvRNN\nnetwork embedded with a gated memory propagation unit. (2) Unlike previous\ninteractive methods (requiring bounding box or seed points), we only ask the\nphysician to merely click on the rough central point of the object before\nsegmentation, which could simultaneously enhance the performance and reduce the\nsegmentation time. (3) We utilize our method in a multi-level framework for\nbetter performance. We systematically evaluate our method in three different\nsegmentation tasks including CT kidney tumor, MR prostate, and PROMISE12\nchallenge, showing promising results compared with state-of-the-art methods.\nThe code is available here:\n\\href{https://github.com/sunalbert/Sequential-patch-based-segmentation}{Sequential-patch-based-segmentation}.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 13:03:42 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 02:51:33 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Sun", "Jinquan", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""], ["Wang", "Lei", ""], ["Zhou", "Luping", ""], ["Yang", "Wanqi", ""], ["Shen", "Dinggang", ""]]}, {"id": "1804.10484", "submitter": "Yinghuan Shi", "authors": "Qian Yu, Yinghuan Shi, Jinquan Sun, Yang Gao, Yakang Dai, Jianbing Zhu", "title": "Crossbar-Net: A Novel Convolutional Network for Kidney Tumor\n  Segmentation in CT Images", "comments": "This paper includes 15 pages and 16 figures", "journal-ref": null, "doi": "10.1109/TIP.2019.2905537", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the irregular motion, similar appearance and diverse shape, accurate\nsegmentation of kidney tumor in CT images is a difficult and challenging task.\nTo this end, we present a novel automatic segmentation method, termed as\nCrossbar-Net, with the goal of accurate segmenting the kidney tumors. Firstly,\nconsidering that the traditional learning-based segmentation methods normally\nemploy either whole images or squared patches as the training samples, we\ninnovatively sample the orthogonal non-squared patches (namely crossbar\npatches), to fully cover the whole kidney tumors in either horizontal or\nvertical directions. These sampled crossbar patches could not only represent\nthe detailed local information of kidney tumor as the traditional patches, but\nalso describe the global appearance from either horizontal or vertical\ndirection using contextual information. Secondly, with the obtained crossbar\npatches, we trained a convolutional neural network with two sub-models (i.e.,\nhorizontal sub-model and vertical sub-model) in a cascaded manner, to integrate\nthe segmentation results from two directions (i.e., horizontal and vertical).\nThis cascaded training strategy could effectively guarantee the consistency\nbetween sub-models, by feeding each other with the most difficult samples, for\na better segmentation. In the experiment, we evaluate our method on a real CT\nkidney tumor dataset, collected from 94 different patients including 3,500\nimages. Compared with the state-of-the-art segmentation methods, the results\ndemonstrate the superior results of our method on dice ratio score, true\npositive fraction, centroid distance and Hausdorff distance. Moreover, we have\nextended our crossbar-net to a different task: cardiac segmentation, showing\nthe promising results for the better generalization.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 13:09:27 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 13:39:34 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2018 12:19:34 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2020 01:50:39 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Yu", "Qian", ""], ["Shi", "Yinghuan", ""], ["Sun", "Jinquan", ""], ["Gao", "Yang", ""], ["Dai", "Yakang", ""], ["Zhu", "Jianbing", ""]]}, {"id": "1804.10541", "submitter": "Lars K\\\"onig", "authors": "Lars K\\\"onig, Jan R\\\"uhaak, Alexander Derksen, Jan Lellmann", "title": "A matrix-free approach to parallel and memory-efficient deformable image\n  registration", "comments": "Accepted for publication in SIAM Journal on Scientific Computing\n  (SISC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel computational approach to fast and memory-efficient\ndeformable image registration. In the variational registration model, the\ncomputation of the objective function derivatives is the computationally most\nexpensive operation, both in terms of runtime and memory requirements. In order\nto target this bottleneck, we analyze the matrix structure of gradient and\nHessian computations for the case of the normalized gradient fields distance\nmeasure and curvature regularization. Based on this analysis, we derive\nequivalent matrix-free closed-form expressions for derivative computations,\neliminating the need for storing intermediate results and the costs of sparse\nmatrix arithmetic. This has further benefits: (1) matrix computations can be\nfully parallelized, (2) memory complexity for derivative computation is reduced\nfrom linear to constant, and (3) overall computation times are substantially\nreduced.\n  In comparison with an optimized matrix-based reference implementation, the\nCPU implementation achieves speedup factors between 3.1 and 9.7, and we are\nable to handle substantially higher resolutions. Using a GPU implementation, we\nachieve an additional speedup factor of up to 9.2.\n  Furthermore, we evaluated the approach on real-world medical datasets. On ten\npublicly available lung CT images from the DIR-Lab 4DCT dataset, we achieve the\nbest mean landmark error of 0.93 mm compared to other submissions on the\nDIR-Lab website, with an average runtime of only 9.23 s. Complete non-rigid\nregistration of full-size 3D thorax-abdomen CT volumes from oncological\nfollow-up is achieved in 12.6 s. The experimental results show that the\nproposed matrix-free algorithm enables the use of variational registration\nmodels also in applications which were previously impractical due to memory or\nruntime restrictions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 14:52:41 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["K\u00f6nig", "Lars", ""], ["R\u00fchaak", "Jan", ""], ["Derksen", "Alexander", ""], ["Lellmann", "Jan", ""]]}, {"id": "1804.10652", "submitter": "Xiao Lin", "authors": "Xiao Lin and Mohamed R. Amer", "title": "Human Motion Modeling using DVGANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel generative model for human motion modeling using\nGenerative Adversarial Networks (GANs). We formulate the GAN discriminator\nusing dense validation at each time-scale and perturb the discriminator input\nto make it translation invariant. Our model is capable of motion generation and\ncompletion. We show through our evaluations the resiliency to noise,\ngeneralization over actions, and generation of long diverse sequences. We\nevaluate our approach on Human 3.6M and CMU motion capture datasets using\ninception scores.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 19:13:34 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 16:52:24 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Lin", "Xiao", ""], ["Amer", "Mohamed R.", ""]]}, {"id": "1804.10660", "submitter": "Ji Zhang", "authors": "Ji Zhang, Yannis Kalantidis, Marcus Rohrbach, Manohar Paluri, Ahmed\n  Elgammal, Mohamed Elhoseiny", "title": "Large-Scale Visual Relationship Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale visual understanding is challenging, as it requires a model to\nhandle the widely-spread and imbalanced distribution of <subject, relation,\nobject> triples. In real-world scenarios with large numbers of objects and\nrelations, some are seen very commonly while others are barely seen. We develop\na new relationship detection model that embeds objects and relations into two\nvector spaces where both discriminative capability and semantic affinity are\npreserved. We learn both a visual and a semantic module that map features from\nthe two modalities into a shared space, where matched pairs of features have to\ndiscriminate against those unmatched, but also maintain close distances to\nsemantically similar ones. Benefiting from that, our model can achieve superior\nperformance even when the visual entity categories scale up to more than\n80,000, with extremely skewed class distribution. We demonstrate the efficacy\nof our model on a large and imbalanced benchmark based of Visual Genome that\ncomprises 53,000+ objects and 29,000+ relations, a scale at which no previous\nwork has ever been evaluated at. We show superiority of our model over\ncarefully designed baselines on the original Visual Genome dataset with 80,000+\ncategories. We also show state-of-the-art performance on the VRD dataset and\nthe scene graph dataset which is a subset of Visual Genome with 200 categories.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 19:41:39 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 21:57:59 GMT"}, {"version": "v3", "created": "Tue, 29 Jan 2019 07:51:45 GMT"}, {"version": "v4", "created": "Fri, 16 Aug 2019 20:58:18 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Zhang", "Ji", ""], ["Kalantidis", "Yannis", ""], ["Rohrbach", "Marcus", ""], ["Paluri", "Manohar", ""], ["Elgammal", "Ahmed", ""], ["Elhoseiny", "Mohamed", ""]]}, {"id": "1804.10684", "submitter": "Fengze Liu", "authors": "Fengze Liu, Lingxi Xie, Yingda Xia, Elliot K.Fishman, Alan L.Yuille", "title": "Joint Shape Representation and Classification for Detecting PDAC", "comments": "Accepted to MICCAI 2019 Workshop(MLMI)(8 pages, 3 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to detect pancreatic ductal adenocarcinoma (PDAC) in abdominal CT\nscans, which sheds light on early diagnosis of pancreatic cancer. This is a 3D\nvolume classification task with little training data. We propose a two-stage\nframework, which first segments the pancreas into a binary mask, then\ncompresses the mask into a shape vector and performs abnormality\nclassification. Shape representation and classification are performed in a\njoint manner, both to exploit the knowledge that PDAC often changes the shape\nof the pancreas and to prevent over-fitting. Experiments are performed on 300\nnormal scans and 136 PDAC cases. We achieve a specificity of 90.2% (false alarm\noccurs on less than 1/10 normal cases) at a sensitivity of 80.2% (less than 1/5\nPDAC cases are not detected), which show promise for clinical applications.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 20:38:36 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 01:26:35 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Liu", "Fengze", ""], ["Xie", "Lingxi", ""], ["Xia", "Yingda", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1804.10687", "submitter": "Tomasz Trzcinski", "authors": "Adam S{\\l}ucki, Tomasz Trzcinski, Adam Bielski, Pawe{\\l} Cyrta", "title": "Extracting textual overlays from social media videos using neural\n  networks", "comments": "International Conference on Computer Vision and Graphics (ICCVG) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textual overlays are often used in social media videos as people who watch\nthem without the sound would otherwise miss essential information conveyed in\nthe audio stream. This is why extraction of those overlays can serve as an\nimportant meta-data source, e.g. for content classification or retrieval tasks.\nIn this work, we present a robust method for extracting textual overlays from\nvideos that builds up on multiple neural network architectures. The proposed\nsolution relies on several processing steps: keyframe extraction, text\ndetection and text recognition. The main component of our system, i.e. the text\nrecognition module, is inspired by a convolutional recurrent neural network\narchitecture and we improve its performance using synthetically generated\ndataset of over 600,000 images with text prepared by authors specifically for\nthis task. We also develop a filtering method that reduces the amount of\noverlapping text phrases using Levenshtein distance and further boosts system's\nperformance. The final accuracy of our solution reaches over 80A% and is au\npair with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 21:09:20 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 17:35:47 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["S\u0142ucki", "Adam", ""], ["Trzcinski", "Tomasz", ""], ["Bielski", "Adam", ""], ["Cyrta", "Pawe\u0142", ""]]}, {"id": "1804.10692", "submitter": "Hsiao-Yu Tung", "authors": "Hsiao-Yu Fish Tung, Adam W. Harley, Liang-Kang Huang, Katerina\n  Fragkiadaki", "title": "Reward Learning from Narrated Demonstrations", "comments": "The work has been accepted to Conference on Computer Vision and\n  Pattern Recognition (CVPR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans effortlessly \"program\" one another by communicating goals and desires\nin natural language. In contrast, humans program robotic behaviours by\nindicating desired object locations and poses to be achieved, by providing RGB\nimages of goal configurations, or supplying a demonstration to be imitated.\nNone of these methods generalize across environment variations, and they convey\nthe goal in awkward technical terms. This work proposes joint learning of\nnatural language grounding and instructable behavioural policies reinforced by\nperceptual detectors of natural language expressions, grounded to the sensory\ninputs of the robotic agent. Our supervision is narrated visual\ndemonstrations(NVD), which are visual demonstrations paired with verbal\nnarration (as opposed to being silent). We introduce a dataset of NVD where\nteachers perform activities while describing them in detail. We map the\nteachers' descriptions to perceptual reward detectors, and use them to train\ncorresponding behavioural policies in simulation.We empirically show that our\ninstructable agents (i) learn visual reward detectors using a small number of\nexamples by exploiting hard negative mined configurations from demonstration\ndynamics, (ii) develop pick-and place policies using learned visual reward\ndetectors, (iii) benefit from object-factorized state representations that\nmimic the syntactic structure of natural language goal expressions, and (iv)\ncan execute behaviours that involve novel objects in novel locations at test\ntime, instructed by natural language.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 21:26:08 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Tung", "Hsiao-Yu Fish", ""], ["Harley", "Adam W.", ""], ["Huang", "Liang-Kang", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "1804.10704", "submitter": "Jeovane Hon\\'orio Alves", "authors": "Jeovane Hon\\'orio Alves and Pedro Martins Moreira Neto and Lucas\n  Ferrari de Oliveira", "title": "Extracting Lungs from CT Images using Fully Convolutional Networks", "comments": "Accepted for presentation at the International Joint Conference on\n  Neural Networks (IJCNN) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of cancer and other pathological diseases, like the interstitial\nlung diseases (ILDs), is usually possible through Computed Tomography (CT)\nscans. To aid this, a preprocessing step of segmentation is performed to reduce\nthe area to be analyzed, segmenting the lungs and removing unimportant regions.\nGenerally, complex methods are developed to extract the lung region, also using\nhand-made feature extractors to enhance segmentation. With the popularity of\ndeep learning techniques and its automated feature learning, we propose a lung\nsegmentation approach using fully convolutional networks (FCNs) combined with\nfully connected conditional random fields (CRF), employed in many\nstate-of-the-art segmentation works. Aiming to develop a generalized approach,\nthe publicly available datasets from University Hospitals of Geneva (HUG) and\nVESSEL12 challenge were studied, including many healthy and pathological CT\nscans for evaluation. Experiments using the dataset individually, its trained\nmodel on the other dataset and a combination of both datasets were employed.\nDice scores of $98.67\\%\\pm0.94\\%$ for the HUG-ILD dataset and\n$99.19\\%\\pm0.37\\%$ for the VESSEL12 dataset were achieved, outperforming works\nin the former and obtaining similar state-of-the-art results in the latter\ndataset, showing the capability in using deep learning approaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 22:27:06 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Alves", "Jeovane Hon\u00f3rio", ""], ["Neto", "Pedro Martins Moreira", ""], ["de Oliveira", "Lucas Ferrari", ""]]}, {"id": "1804.10735", "submitter": "Xiaohuan Cao", "authors": "Xiaohuan Cao, Jianhua Yang, Li Wang, Zhong Xue, Qian Wang and Dinggang\n  Shen", "title": "Deep Learning based Inter-Modality Image Registration Supervised by\n  Intra-Modality Similarity", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-rigid inter-modality registration can facilitate accurate information\nfusion from different modalities, but it is challenging due to the very\ndifferent image appearances across modalities. In this paper, we propose to\ntrain a non-rigid inter-modality image registration network, which can directly\npredict the transformation field from the input multimodal images, such as CT\nand MR images. In particular, the training of our inter-modality registration\nnetwork is supervised by intra-modality similarity metric based on the\navailable paired data, which is derived from a pre-aligned CT and MR dataset.\nSpecifically, in the training stage, to register the input CT and MR images,\ntheir similarity is evaluated on the warped MR image and the MR image that is\npaired with the input CT. So that, the intra-modality similarity metric can be\ndirectly applied to measure whether the input CT and MR images are well\nregistered. Moreover, we use the idea of dual-modality fashion, in which we\nmeasure the similarity on both CT modality and MR modality. In this way, the\ncomplementary anatomies in both modalities can be jointly considered to more\naccurately train the inter-modality registration network. In the testing stage,\nthe trained inter-modality registration network can be directly applied to\nregister the new multimodal images without any paired data. Experimental\nresults have shown that, the proposed method can achieve promising accuracy and\nefficiency for the challenging non-rigid inter-modality registration task and\nalso outperforms the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 03:53:42 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Cao", "Xiaohuan", ""], ["Yang", "Jianhua", ""], ["Wang", "Li", ""], ["Xue", "Zhong", ""], ["Wang", "Qian", ""], ["Shen", "Dinggang", ""]]}, {"id": "1804.10743", "submitter": "Ce Qi", "authors": "Ce Qi, Xiaoping Chen, Pingyu Wang, Fei Su", "title": "Precise Box Score: Extract More Information from Datasets to Improve the\n  Performance of Face Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the training of face detection network based on R-CNN framework, anchors\nare assigned to be positive samples if intersection-over-unions (IoUs) with\nground-truth are higher than the first threshold(such as 0.7); and to be\nnegative samples if their IoUs are lower than the second threshold(such as\n0.3). And the face detection model is trained by the above labels. However,\nanchors with IoU between first threshold and second threshold are not used. We\npropose a novel training strategy, Precise Box Score(PBS), to train object\ndetection models. The proposed training strategy uses the anchors with IoUs\nbetween the first and second threshold, which can consistently improve the\nperformance of face detection. Our proposed training strategy extracts more\ninformation from datasets, making better utilization of existing datasets.\nWhat's more, we also introduce a simple but effective model compression\nmethod(SEMCM), which can boost the performance of face detectors further.\nExperimental results show that the performance of face detection network can\nconsistently be improved based on our proposed scheme.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 05:11:19 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Qi", "Ce", ""], ["Chen", "Xiaoping", ""], ["Wang", "Pingyu", ""], ["Su", "Fei", ""]]}, {"id": "1804.10750", "submitter": "Vincent Lui", "authors": "Vincent Lui and Jonathon Geeves and Winston Yii and Tom Drummond", "title": "Efficient Subpixel Refinement with Symbolic Linear Predictors", "comments": "IEEE/CVF International Conference on Computer Vision and Pattern\n  Recognition 2018 (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient subpixel refinement method usinga learning-based\napproach called Linear Predictors. Two key ideas are shown in this paper.\nFirstly, we present a novel technique, called Symbolic Linear Predictors, which\nmakes the learning step efficient for subpixel refinement. This makes our\napproach feasible for online applications without compromising accuracy, while\ntaking advantage of the run-time efficiency of learning based approaches.\nSecondly, we show how Linear Predictors can be used to predict the expected\nalignment error, allowing us to use only the best keypoints in resource\nconstrained applications. We show the efficiency and accuracy of our method\nthrough extensive experiments.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 06:31:05 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Lui", "Vincent", ""], ["Geeves", "Jonathon", ""], ["Yii", "Winston", ""], ["Drummond", "Tom", ""]]}, {"id": "1804.10764", "submitter": "Christian Wachinger", "authors": "Christian Wachinger and Benjamin Gutierrez Becker and Anna Rieckmann", "title": "Detect, Quantify, and Incorporate Dataset Bias: A Neuroimaging Analysis\n  on 12,207 Individuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimaging datasets keep growing in size to address increasingly complex\nmedical questions. However, even the largest datasets today alone are too small\nfor training complex models or for finding genome wide associations. A solution\nis to grow the sample size by merging data across several datasets. However,\nbias in datasets complicates this approach and includes additional sources of\nvariation in the data instead. In this work, we combine 15 large neuroimaging\ndatasets to study bias. First, we detect bias by demonstrating that scans can\nbe correctly assigned to a dataset with 73.3% accuracy. Next, we introduce\nmetrics to quantify the compatibility across datasets and to create embeddings\nof neuroimaging sites. Finally, we incorporate the presence of bias for the\nselection of a training set for predicting autism. For the quantification of\nthe dataset bias, we introduce two metrics: the Bhattacharyya distance between\ndatasets and the age prediction error. The presented embedding of neuroimaging\nsites provides an interesting new visualization about the similarity of\ndifferent sites. This could be used to guide the merging of data sources, while\nlimiting the introduction of unwanted variation. Finally, we demonstrate a\nclear performance increase when incorporating dataset bias for training set\nselection in autism prediction. Overall, we believe that the growing amount of\nneuroimaging data necessitates to incorporate data-driven methods for\nquantifying dataset bias in future analyses.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 09:11:34 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Wachinger", "Christian", ""], ["Becker", "Benjamin Gutierrez", ""], ["Rieckmann", "Anna", ""]]}, {"id": "1804.10798", "submitter": "Risheng Liu", "authors": "Risheng Liu, Shichao Cheng, Yi He, Xin Fan, Zhongxuan Luo", "title": "Toward Designing Convergent Deep Operator Splitting Methods for\n  Task-specific Nonconvex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operator splitting methods have been successfully used in computational\nsciences, statistics, learning and vision areas to reduce complex problems into\na series of simpler subproblems. However, prevalent splitting schemes are\nmostly established only based on the mathematical properties of some general\noptimization models. So it is a laborious process and often requires many\niterations of ideation and validation to obtain practical and task-specific\noptimal solutions, especially for nonconvex problems in real-world scenarios.\nTo break through the above limits, we introduce a new algorithmic framework,\ncalled Learnable Bregman Splitting (LBS), to perform deep-architecture-based\noperator splitting for nonconvex optimization based on specific task model.\nThanks to the data-dependent (i.e., learnable) nature, our LBS can not only\nspeed up the convergence, but also avoid unwanted trivial solutions for\nreal-world tasks. Though with inexact deep iterations, we can still establish\nthe global convergence and estimate the asymptotic convergence rate of LBS only\nby enforcing some fairly loose assumptions. Extensive experiments on different\napplications (e.g., image completion and deblurring) verify our theoretical\nresults and show the superiority of LBS against existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 12:59:50 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Liu", "Risheng", ""], ["Cheng", "Shichao", ""], ["He", "Yi", ""], ["Fan", "Xin", ""], ["Luo", "Zhongxuan", ""]]}, {"id": "1804.10805", "submitter": "Muhammet Bastan", "authors": "Muhammet Bastan, Kim-Hui Yap, Lap-Pui Chau", "title": "Remote Detection of Idling Cars Using Infrared Imaging and Deep Networks", "comments": "Neural Computing and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Idling vehicles waste energy and pollute the environment through exhaust\nemission. In some countries, idling a vehicle for more than a predefined\nduration is prohibited and automatic idling vehicle detection is desirable for\nlaw enforcement. We propose the first automatic system to detect idling cars,\nusing infrared (IR) imaging and deep networks.\n  We rely on the differences in spatio-temporal heat signatures of idling and\nstopped cars and monitor the car temperature with a long-wavelength IR camera.\nWe formulate the idling car detection problem as spatio-temporal event\ndetection in IR image sequences and employ deep networks for spatio-temporal\nmodeling. We collected the first IR image sequence dataset for idling car\ndetection. First, we detect the cars in each IR image using a convolutional\nneural network, which is pre-trained on regular RGB images and fine-tuned on IR\nimages for higher accuracy. Then, we track the detected cars over time to\nidentify the cars that are parked. Finally, we use the 3D spatio-temporal IR\nimage volume of each parked car as input to convolutional and recurrent\nnetworks to classify them as idling or not. We carried out an extensive\nempirical evaluation of temporal and spatio-temporal modeling approaches with\nvarious convolutional and recurrent architectures. We present promising\nexperimental results on our IR image sequence dataset.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 14:01:58 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Bastan", "Muhammet", ""], ["Yap", "Kim-Hui", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "1804.10819", "submitter": "Anjan Dutta", "authors": "Sounak Dey and Anjan Dutta and Suman K. Ghosh and Ernest Valveny and\n  Josep Llad\\'os and Umapada Pal", "title": "Learning Cross-Modal Deep Embeddings for Multi-Object Image Retrieval\n  using Text and Sketch", "comments": "Accepted at ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a cross modal image retrieval system that allows\nboth text and sketch as input modalities for the query. A cross-modal deep\nnetwork architecture is formulated to jointly model the sketch and text input\nmodalities as well as the the image output modality, learning a common\nembedding between text and images and between sketches and images. In addition,\nan attention model is used to selectively focus the attention on the different\nobjects of the image, allowing for retrieval with multiple objects in the\nquery. Experiments show that the proposed method performs the best in both\nsingle and multiple object image retrieval in standard datasets.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 15:23:25 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Dey", "Sounak", ""], ["Dutta", "Anjan", ""], ["Ghosh", "Suman K.", ""], ["Valveny", "Ernest", ""], ["Llad\u00f3s", "Josep", ""], ["Pal", "Umapada", ""]]}, {"id": "1804.10822", "submitter": "Raphael Abreu", "authors": "Raphael Abreu, Joel dos Santos and Eduardo Bezerra", "title": "A Bimodal Learning Approach to Assist Multi-sensory Effects\n  Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In mulsemedia applications, traditional media content (text, image, audio,\nvideo, etc.) can be related to media objects that target other human senses\n(e.g., smell, haptics, taste). Such applications aim at bridging the virtual\nand real worlds through sensors and actuators. Actuators are responsible for\nthe execution of sensory effects (e.g., wind, heat, light), which produce\nsensory stimulations on the users. In these applications sensory stimulation\nmust happen in a timely manner regarding the other traditional media content\nbeing presented. For example, at the moment in which an explosion is presented\nin the audiovisual content, it may be adequate to activate actuators that\nproduce heat and light. It is common to use some declarative multimedia\nauthoring language to relate the timestamp in which each media object is to be\npresented to the execution of some sensory effect. One problem in this setting\nis that the synchronization of media objects and sensory effects is done\nmanually by the author(s) of the application, a process which is time-consuming\nand error prone. In this paper, we present a bimodal neural network\narchitecture to assist the synchronization task in mulsemedia applications. Our\napproach is based on the idea that audio and video signals can be used\nsimultaneously to identify the timestamps in which some sensory effect should\nbe executed. Our learning architecture combines audio and video signals for the\nprediction of scene components. For evaluation purposes, we construct a dataset\nbased on Google's AudioSet. We provide experiments to validate our bimodal\narchitecture. Our results show that the bimodal approach produces better\nresults when compared to several variants of unimodal architectures.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 15:37:41 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Abreu", "Raphael", ""], ["Santos", "Joel dos", ""], ["Bezerra", "Eduardo", ""]]}, {"id": "1804.10844", "submitter": "Minki Chung", "authors": "Minki Chung, Sungzoon Cho", "title": "CRAM: Clued Recurrent Attention Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To overcome the poor scalability of convolutional neural network, recurrent\nattention model(RAM) selectively choose what and where to look on the image. By\ndirecting recurrent attention model how to look the image, RAM can be even more\nsuccessful in that the given clue narrow down the scope of the possible focus\nzone. In this perspective, this work proposes clued recurrent attention model\n(CRAM) which add clue or constraint on the RAM better problem solving. CRAM\nfollows encoder-decoder framework, encoder utilizes recurrent attention model\nwith spatial transformer network and decoder which varies depending on the\ntask. To ensure the performance, CRAM tackles two computer vision task. One is\nthe image classification task, with clue given as the binary image saliency\nwhich indicates the approximate location of object. The other is the inpainting\ntask, with clue given as binary mask which indicates the occluded part. In both\ntasks, CRAM shows better performance than existing methods showing the\nsuccessful extension of RAM.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 19:27:43 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Chung", "Minki", ""], ["Cho", "Sungzoon", ""]]}, {"id": "1804.10851", "submitter": "Qi Dong", "authors": "Qi Dong, Shaogang Gong, Xiatian Zhu", "title": "Imbalanced Deep Learning by Minority Class Incremental Rectification", "comments": "Accepted for IEEE Trans. Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model learning from class imbalanced training data is a long-standing and\nsignificant challenge for machine learning. In particular, existing deep\nlearning methods consider mostly either class balanced data or moderately\nimbalanced data in model training, and ignore the challenge of learning from\nsignificantly imbalanced training data. To address this problem, we formulate a\nclass imbalanced deep learning model based on batch-wise incremental minority\n(sparsely sampled) class rectification by hard sample mining in majority\n(frequently sampled) classes during model training. This model is designed to\nminimise the dominant effect of majority classes by discovering sparsely\nsampled boundaries of minority classes in an iterative batch-wise learning\nprocess. To that end, we introduce a Class Rectification Loss (CRL) function\nthat can be deployed readily in deep network architectures. Extensive\nexperimental evaluations are conducted on three imbalanced person attribute\nbenchmark datasets (CelebA, X-Domain, DeepFashion) and one balanced object\ncategory benchmark dataset (CIFAR-100). These experimental results demonstrate\nthe performance advantages and model scalability of the proposed batch-wise\nincremental minority class rectification model over the existing\nstate-of-the-art models for addressing the problem of imbalanced data learning.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 22:36:19 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Dong", "Qi", ""], ["Gong", "Shaogang", ""], ["Zhu", "Xiatian", ""]]}, {"id": "1804.10855", "submitter": "Novanto Yudistira", "authors": "Novanto Yudistira, Achmad Ridok, Ali Fauzi", "title": "Evaluation of Feature Detector-Descriptor for Real Object Matching under\n  Various Conditions of Ilumination and Affine Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study attempts to provide explanations, descriptions and evaluations of\nsome most popular and current combinations of description and descriptor\nframeworks, namely SIFT, SURF, MSER, and BRISK for keypoint extractors and\nSIFT, SURF, BRISK, and FREAK for descriptors. Evaluations are made based on the\nnumber of matches of keypoints and repeatability in various image variations.\nIt is used as the main parameter to assess how well combinations of algorithms\nare in matching objects with different variations. There are many papers that\ndescribe the comparison of detection and description features to detect objects\nin images under various conditions, but the combination of algorithms attached\nto them has not been much discussed. The problem domain is limited to different\nillumination levels and affine transformations from different perspectives. To\nevaluate the robustness of all combinations of algorithms, we use a stereo\nimage matching case.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 23:55:45 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 02:24:30 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Yudistira", "Novanto", ""], ["Ridok", "Achmad", ""], ["Fauzi", "Ali", ""]]}, {"id": "1804.10871", "submitter": "Cong Phuoc Huynh", "authors": "Cong Phuoc Huynh, Arridhana Ciptadi, Ambrish Tyagi and Amit Agrawal\n  (Amazon.com)", "title": "CRAFT: Complementary Recommendations Using Adversarial Feature\n  Transformer", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Traditional approaches for complementary product recommendations rely on\nbehavioral and non-visual data such as customer co-views or co-buys. However,\ncertain domains such as fashion are primarily visual. We propose a framework\nthat harnesses visual cues in an unsupervised manner to learn the distribution\nof co-occurring complementary items in real world images. Our model learns a\nnon-linear transformation between the two manifolds of source and target\ncomplementary item categories (e.g., tops and bottoms in outfits). Given a\nlarge dataset of images containing instances of co-occurring object categories,\nwe train a generative transformer network directly on the feature\nrepresentation space by casting it as an adversarial optimization problem. Such\na conditional generative model can produce multiple novel samples of\ncomplementary items (in the feature space) for a given query item. The final\nrecommendations are selected from the closest real world examples to the\nsynthesized complementary features. We apply our framework to the task of\nrecommending complementary tops for a given bottom clothing item. The\nrecommendations made by our system are diverse, and are favored by human\nexperts over the baseline approaches.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 04:52:06 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 01:54:32 GMT"}, {"version": "v3", "created": "Mon, 10 Sep 2018 06:58:24 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Huynh", "Cong Phuoc", "", "Amazon.com"], ["Ciptadi", "Arridhana", "", "Amazon.com"], ["Tyagi", "Ambrish", "", "Amazon.com"], ["Agrawal", "Amit", "", "Amazon.com"]]}, {"id": "1804.10879", "submitter": "Kai Yue", "authors": "Kai Yue, Lei Yang, Ruirui Li, Wei Hu, Fan Zhang, Wei Li", "title": "TreeSegNet: Adaptive Tree CNNs for Subdecimeter Aerial Image\n  Segmentation", "comments": "40 pages; 16 figures; 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the task of subdecimeter aerial imagery segmentation, fine-grained\nsemantic segmentation results are usually difficult to obtain because of\ncomplex remote sensing content and optical conditions. Recently, convolutional\nneural networks (CNNs) have shown outstanding performance on this task.\nAlthough many deep neural network structures and techniques have been applied\nto improve the accuracy, few have paid attention to better differentiating the\neasily confused classes. In this paper, we propose TreeSegNet which adopts an\nadaptive network to increase the classification rate at the pixelwise level.\nSpecifically, based on the infrastructure of DeepUNet, a Tree-CNN block in\nwhich each node represents a ResNeXt unit is constructed adaptively according\nto the confusion matrix and the proposed TreeCutting algorithm. By transporting\nfeature maps through concatenating connections, the Tree-CNN block fuses\nmultiscale features and learns best weights for the model. In experiments on\nthe ISPRS 2D semantic labeling Potsdam dataset, the results obtained by\nTreeSegNet are better than those of other published state-of-the-art methods.\nDetailed comparison and analysis show that the improvement brought by the\nadaptive Tree-CNN block is significant.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 06:17:00 GMT"}, {"version": "v2", "created": "Sat, 25 Aug 2018 16:48:14 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Yue", "Kai", ""], ["Yang", "Lei", ""], ["Li", "Ruirui", ""], ["Hu", "Wei", ""], ["Zhang", "Fan", ""], ["Li", "Wei", ""]]}, {"id": "1804.10892", "submitter": "Radu Tudor Ionescu", "authors": "Mariana-Iuliana Georgescu, Radu Tudor Ionescu, Marius Popescu", "title": "Local Learning with Deep and Handcrafted Features for Facial Expression\n  Recognition", "comments": "Accepted in IEEE Access", "journal-ref": "in IEEE Access, vol. 7, pp. 64827-64836, 2019", "doi": "10.1109/ACCESS.2019.2917266", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approach that combines automatic features learned by\nconvolutional neural networks (CNN) and handcrafted features computed by the\nbag-of-visual-words (BOVW) model in order to achieve state-of-the-art results\nin facial expression recognition. To obtain automatic features, we experiment\nwith multiple CNN architectures, pre-trained models and training procedures,\ne.g. Dense-Sparse-Dense. After fusing the two types of features, we employ a\nlocal learning framework to predict the class label for each test image. The\nlocal learning framework is based on three steps. First, a k-nearest neighbors\nmodel is applied in order to select the nearest training samples for an input\ntest image. Second, a one-versus-all Support Vector Machines (SVM) classifier\nis trained on the selected training samples. Finally, the SVM classifier is\nused to predict the class label only for the test image it was trained for.\nAlthough we have used local learning in combination with handcrafted features\nin our previous work, to the best of our knowledge, local learning has never\nbeen employed in combination with deep features. The experiments on the 2013\nFacial Expression Recognition (FER) Challenge data set, the FER+ data set and\nthe AffectNet data set demonstrate that our approach achieves state-of-the-art\nresults. With a top accuracy of 75.42% on FER 2013, 87.76% on the FER+, 59.58%\non AffectNet 8-way classification and 63.31% on AffectNet 7-way classification,\nwe surpass the state-of-the-art methods by more than 1% on all data sets.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 09:12:13 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 10:36:17 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2018 14:20:10 GMT"}, {"version": "v4", "created": "Tue, 25 Sep 2018 13:21:17 GMT"}, {"version": "v5", "created": "Thu, 10 Jan 2019 12:26:07 GMT"}, {"version": "v6", "created": "Mon, 13 May 2019 18:15:00 GMT"}, {"version": "v7", "created": "Thu, 12 Mar 2020 18:17:04 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Georgescu", "Mariana-Iuliana", ""], ["Ionescu", "Radu Tudor", ""], ["Popescu", "Marius", ""]]}, {"id": "1804.10899", "submitter": "Bowen Wu", "authors": "Bowen Wu, Huaming Wu, Monica M.Y. Zhang", "title": "Scalable Angular Discriminative Deep Metric Learning for Face\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of deep learning, Deep Metric Learning (DML) has\nachieved great improvements in face recognition. Specifically, the widely used\nsoftmax loss in the training process often bring large intra-class variations,\nand feature normalization is only exploited in the testing process to compute\nthe pair similarities. To bridge the gap, we impose the intra-class cosine\nsimilarity between the features and weight vectors in softmax loss larger than\na margin in the training step, and extend it from four aspects. First, we\nexplore the effect of a hard sample mining strategy. To alleviate the human\nlabor of adjusting the margin hyper-parameter, a self-adaptive margin updating\nstrategy is proposed. Then, a normalized version is given to take full\nadvantage of the cosine similarity constraint. Furthermore, we enhance the\nformer constraint to force the intra-class cosine similarity larger than the\nmean inter-class cosine similarity with a margin in the exponential feature\nprojection space. Extensive experiments on Labeled Face in the Wild (LFW),\nYoutube Faces (YTF) and IARPA Janus Benchmark A (IJB-A) datasets demonstrate\nthat the proposed methods outperform the mainstream DML methods and approach\nthe state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 09:40:46 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 01:30:56 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Wu", "Bowen", ""], ["Wu", "Huaming", ""], ["Zhang", "Monica M. Y.", ""]]}, {"id": "1804.10916", "submitter": "Qi Dou", "authors": "Qi Dou, Cheng Ouyang, Cheng Chen, Hao Chen, Pheng-Ann Heng", "title": "Unsupervised Cross-Modality Domain Adaptation of ConvNets for Biomedical\n  Image Segmentations with Adversarial Loss", "comments": "IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks (ConvNets) have achieved great successes in various\nchallenging vision tasks. However, the performance of ConvNets would degrade\nwhen encountering the domain shift. The domain adaptation is more significant\nwhile challenging in the field of biomedical image analysis, where\ncross-modality data have largely different distributions. Given that annotating\nthe medical data is especially expensive, the supervised transfer learning\napproaches are not quite optimal. In this paper, we propose an unsupervised\ndomain adaptation framework with adversarial learning for cross-modality\nbiomedical image segmentations. Specifically, our model is based on a dilated\nfully convolutional network for pixel-wise prediction. Moreover, we build a\nplug-and-play domain adaptation module (DAM) to map the target input to\nfeatures which are aligned with source domain feature space. A domain critic\nmodule (DCM) is set up for discriminating the feature space of both domains. We\noptimize the DAM and DCM via an adversarial loss without using any target\ndomain label. Our proposed method is validated by adapting a ConvNet trained\nwith MRI images to unpaired CT data for cardiac structures segmentations, and\nachieved very promising results.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 12:19:53 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 03:12:15 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Dou", "Qi", ""], ["Ouyang", "Cheng", ""], ["Chen", "Cheng", ""], ["Chen", "Hao", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1804.10938", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A. Nicolaou,\n  Athanasios Papaioannou, Guoying Zhao, Bj\\\"orn Schuller, Irene Kotsia,\n  Stefanos Zafeiriou", "title": "Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge,\n  Deep Architectures, and Beyond", "comments": null, "journal-ref": null, "doi": "10.1007/s11263-019-01158-4", "report-no": null, "categories": "cs.CV cs.AI cs.HC eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic understanding of human affect using visual signals is of great\nimportance in everyday human-machine interactions. Appraising human emotional\nstates, behaviors and reactions displayed in real-world settings, can be\naccomplished using latent continuous dimensions (e.g., the circumplex model of\naffect). Valence (i.e., how positive or negative is an emotion) & arousal\n(i.e., power of the activation of the emotion) constitute popular and effective\naffect representations. Nevertheless, the majority of collected datasets this\nfar, although containing naturalistic emotional states, have been captured in\nhighly controlled recording conditions. In this paper, we introduce the\nAff-Wild benchmark for training and evaluating affect recognition algorithms.\nWe also report on the results of the First Affect-in-the-wild Challenge that\nwas organized in conjunction with CVPR 2017 on the Aff-Wild database and was\nthe first ever challenge on the estimation of valence and arousal in-the-wild.\nFurthermore, we design and extensively train an end-to-end deep neural\narchitecture which performs prediction of continuous emotion dimensions based\non visual cues. The proposed deep learning architecture, AffWildNet, includes\nconvolutional & recurrent neural network layers, exploiting the invariant\nproperties of convolutional features, while also modeling temporal dynamics\nthat arise in human behavior via the recurrent layers. The AffWildNet produced\nstate-of-the-art results on the Aff-Wild Challenge. We then exploit the AffWild\ndatabase for learning features, which can be used as priors for achieving best\nperformances both for dimensional, as well as categorical emotion recognition,\nusing the RECOLA, AFEW-VA and EmotiW datasets, compared to all other methods\ndesigned for the same goal. The database and emotion recognition models are\navailable at http://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 14:18:07 GMT"}, {"version": "v2", "created": "Sun, 6 May 2018 01:27:00 GMT"}, {"version": "v3", "created": "Tue, 8 May 2018 09:50:53 GMT"}, {"version": "v4", "created": "Sat, 1 Sep 2018 13:26:39 GMT"}, {"version": "v5", "created": "Fri, 1 Feb 2019 12:39:52 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Tzirakis", "Panagiotis", ""], ["Nicolaou", "Mihalis A.", ""], ["Papaioannou", "Athanasios", ""], ["Zhao", "Guoying", ""], ["Schuller", "Bj\u00f6rn", ""], ["Kotsia", "Irene", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1804.10969", "submitter": "Evgenii Zheltonozhskii", "authors": "Chaim Baskin, Eli Schwartz, Evgenii Zheltonozhskii, Natan Liss, Raja\n  Giryes, Alex M. Bronstein, Avi Mendelson", "title": "UNIQ: Uniform Noise Injection for Non-Uniform Quantization of Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3444943", "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a novel method for neural network quantization that emulates a\nnon-uniform $k$-quantile quantizer, which adapts to the distribution of the\nquantized parameters. Our approach provides a novel alternative to the existing\nuniform quantization techniques for neural networks. We suggest to compare the\nresults as a function of the bit-operations (BOPS) performed, assuming a\nlook-up table availability for the non-uniform case. In this setup, we show the\nadvantages of our strategy in the low computational budget regime. While the\nproposed solution is harder to implement in hardware, we believe it sets a\nbasis for new alternatives to neural networks quantization.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 17:38:20 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 20:11:25 GMT"}, {"version": "v3", "created": "Tue, 2 Oct 2018 20:19:13 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Baskin", "Chaim", ""], ["Schwartz", "Eli", ""], ["Zheltonozhskii", "Evgenii", ""], ["Liss", "Natan", ""], ["Giryes", "Raja", ""], ["Bronstein", "Alex M.", ""], ["Mendelson", "Avi", ""]]}, {"id": "1804.10975", "submitter": "Stephan R Richter", "authors": "Stephan R. Richter and Stefan Roth", "title": "Matryoshka Networks: Predicting 3D Geometry via Nested Shape Layers", "comments": "Published at the Conference on Computer Vision and Pattern\n  Recognition (CVPR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop novel, efficient 2D encodings for 3D geometry,\nwhich enable reconstructing full 3D shapes from a single image at high\nresolution. The key idea is to pose 3D shape reconstruction as a 2D prediction\nproblem. To that end, we first develop a simple baseline network that predicts\nentire voxel tubes at each pixel of a reference view. By leveraging well-proven\narchitectures for 2D pixel-prediction tasks, we attain state-of-the-art\nresults, clearly outperforming purely voxel-based approaches. We scale this\nbaseline to higher resolutions by proposing a memory-efficient shape encoding,\nwhich recursively decomposes a 3D shape into nested shape layers, similar to\nthe pieces of a Matryoshka doll. This allows reconstructing highly detailed\nshapes with complex topology, as demonstrated in extensive experiments; we\nclearly outperform previous octree-based approaches despite having a much\nsimpler architecture using standard network components. Our Matryoshka networks\nfurther enable reconstructing shapes from IDs or shape similarity, as well as\nshape sampling.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 18:33:21 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Richter", "Stephan R.", ""], ["Roth", "Stefan", ""]]}, {"id": "1804.10992", "submitter": "Qifeng Chen", "authors": "Xiaojuan Qi, Qifeng Chen, Jiaya Jia, and Vladlen Koltun", "title": "Semi-parametric Image Synthesis", "comments": "Published at the Conference on Computer Vision and Pattern\n  Recognition (CVPR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semi-parametric approach to photographic image synthesis from\nsemantic layouts. The approach combines the complementary strengths of\nparametric and nonparametric techniques. The nonparametric component is a\nmemory bank of image segments constructed from a training set of images. Given\na novel semantic layout at test time, the memory bank is used to retrieve\nphotographic references that are provided as source material to a deep network.\nThe synthesis is performed by a deep network that draws on the provided\nphotographic material. Experiments on multiple semantic segmentation datasets\nshow that the presented approach yields considerably more realistic images than\nrecent purely parametric techniques. The results are shown in the supplementary\nvideo at https://youtu.be/U4Q98lenGLQ\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 21:20:43 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Qi", "Xiaojuan", ""], ["Chen", "Qifeng", ""], ["Jia", "Jiaya", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1804.11013", "submitter": "Yang Wang", "authors": "Lin Wu, Yang Wang, Ling Shao", "title": "Cycle-Consistent Deep Generative Hashing for Cross-Modal Retrieval", "comments": "To appeared on IEEE Trans. Image Processing. arXiv admin note: text\n  overlap with arXiv:1703.10593 by other authors", "journal-ref": null, "doi": "10.1109/TIP.2018.2878970", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel deep generative approach to cross-modal\nretrieval to learn hash functions in the absence of paired training samples\nthrough the cycle consistency loss. Our proposed approach employs adversarial\ntraining scheme to lean a couple of hash functions enabling translation between\nmodalities while assuming the underlying semantic relationship. To induce the\nhash codes with semantics to the input-output pair, cycle consistency loss is\nfurther proposed upon the adversarial training to strengthen the correlations\nbetween inputs and corresponding outputs. Our approach is generative to learn\nhash functions such that the learned hash codes can maximally correlate each\ninput-output correspondence, meanwhile can also regenerate the inputs so as to\nminimize the information loss. The learning to hash embedding is thus performed\nto jointly optimize the parameters of the hash functions across modalities as\nwell as the associated generative models. Extensive experiments on a variety of\nlarge-scale cross-modal data sets demonstrate that our proposed method achieves\nbetter retrieval results than the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 01:28:20 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 03:01:33 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Wu", "Lin", ""], ["Wang", "Yang", ""], ["Shao", "Ling", ""]]}, {"id": "1804.11024", "submitter": "Pingkun Yan", "authors": "Pingkun Yan, Sheng Xu, Ardeshir R. Rastinehad, Brad J. Wood", "title": "Adversarial Image Registration with Application for MR and TRUS Image\n  Fusion", "comments": "Presented at the workshop on MLMI 2018, LNCS, volume 11046, pages 197\n  to 204", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust and accurate alignment of multimodal medical images is a very\nchallenging task, which however is very useful for many clinical applications.\nFor example, magnetic resonance (MR) and transrectal ultrasound (TRUS) image\nregistration is a critical component in MR-TRUS fusion guided prostate\ninterventions. However, due to the huge difference between the image\nappearances and the large variation in image correspondence, MR-TRUS image\nregistration is a very challenging problem. In this paper, an adversarial image\nregistration (AIR) framework is proposed. By training two deep neural networks\nsimultaneously, one being a generator and the other being a discriminator, we\ncan obtain not only a network for image registration, but also a metric network\nwhich can help evaluate the quality of image registration. The developed\nAIR-net is then evaluated using clinical datasets acquired through image-fusion\nguided prostate biopsy procedures and promising results are demonstrated.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 02:12:57 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 22:04:36 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Yan", "Pingkun", ""], ["Xu", "Sheng", ""], ["Rastinehad", "Ardeshir R.", ""], ["Wood", "Brad J.", ""]]}, {"id": "1804.11027", "submitter": "Yang Wang", "authors": "Lin Wu, Yang Wang, Junbin Gao, Dacheng Tao", "title": "Deep Co-attention based Comparators For Relative Representation Learning\n  in Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) requires rapid, flexible yet discriminant\nrepresentations to quickly generalize to unseen observations on-the-fly and\nrecognize the same identity across disjoint camera views. Recent effective\nmethods are developed in a pair-wise similarity learning system to detect a\nfixed set of features from distinct regions which are mapped to their vector\nembeddings for the distance measuring. However, the most relevant and crucial\nparts of each image are detected independently without referring to the\ndependency conditioned on one and another. Also, these region based methods\nrely on spatial manipulation to position the local features in comparable\nsimilarity measuring. To combat these limitations, in this paper we introduce\nthe Deep Co-attention based Comparators (DCCs) that fuse the co-dependent\nrepresentations of the paired images so as to focus on the relevant parts of\nboth images and produce their \\textit{relative representations}. Given a pair\nof pedestrian images to be compared, the proposed model mimics the foveation of\nhuman eyes to detect distinct regions concurrent on both images, namely\nco-dependent features, and alternatively attend to relevant regions to fuse\nthem into the similarity learning. Our comparator is capable of producing\ndynamic representations relative to a particular sample every time, and thus\nwell-suited to the case of re-identifying pedestrians on-the-fly. We perform\nextensive experiments to provide the insights and demonstrate the effectiveness\nof the proposed DCCs in person re-ID. Moreover, our approach has achieved the\nstate-of-the-art performance on three benchmark data sets: DukeMTMC-reID\n\\cite{DukeMTMC}, CUHK03 \\cite{FPNN}, and Market-1501 \\cite{Market1501}.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 02:35:46 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Wu", "Lin", ""], ["Wang", "Yang", ""], ["Gao", "Junbin", ""], ["Tao", "Dacheng", ""]]}, {"id": "1804.11031", "submitter": "Samarth Tripathi", "authors": "Samarth Tripathi, Renbo Tu", "title": "Towards Deeper Generative Architectures for GANs using Dense connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the result of adopting skip connections and dense\nlayers, previously used in image classification tasks, in the Fisher GAN\nimplementation. We have experimented with different numbers of layers and\ninserting these connections in different sections of the network. Our findings\nsuggests that networks implemented with the connections produce better images\nthan the baseline, and the number of connections added has only slight effect\non the result.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 02:45:12 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 08:49:08 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Tripathi", "Samarth", ""], ["Tu", "Renbo", ""]]}, {"id": "1804.11127", "submitter": "Michael Wand", "authors": "Michael Wand and Ngoc Thang Vu and Juergen Schmidhuber", "title": "Investigations on End-to-End Audiovisual Fusion", "comments": "Published at ICASSP 2018", "journal-ref": "Proceedings of the 2018 IEEE International Conference on\n  Acoustics, Speech and Signal Processing, pages 3041 - 3045", "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audiovisual speech recognition (AVSR) is a method to alleviate the adverse\neffect of noise in the acoustic signal. Leveraging recent developments in deep\nneural network-based speech recognition, we present an AVSR neural network\narchitecture which is trained end-to-end, without the need to separately model\nthe process of decision fusion as in conventional (e.g. HMM-based) systems. The\nfusion system outperforms single-modality recognition under all noise\nconditions. Investigation of the saliency of the input features shows that the\nneural network automatically adapts to different noise levels in the acoustic\nsignal.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 11:27:26 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Wand", "Michael", ""], ["Vu", "Ngoc Thang", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1804.11142", "submitter": "Shashi Poddar", "authors": "Shashi Poddar, Rahul Kottath, Vinod Karar", "title": "Evolution of Visual Odometry Techniques", "comments": "12 pages, 0 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid advancements in the area of mobile robotics and industrial\nautomation, a growing need has arisen towards accurate navigation and\nlocalization of moving objects. Camera based motion estimation is one such\ntechnique which is gaining huge popularity owing to its simplicity and use of\nlimited resources in generating motion path. In this paper, an attempt is made\nto introduce this topic for beginners covering different aspects of vision\nbased motion estimation task. The evolution of VO schemes over last few decades\nis discussed under two broad categories, that is, geometric and non-geometric\napproaches. The geometric approaches are further detailed under three different\nclasses, that is, feature-based, appearance-based, and a hybrid of feature and\nappearance based schemes. The non-geometric approach is one of the recent\nparadigm shift from conventional pose estimation technique and is thus\ndiscussed in a separate section. Towards the end, a list of different datasets\nfor visual odometry and allied research areas are provided for a ready\nreference.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 12:05:00 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Poddar", "Shashi", ""], ["Kottath", "Rahul", ""], ["Karar", "Vinod", ""]]}, {"id": "1804.11146", "submitter": "Micael Carvalho", "authors": "Micael Carvalho, R\\'emi Cad\\`ene, David Picard, Laure Soulier, Nicolas\n  Thome, Matthieu Cord", "title": "Cross-Modal Retrieval in the Cooking Context: Learning Semantic\n  Text-Image Embeddings", "comments": "accepted at the 41st International ACM SIGIR Conference on Research\n  and Development in Information Retrieval, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing powerful tools that support cooking activities has rapidly gained\npopularity due to the massive amounts of available data, as well as recent\nadvances in machine learning that are capable of analyzing them. In this paper,\nwe propose a cross-modal retrieval model aligning visual and textual data (like\npictures of dishes and their recipes) in a shared representation space. We\ndescribe an effective learning scheme, capable of tackling large-scale\nproblems, and validate it on the Recipe1M dataset containing nearly 1 million\npicture-recipe pairs. We show the effectiveness of our approach regarding\nprevious state-of-the-art models and present qualitative results over\ncomputational cooking use cases.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 12:14:32 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Carvalho", "Micael", ""], ["Cad\u00e8ne", "R\u00e9mi", ""], ["Picard", "David", ""], ["Soulier", "Laure", ""], ["Thome", "Nicolas", ""], ["Cord", "Matthieu", ""]]}, {"id": "1804.11159", "submitter": "Hichem Sahbi", "authors": "Mingyuan Jiu and Hichem Sahbi", "title": "Learning Explicit Deep Representations from Deep Kernel Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep kernel learning aims at designing nonlinear combinations of multiple\nstandard elementary kernels by training deep networks. This scheme has proven\nto be effective, but intractable when handling large-scale datasets especially\nwhen the depth of the trained networks increases; indeed, the complexity of\nevaluating these networks scales quadratically w.r.t. the size of training data\nand linearly w.r.t. the depth of the trained networks. In this paper, we\naddress the issue of efficient computation in Deep Kernel Networks (DKNs) by\ndesigning effective maps in the underlying Reproducing Kernel Hilbert Spaces.\nGiven a pretrained DKN, our method builds its associated Deep Map Network (DMN)\nwhose inner product approximates the original network while being far more\nefficient. The design principle of our method is greedy and achieved\nlayer-wise, by finding maps that approximate DKNs at different (input,\nintermediate and output) layers. This design also considers an extra\nfine-tuning step based on unsupervised learning, that further enhances the\ngeneralization ability of the trained DMNs. When plugged into SVMs, these DMNs\nturn out to be as accurate as the underlying DKNs while being at least an order\nof magnitude faster on large-scale datasets, as shown through extensive\nexperiments on the challenging ImageCLEF and COREL5k benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 12:42:02 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Jiu", "Mingyuan", ""], ["Sahbi", "Hichem", ""]]}, {"id": "1804.11182", "submitter": "Conghui Hu", "authors": "Conghui Hu, Da Li, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales", "title": "Sketch-a-Classifier: Sketch-based Photo Classifier Generation", "comments": "published in CVPR2018 as spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary deep learning techniques have made image recognition a\nreasonably reliable technology. However training effective photo classifiers\ntypically takes numerous examples which limits image recognition's scalability\nand applicability to scenarios where images may not be available. This has\nmotivated investigation into zero-shot learning, which addresses the issue via\nknowledge transfer from other modalities such as text. In this paper we\ninvestigate an alternative approach of synthesizing image classifiers: almost\ndirectly from a user's imagination, via free-hand sketch. This approach doesn't\nrequire the category to be nameable or describable via attributes as per\nzero-shot learning. We achieve this via training a {model regression} network\nto map from {free-hand sketch} space to the space of photo classifiers. It\nturns out that this mapping can be learned in a category-agnostic way, allowing\nphoto classifiers for new categories to be synthesized by user with no need for\nannotated training photos. {We also demonstrate that this modality of\nclassifier generation can also be used to enhance the granularity of an\nexisting photo classifier, or as a complement to name-based zero-shot learning.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 13:38:31 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Hu", "Conghui", ""], ["Li", "Da", ""], ["Song", "Yi-Zhe", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1804.11191", "submitter": "Zhuwei Qin", "authors": "Zhuwei Qin, Fuxun Yu, Chenchen Liu and Xiang Chen", "title": "How convolutional neural network see the world - A survey of\n  convolutional neural network visualization methods", "comments": "32 pages, 21 figures. Mathematical Foundations of Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the Convolutional Neural Networks (CNNs) have achieved impressive\nperformance on many computer vision related tasks, such as object detection,\nimage recognition, image retrieval, etc. These achievements benefit from the\nCNNs outstanding capability to learn the input features with deep layers of\nneuron structures and iterative training process. However, these learned\nfeatures are hard to identify and interpret from a human vision perspective,\ncausing a lack of understanding of the CNNs internal working mechanism. To\nimprove the CNN interpretability, the CNN visualization is well utilized as a\nqualitative analysis method, which translates the internal features into\nvisually perceptible patterns. And many CNN visualization works have been\nproposed in the literature to interpret the CNN in perspectives of network\nstructure, operation, and semantic concept. In this paper, we expect to provide\na comprehensive survey of several representative CNN visualization methods,\nincluding Activation Maximization, Network Inversion, Deconvolutional Neural\nNetworks (DeconvNet), and Network Dissection based visualization. These methods\nare presented in terms of motivations, algorithms, and experiment results.\nBased on these visualization methods, we also discuss their practical\napplications to demonstrate the significance of the CNN interpretability in\nareas of network design, optimization, security enhancement, etc.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 13:47:11 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 20:12:43 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Qin", "Zhuwei", ""], ["Yu", "Fuxun", ""], ["Liu", "Chenchen", ""], ["Chen", "Xiang", ""]]}, {"id": "1804.11207", "submitter": "Pei Li", "authors": "Pei Li, Bingyu Shen, Weishan Dong", "title": "An Anti-fraud System for Car Insurance Claim Based on Visual Evidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically scene understanding using machine learning algorithms has been\nwidely applied to different industries to reduce the cost of manual labor.\nNowadays, insurance companies launch express vehicle insurance claim and\nsettlement by allowing customers uploading pictures taken by mobile devices.\nThis kind of insurance claim is treated as small claim and can be processed\neither manually or automatically in a quick fashion. However, due to the\nincreasing amount of claims every day, system or people are likely to be fooled\nby repeated claims for identical case leading to big lost to insurance\ncompanies.Thus, an anti-fraud checking before processing the claim is\nnecessary. We create the first data set of car damage images collected from\ninternet and local parking lots. In addition, we proposed an approach to\ngenerate robust deep features by locating the damages accurately and\nefficiently in the images. The state-of-the-art real-time object detector YOLO\n\\cite{redmon2016you}is modified to train and discover damage region as an\nimportant part of the pipeline. Both local and global deep features are\nextracted using VGG model\\cite{Simonyan14c}, which are fused later for more\nrobust system performance. Experiments show our approach is effective in\npreventing fraud claims as well as meet the requirement to speed up the\ninsurance claim prepossessing.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 14:03:22 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Li", "Pei", ""], ["Shen", "Bingyu", ""], ["Dong", "Weishan", ""]]}, {"id": "1804.11227", "submitter": "Tobias Geimer", "authors": "Tobias Geimer, Paul Keall, Katharina Breininger, Vincent Caillet,\n  Michelle Dunbar, Christoph Bert, Andreas Maier", "title": "Decoupling Respiratory and Angular Variation in Rotational X-ray Scans\n  Using a Prior Bilinear Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven respiratory signal extraction from rotational X-ray scans is a\nchallenge as angular effects overlap with respiration-induced change in the\nscene. In this paper, we use the linearity of the X-ray transform to propose a\nbilinear model based on a prior 4D scan to separate angular and respiratory\nvariation. The bilinear estimation process is supported by a B-spline\ninterpolation using prior knowledge about the trajectory angle. Consequently,\nextraction of respiratory features simplifies to a linear problem. Though the\nneed for a prior 4D CT seems steep, our proposed use-case of driving a\nrespiratory motion model in radiation therapy usually meets this requirement.\nWe evaluate on DRRs of 5 patient 4D CTs in a leave-one-phase-out manner and\nachieve a mean estimation error of 3.01 % in the gray values for unseen viewing\nangles. We further demonstrate suitability of the extracted weights to drive a\nmotion model for treatments with a continuously rotating gantry.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 14:26:15 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 13:15:32 GMT"}, {"version": "v3", "created": "Mon, 5 Nov 2018 09:23:29 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Geimer", "Tobias", ""], ["Keall", "Paul", ""], ["Breininger", "Katharina", ""], ["Caillet", "Vincent", ""], ["Dunbar", "Michelle", ""], ["Bert", "Christoph", ""], ["Maier", "Andreas", ""]]}, {"id": "1804.11228", "submitter": "Yujia Zhang", "authors": "Yujia Zhang, Michael Kampffmeyer, Xiaodan Liang, Dingwen Zhang, Min\n  Tan, and Eric P. Xing", "title": "Dilated Temporal Relational Adversarial Network for Generic Video\n  Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large amount of videos popping up every day, make it more and more\ncritical that key information within videos can be extracted and understood in\na very short time. Video summarization, the task of finding the smallest subset\nof frames, which still conveys the whole story of a given video, is thus of\ngreat significance to improve efficiency of video understanding. We propose a\nnovel Dilated Temporal Relational Generative Adversarial Network (DTR-GAN) to\nachieve frame-level video summarization. Given a video, it selects the set of\nkey frames, which contain the most meaningful and compact information.\nSpecifically, DTR-GAN learns a dilated temporal relational generator and a\ndiscriminator with three-player loss in an adversarial manner. A new dilated\ntemporal relation (DTR) unit is introduced to enhance temporal representation\ncapturing. The generator uses this unit to effectively exploit global\nmulti-scale temporal context to select key frames and to complement the\ncommonly used Bi-LSTM. To ensure that summaries capture enough key video\nrepresentation from a global perspective rather than a trivial randomly shorten\nsequence, we present a discriminator that learns to enforce both the\ninformation completeness and compactness of summaries via a three-player loss.\nThe loss includes the generated summary loss, the random summary loss, and the\nreal summary (ground-truth) loss, which play important roles for better\nregularizing the learned model to obtain useful summaries. Comprehensive\nexperiments on three public datasets show the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 14:27:24 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 02:09:50 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Zhang", "Yujia", ""], ["Kampffmeyer", "Michael", ""], ["Liang", "Xiaodan", ""], ["Zhang", "Dingwen", ""], ["Tan", "Min", ""], ["Xing", "Eric P.", ""]]}, {"id": "1804.11256", "submitter": "Ammar Qammaz", "authors": "Ammar Qammaz, Sokol Kosta, Nikolaos Kyriazis, Antonis Argyros", "title": "On the Feasibility of Real-Time 3D Hand Tracking using Edge GPGPU\n  Acceleration", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the case study of a non-intrusive porting of a monolithic\nC++ library for real-time 3D hand tracking, to the domain of edge-based\ncomputation. Towards a proof of concept, the case study considers a pair of\nworkstations, a computationally powerful and a computationally weak one. By\nwrapping the C++ library in Java container and by capitalizing on a Java-based\noffloading infrastructure that supports both CPU and GPGPU computations, we are\nable to establish automatically the required server-client workflow that best\naddresses the resource allocation problem in the effort to execute from the\nweak workstation. As a result, the weak workstation can perform well at the\ntask, despite lacking the sufficient hardware to do the required computations\nlocally. This is achieved by offloading computations which rely on GPGPU, to\nthe powerful workstation, across the network that connects them. We show the\nedge-based computation challenges associated with the information flow of the\nported algorithm, demonstrate how we cope with them, and identify what needs to\nbe improved for achieving even better performance.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 15:00:40 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Qammaz", "Ammar", ""], ["Kosta", "Sokol", ""], ["Kyriazis", "Nikolaos", ""], ["Argyros", "Antonis", ""]]}, {"id": "1804.11276", "submitter": "Armin Mustafa", "authors": "Armin Mustafa, Marco Volino, Jean-yves Guillemaut, Adrian Hilton", "title": "4D Temporally Coherent Light-field Video", "comments": "Published in 3D Vision (3DV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light-field video has recently been used in virtual and augmented reality\napplications to increase realism and immersion. However, existing light-field\nmethods are generally limited to static scenes due to the requirement to\nacquire a dense scene representation. The large amount of data and the absence\nof methods to infer temporal coherence pose major challenges in storage,\ncompression and editing compared to conventional video. In this paper, we\npropose the first method to extract a spatio-temporally coherent light-field\nvideo representation. A novel method to obtain Epipolar Plane Images (EPIs)\nfrom a spare light-field camera array is proposed. EPIs are used to constrain\nscene flow estimation to obtain 4D temporally coherent representations of\ndynamic light-fields. Temporal coherence is achieved on a variety of\nlight-field datasets. Evaluation of the proposed light-field scene flow against\nexisting multi-view dense correspondence approaches demonstrates a significant\nimprovement in accuracy of temporal coherence.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 15:33:13 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Mustafa", "Armin", ""], ["Volino", "Marco", ""], ["Guillemaut", "Jean-yves", ""], ["Hilton", "Adrian", ""]]}, {"id": "1804.11294", "submitter": "Artem Sevastopolsky", "authors": "Artem Sevastopolsky, Stepan Drapak, Konstantin Kiselev, Blake M.\n  Snyder, Jeremy D. Keenan, Anastasia Georgievskaya", "title": "Stack-U-Net: Refinement Network for Image Segmentation on the Example of\n  Optic Disc and Cup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a special cascade network for image segmentation,\nwhich is based on the U-Net networks as building blocks and the idea of the\niterative refinement. The model was mainly applied to achieve higher\nrecognition quality for the task of finding borders of the optic disc and cup,\nwhich are relevant to the presence of glaucoma. Compared to a single U-Net and\nthe state-of-the-art methods for the investigated tasks, very high segmentation\nquality has been achieved without a need for increasing the volume of datasets.\nOur experiments include comparison with the best-known methods on publicly\navailable databases DRIONS-DB, RIM-ONE v.3, DRISHTI-GS, and evaluation on a\nprivate data set collected in collaboration with University of California San\nFrancisco Medical School. The analysis of the architecture details is\npresented, and it is argued that the model can be employed for a broad scope of\nimage segmentation problems of similar nature.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 16:15:36 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 12:28:07 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Sevastopolsky", "Artem", ""], ["Drapak", "Stepan", ""], ["Kiselev", "Konstantin", ""], ["Snyder", "Blake M.", ""], ["Keenan", "Jeremy D.", ""], ["Georgievskaya", "Anastasia", ""]]}, {"id": "1804.11317", "submitter": "Isma\\\"el Kon\\'e", "authors": "Isma\\\"el Kon\\'e, Lahsen Boulmane", "title": "Hybrid Forests for Left Ventricle Segmentation using only the first\n  slice label", "comments": "5 pages, 4 figures, 3rd International Conference on Intelligent\n  System and Computer Vision (ISCV 2018), Fez, Morocco", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models produce state-of-the-art results in many MRI images\nsegmentation. However, most of these models are trained on very large datasets\nwhich come from experts manual labeling. This labeling process is very time\nconsuming and costs experts work. Therefore finding a way to reduce this cost\nis on high demand. In this paper, we propose a segmentation method which\nexploits MRI images sequential structure to nearly drop out this labeling task.\nOnly the first slice needs to be manually labeled to train the model which then\ninfers the next slice's segmentation. Inference result is another datum used to\ntrain the model again. The updated model then infers the third slice and the\nsame process is carried out until the last slice. The proposed model is an\ncombination of two Random Forest algorithms: the classical one and a recent one\nnamely Mondrian Forests. We applied our method on human left ventricle\nsegmentation and results are very promising. This method can also be used to\ngenerate labels.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 16:43:29 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Kon\u00e9", "Isma\u00ebl", ""], ["Boulmane", "Lahsen", ""]]}, {"id": "1804.11332", "submitter": "Arantxa Casanova", "authors": "Arantxa Casanova, Guillem Cucurull, Michal Drozdzal, Adriana Romero,\n  Yoshua Bengio", "title": "On the iterative refinement of densely connected representation levels\n  for semantic segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art semantic segmentation approaches increase the receptive\nfield of their models by using either a downsampling path composed of\npoolings/strided convolutions or successive dilated convolutions. However, it\nis not clear which operation leads to best results. In this paper, we\nsystematically study the differences introduced by distinct receptive field\nenlargement methods and their impact on the performance of a novel\narchitecture, called Fully Convolutional DenseResNet (FC-DRN). FC-DRN has a\ndensely connected backbone composed of residual networks. Following standard\nimage segmentation architectures, receptive field enlargement operations that\nchange the representation level are interleaved among residual networks. This\nallows the model to exploit the benefits of both residual and dense\nconnectivity patterns, namely: gradient flow, iterative refinement of\nrepresentations, multi-scale feature combination and deep supervision. In order\nto highlight the potential of our model, we test it on the challenging CamVid\nurban scene understanding benchmark and make the following observations: 1)\ndownsampling operations outperform dilations when the model is trained from\nscratch, 2) dilations are useful during the finetuning step of the model, 3)\ncoarser representations require less refinement steps, and 4) ResNets (by model\nconstruction) are good regularizers, since they can reduce the model capacity\nwhen needed. Finally, we compare our architecture to alternative methods and\nreport state-of-the-art result on the Camvid dataset, with at least twice fewer\nparameters.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 17:26:49 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Casanova", "Arantxa", ""], ["Cucurull", "Guillem", ""], ["Drozdzal", "Michal", ""], ["Romero", "Adriana", ""], ["Bengio", "Yoshua", ""]]}]