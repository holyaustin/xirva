[{"id": "1212.0030", "submitter": "Andrew Habib", "authors": "Osama Khalil, Andrew Habib", "title": "Viewpoint Invariant Object Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Object Detection is the task of identifying the existence of an object class\ninstance and locating it within an image. Difficulties in handling high\nintra-class variations constitute major obstacles to achieving high performance\non standard benchmark datasets (scale, viewpoint, lighting conditions and\norientation variations provide good examples). Suggested model aims at\nproviding more robustness to detecting objects suffering severe distortion due\nto < 60{\\deg} viewpoint changes. In addition, several model computational\nbottlenecks have been resolved leading to a significant increase in the model\nperformance (speed and space) without compromising the resulting accuracy.\nFinally, we produced two illustrative applications showing the potential of the\nobject detection technology being deployed in real life applications; namely\ncontent-based image search and content-based video search.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 22:35:19 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Khalil", "Osama", ""], ["Habib", "Andrew", ""]]}, {"id": "1212.0042", "submitter": "R.C. Johnson", "authors": "R.C. Johnson and Walter J. Scheirer and Terrance E. Boult", "title": "Secure voice based authentication for mobile devices: Vaulted Voice\n  Verification", "comments": null, "journal-ref": null, "doi": "10.1117/12.2015649", "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the use of biometrics becomes more wide-spread, the privacy concerns that\nstem from the use of biometrics are becoming more apparent. As the usage of\nmobile devices grows, so does the desire to implement biometric identification\ninto such devices. A large majority of mobile devices being used are mobile\nphones. While work is being done to implement different types of biometrics\ninto mobile phones, such as photo based biometrics, voice is a more natural\nchoice. The idea of voice as a biometric identifier has been around a long\ntime. One of the major concerns with using voice as an identifier is the\ninstability of voice. We have developed a protocol that addresses those\ninstabilities and preserves privacy. This paper describes a novel protocol that\nallows a user to authenticate using voice on a mobile/remote device without\ncompromising their privacy. We first discuss the \\vv protocol, which has\nrecently been introduced in research literature, and then describe its\nlimitations. We then introduce a novel adaptation and extension of the vaulted\nverification protocol to voice, dubbed $V^3$. Following that we show a\nperformance evaluation and then conclude with a discussion of security and\nfuture work.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 23:24:05 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Johnson", "R. C.", ""], ["Scheirer", "Walter J.", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1212.0059", "submitter": "Minakshi Sharma", "authors": "Minakshi Sharma", "title": "Artificial Neural Network Fuzzy Inference System (ANFIS) For Brain Tumor\n  Detection", "comments": "5 pages", "journal-ref": "IJFLS 2012", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection and segmentation of Brain tumor is very important because it\nprovides anatomical information of normal and abnormal tissues which helps in\ntreatment planning and patient follow-up. There are number of techniques for\nimage segmentation. Proposed research work uses ANFIS (Artificial Neural\nNetwork Fuzzy Inference System) for image classification and then compares the\nresults with FCM (Fuzzy C means) and K-NN (K-nearest neighbor). ANFIS includes\nbenefits of both ANN and the fuzzy logic systems. A comprehensive feature set\nand fuzzy rules are selected to classify an abnormal image to the corresponding\ntumor type. Experimental results illustrate promising results in terms of\nclassification accuracy. A comparative analysis is performed with the FCM and\nK-NN to show the superior nature of ANFIS systems.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2012 03:58:09 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Sharma", "Minakshi", ""]]}, {"id": "1212.0134", "submitter": "Ankit Chaudhary", "authors": "J.L.Raheja, Karen Das, Ankit Chaudhary", "title": "Fingertip Detection: A Fast Method with Natural Hand", "comments": null, "journal-ref": "International Journal of Embedded Systems and Computer\n  Engineering, 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many vision based applications have used fingertips to track or manipulate\ngestures in their applications. Gesture identification is a natural way to pass\nthe signals to the machine, as the human express its feelings most of the time\nwith hand expressions. Here a novel time efficient algorithm has been described\nfor fingertip detection. This method is invariant to hand direction and in\npreprocessing it cuts only hand part from the full image, hence further\ncomputation would be much faster than processing full image. Binary silhouette\nof the input image is generated using HSV color space based skin filter and\nhand cropping done based on intensity histogram of the hand image\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2012 16:59:07 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Raheja", "J. L.", ""], ["Das", "Karen", ""], ["Chaudhary", "Ankit", ""]]}, {"id": "1212.0142", "submitter": "Pierre Sermanet", "authors": "Pierre Sermanet and Koray Kavukcuoglu and Soumith Chintala and Yann\n  LeCun", "title": "Pedestrian Detection with Unsupervised Multi-Stage Feature Learning", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection is a problem of considerable practical interest. Adding\nto the list of successful applications of deep learning methods to vision, we\nreport state-of-the-art and competitive results on all major pedestrian\ndatasets with a convolutional network model. The model uses a few new twists,\nsuch as multi-stage features, connections that skip layers to integrate global\nshape information with local distinctive motif information, and an unsupervised\nmethod based on convolutional sparse coding to pre-train the filters at each\nstage.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2012 18:13:03 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2013 18:05:46 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Sermanet", "Pierre", ""], ["Kavukcuoglu", "Koray", ""], ["Chintala", "Soumith", ""], ["LeCun", "Yann", ""]]}, {"id": "1212.0291", "submitter": "Praveen Kumar P U", "authors": "C. J. Prabhakar and P. U. Praveen Kumar", "title": "An Image Based Technique for Enhancement of Underwater Images", "comments": null, "journal-ref": "International Journal of Machine Intelligence, Volume 3, Issue 4,\n  pages 217-224, 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The underwater images usually suffers from non-uniform lighting, low\ncontrast, blur and diminished colors. In this paper, we proposed an image based\npreprocessing technique to enhance the quality of the underwater images. The\nproposed technique comprises a combination of four filters such as homomorphic\nfiltering, wavelet denoising, bilateral filter and contrast equalization. These\nfilters are applied sequentially on degraded underwater images. The literature\nsurvey reveals that image based preprocessing algorithms uses standard filter\ntechniques with various combinations. For smoothing the image, the image based\npreprocessing algorithms uses the anisotropic filter. The main drawback of the\nanisotropic filter is that iterative in nature and computation time is high\ncompared to bilateral filter. In the proposed technique, in addition to other\nthree filters, we employ a bilateral filter for smoothing the image. The\nexperimentation is carried out in two stages. In the first stage, we have\nconducted various experiments on captured images and estimated optimal\nparameters for bilateral filter. Similarly, optimal filter bank and optimal\nwavelet shrinkage function are estimated for wavelet denoising. In the second\nstage, we conducted the experiments using estimated optimal parameters, optimal\nfilter bank and optimal wavelet shrinkage function for evaluating the proposed\ntechnique. We evaluated the technique using quantitative based criteria such as\na gradient magnitude histogram and Peak Signal to Noise Ratio (PSNR). Further,\nthe results are qualitatively evaluated based on edge detection results. The\nproposed technique enhances the quality of the underwater images and can be\nemployed prior to apply computer vision techniques.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 05:57:46 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Prabhakar", "C. J.", ""], ["Kumar", "P. U. Praveen", ""]]}, {"id": "1212.0318", "submitter": "M HM Krishna Prasad Dr", "authors": "D. Srinivasa Rao, M. Seetha and M. H. M. Krishna Prasad", "title": "Comparison of Fuzzy and Neuro Fuzzy Image Fusion Techniques and its\n  Applications", "comments": "(0975 8887). arXiv admin note: text overlap with arXiv:1209.4535 by\n  other authors", "journal-ref": "International Journal of Computer Applications Volume 43, No.20,\n  2012, pages: 31 - 37", "doi": "10.5120/6222-8800", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image fusion is the process of integrating multiple images of the same scene\ninto a single fused image to reduce uncertainty and minimizing redundancy while\nextracting all the useful information from the source images. Image fusion\nprocess is required for different applications like medical imaging, remote\nsensing, medical imaging, machine vision, biometrics and military applications\nwhere quality and critical information is required. In this paper, image fusion\nusing fuzzy and neuro fuzzy logic approaches utilized to fuse images from\ndifferent sensors, in order to enhance visualization. The proposed work further\nexplores comparison between fuzzy based image fusion and neuro fuzzy fusion\ntechnique along with quality evaluation indices for image fusion like image\nquality index, mutual information measure, fusion factor, fusion symmetry,\nfusion index, root mean square error, peak signal to noise ratio, entropy,\ncorrelation coefficient and spatial frequency. Experimental results obtained\nfrom fusion process prove that the use of the neuro fuzzy based image fusion\napproach shows better performance in first two test cases while in the third\ntest case fuzzy based image fusion technique gives better results.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 08:55:52 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Rao", "D. Srinivasa", ""], ["Seetha", "M.", ""], ["Prasad", "M. H. M. Krishna", ""]]}, {"id": "1212.0383", "submitter": "Asha V", "authors": "V. Asha, N. U. Bhajantri, P. Nagabhushan", "title": "GLCM-based chi-square histogram distance for automatic detection of\n  defects on patterned textures", "comments": "IJCVR, Vol. 2, No. 4, 2011, pp. 302-313", "journal-ref": "IJCVR, Vol. 2, No. 4, 2011, pp. 302-313", "doi": "10.1504/IJCVR.2011.045267", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chi-square histogram distance is one of the distance measures that can be\nused to find dissimilarity between two histograms. Motivated by the fact that\ntexture discrimination by human vision system is based on second-order\nstatistics, we make use of histogram of gray-level co-occurrence matrix (GLCM)\nthat is based on second-order statistics and propose a new machine vision\nalgorithm for automatic defect detection on patterned textures. Input defective\nimages are split into several periodic blocks and GLCMs are computed after\nquantizing the gray levels from 0-255 to 0-63 to keep the size of GLCM compact\nand to reduce computation time. Dissimilarity matrix derived from chi-square\ndistances of the GLCMs is subjected to hierarchical clustering to automatically\nidentify defective and defect-free blocks. Effectiveness of the proposed method\nis demonstrated through experiments on defective real-fabric images of 2 major\nwallpaper groups (pmm and p4m groups).\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 13:40:41 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Asha", "V.", ""], ["Bhajantri", "N. U.", ""], ["Nagabhushan", "P.", ""]]}, {"id": "1212.0402", "submitter": "Khurram Soomro", "authors": "Khurram Soomro, Amir Roshan Zamir and Mubarak Shah", "title": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": "CRCV-TR-12-01", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce UCF101 which is currently the largest dataset of human actions.\nIt consists of 101 action classes, over 13k clips and 27 hours of video data.\nThe database consists of realistic user uploaded videos containing camera\nmotion and cluttered background. Additionally, we provide baseline action\nrecognition results on this new dataset using standard bag of words approach\nwith overall performance of 44.5%. To the best of our knowledge, UCF101 is\ncurrently the most challenging dataset of actions due to its large number of\nclasses, large number of clips and also unconstrained nature of such clips.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 14:45:31 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Soomro", "Khurram", ""], ["Zamir", "Amir Roshan", ""], ["Shah", "Mubarak", ""]]}, {"id": "1212.0433", "submitter": "Laurent Jacques", "authors": "Prasad Sudhakar, Laurent Jacques, Xavier Dubois, Philippe Antoine, Luc\n  Joannes", "title": "Compressive Schlieren Deflectometry", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schlieren deflectometry aims at characterizing the deflections undergone by\nrefracted incident light rays at any surface point of a transparent object. For\nsmooth surfaces, each surface location is actually associated with a sparse\ndeflection map (or spectrum). This paper presents a novel method to\ncompressively acquire and reconstruct such spectra. This is achieved by\naltering the way deflection information is captured in a common Schlieren\nDeflectometer, i.e., the deflection spectra are indirectly observed by the\nprinciple of spread spectrum compressed sensing. These observations are\nrealized optically using a 2-D Spatial Light Modulator (SLM) adjusted to the\ncorresponding sensing basis and whose modulations encode the light deviation\nsubsequently recorded by a CCD camera. The efficiency of this approach is\ndemonstrated experimentally on the observation of few test objects. Further,\nusing a simple parametrization of the deflection spectra we show that relevant\nkey parameters can be directly computed using the measurements, avoiding full\nreconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 16:21:07 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Sudhakar", "Prasad", ""], ["Jacques", "Laurent", ""], ["Dubois", "Xavier", ""], ["Antoine", "Philippe", ""], ["Joannes", "Luc", ""]]}, {"id": "1212.0655", "submitter": "Patrizio Frosini", "authors": "Patrizio Frosini", "title": "G-invariant Persistent Homology", "comments": "14 pages, 4 figures. Remark 4.2 has been expanded to become\n  Subsection 4.2, including a new example (Example 4.3). The section\n  \"Discussion and further research\" and some references have been added. Small\n  changes in the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical persistent homology is a powerful mathematical tool for shape\ncomparison. Unfortunately, it is not tailored to study the action of\ntransformation groups that are different from the group Homeo(X) of all\nself-homeomorphisms of a topological space X. This fact restricts its use in\napplications. In order to obtain better lower bounds for the natural\npseudo-distance d_G associated with a subgroup G of Homeo(X), we need to adapt\npersistent homology and consider G-invariant persistent homology. Roughly\nspeaking, the main idea consists in defining persistent homology by means of a\nset of chains that is invariant under the action of G. In this paper we\nformalize this idea, and prove the stability of the persistent Betti number\nfunctions in G-invariant persistent homology with respect to the natural\npseudo-distance d_G. We also show how G-invariant persistent homology could be\nused in applications concerning shape comparison, when the invariance group is\na proper subgroup of the group of all self-homeomorphisms of a topological\nspace. In this paper we will assume that the space X is triangulable, in order\nto guarantee that the persistent Betti number functions are finite without\nusing any tameness assumption.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 09:57:59 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2012 10:06:06 GMT"}, {"version": "v3", "created": "Fri, 8 Mar 2013 09:37:22 GMT"}, {"version": "v4", "created": "Thu, 4 Apr 2013 17:23:03 GMT"}, {"version": "v5", "created": "Sat, 21 Dec 2013 10:54:55 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Frosini", "Patrizio", ""]]}, {"id": "1212.0695", "submitter": "Emanuele Frandi", "authors": "Emanuele Frandi, Ricardo Nanculef, Maria Grazia Gasparo, Stefano Lodi,\n  Claudio Sartori", "title": "Training Support Vector Machines Using Frank-Wolfe Optimization Methods", "comments": null, "journal-ref": "International Journal on Pattern Recognition and Artificial\n  Intelligence, 27(3), 2013", "doi": "10.1142/S0218001413600033", "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a Support Vector Machine (SVM) requires the solution of a quadratic\nprogramming problem (QP) whose computational complexity becomes prohibitively\nexpensive for large scale datasets. Traditional optimization methods cannot be\ndirectly applied in these cases, mainly due to memory restrictions.\n  By adopting a slightly different objective function and under mild conditions\non the kernel used within the model, efficient algorithms to train SVMs have\nbeen devised under the name of Core Vector Machines (CVMs). This framework\nexploits the equivalence of the resulting learning problem with the task of\nbuilding a Minimal Enclosing Ball (MEB) problem in a feature space, where data\nis implicitly embedded by a kernel function.\n  In this paper, we improve on the CVM approach by proposing two novel methods\nto build SVMs based on the Frank-Wolfe algorithm, recently revisited as a fast\nmethod to approximate the solution of a MEB problem. In contrast to CVMs, our\nalgorithms do not require to compute the solutions of a sequence of\nincreasingly complex QPs and are defined by using only analytic optimization\nsteps. Experiments on a large collection of datasets show that our methods\nscale better than CVMs in most cases, sometimes at the price of a slightly\nlower accuracy. As CVMs, the proposed methods can be easily extended to machine\nlearning problems other than binary classification. However, effective\nclassifiers are also obtained using kernels which do not satisfy the condition\nrequired by CVMs and can thus be used for a wider set of problems.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 12:05:31 GMT"}], "update_date": "2014-01-29", "authors_parsed": [["Frandi", "Emanuele", ""], ["Nanculef", "Ricardo", ""], ["Gasparo", "Maria Grazia", ""], ["Lodi", "Stefano", ""], ["Sartori", "Claudio", ""]]}, {"id": "1212.0819", "submitter": "Evgeny Shchepin", "authors": "Evgeny Shchepin", "title": "A Topological Code for Plane Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is proposed a new code for contours of plane images. This code was applied\nfor optical character recognition of printed and handwritten characters. One\ncan apply it to recognition of any visual images.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 18:39:14 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Shchepin", "Evgeny", ""]]}, {"id": "1212.0888", "submitter": "Roozbeh Rajabi", "authors": "Roozbeh Rajabi, Hassan Ghassemian", "title": "Unmixing of Hyperspectral Data Using Robust Statistics-based NMF", "comments": "4 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed pixels are presented in hyperspectral images due to low spatial\nresolution of hyperspectral sensors. Spectral unmixing decomposes mixed pixels\nspectra into endmembers spectra and abundance fractions. In this paper using of\nrobust statistics-based nonnegative matrix factorization (RNMF) for spectral\nunmixing of hyperspectral data is investigated. RNMF uses a robust cost\nfunction and iterative updating procedure, so is not sensitive to outliers.\nThis method has been applied to simulated data using USGS spectral library,\nAVIRIS and ROSIS datasets. Unmixing results are compared to traditional NMF\nmethod based on SAD and AAD measures. Results demonstrate that this method can\nbe used efficiently for hyperspectral unmixing purposes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 21:59:35 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Rajabi", "Roozbeh", ""], ["Ghassemian", "Hassan", ""]]}, {"id": "1212.0935", "submitter": "Sankar Veeramoni", "authors": "Livio De La Cruz, Stephen Kobourov, Sergey Pupyrev, Paul Shen, Sankar\n  Veeramoni", "title": "Computing Consensus Curves", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of extracting accurate average ant trajectories from\nmany (possibly inaccurate) input trajectories contributed by citizen\nscientists. Although there are many generic software tools for motion tracking\nand specific ones for insect tracking, even untrained humans are much better at\nthis task, provided a robust method to computing the average trajectories. We\nimplemented and tested several local (one ant at a time) and global (all ants\ntogether) method. Our best performing algorithm uses a novel global method,\nbased on finding edge-disjoint paths in an ant-interaction graph constructed\nfrom the input trajectories. The underlying optimization problem is a new and\ninteresting variant of network flow. Even though the problem is NP-hard, we\nimplemented two heuristics, which work very well in practice, outperforming all\nother approaches, including the best automated system.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 05:52:12 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2013 04:13:05 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2013 17:27:07 GMT"}, {"version": "v4", "created": "Thu, 10 Apr 2014 14:44:12 GMT"}, {"version": "v5", "created": "Wed, 14 May 2014 22:57:16 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["De La Cruz", "Livio", ""], ["Kobourov", "Stephen", ""], ["Pupyrev", "Sergey", ""], ["Shen", "Paul", ""], ["Veeramoni", "Sankar", ""]]}, {"id": "1212.1073", "submitter": "Jinshan Pan", "authors": "Jinshan Pan, Risheng Liu, Zhixun Su, Xianfeng Gu", "title": "Kernel Estimation from Salient Structure for Robust Motion Deblurring", "comments": "This work has been accepted by Signal Processing: Image\n  Communication, 2013", "journal-ref": "Signal Processing: Image Communication, 2013", "doi": "10.1016/j.image.2013.05.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image deblurring algorithms have been improving steadily in the past\nyears. Most state-of-the-art algorithms, however, still cannot perform\nperfectly in challenging cases, especially in large blur setting. In this\npaper, we focus on how to estimate a good kernel estimate from a single blurred\nimage based on the image structure. We found that image details caused by\nblurring could adversely affect the kernel estimation, especially when the blur\nkernel is large. One effective way to eliminate these details is to apply image\ndenoising model based on the Total Variation (TV). First, we developed a novel\nmethod for computing image structures based on TV model, such that the\nstructures undermining the kernel estimation will be removed. Second, to\nmitigate the possible adverse effect of salient edges and improve the\nrobustness of kernel estimation, we applied a gradient selection method. Third,\nwe proposed a novel kernel estimation method, which is capable of preserving\nthe continuity and sparsity of the kernel and reducing the noises. Finally, we\ndeveloped an adaptive weighted spatial prior, for the purpose of preserving\nsharp edges in latent image restoration. The effectiveness of our method is\ndemonstrated by experiments on various kinds of challenging examples.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 16:02:43 GMT"}, {"version": "v2", "created": "Sat, 24 May 2014 09:11:31 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Pan", "Jinshan", ""], ["Liu", "Risheng", ""], ["Su", "Zhixun", ""], ["Gu", "Xianfeng", ""]]}, {"id": "1212.1313", "submitter": "Debajyoti Banerji", "authors": "Debajyoti Banerji, Ranjit Ray, Jhankar Basu and Indrajit Basak", "title": "Autonomous Navigation by Robust Scan Matching Technique", "comments": "7 pages, 9 figures", "journal-ref": "INTERNATIONAL JOURNAL OF INNOVATIVE TECHNOLOGY AND CREATIVE\n  ENGINEERING (ISSN:2045-8711), VOL.2 NO.10 OCTOBER 2012", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For effective autonomous navigation,estimation of the pose of the robot is\nessential at every sampling time. For computing an accurate\nestimation,odometric error needs to be reduced with the help of data from\nexternal sensor. In this work, a technique has been developed for accurate pose\nestimation of mobile robot by using Laser Range data. The technique is robust\nto noisy data, which may contain considerable amount of outliers. A grey image\nis formed from laser range data and the key points from this image are\nextracted by Harris corner detector. The matching of the key points from\nconsecutive data sets have been done while outliers have been rejected by\nRANSAC method. Robot state is measured by the correspondence between the two\nsets of keypoints. Finally, optimal robot state is estimated by Extended Kalman\nFilter. The technique has been applied to an operational robot in the\nlaboratory environment to show the robustness of the technique in presence of\nnoisy sensor data. The performance of this new technique has been compared with\nthat of conventional ICP method. Through this method, effective and accurate\nnavigation has been achieved even in presence of substantial noise in the\nsensor data at the cost of a small amount of additional computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 12:53:26 GMT"}], "update_date": "2012-12-07", "authors_parsed": [["Banerji", "Debajyoti", ""], ["Ray", "Ranjit", ""], ["Basu", "Jhankar", ""], ["Basak", "Indrajit", ""]]}, {"id": "1212.1329", "submitter": "Asha V", "authors": "V. Asha, N. U. Bhajantri, and P. Nagabhushan", "title": "Automatic Detection of Texture Defects Using Texture-Periodicity and\n  Gabor Wavelets", "comments": "06 Pages, 04 Figures, ICIP 2011", "journal-ref": "CCIS 157, Computer Networks and Intelligent Computing, Part 9, pp.\n  548-553, Springer-Verlag, Berlin Heidelberg, 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a machine vision algorithm for automatically\ndetecting defects in textures belonging to 16 out of 17 wallpaper groups using\ntexture-periodicity and a family of Gabor wavelets. Input defective images are\nsubjected to Gabor wavelet transformation in multi-scales and\nmulti-orientations and a resultant image is obtained in L2 norm. The resultant\nimage is split into several periodic blocks and energy of each block is used as\na feature space to automatically identify defective and defect-free blocks\nusing Ward's hierarchical clustering. Experiments on defective fabric images of\nthree major wallpaper groups, namely, pmm, p2 and p4m, show that the proposed\nmethod is robust in finding fabric defects without human intervention and can\nbe used for automatic defect detection in fabric industries.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 14:17:21 GMT"}], "update_date": "2012-12-07", "authors_parsed": [["Asha", "V.", ""], ["Bhajantri", "N. U.", ""], ["Nagabhushan", "P.", ""]]}, {"id": "1212.1617", "submitter": "Anil Maheshwari", "authors": "Jean-Lou De Carufel, Amin Gheibi, Anil Maheshwari, J\\\"org-R\\\"udiger\n  Sack, Christian Scheffer", "title": "Similarity of Polygonal Curves in the Presence of Outliers", "comments": "Replaces the earlier version of this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fr\\'{e}chet distance is a well studied and commonly used measure to\ncapture the similarity of polygonal curves. Unfortunately, it exhibits a high\nsensitivity to the presence of outliers. Since the presence of outliers is a\nfrequently occurring phenomenon in practice, a robust variant of Fr\\'{e}chet\ndistance is required which absorbs outliers. We study such a variant here. In\nthis modified variant, our objective is to minimize the length of subcurves of\ntwo polygonal curves that need to be ignored (MinEx problem), or alternately,\nmaximize the length of subcurves that are preserved (MaxIn problem), to achieve\na given Fr\\'{e}chet distance. An exact solution to one problem would imply an\nexact solution to the other problem. However, we show that these problems are\nnot solvable by radicals over $\\mathbb{Q}$ and that the degree of the\npolynomial equations involved is unbounded in general. This motivates the\nsearch for approximate solutions. We present an algorithm, which approximates,\nfor a given input parameter $\\delta$, optimal solutions for the \\MinEx\\ and\n\\MaxIn\\ problems up to an additive approximation error $\\delta$ times the\nlength of the input curves. The resulting running time is upper bounded by\n$\\mathcal{O} \\left(\\frac{n^3}{\\delta} \\log \\left(\\frac{n}{\\delta}\n\\right)\\right)$, where $n$ is the complexity of the input polygonal curves.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2012 14:22:12 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2013 14:26:19 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["De Carufel", "Jean-Lou", ""], ["Gheibi", "Amin", ""], ["Maheshwari", "Anil", ""], ["Sack", "J\u00f6rg-R\u00fcdiger", ""], ["Scheffer", "Christian", ""]]}, {"id": "1212.1819", "submitter": "Edwin Carlinet", "authors": "Edwin Carlinet and Thierry G\\'eraud", "title": "A fair comparison of many max-tree computation algorithms (Extended\n  version of the paper submitted to ISMM 2013", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of connected filters for the last decade, many\nalgorithms have been proposed to compute the max-tree. Max-tree allows to\ncompute the most advanced connected operators in a simple way. However, no fair\ncomparison of algorithms has been proposed yet and the choice of an algorithm\nover an other depends on many parameters. Since the need of fast algorithms is\nobvious for production code, we present an in depth comparison of five\nalgorithms and some variations of them in a unique framework. Finally, a\ndecision tree will be proposed to help user in choosing the right algorithm\nwith respect to their data.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2012 17:38:40 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2013 10:33:27 GMT"}], "update_date": "2013-01-11", "authors_parsed": [["Carlinet", "Edwin", ""], ["G\u00e9raud", "Thierry", ""]]}, {"id": "1212.1863", "submitter": "Madhumita Sengupta", "authors": "Madhumita Sengupta and J. K. Mandal", "title": "Self Authentication of image through Daubechies Transform technique\n  (SADT)", "comments": "4 page paper in 47th Annual National Convention of COMPUTER SOCIETY\n  OF INDIA, The First International Conference on Intelligent Infrastructure,\n  CSI-2012, held during 1st and 2nd December, 2012 at science city, Kolkata", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a 4 x 4 Daubechies transform based authentication technique\ntermed as SADT has been proposed to authenticate gray scale images. The cover\nimage is transformed into the frequency domain using 4 x 4 mask in a row major\norder using Daubechies transform technique, resulting four frequency subbands\nAF, HF, VF and DF. One byte of every band in a mask is embedding with two or\nfour bits of secret information. Experimental results are computed and compared\nwith the existing authentication techniques like Li s method [5], SCDFT [6],\nRegion-Based method [7] and other similar techniques based on Mean Square Error\n(MSE), Peak Signal to Noise Ratio (PSNR) and Image Fidelity (IF), which shows\nbetter performance in SADT.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2012 07:38:33 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Sengupta", "Madhumita", ""], ["Mandal", "J. K.", ""]]}, {"id": "1212.2245", "submitter": "Martin Welk", "authors": "Martin Welk, Patrik Raudaschl, Thomas Schwarzbauer, Martin Erler,\n  Martin L\\\"auter", "title": "Fast and Robust Linear Motion Deblurring", "comments": null, "journal-ref": "Signal, Image and Video Processing, 9 (2015) 1221-1234", "doi": "10.1007/s11760-013-0563-x", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate efficient algorithmic realisations for robust deconvolution of\ngrey-value images with known space-invariant point-spread function, with\nemphasis on 1D motion blur scenarios. The goal is to make deconvolution\nsuitable as preprocessing step in automated image processing environments with\ntight time constraints. Candidate deconvolution methods are selected for their\nrestoration quality, robustness and efficiency. Evaluation of restoration\nquality and robustness on synthetic and real-world test images leads us to\nfocus on a combination of Wiener filtering with few iterations of robust and\nregularised Richardson-Lucy deconvolution. We discuss algorithmic optimisations\nfor specific scenarios. In the case of uniform linear motion blur in coordinate\ndirection, it is possible to achieve real-time performance (less than 50 ms) in\nsingle-threaded CPU computation on images of $256\\times256$ pixels. For more\ngeneral space-invariant blur settings, still favourable computation times are\nobtained. Exemplary parallel implementations demonstrate that the proposed\nmethod also achieves real-time performance for general 1D motion blurs in a\nmulti-threaded CPU setting, and for general 2D blurs on a GPU.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 23:00:10 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Welk", "Martin", ""], ["Raudaschl", "Patrik", ""], ["Schwarzbauer", "Thomas", ""], ["Erler", "Martin", ""], ["L\u00e4uter", "Martin", ""]]}, {"id": "1212.2278", "submitter": "Carl Vondrick", "authors": "Carl Vondrick and Aditya Khosla and Tomasz Malisiewicz and Antonio\n  Torralba", "title": "Inverting and Visualizing Features for Object Detection", "comments": "This paper is a preprint of our conference paper. We have made it\n  available early in the hopes that others find it useful", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce algorithms to visualize feature spaces used by object detectors.\nThe tools in this paper allow a human to put on `HOG goggles' and perceive the\nvisual world as a HOG based object detector sees it. We found that these\nvisualizations allow us to analyze object detection systems in new ways and\ngain new insight into the detector's failures. For example, when we visualize\nthe features for high scoring false alarms, we discovered that, although they\nare clearly wrong in image space, they do look deceptively similar to true\npositives in feature space. This result suggests that many of these false\nalarms are caused by our choice of feature space, and indicates that creating a\nbetter learning algorithm or building bigger datasets is unlikely to correct\nthese errors. By visualizing feature spaces, we can gain a more intuitive\nunderstanding of our detection systems.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 01:59:51 GMT"}, {"version": "v2", "created": "Sun, 5 May 2013 18:17:44 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Vondrick", "Carl", ""], ["Khosla", "Aditya", ""], ["Malisiewicz", "Tomasz", ""], ["Torralba", "Antonio", ""]]}, {"id": "1212.2415", "submitter": "Hyonil Kim", "authors": "Song Han, Jinsong Kim, Cholhun Kim, Jongchol Jo, and Sunam Han", "title": "Robust Face Recognition using Local Illumination Normalization and\n  Discriminant Feature Point Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition systems must be robust to the variation of various factors\nsuch as facial expression, illumination, head pose and aging. Especially, the\nrobustness against illumination variation is one of the most important problems\nto be solved for the practical use of face recognition systems. Gabor wavelet\nis widely used in face detection and recognition because it gives the\npossibility to simulate the function of human visual system. In this paper, we\npropose a method for extracting Gabor wavelet features which is stable under\nthe variation of local illumination and show experiment results demonstrating\nits effectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 13:19:54 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Han", "Song", ""], ["Kim", "Jinsong", ""], ["Kim", "Cholhun", ""], ["Jo", "Jongchol", ""], ["Han", "Sunam", ""]]}, {"id": "1212.2546", "submitter": "Jonathan Masci", "authors": "Jonathan Masci and Jes\\'us Angulo and J\\\"urgen Schmidhuber", "title": "A Learning Framework for Morphological Operators using Counter-Harmonic\n  Mean", "comments": "Submitted to ISMM'13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for learning morphological operators using\ncounter-harmonic mean. It combines concepts from morphology and convolutional\nneural networks. A thorough experimental validation analyzes basic\nmorphological operators dilation and erosion, opening and closing, as well as\nthe much more complex top-hat transform, for which we report a real-world\napplication from the steel industry. Using online learning and stochastic\ngradient descent, our system learns both the structuring element and the\ncomposition of operators. It scales well to large datasets and online settings.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 17:29:04 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Masci", "Jonathan", ""], ["Angulo", "Jes\u00fas", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}, {"id": "1212.2692", "submitter": "Ghazali Osman", "authors": "Ghazali Osman, Muhammad Suzuri Hitam and Mohd Nasir Ismail", "title": "Enhanced skin colour classifier using RGB Ratio model", "comments": "14 pages; International Journal on Soft Computing (IJSC) Vol.3, No.4,\n  November 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skin colour detection is frequently been used for searching people, face\ndetection, pornographic filtering and hand tracking. The presence of skin or\nnon-skin in digital image can be determined by manipulating pixels colour or\npixels texture. The main problem in skin colour detection is to represent the\nskin colour distribution model that is invariant or least sensitive to changes\nin illumination condition. Another problem comes from the fact that many\nobjects in the real world may possess almost similar skin-tone colour such as\nwood, leather, skin-coloured clothing, hair and sand. Moreover, skin colour is\ndifferent between races and can be different from a person to another, even\nwith people of the same ethnicity. Finally, skin colour will appear a little\ndifferent when different types of camera are used to capture the object or\nscene. The objective in this study is to develop a skin colour classifier based\non pixel-based using RGB ratio model. The RGB ratio model is a newly proposed\nmethod that belongs under the category of an explicitly defined skin region\nmodel. This skin classifier was tested with SIdb dataset and two benchmark\ndatasets; UChile and TDSD datasets to measure classifier performance. The\nperformance of skin classifier was measured based on true positive (TF) and\nfalse positive (FP) indicator. This newly proposed model was compared with\nKovac, Saleh and Swift models. The experimental results showed that the RGB\nratio model outperformed all the other models in term of detection rate. The\nRGB ratio model is able to reduce FP detection that caused by reddish objects\ncolour as well as be able to detect darkened skin and skin covered by shadow.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 03:01:00 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Osman", "Ghazali", ""], ["Hitam", "Muhammad Suzuri", ""], ["Ismail", "Mohd Nasir", ""]]}, {"id": "1212.2823", "submitter": "Shuran Song", "authors": "Shuran Song, Jianxiong Xiao", "title": "Tracking Revisited using RGBD Camera: Baseline and Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there has been significant progress in the past decade,tracking is\nstill a very challenging computer vision task, due to problems such as\nocclusion and model drift.Recently, the increased popularity of depth sensors\ne.g. Microsoft Kinect has made it easy to obtain depth data at low cost.This\nmay be a game changer for tracking, since depth information can be used to\nprevent model drift and handle occlusion.In this paper, we construct a\nbenchmark dataset of 100 RGBD videos with high diversity, including deformable\nobjects, various occlusion conditions and moving cameras. We propose a very\nsimple but strong baseline model for RGBD tracking, and present a quantitative\ncomparison of several state-of-the-art tracking algorithms.Experimental results\nshow that including depth information and reasoning about occlusion\nsignificantly improves tracking performance. The datasets, evaluation details,\nsource code for the baseline algorithm, and instructions for submitting new\nmodels will be made available online after acceptance.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 14:02:41 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Song", "Shuran", ""], ["Xiao", "Jianxiong", ""]]}, {"id": "1212.2860", "submitter": "Jan Egger", "authors": "Jan Egger, Tina Kapur, Christopher Nimsky, Ron Kikinis", "title": "Pituitary Adenoma Volumetry with 3D Slicer", "comments": "7 pages, 5 figures, 2 tables, 30 references", "journal-ref": "(2012) PLoS ONE 7(12): e51788", "doi": "10.1371/journal.pone.0051788", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present pituitary adenoma volumetry using the free and open\nsource medical image computing platform for biomedical research: (3D) Slicer.\nVolumetric changes in cerebral pathologies like pituitary adenomas are a\ncritical factor in treatment decisions by physicians and in general the volume\nis acquired manually. Therefore, manual slice-by-slice segmentations in\nmagnetic resonance imaging (MRI) data, which have been obtained at regular\nintervals, are performed. In contrast to this manual time consuming\nslice-by-slice segmentation process Slicer is an alternative which can be\nsignificantly faster and less user intensive. In this contribution, we compare\npure manual segmentations of ten pituitary adenomas with semi-automatic\nsegmentations under Slicer. Thus, physicians drew the boundaries completely\nmanually on a slice-by-slice basis and performed a Slicer-enhanced segmentation\nusing the competitive region-growing based module of Slicer named GrowCut.\nResults showed that the time and user effort required for GrowCut-based\nsegmentations were on average about thirty percent less than the pure manual\nsegmentations. Furthermore, we calculated the Dice Similarity Coefficient (DSC)\nbetween the manual and the Slicer-based segmentations to proof that the two are\ncomparable yielding an average DSC of 81.97\\pm3.39%.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 16:12:32 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Egger", "Jan", ""], ["Kapur", "Tina", ""], ["Nimsky", "Christopher", ""], ["Kikinis", "Ron", ""]]}, {"id": "1212.3034", "submitter": "Rastislav Telgarsky", "authors": "Rastislav Telgarsky", "title": "Multi-target tracking algorithms in 3D", "comments": "7 pages, 2 figures, conference proceedings", "journal-ref": "Scientific Issues, MATHEMATICA IV, Ruzomberok 2012", "doi": null, "report-no": null, "categories": "cs.CV cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ladars provide a unique capability for identification of objects and motions\nin scenes with fixed 3D field of view (FOV). This paper describes algorithms\nfor multi-target tracking in 3D scenes including the preprocessing\n(mathematical morphology and Parzen windows), labeling of connected components,\nsorting of targets by selectable attributes (size, length of track, velocity),\nand handling of target states (acquired, coasting, re-acquired and tracked) in\norder to assemble the target trajectories. This paper is derived from working\nalgorithms coded in Matlab, which were tested and reviewed by others, and does\nnot speculate about usage of general formulas or frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 01:55:15 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["Telgarsky", "Rastislav", ""]]}, {"id": "1212.3268", "submitter": "Gilles Puy", "authors": "Gilles Puy and Pierre Vandergheynst", "title": "Robust image reconstruction from multi-view measurements", "comments": "Accepted in SIAM Journal on Imaging Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to accurately reconstruct a set of images\nrepresenting a single scene from few linear multi-view measurements. Each\nobserved image is modeled as the sum of a background image and a foreground\none. The background image is common to all observed images but undergoes\ngeometric transformations, as the scene is observed from different viewpoints.\nIn this paper, we assume that these geometric transformations are represented\nby a few parameters, e.g., translations, rotations, affine transformations,\netc.. The foreground images differ from one observed image to another, and are\nused to model possible occlusions of the scene. The proposed reconstruction\nalgorithm estimates jointly the images and the transformation parameters from\nthe available multi-view measurements. The ideal solution of this multi-view\nimaging problem minimizes a non-convex functional, and the reconstruction\ntechnique is an alternating descent method built to minimize this functional.\nThe convergence of the proposed algorithm is studied, and conditions under\nwhich the sequence of estimated images and parameters converges to a critical\npoint of the non-convex functional are provided. Finally, the efficiency of the\nalgorithm is demonstrated using numerical simulations for applications such as\ncompressed sensing or super-resolution.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 19:00:17 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2013 10:52:45 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2013 22:38:58 GMT"}], "update_date": "2013-09-19", "authors_parsed": [["Puy", "Gilles", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1212.3373", "submitter": "Jyotsna  Kumar Prof.", "authors": "J. K. Mandal and Somnath Mukhopadhyay", "title": "A Novel Directional Weighted Minimum Deviation (DWMD) Based Filter for\n  Removal of Random Valued Impulse Noise", "comments": "7 page paper in Proceeding of International Conference on Computing\n  and Systems ICCS 2010, ISBN 93-80813-01-5, pp 151-155, University of Burdwan,\n  19th, 20th November, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most median-based de noising methods works fine for restoring the images\ncorrupted by Randomn Valued Impulse Noise with low noise level but very poor\nwith highly corrupted images. In this paper a directional weighted minimum\ndeviation (DWMD) based filter has been proposed for removal of high random\nvalued impulse noise (RVIN). The proposed approach based on Standard Deviation\n(SD) works in two phases. The first phase detects the contaminated pixels by\ndifferencing between the test pixel and its neighbor pixels aligned with four\nmain directions. The second phase filters only those pixels keeping others\nintact. The filtering scheme is based on minimum standard deviation of the four\ndirectional pixels. Extensive simulations show that the proposed filter not\nonly provide better performance of de noising RVIN but can preserve more\ndetails features even thin lines or dots. This technique shows better\nperformance in terms of PSNR, Image Fidelity and Computational Cost compared to\nthe existing filters.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 00:13:11 GMT"}], "update_date": "2012-12-17", "authors_parsed": [["Mandal", "J. K.", ""], ["Mukhopadhyay", "Somnath", ""]]}, {"id": "1212.3385", "submitter": "Mao Shi PhD", "authors": "Mao Shi, Jiansong Deng", "title": "Approximating rational Bezier curves by constrained Bezier curves of\n  arbitrary degree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method to obtain a constrained approximation of a\nrational B\\'{e}zier curve by a polynomial B\\'{e}zier curve. This problem is\nreformulated as an approximation problem between two polynomial B\\'{e}zier\ncurves based on weighted least-squares method, where weight functions\n$\\rho(t)=\\omega(t)$ and $\\rho(t)=\\omega(t)^{2}$ are studied respectively. The\nefficiency of the proposed method is tested using some examples.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 02:56:30 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2013 10:56:56 GMT"}, {"version": "v3", "created": "Wed, 15 May 2013 07:44:18 GMT"}, {"version": "v4", "created": "Sat, 13 Sep 2014 04:33:16 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Shi", "Mao", ""], ["Deng", "Jiansong", ""]]}, {"id": "1212.3530", "submitter": "Erik Bekkers", "authors": "Erik Bekkers, Remco Duits, Tos Berendschot, Bart ter Haar Romeny", "title": "A Multi-Orientation Analysis Approach to Retinal Vessel Tracking", "comments": "Accepted at JMIV. The final publication will become available at\n  springerlink.com", "journal-ref": "Journal of Mathematical Imaging and Vision 49(3) (2014) 583-610", "doi": "10.1007/s10851-013-0488-6", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for retinal vasculature extraction based on\nbiologically inspired multi-orientation analysis. We apply multi-orientation\nanalysis via so-called invertible orientation scores, modeling the cortical\ncolumns in the visual system of higher mammals. This allows us to generically\ndeal with many hitherto complex problems inherent to vessel tracking, such as\ncrossings, bifurcations, parallel vessels, vessels of varying widths and\nvessels with high curvature. Our approach applies tracking in invertible\norientation scores via a novel geometrical principle for curve optimization in\nthe Euclidean motion group SE(2). The method runs fully automatically and\nprovides a detailed model of the retinal vasculature, which is crucial as a\nsound basis for further quantitative analysis of the retina, especially in\nscreening applications.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 17:04:03 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2012 12:45:20 GMT"}, {"version": "v3", "created": "Tue, 26 Feb 2013 15:40:38 GMT"}, {"version": "v4", "created": "Sat, 2 Mar 2013 10:50:37 GMT"}, {"version": "v5", "created": "Mon, 30 Dec 2013 10:17:35 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Bekkers", "Erik", ""], ["Duits", "Remco", ""], ["Berendschot", "Tos", ""], ["Romeny", "Bart ter Haar", ""]]}, {"id": "1212.3767", "submitter": "Hao Wooi Lim", "authors": "Hao Wooi Lim and Yong Haur Tay", "title": "Visual Objects Classification with Sliding Spatial Pyramid Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for visual object classification using only a single\nfeature, transformed color SIFT with a variant of Spatial Pyramid Matching\n(SPM) that we called Sliding Spatial Pyramid Matching (SSPM), trained with an\nensemble of linear regression (provided by LINEAR) to obtained state of the art\nresult on Caltech-101 of 83.46%. SSPM is a special version of SPM where instead\nof dividing an image into K number of regions, a subwindow of fixed size is\nslide around the image with a fixed step size. For each subwindow, a histogram\nof visual words is generated. To obtained the visual vocabulary, instead of\nperforming K-means clustering, we randomly pick N exemplars from the training\nset and encode them with a soft non-linear mapping method. We then trained 15\nmodels, each with a different visual word size with linear regression. All 15\nmodels are then averaged together to form a single strong model.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2012 09:10:54 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2012 15:36:05 GMT"}], "update_date": "2012-12-19", "authors_parsed": [["Lim", "Hao Wooi", ""], ["Tay", "Yong Haur", ""]]}, {"id": "1212.3913", "submitter": "Guoxu Zhou", "authors": "Guoxu Zhou and Andrzej Cichocki and Yu Zhang and Danilo Mandic", "title": "Group Component Analysis for Multiblock Data: Common and Individual\n  Feature Extraction", "comments": "13 pages,11 figures", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, Volume:\n  27, Issue: 11, Nov. 2016", "doi": "10.1109/TNNLS.2015.2487364", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very often data we encounter in practice is a collection of matrices rather\nthan a single matrix. These multi-block data are naturally linked and hence\noften share some common features and at the same time they have their own\nindividual features, due to the background in which they are measured and\ncollected. In this study we proposed a new scheme of common and individual\nfeature analysis (CIFA) that processes multi-block data in a linked way aiming\nat discovering and separating their common and individual features. According\nto whether the number of common features is given or not, two efficient\nalgorithms were proposed to extract the common basis which is shared by all\ndata. Then feature extraction is performed on the common and the individual\nspaces separately by incorporating the techniques such as dimensionality\nreduction and blind source separation. We also discussed how the proposed CIFA\ncan significantly improve the performance of classification and clustering\ntasks by exploiting common and individual features of samples respectively. Our\nexperimental results show some encouraging features of the proposed methods in\ncomparison to the state-of-the-art methods on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 07:56:15 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2013 02:24:36 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2015 02:20:23 GMT"}, {"version": "v4", "created": "Sun, 12 Mar 2017 08:36:27 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Zhou", "Guoxu", ""], ["Cichocki", "Andrzej", ""], ["Zhang", "Yu", ""], ["Mandic", "Danilo", ""]]}, {"id": "1212.4490", "submitter": "Niloy Mitra Dr", "authors": "Xiaohua Xie, Kai Xu, Niloy J. Mitra, Daniel Cohen-Or, Baoquan Chen", "title": "Sketch-to-Design: Context-based Part Assembly", "comments": "11 pages; Executable: see project webpage", "journal-ref": null, "doi": "10.1111/cgf.12200", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing 3D objects from scratch is difficult, especially when the user\nintent is fuzzy without a clear target form. In the spirit of\nmodeling-by-example, we facilitate design by providing reference and\ninspiration from existing model contexts. We rethink model design as navigating\nthrough different possible combinations of part assemblies based on a large\ncollection of pre-segmented 3D models. We propose an interactive\nsketch-to-design system, where the user sketches prominent features of parts to\ncombine. The sketched strokes are analyzed individually and in context with the\nother parts to generate relevant shape suggestions via a design gallery\ninterface. As the session progresses and more parts get selected, contextual\ncues becomes increasingly dominant and the system quickly converges to a final\ndesign. As a key enabler, we use pre-learned part-based contextual information\nto allow the user to quickly explore different combinations of parts. Our\nexperiments demonstrate the effectiveness of our approach for efficiently\ndesigning new variations from existing shapes.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2012 12:52:26 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Xie", "Xiaohua", ""], ["Xu", "Kai", ""], ["Mitra", "Niloy J.", ""], ["Cohen-Or", "Daniel", ""], ["Chen", "Baoquan", ""]]}, {"id": "1212.4522", "submitter": "Yunchao Gong", "authors": "Yunchao Gong and Qifa Ke and Michael Isard and Svetlana Lazebnik", "title": "A Multi-View Embedding Space for Modeling Internet Images, Tags, and\n  their Semantics", "comments": "To Appear: International Journal of Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of modeling Internet images and\nassociated text or tags for tasks such as image-to-image search, tag-to-image\nsearch, and image-to-tag search (image annotation). We start with canonical\ncorrelation analysis (CCA), a popular and successful approach for mapping\nvisual and textual features to the same latent space, and incorporate a third\nview capturing high-level image semantics, represented either by a single\ncategory or multiple non-mutually-exclusive concepts. We present two ways to\ntrain the three-view embedding: supervised, with the third view coming from\nground-truth labels or search keywords; and unsupervised, with semantic themes\nautomatically obtained by clustering the tags. To ensure high accuracy for\nretrieval tasks while keeping the learning process scalable, we combine\nmultiple strong visual features and use explicit nonlinear kernel mappings to\nefficiently approximate kernel CCA. To perform retrieval, we use a specially\ndesigned similarity function in the embedded space, which substantially\noutperforms the Euclidean distance. The resulting system produces compelling\nqualitative results and outperforms a number of two-view baselines on retrieval\ntasks on three large-scale Internet image datasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2012 22:02:43 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2013 19:14:58 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["Gong", "Yunchao", ""], ["Ke", "Qifa", ""], ["Isard", "Michael", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1212.4527", "submitter": "Quan Wang", "authors": "Quan Wang", "title": "GMM-Based Hidden Markov Random Field for Color Image and 3D Volume\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we first study the Gaussian-based hidden Markov random field\n(HMRF) model and its expectation-maximization (EM) algorithm. Then we\ngeneralize it to Gaussian mixture model-based hidden Markov random field. The\nalgorithm is implemented in MATLAB. We also apply this algorithm to color image\nsegmentation problems and 3D volume segmentation problems.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2012 22:30:23 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Wang", "Quan", ""]]}, {"id": "1212.4608", "submitter": "Vittal Premachandran", "authors": "Vittal Premachandran, Ramakrishna Kakarala", "title": "Perceptually Motivated Shape Context Which Uses Shape Interiors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we identify some of the limitations of current-day shape\nmatching techniques. We provide examples of how contour-based shape matching\ntechniques cannot provide a good match for certain visually similar shapes. To\novercome this limitation, we propose a perceptually motivated variant of the\nwell-known shape context descriptor. We identify that the interior properties\nof the shape play an important role in object recognition and develop a\ndescriptor that captures these interior properties. We show that our method can\neasily be augmented with any other shape matching algorithm. We also show from\nour experiments that the use of our descriptor can significantly improve the\nretrieval rates.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 09:40:09 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Premachandran", "Vittal", ""], ["Kakarala", "Ramakrishna", ""]]}, {"id": "1212.4871", "submitter": "Ramin  Norousi", "authors": "Ramin Norousi, Stephan Wickles, Christoph Leidig, Thomas Becker,\n  Volker J. Schmid, Roland Beckmann, Achim Tresch", "title": "Automatic post-picking using MAPPOS improves particle image detection\n  from Cryo-EM micrographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryo-electron microscopy (cryo-EM) studies using single particle\nreconstruction are extensively used to reveal structural information on\nmacromolecular complexes. Aiming at the highest achievable resolution, state of\nthe art electron microscopes automatically acquire thousands of high-quality\nmicrographs. Particles are detected on and boxed out from each micrograph using\nfully- or semi-automated approaches. However, the obtained particles still\nrequire laborious manual post-picking classification, which is one major\nbottleneck for single particle analysis of large datasets. We introduce MAPPOS,\na supervised post-picking strategy for the classification of boxed particle\nimages, as additional strategy adding to the already efficient automated\nparticle picking routines. MAPPOS employs machine learning techniques to train\na robust classifier from a small number of characteristic image features. In\norder to accurately quantify the performance of MAPPOS we used simulated\nparticle and non-particle images. In addition, we verified our method by\napplying it to an experimental cryo-EM dataset and comparing the results to the\nmanual classification of the same dataset. Comparisons between MAPPOS and\nmanual post-picking classification by several human experts demonstrated that\nmerely a few hundred sample images are sufficient for MAPPOS to classify an\nentire dataset with a human-like performance. MAPPOS was shown to greatly\naccelerate the throughput of large datasets by reducing the manual workload by\norders of magnitude while maintaining a reliable identification of non-particle\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 22:17:18 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Norousi", "Ramin", ""], ["Wickles", "Stephan", ""], ["Leidig", "Christoph", ""], ["Becker", "Thomas", ""], ["Schmid", "Volker J.", ""], ["Beckmann", "Roland", ""], ["Tresch", "Achim", ""]]}, {"id": "1212.4920", "submitter": "Kun Tang", "authors": "Jianya Guo, Xi Mei, Kun Tang", "title": "Automatic landmark annotation and dense correspondence registration for\n  3D human facial images", "comments": "33 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense surface registration of three-dimensional (3D) human facial images\nholds great potential for studies of human trait diversity, disease genetics,\nand forensics. Non-rigid registration is particularly useful for establishing\ndense anatomical correspondences between faces. Here we describe a novel\nnon-rigid registration method for fully automatic 3D facial image mapping. This\nmethod comprises two steps: first, seventeen facial landmarks are automatically\nannotated, mainly via PCA-based feature recognition following 3D-to-2D data\ntransformation. Second, an efficient thin-plate spline (TPS) protocol is used\nto establish the dense anatomical correspondence between facial images, under\nthe guidance of the predefined landmarks. We demonstrate that this method is\nrobust and highly accurate, even for different ethnicities. The average face is\ncalculated for individuals of Han Chinese and Uyghur origins. While fully\nautomatic and computationally efficient, this method enables high-throughput\nanalysis of human facial feature variation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 04:31:00 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Guo", "Jianya", ""], ["Mei", "Xi", ""], ["Tang", "Kun", ""]]}, {"id": "1212.5352", "submitter": "Kah Keong Chua", "authors": "Kah Keong Chua, Yong Haur Tay", "title": "On the Adaptability of Neural Network Image Super-Resolution", "comments": "Image Super Resolution, Neural Network, Multilayer Perceptron, Mean\n  Squared Error, Peak Signal-to-Noise Ratio, Structural Similarity Index", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we described and developed a framework for Multilayer\nPerceptron (MLP) to work on low level image processing, where MLP will be used\nto perform image super-resolution. Meanwhile, MLP are trained with different\ntypes of images from various categories, hence analyse the behaviour and\nperformance of the neural network. The tests are carried out using qualitative\ntest, in which Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR) and\nStructural Similarity Index (SSIM). The results showed that MLP trained with\nsingle image category can perform reasonably well compared to methods proposed\nby other researchers.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 07:30:38 GMT"}], "update_date": "2012-12-24", "authors_parsed": [["Chua", "Kah Keong", ""], ["Tay", "Yong Haur", ""]]}, {"id": "1212.5454", "submitter": "Omid David", "authors": "Omid David and Rabin Gerrah", "title": "In Vivo Quantification of Clot Formation in Extracorporeal Circuits", "comments": "In Proceedings of 20th NextMed Medicine Meets Virtual Reality\n  Conference (NextMed / MMVR20), San Diego, CA, February 2013", "journal-ref": "Studies in Health Technology and Informatics, Vol. 184, pp.\n  148--150, January 2013", "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clot formation is a common complication in extracorporeal circuits. In this\npaper we describe a novel method for clot formation analysis using image\nprocessing. We assembled a closed extracorporeal circuit and circulated blood\nat varying speeds. Blood filters were placed in downstream of the flow, and\nclotting agents were added to the circuit. Digital images of the filter were\nsubsequently taken, and image analysis was applied to calculate the density of\nthe clot. Our results show a significant correlation between the cumulative\nsize of the clots, the density measure of the clot based on image analysis, and\nflow duration in the system.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 14:20:05 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["David", "Omid", ""], ["Gerrah", "Rabin", ""]]}, {"id": "1212.5656", "submitter": "Zhongwei  TANG", "authors": "Zhongwei Tang, Rafael Grompone von Gioi, Pascal Monasse, Jean-Michel\n  Morel", "title": "High-precision camera distortion measurements with a \"calibration harp\"", "comments": null, "journal-ref": "JOSA A, Vol. 29, Issue 10, pp. 2134-2143 (2012)", "doi": "10.1364/JOSAA.29.002134", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the high precision measurement of the distortion of a\ndigital camera from photographs. Traditionally, this distortion is measured\nfrom photographs of a flat pattern which contains aligned elements.\nNevertheless, it is nearly impossible to fabricate a very flat pattern and to\nvalidate its flatness. This fact limits the attainable measurable precisions.\nIn contrast, it is much easier to obtain physically very precise straight lines\nby tightly stretching good quality strings on a frame. Taking literally\n\"plumb-line methods\", we built a \"calibration harp\" instead of the classic flat\npatterns to obtain a high precision measurement tool, demonstrably reaching\n2/100 pixel precisions. The harp is complemented with the algorithms computing\nautomatically from harp photographs two different and complementary lens\ndistortion measurements. The precision of the method is evaluated on images\ncorrected by state-of-the-art distortion correction algorithms, and by popular\nsoftware. Three applications are shown: first an objective and reliable\nmeasurement of the result of any distortion correction. Second, the harp\npermits to control state-of-the art global camera calibration algorithms: It\npermits to select the right distortion model, thus avoiding internal\ncompensation errors inherent to these methods. Third, the method replaces\nmanual procedures in other distortion correction methods, makes them fully\nautomatic, and increases their reliability and precision.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2012 05:00:01 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Tang", "Zhongwei", ""], ["von Gioi", "Rafael Grompone", ""], ["Monasse", "Pascal", ""], ["Morel", "Jean-Michel", ""]]}, {"id": "1212.5711", "submitter": "Paul Vitanyi", "authors": "Andrew R. Cohen (Electrical and Computer Engineering, Drexel\n  University, Philadelphia) and Paul M. B. Vitanyi (CWI and University of\n  Amsterdam)", "title": "Normalized Compression Distance of Multisets with Applications", "comments": "LaTeX 28 pages, 3 figures. This version is changed from the\n  preliminary version to the final version. Updates of the theory. How to\n  compute it, special recepies for classification, more applications and better\n  results (see abstract and especially the detailed results in the paper). The\n  title was changed to reflect this. In v4 corrected the proof of Theorem III-7", "journal-ref": "IEEE Trans. Pattern Analysis and Machine Intelligence, 37:8(2015),\n  1602-1614", "doi": "10.1109/TPAMI.2014.2375175", "report-no": null, "categories": "cs.CV cs.IT math.IT physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized compression distance (NCD) is a parameter-free, feature-free,\nalignment-free, similarity measure between a pair of finite objects based on\ncompression. However, it is not sufficient for all applications. We propose an\nNCD of finite multisets (a.k.a. multiples) of finite objects that is also a\nmetric. Previously, attempts to obtain such an NCD failed. We cover the entire\ntrajectory from theoretical underpinning to feasible practice. The new NCD for\nmultisets is applied to retinal progenitor cell classification questions and to\nrelated synthetically generated data that were earlier treated with the\npairwise NCD. With the new method we achieved significantly better results.\nSimilarly for questions about axonal organelle transport. We also applied the\nnew NCD to handwritten digit recognition and improved classification accuracy\nsignificantly over that of pairwise NCD by incorporating both the pairwise and\nNCD for multisets. In the analysis we use the incomputable Kolmogorov\ncomplexity that for practical purposes is approximated from above by the length\nof the compressed version of the file involved, using a real-world compression\nprogram.\n  Index Terms--- Normalized compression distance, multisets or multiples,\npattern recognition, data mining, similarity, classification, Kolmogorov\ncomplexity, retinal progenitor cells, synthetic data, organelle transport,\nhandwritten character recognition\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2012 17:37:03 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2013 16:38:19 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2013 18:06:16 GMT"}, {"version": "v4", "created": "Fri, 29 Mar 2013 17:00:20 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Cohen", "Andrew R.", "", "Electrical and Computer Engineering, Drexel\n  University, Philadelphia"], ["Vitanyi", "Paul M. B.", "", "CWI and University of\n  Amsterdam"]]}, {"id": "1212.5720", "submitter": "Yen-Yun Yu", "authors": "Yen-Yun Yu, P. Thomas Fletcher and Suyash P. Awate", "title": "Hierarchical Graphical Models for Multigroup Shape Analysis using\n  Expectation Maximization with Sampling in Kendall's Shape Space", "comments": "9 pages, 7 figures, International Conference on Machine Learning 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework for multi-group shape analysis relying\non a hierarchical graphical statistical model on shapes within a population.The\nframework represents individual shapes as point setsmodulo translation,\nrotation, and scale, following the notion in Kendall shape space.While\nindividual shapes are derived from their group shape model, each group shape\nmodel is derived from a single population shape model. The hierarchical model\nfollows the natural organization of population data and the top level in the\nhierarchy provides a common frame of reference for multigroup shape analysis,\ne.g. classification and hypothesis testing. Unlike typical shape-modeling\napproaches, the proposed model is a generative model that defines a joint\ndistribution of object-boundary data and the shape-model variables.\nFurthermore, it naturally enforces optimal correspondences during the process\nof model fitting and thereby subsumes the so-called correspondence problem. The\nproposed inference scheme employs an expectation maximization (EM) algorithm\nthat treats the individual and group shape variables as hidden random variables\nand integrates them out before estimating the parameters (population mean and\nvariance and the group variances). The underpinning of the EM algorithm is the\nsampling of pointsets, in Kendall shape space, from their posterior\ndistribution, for which we exploit a highly-efficient scheme based on\nHamiltonian Monte Carlo simulation. Experiments in this paper use the fitted\nhierarchical model to perform (1) hypothesis testing for comparison between\npairs of groups using permutation testing and (2) classification for image\nretrieval. The paper validates the proposed framework on simulated data and\ndemonstrates results on real data.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2012 20:27:22 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2013 20:33:55 GMT"}], "update_date": "2013-01-11", "authors_parsed": [["Yu", "Yen-Yun", ""], ["Fletcher", "P. Thomas", ""], ["Awate", "Suyash P.", ""]]}, {"id": "1212.5877", "submitter": "Andreas Karrenbauer", "authors": "Andreas Karrenbauer and Dominik W\\\"oll", "title": "Blinking Molecule Tracking", "comments": "12th International Symposium on Experimental Algorithms 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a method for tracking individual molecules which globally\noptimizes the likelihood of the connections between molecule positions fast and\nwith high reliability even for high spot densities and blinking molecules. Our\nmethod works with cost functions which can be freely chosen to combine costs\nfor distances between spots in space and time and which can account for the\nreliability of positioning a molecule. To this end, we describe a top-down\npolyhedral approach to the problem of tracking many individual molecules. This\nimmediately yields an effective implementation using standard linear\nprogramming solvers. Our method can be applied to 2D and 3D tracking.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2012 08:58:02 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2013 02:49:46 GMT"}], "update_date": "2013-04-01", "authors_parsed": [["Karrenbauer", "Andreas", ""], ["W\u00f6ll", "Dominik", ""]]}, {"id": "1212.6058", "submitter": "Jian Zhang", "authors": "Xinwei Gao, Jian Zhang, Feng Jiang, Xiaopeng Fan, Siwei Ma, Debin Zhao", "title": "High Quality Image Interpolation via Local Autoregressive and Nonlocal\n  3-D Sparse Regularization", "comments": "4 pages, 5 figures, 2 tables, to be published at IEEE Visual\n  Communications and Image Processing (VCIP) 2012", "journal-ref": null, "doi": "10.1109/VCIP.2012.6410749", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel image interpolation algorithm, which is\nformulated via combining both the local autoregressive (AR) model and the\nnonlocal adaptive 3-D sparse model as regularized constraints under the\nregularization framework. Estimating the high-resolution image by the local AR\nregularization is different from these conventional AR models, which weighted\ncalculates the interpolation coefficients without considering the rough\nstructural similarity between the low-resolution (LR) and high-resolution (HR)\nimages. Then the nonlocal adaptive 3-D sparse model is formulated to regularize\nthe interpolated HR image, which provides a way to modify these pixels with the\nproblem of numerical stability caused by AR model. In addition, a new\nSplit-Bregman based iterative algorithm is developed to solve the above\noptimization problem iteratively. Experiment results demonstrate that the\nproposed algorithm achieves significant performance improvements over the\ntraditional algorithms in terms of both objective quality and visual perception\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2012 14:28:52 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Gao", "Xinwei", ""], ["Zhang", "Jian", ""], ["Jiang", "Feng", ""], ["Fan", "Xiaopeng", ""], ["Ma", "Siwei", ""], ["Zhao", "Debin", ""]]}, {"id": "1212.6094", "submitter": "Shenghuo Zhu", "authors": "Chang Huang, Shenghuo Zhu, Kai Yu", "title": "Large Scale Strongly Supervised Ensemble Metric Learning, with\n  Applications to Face Verification and Retrieval", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": "NEC Labs, 2011-TR115", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning Mahanalobis distance metrics in a high- dimensional feature space is\nvery difficult especially when structural sparsity and low rank are enforced to\nimprove com- putational efficiency in testing phase. This paper addresses both\naspects by an ensemble metric learning approach that consists of sparse block\ndiagonal metric ensembling and join- t metric learning as two consecutive\nsteps. The former step pursues a highly sparse block diagonal metric by\nselecting effective feature groups while the latter one further exploits\ncorrelations between selected feature groups to obtain an accurate and low rank\nmetric. Our algorithm considers all pairwise or triplet constraints generated\nfrom training samples with explicit class labels, and possesses good scala-\nbility with respect to increasing feature dimensionality and growing data\nvolumes. Its applications to face verification and retrieval outperform\nexisting state-of-the-art methods in accuracy while retaining high efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2012 22:49:31 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Huang", "Chang", ""], ["Zhu", "Shenghuo", ""], ["Yu", "Kai", ""]]}, {"id": "1212.6209", "submitter": "Joshua Shaevitz", "authors": "Yi Deng, Philip Coen, Mingzhai Sun, Joshua W. Shaevitz", "title": "Efficient Multiple Object Tracking Using Mutually Repulsive Active\n  Membranes", "comments": "18 pages, 6 figures, 1 table", "journal-ref": null, "doi": "10.1371/journal.pone.0065769", "report-no": null, "categories": "q-bio.QM cs.CV physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies of social and group behavior in interacting organisms require\nhigh-throughput analysis of the motion of a large number of individual\nsubjects. Computer vision techniques offer solutions to specific tracking\nproblems, and allow automated and efficient tracking with minimal human\nintervention. In this work, we adopt the open active contour model to track the\ntrajectories of moving objects at high density. We add repulsive interactions\nbetween open contours to the original model, treat the trajectories as an\nextrusion in the temporal dimension, and show applications to two tracking\nproblems. The walking behavior of Drosophila is studied at different population\ndensity and gender composition. We demonstrate that individual male flies have\ndistinct walking signatures, and that the social interaction between flies in a\nmixed gender arena is gender specific. We also apply our model to studies of\ntrajectories of gliding Myxococcus xanthus bacteria at high density. We examine\nthe individual gliding behavioral statistics in terms of the gliding speed\ndistribution. Using these two examples at very distinctive spatial scales, we\nillustrate the use of our algorithm on tracking both short rigid bodies\n(Drosophila) and long flexible objects (Myxococcus xanthus). Our repulsive\nactive membrane model reaches error rates better than $5\\times 10^{-6}$ per fly\nper second for Drosophila tracking and comparable results for Myxococcus\nxanthus.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2012 16:30:40 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Deng", "Yi", ""], ["Coen", "Philip", ""], ["Sun", "Mingzhai", ""], ["Shaevitz", "Joshua W.", ""]]}, {"id": "1212.6303", "submitter": "Sangeet  Saha", "authors": "Sangeet Saha, Chandrajit pal, Rourab paul, Satyabrata Maity, Suman Sau", "title": "A brief experience on journey through hardware developments for image\n  processing and its applications on Cryptography", "comments": "In the proceedings of 100th Indian Science Congress,03-07\n  January,Kolkata", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of embedded applications on image and video\nprocessing,communication and cryptography domain has been taking a larger space\nin current research era. Improvement of pictorial information for betterment of\nhuman perception like deblurring, de-noising in several fields such as\nsatellite imaging, medical imaging etc are renewed research thrust.\nSpecifically we would like to elaborate our experience on the significance of\ncomputer vision as one of the domains where hardware implemented algorithms\nperform far better than those implemented through software. So far embedded\ndesign engineers have successfully implemented their designs by means of\nApplication Specific Integrated Circuits (ASICs) and/or Digital Signal\nProcessors (DSP), however with the advancement of VLSI technology a very\npowerful hardware device namely the Field Programmable Gate Array (FPGA)\ncombining the key advantages of ASICs and DSPs was developed which have the\npossibility of reprogramming making them a very attractive device for rapid\nprototyping.Communication of image and video data in multiple FPGA is no longer\nfar away from the thrust of secured transmission among them, and then the\nrelevance of cryptography is indeed unavoidable. This paper shows how the\nXilinx hardware development platform as well Mathworks Matlab can be used to\ndevelop hardware based computer vision algorithms and its corresponding crypto\ntransmission channel between multiple FPGA platform from a system level\napproach, making it favourable for developing a hardware-software co-design\nenvironment.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2012 05:16:25 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Saha", "Sangeet", ""], ["pal", "Chandrajit", ""], ["paul", "Rourab", ""], ["Maity", "Satyabrata", ""], ["Sau", "Suman", ""]]}, {"id": "1212.6837", "submitter": "Hai Nguyen", "authors": "Hai Nguyen and Charles C. Kemp", "title": "Autonomously Learning to Visually Detect Where Manipulation Will Succeed", "comments": "15 pages, 10 figures. Submitted to the Autonomous Robots Journal\n  Special Issue \"Beyond Grasping - Modern Approaches for Dexterous\n  Manipulation\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual features can help predict if a manipulation behavior will succeed at a\ngiven location. For example, the success of a behavior that flips light\nswitches depends on the location of the switch. Within this paper, we present\nmethods that enable a mobile manipulator to autonomously learn a function that\ntakes an RGB image and a registered 3D point cloud as input and returns a 3D\nlocation at which a manipulation behavior is likely to succeed. Given a pair of\nmanipulation behaviors that can change the state of the world between two sets\n(e.g., light switch up and light switch down), classifiers that detect when\neach behavior has been successful, and an initial hint as to where one of the\nbehaviors will be successful, the robot autonomously trains a pair of support\nvector machine (SVM) classifiers by trying out the behaviors at locations in\nthe world and observing the results. When an image feature vector associated\nwith a 3D location is provided as input to one of the SVMs, the SVM predicts if\nthe associated manipulation behavior will be successful at the 3D location. To\nevaluate our approach, we performed experiments with a PR2 robot from Willow\nGarage in a simulated home using behaviors that flip a light switch, push a\nrocker-type light switch, and operate a drawer. By using active learning, the\nrobot efficiently learned SVMs that enabled it to consistently succeed at these\ntasks. After training, the robot also continued to learn in order to adapt in\nthe event of failure.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2012 08:39:14 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Nguyen", "Hai", ""], ["Kemp", "Charles C.", ""]]}, {"id": "1212.6933", "submitter": "Habib Moukalled", "authors": "H. J. Moukalled", "title": "On Automation and Medical Image Interpretation, With Applications for\n  Laryngeal Imaging", "comments": "18 pages, 9 figures, 41 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indeed, these are exciting times. We are in the heart of a digital\nrenaissance. Automation and computer technology allow engineers and scientists\nto fabricate processes that amalgamate quality of life. We anticipate much\ngrowth in medical image interpretation and understanding, due to the influx of\ncomputer technologies. This work should serve as a guide to introduce the\nreader to core themes in theoretical computer science, as well as imaging\napplications for understanding vocal-fold vibrations. In this work, we motivate\nthe use of automation, review some mathematical models of computation. We\npresent a proof of a classical problem in image analysis that cannot be\nautomated by means of algorithms. Furthermore, discuss some applications for\nprocessing medical images of the vocal folds, and discuss some of the\nexhilarating directions the art of automation will take vocal-fold image\ninterpretation and quite possibly other areas of biomedical image analysis.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2012 17:38:02 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2013 19:27:01 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2013 23:25:20 GMT"}], "update_date": "2013-01-16", "authors_parsed": [["Moukalled", "H. J.", ""]]}]