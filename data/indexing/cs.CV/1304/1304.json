[{"id": "1304.0019", "submitter": "Tizita Nesibu Shewaye Mrs", "authors": "Tizita Nesibu Shewaye", "title": "Age group and gender recognition from human facial images", "comments": "8 pages, October, 2012", "journal-ref": "Ethiopian Society of Electrical Engineers 6th Scientific\n  Conference on Electrical Engineering (CEE-2012)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an automatic human gender and age group recognition system\nbased on human facial images. It makes an extensive experiment with row pixel\nintensity valued features and Discrete Cosine Transform (DCT) coefficient\nfeatures with Principal Component Analysis and k-Nearest Neighbor\nclassification to identify the best recognition approach. The final results\nshow approaches using DCT coefficient outperform their counter parts resulting\nin a 99% correct gender recognition rate and 68% correct age group recognition\nrate (considering four distinct age groups) in unseen test images. Detailed\nexperimental settings and obtained results are clearly presented and explained\nin this report.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 20:32:04 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Shewaye", "Tizita Nesibu", ""]]}, {"id": "1304.0023", "submitter": "Peter Loxley", "authors": "Peter Loxley", "title": "The two-dimensional Gabor function adapted to natural image statistics:\n  A model of simple-cell receptive fields and sparse structure in images", "comments": null, "journal-ref": "Neural Computation 29, 2769-2799 (2017)", "doi": "10.1162/NECO_a_00997", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-dimensional Gabor function is adapted to natural image statistics,\nleading to a tractable probabilistic generative model that can be used to model\nsimple-cell receptive-field profiles, or generate basis functions for sparse\ncoding applications. Learning is found to be most pronounced in three\nGabor-function parameters representing the size and spatial frequency of the\ntwo-dimensional Gabor function, and characterized by a non-uniform probability\ndistribution with heavy tails. All three parameters are found to be strongly\ncorrelated: resulting in a basis of multiscale Gabor functions with similar\naspect ratios, and size-dependent spatial frequencies. A key finding is that\nthe distribution of receptive-field sizes is scale-invariant over a wide range\nof values, so there is no characteristic receptive-field size selected by\nnatural image statistics. The Gabor-function aspect ratio is found to be\napproximately conserved by the learning rules and is therefore not\nwell-determined by natural image statistics. This allows for three distinct\nsolutions: a basis of Gabor functions with sharp orientation resolution at the\nexpense of spatial-frequency resolution; a basis of Gabor functions with sharp\nspatial-frequency resolution at the expense of orientation resolution; or a\nbasis with unit aspect ratio. Arbitrary mixtures of all three cases are also\npossible. Two parameters controlling the shape of the marginal distributions in\na probabilistic generative model fully account for all three solutions. The\nbest-performing probabilistic generative model for sparse coding applications\nis found to be a Gaussian copula with Pareto marginal probability density\nfunctions.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 20:39:53 GMT"}, {"version": "v2", "created": "Sun, 20 Jul 2014 02:40:52 GMT"}, {"version": "v3", "created": "Tue, 23 Sep 2014 05:29:25 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2020 00:57:00 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Loxley", "Peter", ""]]}, {"id": "1304.0035", "submitter": "Ivan Selesnick", "authors": "Po-Yu Chen and Ivan W. Selesnick", "title": "Translation-Invariant Shrinkage/Thresholding of Group Sparse Signals", "comments": "33 pages, 7 figures, 5 tables", "journal-ref": null, "doi": "10.1016/j.sigpro.2013.06", "report-no": null, "categories": "cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses signal denoising when large-amplitude coefficients form\nclusters (groups). The L1-norm and other separable sparsity models do not\ncapture the tendency of coefficients to cluster (group sparsity). This work\ndevelops an algorithm, called 'overlapping group shrinkage' (OGS), based on the\nminimization of a convex cost function involving a group-sparsity promoting\npenalty function. The groups are fully overlapping so the denoising method is\ntranslation-invariant and blocking artifacts are avoided. Based on the\nprinciple of majorization-minimization (MM), we derive a simple iterative\nminimization algorithm that reduces the cost function monotonically. A\nprocedure for setting the regularization parameter, based on attenuating the\nnoise to a specified level, is also described. The proposed approach is\nillustrated on speech enhancement, wherein the OGS approach is applied in the\nshort-time Fourier transform (STFT) domain. The denoised speech produced by OGS\ndoes not suffer from musical noise.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 22:00:01 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Chen", "Po-Yu", ""], ["Selesnick", "Ivan W.", ""]]}, {"id": "1304.0243", "submitter": "Marc Alexander A{\\ss}mann", "authors": "Marc A{\\ss}mann and Manfred Bayer", "title": "Compressive adaptive computational ghost imaging", "comments": null, "journal-ref": "Scientific Reports 3, 1545 (2013)", "doi": "10.1038/srep01545", "report-no": null, "categories": "physics.optics cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing is considered a huge breakthrough in signal acquisition.\nIt allows recording an image consisting of $N^2$ pixels using much fewer than\n$N^2$ measurements if it can be transformed to a basis where most pixels take\non negligibly small values. Standard compressive sensing techniques suffer from\nthe computational overhead needed to reconstruct an image with typical\ncomputation times between hours and days and are thus not optimal for\napplications in physics and spectroscopy. We demonstrate an adaptive\ncompressive sampling technique that performs measurements directly in a sparse\nbasis. It needs much fewer than $N^2$ measurements without any computational\noverhead, so the result is available instantly.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2013 18:57:52 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["A\u00dfmann", "Marc", ""], ["Bayer", "Manfred", ""]]}, {"id": "1304.0421", "submitter": "Santosh K.C.", "authors": "K.C. Santosh (LORIA), E. Iwata", "title": "Stroke-Based Cursive Character Recognition", "comments": null, "journal-ref": "Advances in Character Recognition INTECH (Ed.) (2012) 175-192", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human eye can see and read what is written or displayed either in natural\nhandwriting or in printed format. The same work in case the machine does is\ncalled handwriting recognition. Handwriting recognition can be broken down into\ntwo categories: off-line and on-line. ...\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2013 19:14:27 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Santosh", "K. C.", "", "LORIA"], ["Iwata", "E.", ""]]}, {"id": "1304.0725", "submitter": "Ashok P", "authors": "P. Ashok, G.M Kadhar Nawaz, E. Elayaraja, V. Vadivel", "title": "Improved Performance of Unsupervised Method by Renovated K-Means", "comments": "7 pages, to strengthen the k means algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a separation of data into groups of similar objects. Every\ngroup called cluster consists of objects that are similar to one another and\ndissimilar to objects of other groups. In this paper, the K-Means algorithm is\nimplemented by three distance functions and to identify the optimal distance\nfunction for clustering methods. The proposed K-Means algorithm is compared\nwith K-Means, Static Weighted K-Means (SWK-Means) and Dynamic Weighted K-Means\n(DWK-Means) algorithm by using Davis Bouldin index, Execution Time and\nIteration count methods. Experimental results show that the proposed K-Means\nalgorithm performed better on Iris and Wine dataset when compared with other\nthree clustering methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 05:28:06 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Ashok", "P.", ""], ["Nawaz", "G. M Kadhar", ""], ["Elayaraja", "E.", ""], ["Vadivel", "V.", ""]]}, {"id": "1304.0823", "submitter": "Liyu Gong", "authors": "Liyu Gong, Meng Chen, Chunlong Hu", "title": "Lie Algebrized Gaussians for Image Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an image representation method which is derived from analyzing\nGaussian probability density function (\\emph{pdf}) space using Lie group\ntheory. In our proposed method, images are modeled by Gaussian mixture models\n(GMMs) which are adapted from a globally trained GMM called universal\nbackground model (UBM). Then we vectorize the GMMs based on two facts: (1)\ncomponents of image-specific GMMs are closely grouped together around their\ncorresponding component of the UBM due to the characteristic of the UBM\nadaption procedure; (2) Gaussian \\emph{pdf}s form a Lie group, which is a\ndifferentiable manifold rather than a vector space. We map each Gaussian\ncomponent to the tangent vector space (named Lie algebra) of Lie group at the\nmanifold position of UBM. The final feature vector, named Lie algebrized\nGaussians (LAG) is then constructed by combining the Lie algebrized Gaussian\ncomponents with mixture weights. We apply LAG features to scene category\nrecognition problem and observe state-of-the-art performance on 15Scenes\nbenchmark.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 02:38:01 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 23:39:21 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Gong", "Liyu", ""], ["Chen", "Meng", ""], ["Hu", "Chunlong", ""]]}, {"id": "1304.0839", "submitter": "Dai-Gyoung Kim", "authors": "Zahid Hussain Shamsi and Dai-Gyoung Kim", "title": "Multiscale Hybrid Non-local Means Filtering Using Modified Similarity\n  Measure", "comments": "7 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new multiscale implementation of non-local means filtering for image\ndenoising is proposed. The proposed algorithm also introduces a modification of\nsimilarity measure for patch comparison. The standard Euclidean norm is\nreplaced by weighted Euclidean norm for patch based comparison. Assuming the\npatch as an oriented surface, notion of normal vector patch is being associated\nwith each patch. The inner product of these normal vector patches is then used\nin weighted Euclidean distance of photometric patches as the weight factor. The\nalgorithm involves two steps: The first step is multiscale implementation of an\naccelerated non-local means filtering in the stationary wavelet domain to\nobtain a refined version of the noisy patches for later comparison. This step\nis inspired by a preselection phase of finding similar patches in various\nnon-local means approaches. The next step is to apply the modified non-local\nmeans filtering to the noisy image using the reference patches obtained in the\nfirst step. These refined patches contain less noise, and consequently the\ncomputation of normal vectors and partial derivatives is more accurate.\nExperimental results indicate equivalent or better performance of proposed\nalgorithm as compared to various state of the art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 04:24:21 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Shamsi", "Zahid Hussain", ""], ["Kim", "Dai-Gyoung", ""]]}, {"id": "1304.0840", "submitter": "Chunhua Shen", "authors": "Peng Wang, Chunhua Shen, Anton van den Hengel", "title": "A Fast Semidefinite Approach to Solving Binary Quadratic Problems", "comments": "Appearing in Proc. IEEE Conf. Computer Vision and Pattern\n  Recognition, 2013", "journal-ref": null, "doi": "10.1109/CVPR.2013.173", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision problems can be formulated as binary quadratic programs\n(BQPs). Two classic relaxation methods are widely used for solving BQPs,\nnamely, spectral methods and semidefinite programming (SDP), each with their\nown advantages and disadvantages. Spectral relaxation is simple and easy to\nimplement, but its bound is loose. Semidefinite relaxation has a tighter bound,\nbut its computational complexity is high for large scale problems. We present a\nnew SDP formulation for BQPs, with two desirable properties. First, it has a\nsimilar relaxation bound to conventional SDP formulations. Second, compared\nwith conventional SDP methods, the new SDP formulation leads to a significantly\nmore efficient and scalable dual optimization approach, which has the same\ndegree of complexity as spectral methods. Extensive experiments on various\napplications including clustering, image segmentation, co-segmentation and\nregistration demonstrate the usefulness of our SDP formulation for solving\nlarge-scale BQPs.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 04:31:10 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1304.0869", "submitter": "Conrad Sanderson", "authors": "Yongkang Wong, Shaokang Chen, Sandra Mau, Conrad Sanderson, Brian C.\n  Lovell", "title": "Patch-based Probabilistic Image Quality Assessment for Face Selection\n  and Improved Video-based Face Recognition", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW), pp. 74-81, 2011", "doi": "10.1109/CVPRW.2011.5981881", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video based face recognition, face images are typically captured over\nmultiple frames in uncontrolled conditions, where head pose, illumination,\nshadowing, motion blur and focus change over the sequence. Additionally,\ninaccuracies in face localisation can also introduce scale and alignment\nvariations. Using all face images, including images of poor quality, can\nactually degrade face recognition performance. While one solution it to use\nonly the \"best\" subset of images, current face selection techniques are\nincapable of simultaneously handling all of the abovementioned issues. We\npropose an efficient patch-based face image quality assessment algorithm which\nquantifies the similarity of a face image to a probabilistic face model,\nrepresenting an \"ideal\" face. Image characteristics that affect recognition are\ntaken into account, including variations in geometric alignment (shift,\nrotation and scale), sharpness, head pose and cast shadows. Experiments on\nFERET and PIE datasets show that the proposed algorithm is able to identify\nimages which are simultaneously the most frontal, aligned, sharp and well\nilluminated. Further experiments on a new video surveillance dataset (termed\nChokePoint) show that the proposed method provides better face subsets than\nexisting face selection techniques, leading to significant improvements in\nrecognition accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 08:41:23 GMT"}, {"version": "v2", "created": "Fri, 14 Mar 2014 15:53:31 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Wong", "Yongkang", ""], ["Chen", "Shaokang", ""], ["Mau", "Sandra", ""], ["Sanderson", "Conrad", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1304.0886", "submitter": "Conrad Sanderson", "authors": "Vikas Reddy, Conrad Sanderson, Brian C. Lovell", "title": "Improved Anomaly Detection in Crowded Scenes via Cell-based Analysis of\n  Foreground Speed, Size and Texture", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW), pp. 55-61, 2011", "doi": "10.1109/CVPRW.2011.5981799", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust and efficient anomaly detection technique is proposed, capable of\ndealing with crowded scenes where traditional tracking based approaches tend to\nfail. Initial foreground segmentation of the input frames confines the analysis\nto foreground objects and effectively ignores irrelevant background dynamics.\nInput frames are split into non-overlapping cells, followed by extracting\nfeatures based on motion, size and texture from each cell. Each feature type is\nindependently analysed for the presence of an anomaly. Unlike most methods, a\nrefined estimate of object motion is achieved by computing the optical flow of\nonly the foreground pixels. The motion and size features are modelled by an\napproximated version of kernel density estimation, which is computationally\nefficient even for large training datasets. Texture features are modelled by an\nadaptively grown codebook, with the number of entries in the codebook selected\nin an online fashion. Experiments on the recently published UCSD Anomaly\nDetection dataset show that the proposed method obtains considerably better\nresults than three recent approaches: MPPCA, social force, and mixture of\ndynamic textures (MDT). The proposed method is also several orders of magnitude\nfaster than MDT, the next best performing method.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 09:31:27 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Reddy", "Vikas", ""], ["Sanderson", "Conrad", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1304.1014", "submitter": "Emanuele Frandi", "authors": "Hector Allende, Emanuele Frandi, Ricardo Nanculef, Claudio Sartori", "title": "A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale\n  SVM Training", "comments": "REVISED VERSION (October 2013) -- Title and abstract have been\n  revised. Section 5 was added. Some proofs have been summarized (full-length\n  proofs available in the previous version)", "journal-ref": "Information Sciences 285, 66-99, 2014", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a renewed interest in the machine learning community\nfor variants of a sparse greedy approximation procedure for concave\noptimization known as {the Frank-Wolfe (FW) method}. In particular, this\nprocedure has been successfully applied to train large-scale instances of\nnon-linear Support Vector Machines (SVMs). Specializing FW to SVM training has\nallowed to obtain efficient algorithms but also important theoretical results,\nincluding convergence analysis of training algorithms and new characterizations\nof model sparsity.\n  In this paper, we present and analyze a novel variant of the FW method based\non a new way to perform away steps, a classic strategy used to accelerate the\nconvergence of the basic FW procedure. Our formulation and analysis is focused\non a general concave maximization problem on the simplex. However, the\nspecialization of our algorithm to quadratic forms is strongly related to some\nclassic methods in computational geometry, namely the Gilbert and MDM\nalgorithms.\n  On the theoretical side, we demonstrate that the method matches the\nguarantees in terms of convergence rate and number of iterations obtained by\nusing classic away steps. In particular, the method enjoys a linear rate of\nconvergence, a result that has been recently proved for MDM on quadratic forms.\n  On the practical side, we provide experiments on several classification\ndatasets, and evaluate the results using statistical tests. Experiments show\nthat our method is faster than the FW method with classic away steps, and works\nwell even in the cases in which classic away steps slow down the algorithm.\nFurthermore, these improvements are obtained without sacrificing the predictive\naccuracy of the obtained SVM model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 17:15:43 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2013 09:50:26 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Allende", "Hector", ""], ["Frandi", "Emanuele", ""], ["Nanculef", "Ricardo", ""], ["Sartori", "Claudio", ""]]}, {"id": "1304.1022", "submitter": "Amelia Carolina Sparavigna", "authors": "Amelia Carolina Sparavigna", "title": "A software for aging faces applied to ancient marble busts", "comments": "Image processing. Aging faces. Freely available software. Ancient\n  marble busts. Augustus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study and development of software able to show the effect of aging of\nfaces is one of the tasks of face recognition technologies. Some software\nsolutions are used for investigations, some others to show the effects of drugs\non healthy appearance, however some other applications can be proposed for the\nanalysis of visual arts. Here we use a freely available software, which is\nproviding interesting results, for the comparison of ancient marble busts. An\nanalysis of Augustus busts is proposed.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 17:34:20 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Sparavigna", "Amelia Carolina", ""]]}, {"id": "1304.1209", "submitter": "Ben Fulcher", "authors": "Ben D. Fulcher, Max A. Little, Nick S. Jones", "title": "Highly comparative time-series analysis: The empirical structure of time\n  series and their methods", "comments": null, "journal-ref": "J. R. Soc. Interface vol. 10 no. 83 20130048 (2013)", "doi": "10.1098/rsif.2013.0048", "report-no": null, "categories": "physics.data-an cs.CV physics.bio-ph q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of collecting and organizing sets of observations represents a\ncommon theme throughout the history of science. However, despite the ubiquity\nof scientists measuring, recording, and analyzing the dynamics of different\nprocesses, an extensive organization of scientific time-series data and\nanalysis methods has never been performed. Addressing this, annotated\ncollections of over 35 000 real-world and model-generated time series and over\n9000 time-series analysis algorithms are analyzed in this work. We introduce\nreduced representations of both time series, in terms of their properties\nmeasured by diverse scientific methods, and of time-series analysis methods, in\nterms of their behaviour on empirical time series, and use them to organize\nthese interdisciplinary resources. This new approach to comparing across\ndiverse scientific data and methods allows us to organize time-series datasets\nautomatically according to their properties, retrieve alternatives to\nparticular analysis methods developed in other scientific disciplines, and\nautomate the selection of useful methods for time-series classification and\nregression tasks. The broad scientific utility of these tools is demonstrated\non datasets of electroencephalograms, self-affine time series, heart beat\nintervals, speech signals, and others, in each case contributing novel analysis\ntechniques to the existing literature. Highly comparative techniques that\ncompare across an interdisciplinary literature can thus be used to guide more\nfocused research in time-series analysis for applications across the scientific\ndisciplines.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 23:24:02 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Fulcher", "Ben D.", ""], ["Little", "Max A.", ""], ["Jones", "Nick S.", ""]]}, {"id": "1304.1233", "submitter": "Conrad Sanderson", "authors": "Andres Sanin, Conrad Sanderson, Brian C. Lovell", "title": "Shadow Detection: A Survey and Comparative Evaluation of Recent Methods", "comments": null, "journal-ref": "Pattern Recognition, Vol. 45, No. 4, pp. 1684-1695, 2012", "doi": "10.1016/j.patcog.2011.10.001", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a survey and a comparative evaluation of recent\ntechniques for moving cast shadow detection. We identify shadow removal as a\ncritical step for improving object detection and tracking. The survey covers\nmethods published during the last decade, and places them in a feature-based\ntaxonomy comprised of four categories: chromacity, physical, geometry and\ntextures. A selection of prominent methods across the categories is compared in\nterms of quantitative performance measures (shadow detection and discrimination\nrates, colour desaturation) as well as qualitative observations. Furthermore,\nwe propose the use of tracking performance as an unbiased approach for\ndetermining the practical usefulness of shadow detection methods. The\nevaluation indicates that all shadow detection approaches make different\ncontributions and all have individual strength and weaknesses. Out of the\nselected methods, the geometry-based technique has strict assumptions and is\nnot generalisable to various environments, but it is a straightforward choice\nwhen the objects of interest are easy to model and their shadows have different\norientation. The chromacity based method is the fastest to implement and run,\nbut it is sensitive to noise and less effective in low saturated scenes. The\nphysical method improves upon the accuracy of the chromacity method by adapting\nto local shadow models, but fails when the spectral properties of the objects\nare similar to that of the background. The small-region texture based method is\nespecially robust for pixels whose neighbourhood is textured, but may take\nlonger to implement and is the most computationally expensive. The large-region\ntexture based method produces the most accurate results, but has a significant\ncomputational load due to its multiple processing steps.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 03:11:38 GMT"}], "update_date": "2013-04-05", "authors_parsed": [["Sanin", "Andres", ""], ["Sanderson", "Conrad", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1304.1250", "submitter": "Chunhua Shen", "authors": "Fumin Shen, Chunhua Shen, Rhys Hill, Anton van den Hengel, Zhenmin\n  Tang", "title": "Fast Approximate L_infty Minimization: Speeding Up Robust Regression", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimization of the $L_\\infty$ norm, which can be viewed as approximately\nsolving the non-convex least median estimation problem, is a powerful method\nfor outlier removal and hence robust regression. However, current techniques\nfor solving the problem at the heart of $L_\\infty$ norm minimization are slow,\nand therefore cannot scale to large problems. A new method for the minimization\nof the $L_\\infty$ norm is presented here, which provides a speedup of multiple\norders of magnitude for data with high dimension. This method, termed Fast\n$L_\\infty$ Minimization, allows robust regression to be applied to a class of\nproblems which were previously inaccessible. It is shown how the $L_\\infty$\nnorm minimization problem can be broken up into smaller sub-problems, which can\nthen be solved extremely efficiently. Experimental results demonstrate the\nradical reduction in computation time, along with robustness against large\nnumbers of outliers in a few model-fitting problems.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 05:57:56 GMT"}], "update_date": "2013-04-05", "authors_parsed": [["Shen", "Fumin", ""], ["Shen", "Chunhua", ""], ["Hill", "Rhys", ""], ["Hengel", "Anton van den", ""], ["Tang", "Zhenmin", ""]]}, {"id": "1304.1262", "submitter": "Conrad Sanderson", "authors": "Arnold Wiliem, Yongkang Wong, Conrad Sanderson, Peter Hobson, Shaokang\n  Chen, Brian C. Lovell", "title": "Classification of Human Epithelial Type 2 Cell Indirect\n  Immunofluoresence Images via Codebook Based Descriptors", "comments": null, "journal-ref": "IEEE Workshop on Applications of Computer Vision (WACV), pp.\n  95-102, 2013", "doi": "10.1109/WACV.2013.6475005", "report-no": null, "categories": "q-bio.CB cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Anti-Nuclear Antibody (ANA) clinical pathology test is commonly used to\nidentify the existence of various diseases. A hallmark method for identifying\nthe presence of ANAs is the Indirect Immunofluorescence method on Human\nEpithelial (HEp-2) cells, due to its high sensitivity and the large range of\nantigens that can be detected. However, the method suffers from numerous\nshortcomings, such as being subjective as well as time and labour intensive.\nComputer Aided Diagnostic (CAD) systems have been developed to address these\nproblems, which automatically classify a HEp-2 cell image into one of its known\npatterns (eg., speckled, homogeneous). Most of the existing CAD systems use\nhandpicked features to represent a HEp-2 cell image, which may only work in\nlimited scenarios. In this paper, we propose a cell classification system\ncomprised of a dual-region codebook-based descriptor, combined with the Nearest\nConvex Hull Classifier. We evaluate the performance of several variants of the\ndescriptor on two publicly available datasets: ICPR HEp-2 cell classification\ncontest dataset and the new SNPHEp-2 dataset. To our knowledge, this is the\nfirst time codebook-based descriptors are applied and studied in this domain.\nExperiments show that the proposed system has consistent high performance and\nis more robust than two recent CAD systems.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 07:51:32 GMT"}], "update_date": "2013-04-05", "authors_parsed": [["Wiliem", "Arnold", ""], ["Wong", "Yongkang", ""], ["Sanderson", "Conrad", ""], ["Hobson", "Peter", ""], ["Chen", "Shaokang", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1304.1408", "submitter": "Ming Yan", "authors": "Ming Yan", "title": "Restoration of Images Corrupted by Impulse Noise and Mixed Gaussian\n  Impulse Noise using Blind Inpainting", "comments": "18 pages, 4 figures", "journal-ref": "SIAM J. Imaging Sci., 6(2013), 1227-1245", "doi": "10.1137/12087178X", "report-no": null, "categories": "math.OC cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the problem of image restoration of observed images\ncorrupted by impulse noise and mixed Gaussian impulse noise. Since the pixels\ndamaged by impulse noise contain no information about the true image, how to\nfind this set correctly is a very important problem. We propose two methods\nbased on blind inpainting and $\\ell_0$ minimization that can simultaneously\nfind the damaged pixels and restore the image. By iteratively restoring the\nimage and updating the set of damaged pixels, these methods have better\nperformance than other methods, as shown in the experiments. In addition, we\nprovide convergence analysis for these methods, these algorithms will converge\nto coordinatewise minimum points. In addition, they will converge to local\nminimum points (or with probability one) with some modifications in the\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 15:48:09 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Yan", "Ming", ""]]}, {"id": "1304.1419", "submitter": "Ali Avanaki", "authors": "Ali N. Avanaki, Kathryn S. Espig, Cedric Marchessoux, Elizabeth A.\n  Krupinski, Predrag R. Bakic, Tom R. L. Kimpe, Andrew D. A. Maidment", "title": "Integration of spatio-temporal contrast sensitivity with a multi-slice\n  channelized Hotelling observer", "comments": null, "journal-ref": null, "doi": "10.1117/12.2001334", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Barten's model of spatio-temporal contrast sensitivity function of human\nvisual system is embedded in a multi-slice channelized Hotelling observer. This\nis done by 3D filtering of the stack of images with the spatio-temporal\ncontrast sensitivity function and feeding the result (i.e., the perceived image\nstack) to the multi-slice channelized Hotelling observer. The proposed\nprocedure of considering spatio-temporal contrast sensitivity function is\ngeneric in the sense that it can be used with observers other than multi-slice\nchannelized Hotelling observer. Detection performance of the new observer in\ndigital breast tomosynthesis is measured in a variety of browsing speeds, at\ntwo spatial sampling rates, using computer simulations. Our results show a peak\nin detection performance in mid browsing speeds. We compare our results to\nthose of a human observer study reported earlier (I. Diaz et al. SPIE MI 2011).\nThe effects of display luminance, contrast and spatial sampling rate, with and\nwithout considering foveal vision, are also studied. Reported simulations are\nconducted with real digital breast tomosynthesis image stacks, as well as\nstacks from an anthropomorphic software breast phantom (P. Bakic et al. Med\nPhys. 2011). Lesion cases are simulated by inserting single\nmicro-calcifications or masses. Limitations of our methods and ways to improve\nthem are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 16:24:16 GMT"}], "update_date": "2013-04-05", "authors_parsed": [["Avanaki", "Ali N.", ""], ["Espig", "Kathryn S.", ""], ["Marchessoux", "Cedric", ""], ["Krupinski", "Elizabeth A.", ""], ["Bakic", "Predrag R.", ""], ["Kimpe", "Tom R. L.", ""], ["Maidment", "Andrew D. A.", ""]]}, {"id": "1304.1517", "submitter": "Tod S. Levitt", "authors": "Tod S. Levitt, John Mark Agosta, Thomas O. Binford", "title": "Model-based Influence Diagrams for Machine Vision", "comments": "Appears in Proceedings of the Fifth Conference on Uncertainty in\n  Artificial Intelligence (UAI1989)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1989-PG-233-244", "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an approach to automated control of machine vision systems based on\nincremental creation and evaluation of a particular family of influence\ndiagrams that represent hypotheses of imagery interpretation and possible\nsubsequent processing decisions. In our approach, model-based machine vision\ntechniques are integrated with hierarchical Bayesian inference to provide a\nframework for representing and matching instances of objects and relationships\nin imagery and for accruing probabilities to rank order conflicting scene\ninterpretations. We extend a result of Tatman and Shachter to show that the\nsequence of processing decisions derived from evaluating the diagrams at each\nstage is the same as the sequence that would have been derived by evaluating\nthe final influence diagram that contains all random variables created during\nthe run of the vision system.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 19:39:23 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Levitt", "Tod S.", ""], ["Agosta", "John Mark", ""], ["Binford", "Thomas O.", ""]]}, {"id": "1304.1568", "submitter": "Odemir Bruno PhD", "authors": "Jo\\~ao Batista Florindo and Odemir Martinez Bruno", "title": "Multiscale Fractal Descriptors Applied to Texture Classification", "comments": "5 pages, 4 figures", "journal-ref": "Journal of Physics: Conference Series, 410, 012022, 2013", "doi": "10.1088/1742-6596/410/1/012022", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes the combination of multiscale transform with fractal\ndescriptors employed in the classification of gray-level texture images. We\napply the space-scale transform (derivative + Gaussian filter) over the\nBouligand-Minkowski fractal descriptors, followed by a threshold over the\nfilter response, aiming at attenuating noise effects caused by the final part\nof this response. The method is tested in the classification of a well-known\ndata set (Brodatz) and compared with other classical texture descriptor\ntechniques. The results demonstrate the advantage of the proposed approach,\nachieving a higher success rate with a reduced amount of descriptors.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 22:07:27 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Florindo", "Jo\u00e3o Batista", ""], ["Bruno", "Odemir Martinez", ""]]}, {"id": "1304.1571", "submitter": "Firas Ajil Jassim", "authors": "Firas A. Jassim", "title": "Hiding Image in Image by Five Modulus Method for Image Steganography", "comments": "5 pages, 5 tables, 5 figures", "journal-ref": "Journal of computing, volume 5, issue 2, 2013", "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper is to create a practical steganographic implementation to hide\ncolor image (stego) inside another color image (cover). The proposed technique\nuses Five Modulus Method to convert the whole pixels within both the cover and\nthe stego images into multiples of five. Since each pixels inside the stego\nimage is divisible by five then the whole stego image could be divided by five\nto get new range of pixels 0..51. Basically, the reminder of each number that\nis not divisible by five is either 1,2,3 or 4 when divided by 5. Subsequently,\nthen a 4-by-4 window size has been implemented to accommodate the proposed\ntechnique. For each 4-by-4 window inside the cover image, a number from 1 to 4\ncould be embedded secretly from the stego image. The previous discussion must\nbe applied separately for each of the R, G, and B arrays. Moreover, a stego-key\ncould be combined with the proposed algorithm to make it difficult for any\nadversary to extract the secret image from the cover image. Based on the PSNR\nvalue, the extracted stego image has high PSNR value. Hence this new\nsteganography algorithm is very efficient to hide color images.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 22:17:47 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Jassim", "Firas A.", ""]]}, {"id": "1304.1572", "submitter": "Nan Hu", "authors": "Nan Hu and Raif M. Rustamov and Leonidas Guibas", "title": "Stable and Informative Spectral Signatures for Graph Matching", "comments": "final version for CVPR2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the approximate weighted graph matching problem\nand introduce stable and informative first and second order compatibility terms\nsuitable for inclusion into the popular integer quadratic program formulation.\nOur approach relies on a rigorous analysis of stability of spectral signatures\nbased on the graph Laplacian. In the case of the first order term, we derive an\nobjective function that measures both the stability and informativeness of a\ngiven spectral signature. By optimizing this objective, we design new spectral\nnode signatures tuned to a specific graph to be matched. We also introduce the\npairwise heat kernel distance as a stable second order compatibility term; we\njustify its plausibility by showing that in a certain limiting case it\nconverges to the classical adjacency matrix-based second order compatibility\nfunction. We have tested our approach on a set of synthetic graphs, the\nwidely-used CMU house sequence, and a set of real images. These experiments\nshow the superior performance of our first and second order compatibility terms\nas compared with the commonly used ones.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 22:19:49 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2013 06:45:09 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2013 02:42:04 GMT"}, {"version": "v4", "created": "Tue, 4 Jun 2013 00:57:16 GMT"}, {"version": "v5", "created": "Tue, 10 Apr 2018 20:42:19 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Hu", "Nan", ""], ["Rustamov", "Raif M.", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1304.1876", "submitter": "Justus Piater", "authors": "Justus Piater and Antonio Rodr\\'iguez-S\\'anchez", "title": "Proceedings of the 37th Annual Workshop of the Austrian Association for\n  Pattern Recognition (\\\"OAGM/AAPR), 2013", "comments": "Contributed papers presented at \\\"OAGM/AAPR 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume represents the proceedings of the 37th Annual Workshop of the\nAustrian Association for Pattern Recognition (\\\"OAGM/AAPR), held May 23-24,\n2013, in Innsbruck, Austria.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 10:36:25 GMT"}, {"version": "v2", "created": "Tue, 21 May 2013 19:23:43 GMT"}, {"version": "v3", "created": "Tue, 28 May 2013 05:24:25 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Piater", "Justus", ""], ["Rodr\u00edguez-S\u00e1nchez", "Antonio", ""]]}, {"id": "1304.1930", "submitter": "Santosh K.C.", "authors": "K.C. Santosh (LORIA), Abdel Bela\\\"id (LORIA)", "title": "Client-Driven Content Extraction Associated with Table", "comments": null, "journal-ref": "Machine Vision Applications (2013)", "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the project is to extract content within table in document images\nbased on learnt patterns. Real-world users i.e., clients first provide a set of\nkey fields within the table which they think are important. These are first\nused to represent the graph where nodes are labelled with semantics including\nother features and edges are attributed with relations. Attributed relational\ngraph (ARG) is then employed to mine similar graphs from a document image. Each\nmined graph will represent an item within the table, and hence a set of such\ngraphs will compose a table. We have validated the concept by using a\nreal-world industrial problem.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 19:24:40 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Santosh", "K. C.", "", "LORIA"], ["Bela\u00efd", "Abdel", "", "LORIA"]]}, {"id": "1304.1972", "submitter": "Amelia Carolina Sparavigna", "authors": "Amelia Carolina Sparavigna", "title": "Facial transformations of ancient portraits: the face of Caesar", "comments": "Image processing, Facial transformation, Morphing, Portraits, Julius\n  Caesar, Arles bust, Tusculum bust", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some software solutions used to obtain the facial transformations can help\ninvestigating the artistic metamorphosis of the ancient portraits of the same\nperson. An analysis with a freely available software of portraitures of Julius\nCaesar is proposed, showing his several \"morphs\". The software helps enhancing\nthe mood the artist added to a portrait.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2013 09:43:47 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Sparavigna", "Amelia Carolina", ""]]}, {"id": "1304.1995", "submitter": "Liang Liu", "authors": "Liu Liang", "title": "Image Retrieval using Histogram Factorization and Contextual Similarity\n  Learning", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval has been a top topic in the field of both computer vision and\nmachine learning for a long time. Content based image retrieval, which tries to\nretrieve images from a database visually similar to a query image, has\nattracted much attention. Two most important issues of image retrieval are the\nrepresentation and ranking of the images. Recently, bag-of-words based method\nhas shown its power as a representation method. Moreover, nonnegative matrix\nfactorization is also a popular way to represent the data samples. In addition,\ncontextual similarity learning has also been studied and proven to be an\neffective method for the ranking problem. However, these technologies have\nnever been used together. In this paper, we developed an effective image\nretrieval system by representing each image using the bag-of-words method as\nhistograms, and then apply the nonnegative matrix factorization to factorize\nthe histograms, and finally learn the ranking score using the contextual\nsimilarity learning method. The proposed novel system is evaluated on a large\nscale image database and the effectiveness is shown.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2013 13:15:17 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2013 17:59:33 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Liang", "Liu", ""]]}, {"id": "1304.2014", "submitter": "Chol-Hui Yun", "authors": "Chol-Hui Yun, W. Metzler and M. Barski", "title": "Image Compression predicated on Recurrent Iterated Function Systems", "comments": "11 pages, presented at 2nd International Conference on Mathematics &\n  Statistics, 16-19 June, 2008, Athens, Greece", "journal-ref": null, "doi": null, "report-no": "KISU-MATH-2008-E-C-001", "categories": "math.DS cs.CV math.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent iterated function systems (RIFSs) are improvements of iterated\nfunction systems (IFSs) using elements of the theory of Marcovian stochastic\nprocesses which can produce more natural looking images. We construct new RIFSs\nconsisting substantially of a vertical contraction factor function and\nnonlinear transformations. These RIFSs are applied to image compression.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2013 15:59:20 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Yun", "Chol-Hui", ""], ["Metzler", "W.", ""], ["Barski", "M.", ""]]}, {"id": "1304.2109", "submitter": "M.M.A. Hashem", "authors": "S.M. Mohsen, S.M. Zamshed Farhan and M.M.A. Hashem", "title": "Automatic Fingerprint Recognition Using Minutiae Matching Technique for\n  the Large Fingerprint Database", "comments": null, "journal-ref": "Procs. of the 3rd International Conference on Electrical and\n  Computer Engineering (ICECE 2004), pp. 116-119, Dhaka, Bangladesh, December\n  28-30, (2004)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting minutiae from fingerprint images is one of the most important\nsteps in automatic fingerprint identification system. Because minutiae matching\nare certainly the most well-known and widely used method for fingerprint\nmatching, minutiae are local discontinuities in the fingerprint pattern. In\nthis paper a fingerprint matching algorithm is proposed using some specific\nfeature of the minutiae points, also the acquired fingerprint image is\nconsidered by minimizing its size by generating a corresponding fingerprint\ntemplate for a large fingerprint database. The results achieved are compared\nwith those obtained through some other methods also shows some improvement in\nthe minutiae detection process in terms of memory and time required.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 06:14:48 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Mohsen", "S. M.", ""], ["Farhan", "S. M. Zamshed", ""], ["Hashem", "M. M. A.", ""]]}, {"id": "1304.2133", "submitter": "Conrad Sanderson", "authors": "Yongkang Wong, Conrad Sanderson, Sandra Mau, Brian C. Lovell", "title": "Dynamic Amelioration of Resolution Mismatches for Local Feature Based\n  Identity Inference", "comments": null, "journal-ref": "International Conference on Pattern Recognition (ICPR), pp.\n  1200-1203, 2010", "doi": "10.1109/ICPR.2010.299", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While existing face recognition systems based on local features are robust to\nissues such as misalignment, they can exhibit accuracy degradation when\ncomparing images of differing resolutions. This is common in surveillance\nenvironments where a gallery of high resolution mugshots is compared to low\nresolution CCTV probe images, or where the size of a given image is not a\nreliable indicator of the underlying resolution (eg. poor optics). To alleviate\nthis degradation, we propose a compensation framework which dynamically chooses\nthe most appropriate face recognition system for a given pair of image\nresolutions. This framework applies a novel resolution detection method which\ndoes not rely on the size of the input images, but instead exploits the\nsensitivity of local features to resolution using a probabilistic multi-region\nhistogram approach. Experiments on a resolution-modified version of the\n\"Labeled Faces in the Wild\" dataset show that the proposed resolution detector\nfrontend obtains a 99% average accuracy in selecting the most appropriate face\nrecognition system, resulting in higher overall face discrimination accuracy\n(across several resolutions) compared to the individual baseline face\nrecognition systems.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 08:36:55 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Wong", "Yongkang", ""], ["Sanderson", "Conrad", ""], ["Mau", "Sandra", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1304.2310", "submitter": "Nilanjan  Dey", "authors": "Nilanjan Dey, Prasenjit Maji, Poulami Das, Shouvik Biswas, Achintya\n  Das, Sheli Sinha Chaudhuri", "title": "Embedding of Blink Frequency in Electrooculography Signal using\n  Difference Expansion based Reversible Watermarking Technique", "comments": "6 Pages, 3 Figures, 4 Tables", "journal-ref": "Scientific Bulletin of the Politehnica University of Timisoara -\n  Transactions on Electronics and Communications p-ISSN 1583-3380, vol. 57(71),\n  no. 2, 2012", "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, like other fields, rapid expansion of digitization and\nglobalization has influenced the medical field as well. For progress of\ndiagnostic results most of the reputed hospitals and diagnostic centres all\nover the world have started exchanging medical information. In this proposed\nmethod, the calculated diagnostic parametric values of the original\nElectrooculography (EOG) signal are embedded as a watermark by using Difference\nExpansion (DE) algorithm based reversible watermarking technique. The extracted\nwatermark provides the required parametric values at the recipient end without\nany post computation of the recovered EOG signal. By computing the parametric\nvalues from the recovered signal, the integrity of the extracted watermark can\nbe validated. The time domain features of EOG signal are calculated for the\ngeneration of watermark. In the current work, various features are studied and\ntwo major features related to blink frequency are used to generate the\nwatermark. The high Signal to Noise Ratio (SNR) and the Bit Error Rate (BER)\nclaim the robustness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2013 13:58:40 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Dey", "Nilanjan", ""], ["Maji", "Prasenjit", ""], ["Das", "Poulami", ""], ["Biswas", "Shouvik", ""], ["Das", "Achintya", ""], ["Chaudhuri", "Sheli Sinha", ""]]}, {"id": "1304.2367", "submitter": "Tod S. Levitt", "authors": "Tod S. Levitt, Thomas O. Binford, Gil J. Ettinger, Patrice Gelband", "title": "Utility-Based Control for Computer Vision", "comments": "Appears in Proceedings of the Fourth Conference on Uncertainty in\n  Artificial Intelligence (UAI1988)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1988-PG-245-256", "categories": "cs.CV cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several key issues arise in implementing computer vision recognition of world\nobjects in terms of Bayesian networks. Computational efficiency is a driving\nforce. Perceptual networks are very deep, typically fifteen levels of\nstructure. Images are wide, e.g., an unspecified-number of edges may appear\nanywhere in an image 512 x 512 pixels or larger. For efficiency, we dynamically\ninstantiate hypotheses of observed objects. The network is not fixed, but is\ncreated incrementally at runtime. Generation of hypotheses of world objects and\nindexing of models for recognition are important, but they are not considered\nhere [4,11]. This work is aimed at near-term implementation with parallel\ncomputation in a radar surveillance system, ADRIES [5, 15], and a system for\nindustrial part recognition, SUCCESSOR [2]. For many applications, vision must\nbe faster to be practical and so efficiently controlling the machine vision\nprocess is critical. Perceptual operators may scan megapixels and may require\nminutes of computation time. It is necessary to avoid unnecessary sensor\nactions and computation. Parallel computation is available at several levels of\nprocessor capability. The potential for parallel, distributed computation for\nhigh-level vision means distributing non-homogeneous computations. This paper\naddresses the problem of task control in machine vision systems based on\nBayesian probability models. We separate control and inference to extend the\nprevious work [3] to maximize utility instead of probability. Maximizing\nutility allows adopting perceptual strategies for efficient information\ngathering with sensors and analysis of sensor data. Results of controlling\nmachine vision via utility to recognize military situations are presented in\nthis paper. Future work extends this to industrial part recognition for\nSUCCESSOR.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 19:44:16 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Levitt", "Tod S.", ""], ["Binford", "Thomas O.", ""], ["Ettinger", "Gil J.", ""], ["Gelband", "Patrice", ""]]}, {"id": "1304.2490", "submitter": "Yanhui Xiao", "authors": "Yanhui Xiao, Zhenfeng Zhu, Yao Zhao", "title": "Kernel Reconstruction ICA for Sparse Representation", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Component Analysis (ICA) is an effective unsupervised tool to\nlearn statistically independent representation. However, ICA is not only\nsensitive to whitening but also difficult to learn an over-complete basis.\nConsequently, ICA with soft Reconstruction cost(RICA) was presented to learn\nsparse representations with over-complete basis even on unwhitened data.\nWhereas RICA is infeasible to represent the data with nonlinear structure due\nto its intrinsic linearity. In addition, RICA is essentially an unsupervised\nmethod and can not utilize the class information. In this paper, we propose a\nkernel ICA model with reconstruction constraint (kRICA) to capture the\nnonlinear features. To bring in the class information, we further extend the\nunsupervised kRICA to a supervised one by introducing a discrimination\nconstraint, namely d-kRICA. This constraint leads to learn a structured basis\nconsisted of basis vectors from different basis subsets corresponding to\ndifferent class labels. Then each subset will sparsely represent well for its\nown class but not for the others. Furthermore, data samples belonging to the\nsame class will have similar representations, and thereby the learned sparse\nrepresentations can take more discriminative power. Experimental results\nvalidate the effectiveness of kRICA and d-kRICA for image classification.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 08:45:57 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Xiao", "Yanhui", ""], ["Zhu", "Zhenfeng", ""], ["Zhao", "Yao", ""]]}, {"id": "1304.2683", "submitter": "Nan Yao", "authors": "Yao Nan, Qian Feng and Sun Zuolei", "title": "Image Classification by Feature Dimension Reduction and Graph based\n  Ranking", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction (DR) of image features plays an important role in\nimage retrieval and classification tasks. Recently, two types of methods have\nbeen proposed to improve the both the accuracy and efficiency for the\ndimensionality reduction problem. One uses Non-negative matrix factorization\n(NMF) to describe the image distribution on the space of base matrix. Another\none for dimension reduction trains a subspace projection matrix to project\noriginal data space into some low-dimensional subspaces which have deep\narchitecture, so that the low-dimensional codes would be learned. At the same\ntime, the graph based similarity learning algorithm which tries to exploit\ncontextual information for improving the effectiveness of image rankings is\nalso proposed for image class and retrieval problem. In this paper, after above\ntwo methods mentioned are utilized to reduce the high-dimensional features of\nimages respectively, we learn the graph based similarity for the image\nclassification problem. This paper compares the proposed approach with other\napproaches on an image database.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 18:11:08 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Nan", "Yao", ""], ["Feng", "Qian", ""], ["Zuolei", "Sun", ""]]}, {"id": "1304.2743", "submitter": "Ze-Nian Li", "authors": "Ze-Nian Li", "title": "Comparisons of Reasoning Mechanisms for Computer Vision", "comments": "Appears in Proceedings of the Third Conference on Uncertainty in\n  Artificial Intelligence (UAI1987)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1987-PG-287-294", "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An evidential reasoning mechanism based on the Dempster-Shafer theory of\nevidence is introduced. Its performance in real-world image analysis is\ncompared with other mechanisms based on the Bayesian formalism and a simple\nweight combination method.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 19:48:54 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Li", "Ze-Nian", ""]]}, {"id": "1304.2749", "submitter": "Minchuan Zhang", "authors": "Minchuan Zhang, Su-shing Chen", "title": "Evidential Reasoning in Image Understanding", "comments": "Appears in Proceedings of the Third Conference on Uncertainty in\n  Artificial Intelligence (UAI1987)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1987-PG-340-346", "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present some results of evidential reasoning in\nunderstanding multispectral images of remote sensing systems. The\nDempster-Shafer approach of combination of evidences is pursued to yield\ncontextual classification results, which are compared with previous results of\nthe Bayesian context free classification, contextual classifications of dynamic\nprogramming and stochastic relaxation approaches.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 19:49:25 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Zhang", "Minchuan", ""], ["Chen", "Su-shing", ""]]}, {"id": "1304.2998", "submitter": "David Ram\\'irez", "authors": "Sofia Olhede, David Ram\\'irez and Peter J. Schreier", "title": "Detecting Directionality in Random Fields Using the Monogenic Signal", "comments": null, "journal-ref": "IEEE Transactions on Information Theory (2014)", "doi": "10.1109/TIT.2014.2342734", "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and analyzing directional structures in images is important in many\napplications since one-dimensional patterns often correspond to important\nfeatures such as object contours or trajectories. Classifying a structure as\ndirectional or non-directional requires a measure to quantify the degree of\ndirectionality and a threshold, which needs to be chosen based on the\nstatistics of the image. In order to do this, we model the image as a random\nfield. So far, little research has been performed on analyzing directionality\nin random fields. In this paper, we propose a measure to quantify the degree of\ndirectionality based on the random monogenic signal, which enables a unique\ndecomposition of a 2D signal into local amplitude, local orientation, and local\nphase. We investigate the second-order statistical properties of the monogenic\nsignal for isotropic, anisotropic, and unidirectional random fields. We analyze\nour measure of directionality for finite-size sample images, and determine a\nthreshold to distinguish between unidirectional and non-unidirectional random\nfields, which allows the automatic classification of images.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 15:34:08 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2013 14:01:49 GMT"}, {"version": "v3", "created": "Tue, 10 Jun 2014 07:49:51 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Olhede", "Sofia", ""], ["Ram\u00edrez", "David", ""], ["Schreier", "Peter J.", ""]]}, {"id": "1304.2999", "submitter": "Gilad Lerman Dr", "authors": "Bryan Poling and Gilad Lerman", "title": "A New Approach To Two-View Motion Segmentation Using Global Dimension\n  Minimization", "comments": null, "journal-ref": "International Journal of Computer Vision, 108 (2014), no. 3,\n  165-185", "doi": "10.1007/s11263-013-0694-0", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to rigid-body motion segmentation from two views.\nWe use a previously developed nonlinear embedding of two-view point\ncorrespondences into a 9-dimensional space and identify the different motions\nby segmenting lower-dimensional subspaces. In order to overcome nonuniform\ndistributions along the subspaces, whose dimensions are unknown, we suggest the\nnovel concept of global dimension and its minimization for clustering subspaces\nwith some theoretical motivation. We propose a fast projected gradient\nalgorithm for minimizing global dimension and thus segmenting motions from\n2-views. We develop an outlier detection framework around the proposed method,\nand we present state-of-the-art results on outlier-free and outlier-corrupted\ntwo-view data for segmenting motion.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 15:34:08 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2014 08:27:32 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Poling", "Bryan", ""], ["Lerman", "Gilad", ""]]}, {"id": "1304.3098", "submitter": "Ze-Nian Li", "authors": "Ze-Nian Li, Leonard Uhr", "title": "Evidential Reasoning in Parallel Hierarchical Vision Programs", "comments": "Appears in Proceedings of the Second Conference on Uncertainty in\n  Artificial Intelligence (UAI1986)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1986-PG-175-182", "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient adaptation and application of the\nDempster-Shafer theory of evidence, one that can be used effectively in a\nmassively parallel hierarchical system for visual pattern perception. It\ndescribes the techniques used, and shows in an extended example how they serve\nto improve the system's performance as it applies a multiple-level set of\nprocesses.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 19:53:08 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Li", "Ze-Nian", ""], ["Uhr", "Leonard", ""]]}, {"id": "1304.3192", "submitter": "Yulan Guo", "authors": "Yulan Guo, Ferdous Sohel, Mohammed Bennamoun, Min Lu, Jianwei Wan", "title": "Rotational Projection Statistics for 3D Local Surface Description and\n  Object Recognition", "comments": "The final publication is available at link.springer.com International\n  Journal of Computer Vision 2013", "journal-ref": null, "doi": "10.1007/s11263-013-0627-y", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Recognizing 3D objects in the presence of noise, varying mesh resolution,\nocclusion and clutter is a very challenging task. This paper presents a novel\nmethod named Rotational Projection Statistics (RoPS). It has three major\nmodules: Local Reference Frame (LRF) definition, RoPS feature description and\n3D object recognition. We propose a novel technique to define the LRF by\ncalculating the scatter matrix of all points lying on the local surface. RoPS\nfeature descriptors are obtained by rotationally projecting the neighboring\npoints of a feature point onto 2D planes and calculating a set of statistics\n(including low-order central moments and entropy) of the distribution of these\nprojected points. Using the proposed LRF and RoPS descriptor, we present a\nhierarchical 3D object recognition algorithm. The performance of the proposed\nLRF, RoPS descriptor and object recognition algorithm was rigorously tested on\na number of popular and publicly available datasets. Our proposed techniques\nexhibited superior performance compared to existing techniques. We also showed\nthat our method is robust with respect to noise and varying mesh resolution.\nOur RoPS based algorithm achieved recognition rates of 100%, 98.9%, 95.4% and\n96.0% respectively when tested on the Bologna, UWA, Queen's and Ca' Foscari\nVenezia Datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 04:26:52 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Guo", "Yulan", ""], ["Sohel", "Ferdous", ""], ["Bennamoun", "Mohammed", ""], ["Lu", "Min", ""], ["Wan", "Jianwei", ""]]}, {"id": "1304.3406", "submitter": "Seyed Hamed Alemohammad", "authors": "Seyed Hamed Alemohammad, Dara Entekhabi", "title": "Merging Satellite Measurements of Rainfall Using Multi-scale Imagery\n  Technique", "comments": "6 pages, 10 Figures, WCRP Open Science Conference, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several passive microwave satellites orbit the Earth and measure rainfall.\nThese measurements have the advantage of almost full global coverage when\ncompared to surface rain gauges. However, these satellites have low temporal\nrevisit and missing data over some regions. Image fusion is a useful technique\nto fill in the gaps of one image (one satellite measurement) using another one.\nThe proposed algorithm uses an iterative fusion scheme to integrate information\nfrom two satellite measurements. The algorithm is implemented on two datasets\nfor 7 years of half-hourly data. The results show significant improvements in\nrain detection and rain intensity in the merged measurements.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 19:31:57 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Alemohammad", "Seyed Hamed", ""], ["Entekhabi", "Dara", ""]]}, {"id": "1304.3447", "submitter": "David Sher", "authors": "David Sher", "title": "Developing and Analyzing Boundary Detection Operators Using\n  Probabilistic Models", "comments": "Appears in Proceedings of the First Conference on Uncertainty in\n  Artificial Intelligence (UAI1985)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1985-PG-245-252", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most feature detectors such as edge detectors or circle finders are\nstatistical, in the sense that they decide at each point in an image about the\npresence of a feature, this paper describes the use of Bayesian feature\ndetectors.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 19:58:23 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Sher", "David", ""]]}, {"id": "1304.3573", "submitter": "Simon Beckouche", "authors": "Simon Beckouche, Jean-Luc Starck and Jalal Fadili", "title": "Astronomical Image Denoising Using Dictionary Learning", "comments": null, "journal-ref": null, "doi": "10.1051/0004-6361/201220752", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astronomical images suffer a constant presence of multiple defects that are\nconsequences of the intrinsic properties of the acquisition equipments, and\natmospheric conditions. One of the most frequent defects in astronomical\nimaging is the presence of additive noise which makes a denoising step\nmandatory before processing data. During the last decade, a particular modeling\nscheme, based on sparse representations, has drawn the attention of an ever\ngrowing community of researchers. Sparse representations offer a promising\nframework to many image and signal processing tasks, especially denoising and\nrestoration applications. At first, the harmonics, wavelets, and similar bases\nand overcomplete representations have been considered as candidate domains to\nseek the sparsest representation. A new generation of algorithms, based on\ndata-driven dictionaries, evolved rapidly and compete now with the\noff-the-shelf fixed dictionaries. While designing a dictionary beforehand leans\non a guess of the most appropriate representative elementary forms and\nfunctions, the dictionary learning framework offers to construct the dictionary\nupon the data themselves, which provides us with a more flexible setup to\nsparse modeling and allows to build more sophisticated dictionaries. In this\npaper, we introduce the Centered Dictionary Learning (CDL) method and we study\nits performances for astronomical image denoising. We show how CDL outperforms\nwavelet or classic dictionary learning denoising techniques on astronomical\nimages, and we give a comparison of the effect of these different algorithms on\nthe photometry of the denoised images.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 08:57:25 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Beckouche", "Simon", ""], ["Starck", "Jean-Luc", ""], ["Fadili", "Jalal", ""]]}, {"id": "1304.3915", "submitter": "Tal Hassner", "authors": "Tal Hassner and Ronen Basri", "title": "Single View Depth Estimation from Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a non-parametric, \"example-based\" method for estimating the depth\nof an object, viewed in a single photo. Our method consults a database of\nexample 3D geometries, searching for those which look similar to the object in\nthe photo. The known depths of the selected database objects act as shape\npriors which constrain the process of estimating the object's depth. We show\nhow this process can be performed by optimizing a well defined target\nlikelihood function, via a hard-EM procedure. We address the problem of\nrepresenting the (possibly infinite) variability of viewing conditions with a\nfinite (and often very small) example set, by proposing an on-the-fly example\nupdate scheme. We further demonstrate the importance of non-stationarity in\navoiding misleading examples when estimating structured shapes. We evaluate our\nmethod and present both qualitative as well as quantitative results for\nchallenging object classes. Finally, we show how this same technique may be\nreadily applied to a number of related problems. These include the novel task\nof estimating the occluded depth of an object's backside and the task of\ntailoring custom fitting image-maps for input depths.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2013 13:56:14 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Hassner", "Tal", ""], ["Basri", "Ronen", ""]]}, {"id": "1304.3992", "submitter": "Thara  Nair", "authors": "K. Phani Tejaswi, D. Shanmukha Rao, Thara Nair, A. V. V. Prasad", "title": "GPU Acclerated Automated Feature Extraction from Satellite Images", "comments": null, "journal-ref": "International Journal of Distributed and Parallel Systems (IJDPS)\n  Vol.4, No.2, March 2013", "doi": "10.5121/ijdps.2013.4201", "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large volumes of remote sensing data insists on higher\ndegree of automation in feature extraction, making it a need of the hour.The\nhuge quantum of data that needs to be processed entails accelerated processing\nto be enabled.GPUs, which were originally designed to provide efficient\nvisualization, are being massively employed for computation intensive parallel\nprocessing environments. Image processing in general and hence automated\nfeature extraction, is highly computation intensive, where performance\nimprovements have a direct impact on societal needs. In this context, an\nalgorithm has been formulated for automated feature extraction from a\npanchromatic or multispectral image based on image processing techniques. Two\nLaplacian of Guassian (LoG) masks were applied on the image individually\nfollowed by detection of zero crossing points and extracting the pixels based\non their standard deviation with the surrounding pixels. The two extracted\nimages with different LoG masks were combined together which resulted in an\nimage with the extracted features and edges. Finally the user is at liberty to\napply the image smoothing step depending on the noise content in the extracted\nimage. The image is passed through a hybrid median filter to remove the salt\nand pepper noise from the image. This paper discusses the aforesaid algorithm\nfor automated feature extraction, necessity of deployment of GPUs for the same;\nsystem-level challenges and quantifies the benefits of integrating GPUs in such\nenvironment. The results demonstrate that substantial enhancement in\nperformance margin can be achieved with the best utilization of GPU resources\nand an efficient parallelization strategy. Performance results in comparison\nwith the conventional computing scenario have provided a speedup of 20x, on\nrealization of this parallelizing strategy.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 06:03:19 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Tejaswi", "K. Phani", ""], ["Rao", "D. Shanmukha", ""], ["Nair", "Thara", ""], ["Prasad", "A. V. V.", ""]]}, {"id": "1304.4041", "submitter": "Humayun Irshad", "authors": "H. Irshad, A. Gouaillard, L. Roux, D. Racoceanu", "title": "Multispectral Spatial Characterization: Application to Mitosis Detection\n  in Breast Cancer Histopathology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection of mitosis plays a critical role in breast cancer\nhistopathology. Manual detection and counting of mitosis is tedious and subject\nto considerable inter- and intra-reader variations. Multispectral imaging is a\nrecent medical imaging technology, proven successful in increasing the\nsegmentation accuracy in other fields. This study aims at improving the\naccuracy of mitosis detection by developing a specific solution using\nmultispectral and multifocal imaging of breast cancer histopathological data.\nWe propose to enable clinical routine-compliant quality of mitosis\ndiscrimination from other objects. The proposed framework includes\ncomprehensive analysis of spectral bands and z-stack focus planes, detection of\nexpected mitotic regions (candidates) in selected focus planes and spectral\nbands, computation of multispectral spatial features for each candidate,\nselection of multispectral spatial features and a study of different\nstate-of-the-art classification methods for candidates classification as\nmitotic or non mitotic figures. This framework has been evaluated on MITOS\nmultispectral medical dataset and achieved 60% detection rate and 57%\nF-Measure. Our results indicate that multispectral spatial features have more\ninformation for mitosis classification in comparison with white spectral band\nfeatures, being therefore a very promising exploration area to improve the\nquality of the diagnosis assistance in histopathology.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 10:11:34 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Irshad", "H.", ""], ["Gouaillard", "A.", ""], ["Roux", "L.", ""], ["Racoceanu", "D.", ""]]}, {"id": "1304.4077", "submitter": "Pritam Ranjan", "authors": "Reshu Agarwal, Pritam Ranjan, Hugh Chipman", "title": "A new Bayesian ensemble of trees classifier for identifying multi-class\n  labels in satellite images", "comments": "31 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of satellite images is a key component of many remote sensing\napplications. One of the most important products of a raw satellite image is\nthe classified map which labels the image pixels into meaningful classes.\nThough several parametric and non-parametric classifiers have been developed\nthus far, accurate labeling of the pixels still remains a challenge. In this\npaper, we propose a new reliable multiclass-classifier for identifying class\nlabels of a satellite image in remote sensing applications. The proposed\nmulticlass-classifier is a generalization of a binary classifier based on the\nflexible ensemble of regression trees model called Bayesian Additive Regression\nTrees (BART). We used three small areas from the LANDSAT 5 TM image, acquired\non August 15, 2009 (path/row: 08/29, L1T product, UTM map projection) over\nKings County, Nova Scotia, Canada to classify the land-use. Several prediction\naccuracy and uncertainty measures have been used to compare the reliability of\nthe proposed classifier with the state-of-the-art classifiers in remote\nsensing.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 12:54:52 GMT"}, {"version": "v2", "created": "Fri, 31 May 2013 16:57:33 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Agarwal", "Reshu", ""], ["Ranjan", "Pritam", ""], ["Chipman", "Hugh", ""]]}, {"id": "1304.4112", "submitter": "Austin Abrams", "authors": "Austin Abrams, Chris Hawley, Kylia Miskell, Adina Stoica, Nathan\n  Jacobs, Robert Pless", "title": "Shadow Estimation Method for \"The Episolar Constraint: Monocular Shape\n  from Shadow Correspondence\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering shadows is an important step for many vision algorithms. Current\napproaches that work with time-lapse sequences are limited to simple\nthresholding heuristics. We show these approaches only work with very careful\ntuning of parameters, and do not work well for long-term time-lapse sequences\ntaken over the span of many months. We introduce a parameter-free expectation\nmaximization approach which simultaneously estimates shadows, albedo, surface\nnormals, and skylight. This approach is more accurate than previous methods,\nworks over both very short and very long sequences, and is robust to the\neffects of nonlinear camera response. Finally, we demonstrate that the shadow\nmasks derived through this algorithm substantially improve the performance of\nsun-based photometric stereo compared to earlier shadow mask estimation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 14:31:21 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Abrams", "Austin", ""], ["Hawley", "Chris", ""], ["Miskell", "Kylia", ""], ["Stoica", "Adina", ""], ["Jacobs", "Nathan", ""], ["Pless", "Robert", ""]]}, {"id": "1304.4344", "submitter": "Conrad Sanderson", "authors": "Mehrtash T. Harandi, Conrad Sanderson, Richard Hartley, Brian C.\n  Lovell", "title": "Sparse Coding and Dictionary Learning for Symmetric Positive Definite\n  Matrices: A Kernel Approach", "comments": null, "journal-ref": "European Conference on Computer Vision, Lecture Notes in Computer\n  Science (LNCS), Vol. 7573, pp. 216-229, 2012", "doi": "10.1007/978-3-642-33709-3_16", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances suggest that a wide range of computer vision problems can be\naddressed more appropriately by considering non-Euclidean geometry. This paper\ntackles the problem of sparse coding and dictionary learning in the space of\nsymmetric positive definite matrices, which form a Riemannian manifold. With\nthe aid of the recently introduced Stein kernel (related to a symmetric version\nof Bregman matrix divergence), we propose to perform sparse coding by embedding\nRiemannian manifolds into reproducing kernel Hilbert spaces. This leads to a\nconvex and kernel version of the Lasso problem, which can be solved\nefficiently. We furthermore propose an algorithm for learning a Riemannian\ndictionary (used for sparse coding), closely tied to the Stein kernel.\nExperiments on several classification tasks (face recognition, texture\nclassification, person re-identification) show that the proposed sparse coding\napproach achieves notable improvements in discrimination accuracy, in\ncomparison to state-of-the-art methods such as tensor sparse coding, Riemannian\nlocality preserving projection, and symmetry-driven accumulation of local\nfeatures.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 06:47:03 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Harandi", "Mehrtash T.", ""], ["Sanderson", "Conrad", ""], ["Hartley", "Richard", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1304.4535", "submitter": "Odemir Bruno PhD", "authors": "N\\'ubia Rosa da Silva and Odemir Martinez Bruno", "title": "Heterogeneous patterns enhancing static and dynamic texture\n  classification", "comments": "6 pages, 5 figures", "journal-ref": "N\\'ubia Rosa da Silva and Odemir Martinez Bruno 2013 J. Phys.:\n  Conf. Ser. 410 012033", "doi": "10.1088/1742-6596/410/1/012033", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some mixtures, such as colloids like milk, blood, and gelatin, have\nhomogeneous appearance when viewed with the naked eye, however, to observe them\nat the nanoscale is possible to understand the heterogeneity of its components.\nThe same phenomenon can occur in pattern recognition in which it is possible to\nsee heterogeneous patterns in texture images. However, current methods of\ntexture analysis can not adequately describe such heterogeneous patterns.\nCommon methods used by researchers analyse the image information in a global\nway, taking all its features in an integrated manner. Furthermore, multi-scale\nanalysis verifies the patterns at different scales, but still preserving the\nhomogeneous analysis. On the other hand various methods use textons to\nrepresent the texture, breaking texture down into its smallest unit. To tackle\nthis problem, we propose a method to identify texture patterns not small as\ntextons at distinct scales enhancing the separability among different types of\ntexture. We find sub patterns of texture according to the scale and then group\nsimilar patterns for a more refined analysis. Tests were performed in four\nstatic texture databases and one dynamic one. Results show that our method\nprovides better classification rate compared with conventional approaches both\nin static and in dynamic texture.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 17:53:16 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["da Silva", "N\u00fabia Rosa", ""], ["Bruno", "Odemir Martinez", ""]]}, {"id": "1304.4634", "submitter": "Leonardo Torres", "authors": "Leonardo Torres and Sidnei J. S. Sant'Anna and Corina C. Freitas and\n  Alejandro C. Frery", "title": "Speckle Reduction in Polarimetric SAR Imagery with Stochastic Distances\n  and Nonlocal Means", "comments": "Accepted for publication in Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.GR math.IT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a technique for reducing speckle in Polarimetric\nSynthetic Aperture Radar (PolSAR) imagery using Nonlocal Means and a\nstatistical test based on stochastic divergences. The main objective is to\nselect homogeneous pixels in the filtering area through statistical tests\nbetween distributions. This proposal uses the complex Wishart model to describe\nPolSAR data, but the technique can be extended to other models. The weights of\nthe location-variant linear filter are function of the p-values of tests which\nverify the hypothesis that two samples come from the same distribution and,\ntherefore, can be used to compute a local mean. The test stems from the family\nof (h-phi) divergences which originated in Information Theory. This novel\ntechnique was compared with the Boxcar, Refined Lee and IDAN filters. Image\nquality assessment methods on simulated and real data are employed to validate\nthe performance of this approach. We show that the proposed filter also\nenhances the polarimetric entropy and preserves the scattering information of\nthe targets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 22:11:53 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Torres", "Leonardo", ""], ["Sant'Anna", "Sidnei J. S.", ""], ["Freitas", "Corina C.", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1304.4652", "submitter": "Ankit Chaudhary", "authors": "Ankit Chaudhary, Jagdish L. Raheja", "title": "A Health Monitoring System for Elder and Sick Persons", "comments": null, "journal-ref": "International Journal of Computer Theory and Engineering, Vol. 5,\n  No. 3, June 2013", "doi": "10.7763/IJCTE.2013.V5.723", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a vision based health monitoring system which would be\nvery easy in use and deployment. Elder and sick people who are not able to talk\nor walk they are dependent on other human beings for their daily needs and need\ncontinuous monitoring. The developed system provides facility to the sick or\nelder person to describe his or her need to their caretaker in lingual\ndescription by showing particular hand gesture with the developed system. This\nsystem uses fingertip detection technique for gesture extraction and artificial\nneural network for gesture classification and recognition. The system is able\nto work in different light conditions and can be connected to different devices\nto announce users need on a distant location.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 00:33:21 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Chaudhary", "Ankit", ""], ["Raheja", "Jagdish L.", ""]]}, {"id": "1304.4662", "submitter": "Ankit Chaudhary", "authors": "J. L. Raheja, A. Chaudhary, K Singal", "title": "Tracking of Fingertips and Centres of Palm using KINECT", "comments": "4 page", "journal-ref": "In proceedings of the 3rd IEEE International Conference on\n  Computational Intelligence, Modelling and Simulation, Malaysia, 20-22 Sep,\n  2011, pp. 248-252", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Hand Gesture is a popular way to interact or control machines and it has been\nimplemented in many applications. The geometry of hand is such that it is hard\nto construct in virtual environment and control the joints but the\nfunctionality and DOF encourage researchers to make a hand like instrument.\nThis paper presents a novel method for fingertips detection and centres of\npalms detection distinctly for both hands using MS KINECT in 3D from the input\nimage. KINECT facilitates us by providing the depth information of foreground\nobjects. The hands were segmented using the depth vector and centres of palms\nwere detected using distance transformation on inverse image. This result would\nbe used to feed the inputs to the robotic hands to emulate human hands\noperation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 01:20:10 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Raheja", "J. L.", ""], ["Chaudhary", "A.", ""], ["Singal", "K", ""]]}, {"id": "1304.4711", "submitter": "Ankit Chaudhary", "authors": "Ankit Chaudhary, Ankur Gupta", "title": "Automated Switching System for Skin Pixel Segmentation in Varied\n  Lighting", "comments": "6 pages", "journal-ref": "19th IEEE International Conference on Mechatronics and Machine\n  Vision in Practice, Auckland, New Zealand, 28-30 Nov, 2012, pp. 26-31", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Computer Vision, colour-based spatial techniquesoften assume a static skin\ncolour model. However, skin colour perceived by a camera can change when\nlighting changes. In common real environment multiple light sources impinge on\nthe skin. Moreover, detection techniques may vary when the image under study is\ntaken under different lighting condition than the one that was earlier under\nconsideration. Therefore, for robust skin pixel detection, a dynamic skin\ncolour model that can cope with the changes must be employed. This paper shows\nthat skin pixel detection in a digital colour image can be significantly\nimproved by employing automated colour space switching methods. In the root of\nthe switching technique which is employed in this study, lies the statistical\nmean of value of the skin pixels in the image which in turn has been derived\nfrom the Value, measures as a third component of the HSV. The study is based on\nexperimentations on a set of images where capture time conditions varying from\nhighly illuminated to almost dark.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 07:07:52 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Chaudhary", "Ankit", ""], ["Gupta", "Ankur", ""]]}, {"id": "1304.4765", "submitter": "Faouzi Benzarti", "authors": "Soumaya Hichri, Faouzi Benzarti, Hamid Amiri", "title": "Robust Noise Filtering in Image Sequences", "comments": "5 pages", "journal-ref": "International Journal of Computer Applications Volume 50 No.18,\n  July 2012, ISSN 0975-8887", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image sequences filtering have recently become a very important technical\nproblem especially with the advent of new technology in multimedia and video\nsystems applications. Often image sequences are corrupted by some amount of\nnoise introduced by the image sensor and therefore inherently present in the\nimaging process. The main problem in the image sequences is how to deal with\nspatio-temporal and non stationary signals. In this paper, we propose a robust\nmethod for noise removal of image sequence based on coupled spatial and\ntemporal anisotropic diffusion. The idea is to achieve an adaptive smoothing in\nboth spatial and temporal directions, by solving a nonlinear diffusion\nequation. This allows removing noise while preserving all spatial and temporal\ndiscontinuities\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 10:55:43 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Hichri", "Soumaya", ""], ["Benzarti", "Faouzi", ""], ["Amiri", "Hamid", ""]]}, {"id": "1304.4994", "submitter": "Edgar Chavez", "authors": "Edgar Ch\\'avez and Ana C. Ch\\'avez-C\\'aliz and Jorge L.\n  L\\'opez-L\\'opez", "title": "Polygon Matching and Indexing Under Affine Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection $\\{Z_1,Z_2,\\ldots,Z_m\\}$ of $n$-sided polygons in the\nplane and a query polygon $W$ we give algorithms to find all $Z_\\ell$ such that\n$W=f(Z_\\ell)$ with $f$ an unknown similarity transformation in time independent\nof the size of the collection. If $f$ is a known affine transformation, we show\nhow to find all $Z_\\ell$ such that $W=f(Z_\\ell)$ in $O(n+\\log(m))$ time.\n  For a pair $W,W^\\prime$ of polygons we can find all the pairs\n$Z_\\ell,Z_{\\ell^\\prime}$ such that $W=f(Z_\\ell)$ and\n$W^\\prime=f(Z_{\\ell^\\prime})$ for an unknown affine transformation $f$ in\n$O(m+n)$ time.\n  For the case of triangles we also give bounds for the problem of matching\ntriangles with variable vertices, which is equivalent to affine matching\ntriangles in noisy conditions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2013 00:40:22 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["Ch\u00e1vez", "Edgar", ""], ["Ch\u00e1vez-C\u00e1liz", "Ana C.", ""], ["L\u00f3pez-L\u00f3pez", "Jorge L.", ""]]}, {"id": "1304.5063", "submitter": "Hichem Bannour", "authors": "Hichem Bannour and C\\'eline Hudelot", "title": "Combinaison d'information visuelle, conceptuelle, et contextuelle pour\n  la construction automatique de hierarchies semantiques adaptees a\n  l'annotation d'images", "comments": "RFIA 2012 (Reconnaissance des Formes et Intelligence Artificielle)\n  Lyon, France pg. 462-469. 9 pages", "journal-ref": "RFIA 2012 (Reconnaissance des Formes et Intelligence Artificielle)\n  Lyon, France pg. 462-469", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper proposes a new methodology to automatically build semantic\nhierarchies suitable for image annotation and classification. The building of\nthe hierarchy is based on a new measure of semantic similarity. The proposed\nmeasure incorporates several sources of information: visual, conceptual and\ncontextual as we defined in this paper. The aim is to provide a measure that\nbest represents image semantics. We then propose rules based on this measure,\nfor the building of the final hierarchy, and which explicitly encode\nhierarchical relationships between different concepts. Therefore, the built\nhierarchy is used in a semantic hierarchical classification framework for image\nannotation. Our experiments and results show that the hierarchy built improves\nclassification results.\n  Ce papier propose une nouvelle methode pour la construction automatique de\nhierarchies semantiques adaptees a la classification et a l'annotation\nd'images. La construction de la hierarchie est basee sur une nouvelle mesure de\nsimilarite semantique qui integre plusieurs sources d'informations: visuelle,\nconceptuelle et contextuelle que nous definissons dans ce papier. L'objectif\nest de fournir une mesure qui est plus proche de la semantique des images. Nous\nproposons ensuite des regles, basees sur cette mesure, pour la construction de\nla hierarchie finale qui encode explicitement les relations hierarchiques entre\nles differents concepts. La hierarchie construite est ensuite utilisee dans un\ncadre de classification semantique hierarchique d'images en concepts visuels.\nNos experiences et resultats montrent que la hierarchie construite permet\nd'ameliorer les resultats de la classification.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2013 09:40:12 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2013 11:49:10 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Bannour", "Hichem", ""], ["Hudelot", "C\u00e9line", ""]]}, {"id": "1304.5212", "submitter": "Duc Phu Chau", "authors": "Duc Phu Chau (INRIA Sophia Antipolis), Fran\\c{c}ois Bremond (INRIA\n  Sophia Antipolis), Monique Thonnat (INRIA Sophia Antipolis)", "title": "Object Tracking in Videos: Approaches and Issues", "comments": null, "journal-ref": "The International Workshop \"Rencontres UNS-UD\" (RUNSUD) (2013)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile object tracking has an important role in the computer vision\napplications. In this paper, we use a tracked target-based taxonomy to present\nthe object tracking algorithms. The tracked targets are divided into three\ncategories: points of interest, appearance and silhouette of mobile objects.\nAdvantages and limitations of the tracking approaches are also analyzed to find\nthe future directions in the object tracking domain.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2013 18:41:47 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["Chau", "Duc Phu", "", "INRIA Sophia Antipolis"], ["Bremond", "Fran\u00e7ois", "", "INRIA\n  Sophia Antipolis"], ["Thonnat", "Monique", "", "INRIA Sophia Antipolis"]]}, {"id": "1304.5319", "submitter": "Martin Kiechle", "authors": "Martin Kiechle, Simon Hawe, Martin Kleinsteuber", "title": "A Joint Intensity and Depth Co-Sparse Analysis Model for Depth Map\n  Super-Resolution", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution depth maps can be inferred from low-resolution depth\nmeasurements and an additional high-resolution intensity image of the same\nscene. To that end, we introduce a bimodal co-sparse analysis model, which is\nable to capture the interdependency of registered intensity and depth\ninformation. This model is based on the assumption that the co-supports of\ncorresponding bimodal image structures are aligned when computed by a suitable\npair of analysis operators. No analytic form of such operators exist and we\npropose a method for learning them from a set of registered training signals.\nThis learning process is done offline and returns a bimodal analysis operator\nthat is universally applicable to natural scenes. We use this to exploit the\nbimodal co-sparse analysis model as a prior for solving inverse problems, which\nleads to an efficient algorithm for depth map super-resolution.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 06:35:33 GMT"}], "update_date": "2013-04-22", "authors_parsed": [["Kiechle", "Martin", ""], ["Hawe", "Simon", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1304.5409", "submitter": "Carsten Gottschlich", "authors": "Carsten Gottschlich and Stephan Huckemann", "title": "Separating the Real from the Synthetic: Minutiae Histograms as\n  Fingerprints of Fingerprints", "comments": null, "journal-ref": "IET Biometrics, vol. 3, no. 4, pp. 291-301, Dec. 2014", "doi": "10.1049/iet-bmt.2013.0065", "report-no": null, "categories": "cs.CV cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we show that by the current state-of-the-art synthetically\ngenerated fingerprints can easily be discriminated from real fingerprints. We\npropose a method based on second order extended minutiae histograms (MHs) which\ncan distinguish between real and synthetic prints with very high accuracy. MHs\nprovide a fixed-length feature vector for a fingerprint which are invariant\nunder rotation and translation. This 'test of realness' can be applied to\nsynthetic fingerprints produced by any method. In this work, tests are\nconducted on the 12 publicly available databases of FVC2000, FVC2002 and\nFVC2004 which are well established benchmarks for evaluating the performance of\nfingerprint recognition algorithms; 3 of these 12 databases consist of\nartificial fingerprints generated by the SFinGe software. Additionally, we\nevaluate the discriminative performance on a database of synthetic fingerprints\ngenerated by the software of Bicz versus real fingerprint images. We conclude\nwith suggestions for the improvement of synthetic fingerprint generation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 13:21:13 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2014 11:28:08 GMT"}, {"version": "v3", "created": "Wed, 15 Oct 2014 14:04:46 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Gottschlich", "Carsten", ""], ["Huckemann", "Stephan", ""]]}, {"id": "1304.5583", "submitter": "Ameet Talwalkar", "authors": "Ameet Talwalkar, Lester Mackey, Yadong Mu, Shih-Fu Chang, Michael I.\n  Jordan", "title": "Distributed Low-rank Subspace Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision problems ranging from image clustering to motion segmentation to\nsemi-supervised learning can naturally be framed as subspace segmentation\nproblems, in which one aims to recover multiple low-dimensional subspaces from\nnoisy and corrupted input data. Low-Rank Representation (LRR), a convex\nformulation of the subspace segmentation problem, is provably and empirically\naccurate on small problems but does not scale to the massive sizes of modern\nvision datasets. Moreover, past work aimed at scaling up low-rank matrix\nfactorization is not applicable to LRR given its non-decomposable constraints.\nIn this work, we propose a novel divide-and-conquer algorithm for large-scale\nsubspace segmentation that can cope with LRR's non-decomposable constraints and\nmaintains LRR's strong recovery guarantees. This has immediate implications for\nthe scalability of subspace segmentation, which we demonstrate on a benchmark\nface recognition dataset and in simulations. We then introduce novel\napplications of LRR-based subspace segmentation to large-scale semi-supervised\nlearning for multimedia event detection, concept detection, and image tagging.\nIn each case, we obtain state-of-the-art results and order-of-magnitude speed\nups.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2013 03:54:48 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2013 02:55:18 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Talwalkar", "Ameet", ""], ["Mackey", "Lester", ""], ["Mu", "Yadong", ""], ["Chang", "Shih-Fu", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1304.5587", "submitter": "Surya Prasath", "authors": "V. B. Surya Prasath, Juan C. Moreno, K. Palaniappan", "title": "Color image denoising by chromatic edges based vector valued diffusion", "comments": "Submitted to IEEE Signal Processing Letters, 4 pages, 4 figures, 2\n  tables. Some mistakes were corrected from previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter we propose to denoise digital color images via an improved\ngeometric diffusion scheme. By introducing edges detected from all three color\nchannels into the diffusion the proposed scheme avoids color smearing\nartifacts. Vector valued diffusion is used to control the smoothing and the\ngeometry of color images are taken into consideration. Color edge strength\nfunction computed from different planes is introduced and it stops the\ndiffusion spread across chromatic edges. Experimental results indicate that the\nscheme achieves good denoising with edge preservation when compared to other\nrelated schemes.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2013 05:05:53 GMT"}, {"version": "v2", "created": "Wed, 15 May 2013 07:22:13 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Prasath", "V. B. Surya", ""], ["Moreno", "Juan C.", ""], ["Palaniappan", "K.", ""]]}, {"id": "1304.5894", "submitter": "Bruno Cornelis", "authors": "Bruno Cornelis, Yun Yang, Joshua T. Vogelstein, Ann Dooms, Ingrid\n  Daubechies, David Dunson", "title": "Bayesian crack detection in ultra high resolution multimodal images of\n  paintings", "comments": "8 pages, double column", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The preservation of our cultural heritage is of paramount importance. Thanks\nto recent developments in digital acquisition techniques, powerful image\nanalysis algorithms are developed which can be useful non-invasive tools to\nassist in the restoration and preservation of art. In this paper we propose a\nsemi-supervised crack detection method that can be used for high-dimensional\nacquisitions of paintings coming from different modalities. Our dataset\nconsists of a recently acquired collection of images of the Ghent Altarpiece\n(1432), one of Northern Europe's most important art masterpieces. Our goal is\nto build a classifier that is able to discern crack pixels from the background\nconsisting of non-crack pixels, making optimal use of the information that is\nprovided by each modality. To accomplish this we employ a recently developed\nnon-parametric Bayesian classifier, that uses tensor factorizations to\ncharacterize any conditional probability. A prior is placed on the parameters\nof the factorization such that every possible interaction between predictors is\nallowed while still identifying a sparse subset among these predictors. The\nproposed Bayesian classifier, which we will refer to as conditional Bayesian\ntensor factorization or CBTF, is assessed by visually comparing classification\nresults with the Random Forest (RF) algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 09:46:47 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2013 09:00:01 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["Cornelis", "Bruno", ""], ["Yang", "Yun", ""], ["Vogelstein", "Joshua T.", ""], ["Dooms", "Ann", ""], ["Daubechies", "Ingrid", ""], ["Dunson", "David", ""]]}, {"id": "1304.6108", "submitter": "Nicolas Charon", "authors": "Nicolas Charon, Alain Trouv\\'e", "title": "The varifold representation of non-oriented shapes for diffeomorphic\n  registration", "comments": "33 pages, 10 figures", "journal-ref": "SIAM Journal on Imaging Sciences, 2013, Vol. 6, No. 4 : pp.\n  2547-2580", "doi": "10.1137/130918885", "report-no": null, "categories": "cs.CG cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of orientation that naturally arises\nwhen representing shapes like curves or surfaces as currents. In the field of\ncomputational anatomy, the framework of currents has indeed proved very\nefficient to model a wide variety of shapes. However, in such approaches,\norientation of shapes is a fundamental issue that can lead to several drawbacks\nin treating certain kind of datasets. More specifically, problems occur with\nstructures like acute pikes because of canceling effects of currents or with\ndata that consists in many disconnected pieces like fiber bundles for which\ncurrents require a consistent orientation of all pieces. As a promising\nalternative to currents, varifolds, introduced in the context of geometric\nmeasure theory by F. Almgren, allow the representation of any non-oriented\nmanifold (more generally any non-oriented rectifiable set). In particular, we\nexplain how varifolds can encode numerically non-oriented objects both from the\ndiscrete and continuous point of view. We show various ways to build a Hilbert\nspace structure on the set of varifolds based on the theory of reproducing\nkernels. We show that, unlike the currents' setting, these metrics are\nconsistent with shape volume (theorem 4.1) and we derive a formula for the\nvariation of metric with respect to the shape (theorem 4.2). Finally, we\npropose a generalization to non-oriented shapes of registration algorithms in\nthe context of Large Deformations Metric Mapping (LDDMM), which we detail with\na few examples in the last part of the paper.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 21:03:45 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Charon", "Nicolas", ""], ["Trouv\u00e9", "Alain", ""]]}, {"id": "1304.6192", "submitter": "Hafeez Anwar", "authors": "Hafeez Anwar, Sebastian Zambanini, Martin Kampel", "title": "A Bag of Visual Words Approach for Symbols-Based Coarse-Grained Ancient\n  Coin Classification", "comments": "Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876)", "journal-ref": null, "doi": null, "report-no": "OAGM AND AAPR/2013/17", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of Numismatics provides the names and descriptions of the symbols\nminted on the ancient coins. Classification of the ancient coins aims at\nassigning a given coin to its issuer. Various issuers used various symbols for\ntheir coins. We propose to use these symbols for a framework that will coarsely\nclassify the ancient coins. Bag of visual words (BoVWs) is a well established\nvisual recognition technique applied to various problems in computer vision\nlike object and scene recognition. Improvements have been made by incorporating\nthe spatial information to this technique. We apply the BoVWs technique to our\nproblem and use three symbols for coarse-grained classification. We use\nrectangular tiling, log-polar tiling and circular tiling to incorporate spatial\ninformation to BoVWs. Experimental results show that the circular tiling proves\nsuperior to the rest of the methods for our problem.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 07:46:11 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["Anwar", "Hafeez", ""], ["Zambanini", "Sebastian", ""], ["Kampel", "Martin", ""]]}, {"id": "1304.6213", "submitter": "Roland Perko", "authors": "Roland Perko, Thomas Schnabel, Gerald Fritz, Alexander Almer, Lucas\n  Paletta", "title": "Counting people from above: Airborne video based crowd analysis", "comments": "Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876)", "journal-ref": null, "doi": null, "report-no": "OAGM-AAPR/2013/15", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd monitoring and analysis in mass events are highly important\ntechnologies to support the security of attending persons. Proposed methods\nbased on terrestrial or airborne image/video data often fail in achieving\nsufficiently accurate results to guarantee a robust service. We present a novel\nframework for estimating human count, density and motion from video data based\non custom tailored object detection techniques, a regression based density\nestimate and a total variation based optical flow extraction. From the gathered\nfeatures we present a detailed accuracy analysis versus ground truth\nmeasurements. In addition, all information is projected into world coordinates\nto enable a direct integration with existing geo-information systems. The\nresulting human counts demonstrate a mean error of 4% to 9% and thus represent\na most efficient measure that can be robustly applied in security critical\nservices.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 09:51:02 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["Perko", "Roland", ""], ["Schnabel", "Thomas", ""], ["Fritz", "Gerald", ""], ["Almer", "Alexander", ""], ["Paletta", "Lucas", ""]]}, {"id": "1304.6291", "submitter": "Fang Wang", "authors": "Fang Wang and Yi Li", "title": "Learning Visual Symbols for Parsing Human Poses in Images", "comments": "IJCAI 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing human poses in images is fundamental in extracting critical visual\ninformation for artificial intelligent agents. Our goal is to learn\nself-contained body part representations from images, which we call visual\nsymbols, and their symbol-wise geometric contexts in this parsing process. Each\nsymbol is individually learned by categorizing visual features leveraged by\ngeometric information. In the categorization, we use Latent Support Vector\nMachine followed by an efficient cross validation procedure to learn visual\nsymbols. Then, these symbols naturally define geometric contexts of body parts\nin a fine granularity. When the structure of the compositional parts is a tree,\nwe derive an efficient approach to estimating human poses in images.\nExperiments on two large datasets suggest our approach outperforms state of the\nart methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 14:07:19 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["Wang", "Fang", ""], ["Li", "Yi", ""]]}, {"id": "1304.6379", "submitter": "Firas Ajil Jassim", "authors": "Firas A. Jassim", "title": "Semi-Optimal Edge Detector based on Simple Standard Deviation with\n  Adjusted Thresholding", "comments": "6 pages, 1 table, 6 figures", "journal-ref": "International Journal of Computer Applications,Volume 68, No.2,\n  April 2013", "doi": "10.5120/11555-6834", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper proposes a novel method which combines both median filter and\nsimple standard deviation to accomplish an excellent edge detector for image\nprocessing. First of all, a denoising process must be applied on the grey scale\nimage using median filter to identify pixels which are likely to be\ncontaminated by noise. The benefit of this step is to smooth the image and get\nrid of the noisy pixels. After that, the simple statistical standard deviation\ncould be computed for each 2X2 window size. If the value of the standard\ndeviation inside the 2X2 window size is greater than a predefined threshold,\nthen the upper left pixel in the 2?2 window represents an edge. The visual\ndifferences between the proposed edge detector and the standard known edge\ndetectors have been shown to support the contribution in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 18:53:58 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["Jassim", "Firas A.", ""]]}, {"id": "1304.6759", "submitter": "Firas Ajil Jassim", "authors": "Firas A. Jassim", "title": "k-Modulus Method for Image Transformation", "comments": "5 pages, 2 tables, 6 figures", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications, Vol. 4, No. 3, 2013", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we propose a new algorithm to make a novel spatial image\ntransformation. The proposed approach aims to reduce the bit depth used for\nimage storage. The basic technique for the proposed transformation is based of\nthe modulus operator. The goal is to transform the whole image into multiples\nof predefined integer. The division of the whole image by that integer will\nguarantee that the new image surely less in size from the original image. The\nk-Modulus Method could not be used as a stand alone transform for image\ncompression because of its high compression ratio. It could be used as a scheme\nembedded in other image processing fields especially compression. According to\nits high PSNR value, it could be amalgamated with other methods to facilitate\nthe redundancy criterion.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 21:34:30 GMT"}], "update_date": "2013-04-26", "authors_parsed": [["Jassim", "Firas A.", ""]]}, {"id": "1304.6899", "submitter": "Bal\\'azs Szalkai", "authors": "Bal\\'azs Szalkai", "title": "An implementation of the relational k-means algorithm", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A C# implementation of a generalized k-means variant called relational\nk-means is described here. Relational k-means is a generalization of the\nwell-known k-means clustering method which works for non-Euclidean scenarios as\nwell. The input is an arbitrary distance matrix, as opposed to the traditional\nk-means method, where the clustered objects need to be identified with vectors.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 12:59:31 GMT"}], "update_date": "2013-04-26", "authors_parsed": [["Szalkai", "Bal\u00e1zs", ""]]}, {"id": "1304.6933", "submitter": "Manuel Keglevic", "authors": "Manuel Keglevic and Robert Sablatnig", "title": "Digit Recognition in Handwritten Weather Records", "comments": "Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876), 8 pages", "journal-ref": null, "doi": null, "report-no": "OAGM-AAPR/2013/07", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the automatic recognition of handwritten temperature\nvalues in weather records. The localization of table cells is based on line\ndetection using projection profiles. Further, a stroke-preserving line removal\nmethod which is based on gradient images is proposed. The presented digit\nrecognition utilizes features which are extracted using a set of filters and a\nSupport Vector Machine classifier. It was evaluated on the MNIST and the USPS\ndataset and our own database with about 17,000 RGB digit images. An accuracy of\n99.36% per digit is achieved for the entire system using a set of 84 weather\nrecords.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 15:14:42 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2013 08:35:18 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Keglevic", "Manuel", ""], ["Sablatnig", "Robert", ""]]}, {"id": "1304.6990", "submitter": "Tanja Schilling", "authors": "Tanja Schilling and Tomas Pajdla", "title": "Euclidean Upgrade from a Minimal Number of Segments", "comments": "Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876)", "journal-ref": null, "doi": null, "report-no": "OAGM-AAPR/2013/03", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an algebraic approach to upgrade a projective\nreconstruction to a Euclidean one, and aim at computing the rectifying\nhomography from a minimal number of 9 segments of known length. Constraints are\nderived from these segments which yield a set of polynomial equations that we\nsolve by means of Gr\\\"obner bases. We explain how a solver for such a system of\nequations can be constructed from simplified template data. Moreover, we\npresent experiments that demonstrate that the given problem can be solved in\nthis way.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 19:44:26 GMT"}], "update_date": "2013-04-26", "authors_parsed": [["Schilling", "Tanja", ""], ["Pajdla", "Tomas", ""]]}, {"id": "1304.7132", "submitter": "Gernot Riegler", "authors": "Gernot Riegler, Thomas Pock, Werner P\\\"otzi and Astrid Veronig", "title": "Filament and Flare Detection in H{\\alpha} image sequences", "comments": "Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876)", "journal-ref": null, "doi": null, "report-no": "OAGM-AAPR/2013/16", "categories": "cs.CV astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solar storms can have a major impact on the infrastructure of the earth. Some\nof the causing events are observable from ground in the H{\\alpha} spectral\nline. In this paper we propose a new method for the simultaneous detection of\nflares and filaments in H{\\alpha} image sequences. Therefore we perform several\npreprocessing steps to enhance and normalize the images. Based on the intensity\nvalues we segment the image by a variational approach. In a final\npostprecessing step we derive essential properties to classify the events and\nfurther demonstrate the performance by comparing our obtained results to the\ndata annotated by an expert. The information produced by our method can be used\nfor near real-time alerts and the statistical analysis of existing data by\nsolar physicists.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 11:40:19 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Riegler", "Gernot", ""], ["Pock", "Thomas", ""], ["P\u00f6tzi", "Werner", ""], ["Veronig", "Astrid", ""]]}, {"id": "1304.7140", "submitter": "Michael Helmberger Michael Helmberger", "authors": "M. Helmberger, M. Urschler, M. Pienn, Z.Balint, A. Olschewski and H.\n  Bischof", "title": "Pulmonary Vascular Tree Segmentation from Contrast-Enhanced CT Images", "comments": "Part of the OAGM/AAPR 2013 proceedings (1304.1876)", "journal-ref": null, "doi": null, "report-no": "OAGM-AAPR/2013/09", "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a pulmonary vessel segmentation algorithm, which is fast, fully\nautomatic and robust. It uses a coarse segmentation of the airway tree and a\nleft and right lung labeled volume to restrict a vessel enhancement filter,\nbased on an offset medialness function, to the lungs. We show the application\nof our algorithm on contrast-enhanced CT images, where we derive a clinical\nparameter to detect pulmonary hypertension (PH) in patients. Results on a\ndataset of 24 patients show that quantitative indices derived from the\nsegmentation are applicable to distinguish patients with and without PH.\nFurther work-in-progress results are shown on the VESSEL12 challenge dataset,\nwhich is composed of non-contrast-enhanced scans, where we range in the\nmidfield of participating contestants.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 12:30:36 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Helmberger", "M.", ""], ["Urschler", "M.", ""], ["Pienn", "M.", ""], ["Balint", "Z.", ""], ["Olschewski", "A.", ""], ["Bischof", "H.", ""]]}, {"id": "1304.7153", "submitter": "Peter Innerhofer BSc.", "authors": "Peter Innerhofer, Thomas Pock", "title": "A Convex Approach for Image Hallucination", "comments": "submitted to \\\"OAGM-AAPR 2013, 8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": "OAGM-AAPR/2013/18", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a global convex approach for image hallucination.\nAltering the idea of classical multi image super resolution (SU) systems to\nsingle image SU, we incorporate aligned images to hallucinate the output. Our\nwork is based on the paper of Tappen et al. where they use a non-convex model\nfor image hallucination. In comparison we formulate a convex primal\noptimization problem and derive a fast converging primal-dual algorithm with a\nglobal optimal solution. We use a database with face images to incorporate\nhigh-frequency details to the high-resolution output. We show that we can\nachieve state-of-the-art results by using a convex approach.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 13:10:22 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Innerhofer", "Peter", ""], ["Pock", "Thomas", ""]]}, {"id": "1304.7184", "submitter": "Albert Kavelar", "authors": "Albert Kavelar, Sebastian Zambanini and Martin Kampel", "title": "Reading Ancient Coin Legends: Object Recognition vs. OCR", "comments": "Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876)", "journal-ref": null, "doi": null, "report-no": "OAGM-AAPR/2013/08", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard OCR is a well-researched topic of computer vision and can be\nconsidered solved for machine-printed text. However, when applied to\nunconstrained images, the recognition rates drop drastically. Therefore, the\nemployment of object recognition-based techniques has become state of the art\nin scene text recognition applications. This paper presents a scene text\nrecognition method tailored to ancient coin legends and compares the results\nachieved in character and word recognition experiments to a standard OCR\nengine. The conducted experiments show that the proposed method outperforms the\nstandard OCR engine on a set of 180 cropped coin legend words.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 14:33:52 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Kavelar", "Albert", ""], ["Zambanini", "Sebastian", ""], ["Kampel", "Martin", ""]]}, {"id": "1304.7211", "submitter": "Martin Welk", "authors": "Martin Welk, Martin Erler", "title": "Algorithmic Optimisations for Iterative Deconvolution Methods", "comments": "Part of the OAGM/AAPR 2013 proceedings (arXiv:1304.1876)", "journal-ref": null, "doi": null, "report-no": "OAGM-AAPR/2013/06", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate possibilities to speed up iterative algorithms for non-blind\nimage deconvolution. We focus on algorithms in which convolution with the\npoint-spread function to be deconvolved is used in each iteration, and aim at\naccelerating these convolution operations as they are typically the most\nexpensive part of the computation. We follow two approaches: First, for some\npractically important specific point-spread functions, algorithmically\nefficient sliding window or list processing techniques can be used. In some\nconstellations this allows faster computation than via the Fourier domain.\nSecond, as iterations progress, computation of convolutions can be restricted\nto subsets of pixels. For moderate thinning rates this can be done with almost\nno impact on the reconstruction quality. Both approaches are demonstrated in\nthe context of Richardson-Lucy deconvolution but are not restricted to this\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 16:02:40 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Welk", "Martin", ""], ["Erler", "Martin", ""]]}, {"id": "1304.7236", "submitter": "Alessandro Perina", "authors": "Alessandro Perina, Nebojsa Jojic", "title": "In the sight of my wearable camera: Classifying my visual experience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and we analyze a new dataset which resembles the input to\nbiological vision systems much more than most previously published ones. Our\nanalysis leaded to several important conclusions. First, it is possible to\ndisambiguate over dozens of visual scenes (locations) encountered over the\ncourse of several weeks of a human life with accuracy of over 80%, and this\nopens up possibility for numerous novel vision applications, from early\ndetection of dementia to everyday use of wearable camera streams for automatic\nreminders, and visual stream exchange. Second, our experimental results\nindicate that, generative models such as Latent Dirichlet Allocation or\nCounting Grids, are more suitable to such types of data, as they are more\nrobust to overtraining and comfortable with images at low resolution, blurred\nand characterized by relatively random clutter and a mix of objects.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 17:28:13 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Perina", "Alessandro", ""], ["Jojic", "Nebojsa", ""]]}, {"id": "1304.7399", "submitter": "Jared Glover", "authors": "Jared Glover and Sanja Popovic", "title": "Bingham Procrustean Alignment for Object Detection in Clutter", "comments": "Submitted to IROS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new system for object detection in cluttered RGB-D images is presented. Our\nmain contribution is a new method called Bingham Procrustean Alignment (BPA) to\nalign models with the scene. BPA uses point correspondences between oriented\nfeatures to derive a probability distribution over possible model poses. The\norientation component of this distribution, conditioned on the position, is\nshown to be a Bingham distribution. This result also applies to the classic\nproblem of least-squares alignment of point sets, when point features are\norientation-less, and gives a principled, probabilistic way to measure pose\nuncertainty in the rigid alignment problem. Our detection system leverages BPA\nto achieve more reliable object detections in clutter.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2013 19:24:30 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Glover", "Jared", ""], ["Popovic", "Sanja", ""]]}, {"id": "1304.7465", "submitter": "M. Emre Celebi", "authors": "M. Emre Celebi and Hassan A. Kingravi", "title": "Deterministic Initialization of the K-Means Algorithm Using Hierarchical\n  Clustering", "comments": "23 pages, 3 figures, 10 tables. arXiv admin note: substantial text\n  overlap with arXiv:1209.1960", "journal-ref": "International Journal of Pattern Recognition and Artificial\n  Intelligence 26 (2012) 1250018", "doi": "10.1142/S0218001412500188", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-means is undoubtedly the most widely used partitional clustering algorithm.\nUnfortunately, due to its gradient descent nature, this algorithm is highly\nsensitive to the initial placement of the cluster centers. Numerous\ninitialization methods have been proposed to address this problem. Many of\nthese methods, however, have superlinear complexity in the number of data\npoints, making them impractical for large data sets. On the other hand, linear\nmethods are often random and/or order-sensitive, which renders their results\nunrepeatable. Recently, Su and Dy proposed two highly successful hierarchical\ninitialization methods named Var-Part and PCA-Part that are not only linear,\nbut also deterministic (non-random) and order-invariant. In this paper, we\npropose a discriminant analysis based approach that addresses a common\ndeficiency of these two methods. Experiments on a large and diverse collection\nof data sets from the UCI Machine Learning Repository demonstrate that Var-Part\nand PCA-Part are highly competitive with one of the best random initialization\nmethods to date, i.e., k-means++, and that the proposed approach significantly\nimproves the performance of both hierarchical methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2013 13:31:44 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Celebi", "M. Emre", ""], ["Kingravi", "Hassan A.", ""]]}, {"id": "1304.7713", "submitter": "Ana Georgina Flesia MS", "authors": "Ana Georgina Flesia, Javier Gimenez, Elena Rufeil Fiori", "title": "Markovian models for one dimensional structure estimation on heavily\n  noisy imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radar (SAR) images often exhibit profound appearance variations due to a\nvariety of factors including clutter noise produced by the coherent nature of\nthe illumination. Ultrasound images and infrared images have similar cluttered\nappearance, that make 1 dimensional structures, as edges and object boundaries\ndifficult to locate. Structure information is usually extracted in two steps:\nfirst, building and edge strength mask classifying pixels as edge points by\nhypothesis testing, and secondly estimating from that mask, pixel wide\nconnected edges. With constant false alarm rate (CFAR) edge strength detectors\nfor speckle clutter, the image needs to be scanned by a sliding window composed\nof several differently oriented splitting sub-windows. The accuracy of edge\nlocation for these ratio detectors depends strongly on the orientation of the\nsub-windows. In this work we propose to transform the edge strength detection\nproblem into a binary segmentation problem in the undecimated wavelet domain,\nsolvable using parallel 1d Hidden Markov Models. For general dependency models,\nexact estimation of the state map becomes computationally complex, but in our\nmodel, exact MAP is feasible. The effectiveness of our approach is demonstrated\non simulated noisy real-life natural images with available ground truth, while\nthe strength of our output edge map is measured with Pratt's, Baddeley an Kappa\nproficiency measures. Finally, analysis and experiments on three different\ntypes of SAR images, with different polarizations, resolutions and textures,\nillustrate that the proposed method can detect structure on SAR images\neffectively, providing a very good start point for active contour methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 16:57:47 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Flesia", "Ana Georgina", ""], ["Gimenez", "Javier", ""], ["Fiori", "Elena Rufeil", ""]]}, {"id": "1304.7948", "submitter": "Christian Osendorfer", "authors": "Christian Osendorfer, Justin Bayer, Patrick van der Smagt", "title": "Convolutional Neural Networks learn compact local image descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard deep convolutional neural network paired with a suitable loss\nfunction learns compact local image descriptors that perform comparably to\nstate-of-the art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 10:41:26 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2013 20:32:05 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Osendorfer", "Christian", ""], ["Bayer", "Justin", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1304.8052", "submitter": "Binjie Qin", "authors": "Binjie Qin, Zhijun Gu, Xianjun Sun, Yisong Lv", "title": "Registration of Images with Outliers Using Joint Saliency Map", "comments": "Preprint version for publication in IEEE Signal Processing Letters,\n  17(1):91-94, 2010", "journal-ref": null, "doi": "10.1109/LSP.2009.2033728", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Mutual information (MI) is a popular similarity measure for image\nregistration, whereby good registration can be achieved by maximizing the\ncompactness of the clusters in the joint histogram. However, MI is sensitive to\nthe \"outlier\" objects that appear in one image but not the other, and also\nsuffers from local and biased maxima. We propose a novel joint saliency map\n(JSM) to highlight the corresponding salient structures in the two images, and\nemphatically group those salient structures into the smoothed compact clusters\nin the weighted joint histogram. This strategy could solve both the outlier and\nthe local maxima problems. Experimental results show that the JSM-MI based\nalgorithm is not only accurate but also robust for registration of challenging\nimage pairs with outliers.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 22:30:09 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Qin", "Binjie", ""], ["Gu", "Zhijun", ""], ["Sun", "Xianjun", ""], ["Lv", "Yisong", ""]]}, {"id": "1304.8092", "submitter": "Togerchety Hitendra sarma", "authors": "P.Shanmugavadivu, V.Sivakumar", "title": "Fractal-Based Detection of Microcalcification Clusters in Digital\n  Mammograms", "comments": "Appeared in ICECIT-2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel method for edge detection of microcalcification\nclusters in mammogram images is presented using the concept of Fractal\nDimension and Hurst co-efficient that enables to locate the microcalcifications\nin the mammograms. This technique detects the edges accurately than the ones\nobtained by the conventional Sobel method. Generally, Sobel method detects the\nedges of the regions/objects in an image using the Fudge factor that assumes\nits value as 0.5, by default. In this proposed technique, the Fudge factor is\nsuitably replaced with Hurst Co-efficient, which is computed as the difference\nof Fractal dimension and the topological dimension of a given input image.\nThese two dimensions are image-dependent, and hence the respective Hurst\nco-efficient too varies with respect to images. Hence, the image-dependent\nHurst co-efficient based Sobel method is proved to produce better results than\nthe Fudge factor based Sobel method. The results of the proposed method\nsubstantiate the merit of the proposed technique.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 17:51:27 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Shanmugavadivu", "P.", ""], ["Sivakumar", "V.", ""]]}]